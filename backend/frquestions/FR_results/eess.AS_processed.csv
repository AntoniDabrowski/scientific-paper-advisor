url,title,further research,primary category,label,x,y,z
http://arxiv.org/pdf/2201.00480v1,TFCN: Temporal-Frequential Convolutional Network for Single-Channel Speech Enhancement,"We will further improve the performance                            Using TCN with Multiple Encoder-Decoder Layers,” in Proc. Inter-
and parameter-efﬁciency of TFCN in our future work. speech 2020, 2020, pp. ",eess.AS,C,-0.007201121,0.0034322534,-0.11753099
http://arxiv.org/pdf/2201.01461v1,Maximizing the Psycho-Acoustic Sweet Spot,"Finally, although we have not fully developed a theory for the convergence of SWEET-ReLU, our experiments show
that it converges in practice. Further analysis will be the subject of future work. 8 Conclusion

In this work, we considered the sweet spot as the region where the a sound scene is psycho-acoustically close to a
desired auditory scene. ",eess.AS,C,-0.00637192,0.23318431,0.095430404
http://arxiv.org/pdf/2201.01461v2,Towards Maximizing a Perceptual Sweet Spot,"Furthermore, our proof-of-concept implementation
avoids any potential issues arising from the discretization of the models, either due to numerical computation of the
Green function or transfer functions, or to the discretization of the integral that deﬁnes the weighted area. Further
analysis about this point will be the subject of future work. 13
8 Conclusion

In this work, we introduced a theoretical framework for spatial audio perception that allows the deﬁnition of a perceptual
sweet spot, that is, the region where the spatial auditory illusion is achieved when approximating one sound wave
by another. ",eess.AS,C,0.022492949,0.41006476,0.039916746
http://arxiv.org/pdf/2201.01669v1,Using Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough,"391–394. to improving model performance, future work should focus
on collecting high-volumes of PCR-validated samples and             [9] I. Song, “Diagnosis of pneumonia from sounds collected using low cost
assessing the model generalizability to new datasets. ",eess.AS,C,-0.091260225,0.101857856,-0.112508014
http://arxiv.org/pdf/2201.01669v2,Using Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough,"CONCLUSION
                                                                      [5] L. Kru¨ger, A. Tanuri, A. Lindner, M. Gaeddert, L. Ko¨ppel, F. Tobian,
   This study is a step forward toward an accurate, reliable, and          L. Bru¨mmer, J. Klein, F. Lainati, P. Schnitzler, O. Nikolai, F. Mock-
accessible audio-based COVID-19 diagnostic test, showing ad-               enhaupt, J. Seybold, V. Corman, T. Jones, C. Drosten, C. Gottschalk,
equate performance on a large and diverse dataset. In addition             S. Weber, S. Weber, O. Ferreira, D. Mariani, S. N. E. Dos, P. C. T.
to improving model performance, future work should focus                   Pereira, R. Galliez, D. Faffe, I. Leita˜o, S. R. C. Dos, T. Frauches,
on collecting high-volumes of PCR-validated samples and                    K. Nocchi, N. Feitosa, S. Ribeiro, N. Pollock, B. Knorr, A. Welker,
assessing the model generalizability to new datasets. Decision-            V. M. de, J. Sacks, S. Ongarello, and C. Denkinger, “Accuracy and
making errors should be closely studied to understand how                  ease-of-use of seven point-of-care SARS-CoV-2 antigen-detecting tests:
prediction performance is affected by audio recording condi-               A multi-centre clinical evaluation.” EBioMedicine, vol. ",eess.AS,C,0.055394974,0.06836602,-0.010290872
http://arxiv.org/pdf/2201.01669v3,Using Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough,"This study is a step forward toward an accurate, reliable, and
accessible audio-based COVID-19 diagnostic test, showing ad-          [5] L. Kru¨ger, A. Tanuri, A. Lindner, M. Gaeddert, L. Ko¨ppel, F. Tobian,
equate performance on a large and diverse dataset. In addition             L. Bru¨mmer, J. Klein, F. Lainati, P. Schnitzler, O. Nikolai, F. Mock-
to improving model performance, future work should focus                   enhaupt, J. Seybold, V. Corman, T. Jones, C. Drosten, C. Gottschalk,
on collecting high-volumes of PCR-validated samples and                    S. Weber, S. Weber, O. Ferreira, D. Mariani, S. N. E. Dos, P. C. T.
assessing the model generalizability to new datasets. Decision-            Pereira, R. Galliez, D. Faffe, I. Leita˜o, S. R. C. Dos, T. Frauches,
making errors should be closely studied to understand how                  K. Nocchi, N. Feitosa, S. Ribeiro, N. Pollock, B. Knorr, A. Welker,
prediction performance is affected by audio recording condi-               V. M. de, J. Sacks, S. Ongarello, and C. Denkinger, “Accuracy and
tions, patient symptoms, and the time point during the course              ease-of-use of seven point-of-care SARS-CoV-2 antigen-detecting tests:
of COVID-19 infection. ",eess.AS,C,-0.006622893,0.027278163,-0.017093567
http://arxiv.org/pdf/2201.02184v1,Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,"An audio-HuBERT model trained with targets gener-
ated by an AV-HuBERT model shows superior performance, achieving the SOTA in the audio-based
speech recognition in the LRS3 dataset. As future work, AV-HuBERT can be applied for multilin-
gual lip-reading in low-resource languages. Additionally, our approach can be extended to other
applications of visual speech representation, such as speech enhancement and generation. ",eess.AS,C,-0.27774158,0.022754356,-0.08936338
http://arxiv.org/pdf/2201.02184v2,Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,"An audio-HuBERT model trained with targets gener-
ated by an AV-HuBERT model shows superior performance, achieving the SOTA in the audio-based
speech recognition in the LRS3 dataset. As future work, AV-HuBERT can be applied for multilin-
gual lip-reading in low-resource languages. Additionally, our approach can be extended to other
applications of visual speech representation, such as speech enhancement and generation. ",eess.AS,C,-0.27774158,0.022754356,-0.08936338
http://arxiv.org/pdf/2201.03211v1,Noisy Neonatal Chest Sound Separation for High-Quality Heart and Lung Sounds,"Addi-
                                                                     tionally, instead of reassignment post decomposition, there is
                                                                     the possibility of adding a temporal constraint or cost function,
                                                                     which would enable the decomposition of the components
                                                                     based on temporal activation. However, future work would be
                                                                     required to develop this temporal decomposition method. With regards to computational cost, the proposed NMCF
                                                                   11

method is not suitable for real-time processing, whereas the       to obtain clean reference sounds with a full frequency range is
proposed NMF method is. ",eess.AS,C,-0.08062621,0.33786702,-0.08013923
http://arxiv.org/pdf/2201.03881v1,Learning to Enhance or Not: Neural Network-Based Switching of Enhanced and Observed Signals for Overlapping Speech Recognition,"sources and mixtures for training, it can also be optimized for real
This is most likely because the issue of whether enhancement im-         data. In our future works, we plan to apply the switching to more
proves or harms ASR cannot be uniquely determined for each SIR           realistic data, including real-recorded mixtures as well as partially
and SNR condition as revealed in Fig. 2.                                 overlapping recordings. ",eess.AS,C,-0.07596139,0.12794453,-0.17454457
http://arxiv.org/pdf/2201.03943v3,Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks,"V). The signiﬁcant WER differences observed         Gumbel-Softmax searched TDNN-F and CNN TDNN-F sys-
       between these two systems suggest further research is       tems (Sys (9) & (11)) on the CHM subset of Hub5’00 test set
       required to improve the transferability and generalization  and Rt03S test sets when compared with the other hybrid and
       of NAS methods across different data sets and quantities. end-to-end systems (Sys (1)-(7) in Tab. ",eess.AS,B,0.2519841,-0.05375291,-0.17074841
http://arxiv.org/pdf/2201.05771v1,"KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics","pronunciation, word skipping, and incomplete words. Another challenge is that Kazakh is an agglutinative
This analysis indicates that there is still room for im-   language, with a very large vocabulary and many char-
provement and future work should focus on eliminating      acters per word. It is also susceptible to morphophone-
these errors. ",eess.AS,A,-0.02044165,-0.28766617,0.33446872
http://arxiv.org/pdf/2201.05771v2,"KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics","Conclusion
common error types among all speakers are mispro-
nunciation, incomplete words, and word skipping. This      We have presented KazakhTTS2, a large-scale open-
analysis indicates that there is still room for improve-   source Kazakh text-to-speech corpus, which further ex-
ment and future work should focus on eliminating these     tends the previous work with more data, voices, and
errors. topics. ",eess.AS,A,-0.16370103,-0.2587207,0.30504256
http://arxiv.org/pdf/2201.05845v1,Recent Progress in the CUHK Dysarthric Speech Recognition System,"The last section draws          line 12 of Table II. This baseline system architecture was
the conclusions and discusses possible future works. For all        manually designed by applying a series of modiﬁcations on
results presented in this paper, matched pairs sentence-segment     top of the ﬁrst phonetic hybrid DNN system (Sys. ",eess.AS,A,-0.11974472,-0.10222024,0.101795234
http://arxiv.org/pdf/2201.05845v2,Recent Progress in the CUHK Dysarthric Speech Recognition System,"The last section draws          line 12 of Table II. This baseline system architecture was
the conclusions and discusses possible future works. For all        manually designed by applying a series of modiﬁcations on
results presented in this paper, matched pairs sentence-segment     top of the ﬁrst phonetic hybrid DNN system (Sys. ",eess.AS,A,-0.11974472,-0.10222024,0.101795234
http://arxiv.org/pdf/2201.06685v1,How Bad Are Artifacts?: Analyzing the Impact of Speech Enhancement Errors on ASR,"We believe that the theoretical interpretations and experimental
                                                                         ﬁndings of this paper will give a novel insight into the issues of a
                                                                         single-channel SE front-end for ASR and open novel research direc-
                                                                         tions for single-channel robust ASR. In our future work, we plan to investigate alternative training
                                                                         schemes for the SE front-end that consider the importance of min-
                                                                         imizing artifacts. Furthermore, we will extend our experiments to
                                                                         other SE approaches and multi-speaker conditions. ",eess.AS,C,-0.0697996,0.05652202,-0.21619876
http://arxiv.org/pdf/2201.06685v2,How Bad Are Artifacts?: Analyzing the Impact of Speech Enhancement Errors on ASR,"We believe that the theoretical interpretations and experi-
                                                                         mental ﬁndings of this paper will give a novel insight into the
                                                                         issues of a single-channel SE front-end for ASR and open novel
                                                                         research directions for single-channel robust ASR. In our future work, we plan to investigate alternative SE
                                                                         training schemes that consider the importance of minimizing
                                                                         artifacts. Furthermore, we will extend our experiments to other
                                                                         SE front-ends and ASR back-ends trained on enhanced data. ",eess.AS,B,0.107055694,-0.01910535,-0.30702934
http://arxiv.org/pdf/2201.06868v1,A Study on the Ambiguity in Human Annotation of German Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis,"test videos. This is certainly due to the insufﬁciency of  In future work with oral history, multi-label training
anger in the training data. The model classiﬁes most       should be considered to account for these aspects. ",eess.AS,A,-0.032300483,-0.24134761,0.052380648
http://arxiv.org/pdf/2201.09422v1,Variational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition,"The absolute WER      obtained by using variability encodings. The future works may
reduction is 2.2% over the baseline system. The joint use of        focus on applying VAEVE to acoustic modeling of noisy data. ",eess.AS,C,-0.042752624,0.15336117,0.0013086218
http://arxiv.org/pdf/2201.09432v1,Investigation of Deep Neural Network Acoustic Modelling Approaches for Low Resource Accented Mandarin Speech Recognition,"Experimental results
are presented in section 4. Section 5 draws the conclusions and
discusses future work. 2.3. ",eess.AS,B,0.4497732,0.1141818,0.23083772
http://arxiv.org/pdf/2201.09586v1,PickNet: Real-Time Channel Selection for Ad Hoc Microphone Arrays,"Dealing with overlapped speech is left         3.2. PickNet: Model with cross-channel layers
to future work [10, 11]. Unlike the model described above, the proposed PickNet model
3. ",eess.AS,C,-0.19380611,0.089534745,-0.07545844
http://arxiv.org/pdf/2201.09913v1,A Novel Temporal Attentive-Pooling based Convolutional Recurrent Architecture for Acoustic Signal Enhancement,"In 43rd Inter-
tion has yet to be carried out. In future work, we aim to consider              national Congress on Noise Control Engineering (Inter-Noise
more diverse training data and different types of non-stationary                2014). noises using a range of benchmark datasets to further scrutinize
the performance of TAP-CRNN, in relation to more recent                     [7] Li, S., Li, F., Tang, S., & Xiong, W. (2020). ",eess.AS,C,0.0549504,0.13337338,-0.15859313
http://arxiv.org/pdf/2201.10105v1,Prediction of Neonatal Respiratory Distress in Term Babies at Birth from Digital Stethoscope Recorded Chest Sounds,"As stated in the previous paragraph, a large percentage of
segments were removed due to poor quality. This suggests          [5] R. Pardasani, R. Chaudhuri, N. Awasthi, S. Chaurasia, and S. Maya,
that future work is required in the denoising and sound                “Quantitative Assessment of Respiratory Distress Using Convolutional
separation method, to enable not only high-quality heart and           Neural Network for Multivariate Time Series Segmentation,” in 2020
lung sounds, but a larger amount of information to work                Computing in Cardiology. IEEE, 2020, pp. ",eess.AS,C,0.039612833,0.20350724,-0.059858155
http://arxiv.org/pdf/2201.10207v1,SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training,"We presume SPIRAL as a general pre-training
method, which can apply to other modalities such as images and text. We leave it for future work. REFERENCES                                               Layer normalization, 2016. ",eess.AS,B,0.2628629,-0.14572074,-0.19686782
http://arxiv.org/pdf/2201.10207v2,SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training,"We presume SPIRAL as a general pre-training
method, which can apply to other modalities such as images and text. We leave it for future work. REFERENCES                                               Layer normalization, 2016. ",eess.AS,B,0.2628629,-0.14572074,-0.19686782
http://arxiv.org/pdf/2201.10207v3,SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training,"We presume SPIRAL as a general pre-training
method, which can apply to other modalities such as images and text. We leave it for future work. 9
Published as a conference paper at ICLR 2022

REFERENCES                                               Layer normalization, 2016. ",eess.AS,B,0.24967909,-0.13389666,-0.24792758
http://arxiv.org/pdf/2201.12557v1,Polyphonic audio event detection: multi-label or multi-class multi-task classification problem?,"An optimal    siren (id=1) and mixer (id=13) were selected for visualization. As
task decomposition would be beneﬁcial, however, we leave this open      can be seen from the ﬁgure, the attention masks of the two subnets
question for future work. Also, it is easy to see that more tasks will  are very distinguishable, suggesting that the subnets learned differ-
result in a larger network and increased computational overhead. ",eess.AS,B,0.21206148,0.20161808,-0.03026733
http://arxiv.org/pdf/2202.00601v1,BEA-Base: A Benchmark for ASR of Spontaneous Hungarian,"The database and
                                                                 the checkpoints of the best ASR baselines have been made
Table 7: Huggingface ID’s of pretrained models                   available for the research community. As for future work, regarding ASR evaluations, our plan is
more to the multilingual self-supervised pretraining than to     to continue investigations by adding external language
the supervised retraining to German. Looking at the error        model – the challenge here is to produce a matching spoken
rates of Turkish6 and Hungarian6 based approaches, we can        language text corpus with a sufficient size. ",eess.AS,A,-0.1510052,-0.44618905,0.0071312534
http://arxiv.org/pdf/2202.00951v1,TONet: Tone-Octave Network for Singing Melody Extraction from Polyphonic Music,"posed as a plug-and-play framework for researchers to test
One possible reason is that the TCFP may lose some octave     and optimize additional models. In the future work, we will
information without the tone-octave fusion, which reduces     focus on validating TONet on more datasets and devising
the ﬁnal OA. This emphasizes the combination of TCFP and      more solid audio features (e.g., music/audio embedding in
the tone-octave decoder to introduce better performance. ",eess.AS,C,0.008543077,0.18774542,-0.06533175
http://arxiv.org/pdf/2202.01092v1,The CORAL++ Algorithm for Unsupervised Domain Adaptation of Speaker Recogntion,"We believe that the core idea of CORAL++ could also
the eigenvalues follow the normal distribution and the adjustment of   provide hints for the improvement of neural network training and
α becomes easier and more stable across of different datasets. adaptation in future works. Table 2. ",eess.AS,B,0.25758678,-0.06327178,-0.16687874
http://arxiv.org/pdf/2202.01664v1,Removing Distortion Effects in Music Using Deep Neural Networks,"By discussing the results, we stressed the usefulness of mul-
                                                                               tiple evaluation metrics suitable to assess distortion removal
      SI-SDR       SI-SDR    PEAQ                         Rnonlin              systems. It remains an issue for future work to conduct a
      PEAQ             1.00    0.37                           0.53             comparative listening test on distorted audio signals to obtain
      Rnonlin          0.37    1.00                           0.56             knowledge on the respective correlation between the metrics
                       0.53    0.56                           1.00             used in this study and human perception. 0                                                 1.0                             Future work should include gathering more clean electric
                                                                               guitar data and generating a dataset using high-quality distor-
1PEAQ [ODG]                                       0.9                          tion emulations, which is required to improve generalization
                                        R-nonlin                               on real-world data. ",eess.AS,C,0.00018524413,0.21095158,-0.026353076
http://arxiv.org/pdf/2202.01664v2,Removing Distortion Effects in Music Using Deep Neural Networks,"By discussing the results, we stressed the usefulness of mul-
                                                                               tiple evaluation metrics suitable to assess distortion removal
      SI-SDR       SI-SDR    PEAQ                         Rnonlin              systems. It remains an issue for future work to conduct a
      PEAQ             1.00    0.37                           0.53             comparative listening test on distorted audio signals to obtain
      Rnonlin          0.37    1.00                           0.56             knowledge on the respective correlation between the metrics
                       0.53    0.56                           1.00             used in this study and human perception. 0                                                 1.0                             Future work should include gathering more clean electric
                                                                               guitar data and generating a dataset using high-quality distor-
1PEAQ [ODG]                                       0.9                          tion emulations, which is required to improve generalization
                                        R-nonlin                               on real-world data. ",eess.AS,C,0.00018524413,0.21095158,-0.026353076
http://arxiv.org/pdf/2202.01664v3,Distortion Audio Effects: Learning How to Recover the Clean Signal,"Lastly, Sec. 7 gives a conclusion and
                                          sufuji, “Distortion Audio Effects: Learning How to Recover the Clean        presents an outlook for future work. Audio examples are
                                          Signal”, in Proc. ",eess.AS,C,-0.028224062,0.34472758,0.11365404
http://arxiv.org/pdf/2202.02687v1,Cross-Channel Attention-Based Target Speaker Voice Activity Detection: Experimental Results for M2MeT Challenge,"Finally, Section 5 concludes this paper and presents
                                         DER by 28% and achieves a DER of 2.26%. Our ﬁnal submitted               the future work. system achieves a DER of 2.98% on the AliMeeting test set, which
                                         ranks 1st in the M2MET challenge. ",eess.AS,B,0.36017534,-0.10260649,0.054899804
http://arxiv.org/pdf/2202.03543v1,Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling,"In our
vestigation of this issue for future work. FaST-VGS+ out-          future work, we plan to investigate improved strategies for
performs other models that are trained solely on LS960, and        combining visual grounding and masked language modeling
ranks the second overall in keyword spotting (KS), which           objectives within self-supervised speech models. shows that visual grounding can drastically reduce the need
for audio data (1.7k hours v.s. ",eess.AS,A,-0.31148547,-0.15783948,-0.12071297
http://arxiv.org/pdf/2202.03543v2,Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling,"In our
vestigation of this issue for future work. FaST-VGS+ out-          future work, we plan to investigate improved strategies for
performs other models that are trained solely on LS960, and        combining visual grounding and masked language modeling
ranks the second overall in keyword spotting (KS), which           objectives within self-supervised speech models. shows that visual grounding can drastically reduce the need
for audio data (1.7k hours v.s. ",eess.AS,A,-0.31148547,-0.15783948,-0.12071297
http://arxiv.org/pdf/2202.03587v1,CALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations,"CALM pretraining can also aid in robustness and
scalability of pretrained systems. While the experimentation in this work is focused on emotion
recognition tasks, we intend to investigate the efﬁcacy of our approach on different tasks, datasets,
signal resolutions, and modalities in future work. Section 4 presents more details of the contributions
and reasoning for the proposed architecture. ",eess.AS,B,0.08707081,-0.052365504,-0.10195868
http://arxiv.org/pdf/2202.03875v1,Unsupervised Source Separation via Self-Supervised Training,"We also defer evaluating our methods on the noisy
on the performance of the unsupervised methods. The bottom         version of Libri2Mix and real-life mixtures [4] to future work. section of the table lists the unsupervised methods. ",eess.AS,B,0.12054201,0.12905434,-0.08909566
http://arxiv.org/pdf/2202.05256v1,Conditional Diffusion Probabilistic Model for Speech Enhancement,"(1):
                                                                                   process that is theoretically sound. To further research the effect of

                                           T                                       incorporating noisy signal in the diffusion model, in Sec. 3.3, we will

        pθ(x0, · · · , xT −1|xT ) = pθ(xt−1|xt),                              (3)  set δt according to Eq. ",eess.AS,B,0.2663961,0.24811602,0.19344416
http://arxiv.org/pdf/2202.05397v1,Neural Architecture Search for Energy Efficient Always-on Audio Models,"We also find 1D convolutional networks
MobileNetV2-Avg-Pool  3       24       (2, 2) 1.4M                        to be significantly more energy efficient but poor in task accuracy. MobileNetV2           5       24       (1, 1) 0.3M                        For use cases where even less energy per inference is targeted, we
Identity              3       -        (1, 1) -                           suggest further research into expanding the search space to use 1D
MobileNetV1           -       40       (1, 1) 0.2M                        convolutional blocks and/or combining this approach with model
MobileNetV2           3       32       (1, 1) 0.9M                        compression techniques (e.g. weight pruning). ",eess.AS,B,0.13565981,-0.030831764,-0.3484114
http://arxiv.org/pdf/2202.06338v1,DEEPCHORUS: A Hybrid Model of Multi-scale Convolution and Self-attention for Chorus Detection,"Experimental results show Deep-
                                                                        Chorus outperforms the existing state-of-the-art methods on three
                                                                        datasets signiﬁcantly. Designing a method to improve the perfor-
                                                                        mance of chorus detection remains an avenue for future work. The code of DeepChorus and the reproduced model is available
                                                                        at https://github.com/Qqi-HE/DeepChorus. ",eess.AS,C,-0.14354989,0.061488267,-0.18064472
http://arxiv.org/pdf/2202.06524v1,Tight integration of neural- and clustering-based diarization through deep unfolding of infinite Gaussian mixture model,"In
ers. Because there is a mismatch between the training and testing         future work, we plan to investigate the proposed EEND-VC-iGMM
conditions, we used a part of the CH data for adaptation. We use the      in such challenging conditions. ",eess.AS,B,0.1713046,-0.1955105,-0.02513858
http://arxiv.org/pdf/2202.06684v1,Partially Fake Audio Detection by Self-attention-based Fake Span discovery,"Table 3 exhausts the experimental settings under      be model-agnostic and feature-agnostic. Our future work will ex-
four different window sizes, three pooling strategies, whether to use   plore the potential of the proposed strategy by adopting other back-
the data augmentation and whether to use the re-synthesised fake au-    bone anti-spooﬁng models and front-end features. dios by Grifﬁn-Lim and WORLD. ",eess.AS,B,0.25996605,0.064167544,-0.06244064
http://arxiv.org/pdf/2202.06684v2,Partially Fake Audio Detection by Self-attention-based Fake Span Discovery,"Table 3 exhausts the experimental settings under      be model-agnostic and feature-agnostic. Our future work will ex-
four different window sizes, three pooling strategies, whether to use   plore the potential of the proposed strategy by adopting other back-
the data augmentation and whether to use the re-synthesised fake au-    bone anti-spooﬁng models and front-end features. dios by Grifﬁn-Lim and WORLD. ",eess.AS,B,0.25996605,0.064167544,-0.06244064
http://arxiv.org/pdf/2202.08883v1,Curriculum optimization for low-resource speech recognition,"It means that harder k is preferred earlier in the    ﬁculty measures improve over the baseline model, CR outper-
epoch and the model gets more information from the harder
forms them in several instances. We believe that the perfor-   [11] R. Wang, M. Utiyama, and E. Sumita, “Dynamic sen-
mance of curriculum learning is dependent on the complexity          tence sampling for efﬁcient training of neural machine
metric distribution and can be further optimized by selecting        translation,” in Proceedings of the 56th ACL (Volume
other values of K. We leave this study for the future work. 2: Short Papers), Melbourne, Australia, July 2018, pp. ",eess.AS,B,0.31351954,-0.2911695,0.0006475728
http://arxiv.org/pdf/2202.09081v1,VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion,"For both GRID and LRW,       tative and qualitative results show that state-of-the-art performance
60 generated utterances are randomly selected from different sys-        can be achieved based on various objective and subjective metrics
tems for MOS tests, where 15 utterances are selected for each of 4       under both constrained (e.g., GRID) and unconstrained (e.g., LRW)
testing speakers of GRID, and the results are reported in Table 1.       conditions. Our future work aims to study more challenging condi-
                                                                         tions for VTS including cross-domain and multi-lingual scenarios. For the GRID, we observe that compared with results on seen
testing speakers, the performance on unseen testing speakers of all                           5. ",eess.AS,A,-0.11162682,-0.16175938,0.1882528
http://arxiv.org/pdf/2202.09124v1,Multi-view and Multi-modal Event Detection Utilizing Transformer-based Multi-sensor fusion,"Fig. 5 shows an example of a visualization of the scene in
formance improvement in future work by further combining more           which one person calls out to another person, who is sitting, and
sophisticated feature fusion with MultiTrans output. These results      then exits the room from the door. ",eess.AS,B,0.16721418,0.09699914,0.11382297
http://arxiv.org/pdf/2202.10290v1,Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition,"Section V presents the experimental
is by far the largest available and widely used dysarthric         results and analysis. Section VI draws the conclusion and
speech database, while DementiaBank Pitt is the largest pub-       discusses possible future works. licly available elderly speech corpus. ",eess.AS,A,-0.17853586,-0.19367684,0.27332294
http://arxiv.org/pdf/2202.10290v2,Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition,"Section V presents the experimental
is by far the largest available and widely used dysarthric         results and analysis. Section VI draws the conclusion and
speech database, while DementiaBank Pitt is the largest pub-       discusses possible future works. licly available elderly speech corpus. ",eess.AS,A,-0.17853586,-0.19367684,0.27332294
http://arxiv.org/pdf/2202.10290v3,Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition,"Section V presents the experimental
is by far the largest available and widely used dysarthric         results and analysis. Section VI draws the conclusion and
speech database, while DementiaBank Pitt is the largest pub-       discusses possible future works. licly available elderly speech corpus. ",eess.AS,A,-0.17853586,-0.19367684,0.27332294
http://arxiv.org/pdf/2202.10536v1,Spanish and English Phoneme Recognition by Training on Simulated Classroom Audio Recordings of Collaborative Learning Environments,"Although we present two diﬀerent programs that facilitate this
procedure, we reiterate that this task remains tedious and time-consuming. For future work, we plan to ﬁnalize the English version of AOLME-Transcripts
by thoroughly checking the produced recordings from TTSMP3. Then, after running
the recordings through our simulation and augmentation framework, we will train our
model on such and assess performance on the English version of AOLME-Sentences. ",eess.AS,A,-0.09157768,-0.33520705,0.06393202
http://arxiv.org/pdf/2202.10593v1,VADOI:Voice-Activity-Detection Overlapping Inference For End-to-end Long-form Speech Recognition,"We also propose a novel Soft-Match mechanism to project
                                                                the operation cost of substitution and matching into continu-
                                                                ous space to compensate for mis-aligned similar words. For
                                                                future work,we plan to train a better Neural-Network based
                                                                VAD to replace the naive statistical VAD. We will also inves-
                                                                tigate how to use acoustic features to estimate optimal over-
                                                                lapping percentage and segmentation length. ",eess.AS,A,-0.07651983,-0.070673406,-0.12407
http://arxiv.org/pdf/2202.12169v1,Closing the Gap between Single-User and Multi-User VoiceFilter-Lite,"1370–1384, 2020.
ple enrolled users. Thus as our future work, we would like to
adopt the best practices from the multi-user VoiceFilter-Lite to     [8] Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian
other speaker-conditioned speech models, including personal-              McGraw, Raziel Alvarez, Ding Zhao, David Rybach, An-
ized ASR or personal VAD. juli Kannan, Yonghui Wu, Ruoming Pang, et al., “Stream-
                                                                          ing end-to-end speech recognition for mobile devices,” in
                   6. ",eess.AS,A,-0.30158228,-0.0012396739,0.11364147
http://arxiv.org/pdf/2202.12169v2,Closing the Gap between Single-User and Multi-User VoiceFilter-Lite,"3-speaker and 4-speaker evaluations. To address this issue, one
of our future work directions is to balance our training data ac-    [3] Jun Wang, Jie Chen, Dan Su, Lianwu Chen, Meng Yu,
cording to the realistic distributions of the number of users on          Yanmin Qian, and Dong Yu, “Deep extractor network for
shared devices, as well as to make the model more robust to               target speaker recovery from single channel speech mix-
unbalanced data. tures,” in Proc. ",eess.AS,C,-0.27377492,0.121540636,-0.11262842
http://arxiv.org/pdf/2202.12326v1,Towards Better Meta-Initialization with Task Augmentation for Kindergarten-aged Speech Recognition,"The reasons why VTLP and SP did not achieve
augmentation that is not task dependent. In raw augmentation     better results will be explored in the future work. (Raw Aug), warping is applied to the original data. ",eess.AS,B,0.29523835,-0.11941275,-0.14073503
http://arxiv.org/pdf/2202.12957v1,Deep Neural Network for Automatic Assessment of Dysphonia,"In the last paragraph of section 4.1 the possibility of achieving clinical applications based on the presented neural
network is mentioned. Although this model is thought to be a good starting point for future work, it should not be
interpreted that the ﬁnal model presented can be used directly in clinical practice. This is a frequent situation in artiﬁcial
intelligence applied to medicine. ",eess.AS,B,0.25152048,-0.0825597,-0.037376598
http://arxiv.org/pdf/2202.13066v1,Revisiting Over-Smoothness in Text to Speech,"Denote
• Enhanced modeling methods outperform MAE
  in synthesized voice quality. Laplacian mixture               4In this work, we mainly focus on text (phoneme) to mel-
                                                            spectrogram mapping, and leave mel-spectrogram to wave-
    3The term ""variance information"" is ﬁrst mentioned in   form mapping and text to waveform mapping to future work. FastSpeech 2 (Ren et al., 2020), which refers some speech-
related conditional information                                 5We approximate the mel-spectrogram distribution on
                                                            LJSpeech dataset. ",eess.AS,C,-0.29072872,0.091878474,0.09896129
http://arxiv.org/pdf/2202.13693v1,Explainable deepfake and spoofing detection: an attack analysis using SHapley Additive exPlanations,"252–265,
ing utterances contained in the ASVspoof 2019 LA evaluation              2021.
partition generated by 13 different attacks, is proving even more
challenging. [6] X. Li, N. Li, C. Weng, X. Liu, D. Su, D. Yu et al., “Re-
                                                                         play and synthetic speech detection with Res2Net archi-
     One current direction for our future work involves the iden-        tecture,” in Proc. ICASSP, 2021, pp. ",eess.AS,A,-0.24852349,-0.09727058,0.06320261
http://arxiv.org/pdf/2202.13693v2,Explainable deepfake and spoofing detection: an attack analysis using SHapley Additive exPlanations,"252–265,
ing utterances contained in the ASVspoof 2019 LA evaluation              2021.
partition generated by 13 different attacks, is proving even more
challenging. [6] X. Li, N. Li, C. Weng, X. Liu, D. Su, D. Yu et al., “Re-
                                                                         play and synthetic speech detection with Res2Net archi-
     One current direction for our future work involves the iden-        tecture,” in Proc. ICASSP, 2021, pp. ",eess.AS,A,-0.24852349,-0.09727058,0.06320261
http://arxiv.org/pdf/2202.13826v1,Magnitude-aware Probabilistic Speaker Embeddings,"15629–15637. In the future work, we aim at training a magnitude-aware
embedding extractor from scratch to get rid of the ad hoc dura-      [14] Brummer, N., Silnova, A., Burget, L., and Stafylakis,
tion variability compensation transform. Another direction in-             T., “Gaussian meta-embeddings for efﬁcient scoring of
                                                                           a heavy-tailed PLDA model,” in Odyssey, 2018, pp. ",eess.AS,B,0.09663275,-0.24234194,-0.16202125
http://arxiv.org/pdf/2202.13826v2,Magnitude-aware Probabilistic Speaker Embeddings,"pus [21], the VoxCeleb1 test and the VOiCES Eval set. From
Figure 2 (Right) one can see that the learned magnitudes allow            In the future work, we aim at training a magnitude-aware
for more accurate separation between the clean speech (Vox-          embedding extractor from scratch to get rid of the ad hoc dura-
Celeb) and non-speech (MUSAN) than those from the extrac-            tion variability compensation transform. Another direction in-
tors shown on Figure 1. ",eess.AS,C,-0.22881174,-0.017265623,-0.010685123
http://arxiv.org/pdf/2202.13826v3,Magnitude-aware Probabilistic Speaker Embeddings,"pus [21], the VoxCeleb1 test and the VOiCES Eval set. From
Figure 2 (Right) one can see that the learned magnitudes allow            In the future work, we aim at training a magnitude-aware
for more accurate separation between the clean speech (Vox-          embedding extractor from scratch to get rid of the ad hoc dura-
Celeb) and non-speech (MUSAN) than those from the extrac-            tion variability compensation transform. Another direction in-
tors shown on Figure 1. ",eess.AS,C,-0.22881174,-0.017265623,-0.010685123
http://arxiv.org/pdf/2203.01573v1,The Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the 2022 ADD Challenge,"Thus, our
hand, compared with [15], our approach exploits the represen-        system ranked ﬁrst and fourth position in tracks 1 and 2 of
tations of the different transformer layers, allowing compara-       2022 ADD challenge, respectively. As future work, we will
tive results while using a simpler downstream model. More-           test other self-supervised models as well as additional data
over, the classiﬁer can be effectively adapted using data aug-       augmentation techniques. ",eess.AS,B,0.12895325,-0.14605357,-0.34504038
http://arxiv.org/pdf/2203.01992v1,On the relevance of language in speaker recognition,"pp.387-390. ICASSP 85

Our future work will include:                                [3] F. Bimbot, L. Mathan ""Text-free speaker recognition
- The study of the influence of the variations when          using an arithmetic-harmonic sphericity measure. pp.169-
training and testing sesions are different. ",eess.AS,A,-0.22013417,0.09014322,0.18477936
http://arxiv.org/pdf/2203.02191v1,Selective Pseudo-labeling and Class-wise Discriminative Fusion for Sound Event Detection,"baselines, and our ﬁnal SED system performances are compet-
                                                                      itive with the ones from top ranked systems in DCASE 2021
     Fourth, by comparing system 6 and 3, it’s clear that our         Task 4 Challenge. Our future work will focus on designing a
proposed selective pseudo-labeling brings absolute 1.4%, 0.8%         better separation system to improve the SED system. and 1.5% collar-based F1, PSDS1 and PSDS2 improvement, re-
spectively. ",eess.AS,B,0.17876896,-0.10131756,0.018906273
http://arxiv.org/pdf/2203.02288v1,Integrating Statistical Uncertainty into Neural Network-Based Speech Enhancement,"In our proposed method with uncertainty estimation,     enhancement performance across different speech and noise datasets. we can use not only the estimated Wiener ﬁlter, but also the estimated     For future work, it would be interesting to integrate the uncertainty
A-MAP mask that incorporates both the estimated uncertainty and            estimation into multi-modal learning systems, which may rely more
Wiener ﬁlter, as given in (9). This estimate is denoted as A-MAP in        on other modalities when audio modality raises high uncertainty. ",eess.AS,C,-0.18825024,0.25354904,-0.10289968
http://arxiv.org/pdf/2203.02680v1,Language vs Speaker Change: A Comparative Study,"Therefore it is hypothesized that, the ldvt distri-      ter observing the training and validation loss and accu-
bution has higher values than the ldvf distribution. With       racy, the model corresponding to the 12th epoch is chosen
increase in x around the change point, the CD measure           for further study. for both the measure should increase, as with increase in
number of voiced frames. ",eess.AS,B,0.1742033,-0.02592519,0.04951053
http://arxiv.org/pdf/2203.05265v1,Echo-enabled Direction-of-Arrival and range estimation of a mobile source in Ambisonic domain,"Despite its simplicity, the method is
surprisingly efﬁcient, as demonstrated on the real audio data              [11] I. Dokmanic´, R. Parhizkar, A. Walther, Y. M. Lu, and M. Vetterli,
from the LOCATA challenge. The future work will focus on                         “Acoustic echoes reveal room shape,” Proceedings of the National
improving its robustness, e.g. in the multiple source scenario. ",eess.AS,C,-0.20867664,0.46309584,-0.034044582
http://arxiv.org/pdf/2203.05383v1,KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset of Stuttering,"reveal why the predictive power of those features is so
Even though the average of all W2V2 features vec-          big, even when averaging values over whole clips. tors per clip was used for classiﬁcation, thereby com-     For future work, we plan to explore multi-class clas-
pletely ignoring the sequential nature of the problem,     siﬁcation of stuttering with a single classiﬁer for all
the SVM utilizing W2V2 features performs best con-         types of stuttering. Future research should focus on
sistently over all experiments and dysﬂuency types. ",eess.AS,B,0.11138795,-0.0019854996,-0.049657628
http://arxiv.org/pdf/2203.05383v2,KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset of Stuttering,"(2018). For future work, we plan to explore multi-class clas-          Fluency Bank: A new resource for ﬂuency research
siﬁcation of stuttering with a single classiﬁer for all        and practice. Journal of Fluency Disorders, 56:69–
types of stuttering. ",eess.AS,A,0.057327252,-0.01706944,0.29978102
http://arxiv.org/pdf/2203.08009v1,Text-free non-parallel many-to-many voice conversion using normalising flows,and prosody). Investigating these controls is left as future work. 6. ,eess.AS,A,-0.005284272,0.007292995,0.32069206
http://arxiv.org/pdf/2203.08350v1,A Squeeze-and-Excitation and Transformer based Cross-task System for Environmental Sound Recognition,"xxiv
5.2.1. Analysis of acoustic scene classiﬁcation

To further study the aforementioned contribution of Transformer and SE

modules for ASC in Sec. 5.1.2, the confusion matrices achieved by CNN9,

CNN4-Trans, and SE-Trans are shown in Fig. ",eess.AS,C,-0.048448,0.16956231,-0.09918706
http://arxiv.org/pdf/2203.09122v1,To train or not to train adversarially: A study of bias mitigation strategies for speaker recognition,"On the other hand,
individual fairness requires that two individuals who are similar to each other be treated similarly [45]. In
the context of this work, we focus on group fairness, leaving individual fairness for future work. 2. ",eess.AS,B,0.32755113,0.058655694,0.27696908
http://arxiv.org/pdf/2203.10274v1,Exploiting Cross Domain Acoustic-to-articulatory Inverted Features For Disordered Speech Recognition,"Experimental results are shown in Section 4. The conclusions
are drawn and future works are discussed in Section 5.                  generalization on the same task, for example, the TORGO corpus. 2. ",eess.AS,A,0.017090594,-0.27616644,0.26513904
http://arxiv.org/pdf/2203.10637v2,Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise,"Also, developing a framework for automatically
that they never heard the same target utterance twice. The total                                     determining optimal vocal effort level with respect to masking
number of samples in the test was 34,560, consisting of 2 voices                                     noise type and level is left for future work. × 4 systems × 2 noise types × 3 SNRs × 720 utterances. ",eess.AS,A,-0.14715096,0.17327522,0.2232303
http://arxiv.org/pdf/2203.10827v1,Separating Content from Speaker Identity in Speech for the Assessment of Cognitive Impairments,"Table 3: CN vs. IM classiﬁcation results using content embed-
dings, extracted with 3 different speaker encoders                        7. Conclusions and future work

Pre-trained      Acc. F1      Spec. ",eess.AS,A,-0.23254013,-0.04412429,-0.031396937
http://arxiv.org/pdf/2203.13422v1,Pseudo-Label Transfer From Frame-Level To Note-Level In A Teacher-Student Framework for Singing Transcription From Polyphonic Music,"This indicates that the pro-     sets cover only Chinese music in this paper, we plan to evaluate
posed method has synergy with supervised learning. the method on various genres of music in different cultural back-
                                                                       grounds as future work. 5. ",eess.AS,C,-0.032366693,0.06848099,-0.078816585
http://arxiv.org/pdf/2203.13422v2,Pseudo-Label Transfer from Frame-Level to Note-Level in a Teacher-Student Framework for Singing Transcription from Polyphonic Music,"This indicates that the pro-     sets cover only Chinese music in this paper, we plan to evaluate
posed method has synergy with supervised learning. the method on various genres of music in different cultural back-
                                                                       grounds as future work. 5. ",eess.AS,C,-0.032366693,0.06848099,-0.078816585
http://arxiv.org/pdf/2203.13919v1,Spatial Processing Front-End For Distant ASR Exploiting Self-Attention Channel Combinator,"This information has many potential appli-
The ASR performance with different front-ends is evaluated      cations including speaker diarization and localization, which
in terms of the word error rate (WER). A baseline condition     is something we will explore in future work. of single distant microphone (SDM) is also included for ref-
erence (this is an ASR model trained with channel 4 of the                             4. ",eess.AS,C,-0.1889224,0.121656984,-0.023507573
http://arxiv.org/pdf/2203.14525v1,Self-supervised curriculum learning for speaker verification,"To valid this assumption, we
                                                                   design an additional experiment, which is less likely to occur
         Base 6.70 5.87 5.54 4.47                                  under natural data collection pipelines, where we modify the
         CL A1 6.35 6.08 5.10 4.69                                 CL D3 to include all speakers regardless of the portion of train
         CL A2 6.64 5.99 5.39 4.85                                 dataset used; in other words, we ﬁx the between-speaker factor
                                                                   and only test the within-speaker factor since only the number
combination of the two strategies that can form an appropriate     of utterances per speaker increases in this experiment. This ex-
level of difﬁculty as future work. periment resulted in an EER of 6.41%, in which the relative
                                                                   improvement over the baseline is 4.3%. ",eess.AS,A,-0.11556521,0.059374265,0.3084867
http://arxiv.org/pdf/2203.14561v1,An Effective Dereverberation Algorithm by Fusing MVDR and MCLP,"Within a certain distortion
range, the proposed method can remove more interference than
the traditional MVDR beamforming. In the future work, the
                   8. References                                                  Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2018,
                                                                                  pp. ",eess.AS,C,-0.09122446,0.45629364,0.058165815
http://arxiv.org/pdf/2203.14593v1,On-the-fly Feature Based Speaker Adaptation for Dysarthric and Elderly Speech Recognition,"5 draws the         the bottleneck layer of the lower classiﬁer in Fig. 1 and con-
conclusion and discusses possible future works. catenated to the acoustic features at the front-end of hybrid
                                                                     DNN/TDNN systems as shown in Fig. ",eess.AS,C,-0.041049458,0.23911767,-0.1317434
http://arxiv.org/pdf/2203.14593v2,On-the-fly Feature Based Speaker Adaptation for Dysarthric and Elderly Speech Recognition,"5 draws the         the bottleneck layer of the lower classiﬁer in Fig. 1 and con-
conclusion and discusses possible future works. catenated to the acoustic features at the front-end of hybrid
                                                                     DNN/TDNN systems as shown in Fig. ",eess.AS,C,-0.041049458,0.23911767,-0.1317434
http://arxiv.org/pdf/2203.14865v1,Towards Transferable Speech Emotion Representation: On loss functions for cross-lingual latent representations,"2 (d), wherein happy and anger samples overlap in the latent        supervision using a continuous distance metric instead of a classiﬁ-
space. A probable reason for this could be that the latent representa-   cation loss and is left for future work. tions correspond to activation more than valence. ",eess.AS,B,0.33893448,-0.23138112,0.16620451
http://arxiv.org/pdf/2203.14867v1,Continuous Metric Learning For Transferable Speech Emotion Recognition and Embedding Across Low-resource Languages,"the latent space. While the R2 with respect to valence
Investigating and including aspects of the true relation                        is generally lower than that for activation, between the
between valence, activation and the embedding will be                           methods, the proposed models outperform the baseline
addressed in future work. In conclusion, we can state                           method for the datasets. ",eess.AS,B,0.1322318,-0.2192694,-0.059842102
http://arxiv.org/pdf/2203.15081v1,"Word Discovery in Visually Grounded, Self-Supervised Speech Models","We veriﬁed that
ES-KMeans [6]     42473 72.3 100.0 43.4 39.6 61.4 48.2               this same ability did not exist in the models prior to them un-
                                                                     dergoing training on the visual grounding task, suggesting that
SEA [43]          240033 89.5 99.5 19.0 27.3 75.9 40.1               a combination of multiple self-supervised objectives may be es-
                                                                     sential in order for a computational model to learn the structure
PDTW [44]         85425 48.2 85.4 64.5 26.5 88.2 40.8                of spoken language from untranscriped speech audio. VG-HuBERT3 (Ours) 104696 42.5 95.4 71.8 44.7 54.2 49.0                    In our future work, we plan to use the word-level represen-
                                                                     tations learned by our model to anchor the learning of sub-word
     Results on Buckeye and ZeroSpeech 2020. Finally, we             structure (e.g. ",eess.AS,A,-0.064461514,-0.3448396,-0.07354423
http://arxiv.org/pdf/2203.15081v2,"Word Discovery in Visually Grounded, Self-Supervised Speech Models","We veriﬁed that
ES-KMeans [6]     42473 72.3 100.0 43.4 39.6 61.4 48.2                   this same ability did not exist in the models prior to them un-
                                                                         dergoing training on the visual grounding task, suggesting that
SEA [43]          240033 89.5 99.5 19.0 27.3 75.9 40.1                   a combination of multiple self-supervised objectives may be es-
                                                                         sential in order for a computational model to learn the structure
PDTW [44]         85425 48.2 85.4 64.5 26.5 88.2 40.8                    of spoken language from untranscriped speech audio. VG-HuBERT3 (Ours) 104696 42.5 95.4 71.8 44.7 54.2 49.0                        In our future work, we plan to use the word-level represen-
                                                                         tations learned by our model to anchor the learning of sub-word
     Results on Buckeye and ZeroSpeech 2020. Finally, we                 structure (e.g. ",eess.AS,A,-0.064461514,-0.3448396,-0.07354423
http://arxiv.org/pdf/2203.15081v3,"Word Discovery in Visually Grounded, Self-Supervised Speech Models","Results on Buckeye and ZeroSpeech 2020. Finally, we                      In our future work, we plan to use the word-level represen-
test our best performing model (VG-HuBERT3) 1, on two addi-              tations learned by our model to anchor the learning of sub-word
                                                                         structure (e.g. automatically learning a pronunciation lexicon
tional benchmarks: the ZeroSpeech 2020 spoken term discov-               in terms of discovered speech units) as well as the higher level
ery track (English) and the Buckeye word segmentation task2. ",eess.AS,A,-0.14464799,-0.29106307,0.074264444
http://arxiv.org/pdf/2203.15081v4,"Word Discovery in Visually Grounded, Self-Supervised Speech Models","We veriﬁed that
ES-KMeans [6]     42473 73.2 100.0 42.3 49.4 66.8 56.7                   this same ability did not exist in the models prior to them un-
                                                                         dergoing training on the visual grounding task, suggesting that
SEA [43]          240033 89.5 99.5 19.0 32.5 78.9 46.1                   a combination of multiple self-supervised objectives may be es-
                                                                         sential in order for a computational model to learn the structure
PDTW [44]         85425 48.2 85.4 64.5 26.5 88.2 40.8                    of spoken language from untranscriped speech audio. VG-HuBERT3 (Ours) 104696 42.5 95.4 71.8 44.7 54.2 49.0                        In our future work, we plan to use the word-level represen-
                                                                         tations learned by our model to anchor the learning of sub-word
     Results on Buckeye and ZeroSpeech 2020. Finally, we                 structure (e.g. ",eess.AS,A,-0.06590997,-0.34581316,-0.07206904
http://arxiv.org/pdf/2203.15147v1,Separate What You Describe: Language-Queried Audio Source Separation,"We draw our conclusions in
                                          limited. In practical applications, instead of using a predeﬁned   Section 6, together with discussions on future work. ﬁxed set of source categories, one may prefer to use a natural
                                          language description to identify and separate the target sound                      2. ",eess.AS,A,-0.10258434,0.028917355,0.25458175
http://arxiv.org/pdf/2203.15405v1,Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations,"Given SSD is mainly manifested in      i-vector using posterior-based features is able to improve de-
phonological errors, the paralinguistic information captured by      tection performance over conventional acoustic features. Our
eGeMAPS may be less informative about the contrast between           future work will be focused on the use of self-supervised rep-
TD and disordered speech. resentation learning and neural conﬁdence measure for subject-
                                                                     level SSD detection. ",eess.AS,A,-0.16603695,-0.097886086,-0.00038431026
http://arxiv.org/pdf/2203.15405v2,Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations,"For disorder symptoms that are mainly
                                                                      manifested in phonological errors, extraction of i-vector us-
     Using speech attribute LPR in the i-vector extraction            ing posterior-based features can improve detection performance
achieves the best detection performance, followed by phone            over conventional acoustic features. Our future work will focus
LPR. The speech attribute LPR describes multiple changes in           on using self-supervised representation learning and neural con-
articulation and aspiration status. ",eess.AS,A,-0.23355344,-0.13605085,-0.061452527
http://arxiv.org/pdf/2203.15405v3,Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations,"For disorder symptoms that are mainly
                                                                      manifested in phonological errors, extraction of i-vector us-
     Using speech attribute LPR in the i-vector extraction            ing posterior-based features can improve detection performance
achieves the best detection performance, followed by phone            over conventional acoustic features. Our future work will focus
LPR. The speech attribute LPR describes multiple changes in           on using self-supervised representation learning and neural con-
articulation and aspiration status. ",eess.AS,A,-0.23355344,-0.13605085,-0.061452527
http://arxiv.org/pdf/2203.15610v1,LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,"(3) We ﬁnd the aSMALL achieves 3.5×                                       ing DistilHuBERT by an absolute improvement of 3.2 point
compression ratio in KS, ASV, and IC tasks with a slight accu-                                    in terms of the overall score. In future work, we will jointly
                                                                                                  employ parameter compression [33] and task-dependent model
                                                                                                  compression [18] to further compress the pre-trained model. 6. ",eess.AS,B_centroid,0.24629754,-0.12556924,-0.24903727
http://arxiv.org/pdf/2203.15610v2,LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,"As                                              KS, and IC tasks with a slight accuracy loss while outperform-
shown in Table 4, we can draw the following conclusions:                                          ing DistilHuBERT by an absolute improvement of 3.2 point
(1) The proposed LightHuBERT can create compressed mod-                                           in terms of the overall score. In future work, we will jointly
els that retain comparable performance on the SUPERB tasks. employ parameter compression [33] and task-dependent model
The aSMALL with around 28% parameter achieves only a 2 point                                      compression [18] to further compress the pre-trained model. ",eess.AS,B,0.2535348,-0.1694863,-0.23690532
http://arxiv.org/pdf/2203.15652v1,CycleGAN-Based Unpaired Speech Dereverberation,"In
overall audio quality. Both evaluations were run in the form         future work, we plan to exploit this and add a latent encoder,
of A/B tests. In each question, the raters were presented with       which will encode reverberation information and will serve as
two audio clips, corresponding respectively to the outputs of the    conditioning to transform the one-to-many mapping into a one-
paired and unpaired models, run on the same reverberant audio        to-one mapping problem. ",eess.AS,C,-0.11093832,0.1914465,-0.11162434
http://arxiv.org/pdf/2203.15863v1,WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models,"In Fluent and SLURP experi-
audio embeddings at a rate closest to that of the text embeddings   ments, increasing shots to four or six yields the best accuracy
as discussed in Sec 2.2.                                            but further increasing shots downgrades the performance. Us-
Calibration We compare the classiﬁcation accuracy with cal-         ing a larger language model might result in a more consistent
ibration versus without calibration using the best downsampling     pattern, which we leave as future work to explore. rate obtained in Table 1. ",eess.AS,A,-0.18389289,-0.14023727,-0.043503713
http://arxiv.org/pdf/2203.15863v2,WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models,"In Fluent and SLURP experi-
audio embeddings at a rate closest to that of the text embeddings   ments, increasing shots to four or six yields the best accuracy
as discussed in Sec 2.2.                                            but further increasing shots downgrades the performance. Us-
Calibration We compare the classiﬁcation accuracy with cal-         ing a larger language model might result in a more consistent
ibration versus without calibration using the best downsampling     pattern, which we leave as future work to explore. rate obtained in Table 1. ",eess.AS,A,-0.18389289,-0.14023727,-0.043503713
http://arxiv.org/pdf/2203.15952v1,4-bit Conformer with Native Quantization Aware Training for Speech Recognition,"In 8-bit cases (I8W), we see no regression in either Large or  It is also worthwhile to experiment with quantizing different
Small model. However, in 4-bit cases (I4W), although the Large      number of layers in the two decoders, which we will continue
model can still retain the ﬂoat performance, the Small model has    investigating in future work. introduced 0.2 WER increase compared to the baseline due to
the limited capacity of the Small model. ",eess.AS,B,0.2675626,0.07953702,-0.1905793
http://arxiv.org/pdf/2203.15952v2,4-bit Conformer with Native Quantization Aware Training for Speech Recognition,"It is also worthwhile to experiment with quantizing different
Small model. However, in 4-bit cases (I4W), although the Large      number of layers in the two decoders, which we will continue
model can still retain the ﬂoat performance, the Small model has    investigating in future work. introduced 0.2 WER increase compared to the baseline due to
the limited capacity of the Small model. ",eess.AS,B,0.2688579,0.10313994,-0.2154037
http://arxiv.org/pdf/2203.15952v3,4-bit Conformer with Native Quantization Aware Training for Speech Recognition,"It is also worthwhile to experiment with quantizing different
Small model. However, in 4-bit cases (I4W), although the Large      number of layers in the two decoders, which we will continue
model can still retain the ﬂoat performance, the Small model has    investigating in future work. introduced 0.2 WER increase compared to the baseline due to
the limited capacity of the Small model. ",eess.AS,B,0.2688579,0.10313994,-0.2154037
http://arxiv.org/pdf/2203.16165v1,Symbolic music generation conditioned on continuous-valued emotions,"PhD
largest open-source symbolic music generation models, that                            thesis, New School for Social Research, 1979.
are trained on the largest multi-instrument symbolic music
dataset, in the literature. [12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
                                                                                      with deep convolutional neural networks,” in Advances in Neural Infor-
   In future work we intend to investigate the potential for                          mation Processing Systems, pp. 1097–1105, 2012.
conditional music generation directly in the audio domain. ",eess.AS,C,-0.061705224,0.11600065,-0.23591483
http://arxiv.org/pdf/2203.16165v2,Symbolic music generation conditioned on continuous-valued emotions,"621–630, 1937. In future work we intend to investigate the potential for
conditional music generation directly in the audio domain. [11] D. S. Levi, Melodic Expression, Melodic Structure, and Emotion. ",eess.AS,C,-0.0481849,0.2207707,0.15853512
http://arxiv.org/pdf/2203.16691v1,MAE-AST: Masked Autoencoding Audio Spectrogram Transformer,"We hypothesize that this implies the MAE architecture is
abstract data, similar to embedding spaces in standard Autoen-    encouraging additional abstraction. We hope that future work
coders. Our results also suggest that the discriminative loss is  can build on these ﬁndings toward the goal of constructing a
more useful for audio tasks while the generative loss is more     scalable multi-domain strategy for self-supervised pretraining. ",eess.AS,C,-0.16200197,-0.0923311,-0.3461514
http://arxiv.org/pdf/2203.16749v1,SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping,tion. These can be possible future work. 6. ,eess.AS,B,0.40889457,-0.0913523,0.16124645
http://arxiv.org/pdf/2203.16749v2,SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping,tion. These can be possible future work. 6. ,eess.AS,B,0.40889457,-0.0913523,0.16124645
http://arxiv.org/pdf/2203.16757v1,Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study,"We
over CHiME-4 is shown in Table 2. Two different front-ends        hope these ﬁndings would be helpful for future work to further
are tested in separate optimization, namely the delay-and-sum     explore better methods to efﬁciently leverage single-channel
beamformer (BeamformIt [28]) and the MVDR based neural            data for multi-channel speech recognition. beamformer (Sec. ",eess.AS,C,-0.13306111,0.13696875,-0.12708749
http://arxiv.org/pdf/2203.16757v2,Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study,"Two different front-ends        forms the other two, but at the cost of longer training time. We
are tested in separate optimization, namely the delay-and-sum     hope these ﬁndings would be helpful for future work to further
beamformer (BeamformIt [28]) and the MVDR based neural            explore better methods to efﬁciently leverage single-channel
beamformer (Sec. 3.1). ",eess.AS,B,0.15015431,0.14748083,-0.25927767
http://arxiv.org/pdf/2203.16758v1,"CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR","While we use Conformer based CTC-
4.3. Discussion                                                      CRF as our ASR model, our approach can be applied to other
                                                                     end-to-end ASR models such as RNN-T and attention based
To observe the quality of the simulated frames, we visualize the     encoder-decoder, which will be interesting future work. simulated log Mel-spectrogram in Fig 3. ",eess.AS,B,0.06948124,-0.118258744,-0.079372086
http://arxiv.org/pdf/2203.16758v2,"CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR","While we use Conformer based CTC-
4.3. Discussion                                                      CRF as our ASR model, our approach can be applied to other
                                                                     end-to-end ASR models such as RNN-T and attention based
To observe the quality of the simulated frames, we visualize the     encoder-decoder, which will be interesting future work. simulated log Mel-spectrogram in Fig 3. ",eess.AS,B,0.06948124,-0.118258744,-0.079372086
http://arxiv.org/pdf/2203.16773v1,An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,"Furthermore, there is a uniﬁed inference process
                                          tion tasks with fewer trainable parameters than ﬁne-tuning spe-    with the original pre-trained LM for all downstream tasks in
                                          cialized downstream models. We further study the technique         the paradigm. Hence, less human labor is required in model au-
                                          in challenging sequence generation tasks. ",eess.AS,B,0.17998631,-0.258275,-0.186982
http://arxiv.org/pdf/2203.16773v2,An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,"Since parameters of tuned prompts are usually
                                         trainable parameters than ﬁne-tuning specialized downstream           several orders smaller than parameters of LMs [17], the prompt-
                                         models. We further study the technique in challenging se-             ing paradigm signiﬁcantly improves memory and computation
                                         quence generation tasks. Prompt tuning also demonstrates its          efﬁciency. ",eess.AS,B,0.26962817,-0.14754611,0.039240655
http://arxiv.org/pdf/2203.16773v3,SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,"Since parameters of tuned prompts are usually
                                          models. We further study the technique in challenging se-              several orders smaller than parameters of LMs [17], the prompt-
                                          quence generation tasks. Prompt tuning also demonstrates its           ing paradigm signiﬁcantly improves memory and computation
                                          potential, while the limitation and possible research directions       efﬁciency. ",eess.AS,B,0.28884363,-0.08451648,0.12640052
http://arxiv.org/pdf/2203.16776v1,An Empirical Study of Language Model Integration for Transducer based Speech Recognition,"This further consolidates and deepens our                 estimated via language models PILM (Y ) ≈ PRNN-T(Y ) and
understanding of the internal LM in RNN-T, and would be help-                 PELM (Y ), respectively. According to [17], PILM (Y ) is es-
ful for future work to further advance LM integration for RNN-                timated by a separately trained neural LM over source domain
T. The code will be released upon the acceptance of the paper. transcripts. ",eess.AS,A,0.0571222,-0.29140627,-0.11318282
http://arxiv.org/pdf/2203.16776v2,An Empirical Study of Language Model Integration for Transducer based Speech Recognition,"This further consolidates and deepens our                 estimated via language models PILM (Y ) ≈ PRNN-T(Y ) and
understanding of the internal LM in RNN-T, and would be help-                 PELM (Y ), respectively. According to [17], PILM (Y ) is es-
ful for future work to further advance LM integration for RNN-                timated by a separately trained neural LM over source domain
T. The code will be released upon the acceptance of the paper. transcripts. ",eess.AS,A,0.0571222,-0.29140627,-0.11318282
http://arxiv.org/pdf/2203.16776v3,An Empirical Study of Language Model Integration for Transducer based Speech Recognition,"This veriﬁes our hypothe-
in-domain test, except that the evaluation is on a new domain       sis and deepens our understanding of the ILM in RNN-T, and
and the ELM is trained on the target domain corpus. would be helpful for future work to further advance LM inte-
                                                                    gration for RNN-T.
     Table. 2 shows the performance of the integration methods
in domain adaptation. ",eess.AS,B,0.072393835,-0.25957283,-0.13196881
http://arxiv.org/pdf/2203.16776v4,An Empirical Study of Language Model Integration for Transducer based Speech Recognition,"This veriﬁes our hypothe-
in-domain test, except that the evaluation is on a new domain       sis and deepens our understanding of the ILM in RNN-T, and
and the ELM is trained on the target domain corpus. would be helpful for future work to further advance LM inte-
                                                                    gration for RNN-T.
     Table. 2 shows the performance of the integration methods
in domain adaptation. ",eess.AS,B,0.072393835,-0.25957283,-0.13196881
http://arxiv.org/pdf/2203.16852v1,JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech,"pus, and the proposed model achieved state-of-the-art results. It
                                                                     would be interesting future works to investigate other combina-
4.3. Evaluation                                                      tions of joint training other than Fastspeech2 and HiFi-GAN, or
                                                                     to search for more efﬁcient E2E-TTS architecture. ",eess.AS,B,0.24045908,-0.085960254,-0.16023727
http://arxiv.org/pdf/2203.16852v2,JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech,"Audio samples are available online 3         state-of-the-art results. It would be interesting for future works
                                                                     to investigate other combinations of joint training other than
    1https://github.com/Kyubyong/g2p                                 FastSpeech2 and HiFi-GAN, or to evaluate on multi-speaker
    2https://zenodo.org/record/4030677                               dataset. 3https://imdanboy.github.io/interspeech2022
                   6. ",eess.AS,C,-0.19046201,0.13607335,-0.045379683
http://arxiv.org/pdf/2203.17019v1,DeepFry: Identifying Vocal Fry Using Deep Neural Networks,"Both the Nuclear and the Prenuclear datasets have the           HubertFry was trained on. Thus, it remains for future work to
same lexical content. However, the Prenuclear and ALLSSTAR          investigate the effect of training with more data on DeepFry. ",eess.AS,A,0.040962245,-0.3480044,-0.001193971
http://arxiv.org/pdf/2203.17019v2,DeepFry: Identifying Vocal Fry Using Deep Neural Networks,"Both the Nuclear and the Prenuclear datasets have the           HubertFry was trained on. Thus, it remains for future work to
same lexical content. However, the Prenuclear and ALLSSTAR          investigate the effect of training with more data on DeepFry. ",eess.AS,A,0.040962245,-0.3480044,-0.001193971
http://arxiv.org/pdf/2203.17068v1,EEND-SS: Joint End-to-End Neural Speaker Diarization and Speech Separation for Flexible Number of Speakers,"The results indicate room for         counting. In addition, we proposed the multiple 1×1 convo-
further improvement using self-supervised features instead of        lutional layer architecture for estimating separation masks for
LMF, which is left for future work. variable number of speakers and the post-processing for reﬁning
                                                                     separated speech with speech activity. ",eess.AS,C,-0.2994133,0.13846925,-0.12039738
http://arxiv.org/pdf/2203.17172v1,Efficient Non-Autoregressive GAN Voice Conversion using VQWav2vec Features and Dynamic Convolution,"Com-
                                                                       paring to the SOTA models, DYGAN-VC has high efﬁciency,
                                                                       and also achieves comparable level of performance of SOTA. For future work, the authors will investigate zero shot VC. 7. ",eess.AS,B,0.30072463,0.025988381,-0.23935229
http://arxiv.org/pdf/2204.00436v1,AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios,"4 can synthesize speech with high quality and similarity in zero-
                                                                   shot scenarios. For future work, we will evaluate AdaSpeech 4
3.3. Ablation Studies                                              in more diverse speaker characteristics and explore advanced
                                                                   techniques to improve the prosody and expressiveness of syn-
     In this section, we conduct ablation studies to verify the    thesized speech in zero-shot scenarios. ",eess.AS,A,-0.22808152,0.03426436,0.18455702
http://arxiv.org/pdf/2204.00465v1,Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition,"Note that when increasing       One limitation is that EMA data is sparsely sampled from ar-
the number of gestures, the PER is not always decreasing, indi-        ticulators and the learned representation is still less intelligible
cating that the intelligibility is not always positive correlated to   than the acoustic features. The future work will focus on ﬁne-
the informativeness. We believe that better intelligibility will be    grained articulatory representations such as [15] to deliver more
achieved when using more ﬁne-grained articulatory representa-          generalizable and intelligible representations. ",eess.AS,C,-0.08544024,-0.004512366,-0.075045526
http://arxiv.org/pdf/2204.00465v2,Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition,"Note that when increasing       One limitation is that EMA data is sparsely sampled from ar-
the number of gestures, the PER is not always decreasing, indi-        ticulators and the learned representation is still less intelligible
cating that the intelligibility is not always positive correlated to   than the acoustic features. The future work will focus on ﬁne-
the informativeness. We believe that better intelligibility will be    grained articulatory representations such as [15] to deliver more
achieved when using more ﬁne-grained articulatory representa-          generalizable and intelligible representations. ",eess.AS,C,-0.08544024,-0.004512366,-0.075045526
http://arxiv.org/pdf/2204.00465v3,Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition,"Note that when increasing       One limitation is that EMA data is sparsely sampled from ar-
the number of gestures, the PER is not always decreasing, indi-        ticulators and the learned representation is still less intelligible
cating that the intelligibility is not always positive correlated to   than the acoustic features. The future work will focus on ﬁne-
the informativeness. We believe that better intelligibility will be    grained articulatory representations such as [21] to deliver more
achieved when using more ﬁne-grained articulatory representa-          generalizable and intelligible representations. ",eess.AS,C,-0.087839246,-0.0031819101,-0.076952055
http://arxiv.org/pdf/2204.00618v1,A single speaker is almost all you need for automatic speech recognition,"comparison in pt-BR, [29] used ca. 158 hrs of speech and a non-
self-supervised model without an external LM and achieved a            In future work, we intend to explore the use of the self-
WER of 47.41% on the test set of BRSD v2 dataset. Despite         supervised model feature extractor as a discriminator during
being different datasets, [30] showed that the test set of the    training of the YourTTS model. ",eess.AS,A,-0.21251684,-0.25290167,-0.030641207
http://arxiv.org/pdf/2204.00618v2,A single speaker is almost all you need for automatic speech recognition,"present in the Wav2Vec 2.0 model pre-training, as well as for an
                                                                    unseen language (Russian). • Upper Bound: ASR models trained on pt-BR and ru-
        RU with Common Voice plus the single-speaker TTS                 In future work, we intend to explore the use of the self-
        dataset. supervised model feature extractor as a discriminator during
                                                                    the training of the YourTTS model. ",eess.AS,A,-0.3276777,-0.1865971,-0.15577617
http://arxiv.org/pdf/2204.00618v3,A single speaker is almost all you need for automatic speech recognition,"We car-      unseen language (Russian). ried out three experiments:
                                                                       In future work, we intend to explore the use of the self-
     • Baseline: ASR models trained with the single-speaker       supervised model feature extractor as a discriminator during
        dataset used during the TTS model training on pt-BR and   the training of the YourTTS model. In this way, the YourTTS
        ru-RU. ",eess.AS,A,-0.2755044,-0.30424145,-0.07248743
http://arxiv.org/pdf/2204.00618v4,ASR data augmentation using cross-lingual multi-speaker TTS and cross-lingual voice conversion,"ple was chosen in preliminary experiments. We would like to
note that, in preliminary experiments, we explored increasing            In future work, we intend to explore the use of the self-
the number of speakers per sentence in the GEN TTS dataset;         supervised model feature extractor as a discriminator during
however, this did not bring signiﬁcant improvements. We car-        the training of the YourTTS model. ",eess.AS,A,-0.18468314,-0.29335275,0.13970965
http://arxiv.org/pdf/2204.00890v1,From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization,"However, with ﬁne-tuning, the performance can
          6.20  7.28                                                  still be leveraged showing that there is still room for improving
CH stats                                                              the generation of synthetic conversations. In our future work we
 + FT     8.26  8.73           22.29  21.53                           plan to explore the use of RIRs that resemble real scenarios and
                               17.38  17.14                           relevance of VAD and overlap related statistics. Also, we plan
          6.13  7.28                                                  to further exploit our approach for creating conversations with
                                                                      more than 2 speakers and use it with wide-band data where there
4.4. ",eess.AS,A,-0.11406779,0.07765955,0.16180444
http://arxiv.org/pdf/2204.00890v2,From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization,"However, with ﬁne-tuning, the performance can
          6.20  7.28                                                  still be leveraged showing that there is still room for improving
CH stats                                                              the generation of synthetic conversations. In our future work we
 + FT     8.26  8.73           22.29  21.53                           plan to explore the use of RIRs that resemble real scenarios and
                               17.38  17.14                           relevance of VAD and overlap related statistics. Also, we plan
          6.13  7.28                                                  to further exploit our approach for creating conversations with
                                                                      more than 2 speakers and use it with wide-band data where there
4.4. ",eess.AS,A,-0.11406779,0.07765955,0.16180444
http://arxiv.org/pdf/2204.01355v1,Target Confusion in End-to-end Speaker Extraction: Analysis and Approaches,"Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “Sdr–
the performance by more than 1dB SI-SDRi. In future work,                       half-baked or well done?” in IEEE International Conference on
we plan to extend our methods to more complicated scenarios,                    Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019,
for example, multi-talker (#spk ≥ 3) and noisy extraction. ",eess.AS,C,-0.12034559,0.18019818,-0.020770676
http://arxiv.org/pdf/2204.01981v1,Unsupervised Data Selection via Discrete Speech Representation for ASR,"It suggests that the
pre-train for 100k steps, as the relative comparison stays the     two data selection methods can be complementary, and we leave
same for more steps. empirical studies on this as future work. Table 5 compares WERs when different quantizers are used                         5. ",eess.AS,B,0.37322825,-0.09307018,0.0026554614
http://arxiv.org/pdf/2204.02135v1,Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction,"Introduction                                  methods (e.g., MUSHRA, MOS). However, further research is
                                                                                                            needed to understand the level of inﬂuence that the data (e.g.,
                                         Speech quality assessment is key to monitoring and evaluating      language, type of distortion, amount of samples) used for ﬁne-
                                         the performance of applications and services in which speech       tuning is exerting over the model’s performance. is an essential component. ",eess.AS,A,-0.20152965,0.024937315,0.17161132
http://arxiv.org/pdf/2204.02281v1,Design Guidelines for Inclusive Speaker Verification Evaluation Datasets,"We propose evaluation dataset design guide-
                                         practices that are inclusive and fair. lines and an outlook on future work in Section 5 and conclude
                                         Index Terms: speaker veriﬁcation, voice biometrics, evalua-         in Section 6.
                                         tion, audit, bias, fairness, design guidelines
                                                                                                                               2. Background
                                                           1. ",eess.AS,A,-0.20085675,0.005601194,0.2032108
http://arxiv.org/pdf/2204.02281v2,Design Guidelines for Inclusive Speaker Verification Evaluation Datasets,"We propose evaluation dataset design guide-
                                          of SV evaluation practices that are inclusive and fair. lines and an outlook on future work in Section 5 and conclude
                                          Index Terms: speaker veriﬁcation, voice biometrics, evalua-         in Section 6.
                                          tion, audit, bias, fairness, design guidelines
                                                                                                                                2. Background
                                                            1. ",eess.AS,A,-0.19508669,-0.018062428,0.21181905
http://arxiv.org/pdf/2204.02306v1,Leveraging Speech Separation for Conversational Telephone Speaker Diarization,"In the framing stage, a windowing op-          tion output. It is applied on each estimated source X˜ indepen-
eration splits Y into I overlapped frames Yi ∈ R1×W , i =                dently but future work could consider a multi-source VAD. We
1, . ",eess.AS,B,0.29432434,0.20809549,0.02071529
http://arxiv.org/pdf/2204.02306v2,Low-Latency Speech Separation Guided Diarization for Telephone Conversations,"∼10000 hours) and results in shorter training times and less burden
regarding additional costs for the generation of simulated mixtures. In future work we will consider several strategies to reduce this
                                                                       gap such as training with more data, comparable to the amount used
4.2. Ofﬂine Separation/Diarization                                     in EEND models, and ﬁne-tuning our models on the CALLHOME
                                                                       adaptation set (as done in [7, 9]). ",eess.AS,C,0.07038227,0.0063410327,-0.18081915
http://arxiv.org/pdf/2204.02381v1,Hear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning,"However, we see that decoder/discriminator                   with MTL. In future work, we aim to investigate regularization
MTL is still not able to beat CTC/decoder MTL when the CTC                    methods that can reduce the peaky behavior of CTC so as to
head is dropped for inference. induce more adversarially robust hybrid inference. ",eess.AS,B,0.032504503,-0.17369345,-0.30178002
http://arxiv.org/pdf/2204.02385v1,Learning Speech Emotion Representations in the Quaternion Domain,"decrease of model’s performance, as shown in Section VI-B. We compare the performance on SER tasks of real-valued
D. Applications and future work                                                 CNNs fed with regular spectrograms and quaternion-valued
   The advantages provided by the combination of RH-emo                         CNNs fed with RH-emo embeddings. We evaluate our ap-
                                                                                proach on a variety of cases, using 4 popular SER datasets
and quaternion-valued networks suggest several application                      (Iemocap, Ravdess, EmoDb, Tess) and with 3 widely-used
scenarios. ",eess.AS,B,0.18008338,-0.053020388,-0.27123526
http://arxiv.org/pdf/2204.02841v1,Spectral Denoising for Microphone Classification,"We will discuss a mitigation          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania). strategy for this issue and other further research directions in the         Association for Computing Machinery, Bucharest, Romania, 278–286. https:
following section. ",eess.AS,B,0.120777294,-0.05375649,-0.12776202
http://arxiv.org/pdf/2204.02841v2,Spectral Denoising for Microphone Classification,"We will discuss a mitigation          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania). strategy for this issue and other further research directions in the         Association for Computing Machinery, Bucharest, Romania, 278–286. https:
following section. ",eess.AS,B,0.120777294,-0.05375649,-0.12776202
http://arxiv.org/pdf/2204.02841v3,Spectral Denoising for Microphone Classification,"We will discuss a miti-          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania). gation strategy for this issue and other further research directions          Association for Computing Machinery, Bucharest, Romania, 278–286. https:
in the following section. ",eess.AS,B,0.103688486,-0.076140694,-0.1284769
http://arxiv.org/pdf/2204.02841v4,Spectral Denoising for Microphone Classification,"We will discuss a miti-          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania). gation strategy for this issue and other further research directions          Association for Computing Machinery, Bucharest, Romania, 278–286. https:
in the following section. ",eess.AS,B,0.103688486,-0.076140694,-0.1284769
http://arxiv.org/pdf/2204.03219v1,DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores,"When we ex-          system-level MOS. We would try to modify DAPT to obtain
clude DAPT, all correlation metrics decline signiﬁcantly, which     better utterance-level results for future works since we observe
implies that DAPT is necessary for DDOS. For data augmenta-         that removing DAPT slightly improves utterance-level MSE. ",eess.AS,B,0.11943158,0.0016180109,0.1232773
http://arxiv.org/pdf/2204.03219v2,DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores,"When we ex-          system-level MOS. We would try to modify DAPT to obtain
clude DAPT, all correlation metrics decline signiﬁcantly, which     better utterance-level results for future works since we observe
implies that DAPT is necessary for DDOS. For data augmenta-         that removing DAPT slightly improves utterance-level MSE. ",eess.AS,B,0.11943158,0.0016180109,0.1232773
http://arxiv.org/pdf/2204.03219v3,DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores,"When we ex-          system-level MOS. We would try to modify DAPT to obtain
clude DAPT, all correlation metrics decline signiﬁcantly, which     better utterance-level results for future works since we observe
implies that DAPT is necessary for DDOS. For data augmenta-         that removing DAPT slightly improves utterance-level MSE. ",eess.AS,B,0.11943158,0.0016180109,0.1232773
http://arxiv.org/pdf/2204.03305v1,MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids,"experimental setup and results. Finally, Section V provides con-     Along this idea, the proposed MBI-Net uses two branches of
clusions and future work. models, ﬁrst processing the speech signal separately, and em-
                                                                     ploying a fusion layer to compute the ﬁnal prediction scores for
                 2. ",eess.AS,C,-0.17125788,0.14758632,-0.058619615
http://arxiv.org/pdf/2204.03305v2,MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids,"experimental setup and results. Finally, Section V provides con-     Along this idea, the proposed MBI-Net uses two branches of
clusions and future work. models, ﬁrst processing the speech signal separately, and em-
                                                                     ploying a fusion layer to compute the ﬁnal prediction scores for
                 2. ",eess.AS,C,-0.17125788,0.14758632,-0.058619615
http://arxiv.org/pdf/2204.03310v1,MTI-Net: A Multi-Target Speech Intelligibility Prediction Model,"Section IV describes experimental setup and results. Finally,
                                                                                                               the conclusions and future work are presented in Section V.
                 2. Related Work                                                    Speech Utterances

2.1. ",eess.AS,A,-0.26344955,-0.025729075,0.4199069
http://arxiv.org/pdf/2204.03310v2,MTI-Net: A Multi-Target Speech Intelligibility Prediction Model,"Finally,
                                          trusive and non-intrusive metrics. Intrusive metrics [1, 2, 3, 4, 5]  the conclusions and future work are presented in Section V.
                                          require the corresponding clean speech as a reference to mea-
                                          sure intelligibility scores, while non-intrusive metrics do not                        2. Related Work
                                          need a reference [6, 7]. ",eess.AS,A,0.00095089036,-0.06980155,0.38355327
http://arxiv.org/pdf/2204.03339v1,Boosting Self-Supervised Embeddings for Speech Enhancement,"By in-
                                         tegrating the SSL representation and spectrogram, the result can           Currently, there are only few studies applying SSL features
                                         be signiﬁcantly boosted. We further study the relationship be-        to SE. Huang et al. ",eess.AS,B,0.17766121,-0.0004924163,0.12872031
http://arxiv.org/pdf/2204.03339v2,Boosting Self-Supervised Embeddings for Speech Enhancement,"By in-
                                         tegrating the SSL representation and spectrogram, the result can           Currently, there are only few studies applying SSL features
                                         be signiﬁcantly boosted. We further study the relationship be-        to SE. Huang et al. ",eess.AS,B,0.17766121,-0.0004924163,0.12872031
http://arxiv.org/pdf/2204.03379v1,Correcting Misproducted Speech using Spectrogram Inpainting,"Results suggest that our proposed model has a slight
74.1% on Original, 74.7% on Vocoder only and 71.0% on the           advantages in subjective listening tests. Our future work will
Generated. We suspect that the main reason for the low values       focus on improving the phoneme embedding by looking at self-
for all audio types is that the tested utterance is too short, and
therefore did not contain enough information for the speaker to
supervised learning algorithms to improve representation. ",eess.AS,A,-0.31073937,-0.05154994,0.0062910863
http://arxiv.org/pdf/2204.03379v2,Correcting Mispronunciations in Speech using Spectrogram Inpainting,"Results suggest that our proposed model has a slight
74.1% on Original, 74.7% on Vocoder only and 71.0% on the           advantages in subjective listening tests. Our future work will
Generated. We suspect that the main reason for the low values       focus on improving the phoneme embedding by looking at self-
for all audio types is that the tested utterance is too short, and
therefore did not contain enough information for the speaker to
supervised learning algorithms to improve representation. ",eess.AS,A,-0.31073937,-0.05154994,0.0062910863
http://arxiv.org/pdf/2204.03417v1,Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0,"where ﬁne-tuning, in general, seems to harm the already weak
performance. Word repetitions need the largest acoustic context           In future work, we plan to improve the feasibility of W2V2
to be recognizable and repeated words are acoustically barely        features for all kinds of dysﬂuency events and extend our work
distinguishable from normal speech. Averaging over all vectors       to the multi-class scenario, not only detecting, but differenti-
extracted from the 3s clips removes much information, w.r.t. ",eess.AS,A,-0.26845312,-0.029351797,-0.011650801
http://arxiv.org/pdf/2204.03417v2,Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0,"the classiﬁcation performance is positive across     the KSoF dataset. all experiments, but the word repetition experiment on KSoF,
where ﬁne-tuning, in general, seems to harm the already weak              In future work, we plan to improve the feasibility of W2V2
performance. Word repetitions need the largest acoustic context      features for all kinds of dysﬂuency events and extend our work
to be recognizable and repeated words are acoustically barely        to the multi-class scenario, not only detecting, but differenti-
distinguishable from normal speech. ",eess.AS,A,-0.23608436,-0.049969237,0.096320175
http://arxiv.org/pdf/2204.03428v1,Detecting Vocal Fatigue with Neural Embeddings,"As such the methods described in this paper detect
                                                                     only one aspect of a complex phenomenon, but do so reliably. We applied SVMs trained on all LMELectures (IMIP and
PA) with a duration of more than 60 minutes, using the set of hy-         In future work, we will strive towards more granular pre-
perparameters that led to the best results in Table 1, on the addi-  dictions (e.g. multiclass classiﬁcation or regression) and over-
tional DL lectures corpus (cf. ",eess.AS,A,0.0041473685,-0.25534013,-0.07058912
http://arxiv.org/pdf/2204.03855v1,Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition,"Monolingual Speech Recognition Evaluation                      beneﬁt similar units that share binary representations and also
                                                                    signiﬁcantly increase the speed of the decoding compared to
As we can see in Table 4, under single language scenario, our       both traditional Softmax methods. In future work, we plan
Huffman code greatly reduced the inference time while achiev-       to design binary codes that can consider not only shallow fre-
                                                                    quency terms but more semantic meaning, such as the word or
    2The implementation can be found at https://github.com/Derek-   character embedding of the token. Gong/hsoftmax
                   5. ",eess.AS,A,-0.15004027,-0.2003001,-0.061089493
http://arxiv.org/pdf/2204.03895v1,SoundBeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning,"38, no. 5,
also imply that future work is required to further improve                    pp. 67–83, 2021.
performance and better measure performance. ",eess.AS,B,0.47531655,0.0414282,0.31680655
http://arxiv.org/pdf/2204.03895v2,SoundBeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning,"improve extraction performance. For example, future works                             4441–4449. could include designing simulated training data that approx-
imate better mixing conditions of real mixtures, exploiting                     [16] J. H. Lee, H. Choi, and K. Lee, “Audio query-based music source
larger datasets for semi-supervised retraining, or combin-                            separation,” in Proc. ",eess.AS,C,-0.13677531,0.120244175,-0.1932905
http://arxiv.org/pdf/2204.04284v1,Auditory-Based Data Augmentation for End-to-End Automatic Speech Recognition,"The values of rlmax and rumax for different hearing im-              riSpeech when both using and not using an external language model. In the future work, the proposed data augmentation technique will
pairment degree settings are shown in Table 1. For LR, the ran-               be veriﬁed with more challenging speech recognition tasks, such as
                                                                              CHiME 5 [38]. ",eess.AS,A,-0.18158412,-0.04396026,-0.0036735162
http://arxiv.org/pdf/2204.04333v2,A Study of Using Cepstrogram for Countermeasure Against Replay Attacks,"in performance improvement in contrast to single systems,                   6369–6373. although both are quefrency-domain features, and we leave it
as future work and focus on the properties of the cepstrogram          [5] Xu Li, Na Li, Chao Weng, Xunying Liu, Dan Su,
in this study. Dong Yu, and Helen Meng, “Replay and synthetic
                                                                            speech detection with res2net architecture,” in ICASSP
                       6. ",eess.AS,A,-0.20445159,0.056021977,-0.05202733
http://arxiv.org/pdf/2204.04370v1,QuiKo: A Quantum Beat Generation Application,"Thus, the comparison between
the audio ﬁles in the database and the input audio is performed classically. However, in the future work
section, designs and ﬁndings are presented from initial experiments in combining the quantum circuit and
comparison process into one quantum circuit. 2
Algorithm Building Blocks

    Before we dive into the speciﬁcs of the design for this application, we ﬁrst need to discuss the underlying
quantum algorithms and properties that are being utilized. ",eess.AS,C,0.042272907,0.4958678,0.036178857
http://arxiv.org/pdf/2204.04370v2,QuiKo: A Quantum Beat Generation Application,"Thus, the comparison between
the audio ﬁles in the database and the input audio is performed classically. However, in the future work
section, designs and ﬁndings are presented from initial experiments in combining the quantum circuit and
comparison process into one quantum circuit. 2
Algorithm Building Blocks

    Before we dive into the speciﬁcs of the design for this application, we ﬁrst need to discuss the underlying
quantum algorithms and properties that are being utilized. ",eess.AS,C,0.042272907,0.4958678,0.036178857
http://arxiv.org/pdf/2204.04811v1,Listen only to me! How well can target speech extraction handle false alarms?,"However, the TSE-
creases for the TSE-IS, while the curve for TSE-V is much           IS system may remain attractive for, e.g., low-latency systems,
smoother. Consequently, it is more challenging to tune at test      which we plan to explore in our future works. 7. ",eess.AS,B,0.48055482,0.0050413646,0.15845749
http://arxiv.org/pdf/2204.04811v2,Listen only to me! How well can target speech extraction handle false alarms?,"However, the TSE-
TSE-V and TSE-IS. We observe that the miss rate rapidly in-         IS system may remain attractive for, e.g., low-latency systems,
creases for the TSE-IS, while the curve for TSE-V is much           which we plan to explore in our future works. smoother. ",eess.AS,B,0.502839,-8.316245e-06,0.13485678
http://arxiv.org/pdf/2204.05177v1,The PartialSpoof Database and Countermeasures for the Detection of Short Generated Audio Segments Embedded in a Speech Utterance,"Results nonetheless show         [14] M. Morrison, L. Rencker, Z. Jin, N. J. Bryan, J.-P. Caceres, and B. Pardo,
scope for improvement. Our future work will include explicit              “Context-aware prosody correction for text-based speech editing,” in
use of linguistic information for the detection of short spoofed          Proc. ICASSP. ",eess.AS,A,-0.29369557,-0.082802825,0.27747148
http://arxiv.org/pdf/2204.05177v2,The PartialSpoof Database and Countermeasures for the Detection of Short Generated Audio Segments Embedded in a Speech Utterance,"Results nonetheless show         [14] M. Morrison, L. Rencker, Z. Jin, N. J. Bryan, J.-P. Caceres, and B. Pardo,
scope for improvement. Our future work will include explicit              “Context-aware prosody correction for text-based speech editing,” in
use of linguistic information for the detection of short spoofed          Proc. ICASSP. ",eess.AS,A,-0.29369557,-0.082802825,0.27747148
http://arxiv.org/pdf/2204.05419v1,Can Self-Supervised Learning solve the problem of child speech recognition?,"self-supervised features learned from adult speech can be the
Model 11 finetuned with 10m of MyST data gave the worst             key to improving child speech recognition. overall WER of 91.72 on MyST_test and 92.74 on
PFSTAR_test, however, in comparison, model 7 finetuned              For future work, we plan to utilize this model to transcribe
with same data gives WER of 28.84 and 43.03 on MyST_test            more of the MyST data to generate more usable child speech
and PFSTAR_test. Similarly, models {8,9,10} outperform              data. ",eess.AS,A,-0.24846953,-0.21602699,-0.013043447
http://arxiv.org/pdf/2204.05419v2,Can Self-Supervised Learning solve the problem of child speech recognition?,"This
                                                               could imply that a model finetuned with child speech data
For set, A training, Model 6, pretrained on 60k hours of       can achieve improvement for the in-domain test set but a
adult speech and finetuned on 960 hours of adult speech,       decrease in performance on the unseen out-of-domain test
set. This can also be attributed to the noisy nature of the     For future work, we plan to utilize this model to transcribe
MyST dataset. more of the MyST data to generate more usable child speech
                                                                data. ",eess.AS,A,-0.20665057,-0.10954979,0.04748565
http://arxiv.org/pdf/2204.06164v1,A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes,"In con-
                                          sizes might still be required for various application constraints,  trary, our preliminary experiments show that it is not straight-
                                          e.g. a large model might be used for short-form applications        forward to perform distillation between the causal layers and
                                          (like voice search) to obtain the best quality, while a medium      non-causal layers to improve the performance of causal layers,
                                          or a small model might be required for long-running applica-        potentially due to the different right context; this direction is left
                                          tions (like dictation or video captioning) to maintain low power    as future work. Lastly, compared with the alternative approach
                                          consumption. ",eess.AS,B,0.1080034,0.027131677,0.005562284
http://arxiv.org/pdf/2204.06164v2,A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes,"In con-
                                          sizes might still be required for various application constraints,  trary, our preliminary experiments show that it is not straight-
                                          e.g. a large model might be used for short-form applications        forward to perform distillation between the causal layers and
                                          (like voice search) to obtain the best quality, while a medium      non-causal layers to improve the performance of causal layers,
                                          or a small model might be required for long-running applica-        potentially due to the different right context; this direction is left
                                          tions (like dictation or video captioning) to maintain low power    as future work. Lastly, compared with the alternative approach
                                          consumption. ",eess.AS,B,0.1080034,0.027131677,0.005562284
http://arxiv.org/pdf/2204.06164v3,A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes,"Even in the same device, different model         trary, our preliminary experiments show that it is not straight-
                                          sizes might still be required for various application constraints,  forward to perform distillation between the causal layers and
                                          e.g. a large model might be used for short-form applications        non-causal layers to improve the performance of causal layers,
                                          (like voice search) to obtain the best quality, while a medium      potentially due to the different right context; this direction is left
                                          or a small model might be required for long-running applica-        as future work. Lastly, compared with the alternative approach
                                          tions (like dictation or video captioning) to maintain low power    of model shrinking with sparsity networks [19, 20], our model
                                          consumption. ",eess.AS,B,0.060334884,-0.088142,-0.1702919
http://arxiv.org/pdf/2204.06478v1,BEHM-GAN: Bandwidth Extension of Historical Music using Generative Adversarial Networks,"A sixth-           of 8 kHz, missing some high-frequency details. The study of
order Butterworth lowpass ﬁlter is applied at the ﬁxed cutoff        the design of broadband reference-free audio quality metrics
frequency of 2 kHz, 3 kHz, and 4 kHz to imitate different band-      is left as future work. width limitations. ",eess.AS,C,0.08696222,0.4416808,0.14754261
http://arxiv.org/pdf/2204.06478v2,BEHM-GAN: Bandwidth Extension of Historical Music using Generative Adversarial Networks,"), the resulting metrics were            resulted in weaker performance. We leave as future work to
disappointing. These results can be explained by looking at      study ways to allow the model to have better generalization
the example in Fig. ",eess.AS,B,0.3724152,-0.09797403,0.012624297
http://arxiv.org/pdf/2204.06907v1,Lombard Effect for Bilingual Speakers in Cantonese and English: importance of spectro-temporal features,"On the other hand, the SGBFB based prediction
worked for all noise conditions, such that the individual SRT50          Furthermore, the aspect of differences between tonal and
of a speaker was predicted with higher accuracy, which is in        non-tonal languages has not yet been fully explored in this con-
line with hypothesis 1, that spectro-temporal features are im-      tribution. Since bilingual speakers were used, further analysis
portant to describe the individual SRT50. Even so, a bias of -2.6   of the data may uncover if signiﬁcant differences in the Lom-
dB is present for the SGBFB based prediction in the stationary      bard Gain for Cantonese and English exist even for the same
ICRA1 noise, that is expected: FADE works as a model of an          speaker. ",eess.AS,A,-0.2023223,-0.045571156,0.22481942
http://arxiv.org/pdf/2204.09079v1,Music Source Separation with Generative Flow,"We observe that     better results than systems under the same supervision setting. the instGlow with MLE objective achieves the best results         In the future work, we’ll further bridge the performance gap
in terms of SDR across all of the tasks except for bass           between full supervision based state-of-the-art systems and
track in Slakh2100-submix. The MLE objective with only KL-        extend this work on general music source separation. ",eess.AS,C,-0.09654932,0.14976992,-0.200591
http://arxiv.org/pdf/2204.09079v2,Music Source Separation with Generative Flow,"We observe          on benchmark separation tasks, achieving much better results
that the InstGlow with the MLE objective achieves the best          than other systems within the same paradigm. For future work,
results in terms of SDR across all of the tasks. For the            we will further bridge its performance gap to full supervision
InstGlow models, the MLE objective that only uses KL-               systems and extend it to more general settings. ",eess.AS,B,0.12773408,-0.09977543,-0.25055417
http://arxiv.org/pdf/2204.11032v1,Heterogeneous Separation Consistency Training for Adaptation of Unsupervised Speech Separation,"2627–2631. need to be improved in our future works before we apply it
                                                                                [22] Z.-Q. Wang and D. Wang, “Count and separate: Incorporating speaker
to perception sensitive related speech separation applications. ",eess.AS,A,-0.2194576,0.16870031,0.26746494
http://arxiv.org/pdf/2204.11032v2,Heterogeneous Separation Consistency Training for Adaptation of Unsupervised Speech Separation,"2627–2631. need to be improved in our future works before we apply it
                                                                                [22] Z.-Q. Wang and D. Wang, “Count and separate: Incorporating speaker
to perception sensitive related speech separation applications. ",eess.AS,A,-0.2194576,0.16870031,0.26746494
http://arxiv.org/pdf/2204.11032v3,Heterogeneous Separation Consistency Training for Adaptation of Unsupervised Speech Separation,"Le Roux, J. R. Hershey, Alter-
certain extent. This limitation of current SCT still needs           native objective functions for deep clustering, in:
to be improved in our future works before we apply it to             2018 IEEE International Conference on Acoustics,
real speech separation applications. Speech and Signal Processing (ICASSP), IEEE,
                                                                     2018, pp. ",eess.AS,C,-0.24936998,0.20466319,-0.07556432
http://arxiv.org/pdf/2204.11232v1,Improving the Naturalness of Simulated Conversations for End-to-End Neural Diarization,"selection and Markov selection. Constructing a new similarity
                                                                    metric that can express this difference is thus a possible avenue
     Finally, we discuss the differences in EEND-EDA with           for future work. and without domain adaptation. ",eess.AS,B,0.14995494,-0.13625808,-0.029487181
http://arxiv.org/pdf/2204.11286v1,Improved far-field speech recognition using Joint Variational Autoencoder,"Therefore from a theoretical point
presented. Conclusion and future work are presented in Sec-         of view, such domain conversion cannot be justiﬁed within the
tion 4.                                                             premises of conventional VAE. For this reason, results are not
                                                                    shown for DVAE in this paper. ",eess.AS,B,0.3245178,0.070120916,0.07830231
http://arxiv.org/pdf/2204.12092v1,Mask scalar prediction for improving robust automatic speech recognition,"4. Finally, conclusions and future work are presented in Sec. 5. ",eess.AS,B,0.4452386,-0.03294941,0.38079888
http://arxiv.org/pdf/2205.00288v1,Baselines and Protocols for Household Speaker Recognition,"The simplicity and
computationally cheap updates make it a viable solution for               We can now rewrite the LLR score as
household speaker recognition. = xTe (αI)xt + xTt (αI)xe + xTe (βI)xt + xTt (βI)xe

     Our planned future work includes (1) improving the base-             + γµT(xe + xt) + k
lines to improve domain robustness. For instance, it would be
interesting to generalize existing domain adaptation methods              = 2αxTe xt + β( xe 2 + xt 2) + γµT(xe + xt) + k.
to handle online data. ",eess.AS,A,-0.22946858,-0.01671747,-0.05141746
http://arxiv.org/pdf/2205.00288v2,Baselines and Protocols for Household Speaker Recognition,"The simplicity and
computationally cheap updates make it a viable solution for               We can now rewrite the LLR score as
household speaker recognition. = xTe (αI)xt + xTt (αI)xe + xTe (βI)xt + xTt (βI)xe

     Our planned future work includes (1) improving the base-             + γµT(xe + xt) + k
lines to improve domain robustness. For instance, it would be
interesting to generalize existing domain adaptation methods              = 2αxTe xt + β( xe 2 + xt 2) + γµT(xe + xt) + k.
to handle online data. ",eess.AS,A,-0.22946858,-0.01671747,-0.05141746
http://arxiv.org/pdf/2205.00944v1,A Meeting Transcription System for an Ad-Hoc Acoustic Sensor Network,"Single-channel vs. single-array vs. multi-array                 outperform a single-array system. In future work we intend to
                                                                     remove the assumption of ﬁxed speaker positions by combining
Table 1 compares the proposed multi-array meeting transcrip-         spatial with spectral signatures of the speakers. tion system with the single-channel and single-array baselines. ",eess.AS,C,-0.062498394,0.39078814,0.07960967
http://arxiv.org/pdf/2205.01897v1,Virtual Analog Modeling of Distortion Circuits Using Neural Ordinary Differential Equations,"Figure 10: Magnitude of the learned derivative of the second state
      0.25                                                           of the second-order diode clipper for y2 = 0. 0.00
y1  −0.25                                                            solutions of this difficulty could be investigated in future work. −0.50
    −0.75        −0.5   0.0          0.5         1.0                 4.5. ",eess.AS,B,0.3760035,0.11870903,0.0061671734
http://arxiv.org/pdf/2205.01897v2,Virtual Analog Modeling of Distortion Circuits Using Neural Ordinary Differential Equations,"This observation
                        Vin                                          was confirmed in [20], where STN, which is similar in nature to
                                                                     ODENet, failed to fit a phaser dataset. Possible explanations and
                 (b) y2 = 1.                                         solutions of this difficulty could be investigated in future work. Figure 9: Magnitude of the learned derivative of the first state of  4.5. ",eess.AS,B,0.40974554,-0.008239511,-0.046904847
http://arxiv.org/pdf/2205.01897v3,Virtual Analog Modeling of Distortion Circuits Using Neural Ordinary Differential Equations,"This observation
                        Vin                                          was confirmed in [20], where STN, which is similar in nature to
                                                                     ODENet, failed to fit a phaser dataset. Possible explanations and
                 (b) y2 = 1.                                         solutions of this difficulty could be investigated in future work. Figure 9: Magnitude of the learned derivative of the first state of  4.5. ",eess.AS,B,0.40974554,-0.008239511,-0.046904847
http://arxiv.org/pdf/2205.03481v1,A Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy,"Ex-      verted into the predicted waveform by the linear decoder layer
perimental results are presented in Section 5, and conclusions       followed by the overlap-add operation. and ideas for future work in Section 6. As noted in Figure 2, we use a conformer model [15] for the
                 2. ",eess.AS,B,0.26914203,0.24280316,-0.068004
http://arxiv.org/pdf/2205.04421v1,NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality,"Meanwhile, although our evaluations are
conducted on LJSpeech dataset, we believe the technologies in NaturalSpeech can be applied to other
languages, speakers, and styles to improve the general synthesis quality. We will further try to achieve
human-level quality in more challenging datasets or scenarios, such as expressive voices, longform
audiobook voices, and singing voices that have more dynamic, diverse, and contextual prosody for
future work. References

 [1] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. ",eess.AS,A,-0.3186476,-0.09555808,0.05354927
http://arxiv.org/pdf/2205.04421v2,NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality,"Meanwhile, although our evaluations are
conducted on LJSpeech dataset, we believe the technologies in NaturalSpeech can be applied to other
languages, speakers, and styles to improve the general synthesis quality. We will further try to achieve
human-level quality in more challenging datasets or scenarios, such as expressive voices, longform
audiobook voices, and singing voices that have more dynamic, diverse, and contextual prosody in our
future work. References

 [1] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. ",eess.AS,A,-0.3231972,-0.09511481,0.05485593
http://arxiv.org/pdf/2205.04433v1,Speaker Reinforcement Using Target Source Extraction for Robust Automatic Speech Recognition,"Additional accuracy improvements
                                                                      are observed for both the simulated and real test sets. ∞           18.4               19.4
                                                                           As future work, we plan to extend the evaluation of proposed
Denoising-SPX 20         16.5               17.7                      method in reverberant, noisy and multi-talker conditions. (vanilla)    10          14.9               15.5                                                  6. ",eess.AS,C,0.07171192,0.33966953,-0.019268062
http://arxiv.org/pdf/2205.05199v1,Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech,"We tried to address it by combining
Channel 2         <sot>thanks I am fine <eot><sot> oh hi mark <eot>                                                              end-of-turn latency penalty with the best STS model that incor-
                                                                                                                                 porates both FastEmit and masking loss, with limited success. SP                   LS        SP       LS                                                                     The major culprit is a numerical instability of the training with
                         FS                  EP       FS        EP                                                               both FastEmit and end-of-turn latency penalties, and we plan to
                                                                                                                                 address it in the future work. Model output
                                                                                                                                                    6. ",eess.AS,B,0.31952217,-0.010624379,-0.06554136
http://arxiv.org/pdf/2205.05474v1,DeepFilterNet2: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio,"Due to its lightweight architecture, it can be run on a
Net2 achieves SOTA-level results while requiring a mini-       Raspberry Pi 4 with a real-time factor of 0.42. In future work,
mal amount of multiply-accumulate operation per second         we plan to extend the idea of speech enhancement to other
(MACS). The number of parameters has slightly increased        enhancements, like correcting lowpass characteristics due to
over DeepFilterNet (Sec. ",eess.AS,C,-0.122885965,0.21818191,-0.13103323
http://arxiv.org/pdf/2205.05581v1,A deep representation learning speech enhancement method using $β$-VAE,"Moreover, β-PVAE obtains
a better disentanglement performance and achieves higher SI-                   [17] S. Leglaive, L. Girin, and R. Horaud, “A variance modeling framework
SDR, PESQ, and STOI scores than PVAE. In future work,                                based on variational autoencoders for speech enhancement,” in Proc. we believe that β-PVAE can achieve better SE performance                             IEEE Workshop Machine Learning. ",eess.AS,C,-0.08014272,0.014688576,-0.15564124
http://arxiv.org/pdf/2205.05586v1,End-to-End Multi-Person Audio/Visual Automatic Speech Recognition,"We can observe a slight decrease in performance in some of the
so by extrapolating the results reported in [1] we believe that including    datasets as compared to the end-to-end model, but still always better
explicitly corrupted speech in the training would stretch the beneﬁts        than the audio only model. We leave for a future work a more thorough
even further. We leave this evaluation for follow-up work though. ",eess.AS,C_centroid,-0.30057174,-0.055202708,-0.10885896
http://arxiv.org/pdf/2205.05684v1,A Closer Look at Audio-Visual Multi-Person Speech Recognition and Active Speaker Selection,"Dataset Noise Tracks SS [SS + A/V ASR]
                                                                              Note that these are potentially harder datasets than what we
                   2  0.99  0.93                                         would normally encounter in practice since all faces are always
                                                                         speaking here, while it is an easier task to differentiate between
                   4  0.97  0.85                                         a speaking and a non-speaking face video as we would encounter
                                                                         most of the time in a meeting scenario, for example. We adopt this
                   8  0.95  0.77                                         evaluation protocol nonetheless and leave as future work to evaluate
                                                                         our model on more realistic datasets. 2  0.99  0.93
                                                                                                     4. ",eess.AS,C,-0.07143527,0.09745042,0.03580921
http://arxiv.org/pdf/2205.05949v1,Automated Audio Captioning: an Overview of Recent Progress and New Challenges,"A. S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, and S. Albanie,
XM was a major contributor in writing the manuscript. XL summarized              “Audio retrieval with natural language queries: A benchmark study,”
challenges and future work. MP and WW substantively revised the                  IEEE Transactions on Multimedia, 2022.
manuscript. ",eess.AS,A,-0.27476153,0.006134225,0.0102721825
http://arxiv.org/pdf/2205.05949v2,Automated Audio Captioning: An Overview of Recent Progress and New Challenges,"Sun, M. D. Plumbley, and W. Wang, “On metric
XM was a major contributor in writing the manuscript. XL summarized               learning for audio-text cross-modal retrieval,” arXiv preprint
challenges and future work. MDP and WW substantially revised the                  arXiv:2203.15537, 2022.
manuscript. ",eess.AS,C,-0.22830856,-0.03903218,-0.05952708
http://arxiv.org/pdf/2205.06157v1,Training Strategies for Own Voice Reconstruction in Hearing Protection Devices using an In-ear Microphone,"For pre-training, the device transfer
real training data in terms of PESQ, and partly in terms of LSD          characteristics seem to be best approximated using a multi-talker,
                                                                         multi-RTF simulation strategy. In future work, the influence of
                                                                         individual and device-specific own voice transmission factors and
                                                                         external noise will be further investigated. 6. ",eess.AS,C,-0.084793724,0.108140066,-0.07680004
http://arxiv.org/pdf/2205.06157v2,Training Strategies for Own Voice Reconstruction in Hearing Protection Devices using an In-ear Microphone,"For pre-training, the device transfer
performance decrease can be observed with respect to using                characteristics seem to be best approximated using a multi-talker,
                                                                          multi-RTF simulation strategy. In future work, the influence of
                                                                          individual and device-specific own voice transmission factors and
                                                                          external noise will be further investigated. 6. ",eess.AS,C,-0.030481443,0.12809159,-0.023641234
http://arxiv.org/pdf/2205.06445v1,Personalized Adversarial Data Augmentation for Dysarthric and Elderly Speech Recognition,"VI draws the conclusion and
(E2E) Conformer [7] ASR system training. discusses possible future works. Experiments are conducted on four tasks across two              II. ",eess.AS,B,0.29789737,-0.04514232,-0.05993767
http://arxiv.org/pdf/2205.06445v2,Personalized Adversarial Data Augmentation for Dysarthric and Elderly Speech Recognition,"VI draws the conclusion and
(E2E) Conformer [7] ASR system training. discusses possible future works. Experiments are conducted on four tasks across two              II. ",eess.AS,B,0.29789737,-0.04514232,-0.05993767
http://arxiv.org/pdf/2205.06445v3,Personalized Adversarial Data Augmentation for Dysarthric and Elderly Speech Recognition,"VI draws the conclusion and
(E2E) Conformer [7] ASR system training. discusses possible future works. Experiments are conducted on four tasks across two              II. ",eess.AS,B,0.29789737,-0.04514232,-0.05993767
http://arxiv.org/pdf/2205.07180v1,Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT,"stick to a simple and standard pipeline for speaker veriﬁcation. How to incorporate extra techniques in AV-HuBERT framework
       Model             Input           VC2 EER (%)                 is left as our future work. AV-HuBERT audio + face video      2.8

       AV-HuBERT audio + lip video       3.7

3.5. ",eess.AS,C,-0.13127181,0.273403,-0.0047390778
http://arxiv.org/pdf/2205.07180v2,Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT,"degrades by comparing it with a variant of AV-HuBERT pre-           How to incorporate extra techniques in AV-HuBERT framework
trained and ﬁne-tuned on face videos. As a baseline, we also        is left as our future work. evaluate a widely used face recognition model RetinaFace [41]
taking a still image as input, which only achieves an EER of
14.6% on VC2. ",eess.AS,B,0.15687752,-0.07380326,-0.18204841
http://arxiv.org/pdf/2205.07211v1,GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis,"Our further extension study to
                                                                adaptation further showed that GenerSpeech performed ro-
                                                                bustly in the few-shot data setting. For future work, we will
GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis

further verify the effectiveness of GenerSpeech on more           Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow
general scenarios such as multilingual generalization. with invertible 1x1 convolutions. ",eess.AS,A,-0.20875917,-0.17104797,-0.10014849
http://arxiv.org/pdf/2205.07211v2,GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech,"Our further extension study to
adaptation further showed that GenerSpeech performed robustly in the few-shot data setting. For
future work, we will further verify the effectiveness of GenerSpeech on more general scenarios
such as multilingual generalization. We envisage that our work could serve as a basis for future
text-to-speech synthesis studies. ",eess.AS,A,-0.21543956,-0.29889363,0.00550819
http://arxiv.org/pdf/2205.07390v1,Learning Representations for New Sound Classes With Continual Self-Supervised Learning,"The proposed

                                                                  CSSL framework does not rely on labels, and the obtained

                                                                  representations are less prone to catastrophic forgetting, gen-

                                                                  eralize better to unseen classes, and are more adaptive to dif-

                                                                  ferent downstream tasks compared to supervised approaches. In future work, we plan to extend our framework to vari-

                                                                  ous audio processing tasks such as multi-label classiﬁcation

                                                                  and explore additional continual learning contexts including

                                                                  domain-incremental learning and online continual learning. 5

                             REFERENCES                                               in Proceedings of the Conference of the North American Chapter of the
                                                                                      Association for Computational Linguistics, 2019. ",eess.AS,A,-0.10444877,-0.17350124,-0.061665222
http://arxiv.org/pdf/2205.07390v2,Learning Representations for New Sound Classes With Continual Self-Supervised Learning,"We showed, for the ﬁrst
objective is optimized jointly with a knowledge distillation                                                    time, that continual self-supervised learning gets competitive
loss Ldist(fθ(x), fθ(t−1) (x)), where fθ(t−1) is the snapshot of                                                performance even if we do not use any mechanism against
the encoder at the completion of task t − 1 whose weights                                                       forgetting, which helps to reduce computational complexity. are frozen at the current task t. We consider the following                                                     In future work, we plan to integrate continual self-supervised
candidates for the distillation loss Ldist: i) mean squared                                                     learning on more challenging audio tasks such as multi-label
error, LMSE [48], ii) similarity-based objective that is used                                                   classiﬁcation. in SimCLR [49], Lsim, and iii) KL-Divergence loss, LKLD
(used for CSUP only) applied to the logits obtained after
the output head [3]. ",eess.AS,B,0.060111433,-0.060460035,-0.26579195
http://arxiv.org/pdf/2205.08555v1,Streaming Noise Context Aware Enhancement For Automatic Speech Recognition in Multi-Talker Environments,"This sug-              -6     46.2    53.3    69.9
gests that either the the thresholds could beneﬁt from more              0     19.3    28.8    66.1
tuning, or alternative methods of selection between the tech-            6      7.7    15.5    67.8
niques should be considered. We leave that to future work. 12      1.9     4.6    75.2
                                                                      Clean                    91.0

6.5. ",eess.AS,B,0.4018641,0.12356406,0.07597867
http://arxiv.org/pdf/2205.09644v1,Neural network for multi-exponential sound energy decay analysis,"It is obvious that
tave bands). The evaluation was carried out on a modern           optimized implementations are an interesting engineering
laptop computer (model year 2019) without a dedicated             task for future work, which could reduce the computation
graphics processing unit (GPU). Whereas the standard              times of all approaches even further. ",eess.AS,B,0.46685866,0.11825689,0.027635857
http://arxiv.org/pdf/2205.09709v1,Bi-LSTM Scoring Based Similarity Measurement with Agglomerative Hierarchical Clustering (AHC) for Speaker Diarization,"However, this is an extremely ideal case, and
tion and ﬁnally, we draw conclusions along with scope        does not occur in real world. So, to approach the problem
for future work in this domain in the last section. of speaker diarization, we can split it into two stages. ",eess.AS,A,-0.21252522,0.071116775,0.34920928
http://arxiv.org/pdf/2205.09784v1,End-to-End Zero-Shot Voice Style Transfer with Location-Variable Convolutions,"The latent space from which speaker embeddings are sampled could also be learned
jointly along with the speech synthesis model. This remains a potential direction of future work. 25 ",eess.AS,A,-0.2771863,-0.03385224,0.036215737
http://arxiv.org/pdf/2205.09812v1,Voice Activity Projection: Self-supervised Learning of Turn-taking Events,"We note a higher aggregate loss at the start of the sam-       ing of more complex conversational dynamics, can beneﬁt from
ples (since less context is available) and choose a threshold of     our proposed VAP objective. For future work, we plan to in-
3s, where the loss decreases to values approximately consistent      vestigate additional projection window representations, using
over the remaining frames, as a point of minimum context for         varying forms on information (e.g., prosodic), to model the dy-
the evaluation events. We iterate over the validation set once,      namics of conversation, in addition to the voice activity projec-
for each trained model, to ﬁnd threshold values for the S/L, S-      tion task explored in this paper. ",eess.AS,A,-0.17095226,-0.13450281,0.07503477
http://arxiv.org/pdf/2205.12872v2,Synthesis of Soundfields through Irregular Loudspeaker Arrays Based on Convolutional Neural Networks,"Notation and preliminaries
would add an increased complexity the computational point
of view without enhancing the conceptual reasoning behind             Let us consider an arrangement of L loudspeakers, or
the proposed method. For this reason, in this manuscript we
decided to focus on 2D deployments and to leave the 3D             secondary sources, as often denoted in the soundﬁeld synthesis
extension to future works. literature, deployed at positions rl, l = 1, . ",eess.AS,C,-0.066996954,0.49712604,0.12733015
http://arxiv.org/pdf/2205.13086v2,Audio Data Augmentation for Acoustic-to-articulatory Speech Inversion using Bidirectional Gated RNNs,"seen speakers. As future work, we are hoping to conduct experi-              Available: http://dx.doi.org/10.21437/Interspeech.2020-1140
ments with multiple articulatory corpora to explore how well data
augmentation with multiple datasets can help in better generalizabil-  [13] A. S. Shahrebabaki, S. M. Siniscalchi, and T. Svendsen, “Raw
ity of SI systems. Speech-to-Articulatory Inversion by Temporal Filtering and
                                                                             Decimation,” in Proc. ",eess.AS,C,-0.1396783,-0.04305006,-0.07132306
http://arxiv.org/pdf/2205.13657v1,An enhanced Conv-TasNet model for speech separation using a speaker distance-based loss function,"relationship between the length of the segment to be processed and the percentage of synchronization error. Alternative
solutions to this problem should be addressed in future works. Acknowledgment

J. Arias-Londoño started this work at the Antioquia University and ﬁnished it supported by a María Zambrano grant
from the Universidad Politécnica de Madrid, Spain. ",eess.AS,B,0.40761662,0.24817713,0.13503487
http://arxiv.org/pdf/2205.13657v2,An enhanced Conv-TasNet model for speech separation using a speaker distance-based loss function,"relationship between the length of the segment to be processed and the percentage of synchronization error. Alternative
solutions to this problem should be addressed in future works. Acknowledgment

J. Arias-Londoño started this work at the Antioquia University and ﬁnished it supported by a María Zambrano grant
from the Universidad Politécnica de Madrid, Spain. ",eess.AS,B,0.40761662,0.24817713,0.13503487
http://arxiv.org/pdf/2205.13657v3,An enhanced Conv-TasNet model for speech separation using a speaker distance-based loss function,"relationship between the length of the segment to be processed and the percentage of synchronization error. Alternative
solutions to this problem should be addressed in future works. Acknowledgment

J. Arias-Londoño started this work at the Antioquia University and ﬁnished it supported by a María Zambrano grant
from the Universidad Politécnica de Madrid, Spain. ",eess.AS,B,0.40761662,0.24817713,0.13503487
http://arxiv.org/pdf/2205.13851v1,Speaker-conditioning Single-channel Target Speaker Extraction using Conformer-based Architectures,"Experimental results for different mixtures
                                                                   show that the proposed TCN-Conformer outperforms the pro-
                                                                   posed Conformer-FFN system and a TCN-based baseline sys-
                                                                   tem. In future work, we will investigate the effect of noisy
                                                                   auxiliary information on the performance of target speaker
                                                                   extraction systems. 7. ",eess.AS,C,-0.08216725,0.33618706,-0.0299203
http://arxiv.org/pdf/2205.14700v1,"To catch a chorus, verse, intro, or anything else: Analyzing a song with structural functions","We have proposed a Transformer-based MSA system that can
                                                                         predict meaningful segment labels, even without context. However,
                                                                         in this work we only study the performance of full-track prediction;
                                                                         to demonstrate its robustness to song fragments remains future work. 6. ",eess.AS,C,-0.12478259,0.12757018,-0.2575239
http://arxiv.org/pdf/2205.14807v1,BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis,"On the contrary, there still exists
a large margin between the synthesized result from the ﬁrst stage and the label (ID 3 vs. ID 4). In
conclusion, the results in Table 4 show that the ﬁrst stage is the bottleneck of the framework, which
points out potential future work directions such as the end-to-end optimization of the two stages. 9
5 Conclusion

In this paper, we propose BinauralGrad, a two-stage framework for binaural audio synthesis condi-
tioned on mono audio. ",eess.AS,C,-0.04018164,0.42783257,0.0038089994
http://arxiv.org/pdf/2205.14807v2,BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis,"On the contrary, there still exists
a large margin between the synthesized result from the ﬁrst stage and the label (ID 3 vs. ID 4). In
conclusion, the results in Table 4 show that the ﬁrst stage is the bottleneck of the framework, which
points out potential future work directions such as the end-to-end optimization of the two stages. 5 Conclusion

In this paper, we propose BinauralGrad, a two-stage framework for binaural audio synthesis condi-
tioned on mono audio. ",eess.AS,C,-0.04996098,0.42522943,0.003179933
http://arxiv.org/pdf/2205.15439v1,StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis,"Our model is faster than the state-of-the-art
model, VITS [1], even though our model was not trained end-to-end like VITS (Table 5). We believe
it is possible to make the inference time shorter if we train in an end-to-end manner in future work. Table 5: Real time factor (RTF) in second. ",eess.AS,B,0.30726606,-0.20557258,-0.13532206
http://arxiv.org/pdf/2205.15700v1,Conversational Speech Separation: an Evaluation Study for Streaming Applications,"Results are shown
and discussed in Section 4. Finally, in Section 5 we draw conclusions and outline possible future work. 1.1 Related Work

In this subsection we review speech separation algorithms based on deep learning, which are the most related
to our work. ",eess.AS,C,-0.29719073,0.105916485,-0.04714259
http://arxiv.org/pdf/2206.00970v1,Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment,"631–648. For future work, we consider that a more sophisticated
detection method which works on equirectangular images is           [8] J. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello, “Look, listen, and learn
worth investigating for sampling of the crops as done in [43]. more: Design choices for deep audio embeddings,” in ICASSP 2019 -
This is because YOLO is trained on normal images, and                    2019 IEEE International Conference on Acoustics, Speech and Signal
might not give the optimal detection when directly applied on            Processing (ICASSP), 2019, pp. ",eess.AS,C,-0.23898044,0.15264994,-0.18235156
http://arxiv.org/pdf/2206.02512v3,UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder,"While the latter one
can be improved with more data involved in the self-supervised                    [16] J. Cong, S. Yang, L. Xie, and D. Su, “Glow-wavegan: Learning speech
or semi-supervised speaker representation training [25]. Thus,                          representations from gan-based variational auto-encoder for high ﬁdelity
our study serves as the preliminary research work, proving                              ﬂow-based speech synthesis,” in 2021 ISCA Interspeech, 2021.
the concept of unsupervised TTS AM, while leaving observed
inadequacies as the future work. [17] A. H. Liu, C.-I. ",eess.AS,A,-0.22725968,-0.07281575,-0.14228687
http://arxiv.org/pdf/2206.02797v1,FedNST: Federated Noisy Student Training for Automatic Speech Recognition,"Each plot in Figure 2 presents       tationally expensive, increasing the overall FL experiment time
two sets of graphs showing results from (i) test-clean and (ii)       by 10x. We leave optimizing on this front as future work. test-other, respectively. ",eess.AS,B,0.5094553,0.1177845,0.06253975
http://arxiv.org/pdf/2206.02797v2,FedNST: Federated Noisy Student Training for Automatic Speech Recognition,"Each plot in Figure 2 presents       tationally expensive, increasing the overall FL experiment time
two sets of graphs showing results from test-clean and test-          by 10x. We leave optimizing on this front as future work. other, respectively. ",eess.AS,B,0.5153326,0.09869007,0.03730132
http://arxiv.org/pdf/2206.03206v1,FlexLip: A Controllable Text-to-Lip System,"Importantly, these results have shown that the speech-to-lip module is robust
                                       to using synthetic data at input, as the performance of the full pipeline is close to that of the
                                       speech-to-lip with natural audio. As future work, we plan to extend our pipeline to the video
                                       domain (with a keypoints-to-video component) and use the proposed objective evaluation
                                       approach to evaluate objectively the output of the end-to-end system. Author Contributions: Methodology, D.O., B.L., A.S. and H.C.; software, D.O., B.L., A.S. and H.C.;
                                            writing—review and editing, D.O., B.L., A.S. and H.C. All authors have read and agreed to the published
                                            version of the manuscript. ",eess.AS,C,-0.2897563,0.14924099,-0.06683113
http://arxiv.org/pdf/2206.04305v1,Context-based out-of-vocabulary word recovery for ASR systems in Indian languages,"We have only explored phone substitution manipulation keeping
the insertion and deletion costs ﬁxed. The insertion and deletion costs could be
adjusted for future work. For example, consonant deletions can be penalized less
compared to vowel deletions. ",eess.AS,A,0.00025300053,-0.06638895,0.2348446
http://arxiv.org/pdf/2206.04850v1,Feature-informed Embedding Space Regularization For Audio Classification,"ilar to Con-Reg, the embedding space is regularized with
Both representations are extracted by models trained on large-
scale audio datasets. VGGish features are extracted by the            1More sophisticated interpolation methods for upsampling to match the time
VGGish model [7] pre-trained on AudioSet [27] to perform           resolutions will be explored in future work. 2Pilot experiments using L2 as the distance function did not lead to
                                                                   competitive results. ",eess.AS,C,-0.24257031,0.07063204,-0.24526866
http://arxiv.org/pdf/2206.06108v1,DCASE 2022 Challenge Task 6B: Language-Based Audio Retrieval,"Through the sub-task B of the        caption is provided for each audio sample. The audio samples are
                                          DCASE 2022 Challenge Task 6, we aim to inspire further research            collected following the procedure described in [8], by randomly se-
                                          into language-based audio retrieval with unconstrained textual de-         lecting those samples that were not used in the Clotho v2. Table 1
                                          scriptions. ",eess.AS,A,-0.28062743,-0.03536001,0.051705383
http://arxiv.org/pdf/2206.06108v2,DCASE 2022 Challenge Task 6B: Language-Based Audio Retrieval,"Through the sub-task B of the        caption is provided for each audio sample. The audio samples are
                                          DCASE 2022 Challenge Task 6, we aim to inspire further research            collected following the procedure described in [8], by optimizing
                                          into language-based audio retrieval with unconstrained textual de-         the tag distribution of the selected samples. The samples were se-
                                          scriptions. ",eess.AS,A,-0.3094878,-0.018919079,0.018989302
http://arxiv.org/pdf/2206.06108v3,Language-based Audio Retrieval Task in DCASE 2022 Challenge,"The data collecting procedure is explained in detail in [8]. audio retrieval is introduced into as Subtask 6B, which aims to in-
                                          spire further research into audio retrieval with unconstrained textual  2.3. Clotho Retrieval Evaluation Dataset
                                          descriptions. ",eess.AS,C,-0.20890662,0.06355295,-0.019395523
http://arxiv.org/pdf/2206.06192v1,Toward Zero Oracle Word Error Rate on the Switchboard Benchmark,"tion of Table 2, and left columns of other tables have italicized
font. This convention is used to clarify which results might be          These results motivate future work to improve lattice gen-
considered unrealistic, due to use of a reference segmentation or   eration [18, 19], particularly in E2E ASR systems. Our current
also because of the oracle nature of selecting a best alternative. ",eess.AS,B,0.34865743,-0.16105005,0.18934417
http://arxiv.org/pdf/2206.06192v2,Toward Zero Oracle Word Error Rate on the Switchboard Benchmark,"also because of the oracle nature of selecting a best alternative. These results motivate future work to improve lattice gen-
     Table 1 presents the WER results from scoring each of the      eration [18, 19], particularly in E2E ASR systems. Our current
ASR systems with successively improved conﬁgurations of the         research also explores open-vocabulary decoding in a WFST
scoring tools, as described in Sections 2.1 through 2.4.            framework, in which novel words may be included in a lattice
                                                                    and derived phrase alternatives. ",eess.AS,B,0.17801963,-0.28962994,0.045350496
http://arxiv.org/pdf/2206.06259v1,Realistic Gramophone Noise Synthesis using a Diffusion Model,"However, we notice
that this conditioning approach is still not easy to control and often  [10] P. J. W. Rayner and S. J. Godsill, “The detection and cor-
cannot yield highly realistic results, unless the faithfulness to the         rection of artefacts in degraded gramophone recordings,” in
guide is sacrificed. We leave for future work to study different              Proc. IEEE WASPAA, Oct. 1991, pp. ",eess.AS,C,-0.13927828,0.21737106,0.07733565
http://arxiv.org/pdf/2206.06259v2,Realistic Gramophone Noise Synthesis using a Diffusion Model,"0_151–0_152. methods for future work. [11] F. Rund, V. Vencovsky`, and M. Semansky`, “An evaluation
     A known limitation of diffusion models is the high compu-               of click detection algorithms against the results of listening
tational cost that they present during the inference stage. ",eess.AS,B,0.13316572,0.06843165,0.07152517
http://arxiv.org/pdf/2206.06811v1,Adversarial Audio Synthesis with Complex-valued Polynomial Networks,"Audio generation is a signiﬁcant task
with tremendous applications, e.g., human-computer interface. As such, audio generation methods
could be used for creating misleading content and we believe that further research is required to
ensure the capabilities are used for creating a positive societal impact. The Generative Adversarial Nets (GANs) [17] that we use for our experiments on audio generation
have a dedicated module, i.e., the discriminator, for detecting the real from the fake content. ",eess.AS,C,-0.22096792,0.2344951,-0.11258382
http://arxiv.org/pdf/2206.06811v2,Adversarial Audio Synthesis with Complex-valued Polynomial Networks,"Audio generation is a signiﬁcant task
with tremendous applications, e.g., human-computer interface. As such, audio generation methods
could be used for creating misleading content and we believe that further research is required to
ensure the capabilities are used for creating a positive societal impact. The Generative Adversarial Nets (GANs) [17] that we use for our experiments on audio generation
have a dedicated module, i.e., the discriminator, for detecting the real from the fake content. ",eess.AS,C,-0.22096792,0.2344951,-0.11258382
http://arxiv.org/pdf/2206.07261v1,Latency Control for Keyword Spotting,"Our results show that we make a signiﬁ-
                                                                    cant 25% relative false accepts improvement over the baseline
                                                                    model at ﬁxed latency. As a future work we plan to experiment
                                                                    with this hyperparameter in other commonly used loss functions
                                                                    like the cross entropy loss, and extends its applicability to other
                                                                    use cases such as utterance detection in signal-to-interpretation
                                                                    applications. 6. ",eess.AS,C,-0.030641045,-0.000702641,-0.048678562
http://arxiv.org/pdf/2206.07327v1,Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition,"75
4.3. T-SNE visualization
A further analysis over the phonetic discrimination brought                  100                          40
by 144-dimension original, or cross-domain and cross-lingual
generated UTI articulatory features via t-distributed stochastic                  75 50 25 0 25 50 75             40      20     0  20    40
neighbour embedding (t-SNE) [52] visualization is shown in the
Figure 2 across four datasets. As expected, the UTI articulatory                  (a) the TaL corpus      (b) the DementiaBank Pitt corpus
features produced clear phonetic discrimination between exam-
ple phonemes /ı/ and /O/ on the TaL test set. ",eess.AS,A,-0.051465113,-0.15439135,0.17671913
http://arxiv.org/pdf/2206.07327v2,Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition,"T-SNE visualization                                                                                                                           5. CONCLUSION

A further analysis over the phonetic discrimination brought by 144-                                                    This paper presents a cross-domain and cross-lingual A2A inver-
dimension original, or cross-domain and cross-lingual generated                                                        sion approach that utilizes the parallel audio and ultrasound tongue
UTI articulatory features via t-distributed stochastic neighbour em-                                                   imaging (UTI) data of the 24-hour TaL corpus in A2A model pre-
bedding (t-SNE) [47] visualization is shown in the Figure 2 across                                                     training before being cross-domain and cross-lingual adapted to
four datasets. As expected, the UTI articulatory features produced                                                     three datasets across two languages: the English DementiaBank
clear phonetic discrimination between example phonemes /ı/ and /O/                                                     Pitt and Cantonese JCCOCC MoCA elderly speech corpora; and
on the TaL test set. ",eess.AS,A,-0.16368034,-0.20307645,0.050767597
http://arxiv.org/pdf/2206.07448v1,The ZevoMOS entry to VoiceMOS Challenge 2022,"VoiceMOS Challenge evaluation                                              of synthetic speech generation architectures. As future work,
                                                                                we plan to investigate such signal-based measures which might
Figure 2 shows the results of all the submissions made to the                   be indicative of quality degradations of speech without a ref-
challenge. The ZevoMOS system is under the T01 team id. ",eess.AS,A,-0.26086807,0.084349126,0.08283082
http://arxiv.org/pdf/2206.07931v1,DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children's ASR,"[17] X. Chang, T. Maekaku et al., “An exploration of self-supervised
      pretrained representations for end-to-end speech recognition,”      [40] S. Kessler, B. Thomas, and S. Karout, “Continual-wav2vec2: an
      ASRU, 2021.                                                               application of continual learning for self-supervised automatic
                                                                                speech recognition,” IEEE ICASSP, 2022. [18] D. Jiang, W. Li et al., “A further study of unsupervised pretrain-
      ing for transformer based speech recognition,” in IEEE ICASSP,      [41] V. Panayotov, G. Chen et al., “Librispeech: an asr corpus based
      2021, pp. 6538–6542. ",eess.AS,A,-0.2731046,-0.20720813,-0.14911532
http://arxiv.org/pdf/2206.08174v1,Strategies to Improve Robustness of Target Speech Extraction to Enrollment Variations,"Ta-         istics of the interfering speaker are more similar to those of the
ble 2 shows SDRi, CER and failure ratio of each system with          target speaker e.g. recordings from the speakers belonging to
worst, 2nd-worst and best-enrollment choices as well as aver-        the same family will be a part of our future work. aged values over enrollments. ",eess.AS,B,0.0898823,0.13894863,0.30479068
http://arxiv.org/pdf/2206.09072v1,Semi-supervised Time Domain Target Speaker Extraction with Attention,"The performance gain can be attributed to the       pipeline incorporating mixtures without reference signals. In
use of the dual-path transformer blocks in the masking network        future work, we would explore target speaker extraction under
which effectively model local and global dependency. Our ob-          noisy and reverberant conditions and investigate approaches to
servations are consistent with the ﬁndings of BSS systems in          include massive unlabeled data such as curriculum learning. ",eess.AS,C,-0.28432417,0.23731375,-0.153498
http://arxiv.org/pdf/2206.09396v1,Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping,"T refers to  pre-train on cross-lingual adult data to make the context repre-
the temporal dimension of the input waveform and C denotes         sentations and subsequent ASR system robust against children’s
the output dimension matching the vocabulary size. speech in a variety of languages without the need to pre-train
                                                                   and optimize separate models in future work. 3.2. ",eess.AS,A,-0.09048332,-0.25505522,0.14795417
http://arxiv.org/pdf/2206.09523v1,Towards Trustworthy Edge Intelligence: Insights from Voice-Activated Services,"Bias Propagation in Voice-Activated Service Composition                                   ers, rather than for all speakers, improves the performance for
                                                                                             all groups [43]. A promising direction for future work is to
   We have discussed bias in two individual components of                                    investigate if the same holds true when tuning thresholds for
voice-activated services: keyword spotting and speaker veriﬁ-                                individual users. Further developing algorithmic approaches,
cation. ",eess.AS,A,-0.21790051,0.06692895,0.16815805
http://arxiv.org/pdf/2206.11000v1,A Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement,"Speciﬁcally, the learned weighted average conditioning
observation not only support previous indication stating that rel-  setting shows signiﬁcant improvement, compared to other pho-
evant information is being scattered across different layers, it    netic settings that are far more common in the speech enhance-
also highlights the importance of the information held in lower     ment domain. This study presents several observations worth
layers which has lower PNMI values, and may imply that shared       exploring which we intend to address in future work. information is taken into account in the weighted average. ",eess.AS,A,-0.027578732,-0.08906816,0.13002196
http://arxiv.org/pdf/2206.11045v1,COVYT: Introducing the Coronavirus YouTube and TikTok speech dataset featuring the same speakers with and without infection,"Moreover, we obtained a UAR near 70 % for the automatic classiﬁcation of speech
samples according to COVID-19 status by using pre-trained speech models. The COVYT dataset together with the provided benchmarks shall boost further research in the ﬁeld of speech-based
COVID-19 detection while ensuring reproducibility and comparability of results. Furthermore, as the dataset contains
samples of the same speakers with and without COVID-19 infection, we expect it to prove a valuable conduit for future
efforts in personalisation approaches that can adapt to the characteristics of individual speakers and, thus, improve
performance and reliability. ",eess.AS,A,-0.38260657,-0.078666866,0.0011872908
http://arxiv.org/pdf/2206.11157v1,Conformer with dual-mode chunked attention for joint online and offline ASR,"the original proposal [16]. In future work, we will apply our
                                                                    ﬁndings to multi-mode ASR [17] for improving robustness of
3.4. Effect of Dual-mode Normalization and Joint Training           the online model in multiple latency requirements. ",eess.AS,B,0.23334388,0.07619428,-0.26587743
http://arxiv.org/pdf/2206.11558v1,Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis,"Experi-
truth, the proposed model is better for samples synthesized us-     mental results conﬁrm that the proposed model performs better
ing the PWG than that of the WORLD. In future work, improv-         than single-task SVS models. Further, the samples synthesized
ing the performance of the 48k sampling rate-based-vocoder for      by the auxiliary features also exhibited better performance than
singing voices is left. ",eess.AS,C,-0.12191221,0.021627475,-0.08270783
http://arxiv.org/pdf/2206.11640v1,Speaker-Independent Microphone Identification in Noisy Conditions,"Future developments will involve developing new tech-                        [12] L. Cuccovillo, S. Mann, M. Tagliasacchi, and P. Aichroth, “Audio tam-
niques for speech reduction addressing sex, age, and language                         pering detection via microphone classiﬁcation,” in IEEE International
generalization. Two further aspects that need further analysis                        Workshop on Multimedia Signal Processing (MMSP), 2013.
are robustness against audio compression and the effect of
different noise types. Furthermore, there is a need for larger                  [13] L. Cuccovillo and P. Aichroth, “Open-set microphone classiﬁcation via
datasets including a broad range of devices and recording                             blind channel analysis,” in IEEE International Conference on Acoustics,
                                                                                      Speech and Signal Processing (ICASSP), 2016. ",eess.AS,C,-0.3467999,0.14960644,-0.0136884395
http://arxiv.org/pdf/2206.11640v2,Speaker-Independent Microphone Identification in Noisy Conditions,"Future developments will involve developing new tech-                        [12] L. Cuccovillo, S. Mann, M. Tagliasacchi, and P. Aichroth, “Audio tam-
niques for speech reduction addressing sex, age, and language                         pering detection via microphone classiﬁcation,” in IEEE International
generalization. Two further aspects that need further analysis                        Workshop on Multimedia Signal Processing (MMSP), 2013.
are robustness against audio compression and the effect of
different noise types. Furthermore, there is a need for larger                  [13] L. Cuccovillo and P. Aichroth, “Open-set microphone classiﬁcation via
datasets including a broad range of devices and recording                             blind channel analysis,” in IEEE International Conference on Acoustics,
                                                                                      Speech and Signal Processing (ICASSP), 2016. ",eess.AS,C,-0.3467999,0.14960644,-0.0136884395
http://arxiv.org/pdf/2206.11750v1,Towards End-to-End Private Automatic Speaker Recognition,"ciency. For 3 and 4-party RSS, we also used probabilistic trun-
cation as proposed by [34, 28], instead of regular truncation,             As future work, we consider that it would be important to
to further improve efﬁciency. Our experiments assume the de-          explore ways to reduce the size of the x-vector extraction net-
fault security parameters for each protocol, namely 40-bit secu-      work, to improve efﬁciency. ",eess.AS,B,0.28204194,0.013568225,0.11242837
http://arxiv.org/pdf/2206.12132v1,SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech,"ilarity during both the cross-lingual and the intralingual synthe-
                                                                           sis. Based on our study, we expect to expand SANE-TTS into
Table 4: Comparison of naturalness MOS in the ablation study               other languages in future work. Model        Intralingual       KO      Cross-lingual       ZH                    6. ",eess.AS,A,0.094741054,-0.28333354,0.32343197
http://arxiv.org/pdf/2206.12285v1,Speech Quality Assessment through MOS using Non-Matching References,"Fourth, NORESQA-MOS approach scores higher than baseline
approaches like DNSMOS and NISQA in terms of lower er-                       5. Conclusions and future work
rors (MSE) and higher correlations, especially on challenging
datasets like BWE which have subtle differences. The standard         In this paper, we presented NORESQA-MOS - a novel approach
deviations for all datasets across Tables 1, 2, and 3 are consis-     for MOS estimation of speech signals which uses non-matching
tently small (∼0.02 rating) suggesting invariance to a particular     references. ",eess.AS,A,-0.12738381,0.03086957,0.079517335
http://arxiv.org/pdf/2206.12297v1,SAQAM: Spatial Audio Quality Assessment Metric,"Our results show that training to predict both together results in
higher performance than training the two models individually;                                  5. Conclusions and future work

Type                Name           P3 (SQ)                     P3 (LQ)                 We present SAQAM, a general purpose, differentiable, non-
                                  Pink Noise                  Pink Noise               matching, non-clean-reference based objective metric to assess
                          Speech              Guitar  Speech               Guitar      listening quality and spatial localization differences between
                                                                                       any two binaural signals. We show the utility of MTL and
Individual M. SQ 0.75 0.61 0.21 - - -                                                  deep-feature distances that guide the model to correlate well
                    LQ       -       -           -    0.40    0.36         0.26        with human subjective ratings without any perceptual training or
                                                                                       calibration. ",eess.AS,C,-0.22495724,0.2191372,-0.034509603
http://arxiv.org/pdf/2206.12857v1,Transport-Oriented Feature Aggregation for Speaker Embedding Learning,"For  optimal transport plans. We plan to explore the use of optimal
the multi-head self-attention baseline, each head is responsible    transport distance as a regularization loss in future work. This is
for one frequency group. ",eess.AS,B,0.29187712,0.02079852,0.07684933
http://arxiv.org/pdf/2206.13016v1,Unsupervised Instance Discriminative Learning for Depression Detection from Speech Signals,"Compared
IDL       DS         0.6458  0.7412                              with the baseline, DS and RS, the proposed approach achieves
                                                                 signiﬁcant improvements. In future work, we will investigate
          PIS 0.6834         0.7435                              context-based, in addition to speaker-based, sampling strategies
                                                                 and apply IDL and PIS to other low-resource tasks. In the IDL-PIS experiments, TM is also chosen for aug-
mentation. ",eess.AS,A,-0.043133285,0.03164207,0.011411614
http://arxiv.org/pdf/2206.13044v1,Extended U-Net for Speaker Verification in Noisy Environments,"Finally, the proposed
tems 1 and 3 exhibited notable performance degradation compared to       models demonstrated outstanding generalization performance in the
the ExU-Net (System 5), whereas System 2 had comparable results          evaluation of the VOiCES dataset. As a future work, we will evaluate
even though FE learning was excluded. These results indicate that        the proposed models in various real-world noise environments. ",eess.AS,C,-0.06937394,0.080490276,-0.13093495
http://arxiv.org/pdf/2206.13066v1,Detection of Doctored Speech: Towards an End-to-End Parametric Learn-able Filter Approach,"We do recognize that optimal fusion weights could be determined by tuning on the
development set. This will be presented in future work. Furthermore, we plan to
extend the use of attention ﬁltering networks Lai et al., 2019 to help enhance the
features in certain frequency regions for better results. ",eess.AS,B,0.17467982,0.08606438,-0.18901636
http://arxiv.org/pdf/2206.13231v1,QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer,"(4) We used the publicly avail-
                                          using Deep Neural Networks (DNNs) [1, 2], Time Delay Neural       able Hey-Snips dataset to conduct experiments on both non far-
                                          Networks [3], Convolutional Neural Networks (CNNs) [4], Re-       ﬁeld and far-ﬁeld environments, which can serve as a founda-
                                          current Neural Networks (RNNs) [5, 6] or transformers [7, 8]. tion for future work on open-vocabulary keyword spotting. However, a large amount of target keyword data is required to
                                          effectively train those models. ",eess.AS,A,-0.17937443,-0.20969065,-0.12597199
http://arxiv.org/pdf/2206.13232v1,Conformer Based Elderly Speech Recognition System for Alzheimer's Disease Detection,"Section 4 shows the AD detection sys-       varying the number of encoder and decoder Transformer layers
tem performance using ASR outputs. Finally, the conclusions         and the resulting impact on performance, the optimized num-
are drawn and future works are discussed in Section 5.              ber of decoder Transformer blocks was increased from 6 to 12. Similarly, the convolution kernel size was manually tuned and
               2. ",eess.AS,B,0.1957536,0.05777172,-0.22612727
http://arxiv.org/pdf/2206.13404v1,Avocodo: Generative Adversarial Network for Artifact-free Vocoder,"Avocodo was successful in generating the artifact-
free high frequency components and clear pitch contour. Additionally, we observed the performance
difference in reconstructing higher-pitch voice and provide further analysis in Appendix C.

Unseen speaker synthesis To verify generalization to unseen data, we conducted MOS tests in
unseen speaker synthesis. In Table 1, the MOS results show that AvocodoV 1 outperformed HiFi-GAN
and VocGAN. ",eess.AS,C,-0.23207776,0.2941785,0.009457071
http://arxiv.org/pdf/2206.13404v2,Avocodo: Generative Adversarial Network for Artifact-free Vocoder,"Avocodo was successful in generating the artifact-
free high frequency components and clear pitch contour. Additionally, we observed the performance
difference in reconstructing higher-pitch voice and provide further analysis in Appendix C.

Unseen speaker synthesis To verify generalization to unseen data, we conducted MOS tests in
unseen speaker synthesis. In Table 1, the MOS results show that AvocodoV 1 outperformed HiFi-GAN
and VocGAN. ",eess.AS,C,-0.23207776,0.2941785,0.009457071
http://arxiv.org/pdf/2206.13411v1,Audio Similarity is Unreliable as a Proxy for Audio Quality,"8192                          8192                          8192                                4. Conclusions and future work

Hz1024                        Hz1024                        Hz1024                        In this paper we discussed several scenarios where the conven-
                                                                                          tional formulation of similarity metrics (e.g. PESQ) contrasts
    128                           128                           128                       from predicting absolute subjective quality, and showed that var-
      00 0.4 0.8 1.2 1.6            00 0.4 0.8 1.2 1.6            00 0.4 0.8 1.2 1.6      ious no-reference metrics are a good alternative in those settings. ",eess.AS,B,0.27204272,0.00029153808,0.17990535
http://arxiv.org/pdf/2206.13420v1,Unsupervised Voice Activity Detection by Modeling Source and System Information using Zero Frequency Filtering,"It can also thus be ex-
is not as well reﬂected in the F1-scores compared to the other             tended to other types of audio signals, such as animal and birds
methods because of the broader boundaries of the ground truth              vocalizations. Our future work will focus in this direction, along
segments, even though our method is able to provide a much                 with modeling the composite signal using the raw waveform
more granular segmentation. The methods yielding higher per-               neural network based modeling approach [33] for supervised
formance comply closely to the broader VAD boundaries and                  voice activity detection [17]. ",eess.AS,C,-0.26728562,0.09171981,-0.09907102
http://arxiv.org/pdf/2206.13865v1,RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion,"We eliminate
     We conduct a similarity test to further conﬁrm the superi-     spectrum over-smoothing with an adversarial training stage. Currently, we have not experimented with our method in highly
                                                                    noisy or reverberant cases, and we make it as our future work. 5. ",eess.AS,C,-0.012659833,0.15672912,-0.235109
http://arxiv.org/pdf/2206.14618v1,On the Prediction Network Architecture in RNN-T for ASR,"When          not able to exploit long-term dependencies and yet limiting the
varying left context length in a matched way at both training         left context to less than 2 tokens severely degrades performance. and inference, a similar picture is observed, where N-Concat          As future work, we would extend our study to take external
improves over Transformer PN aided by the regularization effect       language models into account, and exploit the limited n-gram
of having less parameters. Considering that the average utterance     dependency to generate lattices as outputs for downstream tasks. ",eess.AS,A,-0.0555562,-0.2964036,-0.14204597
http://arxiv.org/pdf/2206.14623v1,Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems,"Contextual density ratio is robust to noise, being
a WERT of 23.9 which is very close to 23.6 of the conversation        more sensitive to similar names. As future work, we want to im-
level LMs in Table 1.                                                 prove the technique in those adversarial conditions, for instance
                                                                      by also taking into account the phonetics. Finally, the entity list per conversation, which is used to
train the CDR contextual LMs, was extended with 16 adversar-                             5. ",eess.AS,A,-0.112914786,-0.1334517,0.055666205
http://arxiv.org/pdf/2206.14639v1,DDKtor: Automatic Diadochokinetic Speech Analysis,"DDKtor-LSTM achieves the highest                    formance. In addition, this model was applied only to voiceless
correlations with the annotator (and lowest errors) across test              targets; future work can extend this approach to examine AMR
sets, followed by DDKtor-CNN and then the model of Arias-                    and SMR subtasks using voiced variants. Finally, the model’s
Vergara et al. ",eess.AS,A,-0.1943698,-0.13113734,-0.08241126
http://arxiv.org/pdf/2206.15432v1,Challenges and Opportunities in Multi-device Speech Processing,"Mass media presentation of a keyword that is picked
                         rooms                                       up by hundreds or thousands of devices simultaneously is a pain
Transition      between  ∼400 RIRs sampled along tran-               point for users and developers today. Consequently, engineer-
rooms [36]               sition path between four pairs              ing systems from different organizations to communicate rel-
                         of rooms                                    evant information to avoid system breakdown and frustration
Spatial room impulse     100 RIRs in a varachoic room                with both organizations is a crucial area of future work. responses [37]
ACE Challenge [38]       70 RIRS from 5 simultaneous                      We have primarily discussed acoustic signal processing
                         mic arrays across 7 rooms, 2                with multiple devices, but in addition to the proliferation of de-
dEchorate [39]           positions per room, plus 58 ane-            vices is also the proliferation of sensing capabilities and modal-
                         choic speech utterances. ",eess.AS,C,-0.20315212,0.19757573,0.07065697
http://arxiv.org/pdf/2207.00216v1,Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models,"ting without the complex procedures used in the standard in-
                                                                                       cremental domain adaptation methods [4, 5]. In future work,
     Finally, we show the results of the element-wise parameter                        we will apply our ﬁndings to conventional incremental domain
selection. We again utilized the Transformer-based model with                          adaptation methods. ",eess.AS,B,0.22286521,0.021131411,-0.16866389
http://arxiv.org/pdf/2207.00660v1,Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones,"The details of Spatial Filtering are outside the scope of this
dissertation. Nevertheless, a brief introduction is presented because future work proposed
in this dissertation includes a possible combination of the proposed method with spatial
filtering and beamforming for speaker separation. 2.3 Modeling of Room Acoustics
         The proposed research requires the modeling of room acoustics. ",eess.AS,C,-0.15308961,0.5448087,0.11299032
http://arxiv.org/pdf/2207.00706v1,UserLibri: A Dataset for ASR Personalization Using Only Text,"However, all baselines still show
higher per-user WER averages than on the full set of combined users. For the remainder of our experiments, we chose to focus on
personalizing the general LM which trained on the full dataset (BL2) as its baseline performance is best, and leave personalizing a
model trained only with the union of user-speciﬁc text examples to future work. C. Per-User WER Histograms

To compare to Figure 1 showing the baseline per-user WER distributions, some additional histograms showing the per-user WERs for
various experiments are shown in Figures 3, 4, and 5. ",eess.AS,B,0.11877869,-0.24044482,-0.022368308
http://arxiv.org/pdf/2207.00774v1,Computer-assisted Pronunciation Training -- Speech synthesis is almost all you need,"Section 5 presents exper-
iments demonstrating the eﬀectiveness of various synthetic speech generation methods
in improving the accuracy of the detection of pronunciation and lexical stress errors. Finally, conclusions and future work are presented in Section 6. 2. ",eess.AS,A,-0.25985748,-0.10377714,0.25867537
http://arxiv.org/pdf/2207.01039v1,Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR,"from current and previous speech is sent to the decoder mod-
                                                                    ule of ASR, which reduces the relative CER by up to 16% on
     As shown in Table 2, although the pretrained model can         data sets MagicData, DDT and HKUST. In the future work, we
improve the accuracy of recognition, our method achieves sig-       will explore the impact of different pretrained speech models
niﬁcantly better results. This shows that our model not only        and language models on the extractor, as well as more effective
makes use of the representation ability of the pretrained model,    ways to integrating the contextual representation into conversa-
but also effectively obtains the cross-modal textual representa-    tional speech recognition. ",eess.AS,A,-0.25973254,-0.2553898,-0.11550698
http://arxiv.org/pdf/2207.01063v1,DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech,"This limitation of our dataset remains in future work: expanding candidate
features in a spontaneous dataset with a more precise deﬁnition of spontaneous conversation. Further-
more, expanding the dataset would be another future work, including a multispeaker version with
more than two speakers or a multilingual version considering the cultural differences in conversation. Acknowledgement

This research was supported by the MSIT(Ministry of Science and ICT), Korea, under the Grand
Information Technology Research Center support program(IITP-2022-2020-0-01489) supervised by
the IITP(Institute for Information & communications Technology Planning & Evaluation)

References

 [1] Rohan Badlani, Adrian Łancucki, Kevin J Shih, Rafael Valle, Wei Ping, and Bryan Catanzaro. ",eess.AS,A,-0.30582505,-0.15280212,0.19829884
http://arxiv.org/pdf/2207.01063v2,DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech,"Further-
beginning turns of the dialogue. To show context piling up        more, developing the dataset would be another future work,
and the apparent difference in context modeling, we divide the    including a multispeaker version with more than two speakers
MOS result into half: before/after turn 5. We set the threshold   or a multilingual version considering the cultural differences
to 5 since the average number of turns per dialogue is 9.3 (see   in conversation. ",eess.AS,A,-0.19505364,-0.049148433,0.27669767
http://arxiv.org/pdf/2207.01454v1,GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion,"Experiment                         GlowVC- GlowVC-                        4. Conclusions and future work
            GT AutoVC cond
                                                explicit           We presented GlowVC: a language-independent voice conver-
                                                                   sion model that demonstrates great potential at maintaining lin-
intra-lingual (S → S) 11.56 22.44        12.44         12.52       guistic content, intelligibility and naturalness, even for unseen
                                                                   languages. We considered two versions of the model: GlowVC-
cross-lingual (S → S) -  40.10           14.63         14.11       conditional and GlowVC-explicit. ",eess.AS,A,-0.11774276,-0.16448453,0.07910594
http://arxiv.org/pdf/2207.01507v1,Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS),"these results are useful guidelines for building training corpora
A brief summary for this is that all female > gender-balanced    for multilingual NTTS systems, we are not yet able to fully ex-
≥ all male for AS and STL data composition. This is true for     plain some of them, which will be investigated as future work. both female and male TSes. ",eess.AS,A,0.094167635,-0.31486374,0.19164422
http://arxiv.org/pdf/2207.01732v1,DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition,"We notice the system can be sensi-
Deformer (mult=1.0)  43.34M   Xavier          11.1 9.0 3.9 3.0       tive to the learning rate multiplier due to layer tying and verify
Deformer (mult=0.5)  43.34M    Zero           10.8 8.8 3.7 3.0       in experiments that using zero initialization for the offset CNNs
Deformer (mult=1.0)  43.34M    Zero           10.5 8.4 3.7 2.9       and MVN for the input alleviate such issues and increase robust-
                                                                     ness. For future work, we want to improve the model robustness
Table 3: WERs and CERs of using different parameter initial-         more, for which the solution can lead to a larger model design
ization schemes for the Deformer. and exploration for more deformable layers. ",eess.AS,B,0.33390516,-0.0015383363,-0.3250805
http://arxiv.org/pdf/2207.01732v2,DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition,"We notice the system can be sensi-
Deformer (mult=1.0)  43.34M   Xavier          11.1 9.0 3.9 3.0       tive to the learning rate multiplier due to layer tying and verify
Deformer (mult=0.5)  43.34M    Zero           10.8 8.8 3.7 3.0       in experiments that using zero initialization for the offset CNNs
Deformer (mult=1.0)  43.34M    Zero           10.5 8.4 3.7 2.9       and MVN for the input alleviate such issues and increase robust-
                                                                     ness. For future work, we want to improve the model robustness
Table 3: WERs and CERs of using different parameter initial-         more, for which the solution can lead to a larger model design
ization schemes for the Deformer. and exploration for more deformable layers. ",eess.AS,B,0.33390516,-0.0015383363,-0.3250805
http://arxiv.org/pdf/2207.03546v1,"BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus","training high-quality speech synthesis models with Coqui TTS. The MOS judgments seem related to the goodness of align-             There are two clear and immediate avenues for future work:
ment evaluations. That is, the language with the best align-         (1) verse-level alignment of the remaining four languages (kik,
ments (ewe) was also rated the best MOS for speech synthe-           lug, luo, and nya), and (2) improvement of the quality of ex-
sized from the resulting model. ",eess.AS,A,-0.069579326,-0.14577717,0.14217937
http://arxiv.org/pdf/2207.03602v1,Rhythm and form in music: a complex systems approach,"Finally, we stress that 1) the inter-onset durations vector is not the only way to understand
rhythm, and 2) rhythm is not the way to understand form. About the first point, we suggest that one
could incorporate silence into the notion of rhythm in future work. If we understand silence as a
sound event on its own, several notions of complexity and form will have a different interpretation. ",eess.AS,B,0.14017057,0.16135941,0.30257183
http://arxiv.org/pdf/2207.05549v1,PoeticTTS -- Controllable Poetry Reading for Literary Studies,"text is ""Lass endlich Vater offenen Aug’s mich dir // Begegnen!"" In future work, we want to include more diverse poetry from
                                                                    different authors and speakers into our approach and simplify
                                                                    the procedure for human-in-the-loop prosody manipulation by
                                                                    means of an intuitive user interface. 6. ",eess.AS,A,-0.10245696,-0.07911607,0.25344238
http://arxiv.org/pdf/2207.05549v2,PoeticTTS -- Controllable Poetry Reading for Literary Studies,"text is ""Lass endlich Vater offenen Aug’s mich dir // Begegnen!"" In future work, we want to include more diverse poetry from
                                                                    different authors and speakers into our approach and simplify
                                                                    the procedure for human-in-the-loop prosody manipulation by
                                                                    means of an intuitive user interface. 6. ",eess.AS,A,-0.10245696,-0.07911607,0.25344238
http://arxiv.org/pdf/2207.05913v1,A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System,"low. [8] Y. Wu, P. L. Tobing, K. Yasuhara, N. Matsunaga, Y. Ohtani,
   For future work, since there is a signiﬁcant quality            and T. Toda, “A cyclical post-ﬁltering approach to mismatch
gap between the upper bound and the proposed method,               reﬁnement of neural vocoder for text-to-speech systems,” in
we intend to investigate advanced frameworks to tackle             Interspeech 2020, 21st Annual Conference of the International
the AM and TM problems such as different acoustic or               Speech Communication Association, Virtual Event, Shanghai,
self-supervised features or different network structures. China, 25-29 October 2020, 2020, pp. ",eess.AS,A,-0.24434242,-0.057637893,-0.09855993
http://arxiv.org/pdf/2207.06127v1,MM-ALT: A Multimodal Automatic Lyric Transcription System,"is still not negligible. We leave it for future work to maximize the
effectiveness of the IMU modality. 6 CONCLUSION

5.3.2 Qualitative Analysis. ",eess.AS,B,0.33564526,-0.03309721,0.27807224
http://arxiv.org/pdf/2207.06127v2,MM-ALT: A Multimodal Automatic Lyric Transcription System,"is still not negligible. We leave it for future work to maximize the
effectiveness of the IMU modality. 6 CONCLUSION

5.3.2 Qualitative Analysis. ",eess.AS,B,0.33564526,-0.033097267,0.2780723
http://arxiv.org/pdf/2207.07307v1,MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources,"0.9 0.6428 0.8059 0.9783 0.9583 0.7759 0.8755
                                                                           6. Conclusions and future work
5.2. The effect of threshold setting
                                                                    This paper proposes a novel multi-channel input and multiple
Due to the limited space, we only present the experimental re-      outputs DoA network (MIMO-DoAnet) to address the limita-
sults of MISO DoA baseline and MIMO-DoAnet on 2 sources             tions of threshold setting problem and overcome the angle as-
test set (SIR=0) in this part. ",eess.AS,B,0.3987509,0.32372585,-0.0038360523
http://arxiv.org/pdf/2207.07307v2,MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources,"0.9 0.6428 0.8059 0.9783 0.9583 0.7759 0.8755
                                                                           6. Conclusions and future work
5.2. The effect of threshold setting
                                                                    This paper proposes a novel multi-channel input and multiple
Due to the limited space, we only present the experimental re-      outputs DoA network (MIMO-DoAnet) to address the limita-
sults of MISO DoA baseline and MIMO-DoAnet on 2 sources             tions of threshold setting problem and overcome the angle as-
test set (SIR=0) in this part. ",eess.AS,B,0.3987509,0.32372585,-0.0038360523
http://arxiv.org/pdf/2207.09514v1,"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding","No utterance is discarded from
training synthetic. The discarded utterances are kept                                 7.7   3.6              73.1  72.5
but treated as single-channel examples with no oracle SE su-
pervision, and are left for possible future work on semi/self-                        13.2  13.2             77.8  77.5
supervised joint SE+SLU. Instead, LT-S retains the same num-
ber of utterances of the original LT as it features only clean                        13.0  13.0             80.1  80.2
speech. ",eess.AS,A,-0.12818877,-0.13284653,0.038433652
http://arxiv.org/pdf/2207.10934v1,DNN-Free Low-Latency Adaptive Speech Enhancement Based on Frame-Online Beamforming Powered by Block-Online FastMNMF,The online WPE            DNN-based mask estimation. We leave this for future work. (Sect. ,eess.AS,C,-0.015608361,0.12652369,-0.17626214
http://arxiv.org/pdf/2207.11388v1,Low-Complexity Acoustic Echo Cancellation with Neural Kalman Filtering,"For
comparing with other methods. In particular, it is interesting                    future work, we will further investigate better architecture for
to notice that the re-convergence speed of the TFDKF is much                      modeling the Kalman gain and extend the proposed method
slower than its convergence speed, whereas the re-convergence                     to multi-channel and non-linear echo cancellation. In addition,
speed of the NKF is as fast as its convergence speed. ",eess.AS,B,0.18362096,0.23706412,-0.06565266
http://arxiv.org/pdf/2207.11906v1,Learning a Dual-Mode Speech Recognition Model via Self-Pruning,"Results of supernet training with both self-supervised and
                                                                                 supervised criteria
B2 35M, dual-mode dense           10.9  8.7
                                                                                 Next, as in Table 4, we ﬁrst build a dual-mode supernet model C1 with
C1 73M, streaming sparsity 0.67   10.9     -                                     labeled data only, and then start to use unlabeled data and examine

C2 73M, non-streaming dense       -     6.4                                          2We note that, training the large transformer/Emformer models like system
                                                                                 B0 from scratch - without additional regularization techniques - signiﬁcantly
D1 73M, streaming sparsity 0.67,  10.6  7.0                                      underperforms. While applying auxiliary training criteria [34] would substan-
                                                                                 tially improve baseline B0, they can be applied to other competing systems
non-streaming dense                                                              like B1 and B2 as well, so we will leave it to future work. D2 non-streaming dense, 50K + D1  10.4  6.6

    1Following [32], we ﬁnd in each transformer layer, including the additional
third layer norm - that prevents the features from bypassing the transformer
layer entirely - noticeably improves ASR accuracies. ",eess.AS,B,0.14901832,-0.18185633,-0.38287178
http://arxiv.org/pdf/2207.11906v2,Learning a Dual-Mode Speech Recognition Model via Self-Pruning,"Results of supernet training with both self-supervised and
                                                                                 supervised criteria
B2 35M, dual-mode dense           10.9  8.7
                                                                                 Next, as in Table 4, we ﬁrst build a dual-mode supernet model C1 with
C1 73M, streaming sparsity 0.67   10.9     -                                     labeled data only, and then start to use unlabeled data and examine

C2 73M, non-streaming dense       -     6.4                                          2We note that, training the large transformer/Emformer models like system
                                                                                 B0 from scratch - without additional regularization techniques - signiﬁcantly
D1 73M, streaming sparsity 0.67,  10.6  7.0                                      underperforms. While applying auxiliary training criteria [36] would substan-
                                                                                 tially improve baseline B0, they can be applied to other competing systems
non-streaming dense                                                              like B1 and B2 as well, so we will leave it to future work. D2 non-streaming dense, 50K + D1  10.4  6.6

    1Following [34], we ﬁnd in each transformer layer, including the additional
third layer norm - that prevents the features from bypassing the transformer
layer entirely - noticeably improves ASR accuracies. ",eess.AS,B,0.14880973,-0.1824643,-0.38256538
http://arxiv.org/pdf/2207.11964v1,ConceptBeam: Concept Driven Target Speech Extraction,"This demonstrates the
these conditions. In future work, we will investigate approaches to   effectiveness of the proposed scheme. mitigate this issue [4, 7, 22, 49]. ",eess.AS,B,0.43409318,0.12675218,0.19203094
http://arxiv.org/pdf/2207.12135v1,Label Uncertainty Modeling and Prediction for Speech Emotion Recognition using t-Distributions,"t-distribution on the ground-truth emotion annotations, which
                                                                   also accounts for the number of annotations available. This is
D. Analysis on training loss curve                                 the ﬁrst time in literature an attempt was made to handle the
                                                                   limited emotion annotations available, from a machine learn-
   To further study the advantages of the proposed t-              ing perspective. For this, we proposed and derived a KL diver-
distribution LKL (13) during the training phase, we compare        gence based loss term that aims to capture emotion annotation
the testing loss curve of (13) with the Gaussian LKL in            distribution as a t-distribution. ",eess.AS,A,0.055270404,-0.31339848,0.022424456
http://arxiv.org/pdf/2207.12895v1,Multimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text,"To analyze the     is available, the text and alignment information are required for
causes of the performance gain, we conduct further experiments      the CAN to work properly. In future work, we plan to extend
to see the effectiveness of the components in our methodology,      our research by integrating the CAN with the automatic speech
which are described in the next sections. recognition system which outputs the text and alignment infor-
                                                                    mation given a speech signal. ",eess.AS,A,-0.21037501,-0.035895277,0.111432225
http://arxiv.org/pdf/2207.13333v1,Knowledge-driven Subword Grammar Modeling for Automatic Speech Recognition in Tamil and Kannada,"21
6. Conclusion and future work

    In this paper, we have presented a novel design for Tamil and Kannada
ASR systems based on subword modeling by manually creating the subword
dictionary and using to design a specialized SG-WFST for word segmentation
purpose. We have used linguistic knowledge and identiﬁed the following
category of words (i) verbs, (ii) pronouns, (iii) numbers and (iv) nouns to
create the preﬁx, inﬁx and suﬃx lists to design the SG-WFST that capture
the word formation rules of Tamil and Kannada. ",eess.AS,A,-0.06274162,-0.24457234,0.15650363
http://arxiv.org/pdf/2207.13888v1,Utterance-by-utterance overlap-aware neural diarization with Graph-PIT,"The average length of the meeting data was 3.3 min-       vestigate such potentials as well as the difﬁculty of the adapta-
utes. For the evaluation with this data, we used models trained     tion in the future work. with the aforementioned simulated data. ",eess.AS,B,0.20885055,-0.09158738,0.12092157
http://arxiv.org/pdf/2207.13934v1,A Unifying View on Blind Source Separation of Convolutive Mixtures based on Independent Component Analysis,"V–357–60. that future work should investigate more powerful ways for
the optimization of the TRINICON cost function that allow to                   [10] N. Charkani and Y. Deville, “A convolutive source separation method
exploit the generality of the cost function and the beneﬁt of                        with self-optimizing non-linearities,” in 1999 IEEE International Con-
the linear convolution model. ference on Acoustics, Speech, and Signal Processing. ",eess.AS,C,-0.092336036,0.3155327,-0.08214555
http://arxiv.org/pdf/2207.14240v1,Dialogue Enhancement and Listening Effort in Broadcast Audio: A Multimodal Evaluation,"This is left for
truth transcript are repeated, both correspond to 100% WRR. future works, for which this study lays the foundation. A clear effect of the presentation order is observed on the                                                       ACKNOWLEDGMENT
pupil size and the self-report (Figure 3), while no effect is
observed in terms of WRR (hence not shown in the ﬁgure). ",eess.AS,B,0.2712735,-0.06736094,0.455265
http://arxiv.org/pdf/2207.14240v2,Dialogue Enhancement and Listening Effort in Broadcast Audio: A Multimodal Evaluation,"This is left for
repeated, both correspond to 100% WRR. future works, for which this study lays the foundation. A clear effect of the presentation order is observed on the                                                       ACKNOWLEDGMENT
pupil size and the self-report (Figure 3), while no effect is
observed in terms of WRR (hence not shown in the ﬁgure). ",eess.AS,B,0.35993692,-0.002892312,0.4393009
http://arxiv.org/pdf/2207.14352v1,Predicting Global Head-Related Transfer Functions From Scanned Head Geometry Using Deep Learning and Compact Representations,"[5] Choi, P. T., Lam, K. C., and Lui, L. M. (2015). Flash: Fast landmark
   For future work, some further perceptual examination of our          aligned spherical harmonic parameterization for genus-0 closed brain
prediction is desired, as subjective evaluation is the ultimate         surfaces. SIAM Journal on Imaging Sciences 8, 67–94
standard for HRTF personalization. ",eess.AS,B,0.21281502,-0.006383083,0.017247606
http://arxiv.org/pdf/2207.14607v1,"Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation","proposed approach; we also did not use a multi-speaker model
as a baseline since [12] already showed that data augmentation               4. Conclusion and future work
leads to signiﬁcant improvements. Our experiments conﬁrmed
the ﬁnding. ",eess.AS,A,-0.3100233,0.007302229,0.11838892
http://arxiv.org/pdf/2208.02189v1,A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis,"It is because the Tacotron2 does not       alization ability, and feasibility. In the future work, we will ex-
have rich contextual information for deciding whether to syn-      plore transferring rising intonation to other speakers/languages
thesize rising intonation. The ImpJoint model, trained with-       and extend the methods to model more ﬁne-grained prosody by
out sentence labels, also synthesizes most declarative questions   considering contextual information other than the intention. ",eess.AS,A,-0.019229496,-0.30748343,0.20307142
http://arxiv.org/pdf/2208.02406v1,Domestic Activity Clustering from Audio via Depthwise Separable Convolutional Autoencoder Network,"MS are, the lower computational complexity and memory
requirement of the method are. The values of MACs and MS                       In the future work, we plan to deploy our method in the
of different methods are listed in Table IV. In terms of                    terminals with low-computing resources. ",eess.AS,B,0.38736233,0.09027178,0.032784984
http://arxiv.org/pdf/2208.02778v1,Data-driven Attention and Data-independent DCT based Global Context Modeling for Text-independent Speaker Recognition,"IX-C.

B. Analysis of Attention Context Model                               C. Analysis of DCT Context Model

   In this section, further analysis is presented regarding factors     As noted in Sec. IV-B, the K lowest DCT components are
that may affect performance of the attention context model           chosen as the bases. ",eess.AS,B,0.20541006,-0.10603101,0.17357975
http://arxiv.org/pdf/2208.03421v1,SSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring,"systems. We then do ablation experiments of the SSDPT for
further analysis. Moreover, we adopted two SSL methods to train the net-
                                                                   work in the SSDPT. ",eess.AS,B,0.3604266,-0.010555584,0.18513739
http://arxiv.org/pdf/2208.04654v1,Extending GCC-PHAT using Shift Equivariant Neural Networks,"We therefore con-
                                                                                                                          clude that our method would be a suitable alternative for time
        GCC-PHAT                   PGCC-PHAT                                                             Ours             delay estimation in real-world speaker localization scenarios. 50                          50                                                               50                                In future work, we will consider integrating our method into
                                                                                                                          a full sound source localization system. This requires tracking
τˆ [cm]0                    0                                                                0                            delays over time, as well as considering the geometry of the mi-
                                          τˆ [cm]                                                                         crophones and sound sources. ",eess.AS,C,-0.23780328,0.32143992,0.062867396
http://arxiv.org/pdf/2208.05445v1,Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech,"future work is to see how a larger model, longer epoch, or
                                                                   more data in DINO pre-training is related to the performance
   In the SV experiments, when the DINO embedding without          of DINO in ﬁne-tuning. As another future work, we could keep
ﬁne-tuning and x-vector were compared, DINO outperformed           the DINO training design for ﬁne-tuning. In SER, for example,
x-vector given the same amount of labeled data during their        we can concatenate all the utterances in an emotion class
training. ",eess.AS,B,0.09144143,-0.18609658,-0.032690577
http://arxiv.org/pdf/2208.05782v1,Comparison and Analysis of New Curriculum Criteria for End-to-End ASR,"Based on our ﬁndings, we believe that curriculum learning
                                                                   has a lot of potential in ASR. As future work, we plan to deﬁne
                                                                   new scoring and pacing functions that incorporate more infor-
                                                                   mation from the neural network under training. In addition, we
                                                                   are planning to test our approaches on some commonly used,
                                                                   large English datasets, to prove the effectiveness of our meth-
                                                                   ods at a larger scale. ",eess.AS,A,0.08172381,-0.41544324,-0.08086995
http://arxiv.org/pdf/2208.11428v1,Automatic music mixing with deep learning and out-of-domain data,"CONCLUSION
guitars. In contrast, the models are conservative when it
comes to panning and are unlikely to hard-pan sources,                      We present a novel data preprocessing approach that allows
however a further analysis is required. Furthermore, al-                    us to train deep learning networks with existing wet or pro-
though we show the models successfully mixing multiple                      cessed multitrack data by reusing them to perform an au-
genres, an in-depth analysis of the genre distribution of the               tomatic mixing task. ",eess.AS,C,-0.12287245,0.017834537,-0.33054584
http://arxiv.org/pdf/2208.11428v2,Automatic music mixing with deep learning and out-of-domain data,"CONCLUSION
guitars. In contrast, the models are conservative when it
comes to panning and are unlikely to hard-pan sources,                      We present a novel data preprocessing approach that allows
however a further analysis is required. Furthermore, al-                    us to train deep learning networks with existing wet or pro-
though we show the models successfully mixing multiple                      cessed multitrack data by reusing them to perform an au-
genres, an in-depth analysis of the genre distribution of the               tomatic mixing task. ",eess.AS,C,-0.12287245,0.017834537,-0.33054584
http://arxiv.org/pdf/2208.13916v1,A Language Agnostic Multilingual Streaming On-Device ASR System,"Future work will investigate   coder block 0 is aware of the endpointer task in training, which
intrasentential code switching in great details. will be explored in future work. Next, with the same number of
                                                                  additional parameters (EP0 vs. EP2), encoder endpointer yields
                                                                  better accuracy, beneﬁting from the pretrained ASR encoder. ",eess.AS,B,0.25284645,-0.10836745,-0.1985747
http://arxiv.org/pdf/2208.13943v1,Classify Respiratory Abnormality in Lung Sounds Using STFT and a Fine-Tuned ResNet18 Network,"There are four different models trained for this   binations, achieving Scores of 0.89, 0.80, 0.71, 0.36 for tasks
task, which includes R-STFT, L-STFT, a pre-trained ResNet18    1-1, 1-2, 2-1 and 2-2, respectively on the testing sets of
with Mel spectrogram as inputs (R-MEL), and an audio           the IEEE BioCAS 2022 Grand Challenge on Respiratory
spectrogram transformer architecture with Wav2vec feature as   Sound Classiﬁcation. In future work, addressing the imbal-
inputs (AST-Wav2vec). The training results are presented in    anced samples is essential to improving sensitivity scores. ",eess.AS,C,-0.14545807,0.03657805,-0.06711585
