url,title,further research,primary category,label,x,y,z
http://arxiv.org/pdf/2201.00292v1,Fair Data Representation for Machine Learning at the Pareto Frontier,"‚Ä¢ We design an algorithm that is computationally eÔ¨Écient in high-dimensional data space,
   by proving a (nearly) closed-form solution of the pseudo-barycenter and the corresponding
   optimal transport maps in both the Gaussian case and the general marginal distribution case. ‚Ä¢ We shed light on the application of the pseudo-barycenter to L2-objective unsupervised learn-
   ing to achieve diverse data allocation and representation, which provides potential access to
   fairness in unsupervised learning and deserves further study, see Figure 1. 3
Figure 1: In the upper-left panel, the three distributions are sampled from isotropic Gaussian distribution with diÔ¨Äerent Ô¨Årst
two moments. ",stat.ML,C,-0.033600435,-0.2457383,-0.16986267
http://arxiv.org/pdf/2201.01036v1,Supervised Homogeneity Fusion: a Combinatorial Approach,"These technical results are
not only of theoretical interest, but also useful to guide practical work such as sample size
determination in a study design. An important future work concerns statistical inference after the operation of L0-Fusion. A thorough investigation on the inÔ¨Çuence of selection errors on statistical inference, in both
aspects of Ô¨Ånite-sample and large-sample properties, is of great interest. ",stat.ML,A,-0.011461431,0.2752027,-0.28107128
http://arxiv.org/pdf/2201.01689v2,Asymptotics of $\ell_2$ Regularized Network Embeddings,"We moreover highlight empirically
that using 2 regularization with node2vec leads to competitive performance on downstream tasks,
when compared to embeddings produced from more recent encoding and training architectures. We end with a brief discussion of some limitations of our works, and directions for future work. Generally, graphons are not realistic models for graphs; we suggest that our work could be extended
to frameworks such as graphexes [10, 11, 58] which can produce more realistic degree distributions
for networks, but have enough underlying latent exchangeability for our arguments to go through. ",stat.ML,C,-0.19714953,-0.2553045,0.17835185
http://arxiv.org/pdf/2201.01689v3,Asymptotics of $\ell_2$ Regularized Network Embeddings,"We moreover highlight empirically
that using 2 regularization with node2vec leads to competitive performance on downstream tasks,
when compared to embeddings produced from more recent encoding and training architectures. We end with a brief discussion of some limitations of our works, and directions for future work. Generally, graphons are not realistic models for graphs; we suggest that our work could be extended
to frameworks such as graphexes [10, 11, 62] which can produce more realistic degree distributions
for networks, but have enough underlying latent exchangeability for our arguments to go through. ",stat.ML,C,-0.19581097,-0.25559974,0.17777403
http://arxiv.org/pdf/2201.02115v2,"The dynamics of representation learning in shallow, non-linear autoencoders","Nonlinear autoencoders learn principal
statistical physics. While we conjecture that they are ex-                 components sequentially
act, we leave it to future work to prove their asymptotic
correctness in the limit D ‚Üí ‚àû. We anticipate that tech-              The dynamical equations of the encoder-encoder overlap
niques to prove the correctness of dynamical equations for            qk , eq. ",stat.ML,B,0.15548697,-0.13261104,0.25711775
http://arxiv.org/pdf/2201.03128v1,Loss-calibrated expectation propagation for approximate Bayesian decision-making,"Curiously, we observe that there is likely more than one way to develop
a loss-calibrated inference procedure around the Ô¨Çexible machinery of expectation propagation, and
future work could better harness the desirable computational properties commonly associated with
EP to improve performance. Following [21], future work could develop an EP-type approximation to Generalized Bayesian
Inference [22], which updates prior beliefs about a parameter of interest that is connected to our
observations by arbitrary loss functions, rather than likelihood functions. Following the substantial
body of theory work into loss functions and machine learning [23], we could perform iterative EP
updates that minimize not the KL divergence, but instead some general divergence measure (also
suggested in [8]) that is uniquely determined by the loss function in the classiÔ¨Åcation case. ",stat.ML,C,-0.11918028,-0.10685523,-0.31972218
http://arxiv.org/pdf/2201.03182v1,Non-Asymptotic Guarantees for Robust Statistical Learning under $(1+\varepsilon)$-th Moment Assumption,"We conclude this paper by the following two examples, robust two-component
mixed linear regression and robust non-negative matrix factorization, whose algorithms are
diÔ¨Äerent from SGD. Here we only roughly address their theoretical results and leave detailed
study to the future work. Robust two-component mixed linear regression. ",stat.ML,C,-0.061593473,-0.11007603,0.024888463
http://arxiv.org/pdf/2201.03182v2,Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite Variance Assumption,"We conclude this paper with the following two examples, ro-
bust two-component mixed linear regression and robust non-negative matrix factorization,
whose algorithms diÔ¨Äer from SGD. Here we only roughly address their theoretical results
and leave detailed study to future work. Robust two-component mixed linear regression. ",stat.ML,C,-0.09525879,-0.09674258,0.032042596
http://arxiv.org/pdf/2201.03949v1,Entropic Optimal Transport in Random Graphs,"We still consider
be diÔ¨Écult to use in practice, as the constant Œ≥ can                                                                                      non-local kernel wN = œÅN w, but here speciÔ¨Åcally with
                                                                                                                                          a kernel of the form:
be hard to adjust. It is however a good inspiration to
                                                                                                                                          w(x, y) = e‚àí         x‚àíy   p
combine with methods that learn the cost for robust                                                                                                               œÉ
                                                                                                                                                                           (15)
OT [11, 16], which we leave for future work. such as, for instance, the Gaussian kernel when p = 2. ",stat.ML,B,0.26397312,-0.21146816,0.0272863
http://arxiv.org/pdf/2201.04469v6,Optimal Best Arm Identification in Two-Armed Bandits with a Fixed Budget under a Small Gap,"(2011) shows the asymptotic distributions of the
sample average by going through that of the AIPW estimator. 6.3 Extension to BAI in the multi-armed bandit (MAB) problems

An important future work is to extend our results for the MAB problems. Unlike two-armed bandit problems and BAI
with Ô¨Åxed conÔ¨Ådence, the lower bounds are also still unknown. ",stat.ML,B,0.25049248,0.13716438,-0.35518575
http://arxiv.org/pdf/2201.04469v7,Optimal Best Arm Identification in Two-Armed Bandits with a Fixed Budget under a Small Gap,"(2011) shows the asymptotic distributions of the
sample average by going through that of the AIPW estimator. 6.3 Extension to BAI in the multi-armed bandit (MAB) problems

An important future work is to extend our results for the MAB problems. Unlike two-armed bandit problems and BAI
with Ô¨Åxed conÔ¨Ådence, the lower bounds are also still unknown. ",stat.ML,B,0.25049248,0.13716438,-0.35518575
http://arxiv.org/pdf/2201.04545v1,On generalization bounds for deep networks based on loss surface implicit regularization,"Both quantities depend on the learning rate
ùúÇùë° and the switch between reaching and non-escaping of local minima is driven by the decay of the
learning rate. We view this work as a very Ô¨Årst step to explore loss surfaced based implicit regularization and
hope that the presented ideas spark further research into this direction. We want to stress that in

                                                                             14
many respects, the obtained results in this work are preliminary. ",stat.ML,B,0.15150484,-0.33751357,-0.005734682
http://arxiv.org/pdf/2201.04545v2,On generalization bounds for deep networks based on loss surface implicit regularization,"The switch between reaching and non-escaping
of local minima is driven by the decay of the learning rate. We view this work as a very Ô¨Årst step to explore loss surfaced based implicit regularization and
hope that the presented ideas spark further research into this direction. We want to stress that in
many respects, the obtained results in this work are preliminary. ",stat.ML,B,0.13744287,-0.3615172,0.0064794747
http://arxiv.org/pdf/2201.04545v3,On generalization bounds for deep networks based on loss surface implicit regularization,"The number of parameters
ùê∑ = ‚Ñé + ‚Ñé2 lies therefore in the interval [30, 930]. Training is             We view this work as a very Ô¨Årst step to explore loss surfaced
                                                                          based implicit regularization and hope that the presented ideas spark
done using Gaussian SGD (2) with multivariate standard normal             further research into this direction. We want to stress that in many
                                                                          respects, the obtained results in this work are preliminary. ",stat.ML,C,0.030435883,-0.37563956,-0.086292
http://arxiv.org/pdf/2201.04738v1,Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks,"In
this fashion we extended existing work to the setting of a network with smooth activations where all
parameters are trained as in practice. We hope damped deviations offers a simple interpretation of
the MSE dynamics and encourages others to compare against other kernels in future work. Acknowledgments
Benjamin Bowman was at the Max Planck Institute for Mathematics in the Sciences while working
on parts of this project. ",stat.ML,C,0.021377213,-0.22998074,0.06702973
http://arxiv.org/pdf/2201.04960v1,Unifying Epidemic Models with Mixtures,"This will require reÔ¨Åned statis-
tical algorithms for estimating the state of an epidemic subject to noisy observations, as well
as an understanding of the mechanisms that policy makers can utilize to inhibit spread of the
disease. Hence, further research can focus on determining optimal ways to estimate epidemic
state from noisy data due to delays and testing variance, as well as understanding the impact
that policy levers such as lockdowns and masks have on the spread of infectious disease. By
unifying the disparate approaches to epidemic forecasting, we hope to take a step towards
reaping the beneÔ¨Åts of both approaches in the design of public health policy for infectious
disease. ",stat.ML,A,-0.09627393,0.22232899,-0.19495276
http://arxiv.org/pdf/2201.05132v1,Hyperparameter Importance for Machine Learning Algorithms,"Theoretical analysis and empirical studies demonstrate
the consistency of the importance estimated by subsampling. A future work to follow up this paper is to study the
convergence speed of the hyperparameter importance for some speciÔ¨Åc machine learning algorithms instead of the
general conclusion for all machine learning algorithms. Acknowledgement

We are grateful to Sheri Kong for useful suggestions and Shuguang Zhang for his help of data processing in the case
studies. ",stat.ML,C,-0.11730072,-0.18359044,-0.24342704
http://arxiv.org/pdf/2201.05340v1,Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?,"Also, mixed problems with numeric and categorical outputs at the same time have to be inves-
tigated. Moreover, as in this study only rather low sample sizes were investigated, the behavior of these
approaches in big data settings with larger sample sizes should also be further researched. References

Breiman, L., 2001. ",stat.ML,A,-0.17631343,0.21564543,-0.044774257
http://arxiv.org/pdf/2201.06133v1,On Maximum-a-Posteriori estimation with Plug & Play priors and stochastic gradient descent,"Another
way to remove this restriction would be to consider an additive term of the form
x ‚Üí (x‚àíŒ†C(x))/Œª in bŒµ (where Œ†C is the projection onto some compact convex set
C and Œª > 0 some hyperparameter) which ensures the stability of the numerical
scheme. We leave this analysis for future work. In practice, we have not observed
any stability issues for PnP-SGD provided that the stepsize is chosen appropriately
see Section 4.3. ",stat.ML,B,0.32337016,-0.12684488,0.08674939
http://arxiv.org/pdf/2201.06652v1,Equitable Community Resilience: The Case of Winter Storm Uri in Texas,"In particular, recent
advancements in the area of microgrid technologies, in combination with the installation of onsite renewable
generation and energy storage resources, are improving the ability to sustain isolated operation of a critical
facility for several days, thereby serving the affected community until service is fully restored, e.g., see (Li,
Shahidehpour, Aminifar, Alabdulwahab, & Al-Turki, 2017; X. Liu et al., 2016). As part of our future work, we plan to investigate how critical community resources were affected during
Winter Storm Uri and similar extreme events to gain a clearer understanding of disparities in resilience related
to specific types of critical infrastructure. The results will help further identify the engineering challenges and
potential solutions required to eliminate existing demographic disparities associated with the response to and
restoration of disaster-caused electrical outages. ",stat.ML,A,-0.08636044,0.1663515,0.16192874
http://arxiv.org/pdf/2201.06652v2,Equitable Community Resilience: The Case of Winter Storm Uri in Texas,"In particular, recent
advancements in the area of microgrid technologies, in combination with the installation of onsite renewable
generation and energy storage resources, are improving the ability to sustain isolated operation of a critical
facility for several days, thereby serving the affected community until service is fully restored, e.g., see (Li,
Shahidehpour, Aminifar, Alabdulwahab, & Al-Turki, 2017; X. Liu et al., 2016). As part of our future work, the authors plan to investigate how critical community resources were affected
during Winter Storm Uri and similar extreme events to gain a clearer understanding of disparities in resilience
related to specific types of critical infrastructure. The results will help further identify the engineering
challenges and potential solutions required to eliminate existing demographic disparities associated with the
response to and restoration of disaster-caused electrical outages. ",stat.ML,A,-0.086387,0.16932735,0.16428474
http://arxiv.org/pdf/2201.07753v1,Deep Capsule Encoder-Decoder Network for Surrogate Modeling and Uncertainty Quantification,"However, it is also noted that the developed framework‚Äôs performance deteriorates when the lengthscales becomes very
small, but at the same time it is understandable as the solutions become quite unsmooth and irregular. Also, at present
the proposed architecture is frequentist in nature; future work in this direction will involve developing a Bayesian
surrogate with the same architecture. Acknowledgement

SC and AT acknowledge the open-source code provided by Tripathy and Bilionis [31] and Xifeng Guo. ",stat.ML,C,-0.13979161,-0.05684744,-0.079219244
http://arxiv.org/pdf/2201.07998v1,Statistical Learning for Individualized Asset Allocation,"An individual‚Äôs preference for a high C5 is typically consistent with a
high W5, which means that one does care about having enough saving for Ô¨Åve years later. In future works
where multi-stage decision-making is studied, this would become even less an issue. 26
5.3 Statistical Learning Implementation and Results

We conduct the analysis based on three samples: estimation sample, evaluation sample
and testing sample. ",stat.ML,A,-0.21665323,0.27296135,-0.22122453
http://arxiv.org/pdf/2201.07998v2,Statistical Learning for Individualized Asset Allocation,"An individual‚Äôs preference for a high C5 is typically consistent with a
high W5, which means that one does care about having enough savings for Ô¨Åve years later. In future works
where multi-stage decision-making is studied, this would become even less an issue. 24
argmaxA‚àà[0,1] E U (C1, C2, ...C5|X, A) , where X is the personal characteristics. ",stat.ML,A,-0.029711857,0.3507954,-0.03399671
http://arxiv.org/pdf/2201.07998v3,Statistical Learning for Individualized Asset Allocation,"An individual‚Äôs preference for a high C5 is typically consistent with a
high W5, which means that one does care about having enough savings for Ô¨Åve years later. In future works
where multi-stage decision-making is studied, this would become even less an issue. 25
5.2 Data

We use the Health and Retirement Study (HRS) data from 1992 to 2014. ",stat.ML,A,-0.15310486,0.4631768,-0.056748334
http://arxiv.org/pdf/2201.08315v1,Predictive Inference with Weak Supervision,"6 Discussion

The new measures of coverage we develop here‚Äîtailored to partially supervised data that
may be easier to collect in many engineering and measurement-centric scientiÔ¨Åc scenarios‚Äî
help to bridge a gap between typical conformal predictive inference methods, which require
expensive supervised data, and problems with partial supervision, whose typical focus is
on prediction but not uncertainty quantiÔ¨Åcation. Our hope is that this work opens several
avenues for future work. The new deÔ¨Ånition (3) a 0-1 loss-based approach, in the sense
that the conÔ¨Ådence set Cn either covers the weakly supervised set or fails. ",stat.ML,C,-0.20010835,-0.06492208,-0.16381031
http://arxiv.org/pdf/2201.08315v2,Predictive Inference with Weak Supervision,"6 Discussion

The new measures of coverage we develop here‚Äîtailored to partially supervised data that
may be easier to collect in many engineering and measurement-centric scientiÔ¨Åc scenarios‚Äî
help to bridge a gap between typical conformal predictive inference methods, which require
expensive supervised data, and problems with partial supervision, whose typical focus is
on prediction but not uncertainty quantiÔ¨Åcation. Our hope is that this work opens several
avenues for future work. The new deÔ¨Ånition (3) a 0-1 loss-based approach, in the sense
that the conÔ¨Ådence set Cn either covers the weakly supervised set or fails. ",stat.ML,C,-0.20010835,-0.06492208,-0.16381031
http://arxiv.org/pdf/2201.08504v1,Deep reinforcement learning under signal temporal logic constraints using Lagrangian relaxation,"the other hand, the syntax in this study is restrictive compared                                              R
with the general STL syntax. Relaxing the syntax restriction
is a future work. Furthermore, our approach cannot guarantee                     [1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction
satisfying the STL constraint during learning of the policy. ",stat.ML,B,-0.082321115,0.07301739,0.065060854
http://arxiv.org/pdf/2201.08504v2,Deep reinforcement learning under signal temporal logic constraints using Lagrangian relaxation,"the other hand, the syntax in this study is restrictive compared                                              R
with the general STL syntax. Relaxing the syntax restriction
is a future work. Furthermore, our approach cannot guarantee                     [1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction
satisfying the STL constraint during learning of the policy. ",stat.ML,B,-0.082321115,0.07301739,0.065060854
http://arxiv.org/pdf/2201.08504v3,Deep reinforcement learning under signal temporal logic constraints using Lagrangian relaxation,"The red, blue, and          compared with the general STL syntax. Relaxing the syntax
green curves show the results of the DDPG-Lagrangian algorithm, the TD3-        restriction is a future work. Furthermore, we may not apply
Lagrangian algorithm, and the SAC-Lagrangian algorithm, respectively. ",stat.ML,B,-0.007806154,-0.076845475,0.2384431
http://arxiv.org/pdf/2201.08504v4,Deep reinforcement learning under signal temporal logic constraints using Lagrangian relaxation,"satisfying a given STL formula for the problems. Solving the
                                                                             issue is also an interesting direction for a future work. proposed a CDRL algorithm with the Lagrangian relaxation. ",stat.ML,B,0.19023713,-0.05512511,0.17113307
http://arxiv.org/pdf/2201.08530v1,Spatiotemporal Analysis Using Riemannian Composition of Diffusion Operators,"Finally, we remark that in our model, we represent each sample by an undirected weighted graph,
and then, analyze the temporal sequence of graphs. Another interesting future work would be to

                                             36
investigate our Riemannian composite operators in the context of graph neural networks (GNNs)
and graph convolutional networks (GCNs) [11, 36, 56]. References

 [1] V. Arsigny, P. Fillard, X. Pennec, and N. Ayache. ",stat.ML,C,-0.09551196,-0.2540474,0.29308876
http://arxiv.org/pdf/2201.08712v1,Improved Random Features for Dot Product Kernels,"Moreover, the ratio of the variances tends to be even smaller for a larger value of p. These
results suggest that the complex TensorSRHT is eÔ¨Äective in reducing the variance of the
real TensorSRHT, and the variance reduction is more signiÔ¨Åcant for a larger value p of the
polynomial degree. We leave a theoretical analysis for explaining this improvement of the
complex TensorSRHT for a future work. 5. ",stat.ML,B,0.23696655,-0.056948032,0.12520954
http://arxiv.org/pdf/2201.08712v2,Improved Random Features for Dot Product Kernels,"Moreover, the ratio of the variances tends to be even smaller for a larger value of p. These
results suggest that the complex TensorSRHT is eÔ¨Äective in reducing the variance of the
real TensorSRHT, and the variance reduction is more signiÔ¨Åcant for a larger value p of the
polynomial degree. We leave a theoretical analysis for explaining this improvement of the
complex TensorSRHT for a future work. 5. ",stat.ML,B,0.23696655,-0.056948032,0.12520954
http://arxiv.org/pdf/2201.08932v1,Overcoming Oversmoothness in Graph Convolutional Networks via Hybrid Scattering Networks,"We also provide evidence that the proposed hy-
                                                                                       brid scattering networks perform well in graph-level tasks,
                                                                                       both in classiÔ¨Åcation and regression. In future work, one
                                                                                       might further explore the potential of features that carry
                                                                                       graph structure information empirically, and analyze more
                                                                                       datasets in the context of graph-level tasks. Dataset                          GCN     GAT    GS-SVM   Sc-GCN         GSAN      APPENDIX A
    COLLAB                            0.592   0.523    0.640    0.690        0.704
IMBD-BINARY                           0.710   0.632    0.710    0.740        0.760     A.1 Supplemental DeÔ¨Ånitions for Sec. ",stat.ML,C,-0.21054786,-0.14202467,0.22028506
http://arxiv.org/pdf/2201.08932v2,Overcoming Oversmoothness in Graph Convolutional Networks via Hybrid Scattering Networks,"49,
both in classiÔ¨Åcation and regression. In future work, one                          no. 3, pp. ",stat.ML,A,0.0062165903,0.35237908,-0.009071195
http://arxiv.org/pdf/2201.10239v1,Design choice and machine learning model performances,"It should be pointed out here that only predic-
tive performance of the algorithms in terms of prediction error will be taken
into account, and other aspects, such as the ability to quantify uncertainty
of predictions and the possibility of investigating causality, are features that
should be considered when choosing an algorithm for prediction or optimiza-
tion2. The authors encourage further research devoted to these topics, but
this is outside the scope of the present work. The paper is organized as follows: section 2 presents the literature back-
ground to the application of ML for the analysis of DOE data; section 3
presents the methodology and details of the simulation study conducted;
section 4 shows the results and section 5 discusses the Ô¨Åndings in depth. ",stat.ML,A,-0.3262868,0.10897048,-0.14926681
http://arxiv.org/pdf/2201.10239v2,Design choice and machine learning model performances,"It should be pointed
                                          out here that only predictive performance of the algorithms in terms of prediction error will be taken into
                                          account, and other aspects, such as the ability to quantify uncertainty of predictions and the possibility of
                                          investigating causality, are features that should be considered when choosing an algorithm for prediction or
                                      Published OA in QREI - DOI: 10.1002/qre.3123

optimization 2. The authors encourage further research devoted to these topics, but this is outside the scope
of the present work. The paper is organized as follows: section 2 presents the literature background to the application of
ML for the analysis of DOE data; section 3 presents the methodology and details of the simulation study
conducted; section 4 shows the results and section 5 discusses the Ô¨Åndings in depth. ",stat.ML,A,-0.2633698,0.10259859,-0.20085855
http://arxiv.org/pdf/2201.10469v1,Convex Analysis of the Mean Field Langevin Dynamics,"Usually, this            ciently large. In future work, we intend to investigate
error depends on the step size Œ∑, hence Œ∑ should be              conditions under which such exponential dependence
chosen suÔ¨Éciently small to achieve a required optimiza-          can be avoided. tion accuracy. ",stat.ML,B,0.3285095,0.014607767,-0.046089668
http://arxiv.org/pdf/2201.10469v2,Convex Analysis of the Mean Field Langevin Dynamics,"ciently large. In future work, we intend to investigate
conditions under which such exponential dependence                                 Theorem 3 (Duality Theorem). Suppose (¬∑, y) is con-
can be avoided (e.g., as in Chen et al. ",stat.ML,B,0.32465282,0.0295336,0.30319512
http://arxiv.org/pdf/2201.11108v1,A probabilistic latent variable model for detecting structure in binary data,"It is unclear how to explain in isolation the spatial
configurations of the two types of strongly participating member cells of the two heterogeneous CAs, shown in top
center plots. These configurations roughly match high contrast structure in the stimulus frames around the time of
CA activations but a quantitative analysis of stimulus properties that trigger these CAs is left to future work. (a) Cell Assembly z5 and stimulus:

(b) Cell Assembly z19 and stimulus
Fig 20. ",stat.ML,A,-0.06328125,0.18963055,0.3912041
http://arxiv.org/pdf/2201.11162v1,Self-Certifying Classification by Linearized Deep Assignment,"Notably, this is without altering the training regime. we will exploit in future work. The indicated error rate on CIFAR-10 is much lower than               Acknowledgements
the one in (Pe¬¥rez-Ortiz et al., 2021). ",stat.ML,A,-0.076665886,0.008159273,-0.07415316
http://arxiv.org/pdf/2201.11162v2,Self-Certifying Classification by Linearized Deep Assignment,"Thus, essentially no improvement of the posterior over the prior is
to be expected. By contrast, the low-rank numerics used to compute the
pushforward under the LDAF reveal additional information about learnt
parameters that we will exploit in future work. Acknowledgements

   This work is funded by the Deutsche Forschungsgemeinschaft (DFG), grant
SCHN 457/17-1, within the priority programme SPP 2298: ‚ÄúTheoretical
Foundations of Deep Learning‚Äù. ",stat.ML,C,-0.11194251,-0.2802748,-0.15695453
http://arxiv.org/pdf/2201.11211v1,Learning Mixtures of Linear Dynamical Systems,"It is possible to adapt the existing
perturbation results in [KSS+20, KSKO20] to our setting (which we include in Lemma 3 in the appendix for

completeness); however, one of them is dependent on the eigenvalue gaps, while the other one incurs a worse
1/Tt1o/ta6l,subspace dependence. It would be interesting future work to investigate whether a gap-free bound with
a 1/Tt1o/ta2l,subspace dependence is possible. Clustering. ",stat.ML,B,0.3185502,-0.064992994,0.28628588
http://arxiv.org/pdf/2201.11211v2,Learning Mixtures of Linear Dynamical Systems,"It is possible to adapt the existing
perturbation results in [KSS+20, KSKO20] to our setting (which we include in Lemma 3 in the appendix for

completeness); however, one of them is dependent on the eigenvalue gaps, while the other one incurs a worse
1/Tt1o/ta6l,subspace dependence. It would be interesting future work to investigate whether a gap-free bound with
a 1/Tt1o/ta2l,subspace dependence is possible. Clustering. ",stat.ML,B,0.3185502,-0.064992994,0.28628588
http://arxiv.org/pdf/2201.11306v1,Multi-view learning with privileged weighted twin support vector machine,"5.4. Friedman test
    We use Friedman test [41] for further analysis since the averaged accuracy

of our MPWTSVM in Table 3 is not always optimal. For each dataset, the

                                               27
highest accuracy ranking is 1, followed by 2, and so on. ",stat.ML,A,-0.081296496,0.18502915,-0.11613199
http://arxiv.org/pdf/2201.12020v1,A Robust and Flexible EM Algorithm for Mixtures of Elliptical Distributions with Missing Data,"(2020), this paper focuses on the data reconstruction task. Section 5
        Ô¨Ånally draws some conclusions and presents some future work that would deserve to be conducted. 1The proposed approach could also be used for clustering, as in the case without missing data. ",stat.ML,C,-0.12542051,-0.035370644,0.04529385
http://arxiv.org/pdf/2201.12020v2,A Robust and Flexible EM Algorithm for Mixtures of Elliptical Distributions with Missing Data,"Finally, it would be interesting to study the case where the data is
missing not at random (MNAR), such as in Sportisse et al. (2021), which is left for future work. 3.1 The E-step

The E-step for elliptical distributions with missing data requires to evaluate E[log Lc(Œ∏; x, z))|Œ∏(t), xo]. ",stat.ML,A,0.112824574,0.09396404,-0.10722478
http://arxiv.org/pdf/2201.12020v3,A Robust and Flexible EM Algorithm for Mixtures of Elliptical Distributions with Missing Data,"Finally, it would be interesting to study the case where the data is
missing not at random (MNAR), such as in Sportisse et al. (2021), which is left for future work. 3.1 The E-step

The E-step for elliptical distributions with missing data requires to evaluate E[log Lc(Œ∏; X , Z))|Œ∏(t), X o]. ",stat.ML,A,0.11718209,0.093742676,-0.1079068
http://arxiv.org/pdf/2201.12151v1,Sampling Theorems for Learning from Incomplete Measurements,"Extending the present theory to the case where the operators Ag present some problem speciÔ¨Åc
constraints (e.g., they are inpainting matrices and/or the diÔ¨Äerent operators are related by some
transformation) is an interesting avenue of future research. We also leave the study of robustness to
noise as well as the cases where the signal model is only approximately low dimensional and/or the
ambient space is inÔ¨Ånite dimensional (i.e., signals with a continuous representation) for future work. Acknowledgements

This work is supported by the ERC C-SENSE project (ERCADG-2015-694888). ",stat.ML,B,0.25360528,-0.1700599,0.14078099
http://arxiv.org/pdf/2201.12151v2,Sampling Theorems for Learning from Incomplete Measurements,"Extending the present theory to the case where the operators Ag present some problem speciÔ¨Åc
constraints (e.g., they are inpainting matrices and/or the diÔ¨Äerent operators are related by some
transformation) is an interesting avenue of future research. We also leave the study of robustness to
noise as well as the cases where the signal model is only approximately low dimensional and/or the
ambient space is inÔ¨Ånite dimensional (i.e., signals with a continuous representation) for future work. 12
Acknowledgements

This work is supported by the ERC C-SENSE project (ERCADG-2015-694888). ",stat.ML,B,0.250306,-0.16687325,0.14190064
http://arxiv.org/pdf/2201.12151v3,Unsupervised Learning From Incomplete Measurements for Inverse Problems,"Note however that Proposition 4.1 does apply to constrained operators. We leave the
study of suÔ¨Écient conditions for these particular cases for future work. The proposed loss might not be eÔ¨Äective in problems where learning the reconstruction function
is impossible, e.g., due to high noise aÔ¨Äecting the measurements [4]. ",stat.ML,B,0.32473284,-0.1937412,-0.02469463
http://arxiv.org/pdf/2201.12151v4,Unsupervised Learning From Incomplete Measurements for Inverse Problems,"Note however that Proposition 4.1 applies to constrained operators. We leave the study
of sufÔ¨Åcient conditions for these particular cases for future work. The proposed loss might not be
effective in problems where learning the reconstruction function is impossible, e.g., due to very high
noise affecting the measurements [4]. ",stat.ML,B,0.27625096,-0.24518187,-0.046713505
http://arxiv.org/pdf/2201.12655v1,Error Rates for Kernel Classification under Source and Capacity Conditions,"While this conjecture is observed to hold in numerical simulations, a
more thorough theoretical analysis of the error rates for r ‚â• 12 is nevertheless warranted. We leave this more challenging
analysis to future work. C. Rates for ridge classiÔ¨Åcation

In this section we derive the rates (21) for ridge classiÔ¨Åers. ",stat.ML,B,0.39642125,-0.07459277,0.11006859
http://arxiv.org/pdf/2201.12655v2,Error Rates for Kernel Classification under Source and Capacity Conditions,"Red dotted lines represent the power law Œª 3 , which provides a very

satisfying Ô¨Åt for all values of capacity/source coeÔ¨Écients Œ±, r.

Note that the rate (15) is always faster:

                                                                       Œ±               12 ,     Œ±

                                                            n‚àí 2Œ±                              3
                                                                    1+ 2                     Œ± +1
                                                                          n . ‚àímin           3                                                           (G9)

In other words, the upper bound of [23] is loose in the present setting when f ‚àà H. While numerical investigations
suggest this is also true for f ‚àà L2(X ) \ H, we leave a more detailed comparison to [23] in this case to future work. 3. ",stat.ML,B,0.407354,0.1412555,0.19585612
http://arxiv.org/pdf/2201.12682v1,Geometry- and Accuracy-Preserving Random Forest Proximities,"and G. C. Paul, ‚ÄúPredicting the deforestation probability using
                                                                                   the binary logistic regression, random forest, ensemble rotational
    Additional random forest proximity applications can                            forest, reptree: A case study at the gumani river basin, india,‚Äù Sci. be explored in future works, including quantifying outlier                         Total Environ., vol. 730, p. 139197, 2020.
detection performance and comparing against other, non-
tree based methods, assessing variable importance, and ap-                   [11] M. Gholizadeh, M. Jamei, I. Ahmadianfar, and R. Pourrajab, ‚ÄúPre-
plying RF-GAP to multi-view learning. ",stat.ML,C,-0.22954789,-0.07869651,-0.18625054
http://arxiv.org/pdf/2201.12947v1,Fair Wrapping for Black-box Predictions,"These factors could be used to scrutinise the original blackbox; and eventually, even provide
constraints on the growth of an Œ±-tree which would aim to avoid certain combinations of attribute. We leave these factors
for future work. Fair Wrapping for Black-box Predictions

                                                                                                                                           Bank Binary Sens. ",stat.ML,A,-0.13321891,0.16416633,0.14156824
http://arxiv.org/pdf/2201.12947v2,Fair Wrapping for Black-box Predictions,"These factors could be used to scrutinise the original blackbox; and eventually, even provide
constraints on the growth of an Œ±-tree which would aim to avoid certain combinations of
attribute. We leave these factors for future work. 50
                                                                                                                                           Bank Binary Sens. ",stat.ML,A,-0.070025854,0.2501149,0.23964632
http://arxiv.org/pdf/2201.12947v3,Fair Wrapping for Black-box Predictions,"These factors could be used to scrutinise the original blackbox; and eventually, even provide
constraints on the growth of an Œ±-tree which would aim to avoid certain combinations of
attribute. We leave these factors for future work. 69
                                                                                                  month=jul_1.0                                                                           age                  emp.var.rate            month=may_1.0
                                                                                               contact=cellular_1.0                                               emp.var.rate                             contact=cellular_1.0
                                                                                                                                                                month=may_1.0

                                                              cons.price.idx                                                                     cons.conf.idx          education=university.degree_1.0          2.41            4.91  cons.conf.idx              education=high.school_1.0

                                          marital=single_1.0  day_of_week=fri_1.0        3.15  day_of_week=fri_1.0         poutcome=failure_1.0        euribor3m           day_of_week=fri_1.0             1.89        0.43            education=high.school_1.0  1.91  2.30

                education=basic.4y_1.0    2.95                day_of_week=thu_1.0  2.56        marital=married_1.0   2.18  0.57  0.87            1.49           euribor3m        euribor3m           2.98                              0.80           1.61

70        job=technician_1.0        3.83        education=university.degree_1.0    2.76        2.95  2.51                                              0.70       1.21     euribor3m                 1.82

          job=admin._1.0      3.49              euribor3m     2.84                                                                                                         2.18                3.16

    job=blue-collar_1.0       3.36        3.03  3.56

    2.95  3.22

    Figure 27: Example tree generated in the optimization of TopDown for CVaR in the Bank dataset with binary sensitive
    attributes. ",stat.ML,A,-0.13896555,0.37614954,0.13876222
http://arxiv.org/pdf/2201.12973v1,GenMod: A generative modeling approach for spectral representation of PDEs with random inputs,"Notably, alternative sparsity promoting algorithms exist that take into
account some of the underlying PC coeÔ¨Écient structure [1, 26, 7]. In future work a more
thorough comparative analysis of our approach with these alternative methods is warranted. However, our approach is unique in that we use a nonlinear function to estimate the coeÔ¨É-
cients and explicitly incorporate the coeÔ¨Écient decay into this function. ",stat.ML,B,0.25805676,-0.20269257,0.16204676
http://arxiv.org/pdf/2201.13055v1,Nystr√∂m Kernel Mean Embeddings,"issn: 2049-8772. Chen, Yutian, Max Welling, and Alex Smola (July 8,
An interesting question for future work is to see           2010). ‚ÄúSuper-Samples from Kernel Herding‚Äù. ",stat.ML,A,0.06419058,0.0073492564,-0.061864123
http://arxiv.org/pdf/2201.13055v2,Nystr√∂m Kernel Mean Embeddings,"Results are presented in Figure 2, and display the same
behavior as with synthetic data. While we don‚Äôt have                  An interesting question for future work is to see
any particular guarantees on the spectrum‚Äôs decay in                  whether our results can be improved with other sam-
this setting, it is still‚àöverified‚àöempirically than choos-            pling strategies: while it is well known that leverage
ing ùëö of the order of ùëõ log( ùëõ) is sufficient to obtain               scores sampling allows in many related contexts to
                                                                      achieve the same error with a smaller number of land-
    1https://fasttext.cc/docs/en/english-vectors. marks, it is not clear for now how to obtain meaningful
html                                                                  theoretical guarantees under this sampling scheme. ",stat.ML,B,0.0604689,0.032314483,-0.0770476
http://arxiv.org/pdf/2201.13114v1,An end-to-end deep learning approach for extracting stochastic dynamical systems with $Œ±$-stable L√©vy noise,"For example, although
the maximum log-likelihood estimation is precise, the training process is slow. We will try to study
better for the probability density function in high-dimensional cases in our future work. Acknowledgements

We would like to thank Lingyu Feng, Wei Wei, Min Dai for helpful discussions. ",stat.ML,C,-0.123662665,-0.2770595,-0.25436234
http://arxiv.org/pdf/2201.13114v2,An end-to-end deep learning approach for extracting stochastic dynamical systems with $Œ±$-stable L√©vy noise,"For example, although
the maximum log-likelihood estimation is precise, the training process is slow. We will try to study
better for the probability density function in high-dimensional cases in our future work. Acknowledgements

We would like to thank Lingyu Feng, Wei Wei, Min Dai, Yang Li for helpful discussions. ",stat.ML,C,-0.12339796,-0.2766918,-0.24756704
http://arxiv.org/pdf/2201.13114v3,An end-to-end deep learning approach for extracting stochastic dynamical systems with $Œ±$-stable L√©vy noise,"For example, although
the maximum log-likelihood estimation is precise, the training process is slow. We will try to study
better for the probability density function in high-dimensional cases in our future work. Acknowledgements

We would like to thank Lingyu Feng, Wei Wei, Min Dai, Yang Li for helpful discussions. ",stat.ML,C,-0.12339796,-0.2766918,-0.24756704
http://arxiv.org/pdf/2201.13114v4,An end-to-end deep learning approach for extracting stochastic dynamical systems with $Œ±$-stable L√©vy noise,"Under
special assumptions, we approximate the drift and diÔ¨Äusion coeÔ¨Écients of a two-dimensional Maier-
Stein model. Finally, we conclude the advantages and future work of this research paper. Moreover,
many detailed explanations and results are presented in Appendices A, B and C, as well as properties
of Œ±-stable random variables, data preprocessing tricks ,richer experimental results and so on. ",stat.ML,B,0.29643244,0.19323793,-0.057164684
http://arxiv.org/pdf/2202.00076v1,Optimal Estimation of Off-Policy Policy Gradient via Double Fitted Iteration,"To our knowledge, this is the Ô¨Årst rigorous analysis
of actor critic methods and it shows that the on policy LQR actor-critic algorithm can get an Œµ‚àíoptimal
policy with O(1/Œµ5) state-action pairs sampled. Since then, many papers have carried out further analysis on
non-asymptotic behavior of on policy actor critic methods. [33] and [13] analyze two variants of actor-critic
and prove their sample complexity to get an Œµ‚àíoptimal policy to be O(1/Œµ14) and O(1/Œµ4) respectively. ",stat.ML,B,0.2072739,0.07566195,-0.06506033
http://arxiv.org/pdf/2202.00076v2,Optimal Estimation of Off-Policy Policy Gradient via Double Fitted Iteration,"To our knowledge, this is the Ô¨Årst rigorous analysis
of actor critic methods and it shows that the on policy LQR actor-critic algorithm can get an Œµ‚àíoptimal
policy with O(1/Œµ5) state-action pairs sampled. Since then, many papers have carried out further analysis on
non-asymptotic behavior of on policy actor critic methods. [34] and [13] analyze two variants of actor-critic
and prove their sample complexity to get an Œµ‚àíoptimal policy to be O(1/Œµ14) and O(1/Œµ4) respectively. ",stat.ML,B,0.20793039,0.07592016,-0.06513867
http://arxiv.org/pdf/2202.00081v2,On solutions of the distributional Bellman equation,"Theorem 4 justiÔ¨Åes to model the tails of the state-value distributions a
priori as a Pareto-like distribution, the correct choice of asymptotic parameters is given by (23). We plan to report on such issues in future work. 2.1 Connection to multivariate Fixed-Point equations

The operator T : P(R)d ‚Üí P(R)d depends only the marginal laws of the random pairs
(R1, J1), . ",stat.ML,B,0.3866806,0.124063194,-0.015392221
http://arxiv.org/pdf/2202.00187v2,Deep Reference Priors: What is the best way to pretrain a model?,"This choice is motivated from the emergent low-dimensional structure of the green particles in Fig. 2;
see the further analysis in in ¬ß4.4. Remark 3 (Variational approximations of reference priors). ",stat.ML,B,0.26130202,-0.02534492,0.10865222
http://arxiv.org/pdf/2202.00293v1,Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks,"This is
a very challenging problem that only recently has been studied (though in a simpler setting) in [24, 25]. Analysing such dependence for the high-dimensional two-layer network is an open question which we
leave for future work. 4 Special cases, and simulations

In order to illustrate the general phase diagram of Fig. ",stat.ML,B,0.1294053,0.028342243,0.3822922
http://arxiv.org/pdf/2202.00293v2,Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks,"This is a challenging
problem that only recently has been studied (though in a simpler setting) in [26, 27, 28, 29] who showed
it yields an additional log(ùëë) time-dependence. Generalizing these results for high-dimensional two-layer
nets is an open question which we leave for future work. 4 Discussion, special cases, and simulations

To illustrate the phase diagram of Figure 1a, we present now several special cases for whi‚àöch we can perform
simulations or numerically solve the set of ODEs. ",stat.ML,B,0.1724695,-0.037859306,0.35631505
http://arxiv.org/pdf/2202.00293v3,Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks,"This is a challenging
problem that only recently has been studied (though in a simpler setting) in [28, 29, 30] who showed it
yields an additional log(ùëë) time-dependence. Generalizing these results for high-dimensional two-layer
nets is an open question which we leave for future work. 4 Discussion, special cases, and simulations

To illustrate the phase diagram of Figure 1a, we present now several special cases for whi‚àöch we can perform
simulations or numerically solve the set of ODEs. ",stat.ML,B,0.17361972,-0.037015602,0.35031936
http://arxiv.org/pdf/2202.00602v1,Meta-Learning Hypothesis Spaces for Sequential Decision-making,"However, we expect to be
                                                               able to obtain an extension to inÔ¨Ånite dimensional base-kernels, i.e. dmax ‚àà N‚à™{‚àû} in future work. Moreover, note that many inÔ¨Ånite
                                                               dimensional kernels can be uniformly approximated to arbitrary
                                                               accuracy with Ô¨Ånite feature maps (cf., Rahimi et al., 2007). ",stat.ML,C,0.10344427,-0.31394562,0.09648687
http://arxiv.org/pdf/2202.00602v2,Meta-Learning Hypothesis Spaces for Sequential Decision-making,"However, we expect to be
                                                               able to obtain an extension to inÔ¨Ånite dimensional base-kernels, i.e. dmax ‚àà N‚à™{‚àû} in future work. Moreover, note that many inÔ¨Ånite
                                                               dimensional kernels can be uniformly approximated to arbitrary
                                                               accuracy with Ô¨Ånite feature maps (cf., Rahimi et al., 2007). ",stat.ML,C,0.10344427,-0.31394562,0.09648687
http://arxiv.org/pdf/2202.00602v3,Meta-Learning Hypothesis Spaces for Sequential Decision-making,"empirically our approach is also applicable to kernels with

                                                               inÔ¨Ånite dimensional feature map.2

                                                               Let œÜ(x) denote the d-dimensional feature map for k‚àó
                                                                            p
                                                               where d =    j=1      dj  and

                                                                   2Meta-learning the hypothesis space in the p ‚Üí ‚àû limit will
                                                               be challenging. However, we expect to obtain an extension to
                                                               inÔ¨Ånite dimensional base-kernels in future work. Meta-Learning Hypothesis Spaces

                Œ∑1‚àóœÜT1 (x), ¬∑ ¬∑ ¬∑ ,                           ‚àó  œÜTp             T                       We will refer to this problem as Meta-Kernel Learning
                                                              p                                          (META-KEL). ",stat.ML,C,-0.019226123,-0.22555038,0.05001362
http://arxiv.org/pdf/2202.00622v1,Datamodels: Predicting Predictions from Training Data,"‚Ä¢ Building data exploration tools. In a similar vein, another opportunity for future work is in building
       user-friendly data exploration tools that leverage datamodel embeddings. In this paper we present the
       simplest such example in the form of PCA, but leave the vast Ô¨Åeld of data bias and feature discovery
       methods (cf. ",stat.ML,C,-0.3951071,-0.039111666,0.067588605
http://arxiv.org/pdf/2202.00824v4,KSD Aggregated Goodness-of-fit Test,"The uniform separation
rates presented in Theorem 3.5 are over the restricted Sobolev balls Sds,t(R, L). Whether those rates
can also be derived under less restrictive assumptions and in the more general setting of (unrestricted)
Sobolev balls Sds(R), is a challenging problem, which is left for future work. 4 Implementation and experiments

We consider three different experiments based on a Gamma one-dimensional distribution, a Gaussian-
Bernoulli Restricted Boltzmann Machine, and a Normalizing Flow for the MNIST dataset. ",stat.ML,B,0.15876026,-0.18692477,-0.015922027
http://arxiv.org/pdf/2202.01185v1,Heterogeneous manifolds for curvature-aware graph embedding,"(2021). In future work, we will study extensions of the proposed framework to such
settings. Second, we used a Ô¨Åxed rotationally symmetric function œï which determines the curvature
proÔ¨Åle of the ambient space. ",stat.ML,B,0.17801163,-0.012477053,0.30050996
http://arxiv.org/pdf/2202.01210v1,Deep Layer-wise Networks Have Closed-Form Weights,"Therefore, we as a community,
must begin addressing how we can ameliorate our own carbon footprint. This exploratory work aims to share
a potential path forward for further research that may address these concerns with the community. While Ws
is still not ready for commercial usage, we sincerely hope that the community begins to build novel algorithms
over our work on H-Sequence and identify a simpler and cheaper path to train our networks. ",stat.ML,C,-0.14824793,-0.02758605,0.16294622
http://arxiv.org/pdf/2202.01277v1,Global Optimization Networks,"First, they considered the selection of training
examples that would lead to good estimates, e.g., by properly covering the input
space, whereas in this paper we take the training examples as given. Second,
they noted that one might need to Ô¨Åt a series of such surrogate functions over
diÔ¨Äerent subregions of the input space, and we leave this question of specifying
a good multi-pass global optimization algorithm open for future work. Amos et al. ",stat.ML,C,-0.02004497,-0.32362399,-0.09626199
http://arxiv.org/pdf/2202.01562v1,Doubly Robust Off-Policy Evaluation for Ranking Policies under the Cascade Behavior Model,"Data Collection. To verify the OPE performance of the pro-                        In future work, we plan to explore methods to conduct estima-
posed estimator in a real-world application, we conducted a data col-             tor selection with logged bandit data. As we have discussed in
                                                                                  Section 4.3, an accurate estimator can change depending on the
lection experiment on an e-commerce platform. ",stat.ML,A,-0.071367234,0.1104611,-0.35381615
http://arxiv.org/pdf/2202.01671v1,Log-Euclidean Signatures for Intrinsic Distances Between Unaligned Datasets,"Realistic distribution shifts can
cause failures of many existing machine learning algorithms [Koh et al., 2021; Santurkar et al.,
2021]. Using LES to anticipate and diagnose poor out-of-distribution generalization is an interesting
future work direction. 11
References

Achille, A., Lam, M., Tewari, R., Ravichandran, A., Maji, S., Fowlkes, C. C., Soatto, S., and Perona,
   P. (2019). ",stat.ML,C,-0.20622885,-0.14615428,-0.32753944
http://arxiv.org/pdf/2202.01671v2,Log-Euclidean Signatures for Intrinsic Distances Between Unaligned Datasets,"Realistic distribution shifts can cause failures of many

                                                                   11
existing machine learning algorithms [Koh et al., 2021; Santurkar et al., 2021]. Using LES to anticipate
and diagnose poor out-of-distribution generalization is an interesting future work direction. Acknowledgements

We thank the anonymous reviewers for their valuable feedback and helpful recommendations. ",stat.ML,C,-0.23723313,-0.18148345,-0.31310627
http://arxiv.org/pdf/2202.01773v1,Multiclass learning with margin: exponential rates with no bias-variance trade-off,"Our analysis can be
experimentally veriÔ¨Åed for several losses, and different margin conditions. Several possible extensions of this work have been left for future work. Beyond the hard-margin
and low-noise conditions, robustness with respect to different kinds of noise may be studied. ",stat.ML,B,0.2573492,-0.07738878,-0.16233733
http://arxiv.org/pdf/2202.01850v1,A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits,"Conclusion                                                         SE kernel, and improves on the best existing bound in all
                                                                      cases where the latter is non-trivial. Perhaps the most imme-
We have provided a new algorithm for corruption-tolerant              diate direction for future work is to establish to what extent
Gaussian process bandits based on phased elimination, in-             the CŒ≥T3/2 dependence can be further improved, particularly
corporating a key idea of rare switching based on a certain           in the case of the Mate¬¥rn kernel. A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits

Acknowledgment                                                 Chowdhury, S. R. and Gopalan, A. ",stat.ML,B,0.07742679,0.004673701,-0.17847939
http://arxiv.org/pdf/2202.01850v2,A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits,"arXiv preprint
cases where the latter is non-trivial. Perhaps the most imme-      arXiv:2007.05554, 2020.
diate direction for future work is to establish to what extent
the CŒ≥T3/2 dependence can be further improved, particularly     Camilleri, R., Jamieson, K., and Katz-Samuels, J. High-
in the case of the Mate¬¥rn kernel. dimensional experimental design and kernel bandits. ",stat.ML,B,0.03248705,0.010207487,-0.06341556
http://arxiv.org/pdf/2202.01940v1,Distribution Embedding Networks for Meta-Learning with Heterogeneous Covariate Spaces,"A
potential solution is to use a random subset of these combinations. We leave the exploration of these options
to future work. References

L. Bertinetto, J. F. Henriques, J. Valmadre, P. H. S. Torr, and A. Vedaldi. ",stat.ML,B,0.027051946,0.057315595,0.10030198
http://arxiv.org/pdf/2202.02031v1,Complex-to-Real Random Features for Polynomial Kernels,"(2020) is subtle, the insights that we gain from our variance discussion are most likely valid for their
sketch as well. We leave an empirical veriÔ¨Åcation of this claim to future work. B.1. ",stat.ML,A,0.20786619,0.31774288,-0.060628187
http://arxiv.org/pdf/2202.02031v2,Complex-to-Real Random Features for Polynomial Kernels,"(2020) is subtle, the insights that we gain from our variance discussion are most likely valid for their
sketch as well. We leave an empirical veriÔ¨Åcation of this claim to future work. B.1. ",stat.ML,A,0.20786619,0.31774288,-0.060628187
http://arxiv.org/pdf/2202.02031v3,Complex-to-Real Random Features for Polynomial Kernels,"As
                                                                 the focus of this work is to obtain a sharp dependence w.r.t. 3.4) that bounds errors relative to the L1-norm instead of       p and Œ¥, we leave this issue to future work and focus on a
                                                                 careful variance analysis of CtR-sketches instead. the L2-norm, which makes their bound much looser than

ours as explained in Appendix A.3. ",stat.ML,B_centroid,0.3816645,-0.015119709,-0.045796912
http://arxiv.org/pdf/2202.02193v1,Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification,"We use a learning rate of 2.10‚àí3 divided by ten at                     learning optimization methods for other set-valued classi-
                                                                              Ô¨Åcation tasks, such as average size control or point-wise
epoch 20 and epoch 25. The batch size and weight decay are                    error control (Chzhen et al., 2021) are left for future work. set to 32 and 1.10‚àí4 respectively. ",stat.ML,C,-0.0006661308,-0.32160527,-0.115764126
http://arxiv.org/pdf/2202.02193v2,Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification,"Studying deep      Chzhen, E., Denis, C., Hebiri, M., and Lorieul, T. Set-
learning optimization methods for other set-valued classi-       valued classiÔ¨Åcation ‚Äì overview via a uniÔ¨Åed framework. Ô¨Åcation tasks, such as average size control or point-wise         arXiv preprint arXiv:2102.12318, 2021.
error control (Chzhen et al., 2021) are left for future work. Cole, E., Deneu, B., Lorieul, T., Servajean, M., Botella, C.,
Acknowledgements                                                  Morris, D., Jojic, N., Bonnet, P., and Joly, A. ",stat.ML,C,-0.018782297,-0.20165843,-0.14266738
http://arxiv.org/pdf/2202.02195v1,Deep End-to-end Causal Inference,"Thus, it is not applicable for DECI with the mixed-type and missing value extensions. We leave a more general theoretical guarantee to future work. B. ",stat.ML,B,0.13161412,0.155651,0.27180827
http://arxiv.org/pdf/2202.02195v2,Deep End-to-end Causal Inference,"Thus, it is not
applicable for DECI with the mixed-type and missing value extensions. We leave a more general
theoretical guarantee to future work. B Additional Details for DECI

B.1 Optimization Details for Causal Discovery

As mentioned in the main text, we gradually increase the values of œÅ and Œ± as optimization pro-
ceeds, so that non-DAGs are heavily penalized. ",stat.ML,B,-0.03004209,0.16107768,0.10499484
http://arxiv.org/pdf/2202.02407v1,An Experimental Design Approach for Regret Minimization in Logistic Bandits,"Developing
the Ô¨Åxed-design concentration inequality for such a regularized estimator with a tighter warmup condition dependence on d will

require different techniques from Jun et al. (2021, Theorem 1), and we leave it as future work. 0.60                                                                  0.06
              0.55
              0.50                                                                  0.04
              0.45
              0.40                                                                  0.02
              0.35
                                                                                    0.00                                         S=2
                    21 22 23 24 25 26 27
                                   N/(¬µÀô (S)‚àí1)                                  ‚àí0.02                                           S=4

                                         (a)                                     ‚àí0.04                                           S=6

                                                                                 ‚àí0.06                                           S=8

                                                                                 ‚àí0.08                                           S=10

                                                                                 ‚àí0.10                                           S=12

                                                                                          21 22 23 24 25 26 27

                                                                                                          N/(¬µÀô (S)‚àí1)

                                                                                                          (b)

Figure 2: Numerical veriÔ¨Åcation of the bias of (a) MLE and (b) KT estimator for Bernoulli(¬µ(S)) with N samples that behave
like 1 (N ¬µÀô (S)) and 1 (N ¬µÀô (S))2 respectively. ",stat.ML,B,0.34086683,0.044633448,-0.2713029
http://arxiv.org/pdf/2202.02474v1,Importance Weighting Approach in Kernel Bayes' Rule,"Fukumizu, K., Song, L., and Gretton, A. Kernel Bayes‚Äô
                                                                   rule: Bayesian inference with positive deÔ¨Ånite kernels. In future work, we suggest exploring diÔ¨Äerent den-            Journal of Machine Learning Research, 14(82):3753‚Äì
sity ratio estimation techniques for our setting. It is           3783, 2013.
well-known in the density ratio estimation context that
KuLSIF estimator may suÔ¨Äer from high variance. ",stat.ML,C,0.030959276,-0.07042601,-0.27979484
http://arxiv.org/pdf/2202.02474v2,Importance Weighting Approach in Kernel Bayes' Rule,"Fukumizu, K., Song, L., and Gretton, A. Kernel Bayes‚Äô
                                                                   rule: Bayesian inference with positive deÔ¨Ånite kernels. In future work, we suggest exploring diÔ¨Äerent den-            Journal of Machine Learning Research, 14(82):3753‚Äì
sity ratio estimation techniques for our setting. It is           3783, 2013.
well-known in the density ratio estimation context that
KuLSIF estimator may suÔ¨Äer from high variance. ",stat.ML,C,0.030959276,-0.07042601,-0.27979484
http://arxiv.org/pdf/2202.02474v3,Importance Weighting Approach in Kernel Bayes' Rule,"Journal of
ral net features, IW-KBR outperforms a neural sequence               Machine Learning Research, 13(25):723‚Äì773, 2012.
model in Ô¨Åltering problems with high-dimensional image
observations. Grunewalder, S., Lever, G., Baldassarre, L., Patterson, S.,
                                                                      Gretton, A., and Pontil, M. Conditional mean embeddings
In future work, we suggest exploring different density ratio          as regressors. In International Conference on Machine
estimation techniques for our setting. ",stat.ML,C,-0.19840187,-0.2540099,-0.04292843
http://arxiv.org/pdf/2202.02649v1,The Implicit Bias of Gradient Descent on Generalized Gated Linear Networks,"This may be because of Ô¨Ånite-time effects or because the fact that ReLU networks learn their weights and
gates in an entangled manner changes their inductive bias. We leave investigating this question (for example using the
results by Lyu and Li (2020)) to future work. 6 Discussion

In this article, we characterized the asymptotic behavior of gradient-descent training of Generalized Gated Linear
Networks. ",stat.ML,C,-0.054086473,-0.28808832,0.13846242
http://arxiv.org/pdf/2202.02651v1,Beyond Black Box Densities: Parameter Learning for the Deviated Components,"We believe that this work is the Ô¨Årst attempt in the eÔ¨Äort of understanding a broader class
of mixture models combining with black box models, and its interpretability. There is room for
future work building on top of this work. From a theoretical viewpoint, one can consider showing
minimax lower bounds for the learning rates of the deviating mixture model, or show uniform inverse
bounds for the model when Œª‚àó and G‚àó are considered as signals that will change with samples. ",stat.ML,C,-0.020377193,-0.11484665,-0.035214953
http://arxiv.org/pdf/2202.02651v2,Beyond Black Box Densities: Parameter Learning for the Deviated Components,"We believe that this work is the Ô¨Årst attempt in the effort of understanding a broader class of mixture
models combining with black box models, and interpreting the learned model parameters. There
is room for future work going forward. From a theoretical viewpoint, one may be interested in
establishing minimax lower bounds for the learning behavior of the deviating mixture model, or show
uniform inverse bounds for the model when Œª‚àó and G‚àó are considered as signals that will change with
samples. ",stat.ML,C,-0.026698029,-0.17231849,-0.05087705
http://arxiv.org/pdf/2202.02943v1,Learning fair representation with a parametric integral probability metric,"our theoretical results that the sigmoid IPM can control the
level of fairness well for a large class of prediction models. There are various directions of future works. Theoretically,
                                                                    the level of DP-fairness for diverse classes of functions
We conduct further downstream classiÔ¨Åcation tasks on                other than the RKHS with the RBF kernel would be worth
Health using Ô¨Åve auxiliary PCG labels, whose results are            pursuing. ",stat.ML,C,-0.16568148,-0.11091101,-0.09651069
http://arxiv.org/pdf/2202.02943v2,Learning fair representation with a parametric integral probability metric,"We demonstrated that our learning method is
competitive or better than other baselines, especially for unsupervised learning tasks, and is also numerically stable. There are various directions of future works. Theoretically, the level of DP-fairness for diverse classes of functions
other than the RKHS with the RBF kernel would be worth pursuing. ",stat.ML,C_centroid,-0.05468838,-0.29416355,-0.090540245
http://arxiv.org/pdf/2202.02943v3,Learning fair representation with a parametric integral probability metric,"implementations, see Appendix D.2. There are various directions for future works. Theoretically,
Table 2 shows that for a Ô¨Åxed prediction performance, the       the level of DP-fairness for diverse classes of functions
sIPM-LFR achieves lower levels of DP-fairness with large        other than the RKHS with the RBF kernel would be worth
margins on the both datasets. ",stat.ML,C,-0.016146373,-0.050253075,-0.09129563
http://arxiv.org/pdf/2202.03036v2,Structure-Aware Transformer for Graph Representation Learning,"Addi-
                                                                               0.16                                                       tionally, the SAT framework is Ô¨Çexible and can incorporate
                                                                                                                                          any structure extractor which produces structure-aware node
                                 0.25                       0.14                                                                          representations, and could even be extended beyond using
                                                                                                                                          GNNs, such as differentiable graph kernels. 0.20                       0.12
                                                                                                                                          Another important area for future work is to focus on re-
                                                            0.10                                                                          ducing the high memory cost and time complexity of the
                                                                                                                                          self-attention computation, as is being done in recent efforts
                                 0.15                       0.08                                                                          for developing a so-called linear transformer, which has
                                                                                                                                          linear complexity in both time and space requirements (Tay
                                 0.10                       0.06                                                                          et al., 2020; Wang et al., 2020; Qin et al., 2022). 0.05                       0.04                                                                          Acknowledgements

                                                            0.02                                                                          This work was supported in part by the Alfried Krupp Prize
                                                                                                                                          for Young University Teachers of the Alfried Krupp von
                                 0.00                       0.00                                                                          Bohlen und Halbach-Stiftung (K.B.). ",stat.ML,C,-0.16282924,-0.24721138,0.33688247
http://arxiv.org/pdf/2202.03036v3,Structure-Aware Transformer for Graph Representation Learning,"Addi-
                                                                               0.16                                                       tionally, the SAT framework is Ô¨Çexible and can incorporate
                                                                                                                                          any structure extractor which produces structure-aware node
                                 0.25                       0.14                                                                          representations, and could even be extended beyond using
                                                                                                                                          GNNs, such as differentiable graph kernels. 0.20                       0.12
                                                                                                                                          Another important area for future work is to focus on re-
                                                            0.10                                                                          ducing the high memory cost and time complexity of the
                                                                                                                                          self-attention computation, as is being done in recent efforts
                                 0.15                       0.08                                                                          for developing a so-called linear transformer, which has
                                                                                                                                          linear complexity in both time and space requirements (Tay
                                 0.10                       0.06                                                                          et al., 2020; Wang et al., 2020; Qin et al., 2022). 0.05                       0.04                                                                          Acknowledgements

                                                            0.02                                                                          This work was supported in part by the Alfried Krupp Prize
                                                                                                                                          for Young University Teachers of the Alfried Krupp von
                                 0.00                       0.00                                                                          Bohlen und Halbach-Stiftung (K.B.). ",stat.ML,C,-0.16282924,-0.24721138,0.33688247
http://arxiv.org/pdf/2202.03101v1,NUQ: Nonparametric Uncertainty Quantification for Deterministic Neural Networks,"OpenReview.net,
certainty quantiÔ¨Åcation methods for deterministic neural         2017.
networks. We also believe that NUQ is suitable for in-depth
theoretical investigation, which we defer to future work. El-Yaniv, R. and Wiener, Y. ",stat.ML,C,-0.09801,-0.062143058,-0.15697071
http://arxiv.org/pdf/2202.03101v2,Nonparametric Uncertainty Quantification for Single Deterministic Neural Network,"We hope that our work opens a new perspective on uncertainty quantiÔ¨Åcation methods for deterministic
neural networks. We also believe that NUQ is suitable for in-depth theoretical investigation, which
we defer to future work. Acknowledgements. ",stat.ML,B,0.04462996,-0.0987819,-0.052733265
http://arxiv.org/pdf/2202.03165v2,SLIDE: a surrogate fairness constraint to ensure fairness consistency,"Specially designed surrogate fair-
ness constraints for the purpose of evaluation of the fairness   where ( , ) ‚à∂= { ‚àà Óà≤ ‚à∂ || ‚àí || ‚â§ }. would be necessary, which we leave as future work. DeÔ¨Ånition 4 (Metric entropy). ",stat.ML,B,0.11214396,0.04475312,-0.013416495
http://arxiv.org/pdf/2202.03223v1,SODA: Self-organizing data augmentation in deep neural networks -- Application to biomedical image segmentation tasks,"the                    0.875
uniform policy but at a faster rate, which may be helpful for
reducing ML carbon impact. 0.850

   In future works, we wish to generalize SODA to situations                    0.825
where there is no relevant data augmentation to choose from. While SODA exhibits improved robustness, we also believe                        0.800 0          10  20  30               40  50
the action-loss signals could be further processed in order to                                           Epochs
discard irrelevant data augmentation types more easily. ",stat.ML,A,-0.15526369,-0.02478623,-0.06767258
http://arxiv.org/pdf/2202.03297v1,Grassmann Stein Variational Gradient Descent,"In Section 6 we present some
Wang (2016), we consider the Bayesian logistic                                                       heuristics for tuning GSVGD hyperparameters that
regression model applied to the Covertype dataset                                                    we found yielded consistent results, but we would
(Asuncion and Newman, 2007). We use HMC posterior                                                    recommend future work to study the sensitivity of
samples as a gold standard for evaluating the posterior                                              the hyperparameters. approximations. ",stat.ML,A,-0.1832634,0.024670936,-0.2513163
http://arxiv.org/pdf/2202.03297v2,Grassmann Stein Variational Gradient Descent,"We use HMC posterior                                                                                heuristics for tuning GSVGD hyperparameters that
samples as a gold standard for evaluating the posterior                                                                          we found yielded consistent results, but we would
approximations. We ran all methods on a subset                                                                                   recommend future work to study the sensitivity of
consisting 1,000 randomly selected data points 10                                                                                the hyperparameters. times. ",stat.ML,A,-0.17780724,-0.012847015,-0.23726222
http://arxiv.org/pdf/2202.03813v1,Learning to Predict Graphs with Fused Gromov-Wasserstein Barycenters,"We deÔ¨Åne an asymmetric partially relaxed structured
loss function ‚àÜ : Zn √ó Y ‚Üí R+. Given a Ô¨Ånite sample                  future work. (xi, yi)Ni=1 independently drawn from an unknown distribu-
tion œÅ on X √ó Y, we consider the problem of estimating a             Structured prediction model. ",stat.ML,B,0.070025146,-0.12604693,-0.075325325
http://arxiv.org/pdf/2202.03813v2,Learning to Predict Graphs with Fused Gromov-Wasserstein Barycenters,"Hence, in principle,

we should also tackle the problem of Ô¨Ånding the best value nÀú that allows to come closer to the

solution in Y in expectation. We leave this bilevel optimization problem as future work. Structured prediction model. ",stat.ML,B,-0.007144789,-0.0399729,-0.06594783
http://arxiv.org/pdf/2202.03926v1,Distribution Regression with Sliced Wasserstein Kernels,"This analysis was beyond
the scope of this work, which focuses on comparing OT and MMD based distribution regression. However, we
plan to account for this further (third) sampling stage in future work. 12
References

Nachman Aronszajn. ",stat.ML,A,0.093193755,0.17573395,-0.28524268
http://arxiv.org/pdf/2202.03926v2,Distribution Regression with Sliced Wasserstein Kernels,"This analysis was
                                                                        beyond the scope of this work, which focuses on comparing
                                                                        OT and MMD based distribution regression. However, we
                                                                        plan to account for this further (third) sampling stage in
                                                                        future work. Figure 2. ",stat.ML,A,0.052875645,0.22762878,-0.273386
http://arxiv.org/pdf/2202.04219v1,Improving Computational Complexity in Statistical Models with Second-Order Information,"For the inhomogeneous population loss function, to the best of our knowledge, the
theories for these settings are only for speciÔ¨Åc statistical models [6, 27]. The general theory for
these settings is challenging and hence we leave this direction for future work. To simplify the
ensuing presentation, we denote the NormGD iterates for solving the samples and population
losses functions (1) and (2) as follows:

Œ∏t+1 := Fn(Œ∏t ) = Œ∏t ‚àí        Œ∑        ‚àáfn(Œ∏t ),

n  n  n Œªmax(‚àá2fn(Œ∏nt ))                         n

Œ∏t+1 := F (Œ∏t) = Œ∏t ‚àí      Œ∑           ‚àáf (Œ∏t). ",stat.ML,B,0.27271903,0.026491748,-0.122933045
http://arxiv.org/pdf/2202.04219v2,Improving Computational Complexity in Statistical Models with Second-Order Information,"For the inhomogeneous population loss function, to the best of our knowledge, the
theories for these settings are only for speciÔ¨Åc statistical models [6, 26]. The general theory for
these settings is challenging and hence we leave this direction for future work. To simplify the
ensuing presentation, we denote the NormGD iterates for solving the samples and population
losses functions (1) and (2) as follows:

Œ∏t+1 := Fn(Œ∏t ) = Œ∏t ‚àí        Œ∑        ‚àáfn(Œ∏t ),

n  n  n Œªmax(‚àá2fn(Œ∏nt ))                         n

Œ∏t+1 := F (Œ∏t) = Œ∏t ‚àí      Œ∑           ‚àáf (Œ∏t). ",stat.ML,B,0.27284676,0.028732704,-0.12206151
http://arxiv.org/pdf/2202.04219v3,Improving Computational Complexity in Statistical Models with Second-Order Information,"For the inhomogeneous population loss function, to the best of our knowledge, the
theories for these settings are only for speciÔ¨Åc statistical models [6, 26]. The general theory for
these settings is challenging and hence we leave this direction for future work. To simplify the
ensuing presentation, we denote the NormGD iterates for solving the samples and population
losses functions (1) and (2) as follows:

Œ∏t+1 := Fn(Œ∏t ) = Œ∏t ‚àí        Œ∑        ‚àáfn(Œ∏t ),

n  n  n Œªmax(‚àá2fn(Œ∏nt ))                         n

Œ∏t+1 := F (Œ∏t) = Œ∏t ‚àí      Œ∑           ‚àáf (Œ∏t). ",stat.ML,B,0.27284658,0.028732706,-0.12206149
http://arxiv.org/pdf/2202.04258v1,A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn Uncertainty Sets,"The technical difÔ¨Åculty is due to the inÔ¨Ånite
                                                                                                                         11

problem size of (11) so that discussions on properties of sample approximation estimators in
[24, Section 5.1] do not apply. We hope to address this issue in future works. Since the importance ratio i,k is supported on supp(GÀÜ i,Œµ), which consists of 2m points, the
LFDs P‚àó0 and P‚àó1 from (13) will have the common support, consisting of 2mn points. ",stat.ML,B,0.31401917,0.08463995,-0.1850363
http://arxiv.org/pdf/2202.04258v2,A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn Uncertainty Sets,"The technical difÔ¨Åculty is due to the inÔ¨Ånite
                                                                                                                         11

problem size of (11) so that discussions on properties of sample approximation estimators in
[24, Section 5.1] do not apply. We hope to address this issue in future works. Since the importance ratio i,k is supported on supp(GÀÜ i,Œµ), which consists of 2m points, the
LFDs P‚àó0 and P‚àó1 from (13) will have the common support, consisting of 2mn points. ",stat.ML,B,0.31401917,0.08463995,-0.1850363
http://arxiv.org/pdf/2202.04258v3,A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn Uncertainty Sets,"The technical difÔ¨Åculty is due to the inÔ¨Ånite
                                                                                                                         11

problem size of (11) so that discussions on properties of sample approximation estimators in
[26, Section 5.1] do not apply. We hope to address this issue in future works. Since the importance ratio i,k is supported on supp(GÀÜ i,Œµ), which consists of 2m points, the
LFDs P‚àó0 and P‚àó1 from (13) will have the common support, consisting of 2mn points. ",stat.ML,B,0.31784612,0.08539907,-0.1847864
http://arxiv.org/pdf/2202.04359v1,Cost-effective Framework for Gradual Domain Adaptation with Multifidelity,"The effectiveness of the proposed method was evaluated on
one artiÔ¨Åcial and four real-world datasets, and it was shown that GDAMF provides reasonable
performance under the condition that access to the intermediate domain is restricted and the
query cost should be considered. For future works, we consider it is important to make GDAMF more scalable. By doing
so, we expect reducing the number of samples required for convergence of the network, and
enable more efÔ¨Åcient active learning without introducing the mini-model. ",stat.ML,C,-0.2977351,-0.16239196,-0.11407901
http://arxiv.org/pdf/2202.04359v2,Cost-effective Framework for Gradual Domain Adaptation with Multifidelity,"Even if there is a domain with low uncertainty, the
algorithm will distribute a budget to this domain. For future works, it is of great importance to
theoretically analyze and give a guarantee for the effectiveness of the use of data in intermediate
domains. 7 Acknowledgments

H.H. ",stat.ML,A,-0.172275,0.17380652,-0.1229415
http://arxiv.org/pdf/2202.04719v2,Multivariate Analysis for Multiple Network Data via Semi-Symmetric Tensor PCA,"As he notes, subsequent deÔ¨Çation, in particular is
useful for encouraging approximate orthogonality of greedly estimated factors: if the target of the
next SS-TPCA iteration is orthogonal to previous V , the next estimated V terms, which explain
that target, is are also likely to be orthogonal. Conversely, we have also noted that imposing these
additional restrictions may propose unexpected estimation diÔ¨Éculties, most notably decreasing

                                          30
cumulative percent of variance explained CVPEk =  Xk  2  /  X  2  :  we  speculate  this  may  be  a  useful
                                                      F        F

diagnostic when selecting the number of factors, K, but these issues are subtle and best left for

future work. Proof of Theorem A.1. ",stat.ML,B,0.15406008,0.074485675,0.09714387
http://arxiv.org/pdf/2202.04719v3,Multivariate Analysis for Multiple Network Data via Semi-Symmetric Tensor PCA,"As he notes, subsequent deÔ¨Çation, in particular is
useful for encouraging approximate orthogonality of greedily estimated factors: if the target of the
next SS-TPCA iteration is orthogonal to previous V , the next estimated V terms, which explain
that target, are also likely to be nearly orthogonal. Conversely, we have also noted that imposing
these additional restrictions may propose unexpected estimation diÔ¨Éculties, most notably decreasing

                                          30
cumulative percent of variance explained CVPEk =  Xk  2  /  X  2  :  we  speculate  this  may  be  a  useful
                                                      F        F

diagnostic when selecting the number of factors, K, but these issues are subtle and best left for

future work. Proof of Theorem A.1. ",stat.ML,B,0.15436223,0.077118255,0.09258607
http://arxiv.org/pdf/2202.04828v1,Learning Latent Causal Dynamics,"& Plis, 2013; Gong et al., 2017). Extending our theories and
framework to address the issue of instantaneous dependency      Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,
or instantaneous causal relations in the latent structure will     Botvinick, M., Mohamed, S., and Lerchner, A. beta-
be one line of our future work. vae: Learning basic visual concepts with a constrained
                                                                  variational framework. ",stat.ML,C,-0.17918232,0.10078633,0.33736473
http://arxiv.org/pdf/2202.04828v2,Learning Latent Causal Dynamics,"& Plis, 2013; Gong et al., 2017). Extending our theories and
framework to address the issue of instantaneous dependency      Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,
or instantaneous causal relations in the latent structure will     Botvinick, M., Mohamed, S., and Lerchner, A. beta-
be one line of our future work. vae: Learning basic visual concepts with a constrained
                                                                  variational framework. ",stat.ML,C,-0.17918232,0.10078633,0.33736473
http://arxiv.org/pdf/2202.04828v3,Learning Latent Causal Dynamics,"In Fig. 6, the
latent causal processes are recovered, as seen from (a) high
MCC for the latent causal processes; (b) the latent factors
are estimated up to componentwise transformation; (c) the
latent traversals conÔ¨Årm that the two latent causal variables
Learning Latent Causal Dynamics

be one line of our future work. Huang, B., Feng, F., Lu, C., Magliacane, S., and Zhang,
                                                                        K. Adarl: What, where, and how to adapt in transfer
References                                                              reinforcement learning. ",stat.ML,C,-0.26133075,0.11175306,0.13555914
http://arxiv.org/pdf/2202.04828v4,Learning Latent Causal Dynamics,"Extending our theories and            Botvinick, M., Mohamed, S., and Lerchner, A. beta-
framework to address the issue of instantaneous dependency             vae: Learning basic visual concepts with a constrained
or instantaneous causal relations in the latent structure will         variational framework. 2016.
be one line of our future work. Huang, B., Zhang, K., Zhang, J., Ramsey, J. D., Sanchez-
References                                                              Romero, R., Glymour, C., and Scho¬®lkopf, B. Causal dis-
                                                                        covery from heterogeneous/nonstationary data. ",stat.ML,C,-0.23230974,0.11181809,0.2717829
http://arxiv.org/pdf/2202.04837v1,Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization,"As such, calibration can also be seen as an interpolation between DNNs and gradient boosted trees. The optimality
analysis should also extend similarly when we instead consider this sequence of partition, but we leave the full details
to future work. 14 More Experimental results

Size Model  Bank Marketing Census data Credit Default        Higgs data       Diabetes
                                                             0.7801           0.6693
S Reg DNN 0.7758  0.8976          0.7781                     0.7816 (+0.19%)  0.6829 (+2.04%)
                                                             0.7498           0.6915
Reg HC      0.7816 (+0.76%) 0.9021 (+0.50%) 0.7793 (+0.16%)  0.7588 (+1.20%)  0.6937 (+0.32%)

Unreg DNN 0.7735  0.8773          0.7768

Unreg HC 0.7804 (+0.88%) 0.8985 (+2.42%) 0.7787 (+0.25%)

M Reg DNN 0.7712  0.8978          0.7787                     0.7773           0.6951

Reg HC      0.7800 (+1.14%) 0.9027 (+0.55%) 0.7794 (+0.09%) 0.7799 (+0.33%) 0.6832 (+3.65%)

Unreg DNN 0.7676  0.8644          0.7768                     0.7477           0.6744

Unreg HC 0.7770 (+1.22%) 0.8994 (+4.05%) 0.7786 (+0.23%) 0.7586 (+1.45%) 0.6856 (+1.66%)

L Reg DNN 0.7707  0.9007          0.7782                     0.7747           0.6567

Reg HC      0.7809 (+1.32%) 0.9027 (+0.22%) 0.7795 (+0.17%) 0.7775 (+0.36%) 0.6824 (+3.91%)

Unreg DNN 0.7642  0.8437          0.7771                     0.7487           0.6679

Unreg HC 0.7770 (+1.67%) 0.8980 (+6.44%) 0.7787n(+0.21%) 0.7595 (+1.44%) 0.6824 (+2.17%)

Table 3: Test e ect of regularization on AUC-ROC (mean of 5 runs) on di erent datasets before and after calibration. ",stat.ML,A,-0.17257465,0.006972874,-0.0378345
http://arxiv.org/pdf/2202.04862v1,Settling the Communication Complexity for Distributed Offline Reinforcement Learning,"Our method relies on constructing hard instances and converting them to statistical inference prob-
lems. This approach can be easily applied to other settings such as state-action value estimation
or off-policy learning, which constitutes one of the directions of future work. We also identiÔ¨Åed a
weakness of TD learning resulting from its initial bias. ",stat.ML,C,-0.20241137,-0.024417922,-0.22768113
http://arxiv.org/pdf/2202.04985v1,Generalization Bounds via Convex Analysis,"5. Conclusion

We discuss some implications and potential directions for future work below. 10
GENERALIZATION BOUNDS VIA CONVEX ANALYSIS

High-probability bounds. ",stat.ML,B,0.30900735,0.06582245,-0.12617975
http://arxiv.org/pdf/2202.04985v2,Generalization Bounds via Convex Analysis,"5. Conclusion

We discuss some implications and potential directions for future work below. Connection with online learning. ",stat.ML,A,-0.11246374,0.19253665,0.16518587
http://arxiv.org/pdf/2202.04985v3,Generalization Bounds via Convex Analysis,"5. Conclusion

We discuss some implications and potential directions for future work below. High-probability bounds. ",stat.ML,B,0.25079358,0.26495242,-0.08004626
http://arxiv.org/pdf/2202.05069v1,Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case,"This aspect can be explored further
gain is mainly positive and is larger when nT is small. In         in future works. A possible extension would be to relax
these cases, the size of the target dataset is too small and the   the assumption to multi-modal data and Ô¨Åt a mixture of
variance of the basic estimator is the highest, so combining       linear regression models using the expectation maximization
the source dataset using our transfer-learning approach is         algorithm. ",stat.ML,C,-0.25218916,-0.061743513,-0.13751523
http://arxiv.org/pdf/2202.05100v1,Adaptively Exploiting d-Separators with Causal Bandits,"We proved
that it is impossible to optimally adapt to this property, but provided a new algorithm (HAC-
UCB) that simultaneously recovers the improved regret for conditionally benign environments
and signiÔ¨Åcantly improves on prior work when the conditionally benign property does not hold;
crucially, our algorithm requires no prior knowledge of the conditionally benign property. We
expect our results to spur future work on (a) improved adaptation to the conditionally benign
property, (b) relaxations of the conditionally benign property for which optimal adaptation is
possible, and (c) deeper study of adaptation to causal structure in general. Acknowledgements

BB is supported by an NSERC Canada Graduate Scholarship and the Vector Institute. ",stat.ML,A,-0.18427935,0.16360047,0.0012001283
http://arxiv.org/pdf/2202.05100v2,Adaptively Exploiting d-Separators with Causal Bandits,"We proved that it is impossible
to optimally adapt to this property, but provided a new algorithm (HAC-UCB) that simultaneously
recovers the improved regret for conditionally benign environments and signiÔ¨Åcantly improves
on prior work when the conditionally benign property does not hold; crucially, our algorithm
requires no more assumptions about the world than vanilla bandit algorithms. We expect our
results to spur future work on (a) improved adaptation to the conditionally benign property, (b)
relaxations of the conditionally benign property for which optimal adaptation is possible, and (c)
deeper study of adaptation to causal structure in general. Acknowledgements

BB is supported by an NSERC Canada Graduate Scholarship and the Vector Institute. ",stat.ML,A,-0.1669519,0.067986235,-0.14722097
http://arxiv.org/pdf/2202.05100v3,Adaptively Exploiting d-Separators with Causal Bandits,"Here, the post-action
context could be gene expressions that are sometimes assumed to be a d-separator (e.g., [3]). We leave the
implementation of our algorithm in clinical settings and collaboration with practitioners for future work. Acknowledgements

BB is supported by an NSERC Canada Graduate Scholarship and the Vector Institute. ",stat.ML,C,-0.25339332,0.11780459,0.22694454
http://arxiv.org/pdf/2202.05193v1,Bayes Optimal Algorithm is Suboptimal in Frequentist Best Arm Identification,"While we demonstrated a striking diÔ¨Äerence between the Bayesian and frequentist algorithms, our
results are speciÔ¨Åc to Gaussian arms and three-armed instances. The following lists the future works that
can generalize our results. ‚Ä¢ Probablity of Error (PoE): It should not be very diÔ¨Écult to show similar results for a Bayes
       optimal algorithm that minimizes the Bayesian PoE. ",stat.ML,A,-0.04210614,0.07981692,-0.2665431
http://arxiv.org/pdf/2202.05193v2,Suboptimal Performance of the Bayes Optimal Algorithm in Frequentist Best Arm Identification,"This paper demonstrates that Bayes
                                                  optimal algorithm, which minimizes the Bayesian simple regret, does not produce an exponential simple
                                                  regret for some parameters, a Ô¨Ånding that contrasts with the many results indicating the asymptotic
                                                  equivalence of Bayesian and frequentist algorithms in the context of Ô¨Åxed sampling regimes. While the
                                                  Bayes optimal algorithm is described in terms of a recursive equation that is virtually impossible to
                                                  compute exactly, we establish the foundations for further analysis by introducing a key quantity that
                                                  we call the expected Bellman improvement. Quote

                                          The following paragraph appears in a conversation between Joseph Leo Doob and James Laurie Snell
                                          [Snell, 1997]. ",stat.ML,A,0.048665598,0.09705474,-0.36656654
http://arxiv.org/pdf/2202.05250v1,Adaptive and Robust Multi-task Learning,"We also veriÔ¨Åed the theories by extensive nu-
merical experiments. We hope that our framework can spur further research on integrative data analysis. It
would be interesting to extend our methods to high-dimensional problems with sparsity or other structures,
and develop inferential tools for uncertainty quantiÔ¨Åcation. ",stat.ML,A,-0.024333166,0.09549341,-0.018369574
http://arxiv.org/pdf/2202.05250v2,Adaptive and Robust Multi-task Learning,"We hope that our framework can spur

                         23
Table 1: Testing errors of ARMUL and benchmarks on the HAR dataset. ARMUL                           Benchmarks

Vanilla Clustered Low-rank  STL Data pooling Clustered Low-rank

1.12 (0.25) 0.84 (0.22) 0.80 (0.19) 1.95 (0.32) 3.48 (0.39) 2.15 (0.33) 1.30 (0.23)

further research on integrative data analysis. It would be interesting to extend our methods to
high-dimensional problems with sparsity or other structures, and develop inferential tools for uncer-
tainty quantiÔ¨Åcation. ",stat.ML,A,-0.21483932,0.081965,0.083575785
http://arxiv.org/pdf/2202.05318v1,Personalization Improves Privacy-Accuracy Tradeoffs in Federated Optimization,"We note
that despite its practical success, the known theoretical guarantees of local SGD typically do not improve
on mini-batch SGD except for speciÔ¨Åc settings (Woodworth et al., 2020). Our study therefore focuses on
understanding personalization in the SGD setting, but we note that extending our analysis to local SGD is
an interesting direction for future work. 7
Algorithm 2 PPSGD with client sampling and heterogeneous minibatch sizes

1: Input: q: client sampling probability,

     mi: minibatch sizes, Œ∑: step size,

     Œ±: global/local ratio, œÉŒ∂: privacy noise level,

     C: clipping parameter. ",stat.ML,C,-0.00283714,-0.16584662,-0.13670494
http://arxiv.org/pdf/2202.05318v2,Personalization Improves Privacy-Accuracy Tradeoffs in Federated Learning,"We note that despite its practical success,                                           k=1
the known theoretical guarantees of local SGD typically
do not improve on mini-batch SGD except for speciÔ¨Åc set-        8:      Update       Œ∏i,t  =  Œ∏i,t‚àí1  ‚àí   Œ∑   gŒ∏t ,i
tings (Woodworth et al., 2020). Our study therefore focuses                                              qM
on understanding personalization in the SGD setting, but                Clip gradient gÀút = gt /max(1, gwt ,i )
we note that extending our analysis to local SGD is an inter-   9:                            w,i        w,i              C
esting direction for future work. 10:     Send gÀúwt ,i to the server
5. ",stat.ML,C,0.0020562913,-0.17498265,-0.022877514
http://arxiv.org/pdf/2202.05568v1,On change of measure inequalities for $f$-divergences,"(2016). We leave for future work the

optimisation on Œª (Dalalyan and Tsybakov, 2012 provide some preliminary heuristics). For  power  p  divergences  with  p  >  1,  only  moments     of  order    p  =   q   for  h  are  needed  rather
                                                                         p‚àí1
than exponential moments, considerably lessening the assumptions needed on the loss l and the

underlying data distribution. ",stat.ML,B,0.3016333,-0.049791016,-0.1394583
http://arxiv.org/pdf/2202.05750v1,Bounded nonlinear forecasts of partially observed geophysical systems with physics-constrained deep learning,"In all numerical experiments, both the data-driven model formulation in an augmented space and the boundedness
constraints are key features to capture the dynamics underlying the observations. An avenue of promising future work will involve extending the proposed framework to other non-linear models through
the generalization of the boundedness constraints. Foremost, polynomial models could be considered by imposing,
similar to the quadratic terms of the proposed architecture, an energy-preserving non-linearity. ",stat.ML,B,0.015780274,-0.089986205,0.14474499
http://arxiv.org/pdf/2202.05750v2,Bounded nonlinear forecasts of partially observed geophysical systems with physics-constrained deep learning,"The light-color intervals refer to the standard deviation. An avenue of promising future work will involve extending the proposed framework to other non-linear models through
the generalization of the boundedness constraints. Foremost, polynomial models could be considered by imposing,
similar to the quadratic terms of the proposed architecture, an energy-preserving non-linearity. ",stat.ML,B,0.12795208,0.05389533,0.15600562
http://arxiv.org/pdf/2202.06742v1,Trace norm regularization for multi-task learning with scarce data,"In light of the experiments, the proposed bounds might be improvable, especially in the subspace esti-
mation. Such an improvement is left open for future work and would require more reÔ¨Åned analytical tools. 12
References

Amit, Y., M. Fink, N. Srebro, and S. Ullman (2007). ",stat.ML,B,0.39910406,0.0051807985,-0.009731175
http://arxiv.org/pdf/2202.06742v2,Trace norm regularization for multi-task learning with scarce data,"In light of the experiments, the proposed bounds might be improvable, especially in the subspace
estimation. Such an improvement is left open for future work and would require more reÔ¨Åned
analytical tools. Acknowledgements

We warmly thank Nilesh Tripuraneni and Alexandre Tsybakov for their time and precious feedback. ",stat.ML,B,0.36217517,-0.06883332,0.07712238
http://arxiv.org/pdf/2202.06891v2,Counterfactual inference for sequential experiments,"3.1 that holds conditional on the latent time factors, with Œ£(va)
and eŒ¥ replaced by B and g(N, T, Œ¥) in all relevant terms and deÔ¨Ånitions. However, it is non-
trivial to identify simple conditions on the sequential policy that would imply (35), and we

leave it for future work. Finally, extending our theory to settings with latent factors that vary

over time with the sequence of assigned treatments is another interesting future direction. ",stat.ML,B,0.08125908,0.29360434,0.07938361
http://arxiv.org/pdf/2202.06930v1,Tensor Moments of Gaussian Mixture Models: Theory and Applications,"On the computational side, much
more investigation is needed into the robustness and reliability of the method and its
comparison to EM on a range of problems. We have also deferred the full algorithm
for general covariances to future work since the details are quite lengthy (though the
main formulas are present in Theorems 4.2 and 4.3). We may also wish to consider
practical issues such as centering and scaling the data before applying the method. ",stat.ML,B,0.16220844,-0.082469046,-0.13107133
http://arxiv.org/pdf/2202.06930v2,Tensor Moments of Gaussian Mixture Models: Theory and Applications,"On the computational side, much
more investigation is needed into the robustness and reliability of the method and its
comparison to EM on a range of problems. We have also deferred the full algorithm
for general covariances to future work since the details are quite lengthy (though the
main formulas are present in Theorems 4.2 and 4.3). We may also wish to consider
practical issues such as centering and scaling the data before applying the method. ",stat.ML,B,0.16220844,-0.082469046,-0.13107133
http://arxiv.org/pdf/2202.07079v1,Synthetically Controlled Bandits,"Interventions over Consecutive Epochs: For some interventions, it may be practically necessary
(for instance, from a consumer experience standpoint) that any intervention be maintained over

a certain minimum number of consecutive epochs. We believe this to be an important area for

future work, closely related to notions of switching costs and batching in the bandit literature. A

number of flavors of this problem have been considered in recent years, including incorporating

switching costs [Dekel et al., 2014], and batching that makes the decision to stop using a potential

intervention irrevocable [Perchet et al., 2016]. ",stat.ML,A,-0.16860524,0.15183854,-0.13642077
http://arxiv.org/pdf/2202.07194v1,One-bit Submission for Locally Private Quasi-MLE: Its Asymptotic Normality and Limitation,"In the context of LDP, vector submission is studied by many re-
searchers e.g., [Duchi et al., 2013, Erlingsson et al., 2014, Bassily and Smith, 2015, Wang et al.,
2019b]. Better selection of cl and cu is another future work. Whether certain cl and cu are good or
bad strongly depends on F , and we have no general strategy to select better cl and cu. ",stat.ML,C,-0.13768038,0.016172137,0.03793398
http://arxiv.org/pdf/2202.07356v1,Realistic Counterfactual Explanations by Learned Relations,"The counterfactual reÔ¨Çects the real world and can be a good reference to suggest a patient with diabetes to be
healthy. While we found age and pregnancies sometimes decrease, we will leave it to future work and constrain the
predictors so that such attribute should either stay constant or could only change in one direction. 12
                   Realistic Counterfactual Explanations                               A PREPRINT

                Table 5: Examples of explanations with our method on Diabete dataset. ",stat.ML,A,-0.24191603,0.37572044,0.08716953
http://arxiv.org/pdf/2202.07356v2,Realistic Counterfactual Explanations by Learned Relations,"The generated counterfactuals preserve the relations that exist in the data and can be a good reference to suggest
a patient what needs to be changed to become healthy. While we found age and pregnancies sometimes decrease, we
will leave it to future work and constrain the predictors so that such attribute should either stay constant or could only
change in one direction. 5 Conclusion and Future Work

In this paper, we propose a novel method for generating counterfactuals that preserves the relationships between
variables learned from data. ",stat.ML,A,-0.39184818,0.318326,0.13574882
http://arxiv.org/pdf/2202.07356v3,Realistic Counterfactual Explanations with Learned Relations,"The generated counterfactuals preserve the relations that exist in the data and can be a good reference to suggest
a patient what needs to be changed to become healthy. While we found age and pregnancies sometimes decrease, we
will leave it to future work and constrain the changes so that such attributes should either stay constant or could only
change in one direction. 5 Conclusion and Future Work

In this paper, we proposed a novel method for generating counterfactuals that preserves the relationships between
variables learned from data. ",stat.ML,A,-0.39132726,0.35008454,0.15082881
http://arxiv.org/pdf/2202.07365v1,A Statistical Learning View of Simple Kriging,"D.2. Future Lines of Research

The goal of this paper is to explain the main ideas to achieve generalization guarantees in the spatial context: to this purpose,
we use several simplifying technical assumptions and discuss here possible extensions that will be studied in a future work. As highlighted, Ô¨Årst in the proofs of the main results and later in the Numerical Experiments (see Appendix C and Section 4),
even in the simplest framework, the analysis is far from straightforward. ",stat.ML,B,0.35597908,0.002740167,0.1455466
http://arxiv.org/pdf/2202.07365v2,A Statistical Learning View of Simple Kriging,"Regarding these last points, even if nonparametric approaches have already been documented,
the nonasymptotic perspective embraced in the Ô¨Ånite-sample analyses we have carried out in this
paper, generates diÔ¨Éculties that had not been addressed before. D.2 Future Lines of Research

The goal of this paper is to explain the main ideas to achieve generalization guarantees in the
spatial context: to this purpose, we use several simplifying technical assumptions and discuss here
possible extensions that will be studied in a future work. As highlighted, Ô¨Årst in the proofs of the main results and later in the Numerical Experiments (see
Appendix C and Section 4), even in the simplest framework, the analysis is far from straightforward. ",stat.ML,B,0.38955325,0.11033973,-0.044937525
http://arxiv.org/pdf/2202.07365v3,A Statistical Learning View of Simple Kriging,"Regarding these last points, even if nonparametric approaches have already been documented,
the nonasymptotic perspective embraced in the Ô¨Ånite-sample analyses we have carried out in this
paper, generates diÔ¨Éculties that had not been addressed before. D.2 Future Lines of Research

The goal of this paper is to explain the main ideas to achieve generalization guarantees in the
spatial context: to this purpose, we use several simplifying technical assumptions and discuss here
possible extensions that will be studied in a future work. As highlighted, Ô¨Årst in the proofs of the main results and later in the Numerical Experiments (see
Appendix C and Section 4), even in the simplest framework, the analysis is far from straightforward. ",stat.ML,B,0.38955325,0.11033973,-0.044937525
http://arxiv.org/pdf/2202.07477v1,Understanding DDPM Latent Codes Through Optimal Transport,"Perhaps, a more elaborate dif-
fusion process leading to feature‚Äìbased cost may be con-          Choi, J., Kim, S., Jeong, Y., Gwon, Y., and Yoon, S. ILVR: Con-
structed. We leave this analysis to future work. ditioning method for denoising diffusion probabilistic models. ",stat.ML,C,-0.06670681,-0.083956584,-0.03229495
http://arxiv.org/pdf/2202.07477v2,Understanding DDPM Latent Codes Through Optimal Transport,"Perhaps,
a more elaborate diffusion process leading to feature-based cost may be constructed. We leave this
analysis to future work. 9
REFERENCES

Jason Altschuler, Jonathan Niles-Weed, and Philippe Rigollet. ",stat.ML,C,-0.02782733,-0.04899933,0.027016059
http://arxiv.org/pdf/2202.07773v1,The efficacy and generalizability of conditional GANs for posterior inference in physics-based inverse problems,"Further, it worth exploring the beneÔ¨Åts of embedding
the underlying physics into the network architecture, or the objective function. These will be the focus of future work. Acknowledgments

The authors acknowledge support from ARO grant W911NF2010050, NRL grant N00173-19-P-1119, and AIER
(USC), and resources from the Center for Advanced Research Computing (USC). ",stat.ML,C,-0.15789434,-0.16134642,0.28608316
http://arxiv.org/pdf/2202.07773v2,The efficacy and generalizability of conditional GANs for posterior inference in physics-based inverse problems,"Further, it worth exploring the beneÔ¨Åts of embedding
the underlying physics into the network architecture, or the objective function. These will be the focus of future work. Acknowledgments

The authors acknowledge support from ARO grant W911NF2010050, NRL grant N00173-19-P-1119, and AIER
(USC), and resources from the Center for Advanced Research Computing (USC). ",stat.ML,C,-0.15789434,-0.16134642,0.28608316
http://arxiv.org/pdf/2202.07965v1,GAN Estimation of Lipschitz Optimal Transport Maps,"‚Üí 0.

                                               n‚Üí+‚àû

    To the best of our knowledge, this is the Ô¨Årst statistical consistency result for a neural-network-based
optimal transport map. We leave the analysis of consistency rates for future work. In particular, we could
obtain sharper results by imposing conditions on the parameter n which characterizes the rate at which the
discriminators Dn approximate the 1-Lipschitz potentials, and by leveraging stronger regularity assumptions
on T0. ",stat.ML,B,0.17063826,-0.21636596,0.0068395683
http://arxiv.org/pdf/2202.08064v1,Learning a Single Neuron for Non-monotonic Activation Functions,"the Hermite coeÔ¨Écients are aÔ¨Äected by the dilation of

œÉ. We leave this to future work. 4.2.1 Convergence with Constant Probability                                                Optimality. ",stat.ML,B,0.47941893,-0.0071732327,-0.029136052
http://arxiv.org/pdf/2202.08236v1,Using the left Gram matrix to cluster high dimensional data,"In ongoing work, we are developing scalable
     techniques that will facilitate the use of our proposed algorithm in large N and large P settings. In
     future work, we will explore the utility of the proposed transformation on other similarity measures
     for meaningful clustering of unstructured data, like images or text documents. 1Our proposed algorithm has been implemented as RJcluster in CRAN as a R package

                                                       8
130 Acknowledgments

         We thank Anirban Bhattacharya and Irina Gaynavova for their helpful comments and discus-
     sions, Marina Romanyuk for checking the computation times, and Rachael Shudde for maintaining
     the R package. ",stat.ML,C,-0.0935362,-0.071478404,0.17179948
http://arxiv.org/pdf/2202.08876v1,Training neural networks using monotone variational inequality,"Thus, one can expect that SVI results in further weight updates than
SGD, which experimentally seem to speed up the initial model convergence. We illustrate this phenomenon
in Figure 3 and will provide explanations in future works. 4 Guarantee of model recovery by MVI

We now present guarantees on model recovery for the last-layer training when previous layers are fully

known; in practice, this can be understood as if previous layers have provided necessary feature extraction. ",stat.ML,C,-0.12274487,-0.29692683,-0.0071733445
http://arxiv.org/pdf/2202.08876v2,An alternative approach to train neural networks using monotone variational inequality,"Thus, one can expect that SVI results in further weight updates than
SGD, which experimentally seem to speed up the initial model convergence. We illustrate this phenomenon
in Figure 3 and will provide explanations in future works. 4 Guarantee of model recovery by MVI

We now present guarantees on model recovery for the last-layer training when previous layers are fully

known; in practice, this can be understood as if previous layers have provided necessary feature extraction. ",stat.ML,C,-0.12274487,-0.29692683,-0.0071733445
http://arxiv.org/pdf/2202.08969v1,Private Quantiles Estimation in the Presence of Atoms,"We always have that eÔ¨Ä ‚â§ but we would like to measure the
diÔ¨Äerence between the two and its dependence on the noise level. The theoretical study of such is out of
the scope of this article and is left as future work. However, we conduct a numerical analysis of such in
Section 5. ",stat.ML,B,0.46454984,0.15307002,-0.063376814
http://arxiv.org/pdf/2202.09008v1,On Variance Estimation of Random Forests,"Thirdly, in our smoothed estimator, the choice of testing sample neighbors can be data-
dependent and relies on the distance deÔ¨Åned by the forests. It is worth considering more robust
smoothing methods for future work. Lastly, this paper focuses on the regression problem using random forest. ",stat.ML,A,0.00092970394,0.019899193,-0.29998338
http://arxiv.org/pdf/2202.09008v2,On Variance Estimation of Random Forests,"Thirdly, in our smoothed estimator, the choice of testing sample neighbors can be data-dependent and
relies on the distance deÔ¨Åned by the forests. It is worth considering more robust smoothing methods for
future work. Lastly, this paper focuses on the regression problem using random forest. ",stat.ML,A,0.00092970394,0.019899193,-0.29998338
http://arxiv.org/pdf/2202.09008v3,On Variance Estimation of Random Forests,"Thirdly, in our smoothed estimator, the choice of testing sample neighbors can be data-
dependent and relies on the forest-deÔ¨Åned distance. It is worth considering more robust
smoothing methods for future work. Lastly, this paper focuses on the regression problem using random forest. ",stat.ML,A,-0.0011923444,0.020359628,-0.29994005
http://arxiv.org/pdf/2202.09054v1,Interpolation and Regularization for Causal Learning,"One could further consider generalizing the assumptions we make in the paper: arbitrary co-
variances, shifts in the marginal distributions of covariates under interventions, more complex hy-
pothesis classes or non-linear causal relationships. Since our simple linear model already exhibits
rich behavior, we focus in this paper on thoroughly understanding the simple setting and leave such
extensions for future work. Acknowledgments

This work has been supported by the German Federal Ministry of Education and Research (BMBF):
Tu¬®bingen AI Center, FKZ: 01IS18039A, the German Research Foundation through the Cluster of
Excellence ‚ÄúMachine Learning ‚Äì New Perspectives for Science‚Äù (EXC 2064/1 number 390727645),
and the Baden-Wu¬®rttemberg Stiftung (Eliteprogram for Postdocs project ‚ÄúClustering large evolving
networks‚Äù). ",stat.ML,A,-0.1774531,0.14870417,0.027835313
http://arxiv.org/pdf/2202.09182v1,Churn modeling of life insurance policies via statistical and machine learning methods -- Analysis of important features,"Since
the values of the model‚Äôs goodness-of-Ô¨Åt are neither particularly bad or good,
these Ô¨Åndings should be viewed with some caution. As an outlook and extension to this work, two major steps can be consid-
ered in further research: Incorporating additional data sources or using other
models, such as e.g. boosting approaches or deep learning methods such as
neural networks. ",stat.ML,A,-0.2834593,-0.034188367,-0.16405502
http://arxiv.org/pdf/2202.09233v1,Nonstationary multi-output Gaussian processes via harmonizable spectral mixtures,"spectral mixture kernels. A sparse implementation of
MOSHM is also part of the future work, yet that is a     [Bonilla et al., 2007] Bonilla, E. V., Chai, K., and
challenge in its own since the relationship between the     Williams, C. (2007). Multi-task Gaussian process
locations of inducing inputs in the multichannel case       prediction. ",stat.ML,C,-0.11113672,-0.30022323,0.00518025
http://arxiv.org/pdf/2202.09497v1,Gradient Estimation with Discrete Stein Operators,"We leave the                                DisARM start to perform better as the training proceeds. In
exploration of these directions for future work. contrast, RODEO has the lowest variance in both phases
                                                                                        with the only exception on Omniglot, where DisARM over-
6.2. ",stat.ML,A,-0.084661365,0.19941694,-0.027896123
http://arxiv.org/pdf/2202.09497v2,Gradient Estimation with Discrete Stein Operators,"Finally, we have restricted our focus in this work to differentiable
target functions f . In future work, this limitation could be overcome by designing effective surrogate
functions that make no use of derivative information. 9
Acknowledgments and Disclosure of Funding

We thank Heishiro Kanagawa for suggesting appropriate names for the Barker Stein operator. ",stat.ML,B,0.12606432,-0.11352439,0.056655515
http://arxiv.org/pdf/2202.09497v3,Gradient Estimation with Discrete Stein Operators,"Finally, we have restricted our focus in this work to differentiable
target functions f . In future work, this limitation could be overcome by designing effective surrogate
functions that make no use of derivative information. 9
Acknowledgments and Disclosure of Funding

We thank Heishiro Kanagawa for suggesting appropriate names for the Barker Stein operator. ",stat.ML,B,0.12606432,-0.11352439,0.056655515
http://arxiv.org/pdf/2202.09497v4,Gradient Estimation with Discrete Stein Operators,"Finally, we have restricted our focus in this work to differentiable
target functions f . In future work, this limitation could be overcome by designing effective surrogate
functions that make no use of derivative information. Acknowledgments and Disclosure of Funding

We thank Heishiro Kanagawa for suggesting appropriate names for the Barker Stein operator. ",stat.ML,B,0.13865736,-0.1267973,0.06587945
http://arxiv.org/pdf/2202.09497v5,Gradient Estimation with Discrete Stein Operators,"Finally, we have restricted our focus in this work to differentiable
target functions f . In future work, this limitation could be overcome by designing effective surrogate
functions that make no use of derivative information. Acknowledgments and Disclosure of Funding

We thank Heishiro Kanagawa for suggesting appropriate names for the Barker Stein operator. ",stat.ML,B,0.13865736,-0.1267973,0.06587945
http://arxiv.org/pdf/2202.09497v6,Gradient Estimation with Discrete Stein Operators,"Finally, we have restricted our focus in this work to differentiable
target functions f . In future work, this limitation could be overcome by designing effective surrogate
functions that make no use of derivative information. Acknowledgments and Disclosure of Funding

We thank Heishiro Kanagawa for suggesting appropriate names for the Barker Stein operator. ",stat.ML,B,0.13865736,-0.1267973,0.06587945
http://arxiv.org/pdf/2202.09671v1,Truncated Diffusion Probabilistic Models,"While we share the same spirit to reduce the
length of the diÔ¨Äusion chain, these two strategies are not conÔ¨Çicting with each other. In the
future work we will look into the integration of these diÔ¨Äerent strategies. C Additional Results

Below we provide complementary experimental results. ",stat.ML,B,0.26341406,0.116786875,0.26834813
http://arxiv.org/pdf/2202.09671v2,Truncated Diffusion Probabilistic Models,"While we share the same spirit to reduce the
length of the diÔ¨Äusion chain, these two strategies are not conÔ¨Çicting with each other. In the
future work we will look into the integration of these diÔ¨Äerent strategies. C Experimental Settings

Below we present our experimental settings in detail. ",stat.ML,B,0.19603774,0.1452723,0.25654352
http://arxiv.org/pdf/2202.09671v3,Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders,"Below we provide an alternative
method proposed in Zheng and Zhou [2021] to Ô¨Åt the truncated distribution. Besides the
training, it‚Äôs also an open question whether TDPM can be incorporated into more advanced
architectures to have further improvements and we leave this exploration for future work. C Algorithm details and complementary Results

Below we provide additional algorithm details and complementary experimental results. ",stat.ML,C,-0.035190944,-0.19554539,-0.18175359
http://arxiv.org/pdf/2202.09673v1,A Regularized Implicit Policy for Offline Reinforcement Learning,"For fair comparison, we use the same network architecture as the oÔ¨Écial implementation of the BCQ
algorithm, which will be stated in detail in Sections D.2.1. Due to limited computational resources, we leave a
Ô¨Åne-tuning of the noise distribution pz (z), the network architectures, and the optimization hyperparameters
for future work, which also leaves room for further improving our results. For a more stable training of the policy, we adopt the warm start strategy (Kumar et al., 2020; Yue
et al., 2020). ",stat.ML,C,-0.05233783,-0.12227315,-0.13267815
http://arxiv.org/pdf/2202.09673v2,A Behavior Regularized Implicit Policy for Offline Reinforcement Learning,"The original s and its NB noisy replications are then
fed into the implicit policy to sample the corresponding action. 1We use the tasks ‚Äúmaze2d-umaze,‚Äù ‚Äúmaze2d-medium,‚Äù and ‚Äúmaze2d-large.‚Äù
    2We use the tasks ‚Äúpen-human,‚Äù ‚Äúpen-cloned,‚Äù ‚Äúpen-expert,‚Äù and ‚Äúdoor-expert.‚Äù

30
    Due to limited computational resources, we leave a Ô¨Åne-tuning of the noise distribution pz (z), the network
architectures, and the optimization hyperparameters for future work, which also leaves room for further
improving our results. Warm-start step. ",stat.ML,C,-0.1657243,-0.0009260252,0.051042147
http://arxiv.org/pdf/2202.09724v4,Bayes-Optimal Classifiers under Group Fairness,"However, for approximate fairness for a multi-class protected attribute, the
total number of equality and inequality constraints is unknown ahead of time. A careful analysis of these two types of
constraints is required, and we leave this to future work. Proof. ",stat.ML,B,0.17783463,0.13689923,0.06106448
http://arxiv.org/pdf/2202.09867v1,Interacting Contour Stochastic Gradient Langevin Dynamics,"Since privacy concerns
and communication cost are not major bottlenecks of our problem, we leave the study of taking the
Monte Carlo average in Eq. (6) every K > 1 iterations for future works. 4 CONVERGENCE PROPERTIES

To study theoretical properties of ICSGLD, we Ô¨Årst show a local stability property that is well-suited
to big data problems, and then we present the asymptotic normality for the stochastic approxi-
mation process in mini-batch settings, which eventually yields the desired result that ICSGLD is
asymptotically more efÔ¨Åcient than a single-chain CSGLD with an equivalent computational cost. ",stat.ML,B,0.2196143,-0.037066825,-0.13186595
http://arxiv.org/pdf/2202.09875v1,Trying to Outrun Causality in Machine Learning: Limitations of Model Explainabilty Techniques for Identifying Predictive Variables,"If we are able to integrate machine learning algorithms into
analyses which adequately account for the underlying structure, we can beneÔ¨Åt from the

                                                                18
Trying to Outrun Causality in Machine Learning (preprint)

power of the machine learning algorithms without the associated problems demonstrated
in this work. In terms of future work, it would be pertinent to undertake further analyses using a
range of other machine learning algorithms, in particular using datasets with alternative
structures and categorical outcomes. References

K. Aas, M. Jullum, and A. Loland. ",stat.ML,A,-0.4066922,0.17134288,0.07681578
http://arxiv.org/pdf/2202.09875v2,Trying to Outrun Causality in Machine Learning: Limitations of Model Explainability Techniques for Identifying Predictive Variables,"If we are able to integrate machine learning algorithms into
analyses which adequately account for the underlying structure, we can beneÔ¨Åt from the
power of the machine learning algorithms without the associated problems demonstrated
in this work. In terms of future work, it would be pertinent to undertake further analyses using a range
of other machine learning algorithms, in particular using datasets with alternative structures
and categorical outcomes. It would also be of interest to check whether these problems apply
to much more complex machine learning models, such as transformers (Vaswani et al.,
2017) and other large-scale deep learning models (such as those used in natural language
processing). ",stat.ML,C,-0.41372555,-0.17756292,0.08927342
http://arxiv.org/pdf/2202.09875v3,Trying to Outrun Causality with Machine Learning: Limitations of Model Explainability Techniques for Identifying Predictive Variables,"If we are able to integrate machine learning algorithms into
analyses which adequately account for the underlying structure, we can beneÔ¨Åt from the
power of the machine learning algorithms without the associated problems demonstrated
in this work. In terms of future work, it would be pertinent to undertake further analyses using a range
of other machine learning algorithms, in particular using datasets with alternative structures
and categorical outcomes. It would also be of interest to check whether these problems apply
to much more complex machine learning models, such as transformers (Vaswani et al.,
2017) and other large-scale deep learning models (such as those used in natural language
processing). ",stat.ML,C,-0.41372555,-0.17756292,0.08927342
http://arxiv.org/pdf/2202.09875v4,Trying to Outrun Causality with Machine Learning: Limitations of Model Explainability Techniques for Identifying Predictive Variables,"Furthermore, if we are able to integrate
machine learning algorithms into analyses which adequately account for the underlying
structure, we can beneÔ¨Åt from the power of the machine learning algorithms without the
associated problems demonstrated in this work. In terms of future work, it would be pertinent to undertake further analyses using a range
of other machine learning algorithms, in particular using datasets with alternative structures
and categorical outcomes. It would also be of interest to check whether these problems apply
to much more complex machine learning models, such as transformers (Vaswani et al.,

                                                                22
Trying to Outrun Causality in Machine Learning (preprint)

2017) and other large-scale deep learning models (such as those used in natural language
processing). ",stat.ML,C,-0.46724594,-0.060484078,0.14255172
http://arxiv.org/pdf/2202.09889v1,Memorize to Generalize: on the Necessity of Interpolation in High Dimensional Linear Regression,"In the anisotropic setting, our lower bounds on prediction error depend on the condition
number of the data covariance, and thus our bounds not apply, i.e., are vacuous, in settings
such as sparse covariance or kernel regression. Extending our results to these settings is an
interesting direction for future work. Furthermore, our analysis relies heavily on the fact that
both the prediction and empirical risk are quadratic in the case of least-squares regression,
and thus strong duality obtains. ",stat.ML,B,0.1731519,-0.13223577,-0.12756522
http://arxiv.org/pdf/2202.09889v2,Memorize to Generalize: on the Necessity of Interpolation in High Dimensional Linear Regression,"In the anisotropic setting, our lower bounds on prediction error depend on the condition
number of the data covariance, and thus our bounds not apply, i.e., are vacuous, in settings
such as sparse covariance or kernel regression. Extending our results to these settings is an
interesting direction for future work. Furthermore, our analysis relies heavily on the fact that
both the prediction and empirical risk are quadratic in the case of least-squares regression,
and thus strong duality obtains. ",stat.ML,B,0.1731519,-0.13223577,-0.12756522
http://arxiv.org/pdf/2202.09924v1,Generalized Bayesian Additive Regression Trees Models: Beyond Conditional Conjugacy,"26
5 Discussion

The approach outlined in this article greatly expands the problems to which BART can
be applied, and we emphasize that none of the models we applied BART to required any
modiÔ¨Åcations to our algorithm. There are many directions for extending this framework
in future work. For example, by modifying the approach to allow for more than one for-
est (Pratola et al., 2020), we could develop Ô¨Çexible gamma regression models with Yi ‚àº
Gam{Œ±(Xi), Œ±(Xi)/¬µ(Xi)} or beta regression models with Yi ‚àº Beta{¬µ(Xi) œÜ(Xi), œÜ(Xi) ‚àí
œÜ(Xi) ¬µ(Xi)}. ",stat.ML,B,0.062629126,0.0948538,-0.07272187
http://arxiv.org/pdf/2202.10244v1,Stochastic Modeling of Inhomogeneities in the Aortic Wall and Uncertainty Quantification using a Bayesian Encoder-Decoder Surrogate,"This framework can also be applied and
further extended to any other constitutive model. In order to be able to derive reliable conclu-
sions about the stress distribution, future work must on the one hand contain the application to
boundary-value problems that are closer to in vivo conditions and on the other hand the correla-
tion length of the local inhomogeneities must correspond to the experiments [4, 116, 117]. It is
proposed that the presented Bayesian framework allows the identiÔ¨Åcation of a law for stochastic
inhomogeneities based on experimental data. ",stat.ML,A,0.057632968,0.26098654,0.047095466
http://arxiv.org/pdf/2202.10574v1,A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation in Two-sided Markets,"In particular, Reich
et al. (2020) gave a systematic review of various statistical models for spatial causal inference
and pinpoint some areas of future work. However, those models in Reich et al. ",stat.ML,A,-0.18175453,0.394811,0.024957698
http://arxiv.org/pdf/2202.10574v2,A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation in Two-sided Markets,"In particular, Reich
et al. (2020) gave a systematic review of various statistical models for spatial causal inference
and pinpoint some areas of future work. However, those models in Reich et al. ",stat.ML,A,-0.18175453,0.394811,0.024957698
http://arxiv.org/pdf/2202.10574v3,A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation in Two-sided Markets,"In particular, Reich
et al. (2020) gave a systematic review of various statistical models for spatial causal inference
and pinpoint some areas of future work. However, those models in Reich et al. ",stat.ML,A,-0.18175453,0.394811,0.024957698
http://arxiv.org/pdf/2202.10613v1,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,"Similar simpliÔ¨Åcations also occur on the special orthogonal group, suggesting
that there may be a general theory that describes them. Finding such a
theory is a promising direction for future work. To do so, the Ô¨Årst step would
be to understand how symmetries of a manifold aÔ¨Äect kernels deÔ¨Åned over it
via spectral methods similar to the ones we have used. ",stat.ML,C,0.054492984,-0.17432183,0.3946795
http://arxiv.org/pdf/2202.10613v2,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,"Similar simpliÔ¨Åcations also occur on the special orthogonal group, suggesting
that there may be a general theory that describes them. Finding such a
theory is a promising direction for future work. To do so, the Ô¨Årst step would
be to understand how symmetries of a manifold aÔ¨Äect kernels deÔ¨Åned over it
via spectral methods similar to the ones we have used. ",stat.ML,C,0.054492984,-0.17432183,0.3946795
http://arxiv.org/pdf/2202.10613v3,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,"Similar simpliÔ¨Åcations also occur on the special orthogonal group, suggesting
that there may be a general theory that describes them. Finding such a
theory is a promising direction for future work. To do so, the Ô¨Årst step would
be to understand how symmetries of a manifold aÔ¨Äect kernels deÔ¨Åned over it
via spectral methods similar to the ones we have used. ",stat.ML,C,0.054492984,-0.17432183,0.3946795
http://arxiv.org/pdf/2202.10615v1,Order-Optimal Error Bounds for Noisy Kernel-Based Bayesian Quadrature,"Thus,  there  is
                                                                                                                                    d                   2

no adaptivity gap in terms of scaling laws. Perhaps the most immediate direction for future work is to extend our results to other kernels, notably

including (i) the Mat√©rn kernel with non-integer values of ŒΩ + d/2, and (ii) the squared exponential kernel. The former has the added diÔ¨Éculty of not being able to use the equivalence with the Sobolev class, and the

latter has the added diÔ¨Éculty of not being able to use the bounded-support bump function (as it has inÔ¨Ånite

RKHS norm under the SE kernel). ",stat.ML,B,0.2505393,-0.1177766,0.14690724
http://arxiv.org/pdf/2202.10638v1,Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations,"MNIST and CIFAR-10 datasets, leading to better marginal
likelihoods and higher test performance. In future work, it      Fong, E. and Holmes, C. On the marginal likelihood and
would be interesting to consider more complex transforma-           cross-validation. Biometrika, 107(2):489‚Äì496, 2020.
tions and apply the method to larger networks. ",stat.ML,C,-0.27004442,-0.06622781,-0.07895986
http://arxiv.org/pdf/2202.10638v2,Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations,"Our work shows that approximate Bayesian inference methods can be
useful for learning complex hyperparameters, even in deep learning, and are therefore relevant beyond
predictive uncertainty estimation. In future work, it would be interesting to improve the scalability
and accuracy of marginal likelihood approximations which could enable learning even more complex
hyperparameters, such as augmentation distributions parameterised by neural networks. References

Javier Antor√°n, Riccardo Barbano, Johannes Leuschner, Jos√© Miguel Hern√°ndez-Lobato, and
   Bangti Jin. ",stat.ML,C,-0.24294594,-0.27084702,-0.17911635
http://arxiv.org/pdf/2202.10638v3,Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations,"Our work shows that approximate Bayesian inference methods can be
useful for learning complex hyperparameters, even in deep learning, and are therefore relevant beyond
predictive uncertainty estimation. In future work, it would be interesting to improve the scalability
and accuracy of marginal likelihood approximations which could enable learning even more complex
hyperparameters, such as augmentation distributions parameterised by neural networks. Alternatively,
improving parameterisations of invariances in neural networks could greatly improve the scalability
of LILA and related approaches. ",stat.ML,C,-0.25286692,-0.27505013,-0.16411766
http://arxiv.org/pdf/2202.10669v1,On Uncertainty Estimation by Tree-based Surrogate Models in Sequential Model-based Optimization,"At least, even if a function estimate is not
correct, an uncertainty estimate should become larger
than the results shown in the paper. In order to ex-
pand the usage of tree-based models in SMO, improv-
ing the ability to extrapolate is left to a future work. On Uncertainty Estimation by Tree-based Surrogate Models in SMO

References                                               Duan, T., Avati, A., Ding, D. Y., Thai, K. K., Basu,
                                                            S., Ng, A., and Schuler, A. ",stat.ML,A,-0.08884728,0.14359026,-0.2946865
http://arxiv.org/pdf/2202.10806v1,Stochastic Causal Programming for Bounding Treatment Effects,"ConÔ¨Ådence or credible
comparable or arguably better results than those from                  intervals for both extrema can help practitioners evaluate
the GAN framework. Indeed, Figure 5 shows that despite                 the reliability of causal inferences and are an interesting
the Ô¨Çexible neural basis functions, our approach can yield             direction for future work. tighter bounds and mostly avoid instabilities as observed
for the GAN approach when we move towards the tails of                 We observe that our procedure can additionally be used
the observed distribution. ",stat.ML,C,-0.08375062,-0.058252707,0.011782419
http://arxiv.org/pdf/2202.10806v2,Stochastic Causal Programming for Bounding Treatment Effects,"Finally, we have not accounted for the
uncertainty of our bounds. ConÔ¨Ådence or credible intervals for both extrema can help practitioners evaluate the
reliability of causal inferences, and are an interesting direction for future work. From an ethical perspective, more reliable methods for bounding causal eÔ¨Äects could help quantify causal eÔ¨Äects
of high-stakes or consequential decisions when identiÔ¨Åability is impossible. ",stat.ML,A,0.05696817,0.41553777,-0.08973409
http://arxiv.org/pdf/2202.10903v1,Confident Neural Network Regression with Bootstrapped Deep Ensembles,"The conÔ¨Ådence intervals of our method increase at the              are doomed to be violated. A more promising avenue for
location of the data points, indicating overÔ¨Åtting, whereas              future work is to combine our method with an orthogonal
those of DE almost vanish. approach, speciÔ¨Åcally for OoD detection (such as Ren et al. ",stat.ML,B,0.16060385,-0.04307318,0.043481752
http://arxiv.org/pdf/2202.11154v1,Parallel MCMC Without Embarrassing Failures,"approach hinges on the ability of the surrogate model to
faithfully represent the subposteriors. Active learning    References
helps, but model mismatch in our method is still a
potential issue that hints at future work combining        L. Acerbi. Variational Bayesian Monte Carlo. ",stat.ML,C,-0.15706345,-0.08379333,-0.14691368
http://arxiv.org/pdf/2202.11154v2,Parallel MCMC Without Embarrassing Failures,"approach hinges on the ability of the surrogate model to
faithfully represent the subposteriors. Active learning    References
helps, but model mismatch in our method is still a
potential issue that hints at future work combining        L. Acerbi. Variational Bayesian Monte Carlo. ",stat.ML,C,-0.15706345,-0.08379333,-0.14691368
http://arxiv.org/pdf/2202.12008v1,A fair pricing model via adversarial learning,"This approach shows to be more eÔ¨Écient in terms of accuracy for similar levels of
fairness on various data sets. As future work, it might be interesting to consider a generalization of our proposal
for telematics insurance where some biases can be mitigated on diÔ¨Äerent aggregate scores. 16
                EDR vs FairQuant        3.0 1e6     MSE vs FairQuant              Dependence between Car risk and Predictions by                                                                                Dependence between Geo. ",stat.ML,A,-0.11827117,0.26257637,-0.21051313
http://arxiv.org/pdf/2202.12008v2,A fair pricing model via adversarial learning,"This approach shows to be more eÔ¨Écient in terms of accuracy for similar levels of
fairness on various data sets. As future work, it might be interesting to consider a generalization of our proposal
for telematics insurance where some biases can be mitigated on diÔ¨Äerent aggregate scores. 16
                EDR vs FairQuant        3.0 1e6     MSE vs FairQuant              Dependence between Car risk and Predictions by                                                                                Dependence between Geo. ",stat.ML,A,-0.11827117,0.26257637,-0.21051313
http://arxiv.org/pdf/2202.12008v3,A Fair Pricing Model via Adversarial Learning,"This approach shows to be more eÔ¨Écient in terms of accuracy for similar levels of
fairness on various data sets. As future work, it might be interesting to consider a generalization of our proposal
for telematics insurance where some biases can be mitigated on diÔ¨Äerent aggregate scores. 23
References

Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., and Wallach, H. (2018). ",stat.ML,A,-0.101943806,0.26363677,-0.20629591
http://arxiv.org/pdf/2202.12275v1,Partitioned Variational Inference: A framework for probabilistic federated learning,"Sudderth et al., 2010) or normalising

                                                                28
                                     Partitioned Variational Inference

Ô¨Çows (Rezende and Mohamed, 2015) can be eÔ¨Éciently and tractably accommodated in the
PVI framework. We leave these directions as future work. The experiments in Section 5 demonstrated that PVI is well-suited to learning with decen-
tralised data. ",stat.ML,C,-0.19917622,-0.17616387,0.09975671
http://arxiv.org/pdf/2202.12275v2,Partitioned Variational Inference: A Framework for Probabilistic Federated Learning,"Sudderth et al., 2010) or normalising

                                                                28
                                     Partitioned Variational Inference

Ô¨Çows (Rezende and Mohamed, 2015) can be eÔ¨Éciently and tractably accommodated in the
PVI framework. We leave these directions as future work. The experiments in Section 5 demonstrated that PVI is well-suited to learning with decen-
tralised data. ",stat.ML,C,-0.19917622,-0.17616387,0.09975671
http://arxiv.org/pdf/2202.12275v3,Partitioned Variational Inference: A Framework for Probabilistic Federated Learning,"Sudderth et al., 2010) or normalising

                                                                28
                                     Partitioned Variational Inference

Ô¨Çows (Rezende and Mohamed, 2015) can be eÔ¨Éciently and tractably accommodated in the
PVI framework. We leave these directions as future work. The experiments in Section 5 demonstrated that PVI is well-suited to learning with decen-
tralised data. ",stat.ML,C,-0.19917622,-0.17616387,0.09975671
http://arxiv.org/pdf/2202.12275v4,Partitioned Variational Inference: A Framework for Probabilistic Federated Learning,"Sudderth et al., 2010) or normalising
Ô¨Çows (Rezende and Mohamed, 2015) can be eÔ¨Éciently and tractably accommodated in the
PVI framework. We leave these directions as future work. The experiments in Section 5 demonstrated that PVI is well-suited to learning with decen-
tralised data. ",stat.ML,C,-0.26550078,-0.17904451,0.16668944
http://arxiv.org/pdf/2202.12297v1,Embedded Ensembles: Infinite Width Limit and Operating Regimes,"In the centered case, both the av-   bles [Havasi et al., 2020], since some of their weights
erage and standard deviation of the normalized scalar     are expected to vanish as a result of training. We leave
                                                          analysis of other embedded ensembles to a future work. Embedded Ensembles: InÔ¨Ånite Width Limit and Operating Regimes

Acknowledgment                                            [Hanin and Nica, 2020] Hanin, B. and Nica, M.
                                                             (2020). ",stat.ML,C,0.0463205,-0.1333061,-0.04000055
http://arxiv.org/pdf/2202.12363v1,Estimators of Entropy and Information via Inference in Probabilistic Models,"The ex-        timal solutions to challenging problems from hepatol-
pressions in (8) and (9) used for EEVI are instances        ogy and endocrinology. Avenues for future work in-
of nested Monte Carlo, where the inner expectation is       clude using EEVI for optimal experiment design and
obtained via pseudo-marginal methods (Andrieu and           information analysis in widely used medical expert sys-
Roberts, 2009) and the non-linear mapping is log. tems (Zhou and Sordo, 2021) by developing the appli-
                                                            cation in tandem with clinical domain experts. ",stat.ML,A,-0.1559563,0.358345,-0.12444194
http://arxiv.org/pdf/2202.12363v2,Estimators of Entropy and Information via Inference in Probabilistic Models,"The ex-        timal solutions to challenging problems from hepatol-
pressions in (8) and (9) used for EEVI are instances        ogy and endocrinology. Avenues for future work in-
of nested Monte Carlo, where the inner expectation is       clude using EEVI for optimal experiment design and
obtained via pseudo-marginal methods (Andrieu and           information analysis in widely used medical expert sys-
Roberts, 2009) and the non-linear mapping is log. tems (Zhou and Sordo, 2021) by developing the appli-
                                                            cation in tandem with clinical domain experts. ",stat.ML,A,-0.1559563,0.358345,-0.12444194
http://arxiv.org/pdf/2202.12363v3,Estimators of Entropy and Information via Inference in Probabilistic Models,"The ex-        timal solutions to challenging problems from hepatol-
pressions in (8) and (9) used for EEVI are instances        ogy and endocrinology. Avenues for future work in-
of nested Monte Carlo, where the inner expectation is       clude using EEVI for optimal experiment design and
obtained via pseudo-marginal methods (Andrieu and           information analysis in widely used medical expert sys-
Roberts, 2009) and the non-linear mapping is log. tems (Zhou and Sordo, 2021) by developing the appli-
                                                            cation in tandem with clinical domain experts. ",stat.ML,A,-0.1559563,0.358345,-0.12444194
http://arxiv.org/pdf/2202.12363v4,Estimators of Entropy and Information via Inference in Probabilistic Models,"The ex-        timal solutions to challenging problems from hepatol-
pressions in (8) and (9) used for EEVI are instances        ogy and endocrinology. Avenues for future work in-
of nested Monte Carlo, where the inner expectation is       clude using EEVI for optimal experiment design and
obtained via pseudo-marginal methods (Andrieu and           information analysis in widely used medical expert sys-
Roberts, 2009) and the non-linear mapping is log. tems (Zhou and Sordo, 2021) by developing the appli-
                                                            cation in tandem with clinical domain experts. ",stat.ML,A,-0.1559563,0.358345,-0.12444194
http://arxiv.org/pdf/2202.12439v1,Learning Invariant Weights in Neural Networks,"40.92                            54.72        We found that we could succesfully learn invariance using
                             46.12       45.20  47.44         55.44        marginal likelihood, also referred to as Empirical Bayes or
                                                                           Type-II ML, which could not be learned with regular max-
                                         41.22  49.28                      imum likelihood (Type-I ML). We did, however, rely on
                                                                           small Œ∑ and learning higher dimensional invariances might
                                         45.77  48.81                      require more sophisticated methods which may need addi-
                                                                           tional priors on Œ∑. Lastly, this work focuses on single layer
Table 3: Test Accuracy scores for learned invariance using                 neural networks, and we will consider deeper architectures
different transformations in a shallow ReLU neural network                 in future work. For deeper models, we should ask the ques-
on the CIFAR-10 dataset. ",stat.ML,C,-0.25885782,-0.23239233,-0.073058374
http://arxiv.org/pdf/2202.12439v2,Learning Invariant Weights in Neural Networks,"We found that we could succesfully
MLP + AfÔ¨Åne (ours)           40.99       40.71  46.77         55.79        learn invariance using marginal likelihood, also referred to
                             40.92                            54.72        as Empirical Bayes or Type-II ML, which is not possible
                             46.12       45.20  47.44         55.44        with regular maximum likelihood (Type-I ML). To do so,
                                                                           we relied on Œ∑ being small and learning higher dimensional
                                         41.22  49.28                      invariances might therefore require more sophisticated meth-
                                                                           ods or additional priors on Œ∑. Lastly, this work focuses on
                                         45.77  48.81                      single layer neural networks, and we will consider deeper
                                                                           architectures in future work. For deeper models, we should
Table 3: Test Accuracy scores for learned invariance using                 ask the question whether the bound on the marginal likeli-
different transformations in a shallow ReLU neural network                 hood will stay sufÔ¨Åciently tight [Dutordoir et al., 2021, Ober
on the CIFAR-10 dataset. ",stat.ML,C,-0.16069925,-0.27122656,-0.094993934
http://arxiv.org/pdf/2202.12780v1,Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice,"Remarkably, it turns out that the models with correction factor are not only better
unconditionally calibrated than the uncorrected ones, but they are also better conditionally
calibrated on Gender, in-sample as well as out-of-sample. If we were to place great
importance on calibration, we could exclude the three model variants without correction
factor, OLS, Gamma GLM and XGBoost, from further analysis as they persistently show
worse calibration than their counterparts with calibration factor. We could go on and investigate interactions with other features, for instance, use
œï(x) = LogWeeklyPay ¬∑ 1{Gender = ‚ÄúM‚Äù}. ",stat.ML,A,-0.2928246,0.21661067,0.0005766023
http://arxiv.org/pdf/2202.12780v2,Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice,"Remarkably, it turns out that the models with correction factor are not only better
unconditionally calibrated than the uncorrected ones, but they are also better conditionally
calibrated on Gender, in-sample as well as out-of-sample. If we were to place great
importance on calibration, we could exclude the three model variants without correction
factor, OLS, Gamma GLM and XGBoost, from further analysis as they persistently show
worse calibration than their counterparts with calibration factor. We could go on and investigate interactions with other features, for instance, use
œï(x) = LogWeeklyPay ¬∑ 1{Gender = ‚ÄúM‚Äù}. ",stat.ML,A,-0.2928246,0.21661067,0.0005766023
http://arxiv.org/pdf/2202.12891v1,Combining Observational and Randomized Data for Estimating Heterogeneous Treatment Effects,"There are 5 continuous covariates: age (year),
weight (kg, coded as wtkg), CD4 count (cells/mm3) at baseline, Karnofsky score (scale
of 0‚Äì100, coded as karnof), CD8 count (mm3) at baseline. They are centered and scaled
before further analysis. In addition, there are 7 binary variables: gender (1 = male, 0 =
female), homosexual activity (homo, 1 = yes, 0 = no), race (1 = nonwhite, 0 = white),
history of intravenous drug use (drug, 1 = yes, 0 = no), symptomatic status (symptom, 1 =
symptomatic, 0 = asymptomatic), antiretroviral history (str2, 1 = experienced, 0 = naive)
and hemophilia (hemo, 1 = yes, 0 = no). ",stat.ML,A,-0.2065571,0.4555411,0.14337513
http://arxiv.org/pdf/2202.13059v1,Variational Inference with Gaussian Mixture by Entropy Approximation,"9
       0.7      Error Graph with c = 0.5 and 1 = 2 = 0.5          0.5      Error Graph with c = 2 and 1 = 2 = 0.5

                mean                                                       mean

       0.6                                                        0.4

       0.5                                                        0.3

error  0.4                                                 error

       0.3                                                        0.2

       0.2                                                        0.1

       0.1                                                        0.0

       0.0   0  20 d4i0mension60m 80 100                                0  20 d4i0mension60m 80 100

           Error Graph with c = 0.5, 1 = 0.9, and 2 = 0.1              Error Graph with c = 2, 1 = 0.9, and 2 = 0.1

       0.30 mean                                                                                                              mean

       0.25                                                       0.20

       0.20                                                       0.15

error  0.15                                                error  0.10

       0.10                                                       0.05

       0.05                                                       0.00
                                                                        0
       0.00     20 d4i0mension60m 80 100                                   20 d4i0mension60m 80 100
             0

Figure 2: Error curve for H[qŒ∏] ‚àí H[qŒ∏] : The blue line indicates its mean, and the Ô¨Ålled region
indicates the mean ¬±1 standard deviation. 6 Limitations and Future Work

The limitations and future work are as follows:

  (a) There is an unsolved problem on the standard deviation c in (4.4) of Corollary 4.3. According
       to this corollary, the approximation error almost surely converges to zero as m ‚Üí ‚àû if we take
       c > 1 (see the discussion after Corollary 4.3 ). ",stat.ML,A,0.10175188,0.25636768,0.03577596
http://arxiv.org/pdf/2202.13059v2,Variational Inference with Gaussian Mixture by Entropy Approximation,"Therefore,
the BNN-GM can automatically control intensities of the ensemble members. 6 Limitations and Future Work

The limitations and future work are as follows:

    ‚Ä¢ There is an unsolved problem on the standard deviation c in (4.2) of Corollary 4.3. According
       to this corollary, the approximation error almost surely converges to zero as m ‚Üí ‚àû if we
       take c > 1 (the discussion after Corollary 4.3). ",stat.ML,B,0.14999533,0.004034033,-0.14808744
http://arxiv.org/pdf/2202.13157v1,High Dimensional Statistical Estimation under One-bit Quantization,"Moreover, we report
that our estimation based on one-bit quantized data may not be satisfactory in some situations,
for example, when the underlying entries vary a lot in magnitude (i.e., absolute value), since a
common dithering scale Œ≥ cannot well handle the quantization. It needs further research how
to address these issues so that the proposed methods are more practical in real problems. References

 [1] Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin. ",stat.ML,B,0.14987355,-0.05691929,0.027904548
http://arxiv.org/pdf/2202.13683v1,Estimating Model Performance on External Samples from Their Limited Statistical Characteristics,"9
Estimating Model Performance on External Samples

et al. (2020) do not provide necessary statisti-                  In future work, we will combine the proposed al-
cal characteristics. Additionally, AUC values of               gorithm with methods that aim to construct robust
IBM MarketScan¬Æ Medicare Supplemental Database                 models such as those that leverage distributionally ro-
(MDCR) were not provided for the full atrial Ô¨Åbril-            bust optimization (Bu¬®hlmann, 2020); study methods
lation cohort and those of IBM MarketScan¬Æ Com-                that exploit the relations between calibration and ro-
mercial Database (CCAE) are missing for the older              bustness (Wald et al., 2022); and look into decompos-
female sub-cohort. ",stat.ML,A,-0.16543932,0.3051623,-0.15861651
http://arxiv.org/pdf/2202.13733v1,On the Benefits of Large Learning Rates for Kernel Methods,"In particular, we show that
this phenomenon occurs in realistic classiÔ¨Åcation tasks with low noise when learning with kernel
methods. In future work, we are planning to study other variants of gradient-based algorithms,
which may be stochastic, or accelerated, and perhaps exploit the insight developed in our work
to design new algorithms, which would focus on statistical eÔ¨Éciency, exploiting prior knowledge
on the test loss, rather than on optimization of the training objective. Acknowledgments

A.R. ",stat.ML,C,-0.07220669,-0.37158662,-0.24137807
http://arxiv.org/pdf/2203.00126v1,Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach,"The analysis requires a more sophisticated argument from
random matrix theory as in [5, 28]. We will pursue these directions in future works. Additionally, the current work mainly concerns the super-critical non-null regime Œ± > Œ≤ + 1. ",stat.ML,B,0.42857593,0.08380614,0.0660758
http://arxiv.org/pdf/2203.00521v1,Causal Structure Learning with Greedy Unconditional Equivalence Search,"and thus could be a useful alternative when the practitioner
                                                                 is willing to assume a small UEC size. native to GES, for example, especially in situations where
the size of a UEC is small and GUES is thus able to dras-        In regards to future work, the performance of GUES could
tically reduce the search space. The line adjusted GUES          likely be further optimized by introducing a turning phase,
records the proportion of times GUES exactly learned the         analogous to that of GIES (Hauser and B√ºhlmann 2012),
data-generating model out of the number of times it correctly    which empirically improves performance of GES in the
recovered the unconditional representative of the UEC con-       purely observational setting. ",stat.ML,A,-0.14911038,0.24039796,-0.073587246
http://arxiv.org/pdf/2203.00521v2,A Transformational Characterization of Unconditionally Equivalent Bayesian Networks,"Error propagation due to CI testing observed in classic hybrid methods is
avoided with UEC estimation via independent pairwise independence tests. Other potential
future work includes an extension of these results to characterizations of DAGs encoding
the same CI relations with conditioning sets of size 0 or 1. Such results could be used
to learn the 0-1 graphs studied by Wille and Bu¬®hlmann (2006), who showed these models
can be useful for estimating causal information in the small sample regime. ",stat.ML,A,-0.07819797,0.2731092,-0.03483601
http://arxiv.org/pdf/2203.00521v3,A Transformational Characterization of Unconditionally Equivalent Bayesian Networks,"Error propagation due to CI testing observed in classic hybrid methods is
avoided with UEC estimation via independent pairwise independence tests. Other potential
future work includes an extension of these results to characterizations of DAGs encoding
the same CI relations with conditioning sets of size 0 or 1. Such results could be used
to learn the 0-1 graphs studied by Wille and Bu¬®hlmann (2006), who showed these models
can be useful for estimating causal information in the small sample regime. ",stat.ML,A,-0.07819797,0.2731092,-0.03483601
http://arxiv.org/pdf/2203.00554v1,Neural Score Matching for High-Dimensional Causal Inference,"The
   smaller the calibration error, the more suitable the estimated propensity score and estimated balancing scores
   obtained from a model are for matching, as we will be closer to the assumption that the propensity score is
   correctly estimated. Connecting the calibration error to the balancing error term in Proposition 7 is left for
   future work. ‚Ä¢ The ATT error, deÔ¨Åned as the absolute diÔ¨Äerence between the ATT estimated by the method and a ground-truth
  ATT. ",stat.ML,A,0.019536868,0.34813997,-0.13721049
http://arxiv.org/pdf/2203.00973v2,A density peaks clustering algorithm with sparse search and K-d tree,"that the present method can produce the correct number of
cluster centers automatically. [8] M. Xu, Y. Li, R. Li, F. Zou, and X. Gu, ‚ÄúEADP: An extended adap-
                                                                                          tive density peaks clustering for overlapping community detection in
   For future work, datasets with insufÔ¨Åcient target data,                                social networks,‚Äù Neurocomputing, vol. 337, pp. ",stat.ML,C,-0.08821276,0.04089832,0.13346827
http://arxiv.org/pdf/2203.00977v1,Chained Generalisation Bounds,(2022). We defer this approach to future work. 5. ,stat.ML,A,-0.040181283,0.288352,0.21462286
http://arxiv.org/pdf/2203.00977v2,Chained Generalisation Bounds,(2022). We defer this approach to future work. 4. ,stat.ML,A,-0.028625872,0.28475845,0.21970508
http://arxiv.org/pdf/2203.01110v1,The Optimal Noise in Noise-Contrastive Learning Is Not What You Think,"Next we make heuristic approximation 3: we neglect any problems of inversion of singular, rank 1 matrices
(note this is not a problem in the 1D case), and obtain further

I‚àí2, B ‚âà tr (g(z)g(z) )‚àí1g(z)g(z) 1 (g(z)g(z) )‚àí1 ‚âà 1 tr (g(z)g(z) )‚àí1 (23)
n                                             pd(z)                                          pd(z)

Minimizing this term is equivalent to the following maximization setup (still applying heuristic approxima-
tion 3):

                                                                                                                             ‚àí1

                                           arg max pd(Œæ)tr (g(Œæ)g(Œæ) )‚àí1

                                                                        Œæ

Those points z obtained by the above condition are the best candidates for pn to concentrate its mass on. We arrived this result by making three heuristic approximations as explained above; we hope to be able to
remove some of them in future work. 22
Addendum: In fact, the informal proof just given implies a stronger result than announced in the Conjecture
of the main paper. ",stat.ML,B,0.34372464,-0.088541836,0.15365386
http://arxiv.org/pdf/2203.01110v2,The Optimal Noise in Noise-Contrastive Learning Is Not What You Think,"Œæ

Those points z obtained by the above condition are the best candidates for pn to concentrate its mass on. We arrived this result by making three heuristic approximations as explained above; we hope to be able to remove some
of them in future work. Numerically, evaluating the optimal noise in the all-data limit requires computing a weight w(x) =

                                    ‚àí1

tr (g(Œæ)g(Œæ) )‚àí1 that is intractable in dimensions bigger than 1, due to the singularity of the rank 1 ma-

trix. ",stat.ML,B,0.257017,-0.1623992,-0.019245777
http://arxiv.org/pdf/2203.01850v2,T-Cal: An optimal test for the calibration of predictive models,"When the residual map resf does not satisfy the H¬®older
assumption, we still have the false detection rate control in Theorem 3.1 and 3.3, but we cannot guarantee
the true detection rate control and the lower bound in Theorem 5.2. Extending our approach beyond the
H¬®older assumption may be possible in future work, inspired by works in nonparametric hypothesis testing
that study‚Äîfor instance‚ÄîBesov spaces of functions (Ingster and Suslina, 2012). 6 Reduction to Two-sample Goodness-of-Ô¨Åt Testing

To further put our work in context in the literature on nonparametric hypothesis testing, in this section we
carefully examine the connections between the problem of testing calibration, and a well-known problem in
that area. ",stat.ML,B,0.21887872,0.075704604,-0.15894383
http://arxiv.org/pdf/2203.02005v1,On consistency of constrained spectral clustering under representation-aware stochastic block model,"In particular, our experiments show that the d-regularity assumption on
the representation graph is not necessary in practice. We conclude the paper in Section 6 with a few remarks on promising directions for
future work. The proofs of all technical lemmas are presented in the supplementary material
[Gupta and Dukkipati, 2021b]. ",stat.ML,B,0.19684443,-0.010084575,0.35430622
http://arxiv.org/pdf/2203.02473v1,Interpretable Off-Policy Learning via Hyperbox Search,"There are 5 continuous covariates: age (years), weight (kg), CD4 count (cells/mm3) at baseline, Karnofsky score (scale of 0-100),
CD8 count (cells/mm3) at baseline. They are scaled before further analysis. In addition, there are 7 binary variables: gender (1 =
male, 0 = female), homosexual activity (1 = yes, 0 = no), race (1 = nonwhite, 0 = white), history of intravenous drug use (1 =
yes, 0 = no), symptomatic status (1 = symptomatic, 0 = asymptomatic), antiretroviral history (1 = experienced, 0 = naive), and
hemophilia (1 = yes, 0 = no). ",stat.ML,A,-0.17923482,0.45550412,0.11268476
http://arxiv.org/pdf/2203.02592v1,Sparsity-Inducing Categorical Prior Improves Robustness of the Information Bottleneck,"Drop-bottleneck: Learning discrete compressed rep-
                                                                          resentation for noise-robust exploration. arXiv preprint
   As part of future work we will further generalize the                  arXiv:2103.12300, 2021.
categorical prior spike distribution through a nonparametric
treatment, explore other approaches to data-driven mutual              Diederik P Kingma and Max Welling. Auto-encoding varia-
information estimation to tighten the lower bound, and apply              tional Bayes, 2014.
this approach to real-world scientiÔ¨Åc datasets. ",stat.ML,C,-0.15597318,-0.13689046,-0.17680472
http://arxiv.org/pdf/2203.02658v1,Koopman operator for time-dependent reliability analysis,"FNN and LSTM, on the other hand fails to capture the Ô¨Årst mode of the PDF for both the cases. For further examination, the PDF plots of Ô¨Årst passage failure time is also obtained for four diÔ¨Äerent sets
of test samples chosen from diÔ¨Äerent distribution (out-of-distribution). While the Ô¨Årst two sets of test
samples are generated in the support interval of x ‚àº (‚àí5, 5) and xÀô ‚àº (0, 10) having distributions Gaussian
and log-normal, the other two sets of test samples are generated in the support interval of x ‚àº (‚àí6, 6)

                                                                      7
         Koopman operator for time-dependent reliability analysis                                    A Preprint

                                   20

            Prediction error, (%)  15

                                   10

                                   5

                                   0

                                   200    400  600     800          1600  1800

                                               No. ",stat.ML,A,0.080728374,0.121058784,-0.21442929
http://arxiv.org/pdf/2203.02658v2,Koopman operator for time-dependent reliability analysis,"FNN and LSTM, on the other hand fails to capture the Ô¨Årst mode of the PDF for both the cases. For further examination, the PDF plots of Ô¨Årst passage failure time is also obtained for four diÔ¨Äerent sets
of test samples chosen from diÔ¨Äerent distribution (out-of-distribution). While the Ô¨Årst two sets of test
samples are generated in the support interval of x ‚àº (‚àí5, 5) and xÀô ‚àº (0, 10) having distributions Gaussian
and log-normal, the other two sets of test samples are generated in the support interval of x ‚àº (‚àí6, 6)

                                                                      7
         Koopman operator for time-dependent reliability analysis                                    A Preprint

                                   20

            Prediction error, (%)  15

                                   10

                                   5

                                   0

                                   200    400  600     800          1600  1800

                                               No. ",stat.ML,A,0.080728374,0.121058784,-0.21442929
http://arxiv.org/pdf/2203.02954v1,"On the importance of stationarity, strong baselines and benchmarks in transport prediction problems","a step in this direction, we provide a centralized repository2
with code for preparing the experimental setups (train/val/test                                 REFERENCES
split, forecasting horizons, evaluation metrics, etc.) for the 9
publicly-available datasets considered in this paper, so that       [1] B. Yu, H. Yin, and Z. Zhu, ‚ÄúSpatio-temporal graph convolutional
future works can easily compare their approaches with the                networks: A deep learning framework for trafÔ¨Åc forecasting,‚Äù in 27th
results from Tables II to X. International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-18),
                                                                         2018. ",stat.ML,C,-0.38488594,-0.1775227,0.043425955
http://arxiv.org/pdf/2203.03116v1,Kernel Packet: An Exact and Scalable Algorithm for Gaussian Process Regression with Mat√©rn Correlations,"(2020)
to develop approximated algorithms, which work for not only regression problems, but also for
other type of supervised learning tasks. Another direction for future work is to estabilish the relationship between KP and the state-
space approaches. These methods leverage the Gauss-Markov process representation of cer-
tain GPs, including Mat√©rn-type GPs with half-integer smoothness, and employs the Kalman

 ltering and related methodologies to handle GP regression, which results in a learning al-
gorithm with time and space complexity both in ( ) (Hartikainen and S√§rkk√§, 2010; Saat√ßi,
2012; Sarkka et al., 2013; Loper et al., 2021). ",stat.ML,C,-0.11215775,-0.16104656,-0.10915076
http://arxiv.org/pdf/2203.03116v2,Kernel Packet: An Exact and Scalable Algorithm for Gaussian Process Regression with Mat√©rn Correlations,"(2020)
to develop approximated algorithms, which work for not only regression problems, but also for
other type of supervised learning tasks. Another direction for future work is to establish the relationship between KP and the state-
space approaches. The latter methods leverage the Gauss-Markov process representation of cer-
tain GPs, including Mat√©rn-type GPs with half-integer smoothness, and employ the Kalman

 ltering and related methodologies to handle GP regression, which results in a learning al-
gorithm with time and space complexity both in ( ) (Hartikainen and S√§rkk√§, 2010; Saat√ßi,
2012; Sarkka et al., 2013; Loper et al., 2021). ",stat.ML,C,-0.114138395,-0.17409256,-0.1106122
http://arxiv.org/pdf/2203.03475v1,State space partitioning based on constrained spectral clustering for block particle filtering,"Although the rule of thumb [29] can give an order of magnitude for the block size given a
number of particles, an open question that remains to be solved is how to exactly determine
the optimal number of blocks. A possible direction for future work consists in coupling the
present contribution with the work in [28] that leverages a parallelisation scheme to optimize
the deployment of the block particle Ô¨Ålter. In particular, one could use Ô¨Ålters with diÔ¨Äerent
numbers of blocks in parallel. ",stat.ML,B,0.17685866,0.06516135,0.14631754
http://arxiv.org/pdf/2203.03597v1,Fast rates for noisy interpolation require rethinking the effects of inductive bias,"Theorems 3 and 4, strongly relies on the noise model and we leave the extension of our
results to low noise settings (i.e. vanishing fraction of Ô¨Çipped labels) as an interesting future work. 16
B.2 Comparison with existing bounds

We now discuss how our proof of Theorem 1 diÔ¨Äers from the proof of Theorem 2 in [KZSS21] . ",stat.ML,B,0.40653917,0.0045526996,0.046392217
http://arxiv.org/pdf/2203.03597v2,Fast Rates for Noisy Interpolation Require Rethinking the Effects of Inductive Bias,"Proceedings of the Conference on Information
data with the CNTK in Figure 4, we further hypothesize               Sciences and Systems, pp. 16‚Äì21, 2008.
that this trade-off carries over to more complex interpolating
models and datasets used in practice and leave a thorough          Chatterji, N. S. and Long, P. M. Foolish crowds support be-
investigation as future work. nign overÔ¨Åtting. ",stat.ML,A,-0.15329903,0.06788343,-0.04007339
http://arxiv.org/pdf/2203.03673v1,AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators,"It should not be prohibitive to incorporate
exogenous features in the statistics t(x) in AgraSSt, for example using ideas
from graph attention networks [VeliÀáckovi¬¥c et al., 2018]. Exploring this idea
in more detail will be another topic of further research. 21
Acknowledgements

G.R. ",stat.ML,A,-0.14962147,0.15399244,0.089606956
http://arxiv.org/pdf/2203.03673v2,AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators,"The theoretical behaviour of the subsampling version AgraSSt(q(x(s)|t(x‚àís))) is addressed in
Proposition 3.4. A detailed examination of the choice of kernel K such that the assumptions of

Corollary A.6 are satisÔ¨Åed is left for future work. B Additional background

In this section, we present additional background to complement the discussions in the main text. ",stat.ML,B,0.34109575,-0.01244445,0.0772358
http://arxiv.org/pdf/2203.03673v3,AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators,"The theoretical behaviour of the subsampling version AgraSSt(q(x(s)|t(x‚àís))) is addressed in
Proposition 3.4. A detailed examination of the choice of kernel K such that the assumptions of

Corollary A.6 are satisÔ¨Åed is left for future work. B Additional background

In this section, we present additional background to complement the discussions in the main text. ",stat.ML,B,0.34109575,-0.01244445,0.0772358
http://arxiv.org/pdf/2203.03791v1,Data adaptive RKHS Tikhonov regularization for learning kernels in operators,"Moreover, when the data is noiseless, RKHS regularizer has rates close to 1 for both kernels, while
the other two regularizers rates are not consistent. 6 Discussion and future work

We have proposed a data adaptive RKHS Tikhonov regularization (DARTR) method for the non-
parametric learning of kernel functions in operators. The DARTR method regularizes the least
squares regression by the norm of a system intrinsic and data adaptive (SIDA) RKHS, which con-
straints the learning to the function space of identiÔ¨Åability. ",stat.ML,C,-0.035608977,-0.34269667,-0.08349695
http://arxiv.org/pdf/2203.04769v1,Autoregressive based Drift Detection Method,"The
higher is the severity, the less relevant is the old model. In future works we aim to use auto-regressive models to detect
concept drifts using directly the input data instead of the error rate or the model‚Äôs uncertainty. This can be very useful in
cases where the true labels are not available. ",stat.ML,A,-0.25672674,0.15200287,-0.12854296
http://arxiv.org/pdf/2203.05561v1,Deep Learning for the Benes Filter,"Further we present the Ô¨Årst study of the neural
                                                  network method with an adaptive domain for the Benes model. Keywords: Nonlinear Filtering, Deep Learning, Stochastic PDE Ap-
                                                  proximation

                                         1 Introduction

                                         In this paper we present a further study of the deep learning method devel-
                                         oped in [4] on the example of the Benes Ô¨Ålter. The algorithm is derived from
                                         the splitting method for SPDEs and replaces the PDE approximation step by
                                         a neural network representation and learning algorithm. ",stat.ML,C,-0.16008282,-0.3273276,0.0028826408
http://arxiv.org/pdf/2203.05813v1,Averaging Spatio-temporal Signals using Optimal Transport and Soft Alignments,"These updates lead to Algorithm 4. While the theoretical analysis of its convergence is left for future work, we empirically observe

that it converges regardless of the initialization of the dual variables. More importantly,

it leads sharper barycenter than the (biased) UOT barycenters for almost no additional

computational cost. ",stat.ML,B,0.18041891,-0.15035847,0.035592146
http://arxiv.org/pdf/2203.05813v2,Averaging Spatio-temporal Signals using Optimal Transport and Soft Alignments,"These updates lead to Algorithm 4. While the theoretical analysis of its convergence is left for future work, we empirically observe

that it converges regardless of the initialization of the dual variables. More importantly,

it leads sharper barycenter than the (biased) UOT barycenters for almost no additional

computational cost. ",stat.ML,B,0.18041891,-0.15035847,0.035592146
http://arxiv.org/pdf/2203.05965v1,Human-Like Navigation Behavior: A Statistical Evaluation Framework,"Nevertheless, as humans tend to be ‚Äúcreatures of habit‚Äù,         ended learning leads to generally capable agents,‚Äù arXiv preprint
we conjecture that analyzing distributions of behavior through           arXiv:2107.12808, 2021.
the lens of Ô¨Åxed-length time horizons could yield further
insights into the driving factors of human-like behavior. We        [4] B. Soni and P. Hingston, ‚ÄúBots trained to play like a human are more
leave this exploration as well as extensions into domains such           fun,‚Äù in 2008 IEEE International Joint Conference on Neural Networks
as reward design or imitation learning for future work. (IEEE World Congress on Computational Intelligence), pp. ",stat.ML,C,-0.17555971,-0.0124729015,0.049344774
http://arxiv.org/pdf/2203.06481v1,GATSBI: Generative Adversarial Training for Simulation-Based Inference,"A.2 and the discussion above). Whether this would also
be possible with the Wasserstein loss remains a subject for future work. Nevertheless, the sequential
extension of GATSBI using the energy-based correction (see Sec. ",stat.ML,B,0.17687213,-0.029510282,0.086546384
http://arxiv.org/pdf/2203.06648v1,The Yield Curve as a Recession Leading Indicator. An Application for Gradient Boosting and Random Forest,"of this term spread and greater than the average value of M3-M6
term spread there is over six times more probability of economic                 Variable Correlated Correlation
recession than the probability of economic recession for the
complementary conditions"". Y1-Y10   Y20-M6                      -0,98

   As a future work suggestion, several paths can be followed. On one            Y20-M6   Y1-Y10                      -0,98
accuracy side, the improvement of the model predictive accuracy is
relevant to have tools with high quality and impact on predicting this           Y1-Y7    Y20-M6                      -0,97
phenomenon. ",stat.ML,A,-0.12287812,0.27072266,-0.0996503
http://arxiv.org/pdf/2203.07136v1,On the Nash equilibrium of moment-matching GANs for stationary Gaussian processes,"The discriminator deÔ¨Åned from second-order statistical moments
                                                  can result in non-existence of Nash equilibrium, existence of consistent non-Nash equilibrium,
                                                  or existence and uniqueness of consistent Nash equilibrium, depending on whether symmetry
                                                  properties of the generator family are respected. We further study the local stability and global
                                                  convergence of gradient descent-ascent methods towards consistent equilibrium. Keywords: GANs, Nash equilibrium, moment-matching, stationary process, statistical consis-
                                          tency

                                          1 Introduction

                                          Estimating the probability distribution of data samples is a classical problem in statistics and ma-
                                          chine learning. ",stat.ML,B,0.116815194,-0.15882894,-0.23600051
http://arxiv.org/pdf/2203.07136v2,On the Nash equilibrium of moment-matching GANs for stationary Gaussian processes,"The discriminator deÔ¨Åned from second-order statistical moments can result in
                                               non-existence of Nash equilibrium, existence of consistent non-Nash equilibrium, or existence and
                                               uniqueness of consistent Nash equilibrium, depending on whether symmetry properties of the gen-
                                               erator family are respected. We further study empirically the local stability and global convergence
                                               of gradient descent-ascent methods towards consistent equilibrium. Keywords: GANs, Nash equilibrium, moment-matching, stationary process, statistical consistency

                                         1. ",stat.ML,B,0.18930593,-0.06727208,-0.08341344
http://arxiv.org/pdf/2203.07136v3,On the Nash equilibrium of moment-matching GANs for stationary Gaussian processes,"The discriminator deÔ¨Åned from second-order statistical moments can result in
                                                non-existence of Nash equilibrium, existence of consistent non-Nash equilibrium, or existence and
                                                uniqueness of consistent Nash equilibrium, depending on whether symmetry properties of the gen-
                                                erator family are respected. We further study empirically the local stability and global convergence
                                                of gradient descent-ascent methods towards consistent equilibrium. Keywords: GANs, Nash equilibrium, moment-matching, stationary process, statistical consistency

                                          1. ",stat.ML,B,0.18930593,-0.06727208,-0.08341344
http://arxiv.org/pdf/2203.07337v1,Phenomenology of Double Descent in Finite-Width Neural Networks,"However, the input-
covariance matrix in least-squares is also the Hessian, and as we have thoroughly established the
Hessian (at the optimum) is indeed the relevant object ‚Äî thus contradicting their conjecture. Limitations and directions for future work. There are many important aspects surrounding
double descent that remain unanswered, in the context of neural networks: (a) Analogous to (Hastie
et al., 2019) for linear regression, what are the conditions for the global optimum to lie in the over-
parameterized regime instead of under-parameterized? ",stat.ML,B,0.089838415,-0.20856331,-0.010695477
http://arxiv.org/pdf/2203.07798v1,Igeood: An Information Geometry Approach to Out-of-Distribution Detection,"It is observed that IGEOOD signiÔ¨Åcantly and
consistently improves the accuracy of OOD detection on several DNN architectures across various
datasets for a WHITE-BOX setting. Some perspectives for future work include studying causal
factors, explainable components for OOD detection, and extensions to textual data. 9
Published as a conference paper at ICLR 2022

ACKNOWLEDGMENTS

This work has been supported by the project PSPC AIDA: 2019-PSPC-09 funded by BPI-France. ",stat.ML,C,-0.31302214,-0.028521234,0.14411266
http://arxiv.org/pdf/2203.07889v1,Comparing two samples through stochastic dominance: a graphical approach,"Although CD does not converge to CD, for any > 0 we can Ô¨Ånd a Œ¥ small enough such that the

                                         Œ¥

diÔ¨Äerence between CD and CD is smaller than . We formalize this claim in Conjecture 1, and
we leave the proof for future work. DeÔ¨Ånition 10. ",stat.ML,B,0.33512324,0.30023512,0.20629714
http://arxiv.org/pdf/2203.07889v2,Comparing two samples through stochastic dominance: a graphical approach,"Although CD does not converge to CD, for any > 0 we can Ô¨Ånd a Œ¥ small enough such that the

                                         Œ¥

diÔ¨Äerence between CD and CD is smaller than . We formalize this claim in Conjecture 1, and
we leave the proof for future work. DeÔ¨Ånition 10. ",stat.ML,B,0.33512324,0.30023512,0.20629714
http://arxiv.org/pdf/2203.07889v3,Comparing two samples through stochastic dominance: a graphical approach,"Although CD does not converge to CD, for any > 0 we can Ô¨Ånd a Œ¥ small enough such that the

                                         Œ¥

diÔ¨Äerence between CD and CD is smaller than . We formalize this claim in Conjecture 1, and
we leave the proof for future work. DeÔ¨Ånition 10. ",stat.ML,B,0.33512324,0.30023512,0.20629714
http://arxiv.org/pdf/2203.07889v4,Comparing Two Samples Through Stochastic Dominance: A Graphical Approach,"Although CD does not converge to CD, for any > 0 we can Ô¨Ånd a Œ¥ small enough such that the

                                         Œ¥

diÔ¨Äerence between CD and CD is smaller than . We formalize this claim in Conjecture 1, and
we leave the proof for future work. DeÔ¨Ånition 10. ",stat.ML,B,0.33512324,0.30023512,0.20629714
http://arxiv.org/pdf/2203.08644v1,Context-Aware Drift Detection,"In                             representations of probability measures. In Cortes,
future work we intend to further explore how certain com-                                 C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and
binations of contexts and statistics may be used to target                                Garnett, R. (eds. ",stat.ML,A,0.04838717,0.19827303,-0.008088719
http://arxiv.org/pdf/2203.08644v2,Context-Aware Drift Detection,"Covariate drift into a mode existing in the                           problems to which drift detectors can be usefully applied. In
reference set would therefore be permitted whereas covari-                             future work we intend to further explore how certain com-
ate drift into a newly emerging concept would be detected. binations of contexts and statistics may be used to target
We use the ImageNet (Deng et al., 2009) class structure                                certain types of drift, such as covariate drift into regions of
developed by Santurkar et al. ",stat.ML,A,-0.13398153,0.093336776,0.013954449
http://arxiv.org/pdf/2203.08887v1,On Redundancy and Diversity in Cell-based Neural Architecture Search,"By relaxing the constraints on how
   the different cells may be connected, there is not only a potential for a much larger and expressive
   search space, but also be a greater variability in overall graph theoric topological properties (as
   opposed to topological properties within cells only) and widths/depths of the resulting networks,
   which have been shown to signiÔ¨Åcantly inÔ¨Çuence the performance (Xie et al., 2019a; Ru et al.,
   2020). We leave the detailed speciÔ¨Åcation of such a search space to a future work, but a concrete
   direction would be to further investigate and improve on the hierarchical search spaces such as the
   one proposed in Ru et al. (2020). ",stat.ML,C,-0.07160972,0.010814302,0.36808056
http://arxiv.org/pdf/2203.09410v1,A Framework and Benchmark for Deep Batch Active Learning for Regression,"Note that due to the summation, it is important that the
(posterior) kernel is scaled correctly. We leave an experimental evaluation of this approach to future work. 30
       A Framework and Benchmark for Deep Batch Active Learning for Regression                                       A PREPRINT

C.2 Random Projections

In the following, we prove the variant of the Johnson-Lindenstrauss theorem mentioned in Section 4.2. ",stat.ML,C,-0.015980411,-0.34156018,-0.117072836
http://arxiv.org/pdf/2203.09410v2,A Framework and Benchmark for Deep Batch Active Learning for Regression,"Note
that due to the summation, it is important that the (posterior) kernel is scaled correctly. We leave an experimental evaluation of this approach to future work. C.2 Random Projections

In the following, we prove the variant of the Johnson-Lindenstrauss theorem mentioned in
Section 4.2.3. ",stat.ML,B,0.30130255,-0.2518843,-0.053823456
http://arxiv.org/pdf/2203.09413v1,Stability and Risk Bounds of Iterative Hard Thresholding,"In view of the stan-
dard OÀú n‚àí1/2‚àöp uniform convergence bound for dense models (see, e.g., [49]), it is more or less
straightforward to derive a generalization bound of order OÀú n‚àí1/2 k log(p) for IHT which is ap-
plicable to the non-convex regime. In view of the recent progresses achieved towards understanding
the beneÔ¨Åt of overparametrization for the optimization and generalization of gradient-based deep
learning algorithms [3, 46], it is interesting to further study the impact of overparametrization on
the generalization performance of IHT-style algorithms for deep learning with sparsity. References

 [1] Felix Abramovich and Vadim Grinshtein. ",stat.ML,B,0.14226273,-0.3612281,-0.079913855
http://arxiv.org/pdf/2203.09675v1,Fast Bayesian Coresets via Subsampling and Quasi-Newton Refinement,"SVI
                                                                      entails. We defer this study to future work. Figure 3. ",stat.ML,A,-0.071455665,0.20521235,0.17052296
http://arxiv.org/pdf/2203.09675v2,Fast Bayesian Coresets via Subsampling and Quasi-Newton Refinement,"(18) holds. We leave the theoretical examination of this claim
for future work. B Proofs

In this section we give the full proofs of Theorems 4.1 to 4.3. ",stat.ML,B,0.4402846,0.08729671,0.2827754
http://arxiv.org/pdf/2203.09751v1,Look-Ahead Acquisition Functions for Bernoulli Level Set Estimation,"In Advances in Neural Information Pro-
with the acquisition functions developed here, there          cessing Systems 29, NIPS, pages 1507‚Äì1515. remain several important areas of future work. First,
in acquisition function development: There is no single    Brier, G. W. (1950). ",stat.ML,C,-0.17072855,-0.029231137,0.012662016
http://arxiv.org/pdf/2203.10975v1,GCF: Generalized Causal Forest for Heterogeneous Treatment Effect Estimation in Online Marketplace,"The
            ùúéùë• (ùúãÀÜ) = (ùúãÀÜ (ùë° |ùë•) ‚àí ùúã (ùë° |ùë•))2 ¬∑ ùëì (ùë°, ùë•)ùëëùë• ‚Üí 0                    proof should be a combination of stochastic analysis of the stochastic
                                                                                  process ùúÉùë• (ùë°) and large-scale approximations. This goes beyond the
                              ùë•                                                   scope of this paper and we defer the proof to future work. ‚àö ‚àöÔ∏Å ‚àöÔ∏Å ùëù
              ùëõ‚Ñé ùúéùë• (ùúáÀÜ) ¬∑ ùúéùë• (ùúãÀÜ) ‚Üí 0

          ùëù                                                                       4.4 Practical Considerations

where ‚Üí means convergence in probability. ",stat.ML,B,0.3818732,0.10750248,-0.029277453
http://arxiv.org/pdf/2203.11276v1,Model Comparison in Approximate Bayesian Computation,"However, the authors further remark that although the
existing methods are usually applied to problems with insuÔ¨Écient statistics, they seem
to select the correct models. This indicates that the approach is not untrustworthy
per se, but that it requires further research on methods to approximate the posterior
probabilities of models (Robert et al., 2011). 2.3 Summary

I presented an introduction to Bayesian model comparison in the context of ABC. ",stat.ML,A,-0.029666618,0.3201561,-0.2517437
http://arxiv.org/pdf/2203.11363v1,PI-VAE: Physics-Informed Variational Auto-Encoder for stochastic differential equations,"Also, we now assume
that all sensors will give us accurate information while, in practice, measurement noises exist and should be accounted
for. Finally, even though our method achieved better accuracy levels compared to PI-WGAN in all the cases, in a
few of numerical experiments presented in the paper, the performance levels can still be improved in further research
extensions. Future research directions also include quantifying the the sensor uncertainty as well as the modeling
uncertainty into the framework. ",stat.ML,A,0.0037257213,0.04045725,-0.17836612
http://arxiv.org/pdf/2203.11377v1,Sequential algorithmic modification with test data reuse,"In
                                                                 addition, model developers are often interested in obtain-
                                                                 ing more detailed test results like p-values and conÔ¨Ådence
                                                                 intervals. Another direction for future work is to design
                                                                 SRGPs that release more information per iteration, perhaps
                                                                 by leveraging differential privacy techniques. Acknowledgements                                                 Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann
                                                                    Pitassi, Omer Reingold, and Aaron Roth. ",stat.ML,A,-0.16887584,0.18369725,-0.0024755932
http://arxiv.org/pdf/2203.11528v1,Out-of-distribution Generalization with Causal Invariant Transformations,"However, generating such causally invariant sentences via deep learning
method is not easy. How to ease the generation and apply our RICE to NLP domain is left
as a future work. References

 [1] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. ",stat.ML,C,-0.3784291,-0.029091105,0.29667783
http://arxiv.org/pdf/2203.11528v2,Out-of-distribution Generalization with Causal Invariant Transformations,"However, generating such causally invariant sentences via deep learning
method is not easy. How to ease the generation and apply our RICE to NLP domain is left
as a future work. References

 [1] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. ",stat.ML,C,-0.3784291,-0.029091105,0.29667783
http://arxiv.org/pdf/2203.11528v3,Out-of-distribution Generalization with Causal Invariant Transformations,"However, generating such causally invariant sentences via deep learning
method is not easy. How to ease the generation and apply our RICE to NLP domain is left
as a future work. References

 [1] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. ",stat.ML,C,-0.3784291,-0.029091105,0.29667783
http://arxiv.org/pdf/2203.11627v1,Bounds on Wasserstein distances between continuous distributions using independent samples,"However,
uncertainty quantiÔ¨Åcation for our estimator L of the lower bound (2) is hampered by a
lack of satisfactory limit theorems for the unsquared Wasserstein distance. We leave such

                                                     23
limit theorems for future work, and suggest the use of Chebyshev conÔ¨Ådence intervals
instead. 5.2 MCMC

Our bounds can be used to understand the convergence properties of MCMC algorithms
sampling continuous distributions, by running multiple chains which start and remain
overdispersed with respect to their stationary distributions. ",stat.ML,B,0.31011575,0.1009579,-0.27176395
http://arxiv.org/pdf/2203.11872v1,Performance of long short-term memory artificial neural networks in nowcasting during the COVID-19 crisis,"In empirical
testing, this rescaling factor is almost always close to 1. This method should be considered experimental, and further research is necessary to
examine additional and more robust ways of implementing interpretability within the
LSTM framework. Further information on usage and methodology is available on the
repository‚Äôs page: https://github.com/dhopp1/nowcast_lstm. ",stat.ML,A,-0.21699323,0.059514113,0.031800743
http://arxiv.org/pdf/2203.12136v1,Wasserstein Distributionally Robust Optimization via Wasserstein Barycenters,"This is as opposed to the theoretical rate of
O(n‚àí1/2). It is interesting to Ô¨Ånd the optimal scaling of Œµ in our proposed Wasserstein Barycentric Shrinkage

Estimator in terms of both n and K, and is left for future work. 22 ",stat.ML,B,0.36856613,-0.04666353,-0.22158381
http://arxiv.org/pdf/2203.12136v2,Wasserstein Distributionally Robust Optimization with Wasserstein Barycenters,"This is as opposed to the theoretical rate of O(n‚àí1/2). It is interesting to Ô¨Ånd the optimal scaling of Œµ in our
proposed Wasserstein Barycentric Shrinkage Estimator in terms of both n and K, and is left for future work. E Simulations under High-Dimensional Setting

In this section, we study the performance of the proposed Sinkhorn barycentric shrinkage estimator (see
Appendix D.4) under the high-dimensional setting (m > n). ",stat.ML,B,0.30718476,-0.06078312,-0.24458525
http://arxiv.org/pdf/2203.12297v1,Increasing the accuracy and resolution of precipitation forecasts using deep generative models,"the local and surrounding weather situation. Ebert-UphoÔ¨Ä, I., Lagerquist, R., Hilburn, K., Lee, Y.,
In future work, one could include recent radar obser-         Haynes, K., Stock, J., Kumler, C., and Stewart,
vations in the input to the GAN, giving the model             J. Q. (2021). ",stat.ML,A,-0.1393664,0.082503304,0.03353445
http://arxiv.org/pdf/2203.12513v1,Sampling Theorems for Unsupervised Learning in Linear Inverse Problems,"Moreover, the bounds are agnostic of the learning algorithms and provide
useful necessary and suÔ¨Écient conditions for designing principled sensing strategies. We leave the study of robustness to noise as well as the extension of the theory for future work. Another interesting avenue of research is to extend our results to more general semigroups, whose
elements do not necessarily have an inverse, but can be useful to capture certain symmetries in the
signal model such as invariance to scale [44, 45]. ",stat.ML,B,0.17687145,-0.26384705,-0.025140725
http://arxiv.org/pdf/2203.12513v2,Sensing Theorems for Unsupervised Learning in Linear Inverse Problems,"An interesting avenue of research is to extend the bounds in this paper to more general classes of
signal models, including compressible distributions [Gribonval et al., 2012], e.g., Bernoulli-Gaussian
or heavy tailed models, and other approximately low-dimensional models, e.g., models whose support
is close to a low-dimensional set. We leave the study of robustness to noise as well as the extension
of the theory for future work. Another interesting avenue of research is to extend our results to more
general semigroups, whose elements do not necessarily have an inverse, but can be useful to capture
certain symmetries in the signal model such as invariance to scale [Florack et al., 1994; Worrall and
Welling, 2019]. ",stat.ML,B,0.24847043,-0.10395537,-0.029192977
http://arxiv.org/pdf/2203.13417v1,Amortized Projection Optimization for Sliced Wasserstein Generative Models,"There are still several other ways of parameterization that can be utilized
in practice based on prior knowledge about data, e.g., we can use convolution operator for saving
parameters or we can strengthen the dependence between samples via recursive functions. We leave
the design of these amortized models for future work. 3.2 Amortized Sliced Wasserstein Generative Models

Based on the amortized sliced Wasserstein losses, our objective function for training a generative
model ŒΩœÜ parametrized by œÜ ‚àà Œ¶ now becomes:

min max E(X,YœÜ)‚àº¬µ‚äóm‚äóŒΩ‚äóm [Wp(fœà(X, YœÜ) PX , fœà(X, YœÜ) PYœÜ )] := min max L(¬µ, ŒΩœÜ, œà). ",stat.ML,C,-0.10812065,-0.19656998,-0.06988346
http://arxiv.org/pdf/2203.13417v2,Amortized Projection Optimization for Sliced Wasserstein Generative Models,"The hardness in designing amortized models is that we need to trade-off between
the performance and computational efÔ¨Åciency. We will leave these questions to future work. 21 ",stat.ML,B,0.0579388,0.045230325,0.12868328
http://arxiv.org/pdf/2203.13417v3,Amortized Projection Optimization for Sliced Wasserstein Generative Models,"The hardness in designing amortized models is that we need to trade-off between
the performance and computational efÔ¨Åciency. We will leave these questions to future work. 21 ",stat.ML,B,0.0579388,0.045230325,0.12868328
http://arxiv.org/pdf/2203.13417v4,Amortized Projection Optimization for Sliced Wasserstein Generative Models,"The hardness in designing amortized models is that we need to trade-off between
the performance and computational efÔ¨Åciency. We will leave these questions to future work. 22 ",stat.ML,B,0.06332097,0.053373642,0.1350047
http://arxiv.org/pdf/2203.13779v1,Origins of Low-dimensional Adversarial Perturbations,"The uniformity assumption on the distribution of the data in the positive-decision region can probably
be replaced by assuming that the distribution of X conditioned on X ‚àà C has density which is
bounded-away from zero. This extension is left for future work. Figure 4: Universal adversarial attacks for compact decision-region. ",stat.ML,B,0.09805323,-0.1078912,-0.09564608
http://arxiv.org/pdf/2203.13779v2,Origins of Low-dimensional Adversarial Perturbations,"We conjecture that the uniformity assumption on the distribution of the data in the positive-decision
region can probably be replaced by assuming that the distribution of X conditioned on X ‚àà C has
density which is bounded-away from zero. This extension is left for future work. 3.4 Empirical veriÔ¨Åcation

Smooth decision-regions. ",stat.ML,B,0.25794432,-0.0024912693,-0.19433716
http://arxiv.org/pdf/2203.13787v1,A Hybrid Framework for Sequential Data Prediction with End-to-End Optimization,"We demonstrate the learning behavior of our algorithm on
                                          synthetic data and the signiÔ¨Åcant performance improvements over the con-
                                          ventional methods over various real life datasets. Furthermore, we openly
                                          share the source code of the proposed method to facilitate further research. Keywords: feature extraction, end-to-end learning, online learning,
                                          prediction, long short-term memory (LSTM), soft gradient boosting
                                          decision tree (sGBDT). ",stat.ML,C,-0.38563582,-0.29200923,-0.0317986
http://arxiv.org/pdf/2203.13787v2,A Hybrid Framework for Sequential Data Prediction with End-to-End Optimization,"We demonstrate the learning behavior of our algorithm on
                                         synthetic data and the signiÔ¨Åcant performance improvements over the con-
                                         ventional methods over various real life datasets. Furthermore, we openly
                                         share the source code of the proposed method to facilitate further research. Keywords: feature extraction, end-to-end learning, online learning,
                                         prediction, long short-term memory (LSTM), soft gradient boosting
                                         decision tree (sGBDT). ",stat.ML,C,-0.38563582,-0.29200923,-0.0317986
http://arxiv.org/pdf/2203.14893v1,Probabilistic Spherical Discriminant Analysis: An Alternative to PLDA for length-normalized embeddings,"results with using PCA and LDA dimensionality reduction to
better Ô¨Åt the data to the model. For future work, we are inter-                           6. Conclusion
ested in modifying the embedding extractor training criterion to
combat such collapse and instead encourage a uniform hyper-            In speaker recognition state of the art (as in many other ma-
sphere between-speaker distribution. ",stat.ML,C,-0.09770894,-0.2797306,0.018619915
http://arxiv.org/pdf/2203.15009v1,DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series,"This paper is kept as simple as possible to make the ideas behind DAMNETS transparent. There are many extensions
which will be part of future work. To name but a few, including edge weights and signed edges is fairly straightforward. ",stat.ML,B,0.023341026,0.06918934,0.40821296
http://arxiv.org/pdf/2203.15236v1,Best Arm Identification in Restless Markov Multi-Armed Bandits,"observations from each arm. An extension of the results of [23]‚Äì[25] to the settings of rested and restless
arms is a possible direction of future work. D. Contributions
   In this paper, we study the problem of Ô¨Ånding the best arm in a restless multi-armed bandit as quickly as possible,

subject to an upper bound on the error probability. ",stat.ML,A,0.06985776,0.181564,-0.20949724
http://arxiv.org/pdf/2203.16437v1,Weakly supervised causal representation learning,"(2020). We plan to study this extension in future work. Real-valued causal variables Theorem 1 assumes real-valued causal and noise variables, Zi =
Ei = R. We can easily extend this to intervals (a, b) ‚àà R, as these are isomorphic to R. However, the
extension to arbitrary continuous spaces or Rn is not straightforward. ",stat.ML,B,0.098769285,0.27025688,0.27735364
http://arxiv.org/pdf/2203.16437v2,Weakly supervised causal representation learning,"[8]. We plan to study this extension in future work. Real-valued causal variables Theorem 1 assumes real-valued causal and noise variables, Zi =
Ei = R. We can easily extend this to intervals (a, b) ‚àà R, as these are isomorphic to R. However, the
extension to arbitrary continuous spaces or Rn is not straightforward. ",stat.ML,B,0.11845928,0.26840067,0.28696162
http://arxiv.org/pdf/2203.16437v3,Weakly supervised causal representation learning,"For up to around 10 causal
variables, our method lets us disentangle the causal variables reliably, and the graphs found by ILCM
are more accurate than those found by the baseline. Scaling ILCMs to even larger system will require
additional research. E Explicit latent causal models

E.1 Setup

Explicit latent causal models (ELCMs) are variational autoencoders, in which the latent variables are
the causal variables of an SCM. ",stat.ML,C,-0.342548,0.14027049,0.17800304
http://arxiv.org/pdf/2203.16662v2,Overcoming challenges in leveraging GANs for few-shot data augmentation,"Similar to our procedure, these hallucinated latent samples are combined with real latent
samples to make one larger dataset. In principle, our method could be adapted to generate latent codes (with respect
to a latent distribution), but this is left to future work. Lastly, while we do not consider meta-learning here, our few-
shot pipeline could be framed as an end-to-end approach under that paradigm. ",stat.ML,C,-0.32841286,-0.10553691,0.10184725
http://arxiv.org/pdf/2203.16662v3,Overcoming challenges in leveraging GANs for few-shot data augmentation,"The variance exhibited between different dataset seeds is of concern, because it reduces
the practical signiÔ¨Åcance of some of the Ô¨Åndings. For future work, we suggest building on top of semi-supervised
training with more stable and recent generative model classes, such as score-based or diffusion models. 10
arXiv preprint, submitted to unspeciÔ¨Åed conference

REFERENCES

Antreas Antoniou, Amos Storkey, and Harrison Edwards. ",stat.ML,C,-0.27965122,-0.14783582,-0.089656614
http://arxiv.org/pdf/2204.00150v1,DBCal: Density Based Calibration of classifier predictions for uncertainty quantification,"We       ‚Äùtrials‚Äù with the same inputs and recording the dis-
conclude with a summary of this methodology and           tribution of the output predictions. The variance of
discussion on future work. those outputs is computed and then used as the un-
                                                          certainty of the prediction. ",stat.ML,A_centroid,-0.17169008,0.2987052,-0.23105821
http://arxiv.org/pdf/2204.00296v1,Scalable Semi-Modular Inference with Variational Meta-Posteriors,"As this isn‚Äôt known in
advance, concentrating them near the points Œ∑1:R ‚àà {0, 1}R is a useful rule as Œª‚àó(Œ∑)
often varies rapidly with Œ∑ near Cut and Bayes. Adaptive sequential estimation and
maximisation of u(Œ∑) may be of interest in future work. We take a meta-SMI loss weighted across Œ∑ ‚àà Œ∑1:R. For the VMP-map this is

                      L(msmi ‚àímap)(Œ±) = EŒ∑‚àºœÅ L(smi,Œ∑)(fŒ±(Œ∑)               (4.5)
                                           = R1 R L(smi,Œ∑r)(fŒ±(Œ∑r))

                                                               r=1

where L(smi,Œ∑)(Œª) is deÔ¨Åned in eq. ",stat.ML,B,0.18623143,-0.06791194,-0.072517216
http://arxiv.org/pdf/2204.00778v1,Distributional Gradient Boosting Machines,"Short-Term Probabilistic Load
                                                               Forecasting using Conditioned Bernstein-Polynomial
   Despite its Ô¨Çexibility, we acknowledge some limitations     Normalizing Flows. of our framework that require additional research. Even
though distributional modelling relaxes the assumption      Arpogaus, M., Voss, M., Sick, B., Nigge-Uricher, M.,
of observations being identically distributed, tree-based      and Du¬®rr, O. ",stat.ML,C,-0.06534782,-0.013863165,-0.16124952
http://arxiv.org/pdf/2204.00887v1,Dimensionless machine learning: Imposing exact units equivariance,"In the approach advocated
here, we have no way to learn or discover missing dimensional constants. That is a limitation
of our approaches, and motivates future work. 8. ",stat.ML,B,0.036420718,-0.11231844,0.26560766
http://arxiv.org/pdf/2204.02291v1,Aggregating distribution forecasts from deep ensembles,"Compared to deep
ensembles based on random initialization, a potential advantage of dropout-based ensembles
is that the lower computational costs make the generation of larger ensembles more feasible. The aggregation methods we investigated are agnostic to the generation of the ensembles of
distribution forecasts provided that they can be considered as realizations of the same basic
type of model, and are thus readily applicable to dropout-based ensembles.8 Therefore, an
interesting avenue for future work is to investigate the performance of the combination methods
for diÔ¨Äerent approaches to generate NN-based probabilistic forecasts, for example within the
framework of comprehensive simulation testbeds (Osband et al., 2021). Finally, we summarize three key recommendations for aggregating distribution forecasts from
deep ensembles based on our results. ",stat.ML,C,-0.21293049,-0.08951472,-0.22089034
http://arxiv.org/pdf/2204.02583v1,PAGP: A physics-assisted Gaussian process framework with active learning for forward and inverse problems of partial differential equations,"In particular, investigating the impact of diÔ¨Äerent time discretization scheme
to the performance of discrete and hybrid PAGP models can be interesting. Other
potential future works include investigating theoretical concepts like prior consistency
and posterior robustness for our models. Acknowledgement

    We gratefully acknowledge the support from the National Science Foundation (DMS-
1555072, DMS-1736364, and DMS-2053746), and Brookhaven National Laboratory
Subcontract 382247, and U.S. Department of Energy (DOE) OÔ¨Éce of Science Ad-
vanced ScientiÔ¨Åc Computing Research program DE-SC0021142. ",stat.ML,B,0.103275515,0.04102722,-0.064919546
http://arxiv.org/pdf/2204.03123v1,"A novel nonconvex, smooth-at-origin penalty for statistical learning","The fact that the new penalty did not yield the best
results for two of the datasets, suggests scope for improvement. One direction for future work
would be to tune over all the penalty functions (or may be the nonconvex penalty functions
separately from the convex penalty functions) for any given dataset. Another direction will
be to introduce randomness into the hyperparameter selection of the nonconvex penalty
functions, especially for deep neural network architectures. ",stat.ML,C,-0.085051574,-0.19605178,-0.060054615
http://arxiv.org/pdf/2204.03193v1,"MultiAuto-DeepONet: A Multi-resolution Autoencoder DeepONet for Nonlinear Dimension Reduction, Uncertainty Quantification and Operator Learning of Forward and Inverse Stochastic Problems","We add L1 regularization in our network to introduce sparsity in this paper. The future work includes investigating the impact of diÔ¨Äerent regularization terms, for
example, L2 and physics-informed regularization to the performance of our model. We
will also present our works on learning more complex, especially higher dimensional
stochastic operators in the future. ",stat.ML,C,0.0044197515,-0.3464373,0.05127393
http://arxiv.org/pdf/2204.03771v1,Q-learning with online random forests,"While not managing to land, the DQN method provided more efÔ¨Åcient fuel use before cratering, leading to
improved average reward over RL-ORFs. 6 Discussion and future work

We discovered that the online tree method could outperform some deep neural networks in terms of average total reward. In the process, we found that starting the forest size with a small number of trees and then expanding the size after the
set episodes performed better in later episodes. ",stat.ML,C,-0.22799787,-0.1303968,-0.0017358102
http://arxiv.org/pdf/2204.04512v1,Spectral bounds of the $\varepsilon$-entropy of kernel classes,"Literature dedicated to covering numbers of kernel classes includes monographs [6] and [3]. The earliest
                                         bound on the -entropy of a ball in RKHS was given in [5] and further study of this subject was carried out
                                         in [30]. The case of special types of kernels, such as translation invariant and analytical kernels, was considered
                                         in [29]. ",stat.ML,B,0.29549438,-0.12859027,0.13569102
http://arxiv.org/pdf/2204.04597v1,"Private Sequential Hypothesis Testing for Statisticians: Privacy, Error Rates, and Sample Size","Intuitively, it appears that the overshoot analysis when adding Laplace
noise relies heavily on the additional noise, rather than the statistical information provided by log-likelihood
ratios. Characterizing the relevant statistical properties when adding Laplace noise requires new tools, which
we leave as future work for the privacy and statistics communities. Table 3: Numerical values of expected sample sizes E0[T ] and E1[T ], Type I error and Type II error for
testing Bernoulli parameter using the original AboveThresh algorithm with Laplace noise. ",stat.ML,A,0.1387865,0.15274268,-0.18915817
http://arxiv.org/pdf/2204.04773v1,Worst-case Performance of Greedy Policies in Bandits with Imperfect Context Observations,"First, it will be of interest to study
reinforcement learning policies for settings that each arm has its own parameter. Further, regret analysis for
contextual bandits under imperfect context observations where the sensing matrix is unknown, is another problem
for future work. 8
7 Appendices

7.1 Proof of Lemma 1

We use the following decomposition

      Sy‚àí0.5ya(t)(t) = PC(S0.5Œ∑(t))Sy‚àí0.5ya(t)(t) + PC(S0.5Œ∑(t))‚ä• Sy‚àí0.5ya(t)(t).y   y                                       (32)

We claim that PC(S0.5Œ∑(t))Sy‚àí0.5ya(t) and PC(S0.5Œ∑(t))‚ä• Sy‚àí0.5yi(t) are statistically independent. ",stat.ML,B,0.12713246,0.030358931,-0.20120512
http://arxiv.org/pdf/2204.04773v2,Worst-case Performance of Greedy Policies in Bandits with Imperfect Context Observations,"First, it will be of interest to study
reinforcement learning policies for settings that each arm has its own parameter. Further, regret analysis for
contextual bandits under imperfect context observations where the other parameters such as the covariance matrices
of contexts and observations and the sensing matrix are unknown, is another problem for future work. 7 Appendices

7.1 Proof of Lemma 1

Note that Sy‚àí0.5yi(t) has the normal distribution N (0, Idy ). ",stat.ML,B,0.104884386,0.017842729,-0.19087186
http://arxiv.org/pdf/2204.04819v1,RMFGP: Rotated Multi-fidelity Gaussian process with Dimension Reduction for High-dimensional Uncertainty Quantification,"In particular, investigating the regres-
sion tasks with missing data or labels can be interesting in the future works. Another
potential future work is to Ô¨Ånd an approach to determined the optimal dimension s if
parameter f lag = 1 in the algorithm. With the proposed RMFGP model, one can build
up an accurate surrogate model with lower dimensional inputs than the original data. ",stat.ML,C,-0.1721819,-0.15788189,-0.0013691913
http://arxiv.org/pdf/2204.04875v1,Learning to Induce Causal Structure,"Further work is required
to scale our method to very large datasets. Finally, another possible direction of future work would be to use the proposed framework for learning
causal structure from raw visual data. This could be useful, e.g., in an RL setting in which an RL
agent interacts with the environment via observing low level pixel data. ",stat.ML,C,-0.398178,0.00845201,0.24036784
http://arxiv.org/pdf/2204.04875v2,Learning to Induce Causal Structure,"Our model
generalizes well to test OOD test distributions, where either the graphs can vary in terms of sparsity
or the conditional probability can vary in terms of Œ± values for the Dirichlet distributions. A.7 ABLATION STUDIES AND ANALYSES

In order to better understand the performance of our model, we performed further analysis and
ablations. This section contains full results for Section 6.4. ",stat.ML,A,0.08448686,0.25228193,-0.10036258
http://arxiv.org/pdf/2204.06274v1,Overparameterized Linear Regression under Adversarial Attacks,"lasso, and between 2 adversarial training and ridge regression. [12] B. Biggio and F. Roli, ‚ÄúWild patterns: Ten years
We intend to further explore the similarities and differences    after the rise of adversarial machine learning,‚Äù Pattern
between these methods in future work. We also intend to further
                                                                 Recognition, vol. ",stat.ML,C,-0.16500539,-0.29580694,-7.979944e-05
http://arxiv.org/pdf/2204.07124v1,Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods in Medicine,"Third, we acknowledge that we demonstrated the eÔ¨Äectiveness of our method through
simulation studies. A promising path for future work is to implement our methods in the
operational decision-making of healthcare providers to demonstrate the utility for patients
in the Ô¨Åeld. 17
                       Learning Optimal DTRs Using Causal Tree Methods

Acknowledgments

We thank Milan Kuzmanovic for his introduction to MIMIC-III and his input to the empir-
ical application. ",stat.ML,A,-0.2394731,0.21073166,-0.007109739
http://arxiv.org/pdf/2204.07172v1,Diagnosing and Fixing Manifold Overfitting in Deep Generative Models,"Also, our two-step methodology is only one
Interestingly, whereas the VAEs used in Nalisnick et al. way to Ô¨Åx manifold overÔ¨Åtting, and we hope our work will
(2019) have Bernoulli likelihoods, we Ô¨Ånd that our single-    encourage additional research in this direction. Finally, we
step likelihood-based Gaussian-decoder VAE and AVB mod-       treated d as a hyperparameter, but in practice d is unknown
els perform quite well on distinguishing FMNIST from          and improvements can likely be had by estimating it (Levina
MNIST, yet still fail on the CIFAR-10 task. ",stat.ML,C,-0.11579403,-0.108537644,-0.03251416
http://arxiv.org/pdf/2204.07172v2,Diagnosing and Fixing Manifold Overfitting in Deep Generative Models,"However, we note that our approach at least provides a mechanism to encourage our trained
encoder-decoder pair to invert each other, suggesting that (2) might not be too far oÔ¨Ä. We also believe that a
Ô¨Ånite-sample extension of our result, while challenging, would be a relevant direction for future work. We
hope our work will encourage follow-up research exploring diÔ¨Äerent ways of addressing manifold overÔ¨Åtting,
or its interaction with the score-matching objective. ",stat.ML,C,-0.15268795,-0.21329607,0.18263379
http://arxiv.org/pdf/2204.07172v3,Diagnosing and Fixing Manifold Overfitting in Deep Generative Models,"However, we note that our approach at least provides a mechanism to encourage our trained
encoder-decoder pair to invert each other, suggesting that (2) might not be too far oÔ¨Ä. We also believe that a
Ô¨Ånite-sample extension of our result, while challenging, would be a relevant direction for future work. We
hope our work will encourage follow-up research exploring diÔ¨Äerent ways of addressing manifold overÔ¨Åtting,
or its interaction with the score-matching objective. ",stat.ML,C,-0.15268795,-0.21329607,0.18263379
http://arxiv.org/pdf/2204.07172v4,Diagnosing and Fixing Manifold Overfitting in Deep Generative Models,"However, we note that our approach at least provides a mechanism to encourage our trained
encoder-decoder pair to invert each other, suggesting that (2) might not be too far oÔ¨Ä. We also believe that a
Ô¨Ånite-sample extension of our result, while challenging, would be a relevant direction for future work. We
hope our work will encourage follow-up research exploring diÔ¨Äerent ways of addressing manifold overÔ¨Åtting,
or its interaction with the score-matching objective. ",stat.ML,C,-0.15268795,-0.21329607,0.18263379
http://arxiv.org/pdf/2204.07747v1,A Variational Approach to Bayesian Phylogenetic Inference,"This
way, the SBN approximation given by VBPI can be viewed as a compact representation of the topological

                                                                       29
uncertainty of phylogenetic trees that can be learned in a timely manner. There are many extensions for future work. The Ô¨Årst extension is to construct more Ô¨Çexible variational

family of distributions. ",stat.ML,B,0.07572566,0.004033964,0.051771306
http://arxiv.org/pdf/2204.07826v1,Beyond L1: Faster and Better Sparse Models with skglm,"This highlights the importance
of combining Anderson acceleration with working sets. Conclusion and future works

In this paper, we have proposed an accelerated versatile algorithm for non-convex sparse-
like problems. Based on working sets, coordinate descent and Anderson acceleration, we

                                                          13
improved state-of-the-art on convex problems, and handled previously out-of-reach prob-
lems. ",stat.ML,B,0.103375405,-0.28805363,0.0942154
http://arxiv.org/pdf/2204.08155v1,A dynamical systems based framework for dimension reduction,"Each of these methods used 1000 epochs and 500 epochs, respectively. Our
implementation of the DDR method is slower than these other methods, which could be improved
in future work. 6. ",stat.ML,B,0.046935275,-0.045128956,0.110217914
http://arxiv.org/pdf/2204.09039v1,A stochastic Stein Variational Newton method,"To demonstrate the performance of our proposed
algorithm, we examined the Ô¨Çows and sample quality on a difÔ¨Åcult class of test problems‚Äîthe Hybrid Rosenbrock
density‚Äîand showed that sSVN successfully reconstructs the posterior with at least three orders of magnitude fewer
gradient evaluations of the log-likelihood than sSVGD. In future work, it will be interesting to compare sSVN to
other state of the art sampling algorithms such as dynamic nested sampling Higson et al. (2019); Skilling (2006) and
Hamiltonian Monte Carlo Betancourt (2018); Neal et al. ",stat.ML,C,-0.09170078,-0.090089016,-0.3086024
http://arxiv.org/pdf/2204.09086v1,Choosing the number of factors in factor analysis with incomplete data via a hierarchical Bayesian information criterion,"Therefore, HBIC is a better criterion than BIC for incomplete data. For future work, it would be interesting to investigate how to extend the proposed HBIC to the FA-related models
in the presence of incomplete data such as mixtures of factor analyzers (MFA) (Wang and Lin, 2020), and mixtures
of common FA (MCFA) (Baek et al., 2010; Wang, 2013) and etc. Table 3: Results of AIC, BIC, CAIC and HBIC on cereals data with various missing rates over 100 replications: Rates of underestimation (U ),

success (S), and overestimation (O). ",stat.ML,A,-0.18747583,0.34533203,0.06704644
http://arxiv.org/pdf/2204.09938v1,Ultra Marginal Feature Importance,"Further, SLC22A5 ranks in the top three genes for all methods, and
TEX14 is always the least important BRCA associated gene. Among the non-BRCA associated genes CST9L is highly
ranked in all methods, so this is a good candidate for further research. While there are clearly important similarities
in the results of all methods, the glaring difference is the number of features given 0 importance. ",stat.ML,A,-0.25764853,0.18960236,0.11305541
http://arxiv.org/pdf/2204.09938v2,Ultra-marginal Feature Importance,"If a third method is used that does not quantify interactions, perhaps the unique,
redundant, and synergistic information from the partial information diagrams referenced in Section
A.2 can each be quantiÔ¨Åed and compared. This may be an interesting project for future work. Budescu [12] was concerned that current feature importance metrics depend on the subset of features
used in a linear regression model. ",stat.ML,A,-0.1987667,0.15172492,0.15756191
http://arxiv.org/pdf/2204.09938v3,Ultra-marginal Feature Importance,"If a third method is used that does not quantify interactions, perhaps the unique,
redundant, and synergistic information from the partial information diagrams referenced in Appendix
A.2 can each be quantiÔ¨Åed and compared. This may be an interesting project for future work. Budescu [14] was concerned that current feature importance metrics depend on the subset of features
used in a linear regression model. ",stat.ML,A,-0.20946929,0.14361805,0.15300258
http://arxiv.org/pdf/2204.10663v1,3D pride without 2D prejudice: Bias-controlled multi-level generative models for structure-based ligand design,"To connect the Œ±n‚Äôs to the likelihood factors q and r, it
architecture can considerably beneÔ¨Åt structure-based design
applications, as well as steer us towards increased automation                  is convenient to use a binary cross-entropy loss term,
of the design process. As the learning framework provides
feedback on the performance of the 3D component, we hope                        Ln(vi|ua,n) = ‚àíyi|a log Œ±n(vi; ua,n)
that future work will quickly surpass the hypergraph approach                                    ‚àí (1 ‚àí yi|a) log [1 ‚àí Œ±n(vi; ua,n)] ,
pursued here, and guide us towards increasingly powerful
representations of the structural context. Such improved                        which, over the course of a long training run, is smoothened into a rate
representations that accurately anticipate interactions with the                diÔ¨Äerential
protein and/or implicit water network would be an important
step towards incorporating ligand eÔ¨Éciency and binding                          Ln(vi|ua,n) data ‚àù ‚àíri|a log Œ±n(vi; ua,n)
aÔ¨Énity objectives into the generative process. ",stat.ML,C,-0.19770631,-0.20190878,0.24618629
http://arxiv.org/pdf/2204.10975v1,Spherical Rotation Dimension Reduction with Geometric Loss Functions,"In terms of DR research, we point out
and give a partial solution to a problem in the research by various novel examples in the
text (i.e., Appendices A and B): how should we respect the geometry and/or topology
of the underlying space when performing DR or subsequent statistical problem? There are several directions of future work that we wish to pursue:

1. Generalize the methodology of SRCA to other manifolds other than spheres like the
topologically non-trivial tori. ",stat.ML,A,-0.05606346,0.17327023,0.17691976
