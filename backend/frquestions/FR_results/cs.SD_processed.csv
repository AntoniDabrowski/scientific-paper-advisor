url,title,further research,primary category,label,x,y,z
http://arxiv.org/pdf/2201.00052v1,Evaluating Deep Music Generation Methods Using Data Augmentation,"4, pp. 299–310,
from a different distribution to the training data, using domain                    2012.
adversarial training [27], [28] would help to uncover domain
invariant features in the generated samples, and is therefore an              [10] N. Chen, A. Klushyn, R. Kurle, X. Jiang, J. Bayer, and P. Smagt,
interesting avenue for further research. We have also assumed                       “Metrics for deep generative models,” in International Conference on
that generated samples strictly inherit the label from an origi-                    Artiﬁcial Intelligence and Statistics. ",cs.SD,A,0.057216942,-0.19218987,0.2692148
http://arxiv.org/pdf/2201.00927v1,Classifying Autism from Crowdsourced Semi-Structured Speech Recordings: A Machine Learning Approach,"Such approaches have been demonstrated to improve socialization among children with
autism spectrum disorder [31, 36], suggesting that they could also be used to collect
naturalistic data similar to this experiment in an unobtrusive way. Another area of interest for future work may be examining the possibility of leveraging a
distributed workforce of humans for extracting audio-related features to bolster screening
accuracy. Previous work examined the use of crowdsourced annotations for autism,
indicating that similar approaches could perhaps be applied through audio [39-45]. ",cs.SD,B,-0.1629921,0.0603036,0.012805516
http://arxiv.org/pdf/2201.01763v1,Robust Self-Supervised Audio-Visual Speech Recognition,"With less than 10% of labeled data, our
noise-augmented pre-training approach reduces recognition error       model outperforms prior SOTA by ∼ 50%. Our future work
by 81.8% (42.9%→7.8%) / 60.8% (14.8%→5.8%) in low/mid-                includes applying audio-visual speech recognition in real-world
resource settings. low-resource and multilingual settings. ",cs.SD,C,-0.20170346,-0.09094983,0.21731779
http://arxiv.org/pdf/2201.01763v2,Robust Self-Supervised Audio-Visual Speech Recognition,"With less than 10% of labeled data, our
noise-augmented pre-training approach reduces recognition error       model outperforms prior SOTA by ∼ 50%. Our future work
by 81.8% (42.9%→7.8%) / 60.8% (14.8%→5.8%) in low/mid-                includes applying audio-visual speech recognition in real-world
resource settings. low-resource and multilingual settings. ",cs.SD,C,-0.20170346,-0.09094983,0.21731779
http://arxiv.org/pdf/2201.01763v3,Robust Self-Supervised Audio-Visual Speech Recognition,"With less than 10% of labeled data, our
noise-augmented pre-training approach reduces recognition error       model outperforms prior SOTA by ∼ 50%. Our future work
by 81.8% (42.9%→7.8%) / 60.8% (14.8%→5.8%) in low/mid-                includes applying audio-visual speech recognition in real-world
resource settings. low-resource and multilingual settings. ",cs.SD,C,-0.20170346,-0.09094983,0.21731779
http://arxiv.org/pdf/2201.01771v1,Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning,"We struggled to train the model however. 37
38 discussion and future work

        learning, we truly believe that audio source separation algorithms
        such as Spleeter [27] could serve as a data augmentation technique
        tailored to musical representation learning. A key property of music
        is that it mixes sounds from one or more sources to produce a whole. ",cs.SD,B,-0.18450809,0.32688808,0.15112302
http://arxiv.org/pdf/2201.02994v1,Emotional Speaker Identification using a Novel Capsule Nets Model,"In addition, we plan to study speaker
identification performance in emotional environments using different input features, e.g., spectrograms. Finally,
further study into employing capsule networks in speaker and speech recognition with proven results is required
because capsule networks are currently in development. Compliance with Ethical Standards

   The authors thank the University of Sharjah for supporting this work through the Machine Learning and Arabic
Language Processing research group. ",cs.SD,C,-0.19969003,-0.19611305,-0.014416056
http://arxiv.org/pdf/2201.03217v2,Local Information Assisted Attention-free Decoder for Audio Captioning,"In       information, and overcomes the limitation of these attention-
                                                                 based methods. In a future work, we may explore the choice of
                                                                 adaptive window size to capture local information from audio
                                                                 clips with acoustic events and scenes of different duration. 1https://github.com/LittleFlyingSheep/P-LocalAFT/tree/main/examples
                                                                               5

                             REFERENCES                                        [16] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, “Swin
                                                                                     Transformer: Hierarchical vision Transformer using shifted windows”, in
 [1] K. Drossos, S. Lipping, T. Virtanen, “Clotho: An audio captioning               Proc. ",cs.SD,C,-0.15297656,0.1362715,0.10691818
http://arxiv.org/pdf/2201.03386v1,Sub-mW Keyword Spotting on an MCU: Analog Binary Feature Extraction and Binary Neural Networks,"and tiny circuit with a sufﬁciently simple sound detector based
on binary features. The full integration is left for future work. REFERENCES

                         VII. ",cs.SD,B,0.034848217,0.29658863,0.11198865
http://arxiv.org/pdf/2201.03967v2,Emotion Intensity and its Control for Emotional Voice Conversion,"Inspired by the successful  into the emotional speech synthesis systems, which moti-
attempts in prosody style control, several studies control        vates our study. We further study the use of perceptual
                                                                  losses in EVC training to improve the intelligibility of the
                                                                  converted emotion. 4

2.5 Research Gap (Summary)                                         Source            Linguistic Transplant
                                                                   Speech                                     Linguistic
Below, we summarise the gaps in the literature of emotional        (Neutral)
voice conversion that we aim to address in this paper. ",cs.SD,C,-0.31122717,-0.21687159,-0.26403183
http://arxiv.org/pdf/2201.03967v3,Emotion Intensity and its Control for Emotional Voice Conversion,"Inspired by the successful  into the emotional speech synthesis systems, which moti-
attempts in prosody style control, several studies control        vates our study. We further study the use of perceptual
                                                                  losses in EVC training to improve the intelligibility of the
                                                                  converted emotion. 4

2.5 Research Gap (Summary)                                         Source            Linguistic Transplant
                                                                   Speech                                     Linguistic
Below, we summarise the gaps in the literature of emotional        (Neutral)
voice conversion that we aim to address in this paper. ",cs.SD,C,-0.31122717,-0.21687159,-0.26403183
http://arxiv.org/pdf/2201.04583v1,VoxSRC 2021: The Third VoxCeleb Speaker Recognition Challenge,"Future work could involve increasing the amount of
VoxSRC workshops will be in person, but to maintain this wide        multi-modal data in the test set to reduce the uncertainty for less
access, we will endeavor to keep recording and livestreaming         frequent languages. presentations during future workshops. Participation in all four
of the tracks increased signiﬁcantly from last year (an increase          We are unable to make a direct comparison between the
of 21%, 157%, 140%, and 94% for Tracks 1, 2, 3, and 4 respec-        2020 and 2021 test sets, although the signiﬁcantly higher %
tively, in terms of numbers of teams submitting results). ",cs.SD,B,0.02583431,-0.015451545,-0.12548535
http://arxiv.org/pdf/2201.04583v2,VoxSRC 2021: The Third VoxCeleb Speaker Recognition Challenge,"We hope that in future the      work to be done to provide equally high performance across all
VoxSRC workshops will be in person, but to maintain this wide        languages. Future work could involve increasing the amount of
access, we will endeavor to keep recording and livestreaming         multi-modal data in the test set to reduce the uncertainty for less
presentations during future workshops. Participation in all four     frequent languages. ",cs.SD,C,-0.027899534,-0.08178306,-0.117328316
http://arxiv.org/pdf/2201.05554v1,Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition,"Sec.4 presents experiments and results of both speech intelli-     10                                                       10
gibility assessment and speech recognition on UASpeech. The
last section concludes and discuss possible future works. 20         U      0                               Σ      20                                                                                         ! ",cs.SD,C,-0.17569895,-0.14071226,-0.14321917
http://arxiv.org/pdf/2201.05562v1,Investigation of Data Augmentation Techniques for Disordered Speech Recognition,"Given an audio segment x(t), a perturbation factor α is
results on the UASpeech database. The last section concludes        applied along the time axis and gives the output y(t) as:
and discusses possible future works. y(t) = x(αt)                              (3)

              2. ",cs.SD,B,0.14261721,0.26731962,-0.17365873
http://arxiv.org/pdf/2201.05782v1,A Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition,"BERT: pre-training of deep bidirectional
   We would like to apply the MT-SMNN framework to other                transformers for language understanding. In Proceedings of the
areas for future work. For example, the MT-SMNN based                   2019 Conference of the North American Chapter of the Associa-
models can be employed to build a metric for evaluating the             tion for Computational Linguistics: Human Language Technolo-
performance of emotion conditioned symbolic music genera-               gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,
tion models. ",cs.SD,C,-0.101678595,-0.2844941,0.016559776
http://arxiv.org/pdf/2201.06426v1,On Training Targets and Activation Functions for Deep Representation Learning in Text-Dependent Speaker Verification,"We                  [17] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed
also believe that a better fusion strategy can further improve                       embedding for face recognition and clustering,” in arXiv:1503.03832/
the fusion system. Fusion in the feature domain is of interest                       IEEE Computer Society Conference on Computer Vision and Pattern
to investigate [52] and we keep it for future work. Recognition, 2015. ",cs.SD,A,0.19261706,-0.1607942,0.26101866
http://arxiv.org/pdf/2201.06460v1,"MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis","step-wise method also increases the complexity of model
                                                                   training. For future work, jointly learning the classiﬁer under a
C. Local-level emotion presenting module (LM)                      multi-task framework is an alternative way to learn the global
                                                                   emotion representations within a uniﬁed acoustic model. As
   To demonstrate the effectiveness of the local emotion           for the local emotional strengths, it is worth exploring how to
strengths on the emotional speech synthesis, we replace the        model the strengths with neural networks in an unsupervised
syllable-level emotion strengths with the sentence-level emo-      manner, which beneﬁts to simplify both the training and
tion strengths, in which way only the global emotion strength      generation process. ",cs.SD,C,-0.2080144,-0.16322815,-0.06696716
http://arxiv.org/pdf/2201.07429v1,Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis,"Opencpop
3.3. Results                                                                                                              can open up a number of possibilities for further research in the
                                                                                                                          areas of SVS. Following [37], evaluation metrics, i.e., F0 Root Mean Square
Error (F0-RMSE), F0 Pearson Correlation Coefﬁcient (F0-PCC),                                                                            5. ",cs.SD,A,0.42120147,-0.049246002,-0.23235747
http://arxiv.org/pdf/2201.07429v2,Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis,"Opencpop
3.3. Results                                                                                                              can open up a number of possibilities for further research in the
                                                                                                                          areas of SVS. Following [37], evaluation metrics, i.e., F0 Root Mean Square
Error (F0-RMSE), F0 Pearson Correlation Coefﬁcient (F0-PCC),                                                                            5. ",cs.SD,A,0.42120147,-0.049246002,-0.23235747
http://arxiv.org/pdf/2201.07876v1,Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech,"The experimental results showed that        center session to a single user). As a future work,
                                                          we will evaluate more sophisticated methods to as-
sess the similarity of speakers by considering more                                                                                                                   15
than acoustic similarities. Also, we will explore the
use of the proposed adaptation schemes in other                           [11] E. Tournier, “Valence of emotional memories: A study
deep learning frameworks such as autoencoders,                                  of lexical and acoustic features in older adult affective
generative adversarial networks (GANs) or long short-                           speech,” Master’s thesis, Radboud University Nijmegen,
term memory (LSTM). ",cs.SD,C,-0.25847676,-0.1815685,0.03952142
http://arxiv.org/pdf/2201.08124v1,Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training,"In general, by using MTL and speaker classiﬁer joint     model. The future work will focus on the improvement in the
training, the speaker similarity can be consistently improved in     voice quality and naturalness of the cross-lingual speech. each language. ",cs.SD,C,-0.28482035,-0.09341852,-0.15801054
http://arxiv.org/pdf/2201.08448v1,"Kinit Classification in Ethiopian Chants, Azmaris and Modern Music: A New Dataset and CNN Benchmark","The authors deduced three essential features
for musical content, namely timbral texture, rhythm, and pitch content for Western music in various styles, including
classical, jazz, pop and rock. This work paved the way for further research in the area of genre classiﬁcation. Either
whole recordings or homogeneous sections within them were used, and a classiﬁcation accuracy of 61% was achieved
for ten genres, using statistical pattern recognition classiﬁers. ",cs.SD,B,-0.027412053,0.42959166,-0.13085975
http://arxiv.org/pdf/2201.09486v1,Bias in Automated Speaker Recognition,"Most affected by bias are female speakers
and non-US nationalities, who experience significant performance degradation due to aggregation, learning, evaluation,
deployment, historic and representation bias. Our findings lay a strong foundation for future work on bias and fairness
in automated speaker recognition. Manuscript submitted to ACM
16  Toussaint and Ding

ACKNOWLEDGMENTS

This research was partially supported by projects funded by EU Horizon 2020 research and innovation programme

under GA No. ",cs.SD,C,-0.24785592,-0.20575805,-0.097740576
http://arxiv.org/pdf/2201.09486v2,Bias in Automated Speaker Recognition,"DET curves, which have history in speaker           deployment, historic and representation bias. Our findings lay a
verification evaluation, should be used for visualizing model perfor-    strong foundation for future work on bias and fairness in automated
mance across speaker subgroups. Additionally error metrics should        speaker recognition. ",cs.SD,C,-0.21420664,-0.11959413,-0.19212642
http://arxiv.org/pdf/2201.09592v1,Unsupervised Audio Source Separation Using Differentiable Parametric Source Models,"Progress in research
on multiple F0 estimation may lead to further improvements. ACKNOWLEDGMENT
An extension of our method to polyphonic sources as well
as estimating the F0 jointly with the other source parameters        The authors would like to thank Emmanouil Benetos for
may be an interesting direction for future work. Moreover,        providing the Bach Chorals and Barbershop Quartet dataset. ",cs.SD,B,-0.02528994,0.29374492,-0.1425333
http://arxiv.org/pdf/2201.09692v1,Improving Factored Hybrid HMM Acoustic Modeling without State Tying,"FH+ivec         4-gram 8.0 16.3 8.7 12.0 16.1
For a similar search space size, the RTF of our decoder in a                   (GMM-Mono)   101∗ LSTM 7.2 14.8 7.8 10.7 14.4
CART system is 0.5. Despite the discrepancy, we believe that
in terms of an efﬁcient implementation there can still be room                                    4-gram 8.5 17.1 9.0 12.3 16.5
for improvement in a future work. ∗ Phonetic training: 20 (monophone) + 53 (diphone) + 28 or 30 (triphone)

               4. ",cs.SD,C,-0.014886482,-0.081981316,-0.18013614
http://arxiv.org/pdf/2201.10693v1,Noise-robust voice conversion with domain adversarial training,"Figure 10

                                                                                10
Figure 12: Similarity preference tests under clean scenario with 95%                Figure 16: Speech quality preference tests under real noisy scenario with 95%
conﬁdence intervals for (a) VAE-CN-C vs VAED-CN-C, (b) VAE-CD-C vs                  conﬁdence intervals for (a) VAE-CN-C vs VAED-CN-C, (b) VAE-CD-C vs
VAED-CN-C, (c) FHVAE-CN-CN vs VAED-CN-C.                                            VAED-CN-C, (c) FHVAE-CN-CN vs VAED-CN-C.

                                                                                    6. Conclusion and future work

Figure 13: Mean opinion score listening tests among diﬀerent systems under             In this paper, we propose a novel noise-robust voice
six types of noise with 5dB SNR. Error bar represents 95% conﬁdence interval. ",cs.SD,C,-0.18161774,-0.010107355,-0.21894051
http://arxiv.org/pdf/2201.10936v1,FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control,"7.1. Future Work                                                 On a broader perspective, we hope that the proposed method
                                                                 is a step toward facilitating artists in their creative process as
While our work represents a step towards high-quality con-       well as enabling amateurs to express themselves by lowering
trollable symbolic music generation, we recognize several        the barrier of entry to music creation and making the process
avenues for future work. In terms of quality, the proposed       faster and easier over all. ",cs.SD,B,0.05196718,0.32588387,-0.09059358
http://arxiv.org/pdf/2201.10936v2,FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control,"7.1. Future Work
                                                                 Ethical Considerations
While our work represents a step towards high-quality con-
trollable symbolic music generation, we recognize several        Automatic music generation may raise ethical concerns sim-
avenues for future work. The proposed model is good at           ilar to those of large language models. ",cs.SD,B,-0.072882175,0.08646169,-0.18569341
http://arxiv.org/pdf/2201.10936v3,FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control,"7.1. Future Work
                                                                 Ethical Considerations
While our work represents a step towards high-quality con-
trollable symbolic music generation, we recognize several        Automatic music generation may raise ethical concerns sim-
avenues for future work. The proposed model is good at           ilar to those of large language models. ",cs.SD,B,-0.072882175,0.08646169,-0.18569341
http://arxiv.org/pdf/2201.11207v1,Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition,"Rare sounds, which are arguably the most interesting
to discover, and languages with large inventories (especially tone languages)
remain as the largest challenge for the task of automatic phonetic inventory
discovery, at least for our, arguably simple, proposed method (if they can
be automatically discovered at all). We believe that these challenges are
adequate future work candidates. An interesting future direction is the assessment of discovered inventory
quality by training and testing an ASR system using the discovered inven-
tory. ",cs.SD,B,-0.06913757,0.100378275,-0.058789797
http://arxiv.org/pdf/2201.11207v2,Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition,"Rare sounds, which are arguably the most interesting
to discover, and languages with large inventories (especially tone languages)
remain as the largest challenge for the task of automatic phonetic inventory
discovery, at least for our, arguably simple, proposed method (if they can
be automatically discovered at all). We believe that these challenges are
adequate future work candidates. An interesting future direction is the assessment of discovered inventory
quality by training and testing an ASR system using the discovered inven-
tory. ",cs.SD,B,-0.06913765,0.10037824,-0.058789887
http://arxiv.org/pdf/2201.11999v1,Dual Learning Music Composition and Dance Choreography,"viable music compositions. For future work, we seek to extend our
                                                                          framework to raw waveform based music composition and explore
4.5 Ablation Studies                                                      multi-persons dance choreography. We perform ablation studies focused on investigating the effec-
tiveness of our dual learning scheme. ",cs.SD,B,0.014613209,0.3341851,-0.08803882
http://arxiv.org/pdf/2202.00874v1,HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection,"As shown in Table 2, the results             ther explore its potential. Combining the audio classiﬁcation
shows that our HTS-AT achieves a new SOTA as 97.0% on                   model into more downstreaming tasks [28, 29] is also consid-
ESC-50 dataset and equals the SOTA 98.0% on Speech Com-                 ered a future work. mand V2. ",cs.SD,C,0.008443916,0.008978665,-0.097873196
http://arxiv.org/pdf/2202.01614v1,The RoyalFlush System of Speech Recognition for M2MeT Challenge,"in Section 4. Finally, Section 5 concludes the paper and discusses
                                       Some researchers pay more attention to data augmentation based          the future work. model training [10–12]. ",cs.SD,A,0.21897624,-0.15789422,0.19152302
http://arxiv.org/pdf/2202.01614v2,The RoyalFlush System of Speech Recognition for M2MeT Challenge,"The experiments and results are shown in Section 4. Finally,
                                        data augmentation techniques also play a crucial role in addressing   Section 5 concludes the paper and discusses the future work. data sparsity issues and the robustness of the model for far-ﬁeld
                                        ASR [2, 8, 9]. ",cs.SD,A,0.298221,-0.038972165,0.08327928
http://arxiv.org/pdf/2202.01646v1,Improving Lyrics Alignment through Joint Pitch Detection,"One possible solution is to add one pitch class for such voice. For
     The predicted pitch results and the ground truth on a short clip     future work, we plan to evaluate the acoustic model on lyrics tran-
from Jamendo are visualized in Fig. 4. ",cs.SD,B,-0.2797645,0.21836162,-0.123348616
http://arxiv.org/pdf/2202.02112v1,Musical Audio Similarity with Self-supervised Convolutional Neural Networks,"puted with Adam [12] for minibatches of 256 audio clips. In future work, we will explore stereophonic signals,
                       3. EVALUATION                          multi-track recordings, alternative loss functions [14] and
                                                              musical transformations [15], and ways of granting user
To know that encoders produce meaningful points in the        control to the similarity measure [16]. ",cs.SD,B,-0.08159709,0.3971998,0.10214262
http://arxiv.org/pdf/2202.02115v1,Polyphonic pitch detection with convolutional recurrent neural networks,"There-               Hochreiter. Fast and accurate deep network learn-
fore, for future work we are looking into complementing              ing by exponential linear units (elus). arXiv preprint
the deep learning approach with traditional signal process-          arXiv:1511.07289, 2015.
ing techniques such as partial tracking [21] although we
strongly believe in a data-driven, end-to-end approach that     [9] Tim Cooijmans, Nicolas Ballas, Ce´sar Laurent, C¸ ag˘lar
avoids hard-coded heuristics. ",cs.SD,A,0.18885756,-0.10904594,0.4544364
http://arxiv.org/pdf/2202.03514v1,"Maximizing Audio Event Detection Model Performance on Small Datasets Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation Study","How-      the ESC-50 leaderboard at the time of its publication. ever, further study is necessary to determine which of these
                                       techniques or technique combinations provide the optimal so-         Xception models have also been used for AED, such as
                                       lution. This paper analyzes the effects of knowledge transfer,   in [7, 16], as the model provides high performance with rela-
                                       data augmentation, and pretraining on a popular small dataset,   tively few parameters compared to other top-performing mod-
                                       ESC-50 [3]. ",cs.SD,A,0.29716867,-0.19281799,0.029367624
http://arxiv.org/pdf/2202.03896v1,Speech Emotion Recognition using Self-Supervised Features,"pre-training self-supervised acoustic features. Table 2 shows a comparison between the results achieved                 In future work we intend to extend our proposed
by our method (experiment 7) against the best baselines            Upstream + Downstream SER model to (i) support
found on the literature for 5-fold CV on IEMOCAP. The              multitask learning and (ii) exploit multimodality, such as
first 4 baselines use audio-only and the fifth baseline uses       speech + text modalities and speech + text + visual
                                                                   modalities. ",cs.SD,C,-0.23007935,-0.06850391,0.20367432
http://arxiv.org/pdf/2202.04464v1,Conditional Drums Generation using Compound Word Representations,"In addition, participants thought that our
generated pieces were human, more so then actual human-generated pieces from
the test set. As for future work, it would be interesting to examine the eﬀect of
adding more musical parameters and accompanying instruments to the Encoder
representation. Acknowledgements: This work is supported by Singapore Ministry of Ed-
ucation Grant no. ",cs.SD,B,-0.025369013,0.24937332,-0.14835781
http://arxiv.org/pdf/2202.04464v2,Conditional Drums Generation using Compound Word Representations,"In addition, participants thought that our
generated pieces were human, more so then actual human-generated pieces from
the test set. As for future work, it would be interesting to examine the eﬀect of
adding more musical parameters and accompanying instruments to the Encoder
representation. Acknowledgements This work is supported by Singapore Ministry of Ed-
ucation Grant no. ",cs.SD,B,-0.024950309,0.2488617,-0.15093273
http://arxiv.org/pdf/2202.04528v1,A Multimodal Canonical-Correlated Graph Neural Network for Energy-Efficient Speech Enhancement,"Finally, one could notice by the neuron’s ﬁring behavior
that using prior frame node connection reinforces the information present in
the noisy audio channel, making it less dependant on the visual data context
for clean audio reconstruction. Regarding future work, we aim to develop a more biologically realistic neu-
ronal model, introduce the concept of memory, and improve communication
mechanisms between the channels. We also aim at using Graph Neural Networks
with Canonical Correlation Analysis to improve cross channels communication
blocks within convolution neural networks. ",cs.SD,B,-0.022311457,0.09585005,0.32293737
http://arxiv.org/pdf/2202.04814v1,Royalflush Speaker Diarization System for ICASSP 2022 Multi-channel Multi-party Meeting Transcription Challenge,"We evaluated the proposed system on far-ﬁeld Al-
                                                                          imeeting evaluation set, and signiﬁcantly reduced DER from 15.25%
                                                                          to 6.30%. For future work, we will further introduce the End-to-End
                                                                          sub-system to the whole system. 6. ",cs.SD,A,0.32341844,-0.06969975,-0.19477913
http://arxiv.org/pdf/2202.04814v2,Royalflush Speaker Diarization System for ICASSP 2022 Multi-channel Multi-party Meeting Transcription Challenge,"We evaluated the proposed system on far-ﬁeld Al-
                                                                          imeeting evaluation set, and signiﬁcantly reduced DER from 15.25%
                                                                          to 6.30%. For future work, we will further introduce the End-to-End
                                                                          sub-system to the whole system. 6. ",cs.SD,A,0.32341844,-0.06969975,-0.19477913
http://arxiv.org/pdf/2202.04981v1,Barwise Compression Schemes for Audio-Based Music Structure Analysis,"same dc values. A clever dimension selection could be
studied in future work. 4.2 Technical Details

          4. ",cs.SD,A,0.41910273,0.1031837,-0.09436306
http://arxiv.org/pdf/2202.04981v2,Barwise Compression Schemes for Audio-Based Music Structure Analysis,"A clever dimension selection method
divergence, but we restrict this work to MSE. could be studied in future work. An autoencoder is divided into two parts: an encoder,                    4. ",cs.SD,A,0.107197955,-0.041823316,0.20721227
http://arxiv.org/pdf/2202.05236v1,Learnable Nonlinear Compression for Robust Speaker Verification,"For all cases in this work, N = 3. Further tuning of number of
above, the DRC operation is deﬁned by:                                   intermediates and parameter search is left as future work. Y [t, f ] = (X[t, f ] + δ)r − δr,  (2)                                       Method        CD              MR-CD
                                                                         cube root [31]  α=3       αmax = 3, αmin = 1
                                                                         power law [33]  α = 15    αmax = 15, αmin = 1
                                                                                         δ = 2.0  δmax = 2.0, δmin = 1.0
                                                                               DRC       r = 0.5  rmax = 1.0, rmin = 0.0

where δ > 0 is a positive bias and r is the exponential offset. ",cs.SD,A,0.522978,0.12760241,-0.20865695
http://arxiv.org/pdf/2202.05718v1,Audio Defect Detection in Music with Deep Networks,"This process is applicable to other audio codecs,
                                                                   depending on decoder robustness and consistency checks. For this task, pre-caching of degraded audio allows us to
use larger subsets of FMA for training (66476 pieces), val-            In future work we plan to extend the range of lossy com-
idation (14244 pieces) and test (14244 pieces). pression defects simulated and apply our architecture to
                                                                   further and more generalised local audio degradations. ",cs.SD,B,-0.08309494,0.22897932,0.017382799
http://arxiv.org/pdf/2202.05756v1,A Novel Speech Intelligibility Enhancement Model based on CanonicalCorrelation and Deep Learning,"2125–2136, 2011.
plan to expand on this work by evaluating the performance
of CC-STOI based SE for more complex real-world audio-                  [10] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An audio-visual
visual datasets. This ongoing future work will pave the                       corpus for speech perception and automatic speech recognition,” The
way for design of future of multi-modal hearing-assistive                     Journal of the Acoustical Society of America, vol. 120, no. ",cs.SD,C,-0.1853753,0.07991857,0.018517056
http://arxiv.org/pdf/2202.06034v1,Deep Performer: Score-to-Audio Music Performance Synthesis,"The participants are             violin dataset. For future work, we plan to utilize the articu-
instructed to rate the synthesized audios in a 5-point Likert          lation marks and ornaments on scores to better model playing
scale in terms of pitch accuracy, timbre and noise level as well       techniques [29, 30], disentangle the timbre from room acous-
as the overall quality. We report the results in Table 1. ",cs.SD,B,-0.14422113,0.38755298,-0.20958903
http://arxiv.org/pdf/2202.06034v2,Deep Performer: Score-to-Audio Music Performance Synthesis,"The participants are             achieves competitive quality against the baseline on the violin
instructed to rate the synthesized audios in a 5-point Likert          dataset. For future work, we plan to utilize the articulation
scale in terms of pitch accuracy, timbre and noise level as well       marks and ornaments on scores to better model playing tech-
as the overall quality. We report the results in Table 1. ",cs.SD,B,-0.13177887,0.3747338,-0.21200672
http://arxiv.org/pdf/2202.06850v1,Multi-Task Deep Residual Echo Suppression with Echo-aware Loss,"This means              tion accuracy (WAcc). In future work, we will explore whether our
that the performance of the neural AEC may be affected if the input         echo-aware weighting loss can be transferred to related tasks like
signals contains X (such as DX and EX), because unlike the DSP              target speaker extraction (TSE), and explore more cascade schemes. methods, it is difﬁcult for a neural network to ﬁnd consistent rules
for such uncorrelated input combinations. ",cs.SD,C,-0.042558745,-0.064826205,0.06953788
http://arxiv.org/pdf/2202.06850v2,Multi-Task Deep Residual Echo Suppression with Echo-aware Loss,"This means              challenge with good subjective quality (MOS) and speech recogni-
that the performance of the neural AEC may be affected if the input         tion accuracy (WAcc). In future work, we will explore whether our
signals contains X (such as DX and EX), because unlike the DSP              echo-aware weighting loss can be transferred to related tasks like
                                                                            target speaker extraction (TSE), and explore more cascade schemes. 1https://echocatzh.github.io/GFTNN
                            5. ",cs.SD,C,-0.15266655,-0.09594565,0.09282706
http://arxiv.org/pdf/2202.06850v3,Multi-Task Deep Residual Echo Suppression with Echo-aware Loss,"This means              challenge with good subjective quality (MOS) and speech recogni-
that the performance of the neural AEC may be affected if the input         tion accuracy (WAcc). In future work, we will explore whether our
signals contains X (such as DX and EX), because unlike the DSP              echo-aware weighting loss can be transferred to related tasks like
                                                                            target speaker extraction (TSE), and explore more cascade schemes. 1https://echocatzh.github.io/GFTNN
                            5. ",cs.SD,C,-0.15266655,-0.09594565,0.09282706
http://arxiv.org/pdf/2202.06850v4,Multi-Task Deep Residual Echo Suppression with Echo-aware Loss,"This means              challenge with good subjective quality (MOS) and speech recogni-
that the performance of the neural AEC may be affected if the input         tion accuracy (WAcc). In future work, we will explore whether our
signals contains X (such as DX and EX), because unlike the DSP              echo-aware weighting loss can be transferred to related tasks like
                                                                            target speaker extraction (TSE), and explore more cascade schemes. 1https://echocatzh.github.io/GFTNN
                            5. ",cs.SD,C,-0.15266655,-0.09594565,0.09282706
http://arxiv.org/pdf/2202.07484v1,Phase-Based Signal Representations for Scattering,"Introduction                                   of-concept examples in a direct comparison to (magnitude)
                                                                                                                       scattering based on the STFT. This ﬁrst study sets the basis
                                           For most applications using time-frequency representations,                 and intuition for further research on this idea. only the magnitude of the obtained complex coeﬃcients
                                        is considered as, e.g., in the cases of spectrogram and                           The content of this paper was mainly developed in the Mas-
                                        scalogram [1]. ",cs.SD,B,0.20636605,0.227934,-0.02992125
http://arxiv.org/pdf/2202.07841v1,Learning Deep Direct-Path Relative Transfer Function for Binaural Sound Source Localization,"Acoust., Speech, Signal Process.,
for a single source. In future work, this will be extended to the                       2016, pp. 405–409. ",cs.SD,C,-0.0167225,0.04680396,-0.21545604
http://arxiv.org/pdf/2202.07968v1,On loss functions and evaluation metrics for music source separation,"Fi-
                                                                        nally, we also cross-correlated the set of metrics we investigated with
                                                                        the results of our subjective test. Out of this experiment, we conclude
                                                                        that it could be informative if future works on music source separa-
                                                                        tion also reported spectral distortion metrics (like L1freq) together
                                                                        with SDR-based ones. Note that such metrics are already being used
                                                                        to evaluate speech synthesis models [40]. ",cs.SD,B,-0.12897423,0.2395365,-0.2304169
http://arxiv.org/pdf/2202.07991v1,ADIMA: Abuse Detection In Multilingual Audio,"Majority of the ex-       detection. We setup baselines for monolingual and zero-shot
                                        isting work has focused on detecting abusive behaviour in        cross-lingual setting for encouraging further research in this
                                        textual data [1–6]. Abusive content detection on images and      direction. ",cs.SD,A,0.050636187,-0.2541995,-0.060269274
http://arxiv.org/pdf/2202.08862v3,RemixIT: Continual self-training of speech enhancement models via bootstrapped remixing,"One could make this theorem even more
bootstrapped mixtures mb = s∗+nb, ∀b. Thus, the student es-          applicable to real-world settings where the student errors
timates some speech waveform sb for each input bootstrapped          given different bootstrapped mixtures from the initial teacher
mixture mb and the latter term of the error correlation can be       estimate s∗ are weakly dependent [49] but we defer this
written as follows:                                                  derivation to future work. E RS , RT  = E (s∗ − s∗)T 1        B    (sb  −  s∗)     =                           III. ",cs.SD,C,0.005397345,-0.021825656,-0.1672885
http://arxiv.org/pdf/2202.08898v1,Word Embeddings for Automatic Equalization in Audio Mixing,"All EQ bands were given
equal importance when averaging the error for the loss function. In future work,
it would be interesting to weight the EQ bands based on perceptual frequency
band weights. However, that is beyond the scope of this study. ",cs.SD,A,0.3421201,0.15987962,-0.1275482
http://arxiv.org/pdf/2202.08898v2,Word Embeddings for Automatic Equalization in Audio Mixing,"importance when averaging the error for the loss function. We did not consider setting this to trainable because of the      In future work, it would be interesting to weight the EQ
limited data we have. bands based on perceptual frequency band weights. ",cs.SD,A,0.3197597,0.09171912,0.025870955
http://arxiv.org/pdf/2202.09102v1,Predicting Sex and Stroke Success -- Computer-aided Player Grunt Analysis in Tennis Matches,"of the player. In future work, we plan to investigate whether
The mean and middle feature vectors of any basic feature set         audio denoising and an even larger amount of data will increase
in combination with SVM did not result in a UAR higher than          the prediction quality. Furthermore, we want to study different
by-chance level (50 %). ",cs.SD,B,-0.11200598,0.19996731,0.07214667
http://arxiv.org/pdf/2202.09907v1,towards automatic transcription of polyphonic electric guitar music:a new dataset and a multi-loss transformer model,"We plan to do further error analysis in the future to gain      higher than 0.950 on MAESTRO [5]. This prompts future work to
insights into the behavior of the models. close the performance gap between piano and guitar transcription. ",cs.SD,B,0.048416134,0.22327656,-0.1393108
http://arxiv.org/pdf/2202.09950v1,CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech Editing,"Improving the
can signiﬁcantly improve the performance in objective metrics. speech quality further based on CampNet is the future work. Even if one utterance is used for ﬁne-tuning the model, it can
be found that the objective metrics are improved by comparing                         VI. ",cs.SD,C,-0.17892805,-0.11611315,-0.13155033
http://arxiv.org/pdf/2202.09950v2,CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech Editing,"In addition, the few-shot learning ability based on     [7] H. Kawahara, “Straight, exploitation of the other aspect of
CampNet is better than TTS and VC systems. Improving the                   vocoder: Perceptually isomorphic decomposition of speech
speech quality further based on CampNet is the future work. sounds,” Acoustical science and technology, vol. ",cs.SD,C,-0.26450217,-0.016561132,0.16416675
http://arxiv.org/pdf/2202.10594v1,Adversarial Attacks on Speech Recognition Systems for Mission-Critical Applications: A Survey,"Also, we provided comprehensive view of adversarial
attack methods and defense strategies. Finally, we outlined research challenges, defense recommendations,
and future work. We expect this paper to serve researchers and practitioners as a reference to help them in
understanding the challenges and to help them to improve existing models of speech recognition systems for
mission-critical applications. ",cs.SD,C,-0.27030092,-0.2636085,0.18641025
http://arxiv.org/pdf/2202.11136v1,FlowSense: Monitoring Airflow in Building Ventilation Systems Using Audio Sensing,"We can also use beam-forming to identify the location of the air source. These efforts remain as one direction of future work. 7 RELATED WORK
Our focus here is to summarize existing literature on mobile sensing for building and health monitoring. ",cs.SD,A,0.30343902,0.19410646,0.04125313
http://arxiv.org/pdf/2202.11823v1,Differentially Private Speaker Anonymization,"This
                                                                                gap may become even larger if we consider the DP guarantee
         Privacy budget ε                            Privacy budget ε           at the user level (i.e., adding the budget of all utterances
                   100                                                          contributed by a given user). While such a gap is expected and
                                                                                commonly observed in DP literature [31], [33], [47], we brieﬂy
             UASR (%)  95                                                       discuss below several ways in which it could be reduced,
                                                                                leaving their detailed investigation to future work. 90
                                                                                   More advanced techniques may be leveraged to bound the
                       85                                                       analytical privacy budget more tightly for utterance and user-
                                                                                level DP. ",cs.SD,C,0.023951188,-0.22733763,-0.23372069
http://arxiv.org/pdf/2202.11823v2,Differentially Private Speaker Anonymization,"An interesting future direction is to address this poten-

   Below, we discuss some limitations of our approach and promis-        tial privacy leakage without harming the utility of utterances. This
ing directions for future work. is a challenging problem because the duration of some phonemes
Tightness of analytical privacy guarantees. ",cs.SD,A,0.06570217,-0.24117604,-0.18500088
http://arxiv.org/pdf/2202.12243v1,Flat latent manifolds for music improvisation between human and machine,"[Knees and
4 Related work                                                    Schedl, 2013] conducts a survey of music similarity and its
                                                                  application to recommendation systems. Music with VAEs MusicVAE [Roberts et al., 2018], which
improved over sketch-RNN [Ha and Eck, 2018] using a hier-         5 Conclusion and future work
archical decoder, generates musical sequence from a single
point in the latent space. A few works followed the line of       We presented FM-Music-VAE, a theoretically-motivated ap-
MusicVAE since then. ",cs.SD,B,0.0014275685,0.35225457,-0.06402715
http://arxiv.org/pdf/2202.12243v2,Flat Latent Manifolds for Human-machine Co-creation of Music,"An excerpt of the performance video is available
online. 5 Conclusion and future work

We presented FM-Music-VAE, a theoretically-motivated approach to live interaction with a human
musician while learning a high-level music representation. Our method in effect learns a distance
metric for sequence data that allows for smooth and meaningful linear interpolations. ",cs.SD,B,-0.08240859,0.34620237,0.047265656
http://arxiv.org/pdf/2202.12243v3,Flat Latent Manifolds for Human-machine Co-creation of Music,"An excerpt of the performance video is available
online. 5 Conclusion and future work

We presented FM-Music-VAE, a theoretically-motivated approach to live interaction with a human
musician while learning a high-level music representation. Our method in effect learns a distance
metric for sequence data that allows for smooth and meaningful linear interpolations. ",cs.SD,B,-0.08240859,0.34620237,0.047265656
http://arxiv.org/pdf/2202.12257v1,A Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions,"4 compares them on our
excerpt dataset. 4.2 p-dispersion problem

Here we brieﬂy describe the algorithm and compare it with a state-of-art method for
solving the p-dispersion problem but we leave to future works the mathematical study
of the method2. Our approach consists in ﬁnding p subsets using hierarchical clustering. ",cs.SD,A,0.36667418,0.16478504,-0.050975926
http://arxiv.org/pdf/2202.12257v2,A Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions,"4 compares them on our
excerpt dataset. 4.2 p-dispersion problem

Here we brieﬂy describe the algorithm and compare it with a state-of-art method for
solving the p-dispersion problem but we leave to future works the mathematical study
of the method2. Our approach consists in ﬁnding p subsets using hierarchical clustering. ",cs.SD,A,0.36667418,0.16478504,-0.050975926
http://arxiv.org/pdf/2202.12719v1,Ask2Mask: Guided Data Selection for Masked Speech Modeling,"746–
and ATM+S can also be easily adopted into self-supervised             750.
pre-training methods. In our future work, we wish to apply
ATM over pretraining data containing data from multiple         [15] J. Zhang, Y. Zhao, M. Saleh, and P. Liu, “Pegasus: Pre-training with ex-
domains [10], [28] to achieve further improvements. We also           tracted gap-sentences for abstractive summarization,” in ICML. ",cs.SD,A,0.055239342,-0.2417463,0.22796392
http://arxiv.org/pdf/2202.13226v1,An acoustic signal cavitation detection framework based on XGBoost with adaptive selection feature engineering,"Thirdly, the low recognition of the incipient
cavitation. Therefore, in future work, we will explore the end-to-end cavitation detection with solely acoustic signal
data using 1D residual network or other convolutional neural networks. In addition, we will also explore Few-shot
learning to solve data scarcity and small sample problems. ",cs.SD,B,0.07388048,0.10028343,0.50294185
http://arxiv.org/pdf/2202.13226v2,An acoustic signal cavitation detection framework based on XGBoost with adaptive selection feature engineering,"Thirdly, the low recognition of the incipient
cavitation. Therefore, in future work, we will explore the end-to-end cavitation detection with solely acoustic signal
data using 1D residual network or other convolutional neural networks. In addition, we will also explore Few-shot
learning to solve data scarcity and small sample problems. ",cs.SD,B,0.07388048,0.10028343,0.50294185
http://arxiv.org/pdf/2203.00232v1,Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR,"As shown in Table 2, we obtain       experiments on the LibriMix 2-speaker dataset, showing promising
averaged test TERs for PIT-CTC and GTC-e of 22.8% and 25.0%            results demonstrating the feasibility of the approach. In future work,
respectively, from which we can tell that the token recognition per-   we will explore other applications of GTC-e and investigate ways to
formance is comparable. It indicates that we should consider how to    improve the performance of extended GTC on multi-speaker ASR
improve the speaker prediction in the next step. ",cs.SD,C,-0.20862906,-0.05555178,-0.048773043
http://arxiv.org/pdf/2203.00951v1,Speaker Adaption with Intuitive Prosodic Features for Statistical Parametric Speech Synthesis,"demonstrated that our proposed models with prosodic features can
   Comparing the MOS results of male speakers (Fig 4(a) and 4(c))
with female speakers (Fig 4(b) and 4(d)), we can see that the overall  achieve lower acoustic distortion and higher subjective similarity
scores (both naturalness and similarity) of female speakers were
higher than that of male speakers, which may also be attributed to     compared to baseline models. Our future work will focus on inte-
the gender-imbalance issue in the AISHELL-3 dataset, as mentioned
above in Section 3.4. Comparing Baseline-Enc and Baseline-Emb,         grating more intuitive acoustic features that can describe speaker
Baseline-Enc achieved higher similarity MOS than Baseline-Emb for
speaker 0966 (𝑝 = 7.0 × 10−5) and the similarity differences were      characteristics into our model and investigating the disentangle-
insignificant for the other three speakers (𝑝 > 0.05). ",cs.SD,C,-0.2914445,0.013485538,-0.18022808
http://arxiv.org/pdf/2203.01118v1,A multi-task learning for cavitation detection and cavitation intensity recognition of valve acoustic signals,"Although the proposed method has improved recognition of the incipient cavitation recognition, which is still relatively
low. In future work, convolutional neural networks (CNN) have achieved remarkable results in mechanical device
health management due to their stronger capability of representation learning. Therefore, we directly extract deeper and
more expressive valve cavitation features using the 1D CNN to enhance the performance of valve cavitation detection
and cavitation intensity recognition. ",cs.SD,A,0.15548262,-0.029163957,0.5290799
http://arxiv.org/pdf/2203.01118v2,A multi-task learning for cavitation detection and cavitation intensity recognition of valve acoustic signals,"Although the proposed method has improved recognition of the incipient cavitation recognition, which is still relatively
low. In future work, convolutional neural networks (CNN) have achieved remarkable results in mechanical device
health management due to their stronger capability of representation learning. Therefore, we directly extract deeper and
more expressive valve cavitation features using the 1D CNN to enhance the performance of valve cavitation detection
and cavitation intensity recognition. ",cs.SD,A,0.15548262,-0.029163957,0.5290799
http://arxiv.org/pdf/2203.01205v1,Audio Self-supervised Learning: A Survey,"Such a
between SSL methods and other confusing machine learning           kind of generative contrastive model has been successfully
                                                                   investigated for NLP tasks, such as in ELETRA [168], but
   3https://superbbenchmark.org/                                   rarely been explored for audio SSL. Hence, we did not
   4http://lebenchmark.com/
   5https://neuralaudio.ai/hear2021-holistic-evaluation-of-audio-
representations.html
20

introduce it as an audio SSL form in the literature review,         as ‘image’ to some SSL models designed for CV tasks [37],
though it should be naturally considered for future works. [89], [93]. ",cs.SD,C,-0.16012494,-0.027447902,0.07191235
http://arxiv.org/pdf/2203.02678v1,NeuralDPS: Neural Deterministic Plus Stochastic Model with Multiband Excitation for Noise-Controllable Waveform Generation,"Finally, we
show the effectiveness of editing speech based on the proposed        [13] Q. Hu, K. Richmond, J. Yamagishi, and J. Latorre, “An experi-
framework. Decoupling the various components of the neural                  mental comparison of multiple vocoder types,” in Eighth ISCA
vocoder to achieve richer voice editing is the future work. Workshop on Speech Synthesis, 2013. ",cs.SD,C,-0.315041,-0.10963112,-0.03624361
http://arxiv.org/pdf/2203.02942v1,C-P Map: A Novel Evaluation Toolkit for Speaker Verification,"The C-P map of the TSP                         techniques. As for the future work, more comprehensive anal-
system (System 4) corresponds to CPref (·) and the C-P map of                      ysis will be conducted to understand the behavior of different
the ASP system (System 6) corresponds to CPtest(·). It can be                      models at different locations on the C-P map. ",cs.SD,A,0.45078406,-0.077271715,-0.36052537
http://arxiv.org/pdf/2203.02967v1,Variational Auto-Encoder based Mandarin Speech Cloning,"4. Some additional tagging for future work (e.g., scenarios, emotions and overviews). Private dataset was produced with the consent of the speaker, but unfortunately we cannot make it public in
addition to the samples used for experiments due to the agreement. ",cs.SD,A,0.045738406,-0.15208596,-0.08761948
http://arxiv.org/pdf/2203.03190v1,Speaker recognition by means of a combination of linear and nonlinear predictive models,"We believe that           magnitude than the LPCC coefficients. with additional research this results can be improved. Main
subjects that must be studied are:                                     2.3 Nonlinear codebook generation
    • An algorithm for clustering the frames. ",cs.SD,A,0.28503132,0.105094105,-0.113674924
http://arxiv.org/pdf/2203.04099v1,VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer,"evaluate AV speech separation systems in singing voice, showing empirically that the characteristics of the singing
voice differ substantially from the ones of speech. As future work we would like to explore lighter and faster embedding generators for the transformer and different
optimisations in its architecture which leads to a fast and powerful system. 10
VoViT  A PREPRINT

References

E Colin Cherry. ",cs.SD,C,-0.25276613,0.14745533,-0.11321402
http://arxiv.org/pdf/2203.04099v2,VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer,"For the first time, we evalu-
ate AV speech separation systems in singing voice, showing empirically that the
characteristics of the singing voice differ substantially from the ones of speech. As future work we would like to explore lighter and faster embedding gen-
erators for the transformer and different optimisations in its architecture which
leads to a fast and powerful system. Acknowledgments. ",cs.SD,C,-0.20269275,0.13814205,-0.06851897
http://arxiv.org/pdf/2203.04638v1,Speaker Identification Experiments Under Gender De-Identification,"Identification rates

                                                                     VI. Conclusion and future work

                                                                   In this paper, we studied the potential of voice transformation
                                                                   for gender de-identification. The method does not require
                                                                   enrolment of speakers for de-identification thus greatly
                                                                   extending possible applications of the system. ",cs.SD,C,-0.24861307,-0.06928542,-0.13400446
http://arxiv.org/pdf/2203.04696v1,Robust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition,"Thus, adversarial training doesn’t
adversarial attacks; a comparison with work on centralised training   effectively defend against the iterative attacks. on DEMoS; and ﬁnally, considerations regarding future work that
could be conducted with our pipeline. As a result, we propose the randomisation – the second defence
                                                                      strategy within our pipeline – to be more effective, via utilising
    White-box attack comparison: In our work, we analyse two          resizing and padding by random proportions on the test data at
types of adversarial white-box attacks, single-step attacks (i. e.,   inference time. ",cs.SD,A,0.13286465,-0.25498977,0.26656985
http://arxiv.org/pdf/2203.06064v1,Climate Change & Computer Audition: A Call to Action and Overview on Audio Intelligence to Help Save the Planet,"It constitutes a major challenge for
real world applications as it makes model selection and, more crucially, model testing harder, leading to catastrophic
failures during deployment. This is of particular importance for a large-scale deployment required for environmental
monitoring, where a model will have to perform equally well across several different locations, and, correspondingly,
needs to be addressed via further research. For audio analysis in particular, the notion of generalisation is closely linked to that of robustness to different
perturbations, a topic that has received considerable attention over the years. ",cs.SD,B,-0.017032541,0.19253612,-0.012765428
http://arxiv.org/pdf/2203.06220v1,"Infrastructure-free, Deep Learned Urban Noise Monitoring at $\sim$100mW","(Note that we            918.5       0.9976  0.9951  0.9951                               Online
eschew adaptation of the frequency that is used for discovery: this           908        0.9973  0.9946  0.9965
is to achieve fault-tolerance, so that nodes that are inadvertently          922.5       0.9972  0.9941  0.9926  1       1       1
not up during adaption are not excluded from the WSN.) 912        0.9971  0.9946  0.9962
                                                                             910.5       0.9970  0.9960  0.9959  2
                                                                              903        0.9950  0.9951  0.9934
                                                                             917.5       0.9863  0.9942  0.9174  3
                                                                              910        0.9957  0.9939  0.9947
                                                                              918        0.9967  0.9941  0.9938  4       5
                                                                          All others     0.8866  0.8991  0.8924
                                                                                                                 5       2       2

                                                                                                                         4

                                                                                                                         3       3

                                                                                                                                 4

                                                                                                                                 5
Infrastructure-free, Deep Learned Urban Noise Monitoring at ∼100mW                                                               ICCPS’ 22, May 4-6, 2022, Milan, Italy

Table 9: Impact of density increase on reliability if node duty         Figure 10: Likelihood of occurrence of engine, human-voice
cycle remains unchanged                                                 or alert noise source in contiguous 2-hour periods

Edge Degrees  Total Sent  Total Received  PRR                           case for updating native software components such as the CNN
                                                                        model in the field remains a relevant consideration for future work,
        2       102217         100142     0.9797                        as we expect active learning and federated learning support at the
        4        62421          55084     0.8825                        mote level to become increasingly important for long lived systems. 5        42601          29085     0.6915                        This suggests that we either refine the eMote interpreter via a JIT
                                                                        compiler to reduce the slowdown incurred in the VM (especially
while maintaining network connectivity as well as to yield afford-      for compute-intensive tasks) and migrate these components to
able coverage. ",cs.SD,A,0.36806342,0.04793774,-0.04385634
http://arxiv.org/pdf/2203.06517v1,SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System,"Further, the SASV-EER is improved from the 6.05%
                                                                   produced by prior state of the art approaches to 1.81% without
     SASV-Baseline2                  8.75 16.01 12.23              an ensembling strategy. Using an ensemble would likely further
                                                                   boost the performance and we will explore this in future work. SA-SASV              1.81 9.01 0.78
                                                                                      6. ",cs.SD,A,0.22794929,-0.04051472,0.0012346944
http://arxiv.org/pdf/2203.06517v3,SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System,"A larger dataset and differ-
and SV-EER therefore show a different tendency in the train-       ent encoders would likely boost the performance of the
ing and evaluation stages. We believe it is a reasonable solution  SV-EER and we will explore this in future work. The
to train end-to-end SASV systems on complete ASV and CM            code described here is available in open-source form from:
datasets to avoid the overﬁtting problem. ",cs.SD,A,0.15698919,-0.13298663,0.053890705
http://arxiv.org/pdf/2203.06583v1,Bi-Sampling Approach to Classify Music Mood leveraging Raga-Rasa Association in Indian Classical Music,"Psychol. 6:513. doi: 10.3389/fpsyg.2015.00513
challenges, which the researchers hope the future researchers
would be able to resolve with further study. Accuracy of 0.77               [15] Mukherjee, Samarpita & Mukherjee, Roan. ",cs.SD,A,0.20903656,-0.19258304,-0.2721581
http://arxiv.org/pdf/2203.07996v1,Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition,"The
results are shown in Table 8. The audio-only model                  As there has not emerged a dominating cross-
trained with 28 hours data achieves a WER of 3.4%,               modality self-supervised learning approach in the
which is a little bit worse than the one trained with            ﬁeld of AVSR, in future work, we are going to ex-
224 hours data. The result indicates that for the                plore two more directions in the self-supervised
audio-only model, the self-supervised model pre-                 learning scenario based on this work. ",cs.SD,B,-0.14615688,0.07040938,0.21184528
http://arxiv.org/pdf/2203.07996v2,Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition,"As there has not emerged a dominating cross-           Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
modality self-supervised learning approach in the            and Michael Auli. 2020. wav2vec 2.0: A frame-
ﬁeld of AVSR, in future work, we are going to                work for self-supervised learning of speech repre-
explore two more directions in the self-supervised           sentations. In Advances in Neural Information
learning scenario based on this work. ",cs.SD,C,-0.24441528,-0.17099465,0.16108263
http://arxiv.org/pdf/2203.08439v2,Instance-level loss based multiple-instance learning framework for acoustic scene classification,"labels for instance-level labels was good for instance clustering
in this study. Therefore, for future work, we plan to improve the       BC-ResNet-ASC-8 [43] was the winner system in the
proposed system into an MIL-based recording device-invariant       DCASE challenge 2021 task 1-A. BC-ResNet was ﬁrst pro-
ASC system with similar model complexity, in which instances       posed for a low-complexity keyword spotting system that con-
will be embedded in the instance-space regardless of device        sisted of FDW and TDW convolutional layers to reduce model
characteristics. ",cs.SD,C,-0.019072535,-0.038749762,0.28803843
http://arxiv.org/pdf/2203.08490v1,Learning Audio Representations with MLPs,"In order to enable fast pretraining and have an extremely eﬃcient method, kwmlp has
compromised in certain aspects, such as the pretraining dataset or task, and the model size. In future works, we would like to explore self-supervised representation learning methods
with similar all-MLP architectures on larger and richer audio corpuses, such as LibriSpeech
(Panayotov et al., 2015), AudioSet (Gemmeke et al., 2017) or Synth1B1 (Turian et al.,
2021), among others. One fact the HEAR challenge has made apparent is that perhaps it is
diﬃcult to learn truly generalized and holistic representations from solely one type of audio
data, such as speech. ",cs.SD,C,-0.21482983,-0.00036114268,0.27681005
http://arxiv.org/pdf/2203.09098v1,TMS: A Temporal Multi-scale Backbone Design for Speaker Embedding,"performance in ASV. Conclusions and future works are given
                                                                  in Section V.

C. Our focus and contributions                                        II. PROPOSED TEMPORAL MULTI-SCALE STRATEGY

   Considering the problems mentioned above, in this paper,          The general sketch of the proposed TMS module was intro-
we propose an effective speaker embedding model with inte-        duced in Fig. ",cs.SD,C,-0.13534771,-0.053113222,-0.08384605
http://arxiv.org/pdf/2203.09295v1,Assessing Progress of Parkinson s Disease Using Acoustic Analysis of Phonation,"clinical neurologists/psychologists and compare it to estima-
                                                                   tion error that was measured in this work. Other interesting
   Next, we tried to ﬁnd a possible candidate for preliminary      idea for future work would be to deeper investigate relations
PD assessment using Spearman’s rank correlation between            between speech and depression or sleep disorders. It is well
particular feature vector and selected clinical information. ",cs.SD,A,0.08193847,-0.08866576,-0.25142464
http://arxiv.org/pdf/2203.09402v1,Robust and Complex Approach of Pathological Speech Signal Analysis,"This would enable
       doctors to start the treatment very early and slow down the progress. We are going to deal with these issues in future works. However this research is very dependent on good
databases and especially in the case of voice pathology identiﬁcation, as in its ﬁrst stage, there is still a lack
of suitable training data. ",cs.SD,C,-0.12639263,-0.086441875,-0.016761832
http://arxiv.org/pdf/2203.09849v1,Neural Predictor for Black-Box Adversarial Attacks on Speech Recognition,"building robust ASR systems. In future work, we will further
                                                                               extend our method to targeted attacks. It would also be interest-
                                                                               ing to study the performance of NP-Attack on different network
                                                                               architectures of the neural predictor. ",cs.SD,A,0.18263534,-0.27335685,0.20053263
http://arxiv.org/pdf/2203.09893v1,A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation,"The main            computational requirements. We hope to encourage further research
reason for the difference in performance seems to be due to the onset        into low-resource, multi-purpose AMT systems and believe that the
detection accuracy which is higher in OF, since Acc is more similar          proposed solution can be a valuable baseline. for both methods (42.8% for OF vs. 37.5% for NMP). ",cs.SD,A,0.44361895,-0.05122405,-0.061564043
http://arxiv.org/pdf/2203.09893v2,A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation,"The main            computational requirements. We hope to encourage further research
reason for the difference in performance seems to be due to the onset        into low-resource, multi-purpose AMT systems and believe that the
detection accuracy which is higher in OF, since Acc is more similar          proposed solution can be a valuable baseline. for both methods (42.8% for OF vs. 37.5% for NMP). ",cs.SD,A,0.44361895,-0.05122405,-0.061564043
http://arxiv.org/pdf/2203.09961v1,Personalized filled-pause generation with group-wise prediction models,"performance improvement for those speakers. More-
over, our future work will involve synthesizing spon-     Levelt, W. J. (1983). ",cs.SD,B,-0.013126836,0.1832803,-0.1727045
http://arxiv.org/pdf/2203.09961v2,Personalized Filled-pause Generation with Group-wise Prediction Models,"More-
personalized model for inference. over, our future work will involve synthesizing spon-
                                                          taneous speech containing FPs predicted by the group-
4.5.6. Discussion                                         dependent models proposed in this paper and subjec-
                                                          tively evaluating individuality. ",cs.SD,C,-0.16607156,-0.12435925,-0.17791587
http://arxiv.org/pdf/2203.10425v1,A Study on Robustness to Perturbations for Representations of Environmental Sound,"876–924, 2019.
can help choose among different perturbations to be used as
augmentations for making sound event detection models more                      [17] J. Ardouin, L. Charpentier, M. Lagrange, F. Gontier, N. Fortin,
robust. D. Ecotie`re, J. Picaut, and C. Mietlicky, “An innovative low cost sensor
                                                                                      for urban sound monitoring,” in INTER-noise and noise-con congress
   As a future work, we intend to repeat this study for a                             and conference proceedings, vol. 258, no. ",cs.SD,B,0.04843998,0.26012093,0.07573563
http://arxiv.org/pdf/2203.10425v2,A Study on Robustness to Perturbations for Representations of Environmental Sound,"2226–2237. As future work, we intend to repeat this study for a wide
variety of embeddings and datasets and extend the analysis                     [15] J. M. Lo´pez, J. Alonso, C. Asensio, I. Pavo´n, L. Gasco´, and G. de Arcas,
to include correlations between distance metrics and different                       “A digital signal processor based acoustic sensor for outdoor noise
sound event classes. monitoring in smart cities,” Sensors, vol. ",cs.SD,B,-0.021528529,0.2989726,0.08687036
http://arxiv.org/pdf/2203.10425v3,A Study on Robustness to Perturbations for Representations of Environmental Sound,"models more robust. [13] J. Salamon, D. MacConnell, M. Cartwright, P. Li, and J. P. Bello,
   In future work, we intend to repeat this study on a wide               “Scaper: A library for soundscape synthesis and augmentation,” in 2017
variety of embeddings and datasets and extend the analysis                IEEE Workshop on Applications of Signal Processing to Audio and
to include correlations between distance metrics and different            Acoustics (WASPAA). IEEE, 2017, pp. ",cs.SD,B_centroid,-0.18053403,0.35101998,0.048953414
http://arxiv.org/pdf/2203.10473v1,ECAPA-TDNN for Multi-speaker Text-to-speech Synthesis,"Besides, to lighten the burden
the x-vector model can discern the utterance from the same          of subjective evaluation, we are the ﬁrst to adopt automatic
speaker, but the distribution of ECAPA-TDNN is more con-            MOS predictors to assess our testing results and these mod-
tinuous which suggests that ECAPA-TDNN can capture even             els show great potential. For future work, we will continue to
the subtle speaker characteristics from different utterances spo-   investigate the performance of few-shot multi-speaker speech
ken by the same speaker. It is important to be noticed that the     synthesis. ",cs.SD,C,-0.32757738,-0.081941806,-0.14771602
http://arxiv.org/pdf/2203.10473v2,ECAPA-TDNN for Multi-speaker Text-to-speech Synthesis,"Previous    els show great potential. For future work, we will continue to
studies [24] suggest that the continuous distribution of speaker    investigate the performance of few-shot multi-speaker speech
embeddings has better performance in the multi-speaker TTS          synthesis. task. ",cs.SD,C,-0.2653516,0.007973291,-0.10419738
http://arxiv.org/pdf/2203.10750v1,WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses,"For brevity, the ground-truth  higher quality and naturalness and achieve state-of-the-art per-
durations are used as the target duration for systems to synthe-  formance on the public corpus Opencpop. For future work, we
size. The calculated results are listed in Table 3 and we can     will investigate how to improve the robustness further and adapt
come to the following conclusions: 1) With the progressive        to any unseen singer with few-shot learning. ",cs.SD,C,-0.044952504,-0.030648643,-0.01992938
http://arxiv.org/pdf/2203.10750v2,WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses,"For brevity, the ground-truth  higher quality and naturalness and achieve state-of-the-art per-
durations are used as the target duration for systems to synthe-  formance on the public corpus Opencpop. For future work, we
size. The calculated results are listed in Table 3 and we can     will investigate how to improve the robustness further and adapt
come to the following conclusions: 1) With the progressive        to any unseen singer with few-shot learning. ",cs.SD,C,-0.044952504,-0.030648643,-0.01992938
http://arxiv.org/pdf/2203.10750v3,WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses,"For brevity, the ground-truth  higher quality and naturalness and achieve state-of-the-art per-
durations are used as the target duration for systems to synthe-  formance on the public corpus Opencpop. For future work, we
size. The calculated results are listed in Table 3 and we can     will investigate how to improve the robustness further and adapt
come to the following conclusions: 1) With the progressive        to any unseen singer with few-shot learning. ",cs.SD,C,-0.044952504,-0.030648643,-0.01992938
http://arxiv.org/pdf/2203.10750v4,WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses,"For brevity, the ground-truth  higher quality and naturalness and achieve state-of-the-art per-
durations are used as the target duration for systems to synthe-  formance on the public corpus Opencpop. For future work, we
size. The calculated results are listed in Table 3 and we can     will investigate how to improve the robustness further and adapt
come to the following conclusions: 1) With the progressive        to any unseen singer with few-shot learning. ",cs.SD,C,-0.044952504,-0.030648643,-0.01992938
http://arxiv.org/pdf/2203.10750v5,WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses,"The calculated results are listed in Table 3 and we can      augmented WeSinger system can synthesize singing voices with
come to the following conclusions: 1) With the progressive         higher quality and naturalness and achieve state-of-the-art per-
pitch-weighted loss, the F0 RMSE can be reduced from 15.3          formance on the public corpus Opencpop. For future work, we
to 13.9; 2) With VS-augmented data, the quality of predicted       will investigate how to improve the robustness efﬁciently and
BFCC can be slightly improved; 3) Combining VS-augmented           explore how to adapt to any unseen singer with lower cost. data with multi-singer pre-training, the optimal F0 RMSE of
12.9 can be obtained. ",cs.SD,C,-0.15156369,0.06989671,-0.028087946
http://arxiv.org/pdf/2203.10793v1,Phase-Aware Spoof Speech Detection Based on Res2Net with Phase Network,"Considering the performance       domness. Although proposed method showed outstanding per-
both in LA and PA, we choose the Res2Net-A–C with CQT fea-          formance for LA in known-kind-scenario, there is a perfor-
                                                                    mance degradation for LA in unknown-kind-scenario; thus, re-
    1https://github.com/clovaai/aasist                              constructing that performance is our future work. 5. ",cs.SD,A,0.4409008,-0.115357995,-0.10922964
http://arxiv.org/pdf/2203.10837v1,Multi-class versus One-class classifier in spontaneous speech analysis oriented to Alzheimer Disease diagnosis,"The approach of this work complements the previous
multi-class modelling (two class) and is robust in terms of capturing the dynamics of
the speech, and it offers many advantages in terms of be easier to compare the power
of the new features against the previous ones. In future works we will introduce new
features relatives to speech modelling oriented to standard medical tests for AD
diagnosis and to emotion response analysis. Acknowledgement

This work has been supported by FEDER and Ministerio de ciencia e Innovación,
TEC2012-38630-C04-03 and the University of the Basque Country by EHU-14/58. ",cs.SD,C,-0.20881498,-0.16308057,-0.17023739
http://arxiv.org/pdf/2203.10992v1,Spoofing-Aware Speaker Verification with Unsupervised Domain Adaptation,"We particularly
                                        spectively. We perform additional studies such as per-attack     focus on unsupervised domain adaptation (DA) by consider-
                                        breakdown analysis, data composition, and integration with a     ing the spoofed audio samples available for training CM sys-
                                        countermeasure system at score-level with Gaussian back-end. tems. ",cs.SD,C,-0.122976094,0.05126224,0.13964278
http://arxiv.org/pdf/2203.10992v2,Spoofing-Aware Speaker Verification with Unsupervised Domain Adaptation,"We particularly
                                        spectively. We perform additional studies such as per-attack     focus on unsupervised domain adaptation (DA) by consider-
                                        breakdown analysis, data composition, and integration with a     ing the spoofed audio samples available for training CM sys-
                                        countermeasure system at score-level with Gaussian back-end. tems. ",cs.SD,C,-0.122976094,0.05126224,0.13964278
http://arxiv.org/pdf/2203.11049v1,Differentiable Duration Modeling for End-to-End Text-to-Speech,"It        TTS systems. For future work, we will further reduce the model
is also important to mention that end-to-end TTS systems such           size, while keeping the audio quality to ensure fast speech syn-
as FastSpeech 2s [10] and EATS [17] performed worse than                thesis on small devices. Another promising direction is to apply
two-stage TTS systems as reported in their studies. ",cs.SD,C,-0.09155711,-0.037326343,-0.20584852
http://arxiv.org/pdf/2203.11138v1,Individualizing Head-Related Transfer Functions for Binaural Acoustic Applications,"Another limitation arises from the fact that the
                                                                                    measured HRTFs contain artifacts from room reverberations and
the used speaker and microphone. As future work, we will further                                                             Navid H. Zandi, Awny M. El-Mohandes, and Rong Zheng

develop faster and more robust data acquisition procedure that                              [25] Brian F. G. Katz and Gaëtan Parseihian. 2012. ",cs.SD,B,0.07350306,0.19212314,-0.15202136
http://arxiv.org/pdf/2203.11499v1,Residual-Guided Non-Intrusive Speech Quality Assessment,"The base-             speeches, causing the ﬁnal result may not to be the best. In fu-
line1 [24] is adapted from the NI-SQA model and comprises            ture work, we will conduct further research to demonstrate our
a deep feedforward network followed by LSTM and average              idea. Additionally, the loss function used in SQA-Discriminator
pooling. ",cs.SD,A,0.059622932,-0.19064112,0.21013764
http://arxiv.org/pdf/2203.11562v1,"A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis","Synthetic child speech                          Parametric Speech Synthesis with Joint Estimation of
samples can be viewed in our GitHub repository9. Acoustic and Excitation Model Parameters.”

For future work, we aim to improve this method by                    [8] Y. Wang et al., “Tacotron: Towards End-To-End Speech
incorporating more information into our multispeaker TTS                       Synthesis.”
model such as duration predictor and energy as implemented
in FastSpeech2 [12]. The trained vocoder was also finetuned          [9] J. Shen et al., “Non-Attentive Tacotron: Robust and
on the TinyMyST dataset. ",cs.SD,C,-0.27822307,0.0055027623,0.061577678
http://arxiv.org/pdf/2203.11562v2,"A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis","This information will be used to            [10] J. Shen et al., “Natural TTS Synthesis by Conditioning
collect better TTS-based child speech data based on Harvard                 Wavenet on Mel Spectrogram Predictions.”
sentences to accord with ‘end of the phrase’ information loss
and voice inconsistency observed with our current results. [11] Y. Ren et al., “FastSpeech: Fast, Robust and Controllable
The use of synthetically generated child speech to improve                  Text to Speech.”
other areas of child speech research such as ASR and speaker
recognition will also be investigated in future work. TTS-        [12] Y. Ren et al., “Fastspeech 2: Fast and High-Quality End-
generated child voices can be used as a data augmentation                   To-End Text to Speech.”
technique for training these models with additional data. ",cs.SD,C,-0.3480162,-0.16321701,0.01848621
http://arxiv.org/pdf/2203.11570v1,Conditional Generative Data Augmentation for Clinical Audio Datasets,"We reconstructed waveforms from a few generated
spectrograms using the Griﬃn-Lim algorithm and could, despite artifacts being
present, recognize acoustic similarities to the original samples for each class, re-
spectively. In future work, the proposed augmentation method could furthermore
be transferred to other medical and non-medical grid-like data domains. 5 Conclusion

In the presented work, we introduce a novel data augmentation method for
medical audio data and evaluate it on a clinical dataset which was recorded in
real-world Total Hip Arthroplasty (THA) surgeries. ",cs.SD,B,-0.0798711,0.35473424,0.18566331
http://arxiv.org/pdf/2203.11570v2,Conditional Generative Data Augmentation for Clinical Audio Datasets,"We reconstructed waveforms from a few generated
spectrograms using the Griﬃn-Lim algorithm and could, despite artifacts being
present, recognize acoustic similarities to the original samples for each class, re-
spectively. In future work, the proposed augmentation method could furthermore
be transferred to other medical and non-medical grid-like data domains. 5 Conclusion

In the presented work, we introduce a novel data augmentation method for
medical audio data and evaluate it on a clinical dataset which was recorded in
real-world Total Hip Arthroplasty (THA) surgeries. ",cs.SD,B,-0.0798711,0.35473424,0.18566331
http://arxiv.org/pdf/2203.11570v3,Conditional Generative Data Augmentation for Clinical Audio Datasets,"We reconstructed waveforms from a few generated
spectrograms using the Griﬃn-Lim algorithm and could, despite artifacts being
present, recognize acoustic similarities to the original samples for each class, re-
spectively. In future work, the proposed augmentation method could furthermore
be transferred to other medical and non-medical grid-like data domains. 5 Conclusion

In the presented work, we introduce a novel data augmentation method for
medical audio data and evaluate it on a clinical dataset which was recorded in
real-world Total Hip Arthroplasty (THA) surgeries. ",cs.SD,B,-0.0798711,0.35473424,0.18566331
http://arxiv.org/pdf/2203.11774v1,Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model,"We employed the homoscedastic un-
works built separate models for age and height, or separate           certainty principle to combine multiple losses of our multi-task
models for male and female, we compare our multi-task model           model. As part of future work, we plan to explore different fea-
with all of them. We report our results both in root mean             ture extractors and self-supervised learning to further improve
squared error (RMSE) and mean absolute error (MAE). ",cs.SD,A,0.1516564,-0.21688181,0.035847582
http://arxiv.org/pdf/2203.12105v1,Music Generation Using an LSTM,"In the same sense,
our network’s tiered LSTM architecture may allow it to pick apart the various aspects of
music such as if it is building tension, speeding up, or looking at any other type of pattern
that may be found in music. 4 Challenges

In the process of building and experimenting with this project, we ran into many
challenges, many of which we have overcome, and others which we plan to address in
future work. Among these includes how to evaluate non-deterministic outputs, how the
time between notes should be represented, how the data as a whole should be represented,
and how to have multiple tracks of music generated. ",cs.SD,B,0.042219184,0.29779977,0.08990372
http://arxiv.org/pdf/2203.12245v1,Quantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language,"However, test-
the latter discussing the validation questionnaire. Finally,    ing all interactions between all local translations would be
Section 5 concludes the paper and lays out future work. For     intractable. ",cs.SD,A,0.13537262,-0.24218021,-0.36509997
http://arxiv.org/pdf/2203.12245v2,Quantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language,"However, test-
the latter discussing the validation questionnaire. Finally,    ing all interactions between all local translations would be
Section 5 concludes the paper and lays out future work. For     intractable. ",cs.SD,A,0.13537262,-0.24218021,-0.36509997
http://arxiv.org/pdf/2203.12245v3,Quantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language,"Finally, Section 6 concludes the
the translation of soundscape attributes. Crucially, instead of   paper and lays out future work. a choice-based evaluation of overall suitability, our frame-
work utilizes a score-based approach to assess the quality            For readability, Thai words used in this paper are fol-
of a translation across multiple criteria concerning the ap-      lowed by their International Phonetic Alphabet (IPA) tran-
propriateness of translation, understandability, clarity, and     scription (adapted from Tingsabadh and Abramson, 1993;

K. N. Watcharasupat et al. ",cs.SD,C,-0.15357849,0.07928988,-0.395163
http://arxiv.org/pdf/2203.12369v1,MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,"The Short Time Fourier
Figure 2 except that G is replaced by N , sˆ, Sˆf by y, Yf                   Transform (STFT) is used with a DFT length of LDFT =512,
and the 1 in the MSE by w. This means that the training of                   a window length of 512 (32 ms) at sampling frequency of
N is inﬂuenced entirely by it’s performance as assessed by                   fs = 16 kHz and a hop (overlap) length 256 (16 ms), resulting
D, in the same manner as G. We use an identical network                      in a 50% overlap between frames. The minimum value in the
structure to G for N - We leave to future work to change                     time frequency masks MG and MN , ξ is set to 0.05.
this structure, as well as related hyper-parameters such as the
clamp threshold. We experiment with both PESQ and STOI as objective Q
                                                                             and different values of w. The values of w are selected such
   The training of MetricGAN+/- is similar to that that of                   that they correspond to sparely populated values of Q in
MetricGAN+ given above with slight differences. ",cs.SD,A,0.32835975,0.10396236,0.1203409
http://arxiv.org/pdf/2203.12369v2,MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,"The Short Time Fourier
Figure 2 except that G is replaced by N , sˆ, Sˆf by y, Yf                   Transform (STFT) is used with a DFT length of LDFT =512,
and the 1 in the MSE by w. This means that the training of                   a window length of 512 (32 ms) at sampling frequency of
N is inﬂuenced entirely by it’s performance as assessed by                   fs = 16 kHz and a hop (overlap) length 256 (16 ms), resulting
D, in the same manner as G. We use an identical network                      in a 50% overlap between frames. The minimum value in the
structure to G for N - We leave to future work to change                     time frequency masks MG and MN , ξ is set to 0.05.
this structure, as well as related hyper-parameters such as the
clamp threshold. We experiment with both PESQ and STOI as objective Q
                                                                             and different values of w. The values of w are selected such
   The training of MetricGAN+/- is similar to that that of                   that they correspond to sparely populated values of Q in
MetricGAN+ given above with slight differences. ",cs.SD,A,0.32835975,0.10396236,0.1203409
http://arxiv.org/pdf/2203.12369v3,MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,"The Short Time Fourier
Figure 2 except that G is replaced by N , sˆ, Sˆf by y, Yf                   Transform (STFT) is used with a DFT length of LDFT =512,
and the 1 in the MSE by w. This means that the training of                   a window length of 512 (32 ms) at sampling frequency of
N is inﬂuenced entirely by it’s performance as assessed by                   fs = 16 kHz and a hop (overlap) length 256 (16 ms), resulting
D, in the same manner as G. We use an identical network                      in a 50% overlap between frames. The minimum value in the
structure to G for N - We leave to future work to change                     time frequency masks MG and MN , ξ is set to 0.05.
this structure, as well as related hyper-parameters such as the
clamp threshold. We experiment with both PESQ and STOI as objective Q
                                                                             and different values of w. The values of w are selected such
   The training of MetricGAN+/- is similar to that that of                   that they correspond to sparely populated values of Q in
MetricGAN+ given above with slight differences. ",cs.SD,A,0.32835975,0.10396236,0.1203409
http://arxiv.org/pdf/2203.12369v4,MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,"The Short Time Fourier
Figure 2 except that G is replaced by N , sˆ, Sˆf by y, Yf and               Transform (STFT) is used with a DFT length of LDFT =512,
the 1 in the MSE by w. This means that the training of N is                  a window length of 512 (32 ms) at sampling frequency of
inﬂuenced entirely by its performance as assessed by D, in the               fs = 16 kHz and a hop (overlap) length 256 (16 ms), resulting
same manner as G. We use an identical network structure to                   in a 50% overlap between frames. The minimum value in the
G for N - We leave to future work to change this structure, as               time frequency masks MG and MN , ξ is set to 0.05.
well as related hyper-parameters such as the clamp threshold. We experiment with both PESQ and STOI as objective Q
   The training of MetricGAN+/- is similar to that that of                   and different values of w. The values of w are selected such
MetricGAN+ given above with slight differences. ",cs.SD,A,0.34553266,0.1178971,0.14625354
http://arxiv.org/pdf/2203.12369v5,MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,"2 except that G is replaced by N , sˆ, Sˆf by y, Yf and                 length of LDFT =512, a window length of 512 (32 ms) at
the 1 in the MSE by w. This means that the training of N is                  sampling frequency of fs = 16 kHz and a hop (overlap) length
inﬂuenced entirely by its performance as assessed by D, in the               256 (16 ms), resulting in a 50% overlap between frames. The
same manner as G. We use an identical network structure to                   minimum value in the time frequency masks MG and MN is
G for N - We leave to future work to change this structure, as               set to ξ = 0.05.
well as related hyper-parameters such as the clamp threshold. We experiment with both PESQ and STOI as objective Q
   The training of MetricGAN+/- is similar to that that of                   and different values of w. The values of w are selected such
MetricGAN+ given above with slight differences. ",cs.SD,A,0.36492193,0.103488475,0.11250261
http://arxiv.org/pdf/2203.12937v1,SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling,"There were two     previous supervised method in several simulated data and was
kinds of training data: “Simulated” generated with quantization    effective even with low-resource real data. Our future work in-
and resampling as in Section 4.2 and “Real” used in Section 4.3.   cludes applying our method to a wider variety of real data, such
The high-quality speech to be modiﬁed was sampled from the         as where multiple speakers are speaking at the same time. Acknowledgements: Part of this work was supported by
                                                                   JSPS KAKENHI Grant Number 21H04900. ",cs.SD,C,-0.32366338,-0.0629253,0.065296404
http://arxiv.org/pdf/2203.12937v2,SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling,"There were two     previous supervised method in several simulated data and was
kinds of training data: “Simulated” generated with quantization    effective even with low-resource real data. Our future work in-
and resampling as in Section 4.2 and “Real” used in Section 4.3.   cludes applying our method to a wider variety of real data, such
The high-quality speech to be modiﬁed was sampled from the         as where multiple speakers are speaking at the same time. Acknowledgements: Part of this work was supported by
                                                                   JSPS KAKENHI Grant Number 21H04900. ",cs.SD,C,-0.32366338,-0.0629253,0.065296404
http://arxiv.org/pdf/2203.13285v1,Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition,"arousal as a metric, in order to judge the overall perfor-
                                                                mance of the models. We leave the analysis of trade-offs
   For the cross-modal attention (CMA) models, replacing        between valence and arousal for future work. FaceNet with MobileFaceNet also increases performance,
from 0.378 to 0.392. ",cs.SD,A,0.11703189,-0.27565795,-0.09832212
http://arxiv.org/pdf/2203.13285v2,Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition,"mance of the models. We leave the analysis of trade-offs
   For the cross-modal attention (CMA) models, replacing        between valence and arousal for future work. FaceNet with MobileFaceNet also increases performance,          7. ",cs.SD,A,0.057542153,-0.27218938,-0.030044388
http://arxiv.org/pdf/2203.13497v1,WaveFuzz: A Clean-Label Poisoning Attack to Protect Your Voice,"We plan to design
creases as the SNR increases, whereas a low SNR indicates          more effective methods to avoid various information in voice
more noise added in the clean example. The high SNR or             data extracted by various audio models as future work. slight noise limits us from fully interfering with the sample,
so the feature distance between the original example and the       6 Conclusion
poisoned example is not huge enough to confuse the mali-
cious model. ",cs.SD,C,-0.16822265,0.03468109,-0.12070379
http://arxiv.org/pdf/2203.13628v1,DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning,"We also show this through an experimental          and architectural preferences, most of which are inspired
study by altering the batch size in upstream SSL pre-training           from prior art. This will help future work from making
of DeLoRes-M. Adding to this, through layer-wise probing                wiser decisions for their experimental setup. ",cs.SD,A,0.35186368,-0.24208984,0.053332172
http://arxiv.org/pdf/2203.13628v2,DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning,"We also show this through an experimental          and architectural preferences, most of which are inspired
study by altering the batch size in upstream SSL pre-training           from prior art. This will help future work from making
of DeLoRes-M. Adding to this, through layer-wise probing                wiser decisions for their experimental setup. ",cs.SD,A,0.35186368,-0.24208984,0.053332172
http://arxiv.org/pdf/2203.13645v1,Audio-text Retrieval in Context,"Overall, our approach of
for the failure example in the right column of Table 3, the top three   incorporating PANNs features combined with NetRVLAD delivers
retrievals are all semantically aligned with the given audio. state-of-the-art performance for audio-text retrieval, hereby provid-
                                                                        ing additional directions for further research and contributing to the
4.4. Comparison with state-of-the-art                                   promotion of content-based retrieval solutions. ",cs.SD,B,-0.08442926,0.20790182,0.03817713
http://arxiv.org/pdf/2203.13645v2,Audio-text Retrieval in Context,"Overall, our approach of
for the failure example in the right column of Table 3, the top three   incorporating PANNs features combined with NetRVLAD delivers
retrievals are all semantically aligned with the given audio. state-of-the-art performance for audio-text retrieval, hereby provid-
                                                                        ing additional directions for further research and contributing to the
4.4. Comparison with state-of-the-art                                   promotion of content-based retrieval solutions. ",cs.SD,B,-0.08442926,0.20790182,0.03817713
http://arxiv.org/pdf/2203.14409v1,SMP-PHAT: Lightweight DoA Estimation by Merging Microphone Pairs,"This makes SMP-PHAT well-
suited for embedded processing on portable devices with lim-            ∆
ited computing ressources. In future work, the method could
be adapted for multiple sound source localization, and combine          SRP
with sound source tracking algorithms. Time-frequency mask-
ing could also be introduced to ﬁlter undesirable sound sources  Additions SMP
and focus on sources of interest, such as speech. ",cs.SD,B,-0.080068775,0.31139013,-0.007202739
http://arxiv.org/pdf/2203.14688v1,Training speaker recognition systems with limited data,"Such a change could then perhaps result in even better perfor-
                                                                                             mance on the tiny datasets, and speciﬁcally on tiny-few-sessions. We are also interested in future work pre-training on a dataset
                                                                                             other than LibriSpeech, which has limited variability per speaker. It might also be relevant to pre-train on VoxCeleb2, so that the
                                                                                             model has a prior on speech patterns, requiring less ﬁne-tuning. ",cs.SD,C,-0.21364838,-0.10376151,-0.008556189
http://arxiv.org/pdf/2203.14725v1,vTTS: visual-text to speech,"When listening to speech    speech, and 3) it can generate more natural speech from OOV
synthesized from the Koruri typeface, 79% of the listeners an-     and rare characters. In future work, we plan to apply our method
swered that it sounded joyful, and 89% of the listeners answered   to a variety of media such as manga or posters. We hope that
that it sounded sad when listening to speech synthesized from      this research will serve as a ﬁrst benchmark in the development
the Aiharahudemozikaisyo typeface. ",cs.SD,C,-0.25228179,0.011936726,-0.24248748
http://arxiv.org/pdf/2203.15249v1,MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification,"Our ablation study shows the com-
                                                                      bination of local and global feature modeling can lead to the
                                                                      robust speaker embedding extraction, this can provide inspira-
                                                                      tion for the future ASV system design and acceleration. In the
                                                                      future work, we will extend the MFA-Conformer for streaming
                                                                      speaker recognition scenarios. 6. ",cs.SD,C,-0.16420598,0.08246104,0.054341782
http://arxiv.org/pdf/2203.15249v2,MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification,"Our ablation study shows the com-
                                                                 bination of local and global feature modeling can lead to the
       12 2.61 0.224 2.65 0.238                                  robust speaker embedding extraction, this can provide inspira-
                                                                 tion for the future ASV system design and acceleration. In the
       1 3.38 0.280 3.73 0.308                                   future work, we will extend the MFA-Conformer for streaming
                                                                 speaker recognition scenarios. Conformer 3 2.15 0.186 2.14 0.195
                 6 1.29 0.137 1.63 0.153                                       6. ",cs.SD,C,-0.09362187,0.050183944,-0.018692411
http://arxiv.org/pdf/2203.15276v1,Applying Syntax$\unicode{x2013}$Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis,"psychological reality of the phonological structures. However,
                                                                                                                 since the principles of neural net TTS are different from those
baseline 2 (phonemes, accents, initial lowering, and dependency length)                                          of human speech, this proposal needs further research toward
    4N: k i / n o o #6 y a / m a \ n a sh i n o #1 m o / r i \ g u ch i n o #1 a / n i \ y o m e n o             improving the psychological reality of computational models. #1 w a / r u \ g u ch i o #2 k o / o e N d e #1 ts U / t a e t a . ",cs.SD,C,-0.14259803,-0.17436282,-0.17770627
http://arxiv.org/pdf/2203.15335v2,Iranian Modal Music (Dastgah) detection using deep neural networks,"The variety of art-
                                                             ists and instruments in the Nava dataset is an appropriate
Figure 5. Confusion matrix                                   starting point for future works that could create a bench-
                                                             mark. Furthermore, in our opinion, the cooperation of
        Mode      Precision  Recall  F1-Score  Support       online music platforms, artists, and scientists in this field
                                                             can be valuable in creating a more suitable dataset, includ-
        Shur        0.90     0.90      0.90      63          ing ensemble performances, in which the samples can be
       Segah        0.89     0.83      0.86      69          labeled by Gushe instead of Dastgah. ",cs.SD,B,0.13146396,0.22254667,-0.018727252
http://arxiv.org/pdf/2203.15377v1,Spoofing-Aware Speaker Verification by Multi-Level Fusion,"ous work [30]. In the future work, we will introduce more CM
models to investigate the potential of the proposed method. [20] J.-w. Jung, H.-j. ",cs.SD,A,0.4197049,-0.026821136,-0.17348975
http://arxiv.org/pdf/2203.15379v2,VoiceMe: Personalized voice generation in TTS,"faces moving from a 2.7 MOS at iteration 0 to a MOS of 4.0          and face rather than intelligibility. For future work, we may
(‘Good match’) in the later iterations (Wilcoxon rank sum test,     also explore running iterations within the same participants and
Z = .42, p < 0.001, Bonferroni-adjusted). The trend is followed     without aggregating multiple responses per iteration (see [10, 9]
by the cartoons (Wilcoxon rank sum test, Z = .18, p < 0.001,        for an exploration of these options). ",cs.SD,A,0.41990578,-0.14204787,-0.1837413
http://arxiv.org/pdf/2203.15468v1,Machine Composition of Korean Music via Topological Data Analysis and Artificial Neural Network,"Also, the current research used a rather simple periodic extension method for
generating the training data set from the given seed music. Our future work will conduct a study on
how to provide training data when the number of considered music pieces is small. These should be
fully considered for the construction of more generalized machine composition of Korean music. ",cs.SD,B,0.06019949,0.38220453,-0.022475727
http://arxiv.org/pdf/2203.15568v1,A Dataset for Speech Emotion Recognition in Greek Theatrical Plays,"effective. This indicates that future works in the
ﬁeld of recognizing emotions in theatrical data should     Ekman, P. (1992). An argument for basic emotions. ",cs.SD,B,0.006145619,-0.044366635,-0.25446594
http://arxiv.org/pdf/2203.15683v1,DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning,"In the Noise condition,      outperformed existing noise-robust TTS methods with respect
the average MOS of Noise-robust TTS was greater than that of       to both objective and subjective measures in conditions includ-
Degraded GT and Enhancement TTS; this is consistent with the       ing reverberation. Our future work includes conducting experi-
result of the previous study [3]. Moreover, DRSpeech outper-       ments with a wider variety of types of distortion. ",cs.SD,C,-0.043367274,0.1605148,-0.16850564
http://arxiv.org/pdf/2203.15683v2,DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning,"Because a                 ing reverberation. Our future work includes conducting experi-
noise is added to the entire speech utterance, DRSpeech, which            ments with a wider variety of types of distortion. considers global conditions, may be expected to achieve better
results than Noise-robust TTS. ",cs.SD,C,-0.21834895,-0.0032999003,-0.20909753
http://arxiv.org/pdf/2203.15873v1,An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion,"Similarly, the dataset in Choi and Hahn [20] includes       of samples; however, there are non-speech utterances included,
100 sentences in 4 different emotional categories (neutral, anger,    such as laughter and yawn. The collection and release of suit-
happiness, and sadness) pronounced by 5 Korean actors and 5           able datasets to the public would foster further research in this
Korean actresses, for a total of 4 000 utterances. promising ﬁeld and help improve the performance. ",cs.SD,C,-0.2074888,-0.1728811,-0.18376034
http://arxiv.org/pdf/2203.15966v1,Federated Domain Adaptation for ASR with Full Self-Supervision,"We highlight that ﬁnetuning only        tion costs by adapting only a subset of weights in the model. the key/value matrices in the attention not only achieves a low    We will further explore effects of non-IIDness and differential
target domain WER (4.32), but also prevents catastrophic for-      privacy on our proposed techniques as part of future work. getting on the source domain as the number of updates increases
(see ﬁgure 3). ",cs.SD,A_centroid,0.32666957,-0.32799554,0.13462588
http://arxiv.org/pdf/2203.15966v2,Federated Domain Adaptation for ASR with Full Self-Supervision,"Second, we explored
                                                                   techniques reducing the FL compute and network communica-
Attention               8.2 (11%)         4.35        6.16         tion costs by adapting only a subset of weights in the model. We will further explore effects of non-IID-ness and differential
KeyValue ( 5 ) 4.1 (5.7%)                 4.32        5.95         privacy on our proposed techniques as part of future work. Predictor               1.7 (2.5%)        4.68        6.19

Joiner                  3.1 (4.4%)        4.70        6.20

Bias                    0.2 (0.3%)        4.35        5.75
                   5. ",cs.SD,A,0.32953334,-0.37003115,0.0062699206
http://arxiv.org/pdf/2203.16037v1,Enhancing Zero-Shot Many to Many Voice Conversion with Self-Attention VAE,"taining the generated voice quality of VAE while signiﬁcantly
                                                                    increasing speaker classiﬁcation accuracy of converted utterance
     For each converted utterance, the voice quality is measured    of seen/unseen speakers. In future work, we plan to extend the
by MOSNet [26] that predicts human ratings of converted speech. self-attention layer in this study to more advanced form and
MOSnet score ranges from 1 to 5, with lowest score of 1 and         explore cross-attention as well for speaker conversion tasks. ",cs.SD,C,-0.26539564,-0.16353318,-0.12830856
http://arxiv.org/pdf/2203.16037v2,Enhancing Zero-Shot Many to Many Voice Conversion with Self-Attention VAE,"Evaluation                                                    while signiﬁcantly increasing speaker classiﬁcation accuracy
                                                                 of converted utterance of seen/unseen speakers. In future work,
   To evaluate our method, we consider two objective metrics:    we plan to explore some of the feature extraction and attention
i) voice quality and ii) speaker classiﬁcation accuracy. Both    structures in [26] to further reduce generalization error. ",cs.SD,C,-0.28320932,-0.09494436,-0.13162927
http://arxiv.org/pdf/2203.16085v1,"Combination of Time-domain, Frequency-domain, and Cepstral-domain Acoustic Features for Speech Commands Classification","In section 3, we intro-                                                      Pre-processed
duce the back-end feature score linear combination method. In                                                           waveform
section 4, we discuss the experimental setup and results, and ﬁ-
nally, we provide a conclusion of the paper and some directions                                                Sampled
for future work in Section 5.                                                                                  value

                                                                                                                                                                                                         TTiimmee

2. Features Extraction

2.1. ",cs.SD,B,0.18009596,0.13360733,0.060169157
http://arxiv.org/pdf/2203.16085v2,"Combination of Time-domain, Frequency-domain, and Cepstral-domain Acoustic Features for Speech Commands Classification","In section 3, we intro-                                                      Pre-processed
duce the back-end feature score linear combination method. In                                                           waveform
section 4, we discuss the experimental setup and results, and ﬁ-
nally, we provide a conclusion of the paper and some directions                                                Sampled
for future work in Section 5.                                                                                  value

                                                                                                                                                                                                         TTiimmee

2. Features Extraction

2.1. ",cs.SD,B,0.18009596,0.13360733,0.060169157
http://arxiv.org/pdf/2203.16141v1,Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis,"Experiments show that our approach can
# Criticism  35                                                                             outperform the baselines, and achieve average score of 52.89 %
                                                                                            and unweighted average recall of 46.82 %. In future work, we
                      25                                                                    will explore the affect of adversarial attacks by analysing the
                                                                                            attention map of adversarial data. We also plan to explore other
             18          1513     89                                                        types of explanations such as counterfactuals [34]. ",cs.SD,A,0.08353703,-0.32293063,0.14460018
http://arxiv.org/pdf/2203.16646v1,Generation of Speaker Representations Using Heterogeneous Training Batch Assembly,"Inspired by the concept of guided learning
VBD-based resegmentation have a good combination effect. in [32], our future work will focus on how to make better
                                                                    use of speaker-change information, such as letting the output
   2) The CHiME-6 task: In the CHiME-6 task, we compared            of the last ResNet-based layer (prior to temporal pooling)
different speaker diarization systems in terms of 3 metrics,        contain information to distinguish the temporal speaker-change
including the DER, Jaccard error rate (JER) [31], and word          characteristics. error rate (WER). ",cs.SD,C,-0.2138995,-0.07263737,0.039885987
http://arxiv.org/pdf/2203.16738v1,Improving speaker de-identification with functional data analysis of f0 trajectories,"It should be noted that FDA in this study
focused on mean curves of voice conditions while examining curves of individ-
ual speakers might have revealed more details about the relation of speaker-
dependent and disguise-dependent f0 variation. Using FDA for modelling voice
disguise strategies and speaker-dependent f0 curves would be a potential topic
for future work. 7. ",cs.SD,C,-0.15440801,0.020437023,-0.32098573
http://arxiv.org/pdf/2203.16772v1,Learning Decoupling Features Through Orthogonality Regularization,"Our pro-
                                                                posed method reaches SOTA. In future work, we will explore
Model                 KWS               SV                      the usability of orthogonality regularization in other tasks,
                                                                such as speaker veriﬁcation and emotion classiﬁcation; intent
KWS single-task 1.86 ± 0.10             -                       detection and text sentimental classiﬁcation [24]. SV single-task        -      2.27 ± 0.03                                                5. ",cs.SD,C,-0.08902328,-0.15208994,-0.08705735
http://arxiv.org/pdf/2203.16930v1,WavThruVec: Latent speech representation as intermediate features for neural speech synthesis,"Although we use WavThruVec as
throughout all the converted voices. In contrast, WavThruVec                                                                                            a two-stage architecture, future work can explore joint training
discards most of the acoustic properties through intermediate                                                                                           with a WAV2VEC-like objective. Extending this approach with
                                                                                                                                                        additional modules e.g. ",cs.SD,C,-0.3775962,0.03065554,0.09166485
http://arxiv.org/pdf/2203.16930v2,WavThruVec: Latent speech representation as intermediate features for neural speech synthesis,"However, individual phonetic segments                                                                                          listening tests with regards to speech naturalness, outperforming
can vary between speakers (Figure 3b), while VITS forces the                                                                                            state-of-the-art TTS systems. Although we use WavThruVec as
converted voice to match the input one’s characteristics (Figure                                                                                        a two-stage architecture, future work can explore joint training
3a). This is particularly evident, for example, in the case of the                                                                                      with a WAV2VEC-like objective. ",cs.SD,C_centroid,-0.33138597,-0.08708344,-0.10026209
http://arxiv.org/pdf/2203.17023v1,CTA-RNN: Channel and Temporal-wise Attention RNN Leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition,"The experi-                       beddings across different speakers and domains. In future work,
mental results are reported in Table 2. Compared with the E(i)                      we intend to explore the correlation of emotional speech in sev-
                                                                                    eral languages and expand on our work to multi-lingual SER. ",cs.SD,C,-0.1755257,-0.22647542,-0.37680143
http://arxiv.org/pdf/2203.17113v1,Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data,"10 937–10 947. [24] T. Hori, S. Watanabe, and J. Hershey, “Joint CTC/attention decod-
                                                                                ing for end-to-end speech recognition,” in Proceedings of the 55th
 [8] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                 Annual Meeting of the Association for Computational Linguistics
      K. Han, and X. Li, “A further study of unsupervised pretraining           (Volume 1: Long Papers), Jul. 2017, pp. ",cs.SD,C,-0.21660124,-0.3246496,0.09450257
http://arxiv.org/pdf/2203.17113v2,Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data,"10 937–10 947. [24] T. Hori, S. Watanabe, and J. Hershey, “Joint CTC/attention decod-
                                                                                ing for end-to-end speech recognition,” in Proceedings of the 55th
 [8] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                 Annual Meeting of the Association for Computational Linguistics
      K. Han, and X. Li, “A further study of unsupervised pretraining           (Volume 1: Long Papers), Jul. 2017, pp. ",cs.SD,C,-0.21660124,-0.3246496,0.09450257
http://arxiv.org/pdf/2203.17242v1,Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities,"Lin-SVC and LR classiﬁers performed signiﬁcantly better than
KNN and RF. In future work, we intend to prioritise the expansion of the
                                                                    dataset with additional transcriptions. A larger dataset would
     Figure 2 shows the Receiver Operating Characteristic           open up the possibility of using more sophisticated classiﬁca-
(ROC) curves for one of the representative runs of the best per-    tion models. ",cs.SD,A,0.16746043,-0.122576736,0.06890303
http://arxiv.org/pdf/2204.00061v1,Data-augmented cross-lingual synthesis in a teacher-student framework,"We conclude      ing the approach feasible for low-resource languages. that speech from the proposed model is able to retain the level
of intonation variation present in teacher forced data rather than       We see several opportunities for further research. First, we
the lower intonation variation from the augmented speech. ",cs.SD,C,-0.23762105,-0.16375896,-0.1505312
http://arxiv.org/pdf/2204.00331v1,Using segment-based features of jaw movements to recognize foraging activities in grazing cattle,"Assuming literature values
of 1.5 kg/h of dry matter consumption (Galli et al., 2018), previous val-
ues correspond to an approximate consumption with a median of 14.2 kg
(interquartile range 12.3 - 18.3 kg) and 12.0 kg (interquartile range 11.0 -
12.8 kg) for BUFAR and JMFAR respectively. Certainly, additional research
would be necessary to evaluate the application of acoustic methods for es-
timation of herbage dry matter intake. As the pioneer work by Laca et al. ",cs.SD,B,0.10756907,0.24258898,-0.16896781
http://arxiv.org/pdf/2204.00331v2,Using segment-based features of jaw movements to recognize foraging activities in grazing cattle,"Assuming literature values
of 1.5 kg/h of dry matter consumption (Galli et al., 2018), previous val-
ues correspond to an approximate consumption with a median of 14.2 kg
(interquartile range 12.3 - 18.3 kg) and 12.0 kg (interquartile range 11.0 -
12.8 kg) for BUFAR and JMFAR respectively. Certainly, additional research
would be necessary to evaluate the application of acoustic methods for es-
timation of herbage dry matter intake. As the pioneer work by Laca et al. ",cs.SD,B,0.10756907,0.24258898,-0.16896781
http://arxiv.org/pdf/2204.00652v1,End-to-end multi-talker audio-visual ASR using an active speaker attention module,"Comparison of WERs for T-T based SingleChan and Mul-            single speaker utterances in the training set. The goal of future work
                                                                         is to overcome the degradation associated with this and other unseen
tiTalker models on Single and Overlap test sets. conditions by further augmenting the training set. ",cs.SD,C,-0.21229193,-0.13564143,-0.19910333
http://arxiv.org/pdf/2204.00819v1,Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition,"In addition, deep investigations are carried out into dif-
                                       ferent units in masking, which shows the effectiveness of our          We try to solve PR from a mask training perspective un-
                                       proposed masking unit. We also further study the masking          der the E2E ASR framework. Recently, a lot of excellent works
                                       method and optimize ﬁlling strategy of phone mask. ",cs.SD,A,0.11990252,-0.14579116,0.098192886
http://arxiv.org/pdf/2204.00821v1,Improving Target Sound Extraction with Timestamp Information,"We have shown the pos-
time-domain and frequency-domain loss, we can get our base-          sibility of mutual learning for an extraction task and a detection
line performance of 10.25 for SI-SDRi and 5.14 for SI-SDRi-t.        task. A future work is to further improve the SI-SDRi-t, and
Comparing with [7] and [11], our baseline method has already         we think designing new sound extractors is a potential solution. surpassed them as they only use the time-domain loss. ",cs.SD,B,-0.017493833,0.13059127,0.23350766
http://arxiv.org/pdf/2204.00873v1,Acoustic-to-articulatory Inversion based on Speech Decomposition and Auxiliary Feature,"Ablation Study                                                                                  public datasets demonstrate that both in speaker-dependent
                                                                                                     and speaker-independent scenarios, the SAFN outperforms
To verify the effectiveness of the proposed modules in Sec-                                          SOTA by a large margin. For the future work, the self-
tion 2, we carry out the ablation experiment according to Ta-                                        supervised method based on Meta Learning will be applied
ble 3, and the results are shown in Fig. 3. ",cs.SD,C,-0.10314623,-0.110413805,0.014382966
http://arxiv.org/pdf/2204.00907v1,StyleWaveGAN: Style-based synthesis of drum sounds with extensive controls using generative adversarial networks,"To the best of our knowledge the proposed DNN
                                                            is the ﬁrst achieving drum synthesis with 44.1kHz sample
                                                            rate (for sounds with a duration of 1.5s) with an inference
                                                            speed more than 50 times faster than real time on a con-
                                                            sumer GPU. In terms of future work we will continue to work on the
                                                            sound quality and additional controls, notably regarding
                                                            velocity. Figure 7. ",cs.SD,B,0.017591566,0.24861448,0.18851912
http://arxiv.org/pdf/2204.01115v1,On incorporating social speaker characteristics in synthetic speech,"played lower or equal MOS scores of warmth and competence
                                                                  with that of the baseline. The future work could be to investigate
     Figure 9 displays the plot with the Mean Opinion scores      the feature quantization on male synthetic speech. Additionally,
(MOS) collected for the characteristic, competence across the     a comparison between the female and male speech could also be
acoustic features, Slope, Spectral Flux and their convex com-     an interesting study. ",cs.SD,C,-0.2943202,-0.07597158,-0.25555804
http://arxiv.org/pdf/2204.01295v1,Nonlinear Vectorial Prediction with Neural Nets,"The algorithm is the following:

Computation of the correlation matrices Ri :

Ri  Ni1 xn  ixnT

         n0

where xnT means the transpose of the vector xn. for i=0:P-1,

              i

          Fi  A1ij Ri1 j
                 j0

          K1i  Fi D2i 1

          K 2i  FiT D1i 1
          D1i1  I  K1i K 2i D1i
          D2i1  I  K 2i K1i D2i

          A1i01  A1i0

          A2i01  K 2i A1i0

          A1ii11  K1i A2ii

          A2ii11  A2ii
          for j=1:i,

                            A1ij1  A1ij  K1i A2ij1
   Marcos Faundez-Zanuy “Nonlinear vectorial prediction with neural nets” IWANN
'01: Proceedings of the 6th International Work-Conference on Artificial and Natural
Neural Networks: Bio-inspired Applications of Connectionism-Part II June 2001
Pages 754–761 doi: 10.5555/646370.688874

   __________________________________________________________________
                      A2ij1  A2ij1  K 2i A1ij

          end
end
for i=0:P,

           Ai  A2iP
end

Conclusions and future work

   A PVQ system implies a good predictor and a good quantizer. In this paper we
have first proposed a MLP training with output hints, in order to check that the pro-
posed scheme for vectorial prediction is suitable. ",cs.SD,A,0.30944222,-0.055237137,0.12098728
http://arxiv.org/pdf/2204.02121v1,MetaAudio: A Few-Shot Audio Classification Benchmark,"In this work, we look to alleviate this gap by contributing the following: 1) Experimental evaluation of some of the most
popular few-shot classiﬁers on a variety of audio datasets, spanning multiple sub-settings from environmental sounds
to speech. 2) A fully reproducible few-shot audio classiﬁcation benchmark with at least one published evaluation
split per dataset along with custom data loading allowing for quick plug and play testing in future works. 3) A
generalised prescription for dealing with variable length audio datasets in a few-shot setting. ",cs.SD,B,-0.18111329,0.23687443,0.16732702
http://arxiv.org/pdf/2204.02121v2,MetaAudio: A Few-Shot Audio Classification Benchmark,"In this work, we look to alleviate this gap by contributing the following: 1) Experimental evaluation of some of the most
popular few-shot classiﬁers on a variety of audio datasets, spanning multiple sub-settings from environmental sounds
to speech. 2) A fully reproducible few-shot audio classiﬁcation benchmark with at least one published evaluation
split per dataset along with custom data loading allowing for quick plug and play testing in future works. 3) A
generalised prescription for dealing with variable length audio datasets in a few-shot setting. ",cs.SD,B,-0.18111329,0.23687443,0.16732702
http://arxiv.org/pdf/2204.02143v1,RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection,"Because of the multi-scale feature extractor     Fundamental Research Programs JSGG20191129105421211
and duration-aware focal loss, RaDur provides signiﬁcant
improvement on transient events. However, the detection                  1https://github.com/yangdongchao/RaDur
results for transient and long events still have a large gap,
which deserves further study in our future work. and GXWD20201231165807007-20200814115301001. ",cs.SD,A,0.36261082,-0.1358683,0.14538696
http://arxiv.org/pdf/2204.02172v1,AILTTS: Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech,"Analysis                                                      TTS based model (+0.36 MOS) and LiteTTS (+0.22 MOS)
                                                                   while preserving lightweight properties. As future work, we
Ablation Study We conducted a CMOS (comparison MOS)                will design a generative model-based stochastic prosody predic-
evaluation on the test set to determine whether the proposed two   tor to further extend prosody controllability. In addition, we
methods enhance the performance of the model. ",cs.SD,C,0.047082253,-0.028169451,-0.26868117
http://arxiv.org/pdf/2204.02279v1,How Information on Acoustic Scenes and Sound Events Mutually Benefits Event Detection and Scene Classification Tasks,"On the other hand, the impact of unlabeled
and event layers or train models with fake labels, the perfor-    sound events, such as background noise, on ASC must be fur-
mance of ASC and SED tends to decrease. These results indi-       ther investigated in future work. cate that pieces of information on acoustic scenes and sound
events are effectively used to detect sound events and clas-                        5. ",cs.SD,B,-0.009822087,0.20106113,0.024477914
http://arxiv.org/pdf/2204.03040v1,SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis,"On the model side, SSL has the best overall      in utterance-level evaluation as regards synthesized utterances
performance, even on its pretrained version, conﬁrming supe-        with prosodic variability. In future work, our goal is to utilize
rior generalization capability compared to other models. this dataset so as to develop models which correlate better to
                                                                    human evaluators on the utterance-level TTS assessment task. ",cs.SD,C,-0.29844898,-0.22759433,-0.22629572
http://arxiv.org/pdf/2204.03040v2,SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis,"with prosodic variability. In future work, our goal is to utilize
                                                                         this dataset so as to develop models which correlate better to
     The high system-level performances observed can be at-              human evaluators on the utterance-level TTS assessment task. tributed to the evaluation setup, as the validation set contains
systems which are seen during training and the unseen systems                          6. ",cs.SD,C,-0.29172266,-0.14604838,-0.22257125
http://arxiv.org/pdf/2204.03178v1,"3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition","Our baseline Conformer model is implemented by Wenet                 5. Conclusions and future work
toolkits and it consists of two convolution downsampling layers,
a 18-block Conformer encoder and a 4-block bi-transformer de-      In this paper, we introduce the 3M model for speech recogni-
coder (2-block for forward and 2-block for backward). For MoE      tion. ",cs.SD,C,-0.08533238,-0.029114818,0.069316275
http://arxiv.org/pdf/2204.03178v2,"3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition","Our baseline Conformer model is implemented by Wenet                 5. Conclusions and future work
toolkits and it consists of two convolution downsampling layers,
a 18-block Conformer encoder and a 4-block bi-transformer de-      In this paper, we introduce the 3M model for speech recogni-
coder (2-block for forward and 2-block for backward). For MoE      tion. ",cs.SD,C,-0.08533238,-0.029114818,0.069316275
http://arxiv.org/pdf/2204.03240v1,Speech Pre-training with Acoustic Piece,"7694–7698. [3] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,
      K. Han, and X. Li, “A further study of unsupervised pretraining
      for transformer based speech recognition,” in ICASSP 2021-2021
      IEEE International Conference on Acoustics, Speech and Signal
      Processing (ICASSP). IEEE, 2021, pp. ",cs.SD,C,-0.27365735,-0.045611545,0.15647826
http://arxiv.org/pdf/2204.03594v1,Heterogeneous Target Speech Separation,"4, we explore how           to be less effective for speech separation in general. Mitigating
adding an easier signal characteristic, spatial location S, im-          this issue is an important topic of future work. proves performance on the more difﬁcult language condition-
ing L. Surprisingly, the best performance for language condi-                               5. ",cs.SD,C,-0.13232842,-0.10913733,-0.14142708
http://arxiv.org/pdf/2204.04166v1,Self-supervised Speaker Diarization,"For a fair comparison, we added the performance
of our system against using the oracle VAD labels. In this set-          For future work, we will explore ways of dealing with over-
ting, we achieve a 6.58% DER comparable to their 6.63%. As          lapping speech, which is not referenced in this work. ",cs.SD,C,-0.031032536,-0.09532171,-0.17782061
http://arxiv.org/pdf/2204.04464v1,Multichannel Speech Separation with Narrow-band Conformer,"Conclusions
very small network, as the numbers of hidden units are H1=192
and H2=384, and total number of parameters are 2.0 M. Since        This work has proposed a multichannel speech separation
the proposed narrow-band network needs to run for all frequen-     method with narrow-band Conformer, named NBC. The net-
cies independently, its RTF (=1.32) cannot reach the real time     work learns to automatically exploit narrow-band information
requirement yet, which can be easily solved by further reduce      to perform speech separation, such as based on spatial vector
the network size as for future works. Some audio examples can      clustering, while the self-attention mechanism is especially suit-
be found in our webpage 3.                                         able for this task. ",cs.SD,C,-0.104844466,0.064548224,0.13264573
http://arxiv.org/pdf/2204.04464v2,Multichannel Speech Separation with Narrow-band Conformer,"Experiments show that the proposed narrow-
cies independently, its RTF (=1.32) cannot reach the real time     band Conformer is indeed able to clustering the frames as-
requirement yet, which can be easily solved by further reduce      sociated to the same speaker. Overall, the proposed method
the network size as for future works. Some audio examples can      achieves the state-of-the-art performance, and surpasses other
be found in our webpage 3.                                         methods by a large margin. ",cs.SD,B,0.008083589,0.2664854,-0.04958185
http://arxiv.org/pdf/2204.04579v1,Inferring Pitch from Coarse Spectral Features,"Complicated covariates                                                                    gracefully. We propose that this is a difﬁcult but meaningful
                                                                                               direction for future work in modeling prosodic features. The success of single-frame MFCC features predicting pitch is
unexpected. ",cs.SD,C,-0.24868356,0.046671595,-0.102173276
http://arxiv.org/pdf/2204.04645v1,Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data,"We also show the effectiveness of
                                                                       different components via detailed ablation studies and analy-
                                                                       sis. For future work, we consider extensions of our method
                                                                       to more paired modalities (i.e., vision-and-language). Acknowledgements                                Lample, G.; Conneau, A.; Denoyer, L.; and Ranzato, M.
                                                                 2018a. ",cs.SD,A,0.11581625,-0.043223992,-0.20792386
http://arxiv.org/pdf/2204.05070v1,Fine-grained Noise Control for Multispeaker Speech Synthesis,"Comparing the full proposed model    trol and remove the inherent noise that exists in crowd-sourced
(9) with the alternative without the FVAE (8), we show that by     data. In addition, for future work, we would also like to in-
performing ﬁne-grained prosody modeling we encourage the           vestigate the capability of the proposed method to learn richer
residual encoder to capture other factors than prosody and so,     noise representations in order to further improve the synthesized
result in more clean speech. Supervised noise modeling (2,4)       speech quality. ",cs.SD,C,-0.33483315,-0.052926607,-0.04209729
http://arxiv.org/pdf/2204.05070v2,Fine-grained Noise Control for Multispeaker Speech Synthesis,"Comparing the full proposed model    trol and remove the inherent noise that exists in crowd-sourced
(9) with the alternative without the FVAE (8), we show that by     data. In addition, for future work, we would also like to in-
performing ﬁne-grained prosody modeling we encourage the           vestigate the capability of the proposed method to learn richer
residual encoder to capture other factors than prosody and so,     noise representations in order to further improve the synthesized
result in more clean speech. Supervised noise modeling (2,4)       speech quality. ",cs.SD,C,-0.33483315,-0.052926607,-0.04209729
http://arxiv.org/pdf/2204.05156v1,How to Listen? Rethinking Visual Sound Localization,"We also show that it is critical to have evalua-
                                                                                                                  tion datasets featuring diverse conditions, including location of
                                                                                                                  sources, number of sources and size. For future work, we would
                                                                                                                  like to explore the use of pre-processing and de-noising tech-
                                                                                                                  niques to help boost the foreground objects versus the back-
                                                                                                                  ground noises in the audio, include temporal context via se-
                                                                                                                  quences of images, and include spatial information from multi-
                                                                                                                  channel audio to further distinguish moving from still objects. 6. ",cs.SD,B,-0.07114444,0.16047041,0.25179222
http://arxiv.org/pdf/2204.05445v1,Small Footprint Multi-channel ConvMixer for Keyword Spotting with Centroid Based Awareness,"In par-   model of small footprint KWS. At last, we anticipate a further
ticular, we refer to [12, 22, 23] and modify our networks into a  gain with non-linear neural enhancement incorporated within
multi-look beamforming KWS by simply replacing the raw mi-        the KWS networks in future work. crophone array with a set of beamformed signals. ",cs.SD,A,0.13287516,0.06737114,0.078275435
http://arxiv.org/pdf/2204.05649v1,ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition,"datasets, the model works best when seg len is 20. Note            In future work, we will investigate the ability of our model on
that input with different lengths should have respective opti-     cross-language or cross-genre datasets, and try to introduce pre-
mal seg num parameters, and we have not investigated one by        training to further improve the effectiveness of our model. one here. ",cs.SD,A,0.04944692,-0.07487805,-0.08106851
http://arxiv.org/pdf/2204.05649v2,ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition,"datasets, the model works best when seg len is 20. Note            In future work, we will investigate the ability of our model on
that input with different lengths should have respective opti-     cross-language or cross-genre datasets, and try to introduce pre-
mal seg num parameters, and we have not investigated one by        training to further improve the effectiveness of our model. one here. ",cs.SD,A,0.04944692,-0.07487805,-0.08106851
http://arxiv.org/pdf/2204.06402v1,Sound Event Triage: Detecting Sound Events Considering Priority of Classes,"This implies that more
line system. In particular, as shown in Table 5, the proposed method                                      sophisticated event detection methods can boost SET performance,
which needs to be investigated in future works. Unlike the results                : False positive              # of false positives
in terms of the frame-based F-score, the performance of detecting
many sound events gradually deteriorates as the triage weight in-                                               Intersection- Frame-
creases. ",cs.SD,B,0.12975599,0.27276126,-0.05970426
http://arxiv.org/pdf/2204.07018v1,From Environmental Sound Representation to Robustness of 2D CNN Models Against Adversarial Attacks,"Next, experimental results
and associated discussions are presented in Sections 4 and 5, respectively. Finally, the conclusions
and perspectives of future work are presented in the last section. 2. ",cs.SD,A,0.40270048,-0.05209165,-0.20327823
http://arxiv.org/pdf/2204.07420v1,Deep CardioSound: An Ensembled Deep Learning Model for Heart Sound MultiLabelling,"For these extreme cases, the
network struggled the most with the timing label group,           B. Model performance evaluation at the patient level
which needs to be further investigated in our future work. AV              PV         TV                MV

ID:50238  ID: 46065             ID:49823          ID:49638

ID:84885  ID: 49968             ID:50280          ID:49966

          ID: 84937             ID:72283          ID:50123

                         ID:75440 (1)             ID:50331

                         ID:75440 (2)             ID:69155

Table 5: Incorrectly labelled samples with their patient IDs

We also trained a single model using the data from all four                  Figure 9: The process of patient-level prediction
body positions (with the same training/testing split) to verify
if the model can be body position-independent. ",cs.SD,A,0.29051098,-0.09074438,0.054841034
http://arxiv.org/pdf/2204.07420v2,Deep CardioSound-An Ensembled Deep Learning Model for Heart Sound MultiLabelling,"970  0.998            0. 974
most with the timing label group, which needs to be further
investigated in our future work. 4th              0.979      0.972   0.998            0. ",cs.SD,A,0.41711676,0.11789192,-0.16232672
http://arxiv.org/pdf/2204.07763v1,UFRC: A Unified Framework for Reliable COVID-19 Detection on Crowdsourced Cough Audio,"4. Train Dataset AUC changes in Every Epoch
                                                                  also beneﬁt from this uniﬁed framework and we leave this
   3) Estimation and Application of Uncertainty: In this
section, we estimated the uncertainty of the prediction re-       challenging extension for future work. sults of our proposed framework. ",cs.SD,A,0.24721381,-0.023911346,0.19334501
http://arxiv.org/pdf/2204.07763v2,UFRC: A Unified Framework for Reliable COVID-19 Detection on Crowdsourced Cough Audio,"Thus, it can be concluded
that using ImageNet-pretrained initialization has a huge          also beneﬁt from this uniﬁed framework and we leave this
advantage in generalization and converging time over
random initialization of weights. challenging extension for future work. Fig. ",cs.SD,A,0.24608643,-0.17447306,0.38046497
http://arxiv.org/pdf/2204.08026v1,Advances in Thunder Sound Synthesis,"The frequency response and range          evaluated yet is still distinguishable from the real event. required to reproduce the sound of thunder are gener-      In response to user feedback and subsequent synthesis
ally not offered by computer or mobile phone speakers,     technique analysis, a number of improvements were
however as shown in Table 3, the average rankings          implemented and future work is identiﬁed. across different hardware was negligible. ",cs.SD,B,-0.014031172,0.275457,-0.23169607
http://arxiv.org/pdf/2204.08164v1,Robust End-to-end Speaker Diarization with Generic Neural Clustering,"For example, such a method severely
EEND-EDA [13]        10.91      17.05  25.36  38.58                relies on the quality of local speaker representations, so a good
+ COP-K-means [18]   12.34      20.23  29.21  39.34                chunk-level predictor is important. And it is sensitive to data
                     11.51      18.51  25.19  39.01                adaptation, which should be solved in the future work. Apart
   + switch[18]                                                    from this, some advanced neural architectures like self-attention
                     13.18 18.66       25.46 35.79                 mechanism can also be used to improve the performance. ",cs.SD,C,-0.14981169,-0.17413841,-0.0011672545
http://arxiv.org/pdf/2204.08269v1,Differentiable Time-Frequency Scattering in Kymatio,"How-
networks for supervised classification. We expect this to enable        ever, a clear distinction between them can be observed in terms
further research interfacing JTFS and convnets for audio analysis       of the frequency band alignment in time. Unlike time scattering,
and synthesis. ",cs.SD,B,-0.045274336,0.33057642,0.30069983
http://arxiv.org/pdf/2204.08269v2,Differentiable Time-Frequency Scattering in Kymatio,"How-
networks for supervised classification. We expect this to enable        ever, a clear distinction between them can be observed in terms
further research interfacing JTFS and convnets for audio analysis       of the frequency band alignment in time. Unlike time scattering,
and synthesis. ",cs.SD,B,-0.045274336,0.33057642,0.30069983
http://arxiv.org/pdf/2204.08269v3,Differentiable Time-Frequency Scattering in Kymatio,"How-
networks for supervised classification. We expect this to enable        ever, a clear distinction between them can be observed in terms
further research interfacing JTFS and convnets for audio analysis       of the frequency band alignment in time. Unlike time scattering,
and synthesis. ",cs.SD,B,-0.045274336,0.33057642,0.30069983
http://arxiv.org/pdf/2204.08269v4,Differentiable Time-Frequency Scattering on GPU,"learned 2-dimensional deep convolutional networks for supervised         However, a clear distinction between them can be observed in terms
classification. We expect this to enable further research interfacing    of the frequency band alignment in time. Unlike time scattering,
JTFS and convnets for audio analysis and synthesis. ",cs.SD,B,-0.0024986397,0.3030433,0.4103848
http://arxiv.org/pdf/2204.08345v1,"Extracting Targeted Training Data from ASR Models, and How to Mitigate It","for other sensitive extractions, and it can be practically infea-
                                                                        sible to design mitigation methods customized to each type of
Results with Fixed Noise Duration: In Table 4, we show the              extraction (e.g., like we designed the Name Silencing strategy
results of Noise Masking using train utterances and a ﬁxed dura-        in Section 3.1). While we designed Word Dropout with gen-
tion (set with no knowledge of the masked word duration) of ‘si-        eral applicability in mind, approaches for training ASR models
lence’ noise on the baseline model.4 For reference, we provide          that can provide better protection against leakages of structured
                                                                        training data while achieving utility comparable to SOTA mod-
    4We also conduct experiments for the other ﬁve noises on the base-  els is an interesting direction that we leave for future work. line, and the results show similar trends as shown in Table 4. ",cs.SD,C,-0.022332348,-0.23605272,0.054663796
http://arxiv.org/pdf/2204.08345v2,"Extracting Targeted Training Data from ASR Models, and How to Mitigate It","for other sensitive extractions, and it can be practically infea-
                                                                        sible to design mitigation methods customized to each type of
Results with Fixed Noise Duration: In Table 4, we show the              extraction (e.g., like we designed the Name Silencing strategy
results of Noise Masking using train utterances and a ﬁxed dura-        in Section 3.1). While we designed Word Dropout with gen-
tion (set with no knowledge of the masked word duration) of ‘si-        eral applicability in mind, approaches for training ASR models
lence’ noise on the baseline model.4 For reference, we provide          that can provide better protection against leakages of structured
                                                                        training data while achieving utility comparable to SOTA mod-
    4We also conduct experiments for the other ﬁve noises on the base-  els is an interesting direction that we leave for future work. line, and the results show similar trends as shown in Table 4. ",cs.SD,C,-0.022332348,-0.23605272,0.054663796
http://arxiv.org/pdf/2204.08567v1,Automated Audio Captioning using Audio Event Clues,"Since the same pre-trained network is used in order          It is clear that the inclusion of more semantic information
to obtain acoustic features and audio events, the inclusion of
audio events has less improvement on PANNs features than           will increase the captioning performance. In future work,
log Mel averaging features. Improving audio event extraction
performance and using different acoustic features can give         different methods for adding semantic information to the
better results. ",cs.SD,B,-0.22236498,0.097510815,0.17957881
http://arxiv.org/pdf/2204.08625v1,Self Supervised Adversarial Domain Adaptation for Cross-Corpus and Cross-Language Speech Emotion Recognition,"Further experiments may include evalu-                        103649, 2019.
ating the proposed methods in wild conditions like noisy
speech and adversarial noise. We are also interested in explor-           [6] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm
ing multimodal pretext task techniques in our future work. for deep belief nets,” Neural computation, vol. ",cs.SD,C,-0.07420674,-0.29094434,0.3112049
http://arxiv.org/pdf/2204.09883v1,Layer-wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition,"For example, Basis Two focuses         AESRC2020 cv/test sets and 10% relative WER reduction on
mostly on extracting information about the Portuguese (PT)           Librispeech dev/test sets as well. In future work, we would like
accent, and then the American (US) and Russian (RU) ac-              to investigate different combination methods between the ac-
cents. The inherent correlation between different accents can        cent embedding and acoustic features, i.e. ",cs.SD,C,-0.05493132,-0.034041,-0.15974963
http://arxiv.org/pdf/2204.09911v2,STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency,"closed-form solution is available:

                                    −1                           Note that here we do not use the typical recursive averaging
wˆ (f ; q) = Φˆ (yy)(f ) Φˆ (ys)(f )uq,
                                                      (5)        technique [36], as the target speaker and non-target sources

Φˆ (yy)(f ) = Y(t, f )Y(t, f )H,                      (6)        are assumed not to be moving within each utterance. We will

                       t                                         explore the idea of recursive averaging in future work. Based on the online time-varying ﬁlter wˆ (t, f ; q), the beam-
Φˆ (ys)(f ) = Y(t, f )Sˆ(1)(t, f )H,                  (7)
                       t
                                                                 forming result is obtained as
where Φˆ (yy)(f ) denotes the observed mixture spatial co-
variance matrix, Φˆ (ys)(f ) the estimated covariance matrix        SˆqMCWF(t, f ) = wˆ (t, f ; q)HY(t, f ). ",cs.SD,C,-0.108488806,0.12649687,-0.17808306
http://arxiv.org/pdf/2204.09911v3,STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency,"Since the objective is quadratic, a      technique [36], as the target speaker and non-target sources

closed-form solution is available:                                  are assumed not to be moving within each utterance. We will

                                       −1                           explore the idea of recursive averaging in future work. wˆ (f ; q) = Φˆ (yy)(f ) Φˆ (ys)(f )uq,                                Based on the online time-varying ﬁlter wˆ (t, f ; q), the beam-
                                                      (5)

Φˆ (yy)(f ) = Y(t, f )Y(t, f )H,                      (6)           forming result is obtained as

                       t                                               SˆqMCWF(t, f ) = wˆ (t, f ; q)HY(t, f ). ",cs.SD,C,-0.1052613,0.08522986,-0.2156462
http://arxiv.org/pdf/2204.09976v1,Baseline Systems for the First Spoofing-Aware Speaker Verification Challenge: Score and Embedding Fusion,"We argue that,
protocol. However, the SV-EER increases from 1.66%         with better potential for joint optimisation and hence bet-
for B1 to 11.48%, meaning that, with a vanilla multi-      ter performance, future work should focus on the devel-
layer perception, the discriminative power of the speaker  opment of single, integrated SASV solutions. Their de-
embedding is degraded in the joint representation space. ",cs.SD,C,-0.09033355,-0.086475916,-0.06596376
http://arxiv.org/pdf/2204.10125v1,Physical Modeling using Recurrent Neural Networks with Fast Convolutional Layers,"network. A proper investigation of this topic is left to future work,
with only data sampled on rectilinear spatial grids considered here. 2.2. ",cs.SD,A,0.47737753,0.15661854,0.053211596
http://arxiv.org/pdf/2204.10125v2,Physical Modeling using Recurrent Neural Networks with Fast Convolutional Layers,"(/Hz)               of regularization effect gained by the constriction of the internal
                                                                       states back to the physical states between each time step. Further
Figure 6: Estimated poles of the linear string inferred from the       investigation of this phenomenon is left to future work. model outputs presented in Fig. ",cs.SD,A,0.3095609,0.087762624,-0.050449505
http://arxiv.org/pdf/2204.10561v1,Speaking-Rate-Controllable HiFi-GAN Using Feature Interpolation,"Speciﬁcally, mel-spectrograms output from                                                                efﬁciency and quality performance than the baseline TSM algo-
FastSpeech2 were converted into waveforms by HiFi-GAN af-                                                                  rithm WSOLA. The future work will be testing the generaliza-
ter being interpolated. The experimental setup is almost the                                                               tion ability of the proposed method on other neural vocoders. ",cs.SD,B,-0.00059836917,0.065236,0.08661172
http://arxiv.org/pdf/2204.10749v1,E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR,"We deﬁne utter-       increasing computation. The EOS50 latency is also reduced;
ance length here as the number of words in the ground truth        investigating the cause of this is a point of future work. This
transcript rather than the audio duration (though they are corre-  table suggests that a trade-off between quality and beam search
lated) because it corresponds more closely to the beam search      efﬁciency must be made in resource-constrained situations with
lattice length. ",cs.SD,C,0.030999465,0.03399738,-0.13397308
http://arxiv.org/pdf/2204.10749v2,E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR,"We deﬁne utter-       margin is increased in E13-E20, the WER converges towards
ance length here as the number of words in the ground truth        its value without frame ﬁltering (17.05%), at the cost of slightly
transcript rather than the audio duration (though they are corre-  increasing computation. The EOS50 latency is also reduced;
lated) because it corresponds more closely to the beam search      investigating the cause of this is a point of future work. This
lattice length. ",cs.SD,C,0.007732518,-0.029245082,-0.13682002
http://arxiv.org/pdf/2204.11139v1,Musical Stylistic Analysis: A Study of Intervallic Transition Graphs via Persistent Homology,"A perspective in this direction is given in [4]. We expect to develop this approach in future work. References

 [1] M. G. Bergomi. ",cs.SD,A,0.41151845,-0.054285344,-0.11250031
http://arxiv.org/pdf/2204.11304v1,Dictionary Attacks on Speaker Verification,"That being said, our study still                                                                                                                     sonation capabilities of the given sample). Overall, we believe
has several limitations that warrant further research. While we                                                                                                                     our work will ultimately lead to a better understanding of the
tested multiple speaker encoders based on two different acous-                                                                                                                      speech modality and more secure human-computer interaction. ",cs.SD,C,-0.29542822,-0.073621966,-0.2260294
http://arxiv.org/pdf/2204.11304v2,Dictionary Attacks on Speaker Verification,"We discuss this in more detail in Sections V-C and V-E.

E. Playback simulation                                                An interesting extension of this problem would be to jointly
                                                                   optimize all C samples. We leave this aspect for future work. To assess (and improve) robustness to distortions, we in-
clude an optional playback simulation step. ",cs.SD,B,0.070180066,0.2529809,-0.13093342
http://arxiv.org/pdf/2204.11792v1,SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech,"three datasets (English, Chinese, and multi-speaker, respec-
                                                                                                           tively) and conducted comprehensive ablation studies to ver-
Adversarial Training                                                                                       ify the effectiveness of each component in our model. For
                                                                                                           future work, we will explore the potential of syntax-aware
To demonstrate the effectiveness of adversarial training, we                                               models in other tasks, such as voice conversion and singing
perform a CMOS test on PortaSpeech/SyntaSpeech with the                                                    voice generation. multi-length adversarial training and the post-net. ",cs.SD,C,-0.26833802,-0.25111243,0.10366166
http://arxiv.org/pdf/2204.11942v1,Meta-AF: Meta-Learning for Adaptive Filters,"93, no. 2, 2013.
learning combined with adaptive ﬁlters and hope our complete        [21] L. Lu, K.-L. Yin, R. C. de Lamare, Z. Zheng, Y. Yu, X. Yang, and
code release will stimulate further research and rapid progress. B. Chen, “A survey on active noise control in the past decade—part i:
                                                                          Linear systems,” Elsevier Signal Processing, vol. ",cs.SD,A,0.16725022,0.0561178,0.11428045
http://arxiv.org/pdf/2204.11942v2,Meta-AF: Meta-Learning for Adaptive Filters,"We are excited
parameters and single CPU core RTF of ≈ 0.25.                           about the future of deep learning combined with adaptive
                                                                        ﬁlters and hope our complete code release will stimulate
   IX. DISCUSSION, FUTURE WORK, AND CONCLUSION                          further research and rapid progress. A. ",cs.SD,A,0.22473976,-0.15658826,0.3054645
http://arxiv.org/pdf/2204.12177v1,A Comparative Study on Approaches to Acoustic Scene Classification using CNNs,"Then we cover the experimental process, starting from the preparation of the data and
details on the CNN models used. Finally, we evaluate and discuss the results of the
experiments, and put forward some suggestions for future work. 2 Related Work

Analysis and classification of auditory signals with artificial intelligence have a long
history. ",cs.SD,B,-0.15224814,0.11976366,0.20098555
http://arxiv.org/pdf/2204.12622v1,Named Entity Recognition for Audio De-Identification,"[13] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, and et al.,
   In future work, we think the ﬁrst thing to do is to improve                         “Roberta: A robustly optimized BERT pretraining approach,” CoRR, vol. the training set by creating a more extensive speech corpus                            abs/1907.11692, 2019.
with NER annotations. ",cs.SD,C,-0.15656891,-0.24634615,0.017127296
http://arxiv.org/pdf/2204.13094v1,Unsupervised Word Segmentation using K Nearest Neighbors,"To adapt to this setting, we expect that a
                                                                     smaller window size and a lower layer of the neural network
10 10  avg     27.9 33.2 30.3                                        may achieve the best performance. We leave this investigation
                                                                     for future work. 20 10  avg     28.5 33.8 30.9
                                                                                       5. ",cs.SD,A,0.32599223,-0.0676879,0.06525471
http://arxiv.org/pdf/2204.13206v1,Improving Multimodal Speech Recognition by Data Augmentation and Speech Representations,"To better understand the complicated in-        current line of work. teractions that arise in a multimodal speech recognition net-
work, a future work direction could investigate which parts        Acknowledgements. We thank the anonymous review-
of the inputs (audio, image, previously predicted tokens)       ers and Desmond Elliott for useful suggestions. ",cs.SD,C,-0.22511758,-0.11763564,0.06982696
http://arxiv.org/pdf/2204.13601v1,Emotion Recognition In Persian Speech Using Deep Neural Networks,"Recognition of emotions depends on the type of                speech emotion recognition (SER) approaches consist of
expression that varies between different languages. In this article,  two steps known as feature extraction and feature
to further study this important factor in Farsi, we examine           classification. At the first step of speech processing,
various deep learning techniques on the SheEMO dataset. ",cs.SD,C,-0.13560379,-0.2195994,0.05392222
http://arxiv.org/pdf/2204.13601v2,Emotion Recognition In Persian Speech Using Deep Neural Networks,"In section 3, we
the type of expression that varies between different languages. In  introduce our contribution which includes reviewing the
this paper, to further study important factors in the Farsi         ShEMO data set and two common methods of extracting
language, we examine various DL techniques on a Farsi/Persian       features as LLDs (Low-Level Descriptors) and functionals
dataset, Sharif Emotional Speech Database (ShEMO), which was        from voice signals, as well as testing different DNN models on
released in 2018. Using signal features in low- and high-level      these features. ",cs.SD,C,-0.17186573,-0.19394544,-0.11638908
http://arxiv.org/pdf/2204.13668v1,Unaligned Supervision For Automatic Music Transcription in The Wild,"It can be seen that the contribution of unaligned supervision to offset detection is
small, and increases as the offset tolerance thresholds are increased. We believe frame-level detection, together with offset detection, can be further improved through time-stretching consistency,
and this is an important direction for future work. B.1.8 MAESTRO with unaligned supervision

An important question that arises is what is the accuracy on the test set, when some samples from the test domain, or samples
similar to the test domain, are seen during training, but without labels, only unaligned supervision. ",cs.SD,A,0.26687846,0.0013797395,0.050157845
http://arxiv.org/pdf/2205.00206v1,"Taylor, Can You Hear Me Now? A Taylor-Unfolding Framework for Monaural Speech Enhancement","Note
                                                                   that our framework also applies to other more advanced net-
             q=1 q! work structures and are expected to achieve even better per-
                                                                   formance, which we leave as future work. Besides, as the
where q ∈ {1, · · · , Q} is the order index, and G denotes         derivative operator should be parameter-invariant theoreti-
the mapping function of derivative operator in the high-order      cally, we also investigate the case when the parameters are
module. ",cs.SD,A,0.37067655,-0.06276575,-0.11770387
http://arxiv.org/pdf/2205.00941v1,Music Interpretation Analysis. A Multimodal Approach To Score-Informed Resynthesis of Piano Recordings,"For this reason, the proposed framework can be easily extended to include other pa-
rameters and to consider the performer-specific models of the context-adaptation functions [181],
as well as other instruments. According to the discussion in Sections 8.1 and 2.4.1, future works
could estimate non-MIDI parameters that are relevant for the timbre realization of pianists [197]. Another attractive addition would be the note offset precise inference based on the hammer sec-
ond and third impulsive sound – see Figure 8.2; given the low accuracy of the note offset infer-
ence in state-of-the-art AMT models, such an addition could be useful for precisely defining the
performer interpretation. ",cs.SD,B,-0.006055799,0.30375236,-0.17466937
http://arxiv.org/pdf/2205.01019v1,HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation,"5.                                                                 estimation, it also has potential in melody extraction, poly-
                                                                           phonic pitches tracking, and other music information retrieval
                                                                           ﬁelds. We will continue to improve the structure of the model,
                                                                           explore its application in other tasks, and evaluate its perfor-
                                                                           mance in future works. 97.5                                                                                 7. ",cs.SD,B,-0.03480587,0.3178972,-0.11372575
http://arxiv.org/pdf/2205.01019v2,HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation,"We will continue to improve the structure of the model,
in Fig. 5.                                                      explore its application in other tasks, and evaluate its perfor-
                                                                mance in future works. The proposed model outperforms all the other three ab-
lated models. ",cs.SD,A,0.35947412,-0.13160726,-0.119126305
http://arxiv.org/pdf/2205.01751v1,On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training,"enhancement network to reconstruct a less distorted signal. For the future work, we would like to retrain the acoustic
     Next, to investigate the effect of the amount of unpaired        model with an enhanced training set processed by the proposed
clean speech data used for the proposed training framework,           enhancement model to further reduce the mismatch. We will
more clean speech data is added to increase acoustic diversity. ",cs.SD,C,-0.22783068,0.028590333,0.14415498
http://arxiv.org/pdf/2205.01751v2,On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training,"the proposed system achieves comparable result with the cur-
rent state-of-the-art system on the real evaluation set. For the future work, we would like to retrain the acoustic
                   6. References                                          [19] D. B. Paul and J. M. Baker, “The design for the Wall Street
                                                                                Journal-based CSR corpus,” in Proceedings of the Workshop on
 [1] Z. Zhang, J. Geiger, J. Pohjalainen, A. E.-D. Mousa, W. Jin, and           Speech and Natural Language, 1992. ",cs.SD,C,-0.29391447,-0.008724816,-0.110216565
http://arxiv.org/pdf/2205.01800v1,Synthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis,"The views
enables a transformer to achieve high success with less data     and conclusions contained herein are those of the authors
than transformers typically require. and should not be interpreted as necessarily representing the
                                                                 ofﬁcial policies or endorsements, either expressed or implied,
   Although this approach is promising, future work should       of DARPA and AFRL or the U.S. Government. consider more diverse speech features. ",cs.SD,C,-0.110441625,-0.17797714,-0.11659199
http://arxiv.org/pdf/2205.02058v1,SVTS: Scalable Video-to-Speech Synthesis,"For both corpora,      a direct measure of the difference in intelligibility between real
utterances exceeding 24 seconds are excluded from training due     and generated audio, and it also removes the requirement for la-
to hardware limitations. belled datasets in future work. For our GRID experiments, we
                                                                   use a model pre-trained on LRW, LRS2, and LRS3 [21], and
3.2. ",cs.SD,C,-0.23343074,-0.07673219,0.052373976
http://arxiv.org/pdf/2205.02058v2,SVTS: Scalable Video-to-Speech Synthesis,"For both corpora,      a direct measure of the difference in intelligibility between real
utterances exceeding 24 seconds are excluded from training due     and generated audio, and it also removes the requirement for la-
to hardware limitations. belled datasets in future work. For our GRID experiments, we
                                                                   use a model pre-trained on LRW, LRS2, and LRS3 [21], and
3.2. ",cs.SD,C,-0.23343074,-0.07673219,0.052373976
http://arxiv.org/pdf/2205.03043v1,Sound2Synth: Interpreting Sound via FM Synthesizer Parameters Estimation,"This technique could be applied only if the number of train-
ing samples in dataset D is small, since rendering audio using
a synthesizer and computing audio space loss for every param-
eter of every data point is very time-consuming. Optimizing
this method will be left as future work. 3 Dataset                                                              *Hill Climbing                  21.96
                                                                       *Genetic Algorithm              31.32
There are 155 parameters for the Dexed synthesizer in total:           APVST MLP                       31.38
94 continuous parameters, 59 discrete parameters, and 2 ﬁxed           APVST LSTM                      32.76
parameters (Algorithm and Output). ",cs.SD,B,0.042654343,0.24309054,0.019520583
http://arxiv.org/pdf/2205.03043v2,Sound2Synth: Interpreting Sound via FM Synthesizer Parameters Estimation,"*Hill Climbing                  21.96
                                                                       *Genetic Algorithm              31.32
   This technique could be applied only if the number of train-        APVST MLP                       31.38
ing samples in dataset D is small, since rendering audio using         APVST LSTM                      32.76
a synthesizer and computing audio space loss for every param-          APVST LSTM++                    22.59
eter of every data point is very time-consuming. Optimizing            PresetGen VAE                   14.70
this method will be left as future work. *Similarity Threshold           10 ∼ 15
                                                                       for Human Perception

3 Dataset                                                              SOUND2SYNTH (OURS)              6.32
                                                                       SOUND2SYNTH multi-modal (OURS)  5.36
There are 155 parameters for the Dexed synthesizer in total:
87 continuous parameters, 66 discrete parameters, and 2 ﬁxed           Table 1: Experiment results. ",cs.SD,B,0.012160241,0.22430895,-0.08150739
http://arxiv.org/pdf/2205.03247v1,Musical Score Following and Audio Alignment,"11.9 Future Work

While the sliCQ audio feature extraction technique performs remarkably well, there are many potential areas
of improvement for this system. In this section, possible areas of future work are outlined and discussed. 11.9.1 Improvements to OLTW

The OLTW algorithm, ﬁrst introduced by Dixon in 2005 [51], has seen many iterations and improvements, some
of which are detailed in subsection 11.3.5 and [26]. ",cs.SD,B,-0.041063067,0.15121818,0.1347259
http://arxiv.org/pdf/2205.04328v1,"Insights on Modelling Physiological, Appraisal, and Affective Indicators of Stress using Audio Features","In contrast, the
STL network trained on the appraisal target shows a slight                      [5] A. Baird, S. Amiriparian, N. Cummins, S. Sturmbauer, J. Janson,
tendency towards the arithmetic task. This shows how stress                            E.-M. Messner, H. Baumeister, N. Rohleder, and B. W. Schuller,
may manifest differently in the two TSST tasks; future work                            “Using speech to predict sequentially measured cortisol levels during
could thus treat them separately. a Trier Social Stress Test,” in Proc. ",cs.SD,A,0.07340926,-0.19086531,-0.22866395
http://arxiv.org/pdf/2205.04923v1,Gamified Speaker Comparison by Listening,"This necessarily limits the
                                                                   statistical power of ﬁndings. We nonetheless hope our work
                                                                   to inspire further research or applications. A game perceived
                                                                   sufﬁciently ‘cool’ may have the potential to go viral. ",cs.SD,A,0.24477537,-0.05509187,-0.21906573
http://arxiv.org/pdf/2205.05330v1,Generalized Fast Multichannel Nonnegative Matrix Factorization Based on Gaussian Scale Mixtures for Blind Source Separation,"It is thus necessary to
                                                                      approximately compute φ˜−ft1 using an MH sampler as in [17]. C. Existing Instances of GSM-FastMNMF                                 Investigation of the existence and derivation of a closed-form
                                                                      expression of φ˜−ft1 remains as future work. We show that N -FastMNMF (Section II-D1), t-FastMNMF
(Section II-D2), leptokurtic GG-FastMNMF (Section II-D3),
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. ",cs.SD,A,0.13724087,0.011113949,-0.12257011
http://arxiv.org/pdf/2205.05448v2,Symphony Generation with Permutation Invariant Language Model,"This section introduces the novel symphony dataset we                                                                                       6.3 Music BPE Result
propose and presents two stages in the training process 1 . After constructing a vocabulary list of length 1, 000 with
   1 Our code, demos, dataset and further analysis can be accessed at                                                                       Music BPE algorithm, the top-5 merged pairs shown in
https://symphonynet.github.io                                                                                                               Fig. 4 with the highest frequency are: (D4, F 4), (C4, E4),
                                                                          Model  CDH S O P

                                                                          Baseline 3.5 3.57 3.07 3.00 3.21 3.29

                                                                  Chord   BPE    3.64 3.64 3.14 3.15 3.43 3.29

                                                                          3D + BPE 3.71 3.72 3.21 3.07 3.5 3.5

                                                                          Human 4.43 3.43 4.14 4.36 4.14 4.14

                                                                          Baseline 3.79 2.79 3.21 3.43 3.36 3.36

                                                                  Prime   BPE    3.86 3.5 3.5 3.5 3.64 3.86

                                                                          3D + BPE 3.86 3.14 3.43 3.57 3.93 3.64

                                                                          Human 4.36 3.57 4.36 4.00 4.36 4.36

                                                                          Baseline 3.52 3.46 3.04 3.07 3.11 3.07

                                                                  Uncondi. ",cs.SD,B,0.07916619,0.26292193,-0.06760667
http://arxiv.org/pdf/2205.05580v1,Scream Detection in Heavy Metal Music,"REFERENCES
                       7. CONCLUSION
                                                               [1] O. Nieto, “Unsupervised clustering of extreme vocal
We introduced a new annotated dataset to aid and encourage          effects,” in Proceedings of the 10th International Con-
further research in vocal detection in heavy metal music. ference Advances in Quantitative Laryngology, 2013, p.
Both the dataset and code have been made publicly avail-           115.
able. ",cs.SD,B,-0.13362896,0.1906061,0.05394144
http://arxiv.org/pdf/2205.05871v1,Towards Robust Unsupervised Disentanglement of Sequential Data -- A Case Study Using Music Audio,"to consistently work over a wide range of settings. Our evalua-
                                                                       tion focuses on the ability to robustly achieve disentanglement,
6.5 Multiple Global Factors                                            and we leave evaluations on multi-modal data generation from
We now consider both the fourth and ﬁfth octaves when syn-             unconditional prior sampling for future work. We would also
thesising the dMelodies dataset, introducing octave number as          like to verify the applicability of TS-DSAE to modalities be-
the other global factor of variation in addition to instrument         yond the music audio datasets. ",cs.SD,B,0.0173762,0.2875734,-0.072820395
http://arxiv.org/pdf/2205.05871v2,Towards Robust Unsupervised Disentanglement of Sequential Data -- A Case Study Using Music Audio,"to consistently work over a wide range of settings. 3 Our
                                                                       evaluation focuses on the ability to robustly achieve disen-
6.5 Multiple Global Factors                                            tanglement, and we leave evaluations on multi-modal data
We now consider both the fourth and ﬁfth octaves when syn-             generation from unconditional prior sampling for future work. thesising the dMelodies dataset, introducing octave number as          We would also like to verify the applicability of TS-DSAE to
the other global factor of variation in addition to instrument         modalities beyond the music audio datasets. ",cs.SD,B,0.04434688,0.29389042,-0.113017686
http://arxiv.org/pdf/2205.06053v1,Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation,"The proposed
             0.10  12  3.80 3.45 ± 0.06                            method has also shown comparative F0 controllability as the
                                                                   WORLD vocoder which is a conventional vocoder with strong
             0.10  13  3.32 3.28 ± 0.07                            F0 controllability while signiﬁcantly outperforming WORLD in
                                                                   sound quality. For future work, we intend to further improve the
             0.10  13  3.39 3.42 ± 0.06                            sound quality of HN-uSFGAN to tackle the occasional buzzy
                                                                   noises in the unvoiced segments. top rows of Table1. ",cs.SD,B,-0.021160575,0.2147085,0.0014670901
http://arxiv.org/pdf/2205.06053v2,Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation,"The proposed
             0.10  12  3.80 3.45 ± 0.06                            method has also shown comparative F0 controllability as the
                                                                   WORLD vocoder which is a conventional vocoder with strong
             0.10  13  3.32 3.28 ± 0.07                            F0 controllability while signiﬁcantly outperforming WORLD in
                                                                   sound quality. For future work, we intend to further improve the
             0.10  13  3.39 3.42 ± 0.06                            sound quality of HN-uSFGAN to tackle the occasional buzzy
                                                                   noises in the unvoiced segments. 4.2.1. ",cs.SD,B,-0.018663425,0.21088654,0.0065998286
http://arxiv.org/pdf/2205.07450v1,PRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification,"each system assigns segments to the corresponding clusters. For future works we are interested in investigating PRISM’s
     Both x-vector clustering system and end-to-end diarization    effects on the other speech related tasks such as target speaker
system with encoder-decoder based attractors are compared to       extraction, overlapping speech detection, etc. Even though the
our proposed system, as listed in table 4. ",cs.SD,C,-0.12819438,0.05431927,-0.006769385
http://arxiv.org/pdf/2205.07450v2,PRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification,"each system assigns segments to the corresponding clusters. For future works we are interested in investigating PRISM’s
     Both x-vector clustering system and end-to-end diarization    effects on the other speech related tasks such as target speaker
system with encoder-decoder based attractors are compared to       extraction, overlapping speech detection, etc. Even though the
our proposed system, as listed in table 4. ",cs.SD,C,-0.12819438,0.05431927,-0.006769385
http://arxiv.org/pdf/2205.07682v1,L3-Net Deep Audio Embeddings to Improve COVID-19 Detection from Smartphone Data,"Available:
mobile devices. https://doi.org/10.1145/3465398

   As a future work, we would like to make an extensive                [3] Q.-V. Pham, D. C. Nguyen, T. Huynh-The, W.-J. Hwang, and P. N.
comparison of different deep audio embeddings models for                    Pathirana, “Artiﬁcial intelligence (ai) and big data for coronavirus
COVID-19 detection and, if other datasets are available,                    (covid-19) pandemic: A survey on the state-of-the-arts,” IEEE Access,
for the automatic detection of other important diseases, like               vol. ",cs.SD,B,-0.089743495,0.06906847,0.23378435
http://arxiv.org/pdf/2205.09667v1,The AI Mechanic: Acoustic Vehicle Characterization Neural Networks,"There is value
in developing a real-world application that would make it easy for a consumer to collect data on their mobile
device and store the waveform at its original 48 kHz sampling rate to prevent lossy compression or frequency attenuation. Another avenue for future work to address the lack of data, especially in underrepresented classes, would be generative
modeling. In particular, Generative Adversarial Networks (GANs) [26] have shown great power in developing realistic
samples in numerous ﬁelds, most notably facial recognition. ",cs.SD,A,-0.006112065,-0.0017235279,0.31042314
http://arxiv.org/pdf/2205.11738v1,Adaptive Few-Shot Learning Algorithm for Rare Sound Event Detection,"So, we follow the exper-                 improvement over baselines. iments from [41], such a out-of-domain testing could display
the ability of few-shot learning methods to generalize [42],                  In future work we plan to further analyze why adaptive
[43]. Following the previous setup, the selected AudioSet [44]             training of current state-of-the-art models does not yield
with 99 events for meta-train, meta-validation with 21 events              substantial improvements in the multi-shots setting. ",cs.SD,B,-0.073089756,0.0359425,0.2399953
http://arxiv.org/pdf/2205.11738v2,Adaptive Few-Shot Learning Algorithm for Rare Sound Event Detection,"mon and some patterns exist in other sounds as well such as
bell ringing. On the other hand animal sounds such as “dog                    In future work we plan to further analyze why adaptive
growling” rarely resemble other sounds. The gain of meta-                  training of current state-of-the-art models does not yield
learning approaches over supervised baselines are diminished               substantial improvements in the multi-shots setting. ",cs.SD,B,-0.14972,0.14585748,0.18071708
http://arxiv.org/pdf/2205.12594v1,Heterogeneous Reservoir Computing Models for Persian Speech Recognition,"Besides, in this study, we only reported the performance in terms of frame
recognition accuracy. It remains for future works to conduct recognition tasks at phoneme and word levels. 6 Acknowledgments

Fatemeh Hadeaghi’s research was supported by the Deutsche Forschungsgemeinschaft, Germany (TRR 169/A2). ",cs.SD,C,-0.055122096,-0.09520917,-0.04718619
http://arxiv.org/pdf/2205.14329v1,Speech Augmentation Based Unsupervised Learning for Keyword Spotting,"In Sec. V,         and compared with other unsupervised approaches, including
we conclude with the summary of the paper and future works. CPC [23], APC [24] and MPC [25]. ",cs.SD,A,0.45481527,-0.08101537,-0.13803735
http://arxiv.org/pdf/2205.14411v1,Feature Pyramid Attention based Residual Neural Network for Environmental Sound Classification,"The
experimental results are shown in Section 4. The conclusion and future work
about our research come in Section 5. 2. ",cs.SD,A,0.44211847,-0.04961499,-0.21071702
http://arxiv.org/pdf/2205.14701v1,Modeling Beats and Downbeats with a Time-Frequency Transformer,"The improvement
varies across different datasets. Our proposed models perform bet-            For future work, we will try to mitigate the sub-optimal issue
ter than the baseline by a wider margin for non-pop-centric datasets:    (see Section 4.4) with better solutions, as we expect it is currently
Ballroom and Hainsworth. In particular, the performance difference       the limitation for SpecTNT to improve beat tracking. ",cs.SD,B,0.08668265,0.23548672,-0.0010713576
http://arxiv.org/pdf/2205.15195v1,Personalized Acoustic Echo Cancellation for Full-duplex Communications,"Overall,              iliary information. In future work, we will test whether speaker
GTCNN-Es outperforms pGTCNN-Ex in all three scenarios                   recognition models with lower equal error rates will provide
(D1, D2, D3). Obviously, “knowing who is talking” is far more           consistent performance gain for the AEC models, and explore
important than “knowing who you are talking to”. ",cs.SD,C,-0.15547955,-0.20449609,0.0235313
http://arxiv.org/pdf/2205.15195v2,Personalized Acoustic Echo Cancellation for Full-duplex Communications,"In
far-end speaker can improve PESQ and ERLE scores. Overall,              future work, we will test whether more advanced speaker recog-
GTCNN-Es outperforms pGTCNN-Ex in all three scenarios                   nition models with lower equal error rates will provide consis-
(D1, D2, D3). Obviously, “knowing who is talking” is far more           tent performance gain for the AEC models, and explore more
important than “knowing who you are talking to”. ",cs.SD,C,-0.16701974,-0.198374,-0.14243518
http://arxiv.org/pdf/2205.15360v1,Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite,"Aerosol devices deliver a ﬁxed medication dose, rapidly
                                        paper revisits audio pattern recognition and machine learning             and directly into the airways, from a pressurized canister,
                                        techniques for asthma medication adherence assessment and                 containing a medication/propellant mixture [14], [15]. The
                                        presents the Respiratory and Drug Actuation (RDA) Suite1 for              efﬁcient and effective management of asthma is strongly
                                        benchmarking and further research. The RDA Suite includes a               connected with the patient adherence to the prescribed action
                                        set of tools for audio processing, feature extraction and classiﬁca-      plan, while reduced adherence has been strongly linked with
                                        tion and is provided along with a dataset consisting of respiratory       signiﬁcant indicators of health degradation. ",cs.SD,B,0.05119632,0.114707634,0.109889366
http://arxiv.org/pdf/2205.15360v2,Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite,"This               containing a medication/propellant mixture [14], [15]. The
                                       paper revisits audio pattern recognition and machine learning             efﬁcient and effective management of asthma is strongly
                                       techniques for asthma medication adherence assessment and                 connected with the patient adherence to the prescribed action
                                       presents the Respiratory and Drug Actuation (RDA) Suite1 for              plan, while reduced adherence has been strongly linked with
                                       benchmarking and further research. The RDA Suite includes a               signiﬁcant indicators of health degradation. ",cs.SD,B,0.009196404,0.12730683,0.1175694
http://arxiv.org/pdf/2205.15370v1,Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data,"Although we do not release the code, due to the adaptation ability of the diffusion-based model,
we expect that the adaptive TTS technology is highly likely to be misused like Deepfake. We leave
the research on anti-spooﬁng that distinguishes generated speech from real audio as future work,
considering the potential for misuse of Guided-TTS 2. 16 ",cs.SD,C,-0.22078797,-0.053855617,0.028056834
http://arxiv.org/pdf/2206.00454v1,Towards Context-Aware Neural Performance-Score Synchronisation,"Future work  162

relatively unexplored for music alignment. An example of such a positive feedback
loop is presented in Figure 6.1, providing a vision for future work. To this end,
research on optimal ways to eﬀectively leverage user context as well as developing
eﬀective techniques for adapting an existing model to a stream of manual corrections
could be explored. ",cs.SD,B,-0.019695822,0.25983715,-0.11350541
http://arxiv.org/pdf/2206.00635v1,Speech Artifact Removal from EEG Recordings of Spoken Word Production with Tensor Decomposition,"Eight speakers were right-
of extracted components used by the matrix decomposition          handed, and the rest were corrected-to-right. One subject was
was also beneﬁcial when applying CP decomposition, such           removed from further analysis due to a high rate of errors. as ICA, which uses the number of sources in the brain [5]. ",cs.SD,A,0.1533924,-0.09194504,-0.110345766
http://arxiv.org/pdf/2206.01071v1,Partitura: A Python Package for Symbolic Music Processing,"The package
functionalities are detailed in Section 2, and in Section 3 we provide a short usage example. Finally, in Section 4 we draw some conclusions on this paper and discuss possible future work. 1 Related Work

Among the available Python packages for parsing and processing music in symbolic formats,
there are two that stand out in terms of popularity and usability, Pretty MIDI and music21. ",cs.SD,B,0.07386005,0.26082426,-0.13249397
http://arxiv.org/pdf/2206.01104v1,The match file format: Encoding Alignments between Scores and Performances,"This paper marks the release of the
stable version 1.0.0 but further modiﬁcations are to be expected to support more features and
solve problems that arise from its usage in practical applications. Other future works involve a
graphical utility for the visualization and modiﬁcation of alignments and tools to create them
automatically (at least partially) given a corresponding score and performance. Acknowledgements

This project receives funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme, grant agreement No 101019375
(Whither Music?). ",cs.SD,B,0.08792461,0.35228345,-0.06443719
http://arxiv.org/pdf/2206.02211v1,Variable-rate hierarchical CPC leads to acoustic unit discovery in speech,"Yet, by being many-to-one mappings, they could make it impossible
to retrieve the original constituents after compression. We consider that an interesting objective for
future work is characterizing the patterns of the discovered pseudo-units sequences, and perhaps
investigate their correlation with models of human speech perception. 6 Related Work

Perhaps the most widely used variable-length feature discovery approach in deep learning is the
extraction of text tokens using byte-pair encodings [Sennrich et al., 2015] or unigram pieces [Kudo
and Richardson, 2018]. ",cs.SD,C,-0.1166591,-0.15384854,0.25682604
http://arxiv.org/pdf/2206.02211v2,Variable-rate hierarchical CPC leads to acoustic unit discovery in speech,"Yet, by being many-to-one mappings, they could make it impossible
to retrieve the original constituents after compression. We consider that an interesting objective for
future work is characterizing the patterns of the discovered pseudo-units sequences, and perhaps
investigate their correlation with models of human speech perception. 6 Related Work

Perhaps the most widely used variable-length feature discovery approach in deep learning is the
extraction of text tokens using byte-pair encodings [Sennrich et al., 2015] or unigram pieces [Kudo
and Richardson, 2018]. ",cs.SD,C,-0.1166591,-0.15384854,0.25682604
http://arxiv.org/pdf/2206.02211v3,Variable-rate hierarchical CPC leads to acoustic unit discovery in speech,"Yet, by being many-to-one mappings, they could make it impossible
to retrieve the original constituents after compression. We consider that an interesting objective for
future work is characterizing the patterns of the discovered pseudo-units sequences, and perhaps
investigate their correlation with models of human speech perception. 6 Related Work

Perhaps the most widely used variable-length feature discovery approach in deep learning is the
extraction of subword units from text [Schuster and Nakajima, 2012, Sennrich et al., 2016, Kudo
and Richardson, 2018]. ",cs.SD,C,-0.12667742,-0.16494182,0.23263013
http://arxiv.org/pdf/2206.02284v2,Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator,"Therefore, our
framework oﬀered the potential to help clinicians improve treatment strategies
for patients with speech-related disorders. In future work, we will investigate the
use of full three-dimensional plus time tagged-MRI sequences as well as tracking
information from tagged-MRI to achieve spectrogram synthesis. Acknowledgements

This work is supported by NIH R01DC014717, R01DC018511, and R01CA133015. ",cs.SD,C,-0.12402443,0.12170267,0.0155949965
http://arxiv.org/pdf/2206.02284v3,Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator,"Therefore, our
framework oﬀered the potential to help clinicians improve treatment strategies
for patients with speech-related disorders. In future work, we will investigate the
use of full three-dimensional plus time tagged-MRI sequences as well as tracking
information from tagged-MRI to achieve spectrogram synthesis. Acknowledgements

This work is supported by NIH R01DC014717, R01DC018511, and R01CA133015. ",cs.SD,C,-0.12402443,0.12170267,0.0155949965
http://arxiv.org/pdf/2206.02671v1,Canonical Cortical Graph Neural Networks and its Application for Speech Enhancement in Future Audio-Visual Hearing Aids,"Finally, it also showed itself to be less depen-
dent on a more extended prior-frame sequence, i.e., high values of k, since the
memory can store and track temporal information. Regarding future work, we aim to extend a similar cortical-based architecture
to convolutional neural networks and applications to two-dimensional data. We
also aim to implement the model on chips for training and inference for possible
future implementation in hearing-aid devices for AV speech enhancement. ",cs.SD,C,-0.15851535,0.009671267,0.31822455
http://arxiv.org/pdf/2206.02671v2,Canonical Cortical Graph Neural Networks and its Application for Speech Enhancement in Future Audio-Visual Hearing Aids,"Finally, it also showed itself to be less depen-
dent on a more extended prior-frame sequence, i.e., high values of k, since the
memory can store and track temporal information. Regarding future work, we aim to extend a similar cortical-based architecture
to Convolutional Neural Networks and applications to two-dimensional data. We
also aim to implement the model on chips for training and inference for possible
future implementation in hearing aid devices for AV speech enhancement. ",cs.SD,C,-0.16290909,0.015004149,0.31733912
http://arxiv.org/pdf/2206.03351v1,AS2T: Arbitrary Source-To-Target Adversarial Attack on Speaker Recognition Systems,"abs/1704.03453, 2017.
this asymmetric phenomenon. (iii) Another future work is
to investigate defense solutions against adversarial attacks                 [20] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable
in the speaker recognition domains under various settings. adversarial examples and black-box attacks,” in ICLR, 2017. ",cs.SD,C,-0.15128866,-0.12352571,0.18334875
http://arxiv.org/pdf/2206.03393v1,Towards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition,"This opens up a new research direction                      INTERSPEECH, 2021.
on transformations for mitigating adversarial examples. We
pointed out many possible future works in both adversarial                [19] R. Olivier, B. Raj, and M. Shah, “High-frequency adversarial
attacks and defenses in the speaker recognition domain, and                       defense for speech and audio,” in ICASSP, 2021.
released our evaluation platform SPEAKERGUARD to foster
further research. [20] F. Trame`r, N. Carlini, W. Brendel, and A. Madry, “On adaptive
                                                                                  attacks to adversarial example defenses,” in NeurIPS, 2020. ",cs.SD,C,-0.25943553,-0.1925417,0.13326736
http://arxiv.org/pdf/2206.04006v1,Few-Shot Audio-Visual Learning of Environment Acoustics,"The results show its promise: substantial gains over existing models,
faster training, and impact on downstream source localization and depth estimation. In future work,
we plan to explore ways to optimize the placement of the observation set and explore ways to curate
large-scale real world data for sim2real transfer. Acknowledgements: Thanks to Tushar Nagarajan and Kumar Ashutosh for feedback on paper
drafts. ",cs.SD,A,0.3316416,-0.07622762,0.20397359
http://arxiv.org/pdf/2206.04006v2,Few-Shot Audio-Visual Learning of Environment Acoustics,"The results show its promise: substantial gains over existing models,
faster training, and beneﬁts for downstream source localization and depth estimation. In future work,
we plan to explore ways to optimize the placement of the observation set and explore ways to curate
large-scale real world data for sim2real transfer. Acknowledgements: Thanks to Tushar Nagarajan and Kumar Ashutosh for feedback on paper drafts. ",cs.SD,A,0.33499742,-0.062850535,0.22157107
http://arxiv.org/pdf/2206.04769v1,CLAP: Learning Audio Concepts From Natural Language Supervision,We     back. leave the larger batch size investigation to future work. C.2. ,cs.SD,A,0.43482375,-0.16600768,-0.031505805
http://arxiv.org/pdf/2206.04962v1,Feature Learning and Ensemble Pre-Tasks Based Self-Supervised Speech Denoising and Dereverberation,"in real room recordings. Moreover, the proposed method can be used where both
SNR levels and noise types are unseen, however, the speech        F. Ablation Study
enhancement performance suffers a slight degradation, which
will be handled in future work. Firstly, the effectiveness of each feature is investigated in the
                                                                  SSL study. ",cs.SD,C,-0.16454123,0.12393617,-0.15011069
http://arxiv.org/pdf/2206.04984v1,Zero-Shot Audio Classification using Image Embeddings,"Cognitively motivated approaches suggest that      [11] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrance,
human semantic knowledge relies on perceptual information                R. C. Moore, M. Plakal, and M. Ritter, ”Audio Set: An ontology
rather than only linguistic information [17]. In order to allow          and human-labeled dataset for audio events,” in Proceedings of IEEE
the model to capture complementary information from dif-                 International Conference on Acoustics, Speech and Signal Processing
ferent modalities, future work should explore more advanced              (ICASSP), 2017, pp. 776-780.
hybrid models. ",cs.SD,C,-0.18555637,0.02921398,0.040989757
http://arxiv.org/pdf/2206.05876v2,Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques,"For the former approach, obtaining
                                                                      more meaningful synthetic representations from multiple domains
                                                                      is left for future works. For the latter approach, future works can fo-
                                                                      cus on analyzing the effect of this approach on completely unseen
                                                                      types of target domain data. Detection and Classiﬁcation of Acoustic Scenes and Events 2022                3–4 November 2022, Nancy, France

                                 6. ",cs.SD,B,-0.074353404,0.10292948,0.21870175
http://arxiv.org/pdf/2206.05929v1,Improvement of Serial Approach to Anomalous Sound Detection by Incorporating Two Binary Cross-Entropies for Outlier Exposure,"is too different from normal data. In future work, we will                       [16] L. Ruff, R. A. Vandermeulen, N. Go¨rnitz, A. Binder, E. Mu¨ller, K.-R.
                                                                                       Mu¨ller, and M. Kloft, “Deep Semi-Supervised Anomaly Detection,” in
investigate the impact of training data on ASD performance                             International Conference on Learning Representations, 2020. [Online]. ",cs.SD,A,0.15967749,-0.19338048,0.3468553
http://arxiv.org/pdf/2206.06126v1,Robust Time Series Denoising with Learnable Wavelet Packet Transform,"Choice of the NN architectures: airport background
                                                                removal
   This work opens several doors for future directions. First, further research on the δ-modiﬁcation or related            The Table VIII column ""Background"", shows the experi-
modiﬁcations should be conducted. For example, learning         ment from Appendix A-A applied to the ﬁrst fold of the
the L-WPT on diﬀerent noise levels would show if the            Background denoising task. ",cs.SD,A,0.355106,0.03442014,0.043059144
http://arxiv.org/pdf/2206.06126v2,Robust Time Series Denoising with Learnable Wavelet Packet Transform,"This work opens several doors for future directions. First, further research on the δ-modiﬁcation or related mod-
iﬁcations should be conducted. For example, learning the L-WPT on diﬀerent noise levels would show if the kernels
remain similar.This would tell us how optimal the delta-modiﬁcation is. ",cs.SD,A,0.2718047,-0.0038094893,0.004029155
http://arxiv.org/pdf/2206.06126v3,Robust Time Series Denoising with Learnable Wavelet Packet Transform,"14
    This work opens several doors for future directions. First, further research on the δ-modiﬁcation or related mod-
iﬁcations should be conducted. For example, learning the L-WPT on diﬀerent noise levels would show if the kernels
remain similar.This would tell us how optimal the delta-modiﬁcation is. ",cs.SD,A,0.29447347,-0.024992034,-0.02391819
http://arxiv.org/pdf/2206.06126v4,Robust Time Series Denoising with Learnable Wavelet Packet Transform,"14
    This work opens several doors for future directions. First, further research on the δ-modiﬁcation or related mod-
iﬁcations should be conducted. For example, learning the L-WPT on diﬀerent noise levels would show if the kernels
remain similar.This would tell us how optimal the delta-modiﬁcation is. ",cs.SD,A,0.29447347,-0.024992034,-0.02391819
http://arxiv.org/pdf/2206.07229v1,Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning,"More importantly, in the case          strength assessment model. In future work, we intend to inte-
of StrengthNetESD+RAVDESS and StrengthNetESD+SAVEE,                grate our StrengthNet as a front-end or back-end for emotional
the MAE achieves the lowest values of 0.173 and 0.102 on           speech synthesis models to enhance the emotion expressiveness
SVAEE and RAVDESS respectively. As can be seen from the            of output emotional speech. ",cs.SD,C,-0.19936766,-0.20105308,-0.22469983
http://arxiv.org/pdf/2206.07289v1,Text-Aware End-to-end Mispronunciation Detection and Diagnosis,"However, tanh gives         representation learning among given reading texts and acous-
a lower performance ever compared to the baseline and we          tic inputs. In future work, we will investigate extracting more
also notice that “AudioGate”, performing control on the audio     information from transcriptions, such as transferring phonetic
branch, reports poor performance. These results suggest that      knowledge to constrain the text-audio attention matrix and op-
even incorporating the extra reference text, the fusion frame-    timize the learning object toward MDD. ",cs.SD,C,-0.17542459,-0.031561747,0.13786623
http://arxiv.org/pdf/2206.07340v1,On the Design and Training Strategies for RNN-based Online Neural Speech Separation Systems,"vestigation. We leave the application of knowledge distillation
“D” and “R” modes stand for “decomposition” and “reorganiza-        in our framework as a future work. tion” schemes, respectively. ",cs.SD,A,0.44056633,-0.07735729,0.0020430898
http://arxiv.org/pdf/2206.08039v1,Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History,"achieved higher speech quality than that by the conventional
We presented 35 synthetic speech samples to listeners in ran-        method. In future work, we will investigate a method for
dom order. Listeners rated the naturalness of each sample from       building our empathetic DSS model in a supervised or semi-
degrees of 1 (“very unnatural”) to 5 (“very natural”). ",cs.SD,C,-0.25153255,-0.0080646165,-0.1715877
http://arxiv.org/pdf/2206.08170v1,Adversarial Privacy Protection on Speech Enhancement,"models should be successfully attacked by adversarial exam-
ples. Speech is leaked easily, and stopping the behavior of ma-          In the future work, we intend to improve the transferability
licious extracting from leaked speech is unrealistic. Thus, how     to more models. ",cs.SD,C,-0.1884087,-0.32338628,-0.029037908
http://arxiv.org/pdf/2206.08297v1,GoodBye WaveNet -- A Language Model for Raw Audio with Context of 1/2 Million Samples,"We are also excited to have shown signiﬁcant improvements
with a minuscule number of parameters. 6 Future Work

In future work, we would like to see application-speciﬁc improvement by re-
placing wavenet blocks with Transformer based architectures. Wave-Net-based
architectures are a backbone of a variety of audio applications such as denoising,
source-separation, vocoder, audio synthesis, and generative modeling of audio,
and we expect to have signiﬁcant gains in performance. ",cs.SD,B,-0.019328408,0.22312468,0.15640175
http://arxiv.org/pdf/2206.08317v1,Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition,"the AR model by a large margin. Nevertheless, vanilla NAR
still outperforms CTC, which makes a similar conditional inde-         To understand why, we performed further analysis. First,
pendence assumption. ",cs.SD,A,0.28481606,-0.20704749,-0.14604299
http://arxiv.org/pdf/2206.08317v2,Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition,"the AR model by a large margin. Nevertheless, vanilla NAR
still outperforms CTC, which makes a similar conditional inde-         To understand why, we performed further analysis. First,
pendence assumption. ",cs.SD,A,0.28481606,-0.20704749,-0.14604299
http://arxiv.org/pdf/2206.09298v1,GMM based multi-stage Wiener filtering for low SNR speech enhancement,"speech enhancement and noise suppression compared to the
                                                                                                          conventional Wiener ﬁltering methods in terms of PESQ and
    To further understand the staging effect in the proposed                                              STOI. For future work, we will investigate the applicability
method, we examine the time varying spectrograms for the                                                  of the proposed method and its inﬂuence on objective speech
destroyer engine noise source at −5 dB SNR level in Fig. 3.                                               quality at very low SNR conditions. ",cs.SD,C,-0.051669702,0.13259155,-0.12944281
http://arxiv.org/pdf/2206.09298v2,GMM based multi-stage Wiener filtering for low SNR speech enhancement,"Evaluations verify that the proposed method achieves a better
                                                                                                          speech enhancement and noise suppression compared to the
                                                                                                          conventional Wiener ﬁltering methods in terms of PESQ and
                                                                                                          STOI. For future work, we will investigate the applicability
                                                                                                          of the proposed method and its inﬂuence on objective speech
                                                                                                          quality at very low SNR levels. 7. ",cs.SD,C,-0.16881625,0.066371016,-0.18420218
http://arxiv.org/pdf/2206.09920v1,WOLONet: Wave Outlooker for Efficient and High Fidelity Speech Synthesis,"Therefore, we propose that the periodic activation               jective and objective scores when compared against alternative
function could be very useful in other speech synthesis tasks,            baseline methods with fewer parameters and FLOPs. In the fu-
such as text-to-speech, which is our future work. ture, we will train our WOLONet with more different training

    4https://github.com/kan-bayashi/ParallelWaveGAN
    5https://github.com/mindslab-ai/univnet
datasets and develop a universal neural vocoder that can be used          [17] Z. Zeng, J. Wang, N. Cheng, and J. Xiao, “Melglow: Efﬁcient
for various speakers and heterogenous environments. ",cs.SD,C,-0.23667252,-0.08552964,0.057547677
http://arxiv.org/pdf/2206.10421v1,Rethinking Audio-visual Synchronization for Active Speaker Detection,"According to our deﬁnition,       supervised models in artiﬁcially unsynchronized cases and
any face track in dubbed movies should not be regarded as        regular benchmarks. For future work, we plan to broaden the
speaking. However, there are some dubbed movies in the           deﬁnition of active speakers to speaking activities shown in
AVA dataset and they are labeled as speaking which mis-
leads the training and evaluation of ASD models with syn-
chronization cues. ",cs.SD,C,-0.20915762,-0.09326635,0.0785474
http://arxiv.org/pdf/2206.10421v2,Rethinking Audio-visual Synchronization for Active Speaker Detection,"Our method outperforms supervised and self-                     Zhongqin Wu, Shiguang Shan, and Xilin Chen, “UniCon: Uni-
supervised models in artiﬁcially unsynchronized cases and                   ﬁed context network for robust active speaker detection,” in
regular benchmarks. For future work, we plan to broaden the                 Proc. ACM Multimedia, 2021, pp. ",cs.SD,C,-0.18801828,0.007005088,0.11481325
http://arxiv.org/pdf/2206.11632v1,Formant Estimation and Tracking using Probabilistic Heat-Maps,"Focusing on
     To demonstrate the learned underlying structure, we com-       generalization, we introduced novel modeling that can generate
puted the average annotated and predicted vowel frequencies in      predictions with superior accuracy for both in-domain and out-
the VTR for each speakers group (men, women, children) and          of-distribution samples. In future work, we would like to further
plotted them with the surrounding polygon in Fig. 2.                explore the generalization effect by combining additional aug-
                                                                    mentation techniques to simulate out-of-distribution samples. ",cs.SD,C,-0.24358995,0.005358182,-0.05781086
http://arxiv.org/pdf/2206.11643v1,Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Switchboard Corpus,"Experiments and results are shown in Section 6. Finally,        T approaches 0, it has been shown that the Gumbel-Softmax
conclusions and future work are discussed in Section 7.              distribution is close to a categorical distribution [35]. Different
                                                                     samples of the uniform random variable Uil lead to different
        2. ",cs.SD,A,0.19825324,-0.142314,-0.020431649
http://arxiv.org/pdf/2206.12038v1,BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping,"The
addition of ﬁxed, non-trainable features could also improve training stability (i.e., prevent-
ing model collapse), which is known to be one of the issues associated with BYOL-style
methods (usually mitigated by updating the target weights as a moving average of those
of the online network) (Grill et al., 2020). While the supervision module (Figure 2) only
consisted of ﬁxed DSP-based features from openSMILE in this work, using other learnable,
deep learning-based, feature extractors in the supervision module could constitute promis-
ing future work, where the two trainable systems could be even trained or ﬁne-tuned in an
iterative fashion. To explore the impact of the supervised and self-supervised components
on hybrid model performance, we varied the weights α and β given to their respective loss
functions. ",cs.SD,A,0.2212866,-0.29934883,0.26179817
http://arxiv.org/pdf/2206.12038v2,BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping,"The
addition of ﬁxed, non-trainable features could also improve training stability (i.e., prevent-
ing model collapse), which is known to be one of the issues associated with BYOL-style
methods (usually mitigated by updating the target weights as a moving average of those
of the online network) (Grill et al., 2020). While the supervision module (Figure 2) only
consisted of ﬁxed DSP-based features from openSMILE in this work, using other learnable,
deep learning-based, feature extractors in the supervision module could constitute promis-
ing future work, where the two trainable systems could be even trained or ﬁne-tuned in an
iterative fashion. To explore the impact of the supervised and self-supervised components
on hybrid model performance, we varied the weights α and β given to their respective loss
functions. ",cs.SD,A,0.2212866,-0.29934883,0.26179817
http://arxiv.org/pdf/2206.12038v3,BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping,"The
addition of ﬁxed, non-trainable features could also improve training stability (i.e., prevent-
ing model collapse), which is known to be one of the issues associated with BYOL-style
methods (usually mitigated by updating the target weights as a moving average of those
of the online network) (Grill et al., 2020). While the supervision module (Figure 2) only
consisted of ﬁxed DSP-based features from openSMILE in this work, using other learnable,
deep learning-based, feature extractors in the supervision module could constitute promis-
ing future work, where the two trainable systems could be even trained or ﬁne-tuned in an
iterative fashion. To explore the impact of the supervised and self-supervised components
on hybrid model performance, we varied the weights α and β given to their respective loss
functions. ",cs.SD,A,0.2212866,-0.29934883,0.26179817
http://arxiv.org/pdf/2206.12038v4,BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping,"In particular, involving a considerably simple set of
ﬁxed features during training can substantially improve DNN-based audio representation
learning. For future work, it would be worth experimenting with diﬀerent augmentation
methods in time and frequency domains and exploring more encoder architectures to im-
prove the robustness of the learnt representations. References

Akshay Anantapadmanabhan, Ashwin Bellur, and Hema A Murthy. ",cs.SD,B,-0.22956914,0.10453662,0.39308098
http://arxiv.org/pdf/2206.12320v1,PoCaP Corpus: A Multimodal Dataset for Smart Operating Room Speech Assistant using Interventional Radiology Workflow Analysis,"Therefore, the performance of ASR algorithms is
worse than in the usual settings. These challenges may also manifest in the feature extraction tasks of future work. In addition to ASR performance improvements, correctly recognized dialect words should be translated into the
standard German language. ",cs.SD,C,0.027983926,-0.18077661,0.015404084
http://arxiv.org/pdf/2206.12469v1,"Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts","In       INTERSPEECH, pp. 3400–3404, Brno, Czech Republic,
future works, one could explore model architectures which         2021.
combine pre-extracted features and features from a speech
representation model as well as experimenting with different   Ren, Z., Baird, A., Han, J., Zhang, Z., and Schuller, B.
debiasing techniques for multi-task learning. Generating and protecting against adversarial attacks for
                                                                  deep speech-based emotion recognition models. ",cs.SD,C,-0.22728981,-0.3713898,0.13617906
http://arxiv.org/pdf/2206.12469v2,"Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts","In       INTERSPEECH, pp. 3400–3404, Brno, Czech Republic,
future works, one could explore model architectures which         2021.
combine pre-extracted features and features from a speech
representation model as well as experimenting with different   Ren, Z., Baird, A., Han, J., Zhang, Z., and Schuller, B.
debiasing techniques for multi-task learning. Generating and protecting against adversarial attacks for
                                                                  deep speech-based emotion recognition models. ",cs.SD,C,-0.22728981,-0.3713898,0.13617906
http://arxiv.org/pdf/2206.12494v1,Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers,"However, given the success of large self-supervised repre-
sentations for downstream acoustic tasks (Hsu et al., 2021;   4. Caveats
Shor et al., 2022; Baevski et al., 2020), we were surprised
to discover that, on the emotional intensity task, our best   Machine perception models of apparent emotional expres-
pre-trained Conformer-based models only marginally out-       sion, including in vocal bursts, remain an open area of in-
performed (0.648 vs 0.645) relatively smaller models like     vestigation where further research is needed. The models
ResNet34 that were trained directly on HUME-VB without        in this work do not aim to infer the internal emotional state
any pre-training. ",cs.SD,C,-0.09476847,-0.20855945,0.02476161
http://arxiv.org/pdf/2206.13021v1,Speak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion,"are also speaking styles for increasing speech intelligibility in
                                                                    noisy condition. For the future work, the proposed method can
     Before each test, the listeners are provided with instruction  be extended to generate converted speech adaptively to the noise
and some sample stimuli to get used to the sound level. Figure      condition in order to further improve the speech intelligibility in
5 reports the average word correction rate across participants. ",cs.SD,C,-0.28046748,-0.11196189,-0.29521632
http://arxiv.org/pdf/2206.13085v1,Sound Model Factory: An Integrated System Architecture for Generative Audio Modelling,"(c) Parameter sequence for a “reinterpreted” opening segment of
George Gershwin’s Rhapsody in Blue with glides, various note lengths, vibrato,
and a continuous clarinet-to-trumpet instrument morph (audition online6). 5 Conclusion

A few noteworthy limitations are left for future work. For example, by selecting
a subspace of the GAN latent space, the SMF makes a trade-oﬀ between low
dimensional control over a constrained sound space of the RNN and the hundreds
of parameters and diversity of the GAN sound space. ",cs.SD,B,0.0076318355,0.2943344,-0.030559918
http://arxiv.org/pdf/2206.13689v1,Tiny-Sepformer: A Tiny Time-Domain Transformer Network for Speech Separation,"The analysis of attention matrix
As shown in Table 3, we also investigate different channel divi-    weights explain the reason for the choice of this channel divi-
sions of multi-head attention Da and separable convolution Dc       sion conﬁguration. For future works, we are also interested in
in IntraCA and InterCA network respectively. The best Tiny-         exploring better model structure for fast inference speed. ",cs.SD,A,0.21084759,-0.23189713,0.21642327
http://arxiv.org/pdf/2206.13689v2,Tiny-Sepformer: A Tiny Time-Domain Transformer Network for Speech Separation,"The best Tiny-         weights explain the reason for the choice of this channel divi-
Sepformer-32 and Tiny-SepformerS-32 models (Nmask =                 sion conﬁguration. For future works, we are also interested in
Nintra = Ninter = 4) in Table 2 are used. The feed-forward          exploring better model structure for fast inference speed. ",cs.SD,A,0.27401617,-0.1454683,0.15730357
http://arxiv.org/pdf/2206.15056v1,FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition,"the WSJ eval92 and FSC eval sets. Our future work will in-
                                                                   clude exploring other powerful SSLRs, combining additional
4.3.2. Combination Methods Comparison                              features, and experimenting with alternative feature reﬁnement
                                                                   approaches. ",cs.SD,A,0.3114077,-0.15216993,-0.109589234
http://arxiv.org/pdf/2206.15155v1,An Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions,"For example, for the   verse effects caused by noise and reverberation, and the poten-
cases of NR-dr-VC, using TasNet among the three dereverber-        tial degradation introduced by the denoising and dereverbera-
ation models shows the best result, as also shown in Table 1.      tion still causes noticeable adverse effects on VC performance. For the combinations of two prerprocessings, we can ﬁnd that       As future works, we plan to test our framework on 16kHz sam-
NR-dn-T-VC is shown to be the best, also following Table 1.        pled data, compare with other VC frameworks [6][7], and im-
                                                                   prove VC performance by reducing the potential degradation. Acknowledgements: This work was partly supported by
                                                                   JST CREST Grant Number JPMJCR19A3, Japan. ",cs.SD,A,0.20289241,0.06184087,-0.1173343
http://arxiv.org/pdf/2206.15276v1,R-MelNet: Reduced Mel-Spectral Modeling for Neural TTS,"An ideal, pipelined
version of R-MelNet could meet a 30 ms latency, sub real-time output threshold which would enable interactive use, but
requires signiﬁcant engineering effort. As such we leave this exploration to future work. 4 Conclusions

We describe R-MelNet, a two-stage architecture for neural text-to-speech synthesis. ",cs.SD,C,-0.20042905,-0.08570671,0.12069821
http://arxiv.org/pdf/2206.15291v1,Sonification as a Reliable Alternative to Conventional Visual Surgical Navigation,"Future research in
computer-assisted surgery can focus on investigating the possible answers to
such fundamental questions in the application ﬁeld of surgical navigation, as the
foundation is well established in cognitive science [86,87]. Soniﬁcation as a Reliable Alternative ...  19

    Further, previous soniﬁcation research [46,47,48,49,50,51,52,73] has investi-
gated diﬀerent soniﬁcation strategies for navigation, providing the preliminary
basis for further research. Future studies should take the cons and pros of soni-
ﬁcation paradigms into account. ",cs.SD,A,0.2560527,-0.057669885,-0.107688844
http://arxiv.org/pdf/2207.00562v1,Distance-Based Sound Separation,"Experiments showed promising initial performance on this task,
0.5 0.5 3.2   9.6  56.3                                            with our model benchmark 1.5 meter distance threshold model
                                                                   producing an average improvement of 4.4 dB in SI-SDR for
1    1 3.4    7.2  40.6                                            nearby sounds. 1    0.5 2.8  2.6  46.8                                                 In future work, we plan to handle more realistic scenar-
                                                                   ios with both speech and non-speech and to investigate multi-
     Models do best when their training SPP matches the eval       channel approaches. We also aim to elucidate the relative im-
data’s SPP, as expected. ",cs.SD,C,-0.153069,0.01216781,0.004205348
http://arxiv.org/pdf/2207.00760v1,Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors,"plicity using a single model only) with and without limiting the
chosen boundaries. In other words, we evaluate the contribution         For future work, we would like to explore the semi-
of the constraint search described in Section 3 to the ﬁnal model    supervised setting, where we provide the model with a limited
performance. We do this for both the ”Pause” and ”Bar” search        amount of human-labeled segments. ",cs.SD,A,0.1823034,-0.10969097,-0.059777975
http://arxiv.org/pdf/2207.00883v1,Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism,"tasks. We will improve the decoding speed of our method and
                                                                    context-sensitive decoding strategies in our future work. 4.6. ",cs.SD,A,0.16321966,-0.21278211,-0.059218813
http://arxiv.org/pdf/2207.01667v1,Stochastic Restoration of Heavily Compressed Musical Audio using Generative Adversarial Networks,"In Section 3 we describe in depth the proposed
                                        GAN architecture (Section 3.1), the training procedure (Section 3.2), the dataset (Section 3.3)
                                        and the evaluation methods (Section 3.4). Finally, in Section 4 we present and discuss the
                                        results and conclude with suggestions for future work in Section 5. Audio examples of the
                                        work are provided in the accompanying website 1. ",cs.SD,A,0.045865588,-0.044104055,0.2594179
http://arxiv.org/pdf/2207.01698v1,An adaptive music generation architecture for games based on the deep learning Transformer mode,"The game play information (events) emitted by the
game may be about the game situation, player(s) situation, but also from various other sources such as quests,
terrains, etc. How to aggregate these various informations is still an open issue for future work (see Section 5.1). 4.5 Strategy and Control Model

While planning for the future some more advanced state machine for mapping the emotions into generation
control strategies (as will be detailed in Section 5.1), in current prototype we have implemented 9 pre-deﬁned
strategies (corresponding to the 9 emotions shown in Figure 2), with for each one diﬀerent values corresponding
to the parameters for the generation: which layers are activated, which instruments (sampled or synthetic
sounds14) are used and which eﬀects are used. ",cs.SD,A,0.10547453,0.015337506,-0.19248812
http://arxiv.org/pdf/2207.01698v2,An adaptive music generation architecture for games based on the deep learning Transformer mode,"The game play information (events) emitted by the
game may be about the game situation, player(s) situation, but also from various other sources such as quests,
terrains, etc. How to aggregate these various informations is still an open issue for future work (see Section 5.1). 4.5 Strategy and Control Model

While planning for the future some more advanced state machine for mapping the emotions into generation
control strategies (as will be detailed in Section 5.1), in current prototype we have implemented 9 pre-deﬁned
strategies (corresponding to the 9 emotions shown in Fig. ",cs.SD,A,0.21536428,-0.097473666,-0.20145828
http://arxiv.org/pdf/2207.01886v1,WeSinger 2: Fully Parallel Singing Voice Synthesis via Multi-Singer Conditional Adversarial Training,"very limited. We leave the detailed investigation of efﬁcient
few-shot SVS for future work. 4. ",cs.SD,A,0.37261927,-0.0850274,-0.009078834
