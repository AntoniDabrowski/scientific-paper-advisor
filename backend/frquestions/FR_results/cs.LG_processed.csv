url,title,further research,primary category,label,x,y,z
http://arxiv.org/pdf/2201.00005v1,A Literature Review on Length of Stay Prediction for Stroke Patients using Machine Learning and Statistical Approaches,"The authors reduced the features from 273 to 40 based on consultation with domain
experts (anesthesiologists). After preprocessing the dataset, 715,143 patient records were
selected for further analysis. The patients were categorized using their medical group into the
following nine surgical categories: (1) general surgery, (2) vascular, (3) urology, (4) plastics,
(5) otolaryngology, (6) orthopedics, (7) gynecology, (8) neurosurgery, and (9) other surgical
conditions (including thoracic, cardiac surgery, and interventional radiology patients). ",cs.LG,A,0.08414028,-0.3952855,0.036092304
http://arxiv.org/pdf/2201.00006v1,Knowledge intensive state design for traffic signal control,"Section 6 concludes the paper      than queue length as a reward representation. MPLight intro-
and discusses future work. duces pressure into state design and ﬁnds the improvement
                                                                 in the model. ",cs.LG,B,0.15848276,0.24878883,-0.09035659
http://arxiv.org/pdf/2201.00006v2,AttentionLight: Rethinking queue length and attention mechanism for traffic signal control,"With the number of vehicles and current phase as
and demonstrates the results. Section 6 summarizes this pa-     the state and queue length as the reward, FRAP [32] builds
per and discusses future work. a novel network to construct phase features and phase com-
                                                                petition relation. ",cs.LG,B,0.13367635,0.12806307,-0.019199835
http://arxiv.org/pdf/2201.00009v1,Augmentative eXplanation and the Distributional Gap of Confidence Optimization Score,"Furthermore, recall that we illustrated using the 2D
example the reason we avoid sigmoid function: suppressed change in the CO score due to its asymptotic part. We have
avoided Softmax for similar reason, and a further study can be conducted to characterize the scores with Softmax or other
modiﬁcations. From here, the ideal vision is to develop a model that scales with CO score in not only a computationally
relevant way, but also in a human relevant way: we want a model that increases predictive probability when the heatmap
highlights exactly the correct localization of the objects or highly relevant features related to the objects. ",cs.LG,C,-0.17886855,-0.07593982,-0.012545089
http://arxiv.org/pdf/2201.00009v2,Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI,"Furthermore, recall that we illustrated using the 2D
example the reason we avoid sigmoid function: suppressed change in the CO score due to its asymptotic part. We have
avoided Softmax for similar reason, and a further study can be conducted to characterize the scores with Softmax or other
modiﬁcations. From here, the ideal vision is to develop a model that scales with CO score in not only a computationally
relevant way, but also in a human relevant way: we want a model that increases predictive probability when the heatmap
highlights exactly the correct localization of the objects or highly relevant features related to the objects. ",cs.LG,C,-0.17886855,-0.07593982,-0.012545089
http://arxiv.org/pdf/2201.00012v1,MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning,"However, MO-MPO requires explicit prefer-           input. We suppose further research in AIRL will be necessary to
ences over objectives, which is not always feasible when combining     prevent overfitting of the reward network. Similarly to Gleave and
learned rewards that are inherently difficult to compare. ",cs.LG,B,-0.03872841,0.28174412,-0.19982499
http://arxiv.org/pdf/2201.00044v1,Transformer Embeddings of Irregularly Spaced Events and Their Participants,"On RoboCup,
the training time of NDTT and A-NDTT was 62 and 149 seconds per sequence, respectively. See
Appendix D.4 for future work on improving the latter time by exploiting GPU parallelism. For the NHP and A-NHP models in section 8.2, we ran the specialized code for these models on CPU
as well, rather than on GPU as in section 8.1, since the RoboCup sequences were too long to ﬁt in
the memory of our K80 GPU. ",cs.LG,B,-0.10111554,0.08141614,0.23018286
http://arxiv.org/pdf/2201.00044v2,Transformer Embeddings of Irregularly Spaced Events and Their Participants,"On RoboCup,
the training time of NDTT and A-NDTT was 62 and 149 seconds per sequence, respectively. See
Appendix F.4 for future work on improving the latter time by exploiting GPU parallelism. For the NHP and A-NHP models in section 7.2, we ran the specialized code for these models on CPU
as well, rather than on GPU as in section 7.1, since the RoboCup sequences were too long to ﬁt in
the memory of our K80 GPU. ",cs.LG,B,-0.10091387,0.0835468,0.22506025
http://arxiv.org/pdf/2201.00044v3,Transformer Embeddings of Irregularly Spaced Events and Their Participants,"On RoboCup,
the training time of NDTT and A-NDTT was 62 and 149 seconds per sequence, respectively. See
Appendix F.4 for future work on improving the latter time by exploiting GPU parallelism. For the NHP and A-NHP models in section 7.2, we ran the specialized code for these models on CPU
as well, rather than on GPU as in section 7.1, since the RoboCup sequences were too long to ﬁt in
the memory of our K80 GPU. ",cs.LG,B,-0.10091387,0.0835468,0.22506025
http://arxiv.org/pdf/2201.00057v1,Optimal Representations for Covariate Shift,"Thus, we showed that it is
possible to learn robust representations using only large sources of inputs X and augmentations A. 9
Preprint

There are caveats that need to be addressed in future work. First, we studied an idealized DG,
which assumes access to the population distributions. ",cs.LG,C,-0.14165816,0.031872474,-0.10006001
http://arxiv.org/pdf/2201.00057v2,Optimal Representations for Covariate Shift,"Thus, we showed that it is
possible to learn robust representations using only large sources of inputs X and augmentations A. There are caveats that need to be addressed in future work. First, we studied an idealized DG,
which assumes access to the population distributions. ",cs.LG,C,-0.13166136,0.04971157,-0.11753797
http://arxiv.org/pdf/2201.00058v1,Representation Topology Divergence: A Method for Comparing Neural Network Representations,"We used the RTD to gain insights on neural networks representations in

                             9
computer vision and NLP domains for various problems: training dynamics analysis, data distribu-
tion shift, transfer learning, ensemble learning, disentanglement assessment. RTD correlates strikingly well with the disagreement of models’ predictions; this is an intriguing
topic for further research. Finally, R-Cross-Barcode and RTD are general tools and which are not
limited only to representations comparison. ",cs.LG,C,-0.31807828,-0.1319,-0.12405353
http://arxiv.org/pdf/2201.00147v1,High-dimensional Bayesian Optimization Algorithm with Recurrent Neural Network for Disease Control Models in Time Series,"Section
4 demonstrates the effectiveness of the RNN-BO algorithm and makes comparison with the standard Bayesian
Optimization algorithm and a high-dimensional Bayesian Optimization algorithm through numerical simulation
experiments. Finally, conclusions and future works are drawn in Section 5. 2. ",cs.LG,B,-0.0563602,0.2598433,0.043371893
http://arxiv.org/pdf/2201.00171v1,Multi-view Subspace Adaptive Learning via Autoencoder and Attention,"The
experiments on several real-world datasets showed that the proposed MSALAA
mostly outperformed the other baseline methods, which validate that our pro-
posed MSALAA can use self-representation of multi-views to subspace adaptive
learning. In future work, we will investigate some variants for MLRSSC. We will try
to build a common self-representation matrix C∗ and align C∗ with all views to
see the inﬂuence of performance for multi-view subspace Learning. ",cs.LG,C,-0.1460265,-0.0115677565,-0.10550268
http://arxiv.org/pdf/2201.00199v1,The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling,"Dataset                            Gain over           Gain over    Gain over
                                        MLP     TabTransformer         TabNet
bank_marketing
1995_income                                1.3                 1.0          3.1
blastchar                                  0.9                 0.7          2.5
                                           0.4                 0.5          1.6

Figure 3: AUROC gain charts for the 3 datasets - comparison between baseline TabTransformer (blue) and
the proposed GatedTabTransformer (orange). 6 Future work

A fruitful direction for further research would be to investigate the effects of combining our current solutions
with some of the recent breakthroughs in regularization, feature engineering, etc. 6
Radostin Cholakov and Todor Kolev  The GatedTabTransformer. ",cs.LG,A,0.06588806,-0.10559286,0.0876292
http://arxiv.org/pdf/2201.00232v1,Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels,"Various
  convolution with spectral analysis. GNNs can be used in RS-GNN and we leave it as a future work. • SuperGAT [21]: This extends GAT [34] with self-supervised            5.2 Performance on Noisy Graphs
  learning. ",cs.LG,C,-0.34081322,-0.11113615,0.2191794
http://arxiv.org/pdf/2201.00236v1,Operator Deep Q-Learning: Zero-Shot Reward Transferring in Reinforcement Learning,"In this way, we can incorporate more reference
points into the network design. We will leave this to future work once the codebase of random feature
attention in Peng et al. (2021) is available. ",cs.LG,C,-0.03698813,-0.04948562,0.024222748
http://arxiv.org/pdf/2201.00308v1,"DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents","The results suggest that ﬁxing DDPM latents across samples can improve or
degrade the sample quality and can be unpredictable in this regard. Thus, design of stronger conditioning formulations
to alleviate stochasticity in DDPM generated samples can be an interesting direction for future work. 5.4 Generalization to downstream tasks

Recently, [47] showed impressive image super-resolution results using diffusion models. ",cs.LG,B,-0.010582892,0.09215135,0.049843647
http://arxiv.org/pdf/2201.00354v1,Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference,"As the hyper-parameters for
curriculum are not elaborated in our experiments, direct combining the two curriculum may hinder
the performance. The design for curriculum fusion is left to the future work. C Environment Details

FourRewardMaze The FourRewardMaze is a 2-D navigation task where an agent need to ﬁnd all
four solutions to achieve better performance. ",cs.LG,B,-0.021056345,0.0045821182,-0.13817692
http://arxiv.org/pdf/2201.00354v2,Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference,"As the hyper-parameters for
curriculum are not elaborated in our experiments, direct combining the two curriculum may hinder
the performance. The design for curriculum fusion is left to the future work. C Environment Details

FourRewardMaze The FourRewardMaze is a 2-D navigation task where an agent need to ﬁnd all
four solutions to achieve better performance. ",cs.LG,B,-0.021056345,0.0045821182,-0.13817692
http://arxiv.org/pdf/2201.00355v1,Experiment Based Crafting and Analyzing of Machine Learning Solutions,"Include available features in the
       data record, e.g., age of the customer, if available even if it is not required as input
       to the ML based system and it is not identiﬁed as an independent variable. This will
       allow for further analysis aimed at discovering the latent variables Xjl. • Avoid bias. ",cs.LG,A,0.18025005,-0.2780518,-0.13647875
http://arxiv.org/pdf/2201.00355v2,Theory and Practice of Quality Assurance for Machine Learning Systems An Experiment Driven Approach,"Include available features in the
       data record, e.g., age of the customer, if available even if it is not required as input
       to the ML based system and it is not identiﬁed as an independent variable. This will
       allow for further analysis aimed at discovering the latent variables Xjl. • Avoid bias. ",cs.LG,A,0.18025005,-0.2780518,-0.13647875
http://arxiv.org/pdf/2201.00363v1,Semi-Supervised Graph Attention Networks for Event Representation Learning,"The GNEE source code is available in https:                       [9] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,”
//github.com/joaopedromattos/GNEE, as well as the datasets                             IEEE Transactions on Knowledge and Data Engineering, 2020.
and source code to reproduce the experiments. [10] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning
   Directions for future work involve investigating how the                            of social representations,” in Proceedings of the 20th ACM SIGKDD
attention mechanisms act on event graphs using an explainable                          international conference on Knowledge discovery and data mining,
AI methodology. The general idea is to incorporate the re-                             2014, pp. ",cs.LG,C,-0.14902279,-0.2326886,-0.11208283
http://arxiv.org/pdf/2201.00382v1,ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions,"We provide
an easy-to-use Python implementation of ECOD for both                              [4] X. Yang, L. Tan, and L. He, “A robust least squares support vector
single- and multi-thread support. We leave accounting for                                machine for regression and classiﬁcation with noise,” Neurocom-
how features are related for future work. For instance, we                               puting, vol. ",cs.LG,B,-0.058012884,0.030000266,-0.08958662
http://arxiv.org/pdf/2201.00382v2,ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions,"We provide
an easy-to-use Python implementation of ECOD for both                              [4] X. Yang, L. Tan, and L. He, “A robust least squares support vector
single- and multi-thread support. We leave accounting for                                machine for regression and classiﬁcation with noise,” Neurocom-
how features are related for future work. For instance, we                               puting, vol. ",cs.LG,B,-0.058012884,0.030000266,-0.08958662
http://arxiv.org/pdf/2201.00382v3,ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions,"Additionally,
CBLOF 7.1500                                                 6.3333 kNN              ECOD ranks ﬁrst in 12 out of 30 datasets, and ranks in the
                                                                                     top three places on 20 datasets. The further analysis with
   PCA 6.9167                                                                        the critical difference plot in Fig. 4 corroborates that ECOD
                                                                                     outperforms the baselines. ",cs.LG,A,0.24441221,-0.19782397,0.18280163
http://arxiv.org/pdf/2201.00384v1,Randomized Signature Layers for Signal Extraction in Time Series Data,"6, the ﬁlter weights do not depend on time. These represent
two strong limitations which pave the way to several enhancements in future work. Despite these
drastic shortcomings, the resulting algorithm outperforms several deep learning baselines on 128
TSC datasets from the UCR archive, as shown in Fig. ",cs.LG,C,-0.2932881,-0.048244752,0.12918119
http://arxiv.org/pdf/2201.00384v2,Randomized Signature Layers for Signal Extraction in Time Series Data,"Model is only trained on SSBMs,
so the input in the middle and right panels is Out-of-Distribution. number of limitations and, since our aim is mainly to show-
                                                                      case the versatility of Randomized Signatures, we leave
                                                                      to future works further algorithmic enhancements likely
                                                                      leading to performance improvements. Figure 8. ",cs.LG,B,0.008550579,0.03404767,0.013277795
http://arxiv.org/pdf/2201.00426v1,The DONUT Approach to EnsembleCombination Forecasting,"This section oﬀers some thoughts on
possible improvements for the weighting model and the Autoencoder. The relationship between
the post facto optimal solution and the ﬁnal prediction result could also oﬀer appealing topics for
further study. Firstly, one can do considerable optimization of the weighting model, DONUT. ",cs.LG,B,-0.13758552,0.011643988,-0.054292224
http://arxiv.org/pdf/2201.00426v2,Deep Learning and Linear Programming for Automated Ensemble Forecasting and Interpretation,"This section offers some thoughts on possible improvements
for the weighting model and the Autoencoder. The relationship between the post-facto optimal solution and the ﬁnal
prediction result could also offer appealing topics for further study. Firstly, one can do considerable optimization of the weighting model, DONUT. ",cs.LG,B,-0.14486869,0.015586758,-0.05499138
http://arxiv.org/pdf/2201.00437v1,Artificial Intelligence and Statistical Techniques in Short-Term Load Forecasting: A Review,"between 1% and 2%. Therefore, any future work on
The following are the answers to these research questions    STLF should target less than 1% prediction error to be
based on the review:                                         competitive with existing works in the literature. RQ1: What are the techniques used for STLF in the          • Most standalone ANN implementations achieved a
literature? ",cs.LG,A,0.13538243,-0.03678038,-0.050199002
http://arxiv.org/pdf/2201.00519v2,Stochastic Weight Averaging Revisited,"5: Run the SWA procedure again with input wˆ, c, n/3. De-
                                                                                                      5 Conclusions and future works
     note the output to be wtswa. 6: return wtswa                                                                                      We investigate extensively the function of WA for DNN model
                                                                                                      training, through the lens of revisiting SWA. ",cs.LG,C,-0.25582987,-0.08835308,0.05759633
http://arxiv.org/pdf/2201.00519v3,Stochastic Weight Averaging Revisited,"Right: VGG16. As we demonstrate here, there are indeed global scale ge-
                                                                      ometric structures in the DNN loss landscape that can be
5 Conclusions and future works                                        encountered by an SGD at an early stage of its life cycle, and
                                                                      such structures can be efﬁciently exploited by a WA operation. We investigate extensively the function of WA for DNN model           This insight inspires a new algorithm design, namely PSWA,
training, through the lens of revisiting SWA. ",cs.LG,C,-0.34537017,-0.003079163,0.08344572
http://arxiv.org/pdf/2201.00565v1,Swift and Sure: Hardness-aware Contrastive Learning for Low-dimensional Knowledge Graph Embeddings,"2021. RETA: A Schema-Aware, End-to-End Solution for Instance Completion in
   These positive results encourage us to explore further research                              Knowledge Graphs. In Proceedings of the WWW ’21: The Web Conference 2021,
                                                                                                Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. ",cs.LG,C,0.04437037,-0.24792957,-0.14871563
http://arxiv.org/pdf/2201.00565v2,Swift and Sure: Hardness-aware Contrastive Learning for Low-dimensional Knowledge Graph Embeddings,"A       faster convergence speed in limited training time. pre-trained high-dimensional KGE model generates soft labels
for each training sample and accelerates the convergence of               These positive results encourage us to explore further research
the small student model. MulDE [29] is a multi-teacher knowl-          activities in the future. ",cs.LG,B,-0.09297406,-0.008479967,-0.055401094
http://arxiv.org/pdf/2201.00570v1,Asymptotic Convergence of Deep Multi-Agent Actor-Critic Algorithms,"The impact of communication losses on the data in
experience replays has so far not been considered. This is on our agenda for
future work. A Proof of Lemma 2

We deﬁne [t] for t ≥ 0 as [t] := tsup{n|tn≤t}. ",cs.LG,A,0.21281645,0.1977956,-0.07005692
http://arxiv.org/pdf/2201.00570v2,3DPG: Distributed Deep Deterministic Policy Gradient Algorithms for Networked Multi-Agent Systems,"Our numerical experiments
show that 3DPG is highly robust to the use of this old information, which makes it attractive
for distributed online multi-agent learning. For future work, we plan to analyze the trade off between using network resources for policy
communication vs. using network resources for data communication. In addition, we are working
on an analysis of 3DPG an its variants under a traditional two-timescale step-size schedule. ",cs.LG,B,-0.10750392,0.24111113,-0.20727667
http://arxiv.org/pdf/2201.00604v1,An analysis of over-sampling labeled data in semi-supervised learning with FixMatch,"The explicit setting is sig-     ways some direct supervision from true labels for
niﬁcantly more eﬃcient in the beginning but the           each task, in each mini-batch. Studying these and
diﬀerence between both settings is reduced over           other possible sampling methods will be an inter-
training and both perform on par when more la-            esting direction for future work. beled data is available and for longer training runs. ",cs.LG,B,-0.029020604,-0.034428746,-0.016593581
http://arxiv.org/pdf/2201.00604v2,An analysis of over-sampling labeled data in semi-supervised learning with FixMatch,"The explicit setting is sig-     ways some direct supervision from true labels for
niﬁcantly more eﬃcient in the beginning but the           each task, in each mini-batch. Studying these and
diﬀerence between both settings is reduced over           other possible sampling methods will be an inter-
training and both perform on par when more la-            esting direction for future work. beled data is available and for longer training runs. ",cs.LG,B,-0.029020604,-0.034428746,-0.016593581
http://arxiv.org/pdf/2201.00641v1,VDPC: Variational Density Peak Clustering Algorithm,"The data points in the top-left region are not identiﬁed as ﬁnal clusters because they are identiﬁed as boundary

points. (d) The remaining |D| − |Clow| data points that require further analysis. (e) Results of applying aDBSCAN on the
remaining |D|−|Clow| data points, the black points represents noises, and the red, green and orange clusters are three identiﬁed
micro-clusters. ",cs.LG,A,0.26124167,-0.14509822,0.30842498
http://arxiv.org/pdf/2201.00650v1,Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI,"A note about Bibliography                                                                            ACKNOWLEDGEMENTS. The book provides a carefully curated bibliography to guide further study, whether
for interview preparation or simply as a matter of interest or job-relevant research. A                    The thanks and acknowledgements of the publisher are due to the following:
comprehensive bibliography would be far too long to include here, and would be of                      My dear son, Amir Ivry, Matthew Isaac Harvey, Sandy Noymer, Steve foot and Velimir
little immediate use, so the selections have been made with deliberate attention to the                Gayevskiy. ",cs.LG,A,0.2533705,-0.30678827,-0.14786065
http://arxiv.org/pdf/2201.00650v2,Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI,"It will
       inevitably prove more successful at handling some of them than others, but it
       has at least made a sincere and devoted effort. A note about Bibliography
The book provides a carefully curated bibliography to guide further study, whether
for interview preparation or simply as a matter of interest or job-relevant research. A
comprehensive bibliography would be far too long to include here, and would be of
little immediate use, so the selections have been made with deliberate attention to the
value of each included text. ",cs.LG,A,0.21046871,-0.32430208,-0.20102468
http://arxiv.org/pdf/2201.00680v1,"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges","less protocol classiﬁcation. Although there has been numerous        6) We motivate further research in this realm by presenting
articles on deep learning for other RF signal intelligence               open research challenges and future directions. approaches (modulation and wireless protocol classiﬁcation)
[4]–[14], a comprehensive presentation spanning conventional          We emphasize here that unlike existing surveys, our article
principled approaches as well as supervised deep learning for      is comprehensive in presenting all aspects of RF ﬁnger-
RF ﬁngerprinting is lacking. ",cs.LG,C,-0.37958372,-0.050606597,-0.034905
http://arxiv.org/pdf/2201.00680v2,"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges","less protocol classiﬁcation. Although there has been numerous        6) We motivate further research in this realm by presenting
articles on deep learning for other RF signal intelligence               open research challenges and future directions. approaches (modulation and wireless protocol classiﬁcation)
[4]–[14], a comprehensive presentation spanning conventional          We emphasize here that unlike existing surveys, our article
principled approaches as well as supervised deep learning for      is comprehensive in presenting all aspects of RF ﬁnger-
RF ﬁngerprinting is lacking. ",cs.LG,C,-0.37958372,-0.050606597,-0.034905
http://arxiv.org/pdf/2201.00680v3,"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges","less protocol classiﬁcation. Although there has been numerous        6) We motivate further research in this realm by presenting
articles on deep learning for other RF signal intelligence               open research challenges and future directions. approaches (modulation and wireless protocol classiﬁcation)
[4]–[14], a comprehensive presentation spanning conventional          We emphasize here that unlike existing surveys, our article
principled approaches as well as supervised deep learning for      is comprehensive in presenting all aspects of RF ﬁnger-
RF ﬁngerprinting is lacking. ",cs.LG,C,-0.37958372,-0.050606597,-0.034905
http://arxiv.org/pdf/2201.00723v2,A Mixed-Integer Programming Approach to Training Dense Neural Networks,"We consider a full Binary MIP training, greedy layer-wise MIP training, greedy layer-wise
MIP training as pre-training for SGD, SGD applied on binary and ReLU ANNs, greedy layer-
wise SGD applied to ReLU and binary ANNs, and greedy SGD as pre-training for ReLU and
binary ANNs that are then trained fully with SGD. Due to computation resources constraints,
experimentation for our ReLU MIP formulation is beyond the scope of this work and will be
validated in future work. We also evaluate the eﬀectiveness of using MIP in a greedy layer-wise
                                                    Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

14

Table 1  List of models and minimum number of layers and units needed to train to
                                      85% testing accuracy

Model                    Number of Layers   Number of Units
Binary MIP                        1 layer             NaN
Greedy Binary MIP                 1 layer
Greedy Binary MIP + SGD                              5 units
Binary SGD                      > 5 layers          10 units
Greedy Binary SGD               > 5 layers        > 50 units
Greedy Binary SGD + SGD         > 5 layers        > 50 units
ReLU SGD                        > 5 layers        > 50 units
ReLU Greedy SGD                 > 5 layers          50 units
ReLU Greedy SGD + SGD           > 5 layers        > 50 units
                                > 5 layers        > 50 units

algorithm to obtain pre-training parameters for SGD training. ",cs.LG,B,-0.40961897,0.186373,0.09740322
http://arxiv.org/pdf/2201.00766v1,Class-Incremental Continual Learning into the eXtended DER-verse,"E.         We ﬁnally envision several directions for future works:
                                                                          we strongly believe that overcoming the standard schema
                                                                          (embodied in the stability vs. plasticity dilemma) with the
                                                                          guess of incoming tasks can favorably foster new ideas
                                                                          and advances in the ﬁeld. Due to its potential applicability
                                                                          to a variety of CL approaches, we feel there is room for
the proposals of novel strategies for mimicking future data                                                                                                                         15
distributions, which will be the scope of our future works. [20] D. J. Bridge and J. L. Voss, “Hippocampal binding of novel infor-
ACKNOWLEDGMENTS                                                                  mation with dominant memory traces can support both memory
                                                                                 stability and change,” J. ",cs.LG,C,-0.01519087,-0.07034273,-0.26268387
http://arxiv.org/pdf/2201.00766v2,Class-Incremental Continual Learning into the eXtended DER-verse,"We ﬁnally envision several directions for future works:
we strongly believe that overcoming the standard schema                   [16] R. Aljundi, E. Belilovsky, T. Tuytelaars, L. Charlin, M. Caccia,
(embodied in the stability vs. plasticity dilemma) with the                     M. Lin, and L. Page-Caccia, “Online continual learning with
guess of incoming tasks can favorably foster new ideas                          maximal interfered retrieval,” in Adv Neural Inf Process Syst, 2019.
and advances in the ﬁeld. Due to its potential applicability
to a variety of CL approaches, we feel there is room for                  [17] P. Buzzega, M. Boschini, A. Porrello, and S. Calderara, “Rethinking
the proposals of novel strategies for mimicking future data                     Experience Replay: a Bag of Tricks for Continual Learning,” in
distributions, which will be the scope of our future works. ICPR, 2020. ",cs.LG,B,-0.097227335,0.16911581,-0.2932459
http://arxiv.org/pdf/2201.00818v1,Multivariate Time Series Regression with Graph Neural Networks,"After that, Section 4 introduces the used datasets, presents our results and
discusses these in context of a model-based comparison. Finally, Section 5
concludes with a summary and outlines interesting directions for future work. 4 Multivariate Time Series Regression with Graph Neural Networks

2 Background and Related Work

In the following, we brieﬂy outline background and related work on graphs
and Deep Learning in general, CNNs, GNNs and its utilization in time series,
as well as the implementation of Deep Learning for seismic analysis. ",cs.LG,C,-0.2858971,-0.30833918,0.10644823
http://arxiv.org/pdf/2201.00818v2,Multivariate Time Series Regression with Graph Neural Networks,"After that, Section 4 introduces the used datasets, presents our results and
discusses these in context of a model-based comparison. Finally, Section 5
concludes with a summary and outlines interesting directions for future work. 4 Multivariate Time Series Regression with Graph Neural Networks

2 Background and Related Work

In the following, we brieﬂy outline background and related work on graphs
and Deep Learning in general, CNNs, GNNs and its utilization in time series,
as well as the implementation of Deep Learning for seismic analysis. ",cs.LG,C,-0.2858971,-0.30833918,0.10644823
http://arxiv.org/pdf/2201.00818v3,Graph Neural Networks for Multivariate Time Series Regression with Application to Seismic Data,"Finally, Sect. 5     the i-th ﬁlter kernel in layer l, M l(j) the j-th local
   concludes with a summary and outlines interesting
   directions for future work. region in layer l and bli the respective bias. ",cs.LG,C,-0.01701183,0.024948258,0.288516
http://arxiv.org/pdf/2201.00849v1,Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data,"proach is generic and could be ﬂexibly applied to training
Effects of Embedding Dimension. We test our model with            models with biased data on other tasks, which will be put in
different embedding dimension P on the synthetic Cifar10,         future work. and the results are putted in Table 9. ",cs.LG,C,-0.11452621,-0.0912056,-0.07822207
http://arxiv.org/pdf/2201.00879v1,Monitoring and Anomaly Detection Actor-Critic Based Controlled Sensing,"1                performance and ﬂexibility than the model-based algorithms. and 2 and Table I are as follows:                                            An interesting direction for future work is to extend our

1) Role of regularizer λ: From Figs. 1b, 1c, 2b and 2c,                      algorithm to detect when the anomalies exceed the threshold

we see that as λ increases, the detection delay of our RL-                   and to concurrently estimate the anomalous processes. ",cs.LG,A,0.1635676,0.17138782,0.13378674
http://arxiv.org/pdf/2201.00971v1,Submix: Practical Private Prediction for Large-Scale Language Models,"10 total
epochs), we do see modest improvements with DP-SGD. It is possible that more epochs of training
could further improve DP-SGD, but we leave that exploration for future work. We use the PyTorch Opacus5 library for DP-SGD training. ",cs.LG,B,-0.29422534,0.058724612,0.15801105
http://arxiv.org/pdf/2201.00976v1,Survey on the Convergence of Machine Learning and Blockchain,"368-375, doi: 10.1109/Blockchain.2019.00057. a) As proposed by the authors of [1], future work can
be done to not submit data directly to the smart contract by      [2] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
using complex models. ",cs.LG,A,0.24602908,-0.013081674,-0.015297098
http://arxiv.org/pdf/2201.00976v2,Survey on the Convergence of Machine Learning and Blockchain,"That is to say, any user can access the model weights
submitted on the DanKu contracts. A possible way to solve this issue is to include
homomorphic encryption in the protocol to anonymize the models submitted on the
smart contracts;

   Furthermore, many of the protocols or marketplaces so far do not consider
transaction privacy, and so the further study may be necessary for conjunction with
[15]. 4.3 Cost Control

The organizers of the protocol and marketplace also need to take into account the
control of the cost e.g. ",cs.LG,A,0.14972118,-0.0064834394,-0.019721603
http://arxiv.org/pdf/2201.01139v1,Generating synthetic mobility data for a realistic population with RNNs to improve utility and privacy,"One limitation is size. Obtained datasets often represent a                 We conclude with a summary and future work. small sample of the population, making them less useful for analyses
                                       that are meant to address the full population. ",cs.LG,A,0.3246199,-0.30130577,-0.10949385
http://arxiv.org/pdf/2201.01145v1,Evolutionary Multitasking AUC Optimization,"For                         844–874, 2008.
example, the parameters have a significant effect on the
performance of EMTAUC. In future work, we will develop an                     [11] K. C. Tan, L. Feng, and M. Jiang, “Evolutionary transfer optimization - A
effective tuning-free EMTAUC with a policy network. In                              new frontier in evolutionary computation research,” IEEE Computational
addition, applying EMTAUC to solve other machine learning                           Intelligence Magazine, vol. ",cs.LG,B,-0.14314917,0.221698,-0.08313766
http://arxiv.org/pdf/2201.01148v1,Parity-based Cumulative Fairness-aware Boosting,"However, in our preliminary investigations, the interplay
between fairness-related costs and class-related miss-classiﬁcation costs resulted
in an unstable model. We still believe that such an approach is promising and
we plan to pursue this direction in our future work. 4.3. ",cs.LG,A,0.30134925,0.052680664,-0.22752273
http://arxiv.org/pdf/2201.01188v1,Graph Neural Networks: a bibliometrics overview,"Third, some fields of records in Scopus were empty, which could affect
the results. As a future work, we can provide a taxonomy of tasks which have been tackled using GNNs and their
results. 21
References

[1]   J. Scott, ""Social Network Analysis,"" Sociology, vol. ",cs.LG,A,0.07369626,-0.36923605,-0.021543065
http://arxiv.org/pdf/2201.01190v1,Two-level Graph Neural Network,"Finally, Section. V         characteristic leads to an underlying graph isomorphism prob-
concludes the paper and offers directions for future work. lem, the so-called local permutation invariance (LPI). ",cs.LG,C,0.08923888,-0.08545941,0.257594
http://arxiv.org/pdf/2201.01196v1,Rxn Hypergraph: a Hypergraph Attention Model for Chemical Reaction Representation,"Representation                            AP                   top1   top2   top5   top10
  reactionFP                          Morgan2                  58.01  67.05  77.60  84.33
                                                               59.03  69.14  78.24  85.01
 Transformers                             TT                   58.41  68.22  77.31  84.14
rxn-hypergraph
                                  pre-trained rxnfp            81.31  84.19  89.60  92.33
                                         rxnfp                 89.14  93.22  96.09  98.55

                                        RGCN                   82.67  92.57  97.03  99.01
                                        RGAT                   84.23  96.06  98.57  99.28

Task                              USPTO Classiﬁcation          Mechanism Classiﬁcation     Polar Plausibility
Number of Graph Attention Layers
Latent Representation Dimension               10                              10                     5
Learning Rate
Learning Rate Decay                          128                             128                    64
L2 Weight Regularization                4.11 ×10−4                      4.11 ×10−4            1.14 ×10−2

                                          0.999995                       0.999995                0.99986
                                        9.75 ×10−5                      9.75 ×10−5            7.28 ×10−5

Table 4: Table of hyperparameters selected through Bayesian Optimization using SHERPA [Hertel et al., 2020]. left to the future work: (1) several other demanding and use-     tion. Journal of chemical information and modeling, 59
ful reaction-level predictive tasks such as yield prediction,     (8):3370–3388, 2019.
and reaction rate constant prediction that can be beneﬁted
by rxn-hypergraph, and (2) more complicated and expres-        Mohammadamin Tavakoli, Aaron Mood, David
sive attention mechanisms such as multiplicative attentions       Van Vranken, and Pierre Baldi. ",cs.LG,C,-0.22423887,0.064803995,0.13636352
http://arxiv.org/pdf/2201.01221v1,A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning,"This work ﬁll in the gap
ing benchmarks (see Sections 5.3 and 5.4). of theoretical understanding of state-based critics popular in
                                                                                                      multi-agent reinforcement learning, providing a principled
   Second, we observe that the state representation is usually                                        foundation for future works on centralized critics in multi-
a complete version of the observation information in multi-                                           agent reinforcement learning. agent benchmarks used by recent state-of-the-art works. ",cs.LG,B,-0.064653285,0.29305613,-0.3301757
http://arxiv.org/pdf/2201.01221v2,A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning,"This work ﬁll in the gap
involve agent locations, while observations stem from on-                                               of theoretical understanding of state-based critics popular in
screen pixels that are of much higher dimension. This happen                                            multi-agent reinforcement learning, providing a principled
to be not unusual in current multi-agent reinforcement learn-                                           foundation for future works on centralized critics in multi-
ing benchmarks (see Sections 5.3 and 5.4). agent reinforcement learning. ",cs.LG,B,-0.11471113,0.24351725,-0.2886039
http://arxiv.org/pdf/2201.01230v1,Robust Semi-supervised Federated Learning for Images Automatic Recognition in Internet of Drones,"Simulation results show that our robust
SSFL system is signiﬁcantly better than existing solutions                        [20] X. Li, M. JIANG, X. Zhang, M. Kamp, and Q. Dou, “Fedbn: Federated
in performance under different settings. In future work, we                             learning on non-iid features via local batch normalization,” in Proc. of
will further improve the algorithm to maximize the use of                               ICLR, 2020.
unlabeled data. ",cs.LG,B,-0.06021595,0.11031736,0.0110334335
http://arxiv.org/pdf/2201.01235v1,On the Minimal Adversarial Perturbation for Deep Neural Networks with Provable Estimation Error,"attack. All graphs show that the higher d, the higher the
number of samples that escapes the bounds (a sample escapes            The presented results open two research directions to be
the bounds if t(x, l)/ρ∗ is higher than real distance from the     addressed in a future work. boundary). ",cs.LG,A,0.3331445,0.13168688,0.22718193
http://arxiv.org/pdf/2201.01289v1,Self-directed Machine Learning,"time series prediction, link and node prediction in graphs, etc. We will develop neural-symbolic reasoning systems to
                                                                       understand the relationship between high-level variables
   For future work, we plan to investigate the following               in data, develop logical and inductive reasoning systems
research directions. to discover the causality between tasks, and leverage
                                                                       the reasoning outcomes to guide self-directed learning
   • Interpretable SDML Being reliable is almost a must
      for ML models to be willingly used by humans. ",cs.LG,C,-0.07635401,-0.2183366,-0.24031861
http://arxiv.org/pdf/2201.01289v2,Self-directed Machine Learning,"We plan to bridge this gap. We will develop neural-symbolic reasoning systems to
   For future work, we plan to investigate the following                understand the relationship between high-level variables
research directions. in data, develop logical and inductive reasoning systems
                                                                        to discover the causality between tasks, and leverage
   • Interpretable SDML Being reliable is almost a must                 the reasoning outcomes to guide self-directed learning
      for ML models to be willingly used by humans. ",cs.LG,C,-0.105632275,-0.17666185,-0.40853667
http://arxiv.org/pdf/2201.01353v1,Linear Variational State Space Filtering,"The

     bution in our experiments since it is straightforward to implement and corresponds directly to a mean-square-error
     loss term in the resulting ELBO while still producing good results. We leave the problem of tuning this model for
     better performance to future work. 3. ",cs.LG,B,0.063593656,0.22680986,0.0751344
http://arxiv.org/pdf/2201.01353v2,Linear Variational State Space Filtering,"We note that it is also possible to satisfy (iv) by maximizing an objective function given by the

ELBO for the full distribution log p(X1:T |u1:t−1), in addition to an ELBO for

  k

     log p({{x(tj)}kj=1,j=k }Tt=1|u1:t−1),

k =2

i.e by inferring both the full ﬁltering distribution, as well as each ﬁlter where the observation j = k
has been removed. We leave experimental veriﬁcation of this approach as a matter for future work. 17
                                                           L-VSSF

A.5. ",cs.LG,A,0.2646192,0.33192855,0.12362212
http://arxiv.org/pdf/2201.01353v3,Linear Variational State-Space Filtering,"We note that it is also possible to satisfy (iv) by maximizing an objective function given by the

ELBO for the full distribution log p(X1:T |u1:t−1), in addition to an ELBO for

  k

     log p({{x(tj)}kj=1,j=k }Tt=1|u1:t−1),

k =2

i.e by inferring both the full ﬁltering distribution, as well as each ﬁlter where the observation j = k
has been removed. We leave experimental veriﬁcation of this approach as a matter for future work. 17
                                                           L-VSSF

A.5. ",cs.LG,A,0.2646191,0.33192852,0.12362215
http://arxiv.org/pdf/2201.01363v1,Sparse Super-Regular Networks,"Future work requires us to understand why, despite identical
                                                                                 layer densities, X–Nets seemed to slightly outperform SRNs
                                                                                 in terms of training loss and validation precision. Another
                                                                                 direction to take future work is to extend the notion of
                                                                                 transfer learning to embed multiple unrelated pre-trained sparse
                                                                                 networks into a slightly larger SRN, and begin training for a
                                                                                 more complex task using the newly embedded SRN as a starting
                                                                                 point. Additionally, it would be helpful to experimentally
determine the impact that increasing sparsity has on the

performance differential between SRNs and X–Nets. ",cs.LG,C,-0.29613337,0.060444497,0.076677814
http://arxiv.org/pdf/2201.01409v1,Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness,"3.5 Limitations and threats to validity

One of the limitations of our study is that in RQ4, our proposed technique is not appli-
cable in the Backdoor attack where the backdoor task is unknown. We will consider
this as future work and an extension for this study. In terms of conclusion validity, one potential threat could be the random factors
of the study, like client selection for training and selection of byzantine clients. ",cs.LG,A,0.24773875,-0.011603686,-0.039845403
http://arxiv.org/pdf/2201.01413v1,Towards Similarity-Aware Time-Series Classification,"The result again           and leverage multiple GCN layers to capture the multi-
veriﬁes the eﬀectiveness of SimTSC. hop connections, which is deferred as our future work. 4.3 Results on Other Neural Architectures To                     4.5 Analysis of Graph Structure For RQ4, we
investigate RQ2, we show the results of applying                 vary the hyperparameters of graph construction. ",cs.LG,C,-0.23774937,0.0017582905,0.31499285
http://arxiv.org/pdf/2201.01413v2,Towards Similarity-Aware Time-Series Classification,"The result again           and leverage multiple GCN layers to capture the multi-
veriﬁes the eﬀectiveness of SimTSC. hop connections, which is deferred as our future work. 4.3 Results on Other Neural Architectures To                     4.5 Analysis of Graph Structure For RQ4, we
investigate RQ2, we show the results of applying                 vary the hyperparameters of graph construction. ",cs.LG,C,-0.23774937,0.0017582905,0.31499285
http://arxiv.org/pdf/2201.01448v1,Conditional Imitation Learning for Multi-Agent Games,"Of                in the dataset contains about 1000 timesteps on average, which
course, more careful handling of the interaction between the                   means the total available sample size is limited. Nevertheless,
n − 1 players is possible, which we leave for future work. we can see our low-rank partner approach performs well, by
                                                                               inferring the strategy of the partner and predicting the expert
   In Hanabi [3], a group of agents (4 in our case) work                       actions that can complement this partner. ",cs.LG,A,0.15460871,0.010355548,-0.2096001
http://arxiv.org/pdf/2201.01488v1,Exemplar-free Class Incremental Learning via Discriminative and Comparable One-class Classifiers,"Statistical results of the misclassiﬁcations on CIFAR10        criminability and comparability of POC, we leave the above
with the pre-trained feature extractor. All misclassiﬁed samples         exploration for future work. are divided into two categories, namely, caused by lacking compa-
rability or discriminability. ",cs.LG,A,-0.04811996,-0.1284916,0.023467649
http://arxiv.org/pdf/2201.01529v1,Supervised Permutation Invariant Networks for Solving the CVRP with Bounded Fleet Size,"Our work
focuses on and showcases practical aspects for solving the VRP that are important for decision makers
in the planning industry and shows that our method outperforms existing models when accounting
for ﬁxed vehicle costs. In future work, we aim to alleviate the computational shortcomings of the
train loss calculation, such that the model’s fast training capability can be extended to problems with
larger ﬂeet sizes. 9
Under Review

7 REPRODUCIBILITY STATEMENT

Considering the current pace at which new approaches and methods are emerging, not only but also
for the research area of learning based combinatorial optimization, fast and easy access to source
codes with rigorous documentation needs to be established in order to ensure comparability and
veriﬁed state-of-the-arts. ",cs.LG,B,-0.15597072,0.19351,-0.13447693
http://arxiv.org/pdf/2201.01588v1,Using Machine Learning for Anomaly Detection on a System-on-Chip under Gamma Radiation,"#  Experiment duration    Model prediction          pointing out 100% of the anomalies before the board stopped
                                      before the annotation       working. Annotating the anomaly before it stops working and
            0             2:00:11                                 giving time to the designer to take actions allowed to detect
            1             0:41:31              1:43:53            the anomaly before the board dies, considered an extraordinary
            2             0:27:28              0:25:56            achievement since it will enable the designer to use this as an
            3             0:23:23              -0:00:12           assumption for future works. This work employed six boards
            4             0:13:39              0:07:04            that were inoperable after the experiments. ",cs.LG,A,0.22978334,-0.1701616,-0.11739497
http://arxiv.org/pdf/2201.01601v1,Sample Selection with Deadline Control for Efficient Federated Learning on Heterogeneous Clients,"Local Epoch Training Policies. While FedBalancer was                   This is part of our future work. originally designed for FL based on FedAvg [46] that per-
forms full data training per local epoch of client, recent stud-       6 RELATED WORK
ies such as Oort [34] propose to perform single batch training
per local epoch. ",cs.LG,B,-0.15351784,-0.00734034,0.049592253
http://arxiv.org/pdf/2201.01601v2,FedBalancer: Data and Pace Control for Efficient Federated Learning on Heterogeneous Clients,"To address this issue, we could integrate our sam-
ple selection strategy with other sample selection or reweighting       7 CONCLUSION
approaches [60, 74] that are designed to achieve unbiased model
training. We leave this as future work. We presented FedBalancer, a systematic FL framework with sample
                                                                        selection for optimized training process. ",cs.LG,B,0.04181889,-0.03247583,-0.21098828
http://arxiv.org/pdf/2201.01633v1,Adaptive Online Incremental Learning for Evolving Data Streams,"We did not take the transfer learning problem across several streaming
processes problem into account. In future work, we would consider the online incremental learning problem for multiple related
data streams. Because there is widespread multiple data streaming in many practical scenarios, it can

                                                                 36
be regarded as multi-task data streams. ",cs.LG,B,-0.11267157,0.057613753,-0.17161119
http://arxiv.org/pdf/2201.01680v1,Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems,"Dually, systems with poor observability
characteristics may also exhibit a diverging regret lower bound. This opens up an interesting
direction for future work: to ascertain the optimal dependency on system-theoretic quantities
for regret minimization in LQR and LQG. Acknowledgements. ",cs.LG,B,0.20815402,0.47480434,-0.16570139
http://arxiv.org/pdf/2201.01680v2,Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems,"Dually, systems with poor observability
characteristics may also exhibit a diverging regret lower bound. This opens up an interesting
direction for future work: to ascertain the optimal dependency on system-theoretic quantities
for regret minimization in LQR and LQG. Acknowledgements. ",cs.LG,B,0.20815402,0.47480434,-0.16570139
http://arxiv.org/pdf/2201.01702v1,Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations,"This sparsification is useful to capture patterns of original
         EdgePred      65.7±1.3                                               graphs and generate contrastive views. We leave the quantitive
                       65.2±1.6                                               assessment of the “usefulness"" in future work. AttrMasking     64.4±1.3
       ContextPred    67.88±0.85                                              5 CONCLUSIONS
                      68.24±0.87
         GraphCL      71.16±0.28                                              In this paper, we target more adaptive, automatic and generalizable
       LP-InfoMin     70.10±0.76                                              graph self-supervised learning, by introducing a learnable prior and
        LP-InfoBN                                                             a framework to learn it. ",cs.LG,C,-0.18539384,-0.17371182,0.016329547
http://arxiv.org/pdf/2201.01769v1,Knowledge Informed Machine Learning using a Weibull-based Loss Function,"The Weibull-based loss function, therefore, can be one of many parameters,
along with regularization techniques and model hyperparameters, that can be tuned to produce optimal
results. 4.3 Future Work
A natural path for future work is to test the Weibull-based loss functions on data from real-world
industrial environments. We suspect that applications consisting of large ﬂeets of pumps or gas
turbines, where failure modes and the associated Weibull distributions are well understood, would
beneﬁt from the use of the Weibull-based loss functions. ",cs.LG,A,0.083891645,0.08088231,0.13200721
http://arxiv.org/pdf/2201.01770v1,NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting,"2020) but is an obvious         modal aligned earnings calls dataset. avenue for future work to implement more sophisticated trading
strategies. 4. ",cs.LG,A,0.17896563,-0.22003815,-0.05243111
http://arxiv.org/pdf/2201.01811v1,CausalSim: Toward a Causal Data-Driven Simulator for Network Protocols,"Also, as CausalSim promises
                                                                           to increase the utility of such trials, performing RCTs may
   The most closely related line of work within this literature            become more appealing. However, proposing a solution that
we build upon is synthetic controls and its extension synthetic            overcomes the RCT data requirement is another interesting
                                                                           direction of future work. 12
References                                                             [13] Rajeev H Dehejia and Sadek Wahba. ",cs.LG,A,0.22734591,0.06938061,-0.108227625
http://arxiv.org/pdf/2201.01811v2,CausalSim: Unbiased Trace-Driven Network Simulation,"RCT. We believe that exploiting invariance of higher moments                                            https://doi.org/10.1145/1921168.1921197
can relax the current assumptions needed in our analysis,
which we leave as an interesting theoretical future work. [14] Changxiao Cai, Gen Li, Yuejie Chi, H Vincent Poor, and Yuxin Chen. ",cs.LG,A,0.3344819,0.058897115,0.065620646
http://arxiv.org/pdf/2201.01811v3,CausalSim: A Causal Inference Framework for Unbiased Trace-Driven Simulation,"Comparing the ﬁdelity of these approaches
this problem by explicitly estimating latent factors. For details            using real-world ABR data would be interesting future work
regarding the network architecture and training details for                  (Veritas evaluates its method in a network emulator). both SLSim and CausalSim, refer to Table 7 in the appendix. ",cs.LG,A,0.13737047,-0.01735971,0.098530844
http://arxiv.org/pdf/2201.01836v1,"A Generalized Bootstrap Target for Value-Learning, Efficiently Combining Value and Feature Predictions","Forward model-based planning can facilitate efﬁcient
credit assignment. Among the algorithms that address this                                     Many potential directions of investigation have been left
topic, are Dyna-style methods, which use explicit models to                                for future work. (i) The η-return mixture contains a succes-
generate ﬁctitious experience, that they then leverage to im-                              sor feature estimate, which could also be further leveraged
prove the value function (Schoknecht 2002; Parr et al. ",cs.LG,B,0.079090156,0.12664618,-0.18326804
http://arxiv.org/pdf/2201.01874v1,Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations,"An alternative approach could be considered, where
all stocks are aggregated based on their factor exposure. This is left here for a future work. References

Brown, D., Goo, W., Nagarajan, P., and Niekum, S. Extrap-
   olating Beyond Suboptimal Demonstrations via Inverse
   Reinforcement Learning from Observations. ",cs.LG,B,0.05026036,0.2737411,-0.17577964
http://arxiv.org/pdf/2201.01918v1,SABLAS: Learning Safe Control for Black-box Dynamical Systems,"Simulation results show that SABLAS
by the compared methods to achieve a nearly perfect relative            indeed provides a systematic way of learning safe control
                                                                        policies with a great improvement over existing safe RL
                                                                        methods. For future works, we plan to study SABLAS on
                                                                        multi-agent systems, especially with adversarial players. REFERENCES                                          [25] S. Dean, A. J. Taylor, R. K. Cosner, B. Recht, and A. D. Ames,
                                                                                      “Guaranteeing safety of learned perception modules via measurement-
 [1] Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, and S. Savarese, “Keto: Learning            robust control barrier functions,” in 2020 Conference on Robotics
      keypoint representations for tool manipulation,” in 2020 IEEE Inter-            Learning (CoRL), 2020.
      national Conference on Robotics and Automation (ICRA), 2020, pp. ",cs.LG,B,-0.17513233,0.23780558,-0.29015043
http://arxiv.org/pdf/2201.01918v2,SABLAS: Learning Safe Control for Black-box Dynamical Systems,"ACCEPTED JANUARY, 2022

of learning safe control policies with a great improvement over                  [24] H. Dai, B. Landry, L. Yang, M. Pavone, and R. Tedrake, “Lyapunov-
                                                                                       stable neural-network control,” Robotics Science and Systems (RSS),
safe RL methods. For future works, we plan to study SABLAS                             2021.

on multi-agent systems, especially with adversarial players. [25] S. Dean, A. J. Taylor, R. K. Cosner, B. Recht, and A. D. Ames,
                                                                                       “Guaranteeing safety of learned perception modules via measurement-
                             REFERENCES                                                robust control barrier functions,” in 2020 Conference on Robotics
                                                                                       Learning (CoRL), 2020. ",cs.LG,B,-0.12442981,0.29121256,-0.2911377
http://arxiv.org/pdf/2201.01943v1,"Machine Learning: Algorithms, Models, and Applications","Finally, we move onto the design considerations for the machine
learning model. We will focus on models made for diagnostic prediction, rather
than outcome prediction; however, we intend this only as a first step in using
machine learning to support patient care, with future work moving toward models
that provide personalized therapeutic recommendations as well. Throughout this
chapter we will apply the techniques being discussed to DCM to help contextualize
them. ",cs.LG,C,-0.1447956,-0.20069413,-0.36001664
http://arxiv.org/pdf/2201.01978v1,An Abstraction-Refinement Approach to Verifying Convolutional Neural Networks,"Work so far
has focused on merging neurons in order to produce a smaller, abstract network,
whereas our approach focuses on removing edges and then pruning unneeded
neurons entirely. Combining the edge-oriented and node-oriented abstraction
approaches is left for future work. Bound tightening techniques for DNNs have
been very extensively studied [4, 7, 14, 17, 21, 22, 50, 54, 56, 61, 65, 66, 69, 73], focus-
ing mostly on the ReLU function. ",cs.LG,C,-0.43879396,0.0030700825,0.15068546
http://arxiv.org/pdf/2201.01996v1,Skip Vectors for RDF Data: Extraction Based on the Complexity of Feature Patterns,"Therefore, for classiﬁcation tasks in RDF data, Skip vectors successfully represent the
features of target resources and can be combined with SVM, KNN, NN, RF, ADA, and the basic
GCN. In future work, we plan to apply Skip vectors to link prediction in knowledge graphs and logical
reasoning with ontology embeddings. 13
References

 [1] Daichi Arai and Ken Kaneiwa. ",cs.LG,C,-0.02868127,-0.27641696,-0.106637366
http://arxiv.org/pdf/2201.01996v2,Skip Vectors for RDF Data: Extraction Based on the Complexity of Feature Patterns,"Therefore, for classiﬁcation tasks in RDF data, Skip vectors successfully represent the
features of target resources and can be combined with SVM, KNN, NN, RF, ADA, and the basic
GCN. In future work, we plan to apply Skip vectors to link prediction in knowledge graphs and logical
reasoning with ontology embeddings. 13
References

 [1] Daichi Arai and Ken Kaneiwa. ",cs.LG,C,-0.02868127,-0.27641696,-0.106637366
http://arxiv.org/pdf/2201.01996v3,Skip Vectors for RDF Data: Extraction Based on the Complexity of Feature Patterns,"Therefore, for classiﬁcation tasks in RDF data, Skip vectors successfully represent the
features of target resources and can be combined with SVM, KNN, NN, RF, ADA, and the basic
GCN. In future work, we plan to apply Skip vectors to link prediction in knowledge graphs and logical
reasoning with ontology embeddings. References

 [1] Daichi Arai and Ken Kaneiwa. ",cs.LG,C,-0.028989322,-0.27552015,-0.10580639
http://arxiv.org/pdf/2201.02040v1,Deep Fusion of Lead-lag Graphs:Application to Cryptocurrencies,"Stablecoins USDC, USDT and BUSD stand
                                                                    out from the others on ﬁgure 8, it is however not surprising
   Figures 2, 3 and tables I and II demonstrate that a signiﬁ-      because they are not extensively impacted by large market
cant number of synchronous and asynchronous relations exist         moves. In a further study it would be interesting to study the
between assets. The number of asynchronous relations rapidly        relationship between the market return and the discrepancy of
decreases to 0 when the lag becomes large. ",cs.LG,A,0.3339501,0.02502327,0.09854652
http://arxiv.org/pdf/2201.02040v2,Deep Fusion of Lead-lag Graphs: Application to Cryptocurrencies,"Stablecoins USDC, USDT and BUSD stand
                                                                    out from the others on ﬁgure 8, it is however not surprising
   Figures 2, 3 and tables I and II demonstrate that a signiﬁ-      because they are not extensively impacted by large market
cant number of synchronous and asynchronous relations exist         moves. In a further study it would be interesting to study the
between assets. The number of asynchronous relations rapidly        relationship between the market return and the discrepancy of
decreases to 0 when the lag becomes large. ",cs.LG,A,0.3339501,0.02502327,0.09854652
http://arxiv.org/pdf/2201.02088v1,Deep Causal Reasoning for Recommendations,"(λb = 0.3)  High Conf. (λb = 0.7)
further research question: In the battle of the beneﬁts and draw-   Noise      R@20 N@20             R@20 N@20
backs of multi-cause confounders, which one will prevail? The       0.1        0.4071 0.3974         0.4098 0.4039
answer, which the authors believe, is that no matter the results,   0.5        0.4034 0.3963         0.4036 0.3982
a deconfounded recommender should always be preferred over          0.9        0.4010 0.3928         0.3993 0.3934
a non-causality-based one if unobserved confounders indeed          N.F. ",cs.LG,A,0.30336332,-0.0737147,0.015582379
http://arxiv.org/pdf/2201.02088v2,Deep Causal Reasoning for Recommendations,"(λb = 0.3)  High Conf. (λb = 0.7)
further research question: In the battle of the beneﬁts and draw-   Noise      R@20 N@20             R@20 N@20
backs of multi-cause confounders, which one will prevail? The       0.1        0.4071 0.3974         0.4098 0.4039
answer, which the authors believe, is that no matter the results,   0.5        0.4034 0.3963         0.4036 0.3982
a deconfounded recommender should always be preferred over          0.9        0.4010 0.3928         0.3993 0.3934
a non-causality-based one if unobserved confounders indeed          N.F. ",cs.LG,A,0.30336332,-0.0737147,0.015582379
http://arxiv.org/pdf/2201.02155v1,Topological Representations of Local Explanations,"Research Directions to Validate Topolog-
into scalar values. Additionally, future work should be directed              ical Models of Multi-Dimensional Data. In NeurIPS 2020 Workshop on Topological
towards extending GALE to accommodate prediction problems in                  Data Analysis and Beyond. ",cs.LG,C,0.053323127,-0.14962704,0.11111595
http://arxiv.org/pdf/2201.02177v1,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,"They ﬁnd that ﬂatness based measures that aim to quantify the sensitivity of the trained neural network
to parameter perturbations are the most predictive. We conjectured that the grokking phenomena

                                                           8
we report in this work may be due to the noise from SGD driving the optimization to ﬂatter/simpler
solutions that generalize better and hope to investigate in future work whether any of these measures
are predictive of grokking. Zhang et al. ",cs.LG,C,-0.18563512,0.111759335,0.016054502
http://arxiv.org/pdf/2201.02185v1,Admissible Policy Teaching through Reward Design,"On the ﬂip side, this algo-
of the parameters λ and , we vary these parameters and solve     rithm is only applicable to tabular settings, so one of the most
(P4-APT) with the considered approaches. The results are         interesting research directions for future work would be to
shown in Figure 2 for each environment separately. We make       consider its extensions based on function approximation. ",cs.LG,B,0.13512297,0.18854441,0.24754521
http://arxiv.org/pdf/2201.02229v1,Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT,"In the context of our work, the need
for diverse and effective negative samples is pertinent, given the number of false positive
predictions despite relatively high confidence. The creation of effective negative samples
and robust generalisability remain open research questions in need of further study. Human curation augmentation for PTM‑PPI extraction
The prediction quality can be improved by providing more training samples [34, 35],
which requires manual curation. ",cs.LG,A,0.058229297,-0.18499306,-0.16516805
http://arxiv.org/pdf/2201.02331v1,iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection,"Further investigation on
maple-research-lab/AVT-pytorch/tree/master/cifar. this will be our future work. 6We use the wrn architecure from https://github.com/hendrycks/
ss-ood/blob/master/multiclass ood/models/wrn.py
           98                  68                                    74                 5501                                                  100
           96
           94                  66                                    72                 4489                                                  98
                               64                                    70                                                                       96
           92                  62                                    68                 4467
           90                                                        66                                                                       94
           88                  60                                                                                                             92
           86  SVHN            58                              LSUN  64    IMAGENET     4445                                  CIFAR100        90   Places365
           84                  56                                    62                 43
                                                                     60 3 6 9 12 15 18                                                        88 3 6 9 12 15 18
               3 6 9 12 15 18        3 6 9 12 15 18                                                                           3 6 9 12 15 18

                               Figure 3: TNR vs |V(x)| reported by iDECODe on CIFAR-10 as iD. ",cs.LG,A,0.0024539307,-0.04721777,0.3470465
http://arxiv.org/pdf/2201.02397v1,Neural calibration of hidden inhomogeneous Markov chains -- Information decompression in life insurance,"The neural architecture and our intrinsic model evaluation are then presented in Section 4,
followed by the numerical results in Section 5. Lastly, we conclude with a summary and an outlook for further research
in Section 6. 2 General framework

Figure 2 illustrates two common examples in life insurance, where a policyholder evolves over time and can change
or maintain its state of being active, invalid or dead. ",cs.LG,B,0.06294924,0.058433324,-0.2943538
http://arxiv.org/pdf/2201.02463v1,Churn prediction in online gambling,"Section 5 details the diﬀerent conducted experiments and
their results. Section 6 discusses the conclusion and future work perspectives around this paper. 2 Related work

Formalising a churn prediction problem is no small matter. ",cs.LG,A,0.2587319,-0.18266165,-0.13600561
http://arxiv.org/pdf/2201.02534v1,MGAE: Masked Autoencoders for Self-Supervised Learning on Graphs,"Formally,
we use Emask and Ereserve to denote the edge sets that are masked out and remained, respectively,
where Emask ∪ Ereserve = E. To make the process more efﬁcient, we adopt random sampling with a
high masking ratio to obtain the masked edge set Emask. More sophisticated sampling strategies will
be explored as future work. Speciﬁcally, we further consider two types of random sampling schemes
to generate the masked input graphs. ",cs.LG,A,0.07218416,0.056067847,0.27900353
http://arxiv.org/pdf/2201.02582v1,Multi-Model Federated Learning,"We successfully showed
                                                                            that multi-model federated learning is viable in real-world
                                                                            tasks with results at least as good as when these models are
                                                                            trained independently. Potnetial future work directions include incorporating fea-
                                                                            tures like client unavailability and constraints on clients’
                                                                            training capabilities to explore multi-model federated learning
                                                                            in more complex scenarios. (b) Test accuracy as function of number of clients per round                                 REFERENCES
           for Model 2
                                                                            [1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. ",cs.LG,B,-0.15101853,0.038919065,-0.2889072
http://arxiv.org/pdf/2201.02615v1,Posture Prediction for Healthy Sitting using a Smart Chair,"Experiment results are presented in Section 4. Finally, conclusion of our work and recommendations for future work are presented
in Section 5. 2 Related Work

A study by [8] used 19 pressure sensors to identify sitting postures. ",cs.LG,A,0.20323515,-0.28288978,-0.038373552
http://arxiv.org/pdf/2201.02621v1,Spatio-Temporal Graph Representation Learning for Fraudster Group Detection,"by 5% (4%), 12% (5%), 12 (5%)% on the Yelp (Amazon)
                                    Amazon
    1                                                              dataset, respectively. For future work, the stochastic modeling

0.9                                                                of the reviewers’ relations can be considered to provide a better

0.8                                                                behavioral representation of the reviewers, as most fraudsters

0.7                                                                randomly change their grouping relations to escape detection. 0.6                                                                                             REFERENCES

0.5                                                                 [1] A. Mukherjee, B. Liu, and N. Glance, “Spotting fake reviewer groups in
                                                                         consumer reviews,” in Proceedings of the 21st international conference
     Precision  Recall                       F1-value                    on World Wide Web, 2012, pp. ",cs.LG,A,0.20233953,-0.15179023,-0.14153719
http://arxiv.org/pdf/2201.02628v1,Attention Option-Critic,"We
must be relearned upon transfer. Furthermore, results were         leave such avenues of possible combination as future work. References                             McGovern, A.; and Barto, A. G. 2001. ",cs.LG,A,0.19564953,-0.024454098,-0.06625788
http://arxiv.org/pdf/2201.02661v1,Stay Positive: Knowledge Graph Embedding Without Negative Sampling,"Discussion & Conclusion
replacement for negative samples. For further analysis, we
measured the performance of the models and baselines when                                                      In this paper, we proposed a novel regularization technique
applying no dropout. We observed that the ﬁltered MRR                                                          that obviates the need for negative examples during train-
of DistMult reduced by 2.7% and 18.4% on WN18AM                                                                ing and we showed its notable merits on several tasks and
and FB15k-237AM respectively whereas this reduction for                                                        datasets. ",cs.LG,A,-0.050021626,-0.10026923,0.0611718
http://arxiv.org/pdf/2201.02664v2,Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory,"and as shown in the paragraph above, may lead to further
improvements. Another topic of future work is to explore         Cohen, G., Afshar, S., Tapson, J., and Van Schaik, A.
compatibility with privacy protocols. EMNIST: Extending MNIST to handwritten letters. ",cs.LG,A,-0.015723214,-0.07172464,-0.11117585
http://arxiv.org/pdf/2201.02664v3,Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory,"Exploring time-variability in more
depth is a topic for future work, including possibly more sophisticated ways of modulating the R–D
trade-off over time, such as adapting to running training metrics. Another topic of future work is to
explore compatibility with privacy protocols. References

Alham Fikri Aji and Kenneth Heaﬁeld. ",cs.LG,A_centroid,0.14805059,-0.04141005,-0.109149694
http://arxiv.org/pdf/2201.02692v1,Improved Input Reprogramming for GAN Conditioning,"Therefore, how to improve INREP+ on large-scale datasets is still a challenging problem, especially when GANs’ latent
space structure has still not been well-understood. We leave this open problem for future work. Beyond GAN: Can we apply INREP+ to other generative models? ",cs.LG,C,-0.15656292,-0.12668633,-0.019626733
http://arxiv.org/pdf/2201.02692v2,Improved Input Reprogramming for GAN Conditioning,"Therefore, how to improve INREP+ on large-scale datasets is still a challenging problem, especially when GANs’ latent
space structure has still not been well-understood. We leave this open problem for future work. Beyond GAN: Can we apply INREP+ to other generative models? ",cs.LG,C,-0.15656292,-0.12668633,-0.019626733
http://arxiv.org/pdf/2201.02692v3,Improved Input Reprogramming for GAN Conditioning,"12
                                     Improved Input Reprogramming for GAN Conditioning

Therefore, how to improve INREP+ on large-scale datasets is still a challenging problem, especially when GANs’ latent
space structure has still not been well-understood. We leave this open problem for future work. Beyond GAN: Can we apply INREP+ to other generative models? ",cs.LG,C,-0.25152558,-0.047857787,-0.02429791
http://arxiv.org/pdf/2201.02755v1,Machine Learning-Based Disease Diagnosis:A Bibliometric Analysis,"From a bibliometrics standpoint, this article comprehensively
                                                  studies MLBDD papers from 2012 to 2021. Consequently, with particular keywords, 1710 papers
                                                  with associate information have been extracted from Scopus and Web of Science (WOS) database and
                                                  integrated into the excel datasheet for further analysis. First, we examine the publication structures
                                                  based on yearly publications and the most productive countries/regions, institutions, and authors. ",cs.LG,A,0.33139452,-0.2537154,0.0151079
http://arxiv.org/pdf/2201.02863v3,PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++,"1.05                                          1.05
                                                                                                  Compared with WAGE [29], NITI [27], and the original DFA
1.00                                          1.00                                             paper [20], PocketNN achieved almost identical performance with
                                                                                               much simpler integer-only training and inference algorithms which
0.95                                          0.95                                             helps solving integer overflow problem and does not require any
                                                                                               complex quantization schemes. 0.90                                          0.90
                                                                                                  In future work, it will be important to apply PocketNN to other
0.85                                               0.85  PocketNN Fashion-MNIST training       DNN architectures such as convolutional neural networks (CNNs),
0.80                                                     PocketNN Fashion-MNIST validation     recurrent neural networks (RNNs) and residual network (ResNet)
0.75 0          PocketNN MNIST training                  Floating BP Fashion-MNIST training    [13] on other popular datasets such as CIFAR-10, CIFAR-100 and
                PocketNN MNIST validation 0.80           Floating BP Fashion-MNIST validation  ImageNet. Floating BP MNIST training
                Floating BP MNIST validation             20 40 60 80 100
        20 40 60 80 100 0.75 0                                        Epoch

                    Epoch

Figure 4: Comparison of PocketNN and floating-point BP
with SGD on MNIST and Fashion-MNIST datasets. ",cs.LG,B,-0.3696763,-0.04818949,0.14918254
http://arxiv.org/pdf/2201.02863v4,PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++,"1.05                                          1.05
                                                                                                  Compared with WAGE [29], NITI [27], and the original DFA
1.00                                          1.00                                             paper [20], PocketNN achieved almost identical performance with
                                                                                               much simpler integer-only training and inference algorithms which
0.95                                          0.95                                             helps solving integer overflow problem and does not require any
                                                                                               complex quantization schemes. 0.90                                          0.90
                                                                                                  In future work, it will be important to apply PocketNN to other
0.85                                               0.85  PocketNN Fashion-MNIST training       DNN architectures such as convolutional neural networks (CNNs),
0.80                                                     PocketNN Fashion-MNIST validation     recurrent neural networks (RNNs) and residual network (ResNet)
0.75 0          PocketNN MNIST training                  Floating BP Fashion-MNIST training    [13] on other popular datasets such as CIFAR-10, CIFAR-100 and
                PocketNN MNIST validation 0.80           Floating BP Fashion-MNIST validation  ImageNet. Floating BP MNIST training
                Floating BP MNIST validation             20 40 60 80 100
        20 40 60 80 100 0.75 0                                        Epoch

                    Epoch

Figure 4: Comparison of PocketNN and floating-point BP
with SGD on MNIST and Fashion-MNIST datasets. ",cs.LG,B,-0.3696763,-0.04818949,0.14918254
http://arxiv.org/pdf/2201.02863v5,PocketNN: Integer-only Training and Inference of Neural Networks without Quantization via Direct Feedback Alignment and Pocket Activations in Pure C++,"1.05                                          1.05
                                                                                                  Compared with WAGE [29], NITI [27], and the original DFA
1.00                                          1.00                                             paper [20], PocketNN achieved almost identical performance with
                                                                                               much simpler integer-only training and inference algorithms which
0.95                                          0.95                                             helps solving integer overflow problem and does not require any
                                                                                               complex quantization schemes. 0.90                                          0.90
                                                                                                  In future work, it will be important to apply PocketNN to other
0.85                                               0.85  PocketNN Fashion-MNIST training       DNN architectures such as convolutional neural networks (CNNs),
0.80                                                     PocketNN Fashion-MNIST validation     recurrent neural networks (RNNs) and residual network (ResNet)
0.75 0          PocketNN MNIST training                  Floating BP Fashion-MNIST training    [13] on other popular datasets such as CIFAR-10, CIFAR-100 and
                PocketNN MNIST validation 0.80           Floating BP Fashion-MNIST validation  ImageNet. Floating BP MNIST training
                Floating BP MNIST validation             20 40 60 80 100
        20 40 60 80 100 0.75 0                                        Epoch

                    Epoch

Figure 4: Comparison of PocketNN and floating-point BP
with SGD on MNIST and Fashion-MNIST datasets. ",cs.LG,B,-0.3696763,-0.04818949,0.14918254
http://arxiv.org/pdf/2201.02863v6,PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++,"1.05                                          1.05
                                                                                                  Compared with WAGE [29], NITI [27], and the original DFA
1.00                                          1.00                                             paper [20], PocketNN achieved almost identical performance with
                                                                                               much simpler integer-only training and inference algorithms which
0.95                                          0.95                                             helps solving integer overflow problem and does not require any
                                                                                               complex quantization schemes. 0.90                                          0.90
                                                                                                  In future work, it will be important to apply PocketNN to other
0.85                                               0.85  PocketNN Fashion-MNIST training       DNN architectures such as convolutional neural networks (CNNs),
0.80                                                     PocketNN Fashion-MNIST validation     recurrent neural networks (RNNs) and residual network (ResNet)
0.75 0          PocketNN MNIST training                  Floating BP Fashion-MNIST training    [13] on other popular datasets such as CIFAR-10, CIFAR-100 and
                PocketNN MNIST validation 0.80           Floating BP Fashion-MNIST validation  ImageNet. Floating BP MNIST training
                Floating BP MNIST validation             20 40 60 80 100
        20 40 60 80 100 0.75 0                                        Epoch

                    Epoch

Figure 4: Comparison of PocketNN and floating-point BP
with SGD on MNIST and Fashion-MNIST datasets. ",cs.LG,B,-0.3696763,-0.04818949,0.14918254
http://arxiv.org/pdf/2201.02873v1,LoMar: A Local Defense Against Poisoning Attack on Federated Learning,"Section 4 provides our experiment results
and performance evaluations. Section 5 further summarizes          repeats the following three steps to obtain the joint model
the related work, followed by a conclusion and future work         wt from the current wt−1. discussion in Section 6. ",cs.LG,A,0.19132121,-0.033795238,0.12413365
http://arxiv.org/pdf/2201.02880v1,Attention-based Random Forest and Contamination Model,"Moreover, there are diﬀerent deﬁnitions of vector Ak(xs)
itself, for example, we can use the median instead of the mean value. This is also an interesting direction
for further research. 21
Acknowledgement

This work is supported by the Russian Science Foundation under grant 21-11-00116. ",cs.LG,A,0.28055337,0.09349042,0.1292023
http://arxiv.org/pdf/2201.02923v1,Open-Set Recognition of Breast Cancer Treatments,"Whether it is reconstruction or not,
latent representations must encapsulate structural information of the data to be more eﬀective. To be sure, this particular application to healthcare opens interesting avenues of further research
to the expanding scope of open-set recognition. Likewise, this study hopefully represents a stride
towards these techniques beneﬁting actual patients’ treatments in the future. ",cs.LG,C,0.0024746635,-0.27637696,-0.20448008
http://arxiv.org/pdf/2201.02941v1,TPAD: Identifying Effective Trajectory Predictions Under the Guidance of Trajectory Anomaly Detection Model,"In this paper, we make the ﬁrst
attempt to utilize the AD technique to solve the identiﬁcation
problems in stochastic TP model, and achieve preliminary
success. In future works, we will try to ﬁnd or design more
appropriate AD technique for TP area, to further increase the

C. Wang, C. Liang, X. Chen, H. Wang: Preprint submitted to Elsevier                          Page 13 of 14
                    TPAD: Identifying Eﬀective Trajectory Predictions Under the Guidance of Trajectory AD Model

   trajectory prediction, in: CVPR, pp. 14424–14432. ",cs.LG,B,0.13932246,0.15093714,-0.121495545
http://arxiv.org/pdf/2201.03092v1,Uncovering the Source of Machine Bias,"Consistent
with our observation in Section 5, repeated users have smaller bias       Our paper also has certain limitations that can be addressed in
than new users. This indicates that signals can help mitigate bias     future work. First, the microloan users are generally not stable in
in machine learning models as well. ",cs.LG,A,0.12509587,-0.03770922,-0.22089407
http://arxiv.org/pdf/2201.03187v1,An Adaptive Neuro-Fuzzy System with Integrated Feature Selection and Rule Extraction for High-Dimensional Classification Problems,"3, pp. 1110–1123, 2021.
model will be considered in our future work. Use of the
proposed AdaTSK framework for regression/prediction prob-                        [18] S. Solorio-Ferna´ndez, J. F. Mart´ınez-Trinidad, and J. ",cs.LG,A,0.219313,-0.12464533,-0.04525614
http://arxiv.org/pdf/2201.03215v1,Fully automatic scoring of handwritten descriptive answers in Japanese language tests,"As QWK is over 0.8, it represents
acceptable similarity of scoring between the automatic scoring system and the
human examiners. These results are promising for further research on end-to-end
automatic scoring of descriptive answers. Keywords— handwritten language answers, handwriting recognition, automatic
scoring, ensemble recognition, deep neural networks

This is the preprint revision. ",cs.LG,A,-0.062150896,-0.20659041,-0.17505735
http://arxiv.org/pdf/2201.03225v1,Integration of Explainable Artificial Intelligence to Identify Significant Landslide Causal Factors for Extreme Gradient Boosting based Landslide Susceptibility Mapping with Improved Feature Selection,"Thus, in this
evaluation stage, Optimized Extreme Gradient Boosting is identiﬁed as the most efﬁcient algorithm to classify Landslide
Susceptible area using the 15 feature variables from the utilized dataset. 3.4 Evaluation Stage 2: TreeSHAP Analysis

As optimized XgBoost is outperforming all other machine learning classifers besides it has robust explainability, we
further decided to proceed with the optimzed XgBoost model for further analysis. In this stage of evaluation, we
have integrated the TreeSHAP method with the optimized XgBoost (Extreme Gradient Boosting) Classiﬁer(our best
performing classiﬁer from previous evaluation stage) to understand the prediction criteria of XgBoost and the level
of contribution of individual features that strongly corroborate in Landslide Susceptibility prediction. ",cs.LG,C,-0.038529612,-0.1610089,-0.030343521
http://arxiv.org/pdf/2201.03291v1,A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study,"However, compared to variable ranking based on a single RF, ShapleyVIC requires much
longer run-time and larger memory space, which would benefit from the use of high-
performance computers and parallel computing (option available in the ShapleyVIC R
package22). Another limitation of ShapleyVIC is that the sampled set of models generated
using our pragmatic sampling approach may not fully represent the entire set of near-optimal
models (which is formally referred to as the Rashomon set),17 which remains a challenge in
this area of research.1,16 In addition, although collinearity was not present in our example

                                                                                                              11
(where generalized VIF for all 41 candidate variables were below 2), it is generally an
unresolved difficulty in variable importance assessments.16,17,23–25 Our pragmatic solution of
using absolute SAGE values as model reliance measures for variables with VIF>2 worked
well in a previous empirical experiment,17 and future work aims to devise more formal
solutions. Our work contributes to the recent emphasis on interpretability and transparency of
prediction models for high-stakes decision making,26 by devising a robust and interpretable
approach for developing clinical scores. ",cs.LG,A,0.15923236,-0.12092716,-0.23161039
http://arxiv.org/pdf/2201.03326v2,Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach,"Furthermore, we ﬁnd that the embeddings gen-
erated by a model trained with our procedure lead to higher performance on downstream
tasks that were not seen during training, and that the episodic training procedure leads to
better embeddings even in the single-task setting. We believe this work can be of interest to
the community as it explores the under-studied area of multi-task representation learning
on graphs, and further introduces a method built on optimization-based meta-learning (in-
heriting the properties of being model-agnostic), which can be adapted to other domains as
future work. Another interesting direction is to incorporate more advanced meta-learning
strategies like Rajeswaran et al. ",cs.LG,C,-0.2912122,-0.034572113,-0.22442779
http://arxiv.org/pdf/2201.03456v1,Optimizing Diffusion Rate and Label Reliability in a Graph-Based Semi-supervised Classifier,"Preliminary results showed that LGC LVO AutoD is a viable way
to get a good estimate of the optimal diffusion rate, and removing the diagonal entries
proved to be the crucial step for avoiding overﬁtting. For future work, we will be further evaluating LGC LVO AutoD. We will also try to
integrate LGC LVO AutoD and LGC LVO AutoL together into one single algorithm. ",cs.LG,B,0.030741176,0.32561043,0.21104392
http://arxiv.org/pdf/2201.03522v1,When is Offline Two-Player Zero-Sum Markov Game Solvable?,"Another direction is to design decentralized
algorithm for oﬄine MARL. The answer to this question is especially crucial if we want to further study
oﬄine MARL with a large number of agents and we do not want the sample complexity scales exponentially
with the number of agents. Lastly, in this paper we only focus on the most fundamental tabular setting. ",cs.LG,B,0.19968766,0.24122609,0.06735517
http://arxiv.org/pdf/2201.03522v2,When is Offline Two-Player Zero-Sum Markov Game Solvable?,"We believe this work can shed new light on oﬄine MARL. Here we list several open problems for future work. One direction is to ﬁnd the minimax sample complexity
of oﬄine Markov games under the unilateral concentration. ",cs.LG,B,0.2169942,0.44721425,-0.07935152
http://arxiv.org/pdf/2201.03529v2,Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning,"Following a similar motivation, we argue that the
backbone. Furthermore, we did additional investigations           linearized ﬁne-tuning solution should be well captured by a
using the 15 representation learning methods presented            linear combination of the intermediate activations. on the VTAB-leaderboard2, where we calculated median
percentage-improvement over scratch training for each task,       To demonstrate our intuition, consider a multi-layer, fully-
and similarly observed a high Spearman correlation (0.803). ",cs.LG,C,-0.31043437,0.020350287,-0.12186001
http://arxiv.org/pdf/2201.03544v1,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"We observe that different detectors are better for different tasks, suggesting that future detectors
could do better than any of our baselines. Our benchmark and baseline provides a starting point for
further research on mitigating reward hacking. 6 DISCUSSION

In this work, we designed a diverse set of environments and proxy rewards, uncovered several in-
stances of phase transitions, and proposed an anomaly detection task to help mitigate these transi-
tions. ",cs.LG,A,0.07660559,0.11073337,-0.15639158
http://arxiv.org/pdf/2201.03544v2,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"We observe that different detectors are better for different tasks, suggesting that future detectors
could do better than any of our baselines. Our benchmark and baseline provides a starting point for
further research on mitigating reward hacking. 6 DISCUSSION

In this work, we designed a diverse set of environments and proxy rewards, uncovered several in-
stances of phase transitions, and proposed an anomaly detection task to help mitigate these transi-
tions. ",cs.LG,A,0.07660559,0.11073337,-0.15639158
http://arxiv.org/pdf/2201.03696v1,Stratified Graph Spectra,"These issues need to be paid particular attention in practice. The future work will concentrate on addressing these issues. 48
Appendix A. ",cs.LG,A,0.23944658,-0.2987886,-0.13671324
http://arxiv.org/pdf/2201.03789v1,Partial Model Averaging in Federated Learning: Performance Guarantees and Benefits,"In this work, we simply choose a random subset of workers
as the new active workers. Studying the impact of different device selection schemes on the convergence properties and the
accuracy can be an interesting future work. Model Partitioning When synchronizing a part of model in Algorithm 1, the model can be partitioned in many different
ways. ",cs.LG,B,0.03388286,0.19055511,0.057029985
http://arxiv.org/pdf/2201.03819v1,Path differentiability of ODE flows,"The latter includes also convergence
guaranties for the small step gradient like method. Some concluding remarks are collected
in Section 7 together with further research lines. Finally, Appendix A gathers technical
results used throughout the paper. ",cs.LG,B,-0.004573293,0.30789465,0.15430835
http://arxiv.org/pdf/2201.03834v1,Reward Relabelling for combined Reinforcement and Imitation Learning on sparse-reward tasks,"Interestingly, we show that these methods stack together, as the
best results were obtained with SACR2*, which combines SACR2 with the n-step loss from SACfD
and the behaviour cloning loss from SACBC. Regarding SACR2, further analysis needs to be done on the impact of the hyper-parameters b
and N across different tasks, since our results didn’t shed much light on how to choose them. The
main limitation of our method is that it assumes that the expert demonstrations are optimal: Other
works like SACBC, NAC (Gao et al., 2018) and the more recent GRI (Chekroun et al., 2021) are able
to handle sub-optimal demonstrations, which is very useful for most tasks outside of robotics, and
even for more complex robotics tasks where an optimal planner is not available. ",cs.LG,B,-0.05497564,0.36982,-0.13513955
http://arxiv.org/pdf/2201.03916v1,Automated Reinforcement Learning (AutoRL): A Survey and Open Problems,"Such methods are capable of ﬁnding well performing and
robust hyperparameters for a set of environments (Eggensperger et al., 2019). Recently,
evaluation protocols were proposed that consider the performance of RL algorithms across a
set of environments (Jordan et al., 2020; Patterson et al., 2021), which may prove useful for
future work in this space. Related to the problem of algorithm conﬁguration, algorithm selection (Rice, 1976) can
be used to choose which RL algorithm to use for learning (Laroche & Féraud, 2018b). ",cs.LG,B,-0.10197413,0.18383761,-0.047378447
http://arxiv.org/pdf/2201.03916v2,Automated Reinforcement Learning (AutoRL): A Survey and Open Problems,"Such methods are capable of ﬁnding well performing and robust
hyperparameters for a set of environments (Eggensperger et al., 2019). Recently, evaluation
protocols were proposed that consider the performance of RL algorithms across a set of
environments (Jordan et al., 2020; Kirk et al., 2021; Patterson et al., 2021), which may prove
useful for future work in this space. Meanwhile, it also raises the problem of AutoRL-speciﬁc
benchmarking, which considers diﬀerent metrics such as performance improvement, data
eﬃciency and generalization capability of the AutoRL method. ",cs.LG,B,-0.024339348,0.17057411,0.08961162
http://arxiv.org/pdf/2201.03947v1,Active Reinforcement Learning -- A Roadmap Towards Curious Classifier Systems for Self-Adaptation,"steps towards an ARL approach. In our current and
future work, we focus on these challenges. Fredivianus, N., Prothmann, H., and Schmeck, H. (2010). ",cs.LG,B,0.055658218,0.13931285,-0.057042114
http://arxiv.org/pdf/2201.03957v1,Multi-granularity Relabeled Under-sampling Algorithm for Imbalanced Data,"On the basis of our proposed algorithm, we can also
explore the class-overlap undersampling algorithms of multi-class imbalanced data. Due to the essen-
tial diﬀerence between the multi-class classiﬁcation problem and the binary classiﬁcation problem, in
future work, we can further apply this algorithm to the multi-class overlap problem to explore the
classiﬁcation performance of our proposed method in multi-class classiﬁcation problem. Acknowledgements

    This work was supported by the Science Foundation of China University of Petroleum, Beijing
(No.2462020YXZZ023). ",cs.LG,A,0.17642172,-0.032429326,0.01676401
http://arxiv.org/pdf/2201.04038v1,DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation,"the concept drift frequency of T asktest
DDG-DA with GHO for optimization and an MLP proxy               and T asktrain may be different). For future work, we will
model. As the results show in Table 3, DDG-DA[GHO] out-         improve DDG-DA to a dynamic one to solve this limitation. ",cs.LG,B,0.14749172,0.15859702,0.08625282
http://arxiv.org/pdf/2201.04038v2,DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation,"the concept drift frequency of T asktest
and T asktrain may be different). For future work, we will     Gomes, H. M.; Barddal, J. P.; Ferreira, L. E. B.; and Bifet, A.
improve DDG-DA to a dynamic one to solve this limitation. 2018. ",cs.LG,A,0.21772492,0.020615822,0.006460363
http://arxiv.org/pdf/2201.04122v3,In Defense of the Unitary Scalarization for Deep Multi-Task Learning,"Nevertheless, we remark that ﬁne-tuning will be easier for unitary
scalarization due to its shorter runtimes. Finally, we presented the regularization hypothesis only as a
partial explanation of our results: we hope it will steer further analysis and consequently improve the
understanding of MTL. Acknowledgements

VK was funded by Samsung R&D Institute UK through the EPSRC Centre for Doctoral Training
(CDT) in Autonomous Intelligent Machines and Systems (AIMS) at the University of Oxford . ",cs.LG,B,-0.15809828,0.28553438,0.041573256
http://arxiv.org/pdf/2201.04182v1,HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning,"This is useful if, for example, the client model needs to be adjusted to a
particular set of known classes most widely used by this client. We also anticipate that with more
complex data augmentations and additional synthetic tasks, more complex transformer-based mod-
els can further improve their performance on the test set and a deeper analysis of such techniques
will be the subject of our future work. 4.3 SEMI-SUPERVISED RESULTS

In our approach, the weight generation model is trained by optimizing the loss calculated on the
query set and therefore any additional information about the task, including unlabeled samples, can
be provided as a part of the support set to the weight generator without having to alter the optimiza-
tion objective. ",cs.LG,B,-0.15544999,-0.031971734,0.007831299
http://arxiv.org/pdf/2201.04182v2,HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning,"This is useful if, for example, the client model needs to be adjusted to a
particular set of known classes most widely used by this client. We also anticipate that with more
complex data augmentations and additional synthetic tasks, more complex transformer-based mod-
els can further improve their performance on the test set and a deeper analysis of such techniques
will be the subject of our future work. 4.3 SEMI-SUPERVISED RESULTS

In our approach, the weight generation model is trained by optimizing the loss calculated on the
query set and therefore any additional information about the task, including unlabeled samples, can
be provided as a part of the support set to the weight generator without having to alter the optimiza-
tion objective. ",cs.LG,B,-0.15544999,-0.031971734,0.007831299
http://arxiv.org/pdf/2201.04182v3,HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning,"Verifying
on common few-shot learning datasets by generating just the         this hypothesis and understanding the “boundary” in the
last logits layer of the CNN model. But is it advantageous          model space between two regimes where a static backbone
to be generating additional CNN layers (ultimately fully            is sufﬁcient or not will be the subject of our future work. utilizing the capability of the HT model)? ",cs.LG,C,-0.22147922,0.007345793,-0.015521467
http://arxiv.org/pdf/2201.04188v1,Dynamic Price of Parking Service based on Deep Learning,"Each row represents the
    actual class and each column represents the predicted class. as future work. The lowest high-errors, both adjacent and non-adjacent, are
achieved for the conﬁguration of 168 hours, being respectively 11% for the Alert
labels predicted as Warning, and 2% for two cases. ",cs.LG,A,0.16441078,-0.20769241,-0.035409123
http://arxiv.org/pdf/2201.04234v1,Leveraging Unlabeled Data to Predict Out-of-Distribution Performance,"Finally, we should note that while ATC outperforms
previous approaches, it still suffers from large estimation error on datasets with novel populations,
e.g., BREEDS. We hope that our ﬁndings can lay the groundwork for future work for improving
accuracy estimation on such datasets. Reproducibility Statement We have been careful to ensure that our results are reproducible. ",cs.LG,A,0.14910695,-0.21400128,0.020530704
http://arxiv.org/pdf/2201.04234v2,Leveraging Unlabeled Data to Predict Out-of-Distribution Performance,"Finally, we should note that while ATC outperforms
previous approaches, it still suffers from large estimation error on datasets with novel populations,
e.g., BREEDS. We hope that our ﬁndings can lay the groundwork for future work for improving
accuracy estimation on such datasets. Reproducibility Statement We have been careful to ensure that our results are reproducible. ",cs.LG,A,0.14910695,-0.21400128,0.020530704
http://arxiv.org/pdf/2201.04234v3,Leveraging Unlabeled Data to Predict Out-of-Distribution Performance,"Finally, we should note that while ATC outperforms
previous approaches, it still suffers from large estimation error on datasets with novel populations,
e.g., BREEDS. We hope that our ﬁndings can lay the groundwork for future work for improving
accuracy estimation on such datasets. Reproducibility Statement Our code to reproduce all the results is available at https://
github.com/saurabhgarg1996/ATC_code. ",cs.LG,A,0.13693532,-0.18049799,0.06453018
http://arxiv.org/pdf/2201.04292v1,Predicting Terrorist Attacks in the United States using Localized News Data,"Finally, our results aﬃrm the need for localized models; in our case, manifested by creating separate models for each
state. We envision a number of areas for future work, including:

   (1) Machine learning models that can better account for noise, either via manual feature engineering or noise-aware
        learning algorithms, and the diﬀerences between states. (2) The application of news data to other multimodal learning problems. ",cs.LG,C,-0.17183086,-0.0888053,-0.1955041
http://arxiv.org/pdf/2201.04292v2,Predicting Terrorist Attacks in the United States using Localized News Data,"Finally, our results aﬃrm the need for localized models; in our case, manifested by creating separate models for each
state. We envision a number of areas for future work, including:

   (1) Machine learning models that can better account for noise, either via manual feature engineering or noise-aware
        learning algorithms, and the diﬀerences between states. (2) The application of news data to other multimodal learning problems. ",cs.LG,C,-0.17183086,-0.0888053,-0.1955041
http://arxiv.org/pdf/2201.04343v1,An Efficient and Adaptive Granular-ball Generation Method in Classification Problem,"It proves that our method is effective,
  svmguide3        0.6964     0.7265   0.7061  0.6750                but whether there are other adaptive methods, such as based
                   0.9994     1.0000   1.0000  1.0000                on the consistency of the internal distribution of granular-
     sonar         0.8500     0.8586   0.8550  0.8462                balls, may develop a more effective granular-ball adaptive
     splice                                                          optimization method. However, the proposed methods exhibit
  mushrooms                                                          lower accuracy in some cases than the existing method, so we
    Average                                                          will study how to improve their accuracy in the future work. generation method as the comparison method. ",cs.LG,A,0.16119954,0.0003328845,0.19161803
http://arxiv.org/pdf/2201.04343v2,An Efficient and Adaptive Granular-ball Generation Method in Classification Problem,"It proves that our method is effective,
  svmguide3        0.6964     0.7265   0.7061  0.6750                but whether there are other adaptive methods, such as based
                   0.9994     1.0000   1.0000  1.0000                on the consistency of the internal distribution of granular-
     sonar         0.8500     0.8586   0.8550  0.8462                balls, may develop a more effective granular-ball adaptive
     splice                                                          optimization method. However, the proposed methods exhibit
  mushrooms                                                          lower accuracy in some cases than the existing method, so we
    Average                                                          will study how to improve their accuracy in the future work. generation method as the comparison method. ",cs.LG,A,0.16119954,0.0003328845,0.19161803
http://arxiv.org/pdf/2201.04368v1,Preventing Manifold Intrusion with Locality: Local Mixup,"that such a notion of locality is beneﬁcial and could
                                                          be leveraged to a greater level in future work, or could
For these experiments, we also tried to use a K-nearest   be incorporated to the various Mixup extensions that
neighbor graph or a thresholded graph but without
being able to achieve smaller error rates compared to
Rapha¨el Baena, Lucas Drumetz, Vincent Gripon

have been proposed in the community. In future work,     Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
we would like to investigate further the choice of the      Sun. Deep residual learning for image recognition. ",cs.LG,C,-0.2583744,-0.10428451,0.1146076
http://arxiv.org/pdf/2201.04449v1,Intra-domain and cross-domain transfer learning for time series data -- How transferable are the features?,"We found the optimal value of this hyper-
parameter for each model in each scenario through grid search; it was not
the same for all models, but in most cases it was less than or equal to 1.0. However, in some cases and for some models, values greater than 1.0 have
been shown to function well, which could be examined in terms of diﬀerential
learning rate research in future work. We started with the idea that all TS data are essentially signals that all
TS data can be decomposed into a linear combination of sine and cosine
waves, which could indicate that there is common knowledge that can be
used to solve problems in various TS domains. ",cs.LG,B,0.001492959,0.15691683,-0.0020415122
http://arxiv.org/pdf/2201.04455v1,SLISEMAP: Explainable Dimensionality Reduction,"On the other hand, the state-of-
the-art tools to explain black box models are mostly focused on classiﬁers and they typically
provide only local explanations. An interesting future work would be to explore how the SLISEMAP visualizations can be
used to help build better models and to visualize or adjust model parameters. For example,
if SLISEMAP visualization could show that some data items are systematically classiﬁed
wrongly, which could then be used to improve the underlying supervised learning model
with the user feedback. ",cs.LG,C,-0.105959974,-0.207508,-0.16159323
http://arxiv.org/pdf/2201.04455v2,SLISEMAP: Supervised dimensionality reduction through local explanations,"We ﬁnd ﬁve clusters
with different local models. To further study these clusters, we plot density plots for the clusters and variables
28                                                                    A. Bjo¨rklund et al. Table 4 Comparing SLISEMAP with and without the escape heuristic. ",cs.LG,A,0.21320617,-0.063723244,0.28007007
http://arxiv.org/pdf/2201.04600v1,Deep Symbolic Regression for Recurrent Sequences,"We checked that this method allows us to approximate any polynomial function. One could in fact use iterative

re nement to predict the Taylor approximation of any function, or use a similar approach to catch multiplicative

corrections, by tting g(x) = f (x)/fˆ(x); we leave these investigations for future work. D Structure of the embeddings

To gain better understanding on the number embeddings of our models, we depict similarity matrices whose
element (i, j) is the cosine similarity between embeddings i and j in Fig. ",cs.LG,C,-0.016400892,0.06550605,0.15464333
http://arxiv.org/pdf/2201.04600v2,Deep Symbolic Regression for Recurrent Sequences,"We checked that this method allows us to approximate any polynomial function. One could in fact use iterative reﬁnement

to predict the Taylor approximation of any function, or use a similar approach to catch multiplicative corrections, by ﬁtting
g(x) = f (x)/fˆ(x); we leave these investigations for future work. F. Structure of the embeddings

To gain better understanding on the number embeddings of our models, we depict similarity matrices whose element (i, j) is
the cosine similarity between embeddings i and j in Fig. ",cs.LG,C,-0.014018176,0.07949142,0.14886057
http://arxiv.org/pdf/2201.04630v1,Generative time series models using Neural ODE in Variational Autoencoders,"This might indicate that for even more com-
tinguish diﬀerent kind of input by sampling from diﬀer-      plex data the NODE VAE will have decreasing perfor-
ent places in the distribution. Interestingly enough, we     mance, which is an interesting hypothesis for the method
see that the spring type 2 data, which is a damped sine      and should be investigated further in future work. The
wave, appears in two separate clusters in the latent space. ",cs.LG,A,0.26365566,0.0307944,0.31385002
http://arxiv.org/pdf/2201.04651v1,Multi-echelon Supply Chains with Uncertain Seasonal Demands and Lead Times Using Deep Reinforcement Learning,"Another advantage would be in real-time
problems, in which the execution time of a previously trained Deep RL model is very fast. In future works, we intend to compare PPO2 with other Deep RL algorithms to verify which
is the most appropriate RL technique for the proposed problem. Another possible path is to
use stochastic programming approaches to solve the NLP model and compare it with those
Deep RL algorithms. ",cs.LG,B,-0.25966024,0.108576015,-0.08613719
http://arxiv.org/pdf/2201.04727v1,Deep clustering with fusion autoencoder,"Experiments performed on sev-
eral image datasets witness the contributions of these maneuvers to the clustering performance and demonstrate the
eﬀectiveness of DCFEA compared with other state-of-the-art DC models. One future work can be trying to replace
the prior distribution of the FAE’s encoder, so that the clustering results could be directly obtained from the latent
variables. Acknowledgements

References

 [1] I. Inuwa-Dutse, M. Liptrott, I. Korkontzelos, A multilevel clustering technique for community detection, Neurocomputing 441 (2021) 64–78. ",cs.LG,C,-0.09051457,-0.16393048,0.1714848
http://arxiv.org/pdf/2201.04727v2,Deep clustering with fusion autoencoder,"Experiments performed on sev-
eral image datasets witness the contributions of these maneuvers to the clustering performance and demonstrate the
eﬀectiveness of DCFEA compared with other state-of-the-art DC models. One future work can be trying to replace
the prior distribution of the FAE’s encoder, so that the clustering results could be directly obtained from the latent
variables. Acknowledgements

    This work is completed when the author is studying for his master’s degree in Southwest Jiaotong University. ",cs.LG,C,-0.0115695475,-0.1015753,0.2275587
http://arxiv.org/pdf/2201.04729v1,Local2Global: A distributed approach for scaling representation learning on graphs,"We have also demonstrated that local2global achieves a
good trade-oﬀ between scalability and accuracy on large-scale data sets using
a variety of embedding techniques and downstream tasks. Our results also sug-
gest that the application of local2global to the task of anomaly detection is
fruitful with the potential for several future work directions. More speciﬁcally, a ﬁrst direction is to further increase the size of node
and edge sets by another order of magnitude, and consider web-scale graphs
with billions of nodes, which many existing algorithms will not be able to han-
dle and for which distributed training is crucial. ",cs.LG,C,-0.15874009,-0.12497244,0.045554698
http://arxiv.org/pdf/2201.04733v1,Adversarially Robust Classification by Conditional Generative Model Inversion,"To  against white-box attacks compared to naturally trained, feed-
this end, we perform classiﬁcation with and without including   forward classiﬁers. In future work, adversarial training can be
logP in the inversion and class selection process. combined with our method to create a defense that is robust
                                                                to both gradient-based and non-gradient-based attacks. ",cs.LG,B,-0.10420416,0.11317314,-0.13415961
http://arxiv.org/pdf/2201.04768v1,On Sampling Collaborative Filtering Datasets,"Despite the already large experimental cost, we strongly believe that
                                                                      the downstream performance of Data-genie could be further im-
                                                                      proved by simply considering more algorithms and diverse datasets. In addition to better sampling, analyzing the fairness aspects of
                                                                      training algorithms on sub-sampled datasets is an interesting re-
                                                                      search direction, which we plan to explore in future work. Acknowledgments

                                                                      This work was partly supported by NSF Award #1750063. ",cs.LG,B,-0.048382856,-0.02378826,-0.107356295
http://arxiv.org/pdf/2201.04805v1,Non-Stationary Representation Learning in Sequential Linear Bandits,"In the Wisconsin Card Sorting Task,
experimental results show that our algorithm considerably outperforms some classic reinforcement
learning algorithms. Directions of future work include nonlinear representation learning, representation-based clustering,
and task-tailored representation generation from experience in bandit and reinforcement learning
problems. Appendix

Lemma 9 (Matrix Bernstein’s inequality [39]) Let X1, X2, . ",cs.LG,B,-0.10544739,0.26821357,-0.31690854
http://arxiv.org/pdf/2201.04805v2,Non-Stationary Representation Learning in Sequential Linear Bandits,"In the Wisconsin Card Sorting Task,
experimental results show that our algorithm considerably outperforms some classic reinforcement
learning algorithms. Directions of future work include nonlinear representation learning, representation-based clustering,
and task-tailored representation generation from experience in bandit and reinforcement learning
problems. Appendix

A Proof of Lemma 1

Proof. ",cs.LG,B,-0.12856708,0.23415595,-0.33786288
http://arxiv.org/pdf/2201.04822v1,A Geometric Approach to $k$-means,"5 Conclusion

We propose a ﬂexible framework for k-means problem by harnessing the geometric structure of local solutions. It provides a theoretical foundation for future work to design detection routines for varying cluster distributions. Future work includes analyzing the Fission-Fission k-means under the more general setting with empirical
success: (1) clusters could be of diﬀerent sizes and shapes; (2) clusters have moderate or heavy overlaps with
each other. ",cs.LG,A,0.27902502,0.058350742,0.13894477
http://arxiv.org/pdf/2201.04895v1,Solving Dynamic Graph Problems with Multi-Attention Deep Reinforcement Learning,"We evaluate our method by
comparing it against several state-of-the-art approaches, both learning-based methods, as well as hand-crafted heuristics,
and against the optimal controllers, and show that our approach substantially outperforms both learning-based and
hand-crafted heuristics and is on par with the optimal controller when solving famous NP-hard problems (TSP and
VRP) in transportation. There are several interesting directions of future work. First, scalability is an open problem in the neural CO domain
and Section 6.3 provides promising results towards this direction and can be investigated further. ",cs.LG,B,-0.22173965,0.3021726,-0.19076328
http://arxiv.org/pdf/2201.04967v2,Adherence Forecasting for Guided Internet-Delivered Cognitive Behavioral Therapy: A Minimally Data-Sensitive Approach,"Thus, while this work showed that it is possible
would have resulted in the patient dropping out. This challenge    to perform automatic adherence forecasting based solely on
will be especially critical when considering how to update the     login/logout timestamps generated from a G-ICBT treatment,
adherence forecasting model over time as data collected while      future works will investigate how these performances vary
the system is active will necessarily be biased with this model-   across primary diagnosis and G-ICBTs. clinician-patient interaction. ",cs.LG,A,0.23880629,-0.17516947,-0.1534682
http://arxiv.org/pdf/2201.04967v3,Adherence Forecasting for Guided Internet-Delivered Cognitive Behavioral Therapy: A Minimally Data-Sensitive Approach,"502–514,
there may exist differences regarding both patients’ adher-               2018.
ence and adherence forecasting accuracy between these three
populations. Thus, while this work showed that it is possible        [4] E. Karyotaki, Y. Smit, K. H. Henningsen, M. Huibers,
to perform automatic adherence forecasting based solely on                J. Robays, D. De Beurs, and P. Cuijpers, “Combining
login/logout timestamps generated from a G-ICBT treatment,                pharmacotherapy and psychotherapy or monotherapy for
future works will investigate how these performances vary                 major depression? a meta-analysis on the long-term ef-
across primary diagnosis and G-ICBTs. ",cs.LG,A,0.30859601,-0.22808999,-0.055987377
http://arxiv.org/pdf/2201.04968v1,On the Design of Graph Embeddings for the Sensorless Estimation of Road Traffic Profiles,"technique in use. On one hand, Roadside Car Data (RCD) is
                                        Above all, this work intends to stimulate further research efforts               obtained by deploying sensors at particular locations of the
                                        towards enhancing the quality of synthetic trafﬁc samples and                    trafﬁc network, such as inductive loops or trafﬁc cameras. thereby, reducing the need for sensing infrastructure. ",cs.LG,A,0.11262171,-0.18761677,0.117645055
http://arxiv.org/pdf/2201.04968v2,A Graph-based Methodology for the Sensorless Estimation of Road Traffic Profiles,"The so-called Points of Interest (POIs) comprise
                                                                  any kind of business, service center or important area that
   This work has elaborated on the problem of estimating road     might inﬂuence the trafﬁc behavior. Consequently, future work
trafﬁc data over a location of a road network without any         will embrace such POI-based information (when available) to
deployed sensor nor prior collected trafﬁc data in the location   produce better road feature embeddings that leverage even
whatsoever. Under these circumstances, it becomes necessary       further the existence of expert knowledge about the factors
to resort to other sources of information to estimate the trafﬁc  affecting the trafﬁc at the target location. ",cs.LG,A,0.15360865,-0.20805937,-0.10066229
http://arxiv.org/pdf/2201.05000v1,Automated Reinforcement Learning: An Overview,"Most of these works are inherently developed for supervised learn-
ing. However, the same motivation and requirements hold for RL which shows
interesting research directions for future works. In [3], the normal gradient descent formula is replaced with a new formula
in which a function of gradient value is used rather than the original gradient
in the updating rule. ",cs.LG,C,-0.20639336,0.09224684,-0.20980045
http://arxiv.org/pdf/2201.05034v1,Criticality-Based Varying Step-Number Algorithm for Reinforcement Learning,"How-
ever, the spread in criticality values was smaller than in the criticality function that
was used for the ﬁrst CVS agent. We therefore speculate that a weighted variance
approach, which takes into account the current policy of the agent (as mentioned
above) might work better than the plain variance approach, and will test that ap-
proach in future work. 8. ",cs.LG,A,0.2931506,0.3392508,-0.044066846
http://arxiv.org/pdf/2201.05079v1,Improved Multi-objective Data Stream Clustering with Time and Memory Optimization,"The experiments show the eﬀectiveness of IMOC-Stream for detecting
arbitrary shaped, compact, and well-separated clusters with better execution
time. Part of our future work should be the proposition of a parallel solution
to minimize the execution time. Further research needs to be conducted on
incorporating the Ant Colony algorithm since it is suited for parallel algo-
rithms due to its independent agents. ",cs.LG,B,0.051164895,0.10333672,0.11682071
http://arxiv.org/pdf/2201.05125v1,GradMax: Growing Neural Networks using Gradient Information,"We studied fully connected layers and convolutional layers, but did not consider the
combination of the two or other architectures such as transformers. 9
Under review

Future work We are looking to address these limitations in future work. Moreover, in this work
we didn’t address the questions as to when and where networks should be grown, although we
outlined ways in which our method could be adapted to do so. ",cs.LG,C,-0.44337067,-0.035604868,0.26410568
http://arxiv.org/pdf/2201.05125v2,GradMax: Growing Neural Networks using Gradient Information,"We studied fully connected layers and convolutional layers, but did not consider the
combination of the two or other architectures such as transformers. Future work We are looking to address these limitations in future work. Moreover, in this work
we didn’t address the questions as to when and where networks should be grown, although we
outlined ways in which our method could be adapted to do so. ",cs.LG,C,-0.43584606,-0.023747377,0.25002262
http://arxiv.org/pdf/2201.05125v3,GradMax: Growing Neural Networks using Gradient Information,"We studied fully connected layers and convolutional layers, but did not consider the
combination of the two or other architectures such as transformers. Future work We are looking to address these limitations in future work. Moreover, in this work
we didn’t address the questions as to when and where networks should be grown, although we
outlined ways in which our method could be adapted to do so. ",cs.LG,C,-0.43584606,-0.023747377,0.25002262
http://arxiv.org/pdf/2201.05242v1,Neural Circuit Architectural Priors for Embodied Control,"We note that we simpliﬁed our design from actual DDPG
                                                                                                                                                                                                    PPO
C. elegans circuits for pedagogical reasons (Section 4.5), and our goal is not to solve this swimming                                                                                               ES

task per se but rather to investigate the advantages of biologically inspirAed naelgtworoitrhkmasrcpheitreacrtcuhreitemcoturree                                                                               B
generally. C. elegans circuits are not optimized for fast swimming with few segments (Section 4.4);

future work may propose architectures better for this speciﬁc task, e.g. using larval zebraﬁsh circuits. ",cs.LG,B,-0.06350266,0.06735056,0.04895716
http://arxiv.org/pdf/2201.05242v2,Neural Circuit Architectural Priors for Embodied Control,"Such higher-level circuits could inspire NCAP architectures for additional modalities. Ultimately, our work suggests a way of advancing AI and robotics research inspired by systems
neuroscience and encourages future work in more complex embodied control. Acknowledgments and Disclosure of Funding

Thanks to Sergey Shuvaev, Pavel Tolmachev, Liam McCarty, Polina Kirichenko, Pavel Izmailov,
Daniel Barabasi, Christopher Langdon, and Josh Merel for insightful discussions and manuscript
feedback. ",cs.LG,B,-0.14920333,0.038528092,-0.1497896
http://arxiv.org/pdf/2201.05286v1,Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning,"Besides, the
Synchronization Intervals (SIs) of most experiments are shorter than 30 minutes, which means SL nodes would
waste redundant time in waiting for the assignment of a new license. In future work, we will try to cooperate
with HPE to measure the connectivity of SL and improve the robustness of SL in face of network disconnection. Differences between the results of SL and FL. ",cs.LG,A,0.14694974,0.03776128,0.3071389
http://arxiv.org/pdf/2201.05286v2,Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning,"Besides, the Synchronization Intervals (SIs) of most experiments are shorter
than 30 minutes, which means SL nodes would waste redundant time in waiting for the assignment of a new
license. In future work, we will try to cooperate with HPE to measure the connectivity of SL and improve the
robustness of SL in face of network disconnection. Differences between the results of SL and FL. ",cs.LG,A,0.14694974,0.03776128,0.3071389
http://arxiv.org/pdf/2201.05293v1,Structure Enhanced Graph Neural Networks for Link Prediction,"Clustering and preferential attachment in growing
we need to consider a larger range of global structural features, this                                                                  networks. Physical review E 64, 2 (2001), 025102.
is left to our future work. [18] Sankar K Pal and Sushmita Mitra. ",cs.LG,C,0.062065214,-0.24807651,0.16242331
http://arxiv.org/pdf/2201.05336v1,IDEA: Interpretable Dynamic Ensemble Architecture for Time Series Prediction,"We also
showed that IDEA has a good generalization and robustness in the face of suddenly changing input data distributions. In future work, we would like to encourage orthogonal queries in the recurrent input competition which may help base
learners to learn disentangled patterns better. References

 [1] Philip Hans Franses, Dick Van Dijk, et al. ",cs.LG,C,-0.08271961,0.068441086,-0.1852412
http://arxiv.org/pdf/2201.05349v2,Training Free Graph Neural Networks for Graph Matching,"However, once being trained, fully trained

model performs much faster in the inference phase. We make it a future work to ﬁnd a more efﬁcient training-free

method for the supervised setting. 16
                                 Training Free Graph Neural Networks for Graph Matching    A PREPRINT

C Experiments

C.1 Additional Results

BasicTFGM on PascalVOC. ",cs.LG,C,-0.32826537,-0.10926038,0.16825117
http://arxiv.org/pdf/2201.05405v1,The Implicit Regularization of Momentum Gradient Descent with Early Stopping,"The general           characterized the close connections between MGF and ridge. In theoretical aspect, we proved that the risk of MGF is no
case will be considered in our future works. more than 1.54 times that of ridge under the calibration t =

6 Numerical Examples                                                      2/λ. ",cs.LG,A,0.3003192,0.13046402,0.2634321
http://arxiv.org/pdf/2201.05433v1,Comparing Model-free and Model-based Algorithms for Offline Reinforcement Learning,"The newly acquired data is then
used to augment the original dataset, and the model-free policy                 −v˜t−5 − 0.91
training starts again. The regularizing factor in this algorithm is
called unknown state detector (USD) - it prevents the rollouts         πopt =     2f˜t−3 − p˜ + 1.43
from entering areas in the state space that are too unknown to
the transition models:                                                         −3.48h˜t−3 − ˜ht−4 + 2p˜ + 0.81

          USD(s, a) = max[fi(s, a) − fj(s, a)]2 < ǫ (17)               We make the datasets publicly available for future work under
                                                                       https://github.com/siemens/industrialben
                                            i,j                        chmark/tree/offline datasets/datasets

Model-based Ofﬂine Policy Optimization (MOPO) trains an                5.2 Evaluation
ensemble of stochastic transition models with Gaussian outputs
from the initial datasets. The ensemble members are then used          During policy training with the ofﬂine RL algorithms, we reg-
to augment collected rewards, by subtracting a penalty term            ularly evaluate the current policy candidate on the true bench-
based on the estimated uncertainty:                                    mark, and calculate a performance by averaging over ten roll-
                                                                       outs. ",cs.LG,B,-0.09756942,0.20796797,-0.11240139
http://arxiv.org/pdf/2201.05527v1,Decentralized Robot Learning for Personalization and Privacy,"mains. Based on our promising evaluation of Elastic Trans-
                                                                fer, future work can extend this approach to new machine
   In experiment one, we observe that FL-CL approaches out-     learning domains, and evaluate it thoroughly on a series of
perform Local-CL methods. Fig. ",cs.LG,B,-0.11514166,0.12144637,0.049752034
http://arxiv.org/pdf/2201.05530v1,Collaborative learning of images and geometrics for predicting isocitrate dehydrogenase status of glioma,"787–798, 2021.
models. In our future work, we will systematically screen
different combinations of deeper CNN and GNN models to            [6] Pedano, N., Flanders, A. E., Scarpace, L., Mikkelsen, T.,
enhance the prediction performance further. Extra ablation             Eschbacher, J. M., Hermes, B., . ",cs.LG,C,-0.34136903,-0.24817824,0.0979612
http://arxiv.org/pdf/2201.05587v1,Reusing Auto-Schedules for Efficient DNN Compilation,"Au-           tation of tuning-reuse, and evaluated the performance on 9
toTVM [11] takes hand-engineered schedules for operations,           models, with a maximum performance speedup of between
and explores parameter tunings across a space deﬁned by the          1.16× and 4.76×, while clearly outperforming Ansor when
schedule author, such as tiling sizes, unrolling factors, and        given limited search time. For future work, we will explore
others. Choices to efﬁciently explore these potentially large        how tuning-reuse can be used to bootstrap auto-scheduling,
spaces vary, with AutoTVM including approaches such as gra-          as well as investigate its relevance to NAS. ",cs.LG,B,-0.045971066,0.09686481,0.016396882
http://arxiv.org/pdf/2201.05587v2,Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation,"with Ansor requiring over 10.8× as much time to match our perfor-
                                                                                      mance. For future work, we will explore the impact of across-kernel
   Regarding auto-scheduling, FlexTensor [44] is an auto-scheduling                   interactions, the viability of applying transfer-tuning across similar
system similar to Ansor, although it relies on more hand written                      kernel classes, and transfer-tuning across hardware devices. templates, thus seeing worse performance on some benchmarks. ",cs.LG,B,-0.0024313778,0.09081157,0.06589796
http://arxiv.org/pdf/2201.05596v1,DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,"As shown
in Figure 10, we observe that both DeepSpeed and PyTorch reduce the inference latency as
we increase the number of GPUs, as expected; although PyTorch is much slower compared to
DeepSpeed and only scales up to 32 GPUs. 21
The throughput trends, on the other hand, are interesting and merit further analysis. 52 Billion (1.3B+MoE-128)
Latency (ms)
                                                                                                                       Throughput (#tokens-per-second) per GPU70                  700
60                   16 GPUs                   32 GPUs                                                                                                                            600
50                                                                                                                                                                                500
40                                                                                                                                                                                400
30                                                                                                                                                                                300
20                                                                                                                                                                                200
10                                                                                                                                                                                100
                                                                                                                                                                                  0
 0                                                                                                                                                            64 GPUs
             8 GPUs

                     Latency (PyTorch)         Latency (DeepSpeed)
                     Throughput (PyTorch)      Throughput (DeepSpeed)

Figure 10: Latency and throughput improvement oﬀered by DeepSpeed-MoE inference sys-
tem (Optimized) over PyTorch (Baseline) for a 52-Billion MoE model with 128 experts, using
between 8 to 64 GPUs. ",cs.LG,B,-0.2252069,-0.0043719127,0.20780247
http://arxiv.org/pdf/2201.05596v2,DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,"As shown
in Figure 10, we observe that both DeepSpeed and PyTorch reduce the inference latency as
we increase the number of GPUs, as expected; although PyTorch is much slower compared to
DeepSpeed and only scales up to 32 GPUs. 21
The throughput trends, on the other hand, are interesting and merit further analysis. 52 Billion (1.3B+MoE-128)
Latency (ms)
                                                                                                                       Throughput (#tokens-per-second) per GPU70                  700
60                   16 GPUs                   32 GPUs                                                                                                                            600
50                                                                                                                                                                                500
40                                                                                                                                                                                400
30                                                                                                                                                                                300
20                                                                                                                                                                                200
10                                                                                                                                                                                100
                                                                                                                                                                                  0
 0                                                                                                                                                            64 GPUs
             8 GPUs

                     Latency (PyTorch)         Latency (DeepSpeed)
                     Throughput (PyTorch)      Throughput (DeepSpeed)

Figure 10: Latency and throughput improvement oﬀered by DeepSpeed-MoE inference sys-
tem (Optimized) over PyTorch (Baseline) for a 52-Billion MoE model with 128 experts, using
between 8 to 64 GPUs. ",cs.LG,B,-0.2252069,-0.0043719127,0.20780247
http://arxiv.org/pdf/2201.05624v4,Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next,"Belkin et al (2019),
demonstrate the existence of a double-descent risk curve across a wide range of
models and datasets, and they oﬀer a mechanism for its genesis. The behavior
of PINN in such a framework of Learning Theory for Deep Learning remains
to be investigated and could lead to further research questions. In particular,
the function class H of the hypothesis space in which PINN is optimized might
be further examined by specifying such space based on the type of diﬀerential
equations that it is solving and thus taking into account the physics informed
portion of the network. ",cs.LG,C,-0.22359926,0.063545935,-0.107881516
http://arxiv.org/pdf/2201.05634v1,Imputing Missing Observations with Time Sliced Synthetic Minority Oversampling Technique,"Once the full trajectory of each sample has been imputed, we train
an Encoder-Decoder LSTM model [23, 6] to predict trajectories of new samples and classify their endpoints using
logistic regression. We conclude with a discussion as well as ideas for future work. Results

As stated in the introduction our algorithm leverages the well known SMOTE algorithm [4] developed for class
imbalance problems to generate new samples in each time slice. ",cs.LG,A,-0.024806675,-0.09606541,-0.14595091
http://arxiv.org/pdf/2201.05647v1,Tools and Practices for Responsible AI Engineering,"By
exercise of tautology, the rAI-toolbox leverages a style            keeping the core components in the rAI-toolbox strictly
of testing known as property-based testing (MacIver 2019),          aligned with standard PyTorch APIs, the toolbox should
which empowers us to verify that the critical implementation        remain interoperable with new developments in the Py-
details of our tooling are reliable. Torch ecosystem (e.g., related to distributed computing), as
                                                                    well as provide a ﬂexible foundation for additional research
   A property-based test (PBT) is designed to check that            and development of responsible AI capabilities. Some near-
an expected property of a function holds true under an ex-          term directions for rAI-toolbox development include
ceptionally diverse set of inputs to the function. ",cs.LG,B,-0.0005765038,-0.0183673,-0.058502052
http://arxiv.org/pdf/2201.05666v1,Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions,"In this work, we adopt the widely used graphical Lasso (GLasso) [8]
method based on the block coordinate descent algorithm for estimating supp(Θ). Other efﬁcient
estimation method, e.g., QUIC [14], can also be adopted, which is treated as a future work. We provide empirical studies in Appendix E.1, in which faithfulness is exactly violated but not SSCF,
to demonstrate that GLasso ﬁnds the true super-structure in these cases, whereas MMPC fails. ",cs.LG,A,0.07594286,0.11051006,0.15984717
http://arxiv.org/pdf/2201.05697v1,An efficient aggregation method for the symbolic representation of temporal data,"For ABBA
            and fABBA the digitization parameter α = 0.1 has been used. on average, the avoidance of the k-means algorithm, and the removal of the need to recompute
clusterings for diﬀerent values of k.

   In future work we aim to apply fABBA to other data mining tasks on time series like anomaly
detection and motif discovery. In the context of time series forecasting, a combination of ABBA and
LSTM has recently been demonstrated to reduce sensitivity to the LSTM hyperparameters and the
initialization of random weights [16], and we believe that fABBA could simply replace ABBA in this
context. ",cs.LG,C,-0.00041423365,-0.095172614,0.032740973
http://arxiv.org/pdf/2201.05720v1,Deep Reinforcement Learning for Shared Autonomous Vehicles (SAV) Fleet Management,"The following criteria is
                                                                                     used to select reinforcement learning algorithms:

                                                                                          • Works with large state action spaces. Although the simula-
                                                                                             tor’s settings are using only 21 zones (for a 21*3 state space),

                                                                                  7
        we want to investigate algorithms that will support large
        state spaces for future work. • Model-free learning algorithms are preferred as they do not
        require extra step to create a model. ",cs.LG,B,-0.12932944,0.29216862,-0.22158507
http://arxiv.org/pdf/2201.05752v1,Moses: Efficient Exploitation of Cross-device Transferable Features for Tensor Program Optimization,"Besides, we generate
                                                                     a large-scale program performance dataset on two embedded
                                                                     GPUs for learning based DNN compilers. Our future work in-
                                                                     cludes extending Moses to support knowledge transfer from
                                                                     the cross-subgraph tensor optimization perspective. References                                                       Mendis, Sudip Roy, Amit Sabne, and Mike Burrows. ",cs.LG,C,-0.38408774,-0.0056544077,0.044565074
http://arxiv.org/pdf/2201.05760v1,Big Data Application for Network Level Travel Time Prediction,"Kalman Filter method is a linear
quadratic estimation algorithm, which is one of the best prediction methods with high
precision and flexibility. However, as a linear estimation model, when the prediction
interval becomes less than 5 minutes, whether the model can maintain a strong
performance is worth further study. Regression models measure various factors affecting
travel time. ",cs.LG,A,0.1361091,0.058264107,-0.101342335
http://arxiv.org/pdf/2201.05799v1,Hyperplane bounds for neural feature mappings,"We have considered well known networks for the experiments, based on convolution
neural networks, it might be interesting to explore additional network conﬁgurations to
understand the impact of the network architecture with the proposed approach. There has
been as well recent work to better understand the optimisation and behaviour of neural
networks that we would like to explore as future work. 12
                           Hyperplane bounds for neural feature mappings

7. ",cs.LG,C,-0.38657045,-0.07935212,0.14742838
http://arxiv.org/pdf/2201.05918v1,Recursive Least Squares Advantage Actor-Critic Algorithms,"efﬁciency than RMSA2C on most games or tasks, and have
                                                                  higher computational efﬁciency than PPO and ACKTR. In
   All tested algorithms also use two disjoint FNNs deﬁned        future work, we will try to establish the convergence of our
in [15]. One is the critic network, the other is the actor        both algorithms and improve the stability of RLSNA2C. ",cs.LG,B_centroid,-0.06694512,0.36860213,-0.027245898
http://arxiv.org/pdf/2201.05918v2,Recursive Least Squares Advantage Actor-Critic Algorithms,"efﬁciency than RMSA2C on most games or tasks, and have
                                                                  higher computational efﬁciency than PPO and ACKTR. In
   All tested algorithms also use two disjoint FNNs deﬁned        future work, we will try to establish the convergence of our
in [15]. One is the critic network, the other is the actor        both algorithms and improve the stability of RLSNA2C. ",cs.LG,B,-0.06694512,0.36860213,-0.027245898
http://arxiv.org/pdf/2201.05962v1,Enhancement of Healthcare Data Performance Metrics using Neural Network Machine Learning Algorithms,"Scenario 6 has a better performance in connection with
                                                                  lower MSE, higher R value, relatively lower MAPE,
                                                                  higher efficiency, and accuracy compared to the other
                                                                  6 training scenarios. Hence, scenario 6 was selected for
                                                                  further analysis in TABLE V.

                                                                     TABLE IV. BAYESIAN REGULARIZATION TESTING DATA
                                                                                            RESULTS

                                                                  Training       R  MA MAPE Accurac Efficienc
                                                                  scenario MS

                                                                  s  E              E  y                           y

                                                                  Scenario 0.20  03.997 1.44 2%0.67 79.33% 1.11
                                                                  1

                                                                  Scenario 0.32  0.996 1.49 21.35 78.65% 1.24
                                                                  2              5
Training

scenario MS         R    MA MAPE Accurac Efficienc

s           E            E              y   y

Scenario          0.997       20.53 79.47%
            0.28 3       1.43 %             1.42
3

S4cenario 0.30 00.997 1.44 2%0.64 79.36% 1.66

S5cenario 0.25 03.997 1.42 2%0.42 79.57% 2

S6cenario 0.23 05.997 1.45 2%0.87 79.13% 2.5

S7cenario 0.29 01.997 1.45 2%0.90 79.1% 3.33

    In scenario 6 in TABLE V, Bayesian                               Fig. ",cs.LG,A,0.20012951,-0.030561475,0.051888548
http://arxiv.org/pdf/2201.06095v1,Doing More with Less: Overcoming Data Scarcity for POI Recommendation via Cross-Region Transfer,"Experiments
over diverse mobility datasets revealed that Axolotl is able to significantly improve over the state-
of-the-art baselines for POI recommendation in limited data regions and even performs considerably
better across datasets and data-rich source regions. As a future work for this paper, we plan to
modify the transfer procedure of Axolotl to incorporate novel meta-learning approaches, namely
ProtoMAML [56] and Reptile [48]. Additionally, we plan to experiment with approaches [8, 35, 73]
that automatically identify the number of clusters within a region while simultaneously maintaining
the scalability of our model. ",cs.LG,C,-0.08587335,-0.15122214,-0.16395344
http://arxiv.org/pdf/2201.06118v1,DeepCreativity: Measuring Creativity with Deep Learning Techniques,"An      [32]) with quality (through perplexity); while the au-
evaluation of DeepCreativity is presented in Section 4,      thors of [50] uses BLEU only. However, the deﬁnition
considering a case study of text generation in the con-      of value as the weighted sum of sub-components has
text of 19th century American poetry; ﬁnally, we dis-        the limitation of requiring the correct identiﬁcation of
cuss limitations and potential future work in Section        all the relevant factors and their relative weights, which
5.                                                           is a complex and time-consuming task. 2. ",cs.LG,A,0.0908558,-0.14638796,-0.1095204
http://arxiv.org/pdf/2201.06147v1,Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks,"However, it is worth noting that many of these GAN improve-
ments have been proposed for image generation. Therefore, there is a necessity
for further research exploring the usefulness of the improvements in time series
generation. 3.2 On-Demand Anomalies

One of the signiﬁcant advantages of using GAN over other algorithms in the
literature to synthetically augment the data is that it gives us more control
over the generation through the latent space. ",cs.LG,C,-0.105647326,-0.115109116,0.085023835
http://arxiv.org/pdf/2201.06147v2,Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks,"However, it should be
noted that many of these enhancements have been proposed for image gener-
ation. Therefore, further research is needed to investigate their usefulness for
time series generation. To further this purpose, some improvements mentioned
above will be analyzed in Section 5, in the hyperparameter tuning phase. ",cs.LG,A,0.06026859,-0.10568191,0.29757613
http://arxiv.org/pdf/2201.06205v1,Balancing Performance and Energy Consumption of Bagging Ensembles for the Classification of Data Streams in Edge Computing,"Despite of the trade-offs observed, it is possible to balance them to avoid signiﬁcant
loss in predictive performance. In future work, we intend to investigate if it is possible to improve the solution by using an adaptive mini-batching size
regarding predictive performance, throughput, delay, and energy consumption. 15
arXiv Template  A PREPRINT

                                                Figure 9: Throughput for the i5-2400

8 Acknowledgements

This study was ﬁnanced in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil
(CAPES) - Finance Code 001, and Programa Institucional de Internacionalização – CAPES-PrInt UFSCar (Contract
88887.373234/2019-00). ",cs.LG,B,0.02495084,0.122522525,0.064145096
http://arxiv.org/pdf/2201.06298v2,Parameterized Convex Universal Approximators for Decision-Making Problems,"Finally, Section V concludes this study with a       (l.s.c.) at x0 if for every > 0, there exists a neighborhood U
summary and future works. of x0 such that f (x) ≥ f (x0)− for all x ∈ U where f (x0) <
                                                                   ∞, and f (x) tends to +∞ as x → x0 when f (x0) = +∞. ",cs.LG,A,0.24098629,-0.029253885,-0.00023304299
http://arxiv.org/pdf/2201.06321v1,Landscape of Neural Architecture Search across sensors: how much do they differ ?,"CONCLUSION                              cently enough. In this study, we investigate the impact of the choice of an in-    As future work, we propose to investigate how to use the gained
put sensor on the performance of a neural architecture search       insights to help build speed-up techniques for NAS strategies. strategy. ",cs.LG,B,-0.13790154,-0.0686508,-0.10170043
http://arxiv.org/pdf/2201.06321v2,Landscape of Neural Architecture Search across sensors: how much do they differ ?,"CONCLUSION                              cently enough. In this study, we investigate the impact of the choice of an in-    As future work, we propose to investigate how to use the gained
put sensor on the performance of a neural architecture search       insights to help build speed-up techniques for NAS strategies. strategy. ",cs.LG,B,-0.13790154,-0.0686508,-0.10170043
http://arxiv.org/pdf/2201.06344v1,ExpertNet: A Symbiosis of Classification and Clustering,"The best      to that of other deep clustering methods and may be improved
performance is achieved when the approach is used both in        further. This can be explored in future work along with ways
training and prediction (TT). For clustering, there is no pre-   to combine hierarchical clustering algorithms with supervised
diction. ",cs.LG,C,-0.28636837,-0.22400537,0.074022986
http://arxiv.org/pdf/2201.06444v1,Black-box error diagnosis in deep neural networks: a survey of tools,"based on the categorization of errors in four classes: boundary,
The authors analyze four state-of-the-art object detection and     other class with the same object, other class with the same
posture estimation models to uncover ﬂaws and recommend            verb, other class with neither and no class. A further analysis
improvements. evaluates the models w.r.t. ",cs.LG,C,-0.06695473,-0.17267036,-0.18307629
http://arxiv.org/pdf/2201.06507v1,Distillation from heterogeneous unlabeled collections,"As it turns out, 1C-Sum is better
suited to the task. This paper opens some avenues for future works. Firstly, the way the biasing mechanism is controlled
     Appendix A.3 illustrates that a single collection can   could be improved. ",cs.LG,A,0.17920625,0.15544584,0.06083609
http://arxiv.org/pdf/2201.06599v1,Who supervises the supervisor? Model monitoring in production using deep feature embeddings with applications to workpiece inspection,"Research progress
formed on two datasets related to automated, visual quality                    of automated visual surface defect detection for industrial metal planar
inspection. In future work, we would like to extend experi-                    materials. Sensors (Basel, Switzerland) 20.
mental testing to a wider variety of data types and classiﬁer               Feng, X., Gao, X., Luo, L., 2021. ",cs.LG,C,-0.0013584383,-0.25564343,0.042524707
http://arxiv.org/pdf/2201.06610v1,A Brief Survey of Machine Learning Methods for Emotion Prediction using Physiological Data,"These are all potential issues that
is slight not much better than random (50%). For the remaining   can be addressed in future works with solution in the direction
methods, the metric is MAE and its hard to infer whether its     of non-uniform and more intelligent sampling. a large error or small. ",cs.LG,A,0.30852154,-0.00047573633,0.034487676
http://arxiv.org/pdf/2201.06640v2,Evaluating Inexact Unlearning Requires Revisiting Forgetting,"on samples from the deletion set Df and unseen samples Dt                      However, in the case of IC test, we use the mislabelled class
from the same underlying distribution rather than training                     as the target for both, Df and Du samples. Intuitively, the
                                                                               memorization of mislabels in the deletion set would make

                                                                                   5Given that only 1 parameter (threshold) needs to be learnt, the shadow
                                                                               size is sufﬁcient

                                                                           13
the wrong class probability output unnaturally higher than           the relationship between forgetting and learning forms an
other unseen samples of the same class, making the MIA               intriguing direction for future work. stronger. ",cs.LG,A,0.119372696,-0.051036846,-0.11694336
http://arxiv.org/pdf/2201.06653v1,Data-Centric Machine Learning in the Legal Domain,"We iden-
tify potential reasons for these variations, and discuss how the experiments might be
helpful for future research carried out in AI & Law. 9
     In future work, we plan to expand the experiments to additional data sets. The ex-
periments might also be modiﬁed, such as performing the sample-size experiment on a
sentence-level rather than a document-level, or introducing systematic errors based on
the semantic confusion between classes (similar to Figure 9). ",cs.LG,A,0.16809344,-0.24474594,-0.1737416
http://arxiv.org/pdf/2201.06656v1,Generalization in Supervised Learning Through Riemannian Contraction,"In particular (12) implies that the generalization error can ‘overshoot’ by a factor
of Lχ, which gives room for the generalization to increase transiently from its initial value
before it eventually decreases. This will be explored in future work. We conclude with some speculations on how the above results relate to biology, speciﬁcally
neuroscience. ",cs.LG,A,0.19663188,0.11670421,-0.0055809133
http://arxiv.org/pdf/2201.06656v2,Generalization in Supervised Learning Through Riemannian Contraction,"In particular (12) implies that the generalization error can overshoot by a factor of Lχ,
which gives room for the generalization to increase transiently from its initial value before it
eventually decreases. This will be explored in future work. We conclude with some speculations on how the above results relate to biology, speciﬁcally
neuroscience. ",cs.LG,A,0.19289137,0.12953696,0.011462353
http://arxiv.org/pdf/2201.06769v1,DEFER: Distributed Edge Inference for Deep Neural Networks,"(ICDCS), IEEE, 2017. In future work on DEFER, we plan to explore how to                           [7] Z. Zhao, K. M. Barijough, and A. Gerstlauer, “DeepThings: Distributed
optimize model partition size and architecture based on the                          adaptive deep learning inference on resource-constrained iot edge
compute and memory constraints of the edge device. In a                              clusters,” IEEE Transactions on Computer-Aided Design of Integrated
situation without homogeneous compute nodes, heterogeneous                           Circuits and Systems, vol. ",cs.LG,B,-0.35279065,0.090528354,0.09781463
http://arxiv.org/pdf/2201.06814v1,Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling,"of M2M over state-of-the-art methods, especially in new and mi-
                                                                                                               nor marketing scenarios. For future work, we’d like to incorporate
5.4 Online A/B Test                                                                                            more types of tasks like classifier beyond regression on top of the
                                                                                                               prediction network. Besides, it might also be interesting to combine
The proposed M2M method for advertisers modeling builds the                                                    our M2M model with optimization-based meta learning approaches
foundation for many downstream applications such as Rewarded                                                   like MAML, Reptile etc. ",cs.LG,B,-0.042934902,0.0247664,-0.24505538
http://arxiv.org/pdf/2201.06814v2,Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling,"of M2M over state-of-the-art methods, especially in new and mi-
                                                                                                               nor marketing scenarios. For future work, we’d like to incorporate
5.4 Online A/B Test                                                                                            more types of tasks like classifier beyond regression on top of the
                                                                                                               prediction network. Besides, it might also be interesting to combine
The proposed M2M method for advertisers modeling builds the                                                    our M2M model with optimization-based meta learning approaches
foundation for many downstream applications such as Rewarded                                                   like MAML, Reptile etc. ",cs.LG,B,-0.042934906,0.024766374,-0.24505535
http://arxiv.org/pdf/2201.06834v1,Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale,"We provide empiri-
putation resources due to the synchronization barrier and are often         cal evaluations for hyper-parameter tuning problems in Section 5
sensitive to stragglers (See Figure 1). ASHA [40] is able to remove         and end this with the conclusion and future work in Section 6.
these issues associated with synchronous promotions by incur-
ring a number of inaccurate promotions, while this asynchronous             2 RELATED WORK
promotion could hamper the sample efficiency when utilizing the
parallel and distributed resources. Thus, we need to explore an effi-       Bayesian optimization (BO) has been successfully applied to hyper-
cient asynchronous mechanism which pursues both sample efficiency           parameter tuning [7, 23, 25, 59, 71]. ",cs.LG,B,0.09251561,0.25344777,0.0057559283
http://arxiv.org/pdf/2201.06835v1,Ray Based Distributed Autonomous Vehicle Research Platform,"15 arXiv:2201.06835v1 [cs.LG] 18 Jan 2022  RAY BASED DISTRIBUTED AUTONOMOUS VEHICLE RESEARCH
                                                                          PLATFORM

                                                                                                  TECHNICAL REPORT

                                                                                                         Derek Xu
                                                                                            SAAS Research & Publications
                                                                                          University of California, Berkeley

                                                                                               Berkeley, California, USA
                                                                                              xzrderek@berkeley.edu

                                                                                            ABSTRACT

                                                   My project tackles the question of whether Ray can be used to quickly train autonomous vehicles
                                                   using a simulator (Carla), and whether a platform robust enough for further research purposes can
                                                   be built around it. Ray is an open-source framework that enables distributed machine learning
                                                   applications. ",cs.LG,B,-0.18027043,0.121809065,-0.18335243
http://arxiv.org/pdf/2201.06872v1,Deep Graph Convolutional Network and LSTM based approach for predicting drug-target binding affinity,"If the predicted value    searchers may select drugs to conduct further experiments to
(p) closely resembles the measured value (m), a model may      understand their usage as prospective drugs to treat SARS-
be considered to be good and therefore, the output values      CoV-2 patients. As a future work, the receptor based drugs
should be close to the red line (p = m) as shown in the        with high binding aﬃnity may be tested for their interac-
ﬁgures. From Fig. ",cs.LG,A,0.28203237,-0.14319173,-0.031162947
http://arxiv.org/pdf/2201.06880v1,Temperature Field Inversion of Heat-Source Systems via Physics-Informed Neural Networks,"Besides, the CMCN-PSO method can ﬁnd
optimal positions to alleviate the effect of noises under different noise levels and numbers of observations, making the
PINN-TFI method to reconstruct a more robust temperature ﬁeld. In future work, reducing the number of observations
while maintaining reconstructed performances is an interesting research point. Declaration of competing interest

The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have
appeared to inﬂuence the work reported in this paper. ",cs.LG,A,0.14677745,0.046200935,0.21680409
http://arxiv.org/pdf/2201.06910v1,"ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization","How to       gap between small and large models becomes less
further improve zero-shot generalization by opti-      signiﬁcant when having more training tasks. As a
mizing task distribution is left to future work. result, task scaling can substantially improve train-
                                                       ing and serving efﬁciency. ",cs.LG,B,-0.15401846,0.07391973,-0.15288436
http://arxiv.org/pdf/2201.06910v2,"ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization","Note that task distribution
validation set performance of 0.96 and 0.10 for        is orthogonal to scaling the task number. How to
NEWS and production tasks respectively, and im-        further improve zero-shot generalization by opti-
proves the test zero-shot performance of 3.90 and      mizing task distribution is left to future work. 1.23. ",cs.LG,A,0.053388227,0.028144429,-0.030799095
http://arxiv.org/pdf/2201.06938v1,Neuron-Specific Dropout: A Deterministic Regularization Technique to Prevent Neural Networks from Overfitting & Reduce Dependence on Large Training Samples,"While sorting algorithms have gotten faster, and more optimizations could be done
to NSDropout, they still take up the majority of the time during processing. For now
NSDropout just “drops” a unit it ﬁnds a network to be too reliant on, but future work could
look into how adjusting the unit instead of dropping it could improve the performance in a
wider array of applications. Appendix A. ",cs.LG,B,-0.05379172,0.025234591,0.2129394
http://arxiv.org/pdf/2201.06990v1,Knock Detection in Combustion Engine Time Series Using a Theory-Guided 1D Convolutional Neural Network Approach,"cycle and a manageable model size, the method is furthermore
                                                                                   capable of real-time classiﬁcation and incorporation into test
                             TABLE XI                                              bench operation. MODEL D PERFORMANCE ON SMALL DATASETS                                              The authors’ future work will concentrate on expanding the
                                                                                   proposed knock detection approach to construct a condition
               *ONLY NON-KNOCKING CYCLES                                           forecasting model, effectively pursuing the prediction of knock
                                                                                   occurrences in upcoming combustion cycles. Training cycles per subset  Train acc. ",cs.LG,A,0.042022318,-0.0785552,0.051105935
http://arxiv.org/pdf/2201.07000v1,TCR-GAN: Predicting tropical cyclone passive microwave rainfall using infrared imagery via generative adversarial networks,"Since the inherent limitations of            opments in, p. 137, 2016. IR instruments also lead to inaccurate estimates, future work
could use multi-channel data for forecasting, such as the water    [11] P. Isola, J.-Y. Zhu, T. Zhou, and A. ",cs.LG,A,0.2120355,-0.1937521,0.05861811
http://arxiv.org/pdf/2201.07006v3,Time Series Generation with Masked Autoencoder,"To our surprise, ExtraMAE infers complex original signals even under an extreme mask ratio near 92%. We hope this
property will inspire future work. Time series generation may now usher in the era of supervised learning. ",cs.LG,A,0.02253468,-0.11710009,0.025100917
http://arxiv.org/pdf/2201.07013v1,Deep Cervix Model Development from Heterogeneous and Partially Labeled Image Datasets,"The cervical image
CSSL (N 32)           85.14  0.7959    0.8043     0.7800   81.31  0.6471     0.7818     0.7040   datasets for which the labeling efforts made by researchers
CSFSSL                84.46  0.6735    0.7647     0.7416   79.80  0.6912     0.7719     0.7015   at NCI are in progress will be used for this research. The
PPFSSL NHS            84.46  0.7347    0.8250     0.7579   80.81  0.6618     0.7121     0.7031   engineering implementation of federated learning to work in a
PPFSSL ALTS           84.46  0.7755    0.7826     0.7677   80.30  0.6618     0.7500     0.6977   real scenario is another important future work. The presented
                                       0.7600                                0.7377              idea can be employed by other medical image analysis tasks
                                                                                                 for utilizing unlabeled data and providing the same domain
   The Receiver Operating Curves (ROC) for all supervised                                        transfer learning. ",cs.LG,C,-0.077729166,-0.20091552,0.0056510745
http://arxiv.org/pdf/2201.07026v1,Socioeconomic disparities and COVID-19: the causal connections,"That being said, one can argue that variables like the average level of education
in a county are highly correlated with several of the socioeconomic metrics that we consider and maybe it can replace one of the
metrics that we consider here. However, since we test the causal connections between the variables under a certain hypothesis
represented by the directed acyclic graphs, studying the details of all possible hypothesis is beyond the scope of our work and
we would like to motivate future works to consider a different set of variable to understand better, what the true causations in
the real-world are. Additional causal orderings

We have considered three causal orderings in the main text. ",cs.LG,A,0.38441306,-0.20948556,-0.10808213
http://arxiv.org/pdf/2201.07052v1,Differentially Private Reinforcement Learning with Linear Function Approximation,"Our established results build on a ﬂexible procedure that also
allows us to design general private RL algorithms with different privacy schemes under JDP and even under
LDP guarantees by injecting Gaussian noise directly to each user’s data. As discussed before, our work also
opens the door to a series of interesting future works. References

Lihong Li, Wei Chu, John Langford, and Robert E Schapire. ",cs.LG,A,0.21500406,0.15296814,0.06092417
http://arxiv.org/pdf/2201.07052v2,Differentially Private Reinforcement Learning with Linear Function Approximation,"Our established results build on a ﬂexible procedure that also
allows us to design general private RL algorithms with different privacy schemes under JDP and even under
LDP guarantees by injecting Gaussian noise directly to each user’s data. As discussed before, our work also
opens the door to a series of interesting future works. 10 Acknowledgement

The author would like to thank Sayak Ray Chowdhury for insightful discussions and Quanquan Gu for the
discussion on the concurrent work. ",cs.LG,A,0.22593318,0.16099875,0.07760054
http://arxiv.org/pdf/2201.07207v1,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"3Note that this is a different LM than the GPT-style Planning LM. Using a single LM for both purposes could
as well be possible and likely more efﬁcient, but we leave such investigation to future works. 7
   Task: Complete Amazon  Walk to Home Office  Sit on Chair  Switch on Computer  Look at Computer
Turk Surveys

Task: Get Glass of Milk   Walk to Kitchen      Open Fridge   Grab Milk           Close Fridge

Figure 3: Visualization of VirtualHome programs generated by our approach. ",cs.LG,B,0.022672392,-0.026697362,-0.2216441
http://arxiv.org/pdf/2201.07207v2,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"Prior work also adopt similar combination of metrics [38]. We report two
metrics individually to shine light on the deﬁciencies of existing LLMs which we hope could provide
insights for future works. To provide a holistic view, we report results by combining two metrics in
Section 5.2. ",cs.LG,A,0.21063772,-0.09234977,0.00062961364
http://arxiv.org/pdf/2201.07306v1,Bregman Deviations of Generic Exponential Families,"Another example is the growing literature on
heavy-tail and risk-averse bandits, that can be modeled using some member of the exponential family. We
leave these intriguing application-speciﬁc developments to future work. References

Ste´phane Boucheron, Ga´bor Lugosi, and Pascal Massart. ",cs.LG,B,0.24324298,0.31318697,-0.1772126
http://arxiv.org/pdf/2201.07316v1,Towards Federated Clustering: A Federated Fuzzy $c$-Means Algorithm (FFCM),"(a) The knowledge gap increases with higher           All in all, we see ﬁrst promising results leading us to con-
standard deviation (= cluster overlap) on average. (b) The      clude that the approach is viable and open questions are
cluster separation decreases with higher dimensionality on      worth to be studied in future works. average as measured by outside cluster SSE on the y-axis. ",cs.LG,A,0.2560634,-0.10357252,0.10017536
http://arxiv.org/pdf/2201.07322v5,Transparent Single-Cell Set Classification with Kernel Mean Embeddings,"(“sentences”). Below we study the semantic retention of cell scores                                    Predictive Analysis Next, we further study the semantic reten-
through a comparative analysis between 𝑠 (𝐺) and 𝑠 (𝜈), and a pre-                                     tion of cell scores with a predictive analysis. Traditionally [5, 26,

dictive analysis. ",cs.LG,A,0.11233647,-0.25165924,-0.21727052
http://arxiv.org/pdf/2201.07348v1,Learning Tensor Representations for Meta-Learning,"We designed two methods to estimate the underlying tensor and compared
them in terms of recovery guarantees, required assumptions on the tensor, and mean squared error on a new task. There are many interesting directions for future work. An interesting direction is to generalize our model and consider
non-linear models of shared representations that incorporates the observable side-information of the tasks. ",cs.LG,B,-0.017147537,0.15481387,-0.24002245
http://arxiv.org/pdf/2201.07383v1,Online Deep Learning based on Auto-Encoder,"24
(a) forestcovtype  (b) gesture

(c) mnist          (d) rotatedmnist

(e) permutedmnist  (f) rﬁd

                                                             25

                                                                       (g) N-Balo
Figure 7: Comparing the accuracies of diﬀerent algorithms on diﬀerent data sets with and without adding noise
Our experimental results suggest that:(1) online deep learning provides an feasible way to learn eﬀective
implicit representation, and the good predictive results were also obtained;(2) the proposed algorithm
obtains smaller reconstruction and prediction loss, the overall prediction performance is also better
than other state-of-art algorithms;(3) compared with the simple weighted average fusion method, our
paper proposes two diﬀerent fusion strategies, which eﬀectively improve the predictive accuracies of
the algorithm;(4) the encouraging experimental results showed that the robustness of the algorithm is
further improved in comparison to most of diverse online learning algorithms. For future work, we plan to consider sequential characteristics of samples to get better prediction
results. At the same time, we can extend our single task online learning to multi-task online deep
learning setup, adaptively learn task weight vector and task correlation from multi-task streaming
data. ",cs.LG,C,-0.32520157,-0.016382005,-0.20769024
http://arxiv.org/pdf/2201.07395v1,Overview frequency principle/spectral bias in deep learning,"The study of synthetic data shows a clear guidance to examine the F-Principle in the high-dimensional

data. In addition, since frequency is a quantity which theoretical study is relative easy to access in,

the F-Principle provides a theoretical direction for further study. 2.2 Two-dimensional experiments

An image can be regarded as a mapping from two-dimensional space coordinate to pixel intensity. ",cs.LG,A,0.15068835,-0.18092626,0.17668942
http://arxiv.org/pdf/2201.07395v2,Overview frequency principle/spectral bias in deep learning,"The study of synthetic data shows a clear guidance to examine the F-Principle in the high-dimensional

data. In addition, since frequency is a quantity which theoretical study is relative easy to access in,

the F-Principle provides a theoretical direction for further study. 2.2 Two-dimensional experiments

An image can be regarded as a mapping from two-dimensional space coordinate to pixel intensity. ",cs.LG,A,0.15068835,-0.18092626,0.17668942
http://arxiv.org/pdf/2201.07519v1,Privacy-Aware Human Mobility Prediction via Adversarial Networks,"Finally, we review the related work in Section 6 and          tion process. conclude the paper with future work directions in Section 7. Privacy II (PII ): the user re-identification inaccuracy, that is, the
                                                                           user de-identification effectiveness. ",cs.LG,A,0.19226147,-0.11723309,-0.03402404
http://arxiv.org/pdf/2201.07612v1,ReGNL: Rapid Prediction of GDP during Disruptive Events using Nightlights,"[17] summarise some existing work in
the domain of economics using satellite imagery. They talk about the scope that exists for further research in the field
of Economics using remote sensing. The paper also discusses the various shortcomings of using remote sensing for
research in the economic context, such as spatial dependence, privacy concerns, dataset size and measurement error. ",cs.LG,A,0.23656577,-0.1427986,-0.045785736
http://arxiv.org/pdf/2201.07677v1,"Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows","The bias measure that we
have proposed can easily be extended to characterise reliability bias due to system (in)efficiency. We will investigate
this in future work. Additionally, we note that our empirical study is focused on audio-based ML. ",cs.LG,A,0.1599034,-0.12318144,-0.15415585
http://arxiv.org/pdf/2201.07677v2,"Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows","The bias measure that we
have proposed can easily be extended to characterise reliability bias due to system (in)efficiency. We will investigate
this in future work. Additionally, we note that our empirical study is focused on audio-based ML. ",cs.LG,A,0.1599034,-0.12318144,-0.15415585
http://arxiv.org/pdf/2201.07677v3,"Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows","The bias measure that we
have proposed can easily be extended to characterise reliability bias due to system (in)efficiency. We will investigate
this in future work. Additionally, we note that our empirical study is focused on audio-based ML. ",cs.LG,A,0.1599034,-0.12318144,-0.15415585
http://arxiv.org/pdf/2201.07708v1,Debiased Graph Neural Networks with Agnostic Label Selection Bias,"We can easily incorporate the VD/DVD term with other
GNNs. We combine them with GAT and more extensions                   B. Baselines
leave as future work. GAT utilizes an attention mechanism
to aggregate neighbor information. ",cs.LG,C,-0.17945711,-0.19602025,0.017927807
http://arxiv.org/pdf/2201.07708v2,Debiased Graph Neural Networks with Agnostic Label Selection Bias,"We can easily incorporate the VD/DVD term with other
GNNs. We combine them with GAT and more extensions                   B. Baselines
leave as future work. GAT utilizes an attention mechanism
to aggregate neighbor information. ",cs.LG,C,-0.17945711,-0.19602025,0.017927807
http://arxiv.org/pdf/2201.07821v1,Building a Performance Model for Deep Learning Recommendation Model Training on GPUs,"search for the best ML model conﬁguration. We see this as
                                                                      exciting future work. As Figure 9 shows, we also compare our performance model
on two CV models, i.e., ResNet50 and Inception-V3, with two                 b) Op Fusion: Op fusion is a common optimization
previous works, Habitat [16] and MLPredict [17], neither of           technique that brings speedup by replacing multiple ops with a
which supports DLRM mainly because of the limited coverage            mathematically equivalent one. ",cs.LG,B,-0.28526595,0.090772375,0.05611441
http://arxiv.org/pdf/2201.07821v2,Building a Performance Model for Deep Learning Recommendation Model Training on GPUs,"Besides, the extension of this work
automatic search for the best ML model conﬁguration. We see                   to (distributed) multi-GPU platforms also requires kernel per-
this as exciting future work. formance models of communication collectives (e.g., all to all,
                                                                              all reduce). ",cs.LG,B,-0.19389434,0.056086753,0.055568263
http://arxiv.org/pdf/2201.07858v1,Decoupling the Depth and Scope of Graph Neural Networks,"On the other hand, the best pooling function may depend on the
graph characteristics. We leave the in-depth analysis on the effect of pooling as future work. 27
         Table 12: Test accuracy on other architectures (PPR EXTRACT)

                 Flickr                                               Reddit                 ogbn-arxiv

         Normal                                SHADOW         Normal          SHADOW        Normal         SHADOW

JK (3)   0.4945±0.0070                         0.5317±0.0027  0.9649±0.0010  0.9682±0.0003  0.7130±0.0026  0.7201±0.0017
JK (5)   0.4940±0.0083                         0.5328±0.0026  0.9640±0.0013  0.9685±0.0006  0.7166±0.0053  0.7226±0.0024

GIN (3)  0.5132±0.0031                         0.5228±0.0028  0.9345±0.0034  0.9578±0.0006  0.7087±0.0016  0.7173±0.0029
GIN (5)  0.5004±0.0067                         0.5255±0.0023  0.7550±0.0039  0.9552±0.0007  0.6937±0.0062  0.7140±0.0027

         Table 13: Effect of subgraph pooling on 5-layer SHADOW-SAGE

Dataset                                        None           Mean            Max                   Sort

Flickr                  0.5351±0.0026 0.5361±0.0015 0.5354±0.0021 0.5367±0.0026

ogbn-arxiv 0.7302±0.0014 0.7304±0.0015 0.7342±0.0017 0.7295±0.0018

F.4 Cost of Subgrpah Extraction

We evaluate the PPR EXTRACT in terms of its execution time overhead and accuracy-time tradeoff. ",cs.LG,A,0.02643137,-0.03291189,0.32040218
http://arxiv.org/pdf/2201.07877v1,PDE-Based Optimal Strategy for Unconstrained Online Learning,"Moreover, it
achieves a loss-regret trade-oﬀ largely unexplored in existing works, which leads to practical advantages when a
good initialization is not available. Our result may inspire future works in multiple directions. First, we use a PDE to achieve a special trade-oﬀ
in online learning. ",cs.LG,B,-0.022226892,0.26169974,-0.277991
http://arxiv.org/pdf/2201.07934v1,Complexity from Adaptive-Symmetries Breaking: Global Minima in the Statistical Mechanics of Deep Neural Networks,"C, we shall venture into the creative side
   variants of ReLU, e.g., Swish [144] and skip-connection archi-      of scientiﬁc inquiry, and try to make sense of the this work in
   tecture like ResNet [145], but they are not pursued in this work    relation to science in general. We brieﬂy remark the contents
   and are considered future works. Meanwhile, this formalism          there in this section for interested readers. ",cs.LG,C,0.008127654,-0.09708668,0.07504831
http://arxiv.org/pdf/2201.07935v1,Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images,"Therefore, the       that, medical students may face many challenges during fetal US
Radiology Editorial Board [170] has created a list of nine           scanning [173]. Therefore, our future work will propose an AI-
important factors to assist in evaluating AI research as a first     mobile-based application to help radiology or sonography
step. These recommendations are intended to enhance the              students identify features in fetus US image and answer the
validity and usefulness of AI research in diagnostic imaging. ",cs.LG,C,-0.027266176,-0.2754339,-0.15889665
http://arxiv.org/pdf/2201.07935v2,Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images,"2) Dataset is small but
[116]                                                                                                                                      Neural Architecture Search (NAS)                                                                                 acquisition, and splitting are clear. 3) Important performance metrics are used
                                                                                                                                           Gradient-based Differentiable Architecture  14 to 28 weeks   Pre: 0.90                 Rec: 0.86                 e1x)cMepethaocdcuorloagcy and result detailed written
[118]                                                                                                                                      Sampler (GDAS)                                                                                                   2) Second work that used of RL with the fetal dataset
                                                                                                                                           N/A                                         N/A              Dihedral angles between two                         3) Some details are not necessary to be included
[119]                                                                                                                                                                                  14 to 28 weeks                                                       4) Such as great work need future work
[117]                                                                                                                                      Bounding-box regression (object-detection)  18 to 34 weeks   planes (ANG): 9.75                                  5) Some performance metrics are missing
[112]
                                                                                                                                           Region Proposal Network (RPN)               20 to 29 weeks   difference between their ED                         1) Methodology and result are precise and detailed. 2) Dataset is transparent in
[113]                                                                                                                                      Slice-Wise Classification                   16 to 34 weeks                                                       terms of acquisition and splitting. ",cs.LG,C,-0.19624034,-0.20229012,0.06417705
http://arxiv.org/pdf/2201.07958v1,Recursive Constraints to Prevent Instability in Constrained Reinforcement Learning,"In what follows, we compare
is, the more complexity this procedure induces in space and time. naive value iteration based on the Bellman operator T𝜃 , which does
                                                                                                       not hold the xed point property P4, with the idea of recursive
1                          −1.4                                                      1                          −1.4

     threshold                          true                                              threshold                          true

0.8  true                               estimate                                     0.8  true                               estimate

     estimate              −1.6                                                           estimate              −1.6

0.6                                                                                  0.6

0.4                                                                                  0.4
                                                         −1.8                                                                                 −1.8

0.2                                                                                  0.2

0                          −2                                                        0                          −2

     0 0.2 0.4 0.6 0.8 1                                       0 0.2 0.4 0.6 0.8 1        0 0.2 0.4 0.6 0.8 1                                       0 0.2 0.4 0.6 0.8 1

               𝜃                                               𝜃                                    𝜃                                               𝜃

     (a) 𝑃 (𝑠0 ; 𝜋ˆ) vs 𝜃                                      (b) 𝑉 (𝑠0 ; 𝜋ˆ) vs 𝜃       (a) 𝑃 (𝑠0 ; 𝜋ˆ) vs 𝜃                                      (b) 𝑉 (𝑠0 ; 𝜋ˆ) vs 𝜃

     Figure 4: Experimental results for naive value iteration                        Figure 5: Experimental results of our proposed approach

constraints and bounded probabilistic reachability presented in this                    In future work, we will adapt our algorithm to the model-free
section. case, and explore the possibility of using our approach with function
6 EXPERIMENTS
                                                                                     approximation. ",cs.LG,B,0.17054655,0.3531847,0.029150693
http://arxiv.org/pdf/2201.08025v1,Low-Pass Filtering SGD for Recovering Flat Optima in the Deep Learning Optimization Landscape,"Therefore, the table can provide us guidelines on how to design or modify deep architectures. This direction of research will be investigated in the future work. Measure           mo               width     wd          lr      bs    skip    bn
                 -0.891            -0.063  -0.291      -0.692  0.981  0.263   0.996
λmax (H )        -0.930            0.029   -0.474      -0.826  0.994  0.218   0.996
 HF              -0.942            -0.127  -0.381      -0.745  0.984  -0.199  0.987
Trace (H)        -0.360            -0.137  -0.147      -0.139  0.335  -0.268  0.047
def f            -0.781            0.147   -0.321      -0.772  0.967  0.509   1.000
 -sharpness      -0.994            0.981   -0.669      -0.971  0.996  0.322   0.996
µP AC−Bayes      -0.824            -0.226  -0.037      -0.545  0.855  -0.605  1.000
FRN              -0.723            -0.174  0.246       -0.352  0.718  0.613   0.950
Shannon Entropy  -0.169            0.954   -0.036      -0.112  0.117  0.013   0.241
µLE              -0.994            0.874   -0.767      -0.934  0.998  -0.543  0.954
LPF

Table 10: Kendall rank correlation coeﬃcient between various sharpness measures (rows) and hyper-parameters
(columns). ",cs.LG,B,-0.07613226,0.072889104,0.28385144
http://arxiv.org/pdf/2201.08025v2,Low-Pass Filtering SGD for Recovering Flat Optima in the Deep Learning Optimization Landscape,"Therefore, the table can provide us guidelines on how to design or modify deep architectures. This direction of research will be investigated in the future work. 11.3 Training details and additional experimental results for Section 3.2 (Sharpness versus
         generalization under data and label noise)

In order to evaluate the performance of sharpness measures to explain generalization in presence of data and
label noise, we trained 10 ResNet18 models with varying level of label noise and 20 ResNet18 model with varying
level of data noise on the CIFAR-10 (Krizhevsky et al., 2014a) data set (Section 3.2 in the main paper). ",cs.LG,C,-0.39695078,-0.1598973,0.048310786
http://arxiv.org/pdf/2201.08102v1,Safe Deep RL in 3D Environments using Human Feedback,"In terms of future work, one particularly important question is: what challenges arise when trying to
use ReQueST in a real-world environment, such as a robotics task? We look forward to future work
addressing this line of research. 7 ETHICS STATEMENT

Contractor welfare Our method relies on a large quantity of data from human contractors, requir-
ing them to perform a task that is extremely monotonous for several hours per person. ",cs.LG,A,0.19223636,-0.13072088,-0.3620149
http://arxiv.org/pdf/2201.08102v2,Safe Deep RL in 3D Environments using Human Feedback,"In terms of future work, one particularly important question is: what challenges arise when trying to
use ReQueST in a real-world environment, such as a robotics task? We look forward to future work
addressing this line of research. 7 ETHICS STATEMENT

Contractor welfare Our method relies on a large quantity of data from human contractors, requir-
ing them to perform a task that is extremely monotonous for several hours per person. ",cs.LG,A,0.19223636,-0.13072088,-0.3620149
http://arxiv.org/pdf/2201.08105v1,"Statistical Depth Functions for Ranking Distributions: Definitions, Statistical Learning and Applications","21. However, non-asymptotic bounds as well as similar results for the depth regions should be
investigated further and are left for future work, like the discrepancy between empirical and theoretical ranking
depth regions, which can be measured by e.g. the cardinality of their symmetric diﬀerence. ",cs.LG,A,0.22382736,0.011632336,0.24944362
http://arxiv.org/pdf/2201.08310v1,Meta Learning for Code Summarization,"The transformer-based meta                 tions to choose from. At the same time, our results
model achieves the best result at 19.2 BLEU (+0.6             also highlight directions for future work, includ-
BLEU). In contrast, the feature-based meta model              ing meta-model introspection (why does the trans-
even underperforms the best individual code sum-              former succeed where the manual features fail?) ",cs.LG,B,-0.06487757,-0.10814671,0.09943581
http://arxiv.org/pdf/2201.08413v1,Unicorn: Reasoning about Configurable System Performance through the lens of Causality,"Following the guidelines             Therefore, we rank the paths in descending of their causal ef-
of Kocaoglu et al., we set an entropy threshold 𝜃𝑟 = 0.8 ×            fect on each non-functional property. For further analysis, we
𝑚𝑖𝑛 {𝐻 (𝑋 ), 𝐻 (𝑌 )}. If the entropy 𝐻 (𝑍 ) of the unmeasured         use paths with the highest causal effect. ",cs.LG,A,0.35647047,0.081263304,0.06989633
http://arxiv.org/pdf/2201.08417v1,Scalable Sampling for Nonsymmetric Determinantal Point Processes,"One limitation of our rejection
sampler is its practical restriction to the ONDPP subclass. Other opportunities for future work
include the extension of our rejection sampling approach to the generation of ﬁxed-size samples
(from k-NDPPs), the development of approximate sampling techniques, and the extension of DPP
samplers along the lines of Derezinski et al. (2019); Calandriello et al. ",cs.LG,A,0.20299542,0.10362472,0.17389219
http://arxiv.org/pdf/2201.08417v2,Scalable Sampling for Nonsymmetric Determinantal Point Processes,"One limitation of our rejection
sampler is its practical restriction to the ONDPP subclass. Other opportunities for future work
include the extension of our rejection sampling approach to the generation of ﬁxed-size samples
(from k-NDPPs), the development of approximate sampling techniques, and the extension of DPP
samplers along the lines of Derezinski et al. (2019); Calandriello et al. ",cs.LG,A,0.20299542,0.10362472,0.17389219
http://arxiv.org/pdf/2201.08430v1,Reproducibility in Learning,"Thus, a reproducible algorithm seldom leaks information about the speciﬁc
input data. We borrowed techniques from the study of private data analysis and diﬀerential privacy, and we
hope that future work will formalize connections between reproducibility and private data analysis. We also
hope that some applications of diﬀerential privacy will also be achievable through reproducibility. ",cs.LG,A,0.29304367,-0.088051006,-0.03507027
http://arxiv.org/pdf/2201.08445v1,A Prescriptive Dirichlet Power Allocation Policy with Deep Reinforcement Learning,"To the best of our knowledge, it is also the ﬁrst framework that enables
distribution of the load in an end-to-end learning setup, without any additional
inputs of, e.g., SoC estimation. For future work, we aim to apply and deploy this
to more challenging and larger size real-world power allocation tasks and extend
it for larger size problems to evaluate its limitations. Acknowledgement

The contributions of Yuan Tian and Olga Fink were funded by the Swiss National
Science Foundation (SNSF) Grant no. ",cs.LG,B,-0.04305453,0.22505124,-0.046738125
http://arxiv.org/pdf/2201.08459v1,Federated Learning with Heterogeneous Architectures using Graph HyperNetworks,"Finally, a more robust aggregation technique
would alleviate the undesirable degradation of performance when local data is limited. Our hope is
that this research will lead to further study of this important setup. Acknowledgement: We thank the following people for useful discussions and proofreading:
Leonidas Guibas, Zan Gojcic, Francis Williams, and Cinjon Resnick. ",cs.LG,A,0.17784563,0.031161241,0.14320281
http://arxiv.org/pdf/2201.08475v1,GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration,"Compared with CPU, GenGNN achieves 1.49–1.95×
                                                  Ε'Wh͗ϱ͘ϯϵǆ                                                                               speed-up; compared with GPU, it is 2.44× faster on Cora,
                                                                                    Ε'Wh͗Ϯ͘ϭϰǆ                            ϴ͘Ϯϵ             1.32× faster on CiteSeer, but 1.04× slower on PubMed. Since
                                                                                                                                            the main focus of this work is not on large graphs, we will
                             ϳ͘ϱ ΕWh͗ϰ͘Ϭϰǆ                           ΕWh͗ϭ͘ϲϰǆ                             ϱ͘ϯϰ        ΕWh͗ϵ͘ϲϵǆ      improve in future works. Ε'Wh͗ϯ͘ϵϰǆ                       Ε'Wh͗ϭ͘ϵϮǆ                       ϰ͘ϳϰ              Ε'Wh͗ϭϳ͘ϲϲǆ
                             ϱ͘Ϭ
                                                     ϯ͘ϲϴ  ϰ͘ϭϭ        ϯ͘ϬϮϯ͘ϱϮ
                                   Ϯ͘ϴϭϮ͘ϳϰ
                             Ϯ͘ϱ                                                 ϭ͘ϴϰ ϭ͘ϵϬϮ͘ϯϬ
                                                                                                   ϭ͘Ϭϳ              ϭ͘ϯϴ
                                           Ϭ͘ϳϬ                  Ϭ͘ϳϲ                                                            Ϭ͘ϴϲ

                             Ϭ͘Ϭ

                                   '/E            '/EнsE               'E               'd             WEΎ              'E

Figure 7. ",cs.LG,A,0.037826527,-0.056117214,0.32227987
http://arxiv.org/pdf/2201.08528v1,"To SMOTE, or not to SMOTE?","In Conference on Learning Theory,
that understanding this disparity is an interesting avenue for       pages 197–209. PMLR, 2014.
future work. [He et al., 2008] Haibo He, Yang Bai, Edwardo A Garcia,
Acknowledgments                                                      and Shutao Li. ",cs.LG,C,0.032591816,-0.090250835,-0.26823387
http://arxiv.org/pdf/2201.08528v2,"To SMOTE, or not to SMOTE?","In Conference on Learning Theory,
that understanding this disparity is an interesting avenue for       pages 197–209. PMLR, 2014.
future work. [He et al., 2008] Haibo He, Yang Bai, Edwardo A Garcia,
Acknowledgments                                                      and Shutao Li. ",cs.LG,C,0.032591816,-0.090250835,-0.26823387
http://arxiv.org/pdf/2201.08528v3,"To SMOTE, or not to SMOTE?","We believe that understanding this disparity is an
trees were considered in [15]. Few large scale empirical studies of      interesting avenue for future work. Another interesting research
balancing schemes were conducted: [21] compared 85 balancing             direction is evaluating the utility of specialized imbalanced classifier
methods using 104 datasets. ",cs.LG,A,0.10237921,-0.19408968,-0.19142166
http://arxiv.org/pdf/2201.08554v1,Enhancing Hyperbolic Graph Embeddings via Contrastive Learning,"Speciﬁcally, the accuracy is improved by 3.12% on DISEASE and 4.05% on CORA compared with
the second best, i.e., HGAT [4]. What’s more, the ablation study and further analysis are represented
in Appendix A.2 and Appendix A.3, respectively. Table 1: Comparisons of node classiﬁcation (NC) in accuracy with the standard deviation

             Dataset       DISEASE       AIRPORT          PUBMED       CITESEER         CORA

             EUC         32.56 ± 1.19  60.90 ± 3.40     48.20 ± 0.76  61.28 ± 0.91  23.80 ± 0.80
             HYP [17]    45.52 ± 3.09  70.29 ± 0.40     68.51 ± 0.37  61.71 ± 0.74  22.13 ± 0.97

             MLP         28.80 ± 2.23  68.90 ± 0.46     72.40 ± 0.21  59.53 ± 0.90  51.59 ± 1.28
             HNN [22]    41.18 ± 1.85  80.59 ± 0.46     69.88 ± 0.43  59.50 ± 1.28  54.76 ± 0.61

             HGNN [3]    81.27 ± 3.53  84.71 ± 0.98     77.13 ± 0.82  69.99 ± 1.00  78.26 ± 1.19
             HGCN [2]    88.16 ± 0.76  89.26 ± 1.27     76.53 ± 0.63  68.04 ± 0.59  78.03 ± 0.98
             HGAT [4]    90.30 ± 0.62  89.62 ± 1.03     77.42 ± 0.66  68.64 ± 0.30  78.32 ± 1.39

             HGCL(ours)  93.42 ± 0.82  92.35 ± 1.01     79.14 ± 0.68  72.11 ± 0.64  82.37 ± 0.47

5 Conclusion

In this work, we brought the beneﬁts of contrastive learning into hyperbolic graph learning to obtain
more powerful representations. ",cs.LG,A,0.22296056,-0.31251496,0.1913346
http://arxiv.org/pdf/2201.08559v1,Individual Treatment Effect Estimation Through Controlled Neural Network Training in Two Stages,"47, 2 (2019), 1148–1178. As part of our future work, following are some of the possible
extensions and applications of the CDNN framework. [5] Heejung Bang and James M. Robins. ",cs.LG,C,-0.13613488,-0.0072315866,0.22705252
http://arxiv.org/pdf/2201.08565v1,Human Activity Recognition models using Limited Consumer Device Sensors and Machine Learning,"results and AUC score improvements. Secondly, different
sensors can be either ideal or damaging towards the                  Other future work includes looking into using raw data
predictive qualities in a model depending on an activity. This    from sensors to extract features with more desirable
was displayed through the results of using sound data to          information for training the models. ",cs.LG,A,0.010746423,-0.34291637,-0.14582811
http://arxiv.org/pdf/2201.08610v2,Deep Q-learning: a robust control approach,"Yet, the of majority of deep Q-learning applications employ some heuristics such
as a target network [21] or random experience replay [22]. Reformulating learning as a dynamical system poses an opportunity to further study its divergent nature and formulate
stabilizing controllers to improve learning performance. Contribution. ",cs.LG,B,-0.26372784,0.29786175,-0.29592586
http://arxiv.org/pdf/2201.08690v1,A deep learning energy method for hyperelasticity and viscoelasticity,"In Sections 3 and 4, the DEM approach is used to solve three-dimensional (3D) examples involving
hyperelastic and linear viscoelastic constitutive models, respectively. We conclude the paper in Section 5 by highlighting
the signiﬁcant results and stating possible future work directions. 2 Method

The ﬁnite element method (FEM) is commonly used to solve problems with material and/or geometric nonlinearities. ",cs.LG,A,0.14932546,0.014394736,0.10948828
http://arxiv.org/pdf/2201.08724v1,Sequential Item Recommendation in the MOBA Game Dota 2,"all other players play an important role in correct itemization. In future work, we will focus on including different types of                [18] A. Drachen, M. Yancey, J. Maguire, D. Chu, I. Wang, T. Mahlmann,
contextual information extracted from Dota 2 matches into                            M. Schubert, and D. Klabajan, “Skill-Based Differences in Spatio-
recommendation models. Temporal Team Behaviour in Defence of The Ancients 2 (DotA 2)”,
                                                                                     2014. ",cs.LG,A,0.14161682,-0.06396179,-0.28563052
http://arxiv.org/pdf/2201.08731v1,Low-Interception Waveform: To Prevent the Recognition of Spectrum Waveform Modulation via Adversarial Examples,"both numerical and physical experiments and has strong
application potential. Of course, the work described here            [9] S. Kokalj-Filipovic, R. Miller, and J. Morman, “Targeted ad-
represents only the initial exploration of LIW, and many                  versarial examples against RF deep classiﬁers,” in WiseML
physical problems must still be solved in future work. 2019, pp. ",cs.LG,B,0.019410303,0.23971069,0.06733336
http://arxiv.org/pdf/2201.08732v1,Meta Learning MDPs with Linear Transition Models,"An inter-
to each other. Consider the task distribution, where          esting avenue of future work is to analyse alternative
each column of the transition cores is a d-dimensional        objectives, for example the worst case transfer regret
basis vector. In this case one can do meta learning on        within the test distribution. ",cs.LG,B,-0.03169197,0.13560542,-0.20441635
http://arxiv.org/pdf/2201.08770v1,Evaluating Generalization in Classical and Quantum Generative Models,"There are also exciting possibilities ex-
parameters affecting the model’s expressibility impact the            pected from purely quantum generative models such as Quan-
model’s ability to generalize. We ultimately see that our met-        tum Circuit Born Machines [4], as we will be exploring in
rics can detect trainability issues as we increase the model’s        future work. We hope this work incites both quantum and
expressibility. ",cs.LG,B,-0.057754226,0.071541145,-0.0006067343
http://arxiv.org/pdf/2201.08770v2,Evaluating Generalization in Classical and Quantum Generative Models,"Additionally, we see that it is possible to use       pected from purely quantum generative models such as Quan-
our metrics to detect trainability issues in GANs, such as            tum Circuit Born Machines [4], as we will be exploring in
mode collapse. future work. We hope this work incites both quantum and
                                                                      classical ML experts to use this framework to enhance the per-
   Finally, we demonstrate an evaluation and comparison of            formance and design of their models, in this now quantitative
our TNBM and GAN models using our metrics. ",cs.LG,B,-0.1425214,-0.010955244,0.027207728
http://arxiv.org/pdf/2201.08802v1,Deconfounding to Explanation Evaluation in Graph Neural Networks,"(2) The generations is constrained within the complete graph

determined by the node set of the explanatory subgraph, thereby limits the quality of deconfounding. As we mainly focus on the OOD problem, we will leave the ability of the generator as future work. 4.3 STUDY OF GENERATORS (RQ2)

The generator plays an important role in our DSE framework, which aims to generate the valid
surrogates conform to the data distribution. ",cs.LG,A,0.13512477,-0.0074657276,0.3042227
http://arxiv.org/pdf/2201.08802v2,Deconfounding to Explanation Evaluation in Graph Neural Networks,"(2) The generations is constrained within the complete graph

determined by the node set of the explanatory subgraph, thereby limits the quality of deconfounding. As we mainly focus on the OOD problem, we will leave the ability of the generator as future work. 4.3 STUDY OF GENERATORS (RQ2)

The generator plays an important role in our DSE framework, which aims to generate the valid
surrogates conform to the data distribution. ",cs.LG,A,0.13512477,-0.0074657276,0.3042227
http://arxiv.org/pdf/2201.08802v3,Deconfounding to Explanation Evaluation in Graph Neural Networks,"(2) The generations is constrained within the complete graph

determined by the node set of the explanatory subgraph, thereby limits the quality of deconfounding. As we mainly focus on the OOD problem, we will leave the ability of the generator as future work. 4.3 STUDY OF GENERATORS (RQ2)

The generator plays an important role in our DSE framework, which aims to generate the valid
surrogates conform to the data distribution. ",cs.LG,A,0.13512477,-0.0074657276,0.3042227
http://arxiv.org/pdf/2201.08832v1,"Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search","We have furthermore presented empirical results that both validate the theory and
indicate promising empirical performance when compared with state-of-the-art methods on sparse-reward
problems. Interesting directions for future work include extension of our theoretical results to more general
classes of ratio optimization problems, exploration of max-entropy policy gradient methods leveraging our
entropy gradient theorems, development of variants of the Information-Directed Actor-Critic algorithm for
continuous spaces using suitable density estimation techniques, and thorough empirical evaluation of deep
RL variants of IDAC on a broad class of benchmark problems. References

 [1] Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. ",cs.LG,B,-0.15308297,0.4382099,-0.22410113
http://arxiv.org/pdf/2201.08837v1,Marginal Effects for Non-Linear Prediction Functions,"Model-agnostic
methods in general are favorable tools to explain the model behavior but often fail to explain the underlying DGP, as the
quality of the explanations relies on the closeness between model and reality. Throughout the manuscript, we noted various directions that may be explored in future work. These include quantifying
extrapolation risk for the selection of step sizes, stabilizing the found subspaces for cAMEs, or quantifying the subspaces’
uncertainty. ",cs.LG,A,0.16624904,0.12885511,-0.007221438
http://arxiv.org/pdf/2201.08848v1,Lensing Machines: Representing Perspective in Latent Variable Models,"7. Future work

For future work, we are looking at applying lensing to important problems in medicine – especially
cardiology, where it has been shown that differences in points of view between patients and care-
givers can routinely result in misunderstandings that can lead to neglect of potentially grave patient
symptoms (Kreatsoulas et al., 2013). We see our work as part of an emerging movement to promote human-in-the-loop machine
learning (Amershi et al., 2014). ",cs.LG,C,-0.09925097,-0.24250948,-0.33043626
http://arxiv.org/pdf/2201.08896v1,Environment Generation for Zero-Shot Compositional Reinforcement Learning,"Table 2: CoDE performance on real web-
                                                            sites across tasks and difﬁculty levels. 8 Broader impact, limitations, and future work

PN formalism (Appendix A.1) is useful beyond this work, as it exposes compositional task topology. We use expanded workﬂow model, which although expressive for a rich set of tasks seeded with only
a handful of primitives, is still only one topological class of tasks. ",cs.LG,B,-0.09047991,0.0029293373,-0.10304093
http://arxiv.org/pdf/2201.08905v1,Optimal Dynamic Regret in Proper Online Learning with Strongly Convex Losses and Beyond,"Finally summing the
regret contributions from bins in R and S yield the theorem. 6 Conclusion and future work

In this work we presented a new analysis that extends the results of [4] and showed near optimal universal dynamic
regret in a proper learning setting for strongly convex losses. Results on the special case of exp-concave losses and box
decision set are also derived. ",cs.LG,B,0.036615632,0.36151904,-0.14759734
http://arxiv.org/pdf/2201.08924v1,Nearest Class-Center Simplification through Intermediate Layers,"Perhaps this notion of thinking can     tency can assist in the training and generalization. We hope
      aid in deﬁning a concept of “dataset diﬁculty” for a      further research using these methods can continue to enrich
      certain model architecture. the study in deepnets and their training paradigms. ",cs.LG,C_centroid,-0.3811584,-0.25700656,-0.17297772
http://arxiv.org/pdf/2201.08924v2,Nearest Class-Center Simplification through Intermediate Layers,"We
also show how encouraging inner-layer class-center con-        Belkin, M. Fit without fear: remarkable mathematical phe-
sistency can assist in the training and generalization. We        nomena of deep learning through the prism of interpola-
hope further research using these methods can continue to         tion. Acta Numerica, 30:203 – 248, 2021.
enrich the study in deepnets and their training paradigms. ",cs.LG,C,-0.4016646,-0.02357945,-0.0268543
http://arxiv.org/pdf/2201.08984v1,PiCO: Contrastive Label Disambiguation for Partial Label Learning,"Theoretical analysis shows that PiCO can be interpreted from an EM-algorithm
perspective. Applications of multi-class classiﬁcation with ambiguous labeling can beneﬁt from our
method, and we anticipate further research in PLL to extend this framework to tasks beyond image
classiﬁcation. We hope our work will draw more attention from the community toward a broader
view of using contrastive prototypes for partial label learning. ",cs.LG,C,-0.17960894,-0.0915959,-0.1334894
http://arxiv.org/pdf/2201.08984v2,PiCO: Contrastive Label Disambiguation for Partial Label Learning,"Theoretical analysis shows that PiCO can be interpreted from an EM-algorithm
perspective. Applications of multi-class classiﬁcation with ambiguous labeling can beneﬁt from our
method, and we anticipate further research in PLL to extend this framework to tasks beyond image
classiﬁcation. We hope our work will draw more attention from the community toward a broader
view of using contrastive prototypes for partial label learning. ",cs.LG,C,-0.17960894,-0.0915959,-0.1334894
http://arxiv.org/pdf/2201.09044v1,Good Classification Measures and How to Find Them,"Appendix E.2 contains an additional experiment with an unbalanced multiclass dataset, where we
show the inconsistency rates of the considered measures and different multiclass extensions. 6 Conclusion and future work

In this paper, we propose a systematic approach to the analysis of classiﬁcation performance measures:
we propose several desirable properties and theoretically check each property for a list of measures. We also prove an impossibility theorem: some desirable properties cannot be simultaneously satisﬁed,
so either distance or exact constant baseline has to be discarded. ",cs.LG,A,0.15283357,-0.10307124,-0.058832757
http://arxiv.org/pdf/2201.09046v1,Differentially Private SGDA for Minimax Problems,"Diﬀerent Mini-Batch Size. From Theorem 1 and
Theorem 4, we ﬁnd mini-batch size can inﬂuence the                   For future work, it would be interesting to improve
Gaussian noise variances σw2 and σv2 as well as the con-             the utility bound for the nonconvex-strongly-convex
vergence rate. Selecting the mini-batch size must bal-               setting. ",cs.LG,B,0.023662245,0.33729464,0.24256063
http://arxiv.org/pdf/2201.09046v2,Differentially Private SGDA for Minimax Problems,"vergence rate. Selecting the mini-batch size must bal-               For future work, it would be interesting to improve
ance two conﬂicting objectives. On one hand, a small                 the utility bound for the nonconvex-strongly-convex
mini-batch size may lead to sub-optimal performance. ",cs.LG,B,0.0011624265,0.32195324,0.14718902
http://arxiv.org/pdf/2201.09046v3,Differentially Private SGDA for Minimax Problems,"vergence rate. Selecting the mini-batch size must bal-               For future work, it would be interesting to improve
ance two conﬂicting objectives. On one hand, a small                 the utility bound for the nonconvex-strongly-convex
mini-batch size may lead to sub-optimal performance. ",cs.LG,B,0.0011624265,0.32195324,0.14718902
http://arxiv.org/pdf/2201.09046v4,Differentially Private SGDA for Minimax Problems,"Processing Systems, 33:4381–4391, 2020. For future work, it would be interesting to improve the utility  Brett K Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams,
bound for the nonconvex-strongly-convex setting. It also            Ran Lee, Sanjeev P Bhavnani, James Brian Byrd, and
remains unclear to us how to establish the utility bound for        Casey S Greene. ",cs.LG,B,0.053792804,0.3083406,0.19855578
http://arxiv.org/pdf/2201.09051v1,On the Robustness of Counterfactual Explanations to Adverse Perturbations,"Thus, we proposed to control for K-robustness using an approximation, i.e., the
K-robustness score. Our results show that, for the data sets considered, seeking counterfactual examples that maximize
the K-robustness score is often but not always sufficient to obtain a good resilience to perturbations to the features in
K. Therefore, future work should consider whether a better heuristic can be used than randomly sampling points in
the K-neighborhood 𝑁 of a counterfactual example. If instead information on 𝑓 is available, that information can be
used to provide guarantees on 𝑁 (see, e.g., Theorem 2 in [9] for linear 𝑓 ). ",cs.LG,A,0.17364629,0.18764427,-0.04694368
http://arxiv.org/pdf/2201.09051v2,On the Robustness of Counterfactual Explanations to Adverse Perturbations,"Thus, we proposed to control for K-robustness using an approximation, i.e., the
K-robustness score. Our results show that, for the data sets considered, seeking counterfactual examples that maximize
the K-robustness score is often but not always sufficient to obtain a good resilience to perturbations to the features in
K. Therefore, future work should consider whether a better heuristic can be used than randomly sampling points in
the K-neighborhood 𝑁 of a counterfactual example. If instead information on 𝑓 is available, that information can be
used to provide guarantees on 𝑁 (see, e.g., Theorem 2 in [10] for linear 𝑓 ). ",cs.LG,A,0.17391291,0.18661928,-0.051664963
http://arxiv.org/pdf/2201.09051v3,On the Robustness of Sparse Counterfactual Explanations to Adverse Perturbations,"For many real-world problems, it is reasonable to expect that there exist groups of features
that are truly independent from other groups of features. Thus, the study of robustness for C and K could
be carried out at a higher level, i.e., of feature groups in future work. There is a number of further aspects worth mentioning when one wishes to implement a research work
like on counterfactual explanations into practice, including this work. ",cs.LG,A,0.16937837,-0.057595424,-0.17155862
http://arxiv.org/pdf/2201.09065v1,Online Attentive Kernel-Based Temporal Difference Learning,"performing method. Finally, conclusions and future work are stated in Section VI. The rest of this paper is organized as follows. ",cs.LG,A,0.32136598,0.013092818,0.105267555
http://arxiv.org/pdf/2201.09172v1,An Attention-based ConvLSTM Autoencoder with Dynamic Thresholding for Unsupervised Anomaly Detection in Multivariate Time Series,"Finally, Section VII concludes the paper and dis-
feature images input. The structures that exist in the data are   cusses opportunities for future work. learned and consequently leveraged when forcing the input
through the autoencoder’s bottleneck. ",cs.LG,C,-0.3426581,-0.18527767,0.10209956
http://arxiv.org/pdf/2201.09191v2,Revisiting Global Pooling through the Lens of Optimal Transport,"However, given a tensor Xin ∈ RB×C×H×W , a local pooling merges each patch with
size (B × C × 2 × 2) into B C-dimensional vectors and outputs Xout ∈ RB×C× H2 × W2 , which involves

BHW   pooling operations. Such a local pooling requires an eﬃcient CUDA implementation of the
   4

UOTP layers, which will be our future work. 6 Conclusion

In this work, we studied global pooling through the lens of optimal transport and demonstrated
that many existing global pooling operations correspond to solving a UOT problem with diﬀerent
conﬁgurations. ",cs.LG,B,-0.2168628,0.18441673,0.20314863
http://arxiv.org/pdf/2201.09199v1,Deep Learning on Attributed Sequences,"Related
work is discussed in Chapter 7. We conclude the ﬁndings and discuss
future works in Chapter 8. Chapter 2
Background: Attributed
Sequence
                                                                                                 36

Deﬁnition 1 (Sequence.) ",cs.LG,A,0.17338638,-0.076864034,0.093698636
http://arxiv.org/pdf/2201.09209v2,Weight Expansion: A New Perspective on Dropout and Generalization,"That is, vol(w) ≈ i λ1i , where λi is the i-th eigenvalue of the Hessian matrix H. Not only does this conﬁrm
the connection between weight volume and sharp/ﬂat minima (i.e., large/small eigenvalues) of the Hessian,
but also provides another, perhaps more comprehensive, view of ﬂatness with respect to the whole spectra of
the Hessian. 7 Conclusion and future work

This paper introduces ‘weight expansion’, an intriguing phenomenon that appears whenever dropout is applied
during training. From a perspective of covariance, weight volume can be seen as a measure of ﬂatness and
weight expansion can be thought as random weights lying in a more spherical (ﬂatter) local minimum. ",cs.LG,A,0.08672117,0.087098375,0.07843202
http://arxiv.org/pdf/2201.09332v1,Investigating Expressiveness of Transformer in Spectral Domain for Graphs,"It should be possible to learn
                                                                   better graph speciﬁc ﬁlters using the structure of the input
A.4. Experiment Settings                                           graph and the spatial attention of the transformer, which we
                                                                   leave to future works. Table 5 lists the hardware and the run time of the experi-
ments on each dataset. ",cs.LG,C,-0.10930608,-0.16445954,0.4362175
http://arxiv.org/pdf/2201.09332v2,Investigating Expressiveness of Transformer in Spectral Domain for Graphs,"It should be possible to learn
                                                                   better graph speciﬁc ﬁlters using the structure of the input
A.4. Experiment Settings                                           graph and the spatial attention of the transformer, which we
                                                                   leave to future works. Table 5 lists the hardware and the run time of the experi-
ments on each dataset. ",cs.LG,C,-0.10930608,-0.16445954,0.4362175
http://arxiv.org/pdf/2201.09332v3,Investigating Expressiveness of Transformer in Spectral Domain for Graphs,"It should be possible to learn
                                                                   better graph speciﬁc ﬁlters using the structure of the input
A.4. Experiment Settings                                           graph and the spatial attention of the transformer, which we
                                                                   leave to future works. Table 5 lists the hardware and the run time of the experi-
ments on each dataset. ",cs.LG,C,-0.10930608,-0.16445954,0.4362175
http://arxiv.org/pdf/2201.09332v4,How Expressive are Transformers in Spectral Domain for Graphs?,"It should be possible to learn
                                                                   better graph speciﬁc ﬁlters using the structure of the input
A.4. Experiment Settings                                           graph and the spatial attention of the transformer, which we
                                                                   leave to future works. Table 5 lists the hardware and the run time of the experi-
ments on each dataset. ",cs.LG,C,-0.10930608,-0.16445954,0.4362175
http://arxiv.org/pdf/2201.09366v1,Optimal transport for causal discovery,"Because although the measure value,
in theory, is zero in the causal direction when there is no unknown confounder, with ﬁnite samples
the measure value will be larger than zero in practice. And to decide which measure value is close to
zero or not, it requires a threshold which can vary in different applications, or deriving a statistical
test which is nontrivial for the measure without assuming the type of noise distributions and can be
the future work of our method. Thus, we test whether two measure values are signiﬁcantly different,
and if so, we then conclude that the smaller one is in the causal direction; if they are not signiﬁcantly
different, we then conclude that it is the independent case. ",cs.LG,A,0.46187353,-0.032006297,-0.0074934387
http://arxiv.org/pdf/2201.09366v2,Optimal transport for causal discovery,"Because although the measure value,
in theory, is zero in the causal direction when there is no unknown confounder, with ﬁnite samples
the measure value will be larger than zero in practice. And to decide which measure value is close to
zero or not, it requires a threshold which can vary in different applications, or deriving a statistical
test which is nontrivial for the measure without assuming the type of noise distributions and can be
the future work of our method. Thus, we test whether two measure values are signiﬁcantly different,
and if so, we then conclude that the smaller one is in the causal direction; if they are not signiﬁcantly
different, we then conclude that it is the independent case. ",cs.LG,A,0.46187353,-0.032006297,-0.0074934387
http://arxiv.org/pdf/2201.09394v1,An integrated recurrent neural network and regression model with spatial and climatic couplings for vector-borne disease dynamics,"3, the effects of climatic data
                                                         may have a latent period to induce vector growth and
                                                         subsequent case upswing. In future work, we plan
                                                         to introduce a time delay in the climate term of our
                                                         model and learn it from the data for another improve-
                                                         ment. We note that RMSE ≥ MAE, and it can be shown that

RMSE−MAE = Var(e). ",cs.LG,A,0.27824616,-0.13126041,0.04379183
http://arxiv.org/pdf/2201.09398v1,Towards Private Learning on Decentralized Graphs with Local Differential Privacy,"We leave the problem of designing new mechanisms
where p(y|x˜) = g(Ac; Xc, θ). More speciﬁcally, g represents    that satisfy edge-LDP when the users and the data curator
the GNN model and θ are the learned model parameters            may not know the entire set of the users as future work. given the obfuscated graph G˜ = (A˜ , X˜ ) and the denoising
mechanisms. ",cs.LG,C,-0.10816561,0.11113846,0.118651316
http://arxiv.org/pdf/2201.09418v1,Approximation bounds for norm constrained neural networks with applications to regression and GANs,"mHeeannces, gwne,λc∈anGapisplay sTohluetoiroenmo4f.6(4. .7) with discriminator FK and optimization error

                                                               27
5 Conclusions and future work

This paper have established upper and lower approximation bounds for ReLU neural networks
with norm constraint on the weights. We use these bounds to analyze the convergence rate of
estimating H¨older continuous functions by norm constrained neural networks. ",cs.LG,C,-0.28632593,0.14067017,0.11222193
http://arxiv.org/pdf/2201.09418v2,Approximation bounds for norm constrained neural networks with applications to regression and GANs,"Hence, we can apply Theorem 4.6. 5 Conclusions and future work

This paper have established upper and lower approximation bounds for ReLU neural networks
with norm constraint on the weights. We use these bounds to analyze the convergence rate of
estimating H¨older continuous functions by norm constrained neural networks. ",cs.LG,C,-0.25558677,0.16454053,0.096170545
http://arxiv.org/pdf/2201.09433v1,Active Learning Polynomial Threshold Functions,"To answer this question, we will restrict our attention to distributions over PTFs with exactly d
real roots. It is possible to handle more general scenarios query-eﬃciently via basic variants of Sample and
Search,9 but we leave such exploration to future work. To start, it is clear that the “Search” step of Sample
and Search uses at most d log n queries, as it performs d instances of binary search, so the main challenge
lies in analyzing step 1. ",cs.LG,A,0.2937292,0.107840985,0.17810634
http://arxiv.org/pdf/2201.09433v2,Active Learning Polynomial Threshold Functions,"In other words, multivariate PTFs cannot be actively learned via access to basic derivative queries in the
worst-case. It remains an interesting open problem whether there exist natural query sets that can learn
multivariate PTFs, or whether this issue can be avoided in average-case settings; we leave these questions
to future work. 1.3 Related work

Active Learning Halfspaces: While to our knowledge active learning polynomial threshold functions
has not been studied in the literature, the closely related problem of learning halfspaces is perhaps one of
the best-studied problems in the ﬁeld, and indeed in learning theory in general. ",cs.LG,C,-0.013030335,0.12262184,-0.10554529
http://arxiv.org/pdf/2201.09460v1,Probability Distribution on Rooted Trees,"1649–
serious problem if kmax is large. To avoid this summation is                     1668, Dec 2012.
a meaningful future work. [7] Y. Nakahara and T. Matsushima, “A stochastic model for block
                         VIII. ",cs.LG,A,0.3780604,0.14591092,0.110664636
http://arxiv.org/pdf/2201.09568v1,Pearl: Parallel Evolutionary and Reinforcement Learning Library,"0 ES             Sphere                          0 ES             Matyas                                            Ackley                                          Beale
                  AdamES                                           AdamES
                                                                                           -4                        ES                                 0
        -50                                               -1                                                         AdamES

                                                                                           -6                                                           -25,000

Reward  -100                                      Reward  -2                       Reward  -8                                                   Reward  -50,000

        -150 -3 -10                                                                                                                                     -75,000

        -200                                                                               -12                                                          -100,000     ES
                                                                                                                                                                     AdamES
                                                          -4     50 100 150 200 250 -14 0
              0      50 100 150 200 250                       0                                                      50 100 150 200 250                           0  50 100 150 200 250
                          EggSHteoplder                                 HimmSteeplblau                                       RaSstteripgin
                                                          0                                                                                             0                   RoseSntebprock
                                                                                                                -20
                 ES                                                                                                                         ES
                 AdamES                                                                                                                     AdamES
        60
                                                      -200

        50                                                                                 -30                                                          -1,000

Reward                                        Reward  -400                         Reward  -40                                                  Reward

        40                                                                                 -50                                                          -2,000

                                                      -600

        30                                                         -800 EASdamES -60                                 50 100 Step 150 200 250                           ES
            0        50 100 Step 150 200 250 0 50 100 Step 150 200 250 0                                                                                               AdamES
                                                                                                                                                                  0 50 100 Step 150 200 250

                                                                           Figure 3: Learning curves

                                                                                   4
                                                                                                    Pearl                                                                                    A PREPRINT

                                Sphere                                          Matyas                                              Ackley                                                     Beale

               10 3                            ES                                               ES                                                 ES                                        100 150             ES
               10 4                            AdamES                                           AdamES                                             AdamES                                                        AdamES
               10 5                                                                                                                                                                          RoseSntebprock  200 250
               10 6 0                                      10 3                                             10 3                                               10 3                                          200 250
                                                                                                                                                                                             100 Step 150
KL Divergence                                           KL Divergence                                   KL Divergence  10 4                                  KL Divergence

                                                                       10 4                                                                                                 10 4

                                                                                                                       10 5

                                                                       10 5                                            10 6                                                 10 5

                        50 100 150             200 250                       0  50 100 150 200 250                           0      50 100 150 200 250                            0  50
                                                                                HimmSteeplblau                                      RaSstteripgin
                                EggSHteoplder

                                                                                                ES                                                 ES
                                                                                                AdamES                                             AdamES
                                                                                                                       10 3                                                 10 3
               10 3                                                    10 3

KL Divergence  10 4                                     KL Divergence  10 4                                                   10 4KL Divergence              KL Divergence  10 4
                                                                                                                              10 5
               10 5                                                    10 5                                                   10 6                                          10 5
                                                                                50 100 Step 150 200 250 0
               10 6     ES                                             10 6                                                                                                 10 6     ES
                     0  AdamES                                                                                                                                                       AdamES

                            50  100 Step 150   200 250                       0                                                      50 100 Step 150 200 250                       0  50

                                                                                Figure 4: Population divergence with log scale

5 Conclusions and Future Work

This paper has introduced Pearl, a library especially designed to allow researchers to rapidly prototype and test new
ideas in order to optimize decision making algorithms in reinforcement learning type environments. Through easy
access to interpretable tools, Pearl facilitates experimentation, further research and innovation, particularly at the overlap
between RL and EC. Future work will involve new agents and features such as the Intrinsic Curiosity Module (Pathak et al. ",cs.LG,B,-0.0970431,-0.0131667135,-0.023609139
http://arxiv.org/pdf/2201.09603v2,Performance Analysis of Electrical Machines Using a Hybrid Data- and Physics-Driven Model,"We
          8 direct DL approach                                                                                           0.20                                                  have demonstrated that the trained multi-branch DNN meta-
                                                                                                 direct DL approach                                        direct DL approach  model evaluates new designs at much lower computational
                                   0.8 hybrid approach                                           hybrid approach                                           hybrid approach     effort than the FE simulation. In future work, the proposed
          6 0.6                                                                                                                                                                hybrid approach can be applied to many query scenarios,
     MRE                                                                                                                                                                       e.g. uncertainty quantiﬁcation, sensitivity analysis and multi-
          4                                                             0.4                                                                                                    objective optimization. ",cs.LG,B,-0.21657166,0.0390902,0.053860843
http://arxiv.org/pdf/2201.09637v1,DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery -- A Focus on Affinity Prediction Problems with Noise Annotations,"However, DrugOOD can be easily extended by
incorporating 3D structure information of targets by refering to protein sturcute deposit-
ing database, such as PDB and uniprot (Consortium, 2014). This will be left as important
future work. 6.2.2 Model

We use a general SBAP prediction network that extracts molecular and protein features
separately, which are then concatenated together and fed into a fully connected layer
to predict interaction probabilities. ",cs.LG,C,-0.072261915,-0.13057724,-0.028238397
http://arxiv.org/pdf/2201.09644v1,Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models,"Available
available is small. A possible future work could be to explore  online at: https://www.ree.es/en/activities/operation-of-the-
the approach with generative models other than GANs. electricity-system/international-interconnections, last ac-
                                                                cessed on 01 Sep, 2021. ",cs.LG,B,-0.15735924,-0.049410827,0.13514307
http://arxiv.org/pdf/2201.09644v2,Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models,"Available
available is small. A possible future work could be to explore  online at: https://www.ree.es/en/activities/operation-of-the-
the approach with generative models other than GANs. electricity-system/international-interconnections, last ac-
                                                                cessed on 01 Sep, 2021. ",cs.LG,B,-0.15735924,-0.049410827,0.13514307
http://arxiv.org/pdf/2201.09653v1,The Paradox of Choice: Using Attention in Hierarchical Reinforcement Learning,"A.4 Additional Experiments

A.4.1 Analysis of ICnet precision while training

We provide in Fig. A4 an additional investigation of the intent completion network’s (ICnet) precision
during training. We report the fraction of correct positive predictions made as compared to the success
of option policies that use these networks for sampling affordable options during pre-training (as
described in Alg. ",cs.LG,B,-0.13862458,0.029781455,-0.14226589
http://arxiv.org/pdf/2201.09686v1,Learning Sparse and Continuous Graph Structures for Multivariate Time Series Forecasting,"On three public datasets, LSCGF achieves state-of-the-art
results. In future work, we will study the efﬁcient design
of graph structure algorithms in multiple areas so that more
models can get rid of the limitations of the pre-deﬁned graph
structure and improve the effect. Figure 4: The curves of different α values. ",cs.LG,C,0.10264546,-0.060742646,0.30910647
http://arxiv.org/pdf/2201.09699v1,EASY: Ensemble Augmented-Shot Y-shaped Learning: State-Of-The-Art Few-Shot Classification with Simple Ingredients,"We showed the ability of the method to obtain state
              exp −β z−cit 2                                                        of the art results on multiple standardized benchmarks, even
                            2 if z ∈ Q ,                                            beating previous methods by a fair margin in some cases. There
w(z, cit) =  n      exp(−β          )2                                              is no real new ingredient in this methodology, but we expect
                             z−cj t                   (6)                            it to serve as a baseline for future work. 2
                j=1                                                                     1The codes allowing to reproduce our experiments are available at https:
                                                                                    //github.com/ybendou/easy. ",cs.LG,B,0.06009407,0.24927768,0.21727374
http://arxiv.org/pdf/2201.09699v2,EASY: Ensemble Augmented-Shot Y-shaped Learning: State-Of-The-Art Few-Shot Classification with Simple Ingredients,"We showed the ability of the method to obtain state
              exp −β z−cit 2                                                        of the art results on multiple standardized benchmarks, even
                            2 if z ∈ Q ,                                            beating previous methods by a fair margin in some cases. There
w(z, cit) =  n      exp(−β          )2                                              is no real new ingredient in this methodology, but we expect
                             z−cj t                   (6)                            it to serve as a baseline for future work. 2
                j=1                                                                     1The codes allowing to reproduce our experiments are available at https:
                                                                                    //github.com/ybendou/easy. ",cs.LG,B,0.06009407,0.24927768,0.21727374
http://arxiv.org/pdf/2201.09750v1,Online AutoML: An adaptive AutoML framework for online learning,"This demonstrates the beneﬁts of automating both algorithm
selection and hyperparameter tuning for online learning, unlike several previ-
ous studies that focus only on either one of these. In all, we believe that this
work opens up interesting avenues for further research. Springer Nature 2021 LATEX template

22 Online AutoML: An adaptive AutoML framework for online learning

Acknowledgments. ",cs.LG,B,-0.07647189,0.11138594,-0.24299666
