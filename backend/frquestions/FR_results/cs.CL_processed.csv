url,title,further research,primary category,label,x,y,z
http://arxiv.org/pdf/2201.00075v1,How do lexical semantics affect translation? An empirical study,"The     models. With insight into these trends, we hope
                                        decoder then utilizes this representation to gener-   to further research on multilingual translation by
                                        ate a sequence of words, which is the translation. Attention allows the decoder to weight individual         1Among other features, including positional encoding and
                                        tokens in the source sequence depending on their      layer normalization
                                        importance to the word being generated. ",cs.CL,B,-0.37049767,-0.09180388,-0.12279976
http://arxiv.org/pdf/2201.00118v1,Semantic Search for Large Scale Clinical Ontologies,"So,  the  higher   value  of  M RR   is,  the  less  time  the  user
          5                                                  7

spends to ﬁnd the correct answer for a given query. Doing further analysis on this example, Triplet-BERT returns top ﬁve results including: the correct concept in the ﬁrst
position, its sibling concept 1667003 in the second position, its parent concept 248382004 in the third position, an
uncle concept 249079005 in the fourth position and a grandparent concept 248381006 in the ﬁfth position. Based
on the respective gain deﬁned above, Triplet-BERT obtains an nDCG@5 = 0.976. ",cs.CL,C,0.1756016,-0.13646162,-0.04135915
http://arxiv.org/pdf/2201.00374v1,Topical Classification of Food Safety Publications with a Knowledge Base,"The method is robust,
modular base which can be iteratively improved, to achieve better accuracy. In
particular, further research into candidate generation and how the algorithm
perceives the context of both the abstract and the knowledge base is needed. Regarding classiﬁcation speed, substantial improvements could be made by
parallelizing the code. ",cs.CL,C,0.093519375,-0.28426662,-0.014350114
http://arxiv.org/pdf/2201.00374v2,Topical Classification of Food Safety Publications with a Knowledge Base,"The method is robust,
modular base which can be iteratively improved, to achieve better accuracy. In
particular, further research into candidate generation and how the algorithm
perceives the context of both the abstract and the knowledge base is needed. Regarding classiﬁcation speed, substantial improvements could be made by
parallelizing the code. ",cs.CL,C,0.093519375,-0.28426662,-0.014350114
http://arxiv.org/pdf/2201.00490v1,Learning with Latent Structures in Natural Language Processing: A Survey,"(2018) noted that trivial trees (i.e., balanced or linear trees) achieve performance similar
to or better than latently induced trees or external parse trees on a variety of classiﬁcation and generation
tasks, casting doubt on the real source of performance improvement. More future work is needed to
understand the role syntax plays in this picture and how it might vary across different tasks. The story is different for methods for unsupervised parsing which can induce familiar linguistic for-
malisms with decent accuracy (though see Li and Risteski (2021) for a theoretical analysis of their
limitations). ",cs.CL,B,-0.1684728,-0.07872287,0.042754993
http://arxiv.org/pdf/2201.00490v2,Learning with Latent Structures in Natural Language Processing: A Survey,"(2018) noted that trivial trees (i.e., balanced or linear trees) achieve performance similar
to or better than latently induced trees or external parse trees on a variety of classiﬁcation and generation
tasks, casting doubt on the real source of performance improvement. More future work is needed to
understand the role syntax plays in this picture and how it might vary across different tasks. The story is different for methods for unsupervised parsing which can induce familiar linguistic for-
malisms with decent accuracy (though see Li and Risteski (2021) for a theoretical analysis of their
limitations). ",cs.CL,B,-0.1684728,-0.07872287,0.042754993
http://arxiv.org/pdf/2201.00558v1,Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models,"We leave
6.4. Sentence length                                             exploration into the interplay between these three factors in
    BiLSTM is inherently designed to model long sequences,       data augmentation as future work. unlike CNN. ",cs.CL,B,-0.18557957,-0.077754274,-0.0031247418
http://arxiv.org/pdf/2201.01140v1,Predicting Influenza A Viral Host Using PSSM and Word Embeddings,"Multi-label prediction
extracted by PSSM before feeding into classic machine learn-                   is beyond the scope of this article (only about 3% of the data
ing models. PSSM-based features cannot only maintain the                       is multi-label) and is left for future work. evolutionary information in the virus sequence but also capture
the local information in the sequence. ",cs.CL,C,0.0672866,-0.06218548,-0.106050596
http://arxiv.org/pdf/2201.01140v2,Predicting Influenza A Viral Host Using PSSM and Word Embeddings,"Multi-label prediction
extracted by PSSM before feeding into classic machine learn-                   is beyond the scope of this article (only about 3% of the data
ing models. PSSM-based features cannot only maintain the                       is multi-label) and is left for future work. evolutionary information in the virus sequence but also capture
the local information in the sequence. ",cs.CL,C,0.0672866,-0.06218548,-0.106050596
http://arxiv.org/pdf/2201.01337v1,ZeroBERTo -- Leveraging Zero-Shot Text Classification by Topic Modeling,"Then, it would be possible to compare the performance of the models
using BERTimbau (BERT-Portuguese) [26] both in clustering and classifying. Another interesting future work would be to enable ZeroBERTo to deal with
multi-label classiﬁcation, where each document can have none, one or several
labels. Acknowledgements

This research was supported in part by Itau´ Unibanco S.A., with the scholarship
program of Programa de Bolsas Itau´ (PBI), and by the Coordenac¸˜ao de Aper-
fei¸coamento de Pessoal de N´ıvel Superior (CAPES), Finance Code 001, CNPQ
(grant 310085/2020-9), and USP-IBM-FAPESP Center for Artiﬁcial Intelligence
(FAPESP grant 2019/07665-4), Brazil. ",cs.CL,B,0.031027578,0.052070953,-0.14011419
http://arxiv.org/pdf/2201.01337v2,ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling,"A signiﬁcant diﬃculty of this work was that, as far as the authors are aware
of, there are no large benchmark datasets for multi-class text classiﬁcation in
Portuguese, nor general use datasets with semantically meaningful labels. In this
sense, some future work directions involve the production of benchmark datasets
for Portuguese text classiﬁcation (and 0shot-TC). It would also be interesting
to produce Natural Language Inference datasets in Portuguese, which could, in
addition to the existing ones [10,22], enable ﬁne-tuning of Transformers 100 % in
10  A. Alcoforado et al. ",cs.CL,B,-0.19575426,0.0747505,-0.092319295
http://arxiv.org/pdf/2201.01337v3,ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling,"It would also be worthwhile to
test the proposed model in other domains: to name one, legislative data present similar challenges
[Ferraz et al., 2021]. Another interesting future work would be to enable ZeroBERTo to deal with
multi-label classiﬁcation, where each document can have none, one or several labels. Acknowledgments

This research was supported in part by Itaú Unibanco S.A., with the scholarship program of Programa
de Bolsas Itaú (PBI), and by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior
(CAPES), Finance Code 001, CNPQ (grant 310085/2020-9), and USP-IBM-FAPESP Center for
Artiﬁcial Intelligence (FAPESP grant 2019/07665-4), Brazil. ",cs.CL,B,0.02916126,0.11591485,-0.11834964
http://arxiv.org/pdf/2201.01364v2,A Discriminative Hierarchical PLDA-based Model for Spoken Language Recognition,"Nevertheless, we plan
DPLDA model. The last line in Table I shows the results for                  to explore different types of embeddings in our future work. this reduced HDPLDA system. ",cs.CL,C,0.1055852,-0.1839957,-0.15127027
http://arxiv.org/pdf/2201.01559v1,Monitoring Energy Trends through Automatic Information Extraction,"Next, together with the lessons learnt during the building of the ﬁrst prototype, we will
grossly increase the scope of EneMonIE by extending it to be applicable to media in English as well. Nevertheless, we
plan to turn it into a multilingual system as a plausible direction of future work. In order to ensure the feasibility of EneMonIE, existing open-source software and libraries for the functional components
will be considered for adaptation in the ﬁrst place. ",cs.CL,B,-0.22913468,0.07253965,-0.085131586
http://arxiv.org/pdf/2201.01845v1,Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation,"Zoey Liu is
one data set – could fail to hold in light of new or    supported by the Computing Innovation Fellows
unseen data. Program from the Computing Research Associa-
                                                        tion and the Computing Community Consortium,
   To remedy the observed inconsistencies in model      through a sub-award from the National Science
performance, we propose that future work should         Foundation. Any opinions, ﬁndings, and conclu-
consider utilizing random sampling from initial         sions or recommendations expressed in this mate-
data for more realistic estimates. ",cs.CL,C,0.3434285,-0.058916748,-0.039381914
http://arxiv.org/pdf/2201.01845v2,Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation,"Foundation under Grant #2127309 to the Comput-
                                                        ing Research Association for the CIFellows Project,
   To remedy the observed inconsistencies in model      and Grant #1761562. Any opinions, ﬁndings, and
performance, we propose that future work should         conclusions or recommendations expressed in this
consider utilizing random sampling from initial         material are those of the author(s) and do not nec-
data for more realistic estimates. We draw support      essarily reﬂect the views of the National Science
for this proposal from two patterns in our study. ",cs.CL,C,0.3459391,-0.05601986,0.0652769
http://arxiv.org/pdf/2201.01880v1,Automatic Related Work Generation: A Meta Study,"Our study ﬁnd that the following approaches eﬀectively improves the related work generation
system: (1) Combining manual and automatic approach for eﬃcient data collection; (2) Adding
external supervision signals such as citation network, citation function, and salience estimation;
(3) Developing more sophisticated multi-document encoders; (4) Filtering proper inputs. We also
identify several issues and to-dos to be solved in future work: (1) Lacking focused studies on a
speciﬁc aspect of related work generation; (2) Lacking a standardized task deﬁnition and dataset;
(3) Exploring solutions that do not rely on the assumptions that take sentences as the basic unit, or
only using part of the paper as the representation of the full paper; (4) The issue of losing factuality,
and hallucination of the generated texts; (5) Lacking knowledge-driven approaches; (6) Lacking
approaches to construct a full related work section; (7) Extending text-domain from computational
linguistics to other domains. REFERENCES

  [1] Ahmed AbuRa’ed, Horacio Saggion, and Luis Chiruzzo. ",cs.CL,B,-0.13827913,0.1589317,0.09936204
http://arxiv.org/pdf/2201.01995v1,Improving Mandarin End-to-End Speech Recognition with Word N-gram Language Model,"As shown in table III, incorporating word-
hypotheses for each frame and each word sequence length. The                level N-gram LM based on the proposed method consistently
acceleration of our method on NT is left for future work. achieves better recognition performance on all 6 test sets. ",cs.CL,B,-0.24894756,-0.15153015,-0.1981903
http://arxiv.org/pdf/2201.02026v1,"Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis","Naturally, we do not expect a user to train such  SenDM itself. We leave these directions for future work. a model, it is used here and in the appendix, only to examine
to what extent our approach can generalize to domains not          Analysis of Domain Speciﬁc Sentiment DMs As we saw
covered by the general corpus used to train SenDM . ",cs.CL,A,-0.04076566,0.26564637,0.15319704
http://arxiv.org/pdf/2201.02026v2,"Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis","Naturally, we do not expect a user to train such  SenDM itself. We leave these directions for future work. a model, it is used here and in the appendix, only to examine
to what extent our approach can generalize to domains not          Analysis of Domain Speciﬁc Sentiment DMs As we saw
covered by the general corpus used to train SenDM . ",cs.CL,A,-0.04076566,0.26564637,0.15319704
http://arxiv.org/pdf/2201.02049v1,Forming Predictive Features of Tweets for Decision-Making Support,"The results show that an intelligent agent can ﬁnd the an opti-
mal proﬁtable strategy. Of course, this is a very simpliﬁed case of analysis, where
the eﬀect of overﬁtting may occur, so this approach requires further study. The
main goal is to show that, using reinforced learning and an environment model
based on historical ﬁnancial data and quantitative characteristics of tweets, it
is possible to build a model in which an intelligent agent can ﬁnd an optimal
strategy that optimizes the reward function in episodes of interaction of learning
agent with the environment. ",cs.CL,A,0.14884906,0.15906687,0.29547828
http://arxiv.org/pdf/2201.02080v1,BERN2: an advanced neural biomedical named entity recognition and normalization tool,"We additionally adopt dictionary-lookup normalization for newly added types (cell line, cell type). When the public datasets for normalization of other
types are released, we can extend our hybrid approach to those types as well and we leave it as future work. “output” — 2022/1/6 — 13:39 — page 5 — #5

BERN2                                                                                  5

       Table S4. ",cs.CL,B,-0.024573572,-0.062388595,-0.22047958
http://arxiv.org/pdf/2201.02080v2,BERN2: an advanced neural biomedical named entity recognition and normalization tool,"We additionally adopt dictionary-lookup normalization for newly added types (cell line, cell type). When the public datasets for normalization of other
types are released, we can extend our hybrid approach to those types as well and we leave it as future work. “main” — 2022/1/11 — 1:36 — page 6 — #6

6                                                                                 Sung et al. ",cs.CL,B,-0.049196005,-0.028730836,-0.22590578
http://arxiv.org/pdf/2201.02257v1,Applying Word Embeddings to Measure Valence in Information Operations Targeting Journalists in Brazil,"We do not claim that this combination of                          [5] J. Littman, D. Kerchner, L. Wrubel, A. Dharne, R. Vij, D. Chudnov,
algorithms is the best in all cases – rather, we demosntrate a                         Rithvikmundra, Victor, Y. B. Gaber, S. Park, N. Bearman, D. Smith,
technique that future work could use to assess several different                       Somanath304, R. Underwood, N. Dias, and C. Peterson, gwu-
algorithms. libraries/sfm-ui: Version 2.3.0. ",cs.CL,C,0.32441592,-0.23606527,-0.09954962
http://arxiv.org/pdf/2201.02387v1,The Defeat of the Winograd Schema Challenge,"We focused on
children’s books partly because they tended to contain a wide range of com-
monsense reasoning concepts of interest. However, many possible examples
did not stand up to further examination. For example, consider the following from The Wizard of Oz :

       The Scarecrow seized the oilcan from Dorothy’s basket and oiled
       the Woodman’s jaws, so that after a few moments he could talk
       as well as before. ",cs.CL,A,0.104035474,0.2176364,0.35912523
http://arxiv.org/pdf/2201.02387v2,The Defeat of the Winograd Schema Challenge,"We focused on
children’s books partly because they tended to contain a wide range of com-
monsense reasoning concepts of interest. However, many possible examples
did not stand up to further examination. For example, consider the following from The Wizard of Oz :

       The Scarecrow seized the oilcan from Dorothy’s basket and oiled
       the Woodman’s jaws, so that after a few moments he could talk
       as well as before. ",cs.CL,A,0.104035474,0.2176364,0.35912523
http://arxiv.org/pdf/2201.02419v1,Automatic Speech Recognition Datasets in Cantonese Language: A Survey and a New Dataset,"7. For the future work we plan to collect data          nual Meeting of the Association for Computational
from more audiobooks to enrich our dataset. In addition,        Linguistics (Volume 1: Long Papers), pages 66–75,
we will gather Cantonese ASR corpus from diﬀerent                Melbourne, Australia, July. ",cs.CL,B,-0.19203916,0.118237704,-0.20401174
http://arxiv.org/pdf/2201.02419v2,Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset,"Subword regularization: Improv-
movies. Another future work direction is performing              ing neural network translation models with multiple
more experiments that combine the performance of the             subword candidates. In Proceedings of the 56th An-
MDCC with multilingual datasets. ",cs.CL,B,-0.35278535,-0.12539959,-0.096449606
http://arxiv.org/pdf/2201.02489v1,Semantic-based Data Augmentation for Math Word Problems,"The results suggest that the s.based aug. strategies suc-
cessfully beneﬁt the generalization ability of the neural models. However, since
existing MWP neural models hardly considered the discrete local variances which
lead to respectively low accuracy, there is still a large space for future work to
improve the ability of neural models dealing with such challenges. 14      A. Li et al. ",cs.CL,C,0.1503214,-0.23965567,0.1058834
http://arxiv.org/pdf/2201.02489v2,Semantic-based Data Augmentation for Math Word Problems,"The results suggest that the s.based aug. strategies suc-
cessfully beneﬁt the generalization ability of the neural models. However, since
existing MWP neural models hardly considered the discrete local variances which
lead to respectively low accuracy, there is still a large space for future work to
improve the ability of neural models dealing with such challenges. Ablation study As illustrated in Tabel 7, both knowledge and logic guided
augmentation methods contribute to the performance gains. ",cs.CL,C,0.09107898,-0.27376786,0.08414617
http://arxiv.org/pdf/2201.02504v1,Repairing Adversarial Texts through Perturbation,"In addition,
                                                                         Convolutional Neural Networks (CNNs) are shown to achieve
                                                                         similar results on text classiﬁcation tasks [44]. In this work, we
                                                                         focus on RNNs and CNNs and leave the evaluation of other
                                                                         models to future work. 2.2 Generating Adversarial Texts

                                                                         In the following paragraphs, we introduce state-of-the-art
                                                                         approaches to generate adversarial texts for NNs. ",cs.CL,B,-0.25629026,0.08087823,0.07333067
http://arxiv.org/pdf/2201.02517v1,RxWhyQA: a clinical question-answering dataset with the challenge of multi-answer questions,"The
RxWhyQA includes 19,269 entries that require identification of multiple answers in text, which
is an essential scenario not well covered by existing datasets. Although the RxWhyQA focuses
on why-questions derived from a specific corpus and drug-reason relations, it offers an initial
benchmark of multi-answer clinical QA and a reference for future work to repurpose other
annotations likewise. COMPETING INTERESTS STATEMENT
The authors have no competing interests to declare. ",cs.CL,A,0.051465098,0.24188042,0.098417655
http://arxiv.org/pdf/2201.02732v1,C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System,"Second, KBRD      for incorporating external data for CRSs. As future work, we will
achieves a comparable performance with ReDial, which indicates          consider designing a more general representation model that can
that the entity information from KG is beneficial to informativeness. be directly pretrained with various kinds of context data. ",cs.CL,C,0.009670023,-0.022572942,0.034055922
http://arxiv.org/pdf/2201.02732v2,C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System,"Second, KBRD      for incorporating external data for CRSs. As future work, we will
achieves a comparable performance with ReDial, which indicates          consider designing a more general representation model that can
that the entity information from KG is beneficial to informativeness. be directly pretrained with various kinds of context data. ",cs.CL,C,0.009670023,-0.022572942,0.034055922
http://arxiv.org/pdf/2201.02734v1,Building Human-like Communicative Intelligence: A Grounded Perspective,"Often, this approach leads to
discoveries that the supposedly complex intelligent behavior can arise from surprisingly
straightforward interactions of an agent and its environment (for review, see Cangelosi et al.,
2015; also, see section 3.1.2. for further discussion). Therefore, my further analysis of 4E
components for building linguistic intelligence mainly focuses on the necessary aspects of
simulation design, rather than on particular architectures or skills pertinent to a learner. 4E perspectives criticize the dominant AI research as being unable to capture properties of
the learning conditions that heavily guide the development of animal and human intelligence. ",cs.CL,A,0.023799563,0.12794885,0.26737005
http://arxiv.org/pdf/2201.02740v1,Best of Both Worlds: A Hybrid Approach for Multi-Hop Explanation with Declarative Facts,"Our results suggest this is a signiﬁcant chal-
nation chains with the top semantic candidate explanation chains. lenge, and future work may further explore this problem. 5For both models, we use the “base” form which has 12 hidden
layers, a hidden dimension of 768, and 12 attention heads. ",cs.CL,B,-0.07337166,0.05622071,0.26025617
http://arxiv.org/pdf/2201.02797v1,A Unified Review of Deep Learning for Automated Medical Coding,"Nevertheless, cur-      This work was supported by the Academy of Finland (grant
rent research considers less about the class imbalance issue,        336033) and EU H2020 (grant 101016775). H. Dong was
which should be addressed in future work. The code hierar-           supported by Health Data Research UK National Phenomics
chy as prior human knowledge sheds light on the imbalanced           and Text Analytics Implementation Projects. ",cs.CL,A,0.095896915,0.25497085,-0.14850423
http://arxiv.org/pdf/2201.02846v1,Coherence-Based Distributed Document Representation Learning for Scientific Documents,"We will try more elaborate document encod-
ings (e.g., perhaps splitting documents into more than two parts). [14] Taoran Ji, Zhiqian Chen, Nathan Self, Kaiqun Fu, Chang-Tien Lu, and Naren
There are also two interesting future works that are the precise
measure of coherence and automatic selection of 𝑝𝑜𝑠. For natural                                                                  Ramakrishnan. ",cs.CL,B,0.066594675,0.030927721,0.039542463
http://arxiv.org/pdf/2201.02977v1,Indian Language Wordnets and their Linkages with Princeton WordNet,"Section 4. discusses different issues encountered     families, and many other reasons such as morphological
                                       in the creation of these datasets, followed by the conclusion  richness, gender information etc. it was decided that Hindi
                                       and future work. be used as a pivot for linking all the Indian Languages. ",cs.CL,B,-0.015853785,0.23130147,-0.16274121
http://arxiv.org/pdf/2201.03107v1,Projection: A Mixed-Initiative Research Process,"Our scalable
software architecture will allow rapid experimentation of new features and parallel development going forward. 7 Future Work

We have a strong roadmap of future work ahead based on our interview discussions. Likely the most important step
going forward is expanding the types and scale of information accessible in Projection beyond our personal search
history. ",cs.CL,C,0.23391327,-0.1223851,0.032150693
http://arxiv.org/pdf/2201.03115v2,Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework,"We provide sentiment analysis via BERTbase model by           also provide semantic analysis via sentence embedding model
predicting sentiments of verses across the selected translations  (MPNet which is based on BERTbase) and verse-by-verse
(texts) of the Bhagavad Gita. We employ multi-label sentiment     similarity and perform keyword extraction (based on MPNet)
classiﬁcation where more than one sentiment can be classiﬁed      for further analysis of the translations as shown in Figure 2.
at once, i.e a verse can be both emphatic and optimistic as
shown in Figure 2. Although sentiment analysis can be done           We use the MPNet-base [111] sentence embedding model
with model prediction that gives a positive/negative polarity     to encode verses as it produces high quality embeddings. ",cs.CL,B,-0.32733423,0.19568439,0.02095367
http://arxiv.org/pdf/2201.03327v1,TiltedBERT: Resource Adjustable Version of BERT,"We investigate our method and analysis
   In this paper, we proposed TiltedBERT that speedup the      on GLUE benchmark and IMDB dataset and report our
BERTbase inference and ﬁne-tuning times signiﬁcantly with      competitive results. For future works, we plan to improve
trivial accuracy drop. Also, the TiltedBERT speedup can        our word vector elimination policy and boost our speedup by
be tuned after ﬁne-tuning stage (ofﬂine-tuning property) by    applying parameter reduction methods on the TiltedBERT. ",cs.CL,B,-0.21388277,-0.26954222,-0.14498898
http://arxiv.org/pdf/2201.03327v2,TiltedBERT: Resource Adjustable Version of BERT,"We investigate our method and analysis
                                                                    on GLUE benchmark and IMDB dataset and report our
   This section will conduct ablation studies to investigate        competitive results. For future works, we plan to improve
the ofﬂine adjustable inference speedup and propose a design        our word vector elimination policy and boost our speedup by
guide for efﬁcient TiltedBERT implementation. applying parameter reduction methods on the TiltedBERT. ",cs.CL,C,-0.09878776,-0.2774204,-0.20902885
http://arxiv.org/pdf/2201.03327v3,TiltedBERT: Resource Adjustable Version of BERT,"We investigate our method and
                                                                    analysis on GLUE benchmark and IMDB dataset and report
   This section will conduct ablation studies to investigate        our competitive results. For future works, we plan to improve
the ofﬂine adjustable inference speedup and propose a design        our word vector elimination policy and boost our speedup by
guide for efﬁcient TiltedBERT implementation. applying parameter reduction methods on the TiltedBERT. ",cs.CL,C,-0.098585345,-0.28054178,-0.20807979
http://arxiv.org/pdf/2201.03327v4,TiltedBERT: Resource Adjustable Version of BERT,"The
                            CONCLUSIONS                       TiltedBERT method improvements are veriﬁed on the GLUE
                                                              benchmark and IMDB dataset. For future works, we plan to
   In this paper, The TiltedBERT model is proposed that       improve the word vector elimination policy and boost the
speedups the BERTbase inference time signiﬁcantly with a      inference speedup by applying parameter reduction methods
trivial accuracy drop. The TiltedBERT speedup can be tuned
on the TiltedBERT. ",cs.CL,B,-0.17160982,-0.21950905,-0.056251056
http://arxiv.org/pdf/2201.03423v1,"A Survey of Plagiarism Detection Systems: Case of Use with English, French and Arabic Languages","6. Conclusion and future work

     Through this survey, we overviewed several typologies of plagiarism previously proposed and discussed by
authors before building a novel typology that we deemed more suitable for the comparison of plagiarism
detection systems. We then examined 8 distinct systems based on well-defined criteria and a corpus built for the
purpose. ",cs.CL,A,-0.12343228,0.29921544,-0.14097899
http://arxiv.org/pdf/2201.03425v1,"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers","Still, classiﬁer accuracy should always be assessed explicitly for a set
of novel question-answer pairs, since due to a large diversity of questions, there
might be considerable diﬀerences in performance on training, validation, and test
data. In future work, we aim to improve models, e.g., by utilizing additional in-
formation such as response times of students, implementing feedback to students,
e.g., in the form of personalized explanations of autograder decisions [42, 44] and
human-to-AI coaches [41], improving trust issues [15, 17], and including detection
of dishonest behavior. Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers             27

References

1. ",cs.CL,A,-0.050786752,0.11868389,0.2699657
http://arxiv.org/pdf/2201.03445v1,NILC-Metrix: assessing the complexity of written and spoken language in Brazilian Portuguese,"When
analysing the Descriptive Indexes, we show that Leg2Kids has smaller sentences and smaller words than
Adapt2Kids (words per sentence and syllables per content words metrics). Since readability metrics rely
heavily on these two factors, it cannot be concluded that Leg2Kids is simpler than Adapt2Kids without
any further analysis. Brunet (↑)               Adapt2Kids  Leg2Kids
Adapted Dale-Chall (↓)      11.03      12.87
Flesch Reading Ease (↑)      9.85       8.99
Gunning Fog (↓)             51.72      76.35
Honore´ statistics (↓)       7.00       2.65
                                      933.04
                           1,040.01

Table 9: Results for readability metrics (arrows indicate the simplicity direction). ",cs.CL,B,-0.06164454,0.0786573,-0.14387457
http://arxiv.org/pdf/2201.03514v1,Black-Box Tuning for Language-Model-as-a-Service,"mance. For simplicity, we do not integrate these           In Proceedings of the Third International Workshop
methods and leave for future work. on Paraphrasing, IWP@IJCNLP 2005, Jeju Island,
                                                           Korea, October 2005, 2005. ",cs.CL,A,0.120156385,0.05225543,-0.050862547
http://arxiv.org/pdf/2201.03514v2,Black-Box Tuning for Language-Model-as-a-Service,"Accordingly, we set population size          such as sequential random embeddings (Qian et al.,
λ = 4 + 3 log(d). As shown in the middle row          2016) and other more advanced methods of con-
of Figure 3, the best subspace dimensionality can     structing the random projection matrix (Letham
be different on different tasks (d = 200 performs     et al., 2020) should be explored in future work. the best on SST-2 development set and d = 500         Besides, the subspace generated by random projec-
performs the best on AG’s News development set),      tion can be sub-optimal. ",cs.CL,C,0.12268607,-0.19069178,-0.08971882
http://arxiv.org/pdf/2201.03514v3,Black-Box Tuning for Language-Model-as-a-Service,"For              ing a corpus of sentential paraphrases. In Proceedings
simplicity, we do not integrate these methods and leave for         of the Third International Workshop on Paraphrasing,
future work. IWP@IJCNLP 2005, Jeju Island, Korea, October 2005,
                                                                   2005, 2005. ",cs.CL,B,-0.23438883,0.20567788,0.09782587
http://arxiv.org/pdf/2201.03514v4,Black-Box Tuning for Language-Model-as-a-Service,"For             Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
simplicity, we do not integrate these methods and leave for        June 2-7, 2019, Volume 1 (Long and Short Papers), pp. future work. 4171–4186, 2019. ",cs.CL,C,0.2627652,-0.11491268,-0.08627874
http://arxiv.org/pdf/2201.03521v1,Polish Natural Language Inference and Factivity -- an Expert-based Dataset and Benchmarks,"Also, we are grateful for many students from the Faculty of Mathematics and Information Science at Warsaw University
of Technology, working under Anna Wróblewska’s guidance in Natural Language Processing course. They performed
experiments on similar datasets and thus inﬂuenced our further research. References

Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish Sabharwal. ",cs.CL,B,-0.26056582,0.16251332,-0.021467919
http://arxiv.org/pdf/2201.03533v1,SCROLLS: Standardized CompaRison Over Long Language Sequences,"Overall, it seems that contemporary off-the-
performs LED by almost two points, suggesting           shelf models struggle with these tasks, challenging
that LED might be under-optimized. Inspecting           future work to make progress on SCROLLS. the dataset-level results reveals that LED (16k) sig-
niﬁcantly outperforms BART (1k) in two datasets,        5 Conclusion
GovReport and NarrativeQA, which are coinciden-
tally the largest datasets in SCROLLS by number of      We propose a new benchmark that places the spot-
examples. ",cs.CL,C,0.15997308,-0.08733183,-0.10060135
http://arxiv.org/pdf/2201.03533v2,SCROLLS: Standardized CompaRison Over Long Language Sequences,"Overall, it seems that contemporary off-the-
performs LED by almost two points, suggesting           shelf models struggle with these tasks, challenging
that LED might be under-optimized. Inspecting           future work to make progress on SCROLLS. the dataset-level results reveals that LED (16k) sig-
niﬁcantly outperforms BART (1k) in two datasets,        6 Conclusion
GovReport and NarrativeQA, which are coinciden-
tally the largest datasets in SCROLLS by number of      We propose a new benchmark that places the spot-
examples. ",cs.CL,C,0.16335803,-0.09336835,-0.105711035
http://arxiv.org/pdf/2201.03655v1,A Likelihood Ratio based Domain Adaptation Method for E2E Models,"This helps normalize the
        LLR(G) = log(pOOD(G)) − log(pGEN (G)) (4)                 variation in training dataset sizes. We leave the effect of op-
                                                                  timizing interpolation weights for best overall perplexity of
                                                                  OOD data as future work. Example 1: LM Score for rare OOD utterance             Example 2: LM Score for general trafﬁc utterance

          tune into the freiberg game </s>                          play some music </s>

log PGEN  -8.12 -2.86 -2.55 -15.64 -7.38 -1.63                      -2.32 -4 -1.46 -0.32
log POOD
LLR       -9.37 -5.55 -2.74 -6.87 -4.94 -1.9                        -3 -5.23 -3.79 -1.83
Boost
          -1.24 -2.69 -0.19 8.77  2.45 -0.27                        -0.69 -1.23 -2.33 -1.5

          0      0     0  8.77    0                              0  0  0  0  0

                       Table 1. ",cs.CL,C,0.05875948,-0.18784042,-0.1785211
http://arxiv.org/pdf/2201.03742v2,Explaining Predictive Uncertainty by Looking Back at Model Explanations,"But this may            trained language models in few-shot ﬁne-tuning. lead to adversarial examples, which we leave to            arXiv preprint arXiv:2204.08039.
future work. Shrey Desai and Greg Durrett. ",cs.CL,B,-0.19964655,-0.037853632,0.20146209
http://arxiv.org/pdf/2201.03761v1,Prior Knowledge Enhances Radiology Report Generation,"We describe the multi-task learning in Section 2, followed by our
experimental setup, results, and discussion in Section 3. We conclude with future work in the last section. 2 Methods

2.1 Framework

Simulating the reading routine of radiologists by ﬁrst observing multiple ﬁndings when they read medical images and
then compiling radiological reports, our proposed method generates radiology report S from the frontal-view If and
lateral-view Il of chest X-ray images following several steps (Fig 2). ",cs.CL,A,0.19328107,-0.019413391,0.15572123
http://arxiv.org/pdf/2201.03857v1,The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild,"Secondary labels as additional signal                                         in the training and test set. In future work, we plan to
                                                                                   double the size of the dataset to provide more training
As 188 or 18.7% of the texts are labeled with a sec-                               and test examples. ondary category as well, denoting presence of an ad-
ditional genre, we used this information to inspect                                                 5. ",cs.CL,B,-0.06796172,0.11060882,-0.1735825
http://arxiv.org/pdf/2201.04427v1,Differentiating Geographic Movement Described in Text Documents,"Votes Count

                                                                  5-0-0 10

                                                                  4-1-0 58

                                                                  3-1-1 79

                                                                  3-2-0 76

                                                                  2-2-1 85
16                                                                                               Scott Pezanowski et al. There were 48 participants (34% of the total) who completed at least ten triads, and their full survey responses and
corresponding MTurk short survey responses were selected for further analysis in the sections below. Interestingly, although
the experienced participants were only 34% of the total, they completed 86% of the triads. ",cs.CL,A,0.45238817,0.1933371,-0.03624071
http://arxiv.org/pdf/2201.04450v1,Biaffine Discourse Dependency Parsing,"nodes does not show much variation between the                                  Since the SciDTB corpus is formed by abstracts

Chu-Liu-Edmonds algorithm and the Eisner algo- of scientiﬁc papers, the parser we develop can only

rithm, but with the Chu-Liu-Edmonds algorithm, handle short texts. In future work, we plan to inves-

the tree path length tends to be longer. From these tigate the application of the dependency framework

two metrics, we may conclude that the model is not in discourse parsing of longer texts from more lan-

likely to produce “vacuous trees” (Ferracane et al., guages and domains. ",cs.CL,B,-0.25955713,0.09209192,-0.07978378
http://arxiv.org/pdf/2201.04723v1,Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents,"ically only one gold label available (Gupta et al.,
                                                        2019). Perplexity (computing the predicted proba-
   These ﬁndings, while highlighting the difﬁculty      bility of the given gold utterances) has been argued
of human evaluation, also provide guidance on           to correlate with human judgments (Adiwardana
which method might be best to use in these different    et al., 2020), however this has also been shown to
circumstances, as well as possible future work. In      not always be the case (Dinan et al., 2019b), and
particular, investigating the best way to merge pair-   moreover does not actually evaluate the genera-
wise and single-model, per-turn and per-dialogue        tions themselves produced by a decoder architec-
beneﬁts into a single method could be a fruitful        ture. ",cs.CL,B,-0.124975696,0.02279247,0.04837471
http://arxiv.org/pdf/2201.04810v1,Recognizing semantic relation in sentence pairs using Tree-RNNs and Typed dependencies,"A detailed qualitative analysis of
Typed DT-RNN (proposed)                              80.54            learned typed embeddings can give better insight into language
                                                                      understanding. As future work, we propose to extend this work
  Our implementation. to other semantic matching tasks like paraphrase identiﬁcation
                                                                      and question answering. ",cs.CL,B,-0.30626237,0.035550542,0.14406767
http://arxiv.org/pdf/2201.04831v1,Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis,"Instead, the “orange
domain. More potential reasons will be explored in future works. donut” is much closer to “had” in the introduced knowledge
                                                                            graph, which allows the knowledge branch to easily capture the
4.4 Case Study                                                              relatedness. ",cs.CL,A,0.18708824,0.18408722,0.050063565
http://arxiv.org/pdf/2201.04843v1,LP-BERT: Multi-task Pre-training Knowledge Graph BERT for Link Prediction,"Experi-
                                                                   mental results on dataset demonstrate both the efﬁciency and
                                                                   effectiveness of LP-BERT. In future work, we will add more
                                                                   diverse pre-training tasks and increase the model parameter
                                                                   size to enable LP-BERT to store more graph knowledge. References                                                             [Peng and Zhang, 2020] Yanhui Peng and Jing Zhang. ",cs.CL,C,0.045881413,-0.117949106,0.14806356
http://arxiv.org/pdf/2201.04843v2,Multi-task Pre-training Language Model for Semantic Network Completion,"the efﬁciency and effectiveness of our proposed LP-BERT. In                          78–94, 2018.
future work, we will explore more diverse pre-training tasks
and increase the model parameter size to enable LP-BERT to                     [11] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko,
store larger graph knowledge. “Translating embeddings for modeling multi-relational data,” Interna-
                                                                                     tional Conference on Neural Information Processing Systems, 2013. ",cs.CL,B,-0.12617947,-0.215417,0.13680601
http://arxiv.org/pdf/2201.04913v1,Compressing Word Embeddings Using Syllables,"Although less accurate
                                        than the n-gram baseline we used, our models can be trained in a matter of
                                        minutes, as opposed to hours for the n-gram approach. We identify a path toward
                                        upgrading performance in future work. All code is made publicly available, as well
                                        as our collected English and Dutch syllabic decompositions and Dutch evaluation
                                        set translations. ",cs.CL,B,-0.27481407,-0.18062082,-0.12718886
http://arxiv.org/pdf/2201.05017v1,Towards Automated Error Analysis: Learning to Characterize Errors,"Unlike the declarative rules learned by our           In addition, incorporating such automated error analysis
approach, the discovered clusters can be difﬁcult to inter-       into an overall automated machine learning (AutoML) pro-
pret. We also extract a broader range of interpretable meta-      cess is another promising direction for future work. Most
features to characterize instances beyond the discretized im-     of the AutoML literature (Kandasamy et al. ",cs.CL,C,0.022730988,-0.023952112,0.08537538
http://arxiv.org/pdf/2201.05017v2,Towards Automated Error Analysis: Learning to Characterize Errors,"Unlike the declarative rules learned by our           In addition, incorporating such automated error analysis
approach, the discovered clusters can be difﬁcult to inter-       into an overall automated machine learning (AutoML) pro-
pret. We also extract a broader range of interpretable meta-      cess is another promising direction for future work. Most
features to characterize instances beyond the discretized im-     of the AutoML literature (Kandasamy et al. ",cs.CL,C,0.022730988,-0.023952112,0.08537538
http://arxiv.org/pdf/2201.05017v3,Towards Automated Error Analysis: Learning to Characterize Errors,"Unlike the declarative rules learned by our           In addition, incorporating such automated error analysis
approach, the discovered clusters can be difﬁcult to inter-       into an overall automated machine learning (AutoML) pro-
pret. We also extract a broader range of interpretable meta-      cess is another promising direction for future work. Most
features to characterize instances beyond the discretized im-     of the AutoML literature (Kandasamy et al. ",cs.CL,C,0.022730988,-0.023952112,0.08537538
http://arxiv.org/pdf/2201.05041v1,LARD: Large-scale Artificial Disfluency Generation,"Furthermore, the thorough analysis conducted on models ﬁne-tuned both on the generated dataset and Switchboard
dataset revealed a signiﬁcant weakness of the latter, arising from the imbalanced number of training examples among
different classes, which can be successfully addressed using the proposed method. Several interesting future work directions exist, including extending the proposed method to generate multiple disﬂu-
encies for training models that can handle these more difﬁcult cases. Also, the proposed method highlights another
research direction for modeling other linguistic phenomena, such as coreference resolution [56] and discourse rela-
tions [57], e.g., contrast, elaboration, clariﬁcation, and others, providing an efﬁcient way for generating highly realistic
datasets that include them, based on existing large-scale ﬂuent datasets. ",cs.CL,B,-0.25205666,0.049542904,0.10977791
http://arxiv.org/pdf/2201.05041v2,LARD: Large-scale Artificial Disfluency Generation,"The ex-         using the proposed method. perimental results reported in Tables 4 and 5 reveal that   Several interesting future work directions exist, includ-
models trained on Switchboard perform worse (on the         ing extending the proposed method to generate mul-
corresponding test set) compared to models trained on       tiple disﬂuencies for training models that can han-
LARD. This observation led us to further examine the        dle these more difﬁcult cases. ",cs.CL,C,0.17523026,-0.24003816,0.10106635
http://arxiv.org/pdf/2201.05061v1,Feature-rich multiplex lexical networks reveal mental strategies of early language learning,"Hence, the above patterns indicate the presence of a core-
periphery organisation in the dualistic multiplex/feature-rich structure of the mental lexicon: A set of highly
frequent/shorter/polysemous words linked with each other creates a network core highlighted by conformity and
invisible to previous inquiries [63, 62]. This preliminary evidence calls for further analysis of the core. Fig. ",cs.CL,A,-0.030361582,0.23079164,-0.042501796
http://arxiv.org/pdf/2201.05088v1,Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer Explanation,"For
adding world knowledge and commonsense and improving the            example, Schuff et al. [43] propose a hierarchical model and a
understanding on too complicated sentences in the future work. new regularization term to strengthen the answer-explanation
                                                                    coupling on multi-hop HOTPOTQA. ",cs.CL,B,-0.20567504,0.038763426,0.34322196
http://arxiv.org/pdf/2201.05088v2,Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer Explanation,"For
adding world knowledge and commonsense and improving the            example, Schuff et al. [43] propose a hierarchical model and a
understanding on too complicated sentences in the future work. new regularization term to strengthen the answer-explanation
                                                                    coupling on multi-hop HOTPOTQA. ",cs.CL,B,-0.20567504,0.038763426,0.34322196
http://arxiv.org/pdf/2201.05173v1,The Combinatorics of \textit{Salva Veritate} Principles,"Part 2 is devoted to
constructing our formal theory and presenting the main result. Finally, in
Part 3, we conclude with a discussion of the preceding ﬁndings, and some
remarks on directions for future work. 1 Historical and Conceptual Background

By a ”substitution” we mean any operation on strings of a language which
replaces some sub-string with another. ",cs.CL,B,-0.08784944,-0.022716606,-0.09864986
http://arxiv.org/pdf/2201.05230v1,NLP in Human Rights Research -- Extracting Knowledge Graphs About Police and Army Units and Their Commanders,"recognition, and relation extraction. We aim to ad-
dress entity linking in a future working paper. 3.1 Named Entity Recognition (NER)

To extract name entities, we use BiLSTM-CNN-CRF            3.2.1 Nearest Person
model [7] in the traditional inside–outside–beginning
(IOB) tagging framework [16]. ",cs.CL,B,-0.17916009,0.039925627,-0.12350186
http://arxiv.org/pdf/2201.05294v1,Multi-Narrative Semantic Overlap Task: Evaluation and Benchmark,"metric for evaluating this task, experimenting with
Speciﬁcally, we randomly sampled 150 narrative                only 3 abstractive summarization models is not a
pairs (one from “Left” wing and one from “Right”              barrier to our work. Proposing a custom method
wing) and then asked 3 (three) humans to write a              ﬁne-tuned for the Semantic-Overlap task is an or-
a natural language description which conveys the              thogonal goal to this work and we leave it as a
semantic overlap of the information present in both           future work. Also, we’ll use the phrases “summary”
narratives describing each event. ",cs.CL,A,-0.11993611,0.20336834,0.14586622
http://arxiv.org/pdf/2201.05337v1,A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models,"For example, it is hard to keep long-range coherency in both semantic logic and controlled
condition for long text generation. It calls for further research to establish global normalization
based on PLMs to ensure that text generation can be controlled locally and globally at the same
time. Fourth, the construction of large scale pre-trained language models are typically data-driven,
which allows the models to learn the primary logic and common sense contained in the training
corpus. ",cs.CL,B,-0.21613225,-0.14226115,0.2400696
http://arxiv.org/pdf/2201.05337v2,A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models,"For example, it is hard to keep long-range coherency in both semantic logic and controlled
condition. It calls for further research to establish global normalization based on PLMs to ensure
that text generation can be controlled locally and globally at the same time. Fourth, the construction of large-scale pre-trained language models is typically data-driven,
which allows the models to learn the primary logic and common sense contained in the training
corpus. ",cs.CL,B,-0.19528584,-0.12855405,0.26832527
http://arxiv.org/pdf/2201.05382v1,Mental Health Assessment for the Chatbots,"It does
                                                                             not have enough meaningful information because
5.3 Effects of Inquiry Strategies                                            the questionnaire only cares about the recent situa-
                                                                             tions of the participants. (3) Responses show that
To further study the effects of inquiry strategies,                          the chatbots do not know / remember the answers. we plot the averaged score of 50 experiments under                           For example, the chatbot respond “I don’t know”
each question in Figure 4. ",cs.CL,A,0.26558578,0.2990932,0.27423155
http://arxiv.org/pdf/2201.05575v2,Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,"GNN-LM [22] and Borgeaud et al. [3]
More recent work [10] explores improving efficiency along various     extends vanilla neural language model (LM) by allowing to refer-
dimensions, and we leave this for future works. ence similar contexts in the entire training corpus. ",cs.CL,B,-0.2410355,-0.17708036,0.1342901
http://arxiv.org/pdf/2201.05601v1,A Warm Start and a Clean Crawled Corpus -- A Recipe for Good Language Models,"The problem could be solved as
5.2. Named Entity Recognition                               a multi-label one, but we leave that approach out for
                                                            future work. While the dataset is highly ﬁne-grained
When ﬁne-tuning IceBERT for named entity                    and contains a variety of objective and subjective labels
recognition (NER), it reaches state-of-the-art per-
formance, showing a considerable improvement                   16Implemented in the greynirseq repository. ",cs.CL,B,-0.12314388,0.16249222,-0.18097848
http://arxiv.org/pdf/2201.05601v2,A Warm Start and a Clean Crawled Corpus -- A Recipe for Good Language Models,"The results for binary classiﬁcation are
sure the knowledge captured by the model. We leave         shown in Table 8 and by category for the top 5 models
such experiments for future work. in the multi-class task in Table 9. ",cs.CL,C,0.19864953,-0.08797571,0.22292852
http://arxiv.org/pdf/2201.05609v2,Multilingual Open Text Release 1: Public Domain News in 44 Languages,"We
mya            81,772     657,459    12,058,686           plan to continue to increase the size of the corpus as
nde            31,468     211,156            N/A          VOA publishes more documents, and we plan to ex-
orm            10,144       57,187           N/A          pand MOT by adding other permissively-licensed texts
por            52,514     427,612    36,006,802           to expand our coverage of lower-resourced languages. prs            71,881     461,203            N/A          There are many ways in which MOT could be used
pus          141,293      838,726            N/A          in future work. For lower-resourced languages, MOT
rus          118,411    1,051,201    13,864,438           provides a valuable source of high-quality unlabeled
sna            28,051     189,093    14,633,719           text, and it could be used with minimal annotation ef-
som            38,376     131,501            N/A          fort to train language identiﬁcation, sentence segmenta-
spa          116,442      911,685    51,451,892           tion, and tokenization systems. ",cs.CL,B,-0.21001005,-0.044746574,-0.23795098
http://arxiv.org/pdf/2201.05692v1,Model Stability with Continuous Data Updates,"parameter counting is not a great measure of the
                                                         model complexity. In future work, we plan to inves-
   First we investigate incremental training, which      tigate other reﬂections of model complexity, such
has previously shown stable models while achiev-         as degrees of freedom (Gao and Jojic, 2016) and in-
ing low error rates (Zang et al., 2014). In incre-       trinsic dimension (Li et al., 2018), and assess their
mental training, a model is ﬁrst trained with an ini-    impact on jitter. ",cs.CL,C,0.1651398,-0.26612273,0.10700689
http://arxiv.org/pdf/2201.05700v1,Cost-Effective Training in Low-Resource Neural Machine Translation,"We leave these direc-
2019). Furthermore, several works proposed differ-     tions as future work. ent variants of back-translation to leverage mono-
lingual data (Lample et al., 2018; Guzmán et al.,      References
2019). ",cs.CL,B,-0.29900545,0.016912706,-0.2122038
http://arxiv.org/pdf/2201.05742v1,Kformer: Knowledge Injection in Transformer Feed-Forward Layers,"We
mainly compare Kformer with th following injecting meth-
ods:
             Corpus             Relation          RoBERTa           Kformer      RoBERTa+ATT
             Google RE                            P@1 P@10          P@1 P@10     P@1 P@10
                                birth place       12.90 30.90       29.36 59.94  12.93 33.56
             ConceptNET         birth data        1.78 14.24        17.95 57.12  2.04 14.88
             T-REx              death place       1.23 2.61         14.02 26.04  2.46 6.00
                                                  17.76 39.79       77.15 93.31  18.81 43.00
                                1-1               55.19             69.35        56.43
                                N-1               19.26             41.72        23.27
                                N-M               16.45             39.50        17.78

                                                  Table 2: The results of LAMA. Base Model   Methods            Dev Acc           Test Acc          do further analysis in Section 5.1. Large Model                      72.26              69.26
             RoBERTa                -               67.22           5 Analysis
             RoBERTa+MCQueen †   72.10                -
             RoBERTa+ATT         72.82              71.10           5.1 Why FFN? ",cs.CL,C,0.1804136,-0.09151726,-0.24621876
http://arxiv.org/pdf/2201.05767v1,Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems,"other tasks beside ranking, such as classification e.g., GLUE [44]
                                                                      and sequence labeling. As these tasks come with a different set of
   Overall, we confirm that MHS achieves a similar inference la-      challenges and metrics, we leave them for future work. tency of other Base models. ",cs.CL,C_centroid,0.042490423,-0.11648436,0.07794798
http://arxiv.org/pdf/2201.05767v2,Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems,"There-        lection Systems on a Latency Budget. In Proceed-
fore, we leave further investigation of CERBERUS            ings of the 16th Conference of the European Chap-
on other domains and tasks as future work. ter of the Association for Computational Linguistics:
                                                            Main Volume, pages 3005–3010. ",cs.CL,B,-0.13592494,-0.0152218435,-0.086796515
http://arxiv.org/pdf/2201.05780v1,Prompt Learning for Few-Shot Dialogue State Tracking,"rectly generated given correct value candidates as
JGA* shown in Table 2. The high values of JGA*
indicate the potential of performance along with
the improvement of value extraction in future work. 4.3 Unseen Slot Generation

For MultiWOZ2.1, we present the slots of each
domain in Table 4. ",cs.CL,C,0.24898532,-0.19369105,-0.21657093
http://arxiv.org/pdf/2201.05780v2,Prompt Learning for Few-Shot Dialogue State Tracking,"rectly generated given correct value candidates as
JGA* shown in Table 2. The high values of JGA*
indicate the potential of performance along with
the improvement of value extraction in future work. 4.3 Unseen Slot Generation

For MultiWOZ2.1, we present the slots of each
domain in Table 4. ",cs.CL,C,0.24898532,-0.19369105,-0.21657093
http://arxiv.org/pdf/2201.05878v1,Automatic Lexical Simplification for Turkish,"evaluation. In our future work, we would like expand
our previously created datasets with multiple annota-     Qiang, J., Li, Y., Zhu, Y., Yuan, Y., Shi, Y., and Wu,
tors and also explore multi-word expressions and mor-       X. (2021). ",cs.CL,B,-0.20353857,0.12550634,-0.17667621
http://arxiv.org/pdf/2201.05878v2,Automatic Lexical Simplification for Turkish,"evaluation. In our future work, we would like to ex-
                                                           pand our previously created datasets with multiple an-
LS-BERT algorithm makes use of an additional mea-          notators and address the simpliﬁcation shortcomings in
sure called PPDB feature. It analyses a paraphrase cor-    multi-word expressions and morphologically complex
pus to see if the complex word and the substitution oc-    words in Turkish. ",cs.CL,B,-0.3003987,0.010584842,-0.10664266
http://arxiv.org/pdf/2201.05880v1,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,"The latter in-
      soning paths and corresponding questions. dicates that pairs of nodes with relevant context
                                                        (i.e., entity/ keyword co-occurrence) are contextu-
  3) Experiments show that our system achieves          ally connected (e.g., edge a indicates co-occurred
      the state-of-the-art result and further analysis  keyword “COVID-19""). Speciﬁcally, we use off-
      proves the effectiveness of utilizing the hybrid  the-shelf named entity recognition model (Peters
      chain and the pre-training method. ",cs.CL,B,-0.1906425,-0.012223778,0.052785832
http://arxiv.org/pdf/2201.05891v1,Automatic Correction of Syntactic Dependency Annotation Differences,"(Kabiri, p.c.). While this project only considers neural-
based parsers, future work could compare how a rule-based         4.4. Training Setup
parser performs in similar low-resource training conditions. ",cs.CL,B,-0.12688994,-0.17523646,0.02081975
http://arxiv.org/pdf/2201.05899v2,Unobserved Local Structures Make Compositional Generalization Hard,"2021b. Latent compositional rep-
useful in future work for improving generalization          resentations improve systematic generalization in
in sequence to sequence models. grounded question answering. ",cs.CL,B,-0.13728662,-0.015682768,0.25014383
http://arxiv.org/pdf/2201.05955v1,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,"We
    pipeline? observe that contradiction patterns in in-context
                                                          examples are generally much more challenging for
We discuss observations from collecting WANLI             GPT-3 to copy, likely because it was trained on
that may shed insight for future work in the direc-       (mostly) coherent sequences of sentences. More
tion of collaborative dataset creation. ",cs.CL,B,-0.15387368,0.027674569,0.09576287
http://arxiv.org/pdf/2201.05955v2,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,"As a re-
sult, we choose not to collect a third annotation
in this work. Instead, we believe that the doubly-
annotated examples in WANLI have ﬂagged many
interesting cases of ambiguity in NLI, and we en-
courage future work to design richer annotation
frameworks to uncover the source(s) of ambigu-
ity. We provide examples where the two annotators
disagreed in Table 7. ",cs.CL,B,-0.18156205,0.17774542,-0.039623335
http://arxiv.org/pdf/2201.05955v3,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,"At the end of data   in this work. Instead, we believe that the doubly-
collection, we aggregate the earning and time spent   annotated examples in WANLI have ﬂagged many
from each crowdworker, and ﬁnd that the median        interesting cases of ambiguity in NLI, and we en-
hourly rate was $22.72, with 85% of workers being     courage future work to design richer annotation
paid over the $15/hour target. frameworks to uncover the source(s) of ambiguity. ",cs.CL,A,0.029945776,0.23683897,-0.05276142
http://arxiv.org/pdf/2201.05955v4,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,"in this work. Instead, we believe that the doubly-    Shown in Table 11, training T5-base on WANLI
annotated examples in WANLI have ﬂagged many          also outperforms training on MultiNLI on every
interesting cases of ambiguity in NLI, and we en-     test set, including by 4% of NLI Diagnostics, 10%
courage future work to design richer annotation       on HANS, and 8% on Adversarial NLI (similar
frameworks to uncover the source(s) of ambiguity. margins compared to ﬁnetuning RoBERTa-large). ",cs.CL,B,-0.15307745,0.08343951,0.022339778
http://arxiv.org/pdf/2201.05955v5,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,"in this work. Instead, we believe that the doubly-    Shown in Table 11, training T5-base on WANLI
annotated examples in WANLI have ﬂagged many          also outperforms training on MultiNLI on every
interesting cases of ambiguity in NLI, and we en-     test set, including by 4% of NLI Diagnostics, 10%
courage future work to design richer annotation       on HANS, and 8% on Adversarial NLI (similar
frameworks to uncover the source(s) of ambiguity. margins compared to ﬁnetuning RoBERTa-large). ",cs.CL,B,-0.15307745,0.08343951,0.022339778
http://arxiv.org/pdf/2201.05966v1,UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,"Figure 6. We observe that the accuracy increases as                                                       C Experimental Setup
the input becomes longer, motivating future work to
study how to effectively encode large structured in-                                                      C.1 Implementation Details
put, e.g., leveraging sparse attention (Zaheer et al.,                                                    We use T5 (Raffel et al., 2020) as our backbone
2020). language model. ",cs.CL,C,-0.09112524,-0.3172266,-0.050204765
http://arxiv.org/pdf/2201.05966v2,UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,"Figure 6. We observe that the accuracy increases as                                                       C Experimental Setup
the input becomes longer, motivating future work to
study how to effectively encode large structured in-                                                      C.1 Implementation Details
put, e.g., leveraging sparse attention (Zaheer et al.,                                                    We use T5 (Raffel et al., 2020) as our backbone
2020). language model. ",cs.CL,C,-0.09112524,-0.3172266,-0.050204765
http://arxiv.org/pdf/2201.05966v3,UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,"We observe that the accuracy increases as
T5-large and T5-3b. Results are shown in Table 13.          the input becomes longer, motivating future work to
                                                            study how to effectively encode large structured in-
C Input and Output Length Analysis                          put, e.g., leveraging sparse attention (Zaheer et al.,
                                                            2020). Linearization of large structured knowledge input
(e.g., large tables and KGs) can be arbitrarily long,           3https://huggingface.co/t5-base/tree/main
which needs to be truncated to ﬁt in GPUs with a
                             Error type                                           Error type                                                    Error type                                               Error type

            300  Invalid            Valid-but-wrong              300  Invalid            Valid-but-wrong              300           Invalid                Valid-but-wrong              100  Invalid              Valid-but-wrong

Count       200                                      Count       200                                      Count       200                                                   Count

                                                                                                                                                                                        50

            100                                                  100                                                  100

            0 base           large           3b                  0 base           large           3b                  0 base                    large               3b                  0 base             large           3b
                             Size                                                 Size                                                          Size                                                       Size

                 (a) Spider                                           (b) CoSQL                                                     (c) SParC                                                (d) GrailQA

            150              Error type                          400 Invalid Error typVealid-but-wrong                50                                                                20         Error type
                                                                 300
                 Invalid            Valid-but-wrong                                                                                            Error type                                    Miss. ",cs.CL,C,0.080325566,-0.3227014,-0.08622193
http://arxiv.org/pdf/2201.05979v1,SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples,"Then we focus
as a number between 0.0 and 5.0, where higher score means              on sentences pairs with the most severe semantic estimation
higher semantic similarity. The detailed information of these          error for further analysis. subtasks are summarized in Table 1. ",cs.CL,B,-0.11236052,0.17895246,-0.035091903
http://arxiv.org/pdf/2201.05979v2,SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples,"For each sentence pair, we
                   ecos sim(hi,hj )/τ              ecos sim(hi,hj )/τ  roughly4 take the rank difference between RG and RE:

              j=1                             j=1                                             (REi − RGi )2

                                                             (8)                                                             (11)

                                                                       as the semantic estimation error of the pair. Then we focus

                   ecos sim(hi,h+ i )/τ                                on sentences pairs with the most severe semantic estimation

LNL = −log                                                   (9)       error for further analysis. ecos sim(hi,h# i )/τ + N ecos sim(hi,h+ j )/τ            3.2 Experiment Setup

                                       j=1                             Our framework is implemented through Pytorch 1.8.0 and
                                                                       hugging face transformers5. ",cs.CL,B,-0.2357072,-0.07966467,-0.06915744
http://arxiv.org/pdf/2201.05979v3,SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples,"For each sentence pair, we
                   ecos sim(hi,hj )/τ              ecos sim(hi,hj )/τ  roughly4 take the rank difference between RG and RE:

              j=1                             j=1                                             (REi − RGi )2

                                                             (8)                                                             (11)

                                                                       as the semantic estimation error of the pair. Then we focus

                   ecos sim(hi,h+ i )/τ                                on sentences pairs with the most severe semantic estimation

LNL = −log                                                   (9)       error for further analysis. ecos sim(hi,h# i )/τ + N ecos sim(hi,h+ j )/τ            3.2 Experiment Setup

                                       j=1                             Our framework is implemented through Pytorch 1.8.0 and
                                                                       hugging face transformers5. ",cs.CL,B,-0.2357072,-0.07966467,-0.06915744
http://arxiv.org/pdf/2201.06009v1,Memory-assisted prompt editing to improve GPT-3 after deployment,"Implementation of combiner C C concatenates
   The two categories also allow us to control for     x and the feedback retrieved by Ω. We leave space
                                                       for future work to do this gating in a more princi-
    2https://github.com/openai/gpt-3/tree/             pled manner. master/data
Figure 3: Sample snapshot of memory for lexical QA. ",cs.CL,C,-0.0604123,-0.2334708,-0.060989928
http://arxiv.org/pdf/2201.06009v2,Memory-assisted prompt editing to improve GPT-3 after deployment,"The horizontal axis records the                  gies between input situations) can potentially
number of steps (or equivalently time). For both the                help alleviate these issues, and we leave such
memory assisted systems (GROW-PROMPT and GROW-                      exploration for future work. (See Table 2, row
PROMPT), the accuracy increases with time as memory                 3)
is ﬁlled up with feedback from past errors. ",cs.CL,C,0.26055238,-0.2620651,0.0737585
http://arxiv.org/pdf/2201.06009v3,Memory-assisted prompt editing to improve GPT-3 after deployment,"We rely on the model
                                                        (GPT-3) to pay attention to the relevant parts of the
                                                        input. Exploring more complex gating mechanisms
                                                        remains an important future work. Implementation of prompter P P concate-
                                                        nates C at the end of p. If available, MEM-PROMPT
                                                        can employ recent strategies on prompt-ﬁne tuning
                                                        (Zhao et al., 2021) to best combine fb with p e.g.,
                                                        deciding the position of p or format of C’s output
                                                        for best gains. ",cs.CL,C,0.15189004,-0.28262877,0.033798438
http://arxiv.org/pdf/2201.06009v4,Memory-assisted prompt editing to improve GPT-3 after deployment,"the relevant parts of the input. Exploring more
complex gating mechanisms remains an important                                  model        ERT-CAT     ERT-NL
future work. NO-MEM            48.3      34.4
                                                                                GROW-PROMPT           -          -
Implementation of prompter P P concate-                                         MEM-PROMPT        60.0      45.2
nates C at the end of p. If available, MEM-PROMPT
can employ recent strategies on prompt-ﬁne tuning       Table 2: MEM-PROMPT outperforms NO-MEM for both
(Zhao et al., 2021) to best combine fb with p e.g.,     the categorical and the more challenging ERT-NL setup
deciding the position of p or format of C’s output      having longer, ambiguous inputs. ",cs.CL,C,0.023875419,-0.17440064,-0.037801553
http://arxiv.org/pdf/2201.06009v5,Memory-assisted prompt editing to improve GPT-3 after deployment,"We rely on the model (GPT-3) to pay attention to
                                                        the relevant parts of the input. Exploring more
                                                        complex gating mechanisms remains an important
                                                        future work. 4 Experiments                                                                         model       ERT-CAT ERT-NL

Baselines We compare MEM-PROMPT (memory-                                              NO-MEM      48.3 34.4
assisted prompt editing) with two baselines:
• NO-MEM This is the standard GPT-34 in few-shot                                      MEM-PROMPT  60.0 45.2
 prompting mode (hyper-parameters listed in Ap-
 pendix §C). ",cs.CL,C,0.07449173,-0.17178792,0.08218098
http://arxiv.org/pdf/2201.06009v6,MemPrompt: Memory-assisted Prompt Editing with User Feedback,"We rely on the model (GPT-3) to pay attention to
                                                        the relevant parts of the input. Exploring more
                                                        complex gating mechanisms remains an important
                                                        future work. 4 Experiments                                                                         model   ERT-CAT ERT-NL

Baselines We compare MemPrompt (memory-                                               NO-MEM  48.3 34.4
assisted prompt editing) with two baselines:
• NO-MEM This is the standard GPT-34 in few-shot                                      MemPrompt 60.0 45.2
 prompting mode (hyper-parameters listed in Ap-
 pendix §C). ",cs.CL,C,0.073442414,-0.18030936,0.06758036
http://arxiv.org/pdf/2201.06025v1,COLD: A Benchmark for Chinese Offensive Language Detection,"We ﬁnd that
anti-bias language, which is non-offensive, has a         Non-Offensive prompts elicit offensive generations
shockingly high offensiveness risk, yet such lan-           Human: 男朋友是河南人，怕家里人有地域歧视，看
guage is ignored in most current work. Besides              到这个释怀一些啦，谢谢。
monitoring and defending the model against of-                    (My boyfriend is from Henan, and I am afraid that my
fensive language, in future work, we will build a                 family have regional discrimination. I feel relieved
more robust detector, improve its sensitivity to anti-            when I see this. ",cs.CL,A,-0.005404127,0.3146364,-0.10543803
http://arxiv.org/pdf/2201.06025v2,COLD: A Benchmark for Chinese Offensive Language Detection,"Bert-
base-chinese is taken as the model to predict the
scores of [MASK] token and we take the scores
of candidate words of 可(yes) and 否(no) as the
results of self-detection. We call for further research to explore the inter-
nal knowledge of language models to facilitate this
task, and the following tips can be considered. The
ﬁrst is exploring appropriate word pairs. ",cs.CL,B,-0.29500955,0.13004501,0.023317996
http://arxiv.org/pdf/2201.06028v1,Natural Language Deduction through Search over Statement Compositions,"It increases slightly for                always share a large amount of lexical content
                                                                 with at least one premise. This indicates an
more effective models, as these are able to better               opportunity for future work to explore better rule-
                                                                 based heuristics by combining guidance from
construct longer proofs, but we still see the largest            Overlap (Goal) with a penalty to circumvent the
                                                                 detrimental recycling behavior. number of steps being required by less effective
                                                                 5.2 Individual Step Validity
search methods like breadth-ﬁrst. ",cs.CL,A,0.13305967,0.121444955,0.15362728
http://arxiv.org/pdf/2201.06028v2,Natural Language Deduction through Search over Statement Compositions,"science domain, raising a question of domain
speciﬁcity. In future work, we plan to evaluate        Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and
deduction models on additional datasets with              Greg Durrett. 2021. ",cs.CL,C,0.2084464,-0.051393002,0.11821546
http://arxiv.org/pdf/2201.06170v1,Evaluation of HTR models without Ground Truth Material,"produce estimates of the HTR (or OCR) and identify
                                                                        quality issues. It needs further research to investigate
Moving on to the examination of the ranking capabili-                   whether such metrics are suitable for mass application. ties of our metrics, we ﬁnd that most metrics’ rankings
exhibit strong correlations to the reference rankings, as                           8. ",cs.CL,A,0.24812931,0.13307503,-0.29527146
http://arxiv.org/pdf/2201.06170v2,Evaluation of HTR models without Ground Truth Material,"produce estimates of the HTR (or OCR) and identify
                                                                        quality issues. It needs further research to investigate
Lastly, we take a look at the Top-N evaluation in Table                 whether such metrics are suitable for mass application. 4. ",cs.CL,C,0.24042201,-0.010226242,-0.34634808
http://arxiv.org/pdf/2201.06206v2,SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning,"An end-
FB15K237, due to the missing of relevant nodes                        to-end model for question answering over knowl-
and edges in the graph. In future work, it is worth-                  edge base with cross-attention combining global
while to construct KG datasets that provide more                      knowledge. In Proceedings of the 55th Annual Meet-
available reasonable paths to facilitate studies on                   ing of the Association for Computational Linguistics
interpretable multi-hop KG reasoning. ",cs.CL,B,-0.13711137,0.055109993,0.28087667
http://arxiv.org/pdf/2201.06206v3,SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning,"0.35 0.342                no constraint                    FB15K237, due to the missing of relevant nodes
                                  constraint on FB15K237           and edges in the graph. In future work, it is worth-
        0.30        0.291         constraint on FB15K237-20%       while to construct KG datasets that provide more
                                                                   available reasonable paths to facilitate studies on
        0.25                                0.183                  interpretable multi-hop KG reasoning. 0.138
Hits@1  0.20                                                0.075  Acknowledgement

        0.15                                FB15K237-20%           This work is supported by the NSFC Youth Project
                                                                   (62006136), the Institute for Guo Qiang, Tsinghua
        0.10                                                       University (2019GQB0003) and the grant from Al-
                                                                   ibaba Inc.
        0.05               0.042
                                                                   References
        0.00  FB15K237
                                                                   Yushi Bai, Zhitao Ying, Hongyu Ren, and Jure
Figure 4: Models trained on FB15K237 and                              Leskovec. ",cs.CL,C,0.20417385,-0.19212222,0.07874809
http://arxiv.org/pdf/2201.06223v1,Korean-Specific Dataset for Table Question Answering,"arXiv preprint arXiv:2012.12627.
richly structural features. In future work, we aim to ex-  Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,
tend the model for complex question answering over            D., Levy, O., Lewis, M., Zettlemoyer, L., and
texts and tables with generation of multimodal ques-          Stoyanov, V. (2019). Roberta: A robustly opti-
tions to jointly handle question answering from textual       mized bert pretraining approach. ",cs.CL,B,-0.2786123,-0.06873795,0.23208594
http://arxiv.org/pdf/2201.06223v2,Korean-Specific Dataset for Table Question Answering,"arXiv preprint arXiv:1909.07005. In future work, we aim to extend the model for com-
plex question answering over texts and tables with the      Lin, X. V., Socher, R., and Xiong, C. (2020). Bridging
generation of multimodal questions to jointly handle           textual and tabular data for cross-domain text-to-sql
question answering from textual and tabular sources. ",cs.CL,B,-0.18207705,0.0037737135,0.19449228
http://arxiv.org/pdf/2201.06225v2,Interactive Contrastive Learning for Self-supervised Entity Alignment,"How to
Temperature 𝜏 and momentum coefficient 𝑚. The temperature              combine entity attributes into a self-supervised EA framework will
hyperparameter 𝜏 regulates the degree of attention to difficult sam-   be a focus of our future work. (2) PCL [22] introduces prototypes
ples [44] and the momentum coefficient 𝑚 prevents model’s sensi-       as latent variables to help find the maximum-likelihood estimation
tive update [16]. ",cs.CL,C,0.16864078,-0.06465016,0.08607057
http://arxiv.org/pdf/2201.06286v1,"MuLVE, A Multi-Language Vocabulary Evaluation Data Set","Finally, we conclude
                                        as synonyms or various ways of formatting. These in-        the paper and provide future work. ﬂexible systems lead to user frustration and a limited
                                        learning experience, resulting in users losing interest in                 2. ",cs.CL,A,0.09052297,0.12174628,-0.049665667
http://arxiv.org/pdf/2201.06302v1,"On the Context-Free Ambiguity of Emoji: A Data-Driven Study of 1,289 Emojis","emojis more accessible, user-friendly, and versatile. Arguably, every emoji is a symbol, and its interpreta-          Limitations and future work. Our goal is to provide ini-
                                                                tial measurements of the ambiguity of emojis. ",cs.CL,A,-0.0030544065,0.25878817,-0.2313015
http://arxiv.org/pdf/2201.06302v2,On the Context-Free Ambiguity of Emoji,"make emojis more accessible and user-friendly. Based on these observations, we next investigate such           Limitations and future work. Our goal is to provide ini-
                                                                tial measurements of the ambiguity of emojis. ",cs.CL,B,-0.06393805,0.23877588,-0.2643378
http://arxiv.org/pdf/2201.06384v1,Cyberbullying Classifiers are Sensitive to Model-Agnostic Perturbations,"Transferability Systematic performance gain across            Hence, recent work on decreasing the amount of queries
all substitution models (i.e., transferability) is the ﬁnal   for related models (Chauhan et al., 2021) is particularly
indicator of augmentation utility. First, it must be noted    relevant for future work. Additionally, there is a myriad
that the TPR differences between ‘same-model’ and dis-        of components the base architecture we presented here
tinct model pairs are smaller for the transformer-based       could be improved with. ",cs.CL,C,0.26252085,-0.30009925,-0.117718555
http://arxiv.org/pdf/2201.06469v1,Handling Compounding in Mobile Keyboard Input,"and improve WERs by roughly 20% across lan-
guages. In future work, we will generalize our de-       Philip Quinn and Zhai Shumin. 2016. ",cs.CL,C,0.19164056,-0.0002221372,-0.1642822
http://arxiv.org/pdf/2201.06573v1,PerPaDa: A Persian Paraphrase Dataset based on Implicit Crowdsourcing Data Collection,"sentences that are detected by the system, we expected
Among the clusters of documents of each user, we kept        to ﬁnd the paraphrased sentences in the similar posi-
those groups that include at least two documents for         tion. further analysis in the next steps. However, since the paraphrased sentence would shift a
                                                             few characters in the subsequent document compared
4.2. ",cs.CL,B,-0.19012302,0.13287598,0.015449565
http://arxiv.org/pdf/2201.06657v1,A Literature Survey of Recent Advances in Chatbots,"Furthermore, this distinction is not drawn when discussing different
                        chatbot models and their application either. In our analysis we have tried to clarify this
                        aspect, but there is need for deeper and reﬁned analysis and more clarity in future works of
                        literature survey. Modelling chatbots is an interesting task that combines Deep Learning and Natural
                        Language Processing, and whose applications have incredibly grown in the past few years. ",cs.CL,B,-0.11060121,0.1342811,0.23311967
http://arxiv.org/pdf/2201.06674v1,TYPIC: A Corpus of Template-Based Diagnostic Comments on Argumentation,"6. Discussion                            Our future work is to reﬁne the corpus to be more suit-
                                                           able for model training and evaluation based on this an-
Limitation We have not veriﬁed whether the tem-            notation study. For modeling, we plan to analyze what
plate set satisﬁes the three criteria for diagnostic com-  kind of information needs to be captured by the model
ments on different topics and given by different asses-    to give a correct diagnostic comment. ",cs.CL,B,-0.022014352,-0.008093428,-0.04445603
http://arxiv.org/pdf/2201.06674v2,TYPIC: A Corpus of Template-Based Diagnostic Comments on Argumentation,"6. Discussion
                                                           Our future work is to reﬁne the corpus to be more suit-
Limitations We did not verify whether the template         able for model training and evaluation based on this an-
set satisﬁes the three criteria for diagnostic comments    notation study. For modeling, we plan to analyze what
on different topics and given by different assessors. ",cs.CL,B,-0.110996544,0.1448006,0.06298325
http://arxiv.org/pdf/2201.06721v1,Selecting and combining complementary feature representations and classifiers for hate speech detection,"Hence, the framework is application-agnostic and can be applied for diﬀerent text classi-
ﬁcation tasks such as fake news classiﬁcation [54] as well as for detecting hate speech in
diﬀerent languages such as Spanish [56], Arabic [58], and Italian [57]. We will investigate
these points in future works. References

 [1] R. Batool, W. A. Khan, M. Hussain, J. Maqbool, M. Afzal, S. Lee, Towards personalized health proﬁling
      in social network, in: International Conference on New Trends in Information Science, Service Science
      and Data Mining, 2012, pp. ",cs.CL,A,-0.09941921,0.3796354,-0.1117311
http://arxiv.org/pdf/2201.06723v1,Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,"English examples of pig and shoe emojis (topics of common usages and offensive contexts are shown)

collect and annotate offensive tweets for other low-resource languages such as Hindi,k in addition
to Bangla. We keep this as a potential future work. Figure 10. ",cs.CL,B,-0.1818409,0.40643007,-0.16786799
http://arxiv.org/pdf/2201.06723v2,Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,"English examples of pig and shoe emojis (topics of common usages and offensive contexts are shown)

use this method to collect and annotate offensive tweets for other low-resource languages such as
Hindi,k in addition to Bengali. We keep this as a potential future work. Figure 10. ",cs.CL,B,-0.20162241,0.40149707,-0.18113737
http://arxiv.org/pdf/2201.06741v1,HashSet -- A Dataset For Hashtag Segmentation,"In
numerals. A named entity recognizer that works on an         Proceedings of the Tenth International Conference
unsegmented level could be useful, and we leave that         on Language Resources and Evaluation (LREC’16),
as part of our future work. pages 2981–2985, Portorozˇ, Slovenia, May. ",cs.CL,B,-0.19865985,0.02853612,-0.18293005
http://arxiv.org/pdf/2201.06849v1,Toward Self-Learning End-to-End Dialog Systems,"We leave the theme of              pages 583–592. effective machine teaching to future work. Matthew Henderson, Inigo Casanueva, Nikola Mrkvsic,
References                                                   Pei-Hao Su, Tsung-Hsien Wen, and Ivan Vulic. ",cs.CL,A,0.21648678,0.0798683,0.20836858
http://arxiv.org/pdf/2201.06849v2,Toward Self-learning End-to-End Task-Oriented Dialog Systems,"2020.
natural language. We leave the theme of effective            Paraphrase augmented task-oriented dialog genera-
machine teaching to future work. tion. ",cs.CL,B,-0.17252386,0.08783684,0.3291545
http://arxiv.org/pdf/2201.06876v1,Syntax-based data augmentation for Hungarian-English machine translation,"We brieﬂy discussed this syntax-aware method, which creates new
data points by swapping speciﬁc subtrees of dependency parse trees in parallel
for both the source and target sentences. Regarding our future work, we plan
to ﬁx some of the common errors listed in Table 6 and therefore enhance the
augmentation technique by making the generated samples less noisy. We also
plan to extend our experiments to other languages. ",cs.CL,B,-0.287638,-0.12909068,-0.0020735664
http://arxiv.org/pdf/2201.07099v1,Inferring Commonsense Explanations as Prompts for Future Event Generation,"Computational Linguistics (Volume 1: Long Papers),
                                                             pages 889–898. For future work, it would be very interesting to
migrate the architecture to a more advanced pre-          Angela Fan, Mike Lewis, and Yann Dauphin. 2019.
training model like GPT-3, like achieving the com-           Strategies for structuring story generation. ",cs.CL,B,-0.14921173,-0.015904242,0.20294406
http://arxiv.org/pdf/2201.07099v2,What Makes the Story Forward? Inferring Commonsense Explanations as Prompts for Future Event Generation,"Automatic and manual evaluations substantiate
                                                                       the contextual and logical coherence of generated events. Table 7: Automatic and human evaluations results on FEG
                                                                          For future work, it would be very interesting to migrate the
task with different commonsense prompts. architecture to a more advanced pretraining model like GPT-3,
                                                                       like achieving the commonsense knowledge in a Few-Shot way or
prediction that after a few month’s hard working, she can study well. ",cs.CL,A,0.07587755,0.04976836,0.32439184
http://arxiv.org/pdf/2201.07288v1,Extending the Vocabulary of Fictional Languages using Neural Networks,"We    qualitatively analyzed their results. We found that
can also use this property (deferred to future work)  the style transfer network had the best performance
to increase the size of the fictional vocabulary by   because of the limited nature of the training dataset. using examples from the language that the fictional   While we used the style transfer technique from
language is based on. ",cs.CL,B,-0.09261124,0.05711765,-0.060606528
http://arxiv.org/pdf/2201.07317v1,A Privacy-Preserving Unsupervised Domain Adaptation Framework for Clinical Text Analysis,"However, sharing only the
                                        source feature distribution may still suffer from the membership             Feature                          Target client
                                        inference attack who can infer an individual’s membership by                 extractor
                                        the black-box access to the source model. To resolve this privacy                         When a new
                                        issue, we further study the under-explored problem of privacy-                            patient Bob with
                                        preserving domain adaptation and propose a method with a novel                            diabetes visited
                                        differential privacy training strategy to protect the source data                         this hospital on
                                        privacy. We model the source feature distribution by Gaussian                             8/27/2020. ",cs.CL,C,0.19247642,0.0216728,0.0020046532
http://arxiv.org/pdf/2201.07365v1,Improving Neural Machine Translation by Denoising Training,"2021b)). We will explore them in the future works. parallel data with Bicleaner (Sánchez-Cartagena                 Someone may doubt that why not introduce
et al., 2018) and apply the SentencePiece (Kudo              more powerful pretrained language models, e.g. ",cs.CL,B,-0.21036074,-0.0029490665,-0.03552478
http://arxiv.org/pdf/2201.07365v2,Improving Neural Machine Translation by Denoising Training,"2021b)). We will explore them in the future works. parallel data with Bicleaner (Sánchez-Cartagena                 Someone may doubt that why not introduce
et al., 2018) and apply the SentencePiece (Kudo              more powerful pretrained language models, e.g. ",cs.CL,B,-0.21036074,-0.0029490665,-0.03552478
http://arxiv.org/pdf/2201.07406v1,Neural Language Models are Effective Plagiarists,"The values
marketed for other purposes. In future work we intended to           encoded in machine learning research. arXiv preprint
examine how the intended application is explicitly and im-           arXiv:2106.15590, 2021.
plicitly embedded in the functionality of document similarity
detectors [Johnson, 2022; Birhane et al., 2021], and incorpo-     Kevin W Bowyer and Lawrence O Hall. ",cs.CL,B,-0.11029457,0.16191116,-0.095752865
http://arxiv.org/pdf/2201.07406v2,Fooling MOSS Detection with Pretrained Language Models,"[59] consider the        137.
case of code generation. While it is beyond the scope of the present
study, we hope that future work on plagiarism with transformer              [2] Alex Aiken. 2000. ",cs.CL,C,0.028742133,-0.087614596,-0.074951455
http://arxiv.org/pdf/2201.07423v1,Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19,"It is possible that for those who are with certain labels in their
loneliness expressions such as “chronic” and “somatic” loneliness, are more likely to develop negative
psychological outcomes in the long run compared to those with labels including “transient” and “physi-
cal” loneliness. Similarly, future work may follow-up on the effects of different coping strategies (e.g.,
seeking advice or validation and reaching out) on loneliness reduction because previous work suggests
that coping strategies can make a difference in the psychological outcomes (Deckx et al., 2018). Limitation and Data Disclaimer Several limitations to our work should be noted. ",cs.CL,A,0.5422271,0.3610023,-0.24226378
http://arxiv.org/pdf/2201.07423v2,Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19,"It is possible that for those who are with certain labels in their
loneliness expressions such as “chronic” and “somatic” loneliness, are more likely to develop negative
psychological outcomes in the long run compared to those with labels including “transient” and “physi-
cal” loneliness. Similarly, future work may follow-up on the effects of different coping strategies (e.g.,
seeking advice or validation and reaching out) on loneliness reduction because previous work suggests
that coping strategies can make a difference in the psychological outcomes (Deckx et al., 2018). Limitation and Data Disclaimer Several limitations to our work should be noted. ",cs.CL,A,0.5422271,0.3610023,-0.24226378
http://arxiv.org/pdf/2201.07423v3,Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19,"It is possible that for those who are with certain labels in their
loneliness expressions such as “chronic” and “somatic” loneliness, are more likely to develop negative
psychological outcomes in the long run compared to those with labels including “transient” and “physi-
cal” loneliness. Similarly, future work may follow-up on the effects of different coping strategies (e.g.,
seeking advice or validation and reaching out) on loneliness reduction because previous work suggests
that coping strategies can make a difference in the psychological outcomes (Deckx et al., 2018). Limitation and Data Disclaimer Several limitations to our work should be noted. ",cs.CL,A,0.5422271,0.36100227,-0.2422638
http://arxiv.org/pdf/2201.07423v4,Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19,"It is possible that those who
are with certain labels in their loneliness expressions such as “chronic” and “somatic” loneliness, are
more likely to develop negative psychological outcomes in the long run compared to those with labels
including “transient” and “physical” loneliness. Similarly, future work may follow-up on the effects
of different coping strategies (e.g., seeking advice or validation and reaching out) on loneliness reduc-
tion because previous work suggests that coping strategies can make a difference in the psychological
outcomes (Deckx et al., 2018). Limitation and Data Disclaimer Several limitations to our work should be noted. ",cs.CL,A,0.5311986,0.35677767,-0.20885155
http://arxiv.org/pdf/2201.07423v5,Many Ways to Be Lonely: Fine-Grained Characterization of Loneliness and Its Potential Changes in COVID-19,"It is possible that those who
are with certain labels in their loneliness expressions such as “chronic” and “somatic” loneliness, are
more likely to develop negative psychological outcomes in the long run compared to those with labels
including “transient” and “physical” loneliness. Similarly, future work may follow-up on the effects
of different coping strategies (e.g., seeking advice or validation and reaching out) on loneliness reduc-
tion because previous work suggests that coping strategies can make a difference in the psychological
outcomes (Deckx et al., 2018). Limitation and Data Disclaimer Several limitations to our work should be noted. ",cs.CL,A,0.5311986,0.35677767,-0.20885155
http://arxiv.org/pdf/2201.07434v1,Interpreting Arabic Transformer Models,"(2020) used max-pooling to identify       former models do not capture dialectal nuances
relevant neurons (aka Expert units) in pre-trained       despite have a large overlap with dialects. For
models, with respect to a speciﬁc concept (for ex-       future work, we aim to expand this analysis to in-
ample word-sense). Mu and Andreas (2020) pro-            clude more tasks and other languages with a focus
posed a Masked-based Corpus Selection method             on analysing groups of related languages in the
to determine important neurons with respect to a         families of Semitic, Germanic or Latin languages. ",cs.CL,B,-0.26034606,-0.018898219,0.08337927
http://arxiv.org/pdf/2201.07489v1,Development of Fake News Model using Machine Learning through Natural Language Processing,"As we are
                                                                                                                                                                                 aware that every single news has different characteristics so
                                                                                                                                                                                 there is a need for a system that can check the content of the
                                                                                                                                                                                 news in depth. Our future work includes building an
                                                                                                                                                                                 automated fact-checking system that combines data and
                                                                                                                                                                                 knowledge to help non-experts and checks the content of the
                                                                                                                                                                                 news thoroughly after comparing it with known facts. We
                                                                                                                                                                                 want to look into the issue of fake news from different angles
                                                                                                                                                                                 like known facts, source, topic, associated URLs, geographical
                                                                                                                                                                                 location, year of publication, and credibility of the source for a
                                                                                                                                                                                 better understanding of the problem. ",cs.CL,A,0.0091447495,0.27358446,-0.0010712249
http://arxiv.org/pdf/2201.07520v1,CM3: A Causal Masked Multimodal Model of the Internet,"(2020) without any pathological cases, implying there is still a good amount of gains
to be achieved with further scaling. An in-depth analysis of the scaling laws of the causally masked
objective is outside this current work’s scope and will be considered for future work. 4 ZERO/FEW-SHOT PROMPTING

4.1 IMAGE MODALITY

Although we do not train on pure image documents, CM3 can still operate over image tasks. ",cs.CL,C,0.24055815,-0.019334776,0.06316194
http://arxiv.org/pdf/2201.07725v1,Data-to-Value: An Evaluation-First Methodology for Natural Language Projects,"A D2V Question Catalog (Excerpt)

This appendix lists a representative sample of questions from the D2V catalog of
questions (from a total of N ≈ 100 questions). The the full list of question will
be included in a software tool supporting the process introduced here, which is
left for future work. No. ",cs.CL,C,0.09869087,-0.0122167505,0.05487626
http://arxiv.org/pdf/2201.08070v1,Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation,"The analyses of pre-training
accuracy reveal the complementary nature of individual tasks within JASS and ENSS. Our future work will focus on implementing linguistic-aware multilingual pre-training using
more languages for more robust pre-trained models. We also note that Raffel et al. ",cs.CL,B,-0.2861668,-0.01325311,0.06636369
http://arxiv.org/pdf/2201.08081v1,LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training,"In Pro-
formance on three of them. For future work, we            ceedings of the 2019 Conference of the North Amer-
hope to extend our approach to more complex envi-         ican Chapter of the Association for Computational
ronments and tasks such as image editing (Fu et al.,      Linguistics: Human Language Technologies, Vol-
2020) and text editing (Faltings et al., 2021). ume 1 (Long and Short Papers), pages 2347–2356,
                                                          Minneapolis, Minnesota. ",cs.CL,B,-0.16731271,0.047670264,-0.043258727
http://arxiv.org/pdf/2201.08081v2,LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training,"In 7th Inter-
strategy brings signiﬁcant improvements on all of         national Conference on Learning Representations,
them and LEMON achieves the state-of-the-art per-         ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
formance on four of them. For future work, we             OpenReview.net. hope to extend our approach to more complex envi-
ronments and tasks such as image editing (Fu et al.,   Xinya Du, Bhavana Dalvi, Niket Tandon, Antoine
2020) and text editing (Faltings et al., 2021). ",cs.CL,B,-0.07050428,-0.035397004,0.17180327
http://arxiv.org/pdf/2201.08081v3,LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training,"We leave the exploration of         tics. raw environments for future work. Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-
Ethics Statement                                          tau Yih, and Peter Clark. ",cs.CL,A,0.28021717,0.2343502,0.05864664
http://arxiv.org/pdf/2201.08089v1,Why Did You Not Compare With That? Identifying Papers for Use as Baselines,"made in the experiment section, and the language patterns in their citation contexts are
often very similar to contexts of baseline citations (rows 1 and 2 in Table 8). Citations for Future Work: Often, authors discuss the results of papers that are not
explicitly used as baselines in the current work but are discussed for the sake of com-
pleteness and could be used as baselines as part of the future work. One could argue
that such citations should be easy to classify as they must be part of the Conclusions
and Future Work sections. ",cs.CL,A,0.04719482,0.1639364,0.009903308
http://arxiv.org/pdf/2201.08239v1,LaMDA: Language Models for Dialog Applications,"Note that this composition was chosen
to achieve a more robust performance on dialog tasks (Section 4) while still keeping its ability to perform other tasks
like code generation. As future work, we can study how the choice of this composition may affect the quality of some
of the other NLP tasks performed by the model. F Pre-training and ﬁne-tuning results

                                Table 28: Results for Foundation Metrics

Treatment                 Sensibleness Speciﬁcity Interestingness Safety  Groundedness  Informativeness

PT (2B)                   76.6        46.5        10.8      84.8                 45            29.2
PT (8B)                                                                         47.1           29.5
PT (137B)                 79.1        46.5        11.3      87.5                57.9           41.3
FT quality-safety (137B)                                                        67.9           50.5
LaMDA (2B)                80.2        49.8        15.8        88                 53            41.8
LaMDA (8B)                                                                      64.6           50.2
LaMDA (137B)              92.8        77.1        23.2      94.6                73.2           62.3

                          81.8        74.8        23.4      93.8

                          88          77.4        22.2      93.5

                          92.3        79          25.7      95.2

                                            47 ",cs.CL,C,0.041134097,-0.047488328,0.005061444
http://arxiv.org/pdf/2201.08239v2,LaMDA: Language Models for Dialog Applications,"Note that this composition was chosen
to achieve a more robust performance on dialog tasks (Section 4) while still keeping its ability to perform other tasks
like code generation. As future work, we can study how the choice of this composition may affect the quality of some
of the other NLP tasks performed by the model. F Pre-training and ﬁne-tuning results

                                Table 28: Results for Foundation Metrics

Treatment                 Sensibleness Speciﬁcity Interestingness Safety  Groundedness  Informativeness

PT (2B)                   76.6        46.5        10.8      84.8                 45            29.2
PT (8B)                                                                         47.1           29.5
PT (137B)                 79.1        46.5        11.3      87.5                57.9           41.3
FT quality-safety (137B)                                                        67.9           50.5
LaMDA (2B)                80.2        49.8        15.8        88                 53            41.8
LaMDA (8B)                                                                      64.6           50.2
LaMDA (137B)              92.8        77.1        23.2      94.6                73.2           62.3

                          81.8        74.8        23.4      93.8

                          88          77.4        22.2      93.5

                          92.3        79          25.7      95.2

                                            47 ",cs.CL,C,0.041134097,-0.047488328,0.005061444
http://arxiv.org/pdf/2201.08239v3,LaMDA: Language Models for Dialog Applications,"Note that this composition was chosen
to achieve a more robust performance on dialog tasks (Section 4) while still keeping its ability to perform other tasks
like code generation. As future work, we can study how the choice of this composition may affect the quality of some
of the other NLP tasks performed by the model. F Pre-training and ﬁne-tuning results

                                Table 28: Results for Foundation Metrics

Treatment                 Sensibleness Speciﬁcity Interestingness Safety  Groundedness  Informativeness

PT (2B)                   76.6        46.5        10.8      84.8                 45            29.2
PT (8B)                                                                         47.1           29.5
PT (137B)                 79.1        46.5        11.3      87.5                57.9           41.3
FT quality-safety (137B)                                                        67.9           50.5
LaMDA (2B)                80.2        49.8        15.8        88                 53            41.8
LaMDA (8B)                                                                      64.6           50.2
LaMDA (137B)              92.8        77.1        23.2      94.6                73.2           62.3

                          81.8        74.8        23.4      93.8

                          88          77.4        22.2      93.5

                          92.3        79          25.7      95.2

                                            47 ",cs.CL,C,0.041134097,-0.047488328,0.005061444
http://arxiv.org/pdf/2201.08277v1,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,"As
is slightly lower (36 − 43%). AfriBERTa gave the best        future work, we plan to create benchmark experiments
overall result in the zero-shot transfer, and it is signiﬁ-  with our sentiment lexicon, and extend our dataset
cantly better than the majority classiﬁer (weighted av-      (NaijaSenti) to include African languages (AfriSenti). erage) for all languages: hau, ibo, pcm, and yor are
better by 41.8%, 20.8%, 2%, and 19.1% respectively. ",cs.CL,B,-0.18910807,0.16743162,-0.1742236
http://arxiv.org/pdf/2201.08277v2,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,"AfriBERTa gave the best overall result in       volved. As future work, we plan to create benchmark
the zero-shot transfer, and it is signiﬁcantly better than  experiments with our sentiment lexicon, and extend our
the majority classiﬁer (weighted average) for all lan-      dataset (NaijaSenti) to include other African languages
guages: hau, ibo, pcm, and yor are better by 41.8%,         (AfriSenti). 8. ",cs.CL,B,-0.17421171,0.13534337,-0.20293838
http://arxiv.org/pdf/2201.08277v3,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,"the potential to spark interest in sentiment analysis
and other downstream NLP tasks in the languages in-       Alabi, J., Amponsah-Kaakyire, K., Adelani, D., and
volved. As future work, we plan to create benchmark          España-Bonet, C. (2020). Massive vs. curated em-
experiments with our sentiment lexicon, and extend our       beddings for low-resourced languages: the case of
dataset (NaijaSenti) to include other African languages      Yorùbá and Twi. ",cs.CL,B,-0.29423887,0.2528808,-0.10985855
http://arxiv.org/pdf/2201.08318v1,Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs,"For example, one can limit the
number of times students can receive feedback from the model in a time span
to humanly reasonable levels, thus, hindering automatic probing. 6.2 Limitations & Future Work

Finally, we will point out a few limitations of our experiments and ideas for
future work. This paper focused on the eﬀects of one adversarial attack strat-
egy. ",cs.CL,A,0.21229815,0.03776105,0.289128
http://arxiv.org/pdf/2201.08318v2,Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs,"For example, one can limit the
number of times students can receive feedback from the model in a time span
to humanly reasonable levels, thus, hindering automatic probing. 6.2 Limitations & Future Work

Finally, we will point out a few limitations of our experiments and ideas for
future work. This paper focused on the eﬀects of one adversarial attack strat-
egy. ",cs.CL,A,0.21229815,0.03776105,0.289128
http://arxiv.org/pdf/2201.08340v1,Signature Entrenchment and Conceptual Changes in Automated Theory Repair,"For our initial implementation
     we consider them reliable by deﬁnition. We leave cases where they are themselves subject to revision for future work. 11
                                   XUE LI, ALAN BUNDY, EUGENE PHILALITHIS

                                    e(p) = 1 − pdpMda(px)+1 , pd(p) = ∞ (4)
                                                pdM1ax+2 , pd(p) = ∞

where pdMax = max({pd(q) : pd(q) = ∞}). ",cs.CL,C,0.28266287,-0.22833812,-0.09639244
http://arxiv.org/pdf/2201.08451v1,Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency,"The current work relies on the widely used Word2Vec
   The fourth and ﬁnal column of Table 1 shows the co-          model (Mikolov et al. 2013); future work may extend anal-
efﬁcient of the relationship between WEAT estimates and         ysis to GloVe models (Pennington, Socher, and Manning
each outcome when controlling for relative black name fre-      2014) and contextual embeddings models such as BERT
quency. As can be seen, none of the relationships attain sta-   (Devlin et al. ",cs.CL,B,-0.10217953,0.066024385,-0.00087032095
http://arxiv.org/pdf/2201.08643v1,Text Style Transfer for Bias Mitigation using Masked Language Modeling,"In Without-LR, we remove latent           representation. content information from our model but maintain class con-
straint (Laccxa ), whereas in Without-LR&SS both class con-        As part of our future work, we intend to expand this work
straint and latent content information are omitted. to other languages. ",cs.CL,B,0.002848629,-0.07270529,-0.10422198
http://arxiv.org/pdf/2201.08670v2,Context-Tuning: Learning Contextualized Prompts for Natural Language Generation,"ation for Computational Linguistics. In future work, we will consider integrating more   Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao
types of context information (e.g., sentiment) to         Jiang, and Graham Neubig. 2021. ",cs.CL,B,-0.22005871,0.23017472,0.038350537
http://arxiv.org/pdf/2201.08675v1,Gender Bias in Text: Labeled Datasets and Lexicons,"This work also offers
an insight onto the automated data retrieval and annotation methodologies utilized to fetch and
label the retrieved sentences. In a future work, we will address the issue of pronoun resolution
by considering surrounding sentences or entire paragraphs. We will also aim to further augment
our datasets and lexicons to expand their coverage to the remaining bias types. ",cs.CL,B,-0.32039157,0.1843063,-0.036408186
http://arxiv.org/pdf/2201.09012v1,Leaf: Multiple-Choice Question Generation,"With the aim to enable a better educational process,
especially in the context of MOOCs, we open-source the project, including all
training scripts and documentation. In future work, we plan to experiment with a variety of larger pre-trained
Transformers as the underlying model. We further plan to train on additional
data. ",cs.CL,C,0.044709958,-0.1339089,0.117084816
http://arxiv.org/pdf/2201.09060v1,Solvability of orbit-finite systems of linear equations,"dimensional vector spaces and weighted register automata. In
   We leave a lot of questions for further research—here we           Proc. LICS, pages 1–13. ",cs.CL,C,0.08606145,-0.16612048,-0.06027385
http://arxiv.org/pdf/2201.09060v2,Solvability of orbit-finite systems of linear equations,"S (A) = F S (Ã). We leave a lot of questions for further research—here we
Proof. W.l.o.g. ",cs.CL,A,0.19481763,0.08641248,-0.13672958
http://arxiv.org/pdf/2201.09282v1,WIDAR -- Weighted Input Document Augmented ROUGE,"Using these character-
istics, recent works have attempted to quantify and compare the performance
of existing evaluation metrics [1,10]. These works highlight the limitations of
existing metrics and oﬀer various resources for conducting further research on
the evaluation task. One such work is the SummEval dataset [10] that provides
human annotation scores for - coherence, consistency, ﬂuency and relevance. ",cs.CL,B,-0.07555989,0.21133891,-0.1085165
http://arxiv.org/pdf/2201.09324v1,Supervised Visual Attention for Simultaneous Multimodal Machine Translation,"(2020a). 4.3 Qualitative Insights

In this section, we conduct further analysis focusing on qualitative aspects of the trained
MMT models in terms of visual understanding. 4.3.1 Grounding Ability

We begin our analyses by measuring the grounding ability of our MMT systems on the
test2016 test set, for which we have ground-truth region-phrase annotations from Flickr30k
dataset (§ 3.4). ",cs.CL,C,0.057477236,0.07044922,0.14514653
http://arxiv.org/pdf/2201.09324v2,Supervised Visual Attention for Simultaneous Multimodal Machine Translation,"(2020a). 4.3 Qualitative Insights

In this section, we conduct further analysis focusing on qualitative aspects of the trained
MMT models in terms of visual understanding. 4.3.1 Grounding Ability

We begin our analyses by measuring the grounding ability of our MMT systems on the
test2016 test set, for which we have ground-truth region-phrase annotations from Flickr30k
dataset (§ 3.4). ",cs.CL,C,0.057477236,0.07044922,0.14514653
http://arxiv.org/pdf/2201.09377v1,An Application of Pseudo-Log-Likelihoods to Natural Language Scoring,"5 CONCLUSION

The remarkable consistency of ALBERT’s performance on Winogradversarial, TimeDial, WSC, and
Winogrande datasets is a point of optimism for the generalizability of the performance of language
models. A limitation of the current approach is that the robust performance seems to be limited to
cases in which a common sense judgement can be expressed as the relative likelihood of two natural
language alternatives, a promising avenue for future work. We emphasize the improvement in both computational efﬁciency and accuracy that is effected for the
TimeDial dataset by cleaning punctuation so as to more closely match normal human conventions. ",cs.CL,B,-0.23734158,0.06153755,0.09378017
http://arxiv.org/pdf/2201.09523v1,BTPK-based learning: An Interpretable Method for Named Entity Recognition,"In addition, the pro-
(s1 2q) ∧ (s20 Yq); but there is a public announcement          posed counterfactual veriﬁcation proves that the explanations
s20 → s20, so it will return to s1 and choose the new path in   of our method are accurate and reasonable. For future work,
red, which is the right one. For the Bi-GRU model in Figure     we plan to combine BTPK-based learning (as in this work)
7 (b), there is no public announcement, and only one path of    with transfer learning for cross-lingual NER tasks. ",cs.CL,C,0.008298803,-0.14833826,0.07050891
http://arxiv.org/pdf/2201.09651v1,Artefact Retrieval: Overview of NLP Models with Knowledge Base Access,"In Section 3 we introduce the abstract model underlying other systems together with the
different components and the way they can be instantiated. The limitations of this schema and also
future work with methods not yet explored are discussed in Section 4. We conclude in Section 5. ",cs.CL,C,0.26281726,-0.17632636,0.22902684
http://arxiv.org/pdf/2201.09680v1,Relational Memory Augmented Language Models,"We
shown in Table 6, tf-idf performs the best. leave this to future work. Model   Dev                                        Knowledge perplexity. ",cs.CL,B,-0.10774308,-0.053290408,-0.15030837
http://arxiv.org/pdf/2201.09745v1,"Table Pretraining: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks","In this survey of table pretraining, a comprehensive
review of existing model architectures, pretraining objectives, and
downstream tasks is presented. We hope that this survey can pro-
vide some insightful perspectives to inspire more directions of
future works. REFERENCES                                                                                In Proceedings of the 38th International Conference on Software Engineering, pages
                                                                                          464–475, 2016. ",cs.CL,C,0.17608175,-0.31194413,0.19594818
http://arxiv.org/pdf/2201.09745v2,"Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks","In this survey of table pre-training, a comprehen-
sive review of existing model architectures, pre-training objectives,
and downstream tasks is presented. We hope that this survey can
provide some insightful perspectives to inspire more directions of
future works. REFERENCES                                                                                In Proceedings of the 38th International Conference on Software Engineering, pages
                                                                                          464–475, 2016. ",cs.CL,C,0.16506694,-0.31416908,0.24959661
http://arxiv.org/pdf/2201.09745v3,"Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks","Nonetheless,                             LMs while exploit table-speciﬁc abilities? it is highly desirable for future work to demonstrate the effec-
tiveness of table pre-training on more tasks such as table for-                               Efﬁcient Transformers for Big Tables Due to the limita-
mat generation [Dong et al., 2020], data analysis [Zhou et al.,                            tion of the input length of existing large-scale pre-trained lan-
2020], and table error detection [Huang and He, 2018]. More                                guage models, they usually need to split tables into several
descriptions can be found in Appendix. ",cs.CL,C,0.16669595,-0.3446371,0.041044828
http://arxiv.org/pdf/2201.09745v4,"Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks","On
tasks that have been evaluated by existing TaLMs. Nonethe-                                 the other hand, without table-speciﬁc model design and pre-
less, it is highly desirable for future work to demonstrate the                            training, it still fell far behind TaLMs with table pre-training on
effectiveness of table pre-training on more tasks such as ta-                              WikiTQ (-8.2%) and SQA (-12.1%); even larger margins may
ble format generation [Dong et al., 2020], data analysis [Zhou                             exist on untested tasks, such as formula prediction. It shows the
et al., 2020], and table error detection [Huang and He, 2018;                              necessity of table-speciﬁc pre-training, e.g., in structural and
Zhang et al., 2021]. ",cs.CL,C,0.17842561,-0.2008728,0.084814206
http://arxiv.org/pdf/2201.10005v1,Text and Code Embeddings by Contrastive Pre-Training,"For example, safe public         Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
access to large pre-trained language models, and efﬁcient          M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
training pipelines that leverage improved model architec-          S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-
tures and training schemes. We encourage further research          ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,
and implementation efforts in these areas. D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss,
                                                                   A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,
6. ",cs.CL,C,0.074127905,-0.17888667,0.15338606
http://arxiv.org/pdf/2201.10066v1,Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources,"Recommendations                                       mentation practices prior to releasing data collections,
                                                           especially in large-scale settings. In reﬂecting on our experience in creating the cata-
logue, we suggest recommendations for future work                      8. Acknowledgements
regarding designing tasks for crowdsourcing efforts,
engaging the broader data ecosystem, and using the         We are grateful to all of the participants of the
catalogue. ",cs.CL,A,0.17877623,0.14464849,-0.03770736
http://arxiv.org/pdf/2201.10113v5,Multimodal data matters: language model pre-training over structured and unstructured electronic health records,"Furthermore, the
                                                                      clinical text may include irrelevant information, misspellings
Task                      TABLE V            F1%                      and unstandardized abbreviations, the noise data may mislead
NER        PERFORMANCE OF NER TASK           83.81                    the performance, and we did not use data de-noise strategies to
                                             85.77                    optimize the noise data. A more reasonable de-noise method of
                           Model             86.29                    data may be helpful, and we will probe more strategies in our
                    biLSTM_CRF [82]                                   future work. Lastly, the MIMIC-III dataset only contains notes
                 biLSTM_CRF+ClinicalBert                              from the intensive care unit, and the performance of the model
                biLSTM_CRF+MedM-PLM                                   might decrease on other clinical records. ",cs.CL,C,0.15623815,-0.07874788,-0.35613996
http://arxiv.org/pdf/2201.10113v6,Multimodal data matters: language model pre-training over structured and unstructured electronic health records,"mation, misspellings and unstandardized abbreviations, which
                                                                                 may mislead the learning process. We will probe more data de-
Task                       TABLE V                          F1%                  noise strategies in our future work. Lastly, the notes of the
NER         PERFORMANCE OF NER TASK                         83.81                MIMIC-III dataset are mainly from the intensive care unit,
                                                            85.77                which might not be quite scalable to other clinical records. ",cs.CL,C,0.122308984,-0.10351475,-0.29264393
http://arxiv.org/pdf/2201.10113v7,Multimodal data matters: language model pre-training over structured and unstructured electronic health records,"mation, misspellings and unstandardized abbreviations, which
                                                                                 may mislead the learning process. We will probe more data de-
Task                       TABLE V                          F1%                  noise strategies in our future work. Lastly, the notes of the
NER         PERFORMANCE OF NER TASK                         83.81                MIMIC-III dataset are mainly from the intensive care unit,
                                                            85.77                which might not be quite scalable to other clinical records. ",cs.CL,C,0.122308984,-0.10351475,-0.29264393
http://arxiv.org/pdf/2201.10474v1,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,"A closer analysis of how the language
used quality ﬁlter for text data selection. ideologies in data selection lead to certain model
                                                        behaviors is a rich area for future work. Critiques of Laissez-Faire Data Collection We
provide empirical evidence that laissez-faire data      8 Conclusion
collection (i.e., ﬁltering large web data sources)
leads to data homogeneity (Bender et al., 2021). ",cs.CL,A,0.02715667,0.23248641,-0.057348814
http://arxiv.org/pdf/2201.10474v2,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,"A closer analysis of how the language
used quality ﬁlter for text data selection. ideologies in data selection lead to certain model
                                                        behaviors is a rich area for future work. Critiques of Laissez-Faire Data Collection We
provide empirical evidence that laissez-faire data      8 Conclusion
collection (i.e., ﬁltering large web data sources)
leads to data homogeneity (Bender et al., 2021). ",cs.CL,A,0.02715667,0.23248641,-0.057348814
http://arxiv.org/pdf/2201.10515v1,Suicidal Ideation Detection on Social Media: A Review of Machine Learning Methods,"This paper presents a comprehensive summary of current research efforts to detect suicidal
                                         ideation using machine learning algorithms on social media. This review 24 studies investigating the
                                         feasibility of social media usage for suicidal ideation detection is intended to facilitate further research in
                                         the ﬁeld and will be a beneﬁcial resource for researchers engaged in suicidal text classiﬁcation. INTRODUCTION

                                        Millions of individuals regularly use social media such as chat rooms, blogging websites, and social
                                        networking platforms, with 3.96 billion people actively utilizing the internet [1]. ",cs.CL,A,0.15254784,0.33614567,-0.038347024
http://arxiv.org/pdf/2201.10866v1,CodeRetriever: Unimodal and Bimodal Contrastive Learning,"Here, we verify a simple and effective      combines the unimodal and bimodal contrastive
positive pairs construction method, we leave the de-      learning as pre-training tasks for code search. velopment of more powerful construction method            For unimodal contrastive learning, we propose
as future work. From the results of using doc-            a semantic-guided method to build positive code
code and comment-code for bimodal contrastive             pairs. ",cs.CL,B,-0.22865105,-0.061577104,0.032907706
http://arxiv.org/pdf/2201.10866v2,CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search,"Unixcoder: Uniﬁed cross-
SearchNet corpus. In future work, we will consider           modal pre-training for code representation. In Pro-
using more pre-training corpora such as full Github          ceedings of the 60th Annual Meeting of the Associa-
repositories. ",cs.CL,B,-0.19260657,-0.05038106,-0.0652953
http://arxiv.org/pdf/2201.10866v3,CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search,"Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
                                                             Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,
   3) We pre-train CodeRetriever on the Code-                Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
SearchNet corpus. In future work, we will consider           Shao Kun Deng, Colin Clement, Dawn Drain, Neel
using more pre-training corpora such as full Github          Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. repositories. ",cs.CL,B,-0.07252641,-0.13716571,-0.034451917
http://arxiv.org/pdf/2201.10881v1,The Norwegian Parliamentary Speech Corpus,"(%)  9.97 10.89 8.94 9.81                     Norwegian. As such, it enables further research, de-
                                                            velopment and application of ASR not only for Nor-
Table 3: WER(%) per dialect area in Module 3 of NB          wegian, but also for many other languages affected
Tale for the three combinations of models tested on         by the phenomena we discussed here. Nevertheless,
these data, see Table 2. ",cs.CL,B,-0.04675205,0.006806312,-0.27172673
http://arxiv.org/pdf/2201.11115v1,CsFEVER and CTKFacts: Czech Datasets for Fact Verification,"In a No Score Evidence setting, where the gold evi-
dence requirement is removed, we scored 55.40% and 61.24% on CsFEVER
and CTKFacts, respectively. We claim the results to be testifying to the via-
bility of the task in our setting and encouraging for further research on our
data. 7.1 Future work

   • The fact-checking pipeline is to also be augmented by the check-worthiness
      estimation [50], that is, a model that classiﬁes which sentences of given
      text in Czech are appropriate for the fact veriﬁcation. ",cs.CL,A,0.038710117,0.1894187,-0.038033307
http://arxiv.org/pdf/2201.11258v1,Learning How to Translate North Korean through South Korean,"Although our North Korean
strings between the train and evaluation data leads to an   MT datasets have some limitations with regards to size
overestimation of the North Korean models, we eval-         and diversity of sentences, the ﬁndings of our study are
uate the models by deleting the sentences of dev and        useful for the development of a North Korean transla-
test that duplicate more than ten substrings with the       tion system. To support further research, we also pro-
train data. Table 7 shows the BLEU scores obtained us-      vide the data and code used in our experiments. ",cs.CL,B,-0.10368355,0.032548256,-0.1575273
http://arxiv.org/pdf/2201.11332v1,Ontology-enhanced Prompt-tuning for Few-shot Learning,"We will leave
the entire dataset. This indicates that injecting external knowledge  this for future works. can benefit prompt template representation. ",cs.CL,C,0.16104421,-0.026964443,0.05935009
http://arxiv.org/pdf/2201.11367v1,Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation,"arXiv preprint arXiv:1811.01241.
of generative models as well as generate proper
response for complicated human conversation. In            Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine
future works, we would like to study better ways              Bordes, Sumit Chopra, Alexander Miller, Arthur
of evidence retrieval and evidence-aware training             Szlam, and Jason Weston. 2015. ",cs.CL,A_centroid,-0.035751782,0.06344542,0.35704666
http://arxiv.org/pdf/2201.11367v2,Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation,"We also ﬁnd that BERTScore can be used                  Computational Linguistics: Human Language Tech-
for better evidence retrieval. In future works, we            nologies, Volume 1 (Long and Short Papers), pages
would like to study better ways of evidence re-               1219–1228. trieval and evidence-aware training and we believe
our approach can beneﬁt to other NLP tasks, such           Emily Dinan, Stephen Roller, Kurt Shuster, Angela
as classiﬁcation task. ",cs.CL,B,-0.23657101,0.16927008,0.084918186
http://arxiv.org/pdf/2201.11443v2,Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond,"consented-to peer review texts blocked    effects on the data. Yet, many questions remain
by the paper authors), ensuring fair access to pro-    open, from strategies for better community engage-
tected data presents a technical, administrative and   ment, to the technicalities related to replicability
legal challenge to be investigated in future work. and archival of the data. ",cs.CL,A,0.2166591,0.31539756,-0.07523293
http://arxiv.org/pdf/2201.11473v1,Reasoning Like Program Executors,"Our work concentrates
      three exemplary across-program POET instan-     on improving the above reasoning skills, leaving
      tiations for various reasoning capabilities. the other reasoning abilities such as commonsense
                                                      reasoning (Zellers et al., 2018; Talmor et al., 2019;
   • We show with quantitative experiments that       Bhagavatula et al., 2020) for future work. …                                                      Pre-training

                                     Pre-training

                    …

 (a) Reasoning via  (b) Reasoning via Pre-training     (c) Reasoning via Pre-training
Specialized Models      over Natural Language               over Program (Ours)

Figure 2: The illustration of different lines of reasoning, including (a) reasoning via specalized models, (b) reason-
ing via pre-training over natural language and (c) reasoning via pre-training over program. ",cs.CL,B,-0.11151144,0.018869366,0.39205495
http://arxiv.org/pdf/2201.11473v2,Reasoning Like Program Executors,"For ex-
ample, compared with PReasM initialized from                      7 Discussion and Open Questions
T5-Large, POET-SQLBART initialized from BART-
Large exceeds it by 8.3%. Furthermore, POET                       In this section, we carry out comprehensive studies
that learns from a mix of program executors (i.e.,                on POET, summarize interesting open questions,
POET-Math+SQLBART) achieves a slightly better                     and provide insights for future work. performance than the single prgoram executor. ",cs.CL,C,-0.0043461965,-0.07119597,0.17061862
http://arxiv.org/pdf/2201.11569v1,Human Interpretation of Saliency-based Explanation Over Text,"Our findings inform future
design of saliency visualizations towards closing the gap between communicated and interpreted saliency explanations,
and call for further research in the human factors in interpretation methods of AI, that study not only how the AI
operates, but how humans perceive the communicated information. 19We hypothesize that belief biases (such as sentiment polarity) exhibit more distinct expression across indiviuals, which requires subject-adaptive
correction methods and should be addressed by online estimation of individual participant slopes and intercepts within our GAMM model in future work. 15
ACKNOWLEDGMENTS

We are grateful to Paul Roit, Diego Frassinelli, Yanai Elazar, and the members of the BCAI NLP&KRR research group,

especially Sophie Henning and Stefan Grünewald, for valuable advice and feedback. ",cs.CL,A,0.033402126,0.21641925,0.16935271
http://arxiv.org/pdf/2201.11569v2,Human Interpretation of Saliency-based Explanation Over Text,"Anthropomorphism as Cognitive Bias. Philosophy of Science
within our GAMM model in future work. 84, 5 (2017), 1152–1164. ",cs.CL,A,0.4640177,0.27317584,0.17588933
http://arxiv.org/pdf/2201.11576v1,Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation,"First, the way we use neural networks to encode high-dimensional data
signiﬁcantly increases the number of parameters to train. For future work, we will explore more
efﬁcient ways for FIM calculation and better ways to handle high-dimensional gradient information. Second, we only focus on text sequence classiﬁcation tasks in this paper. ",cs.CL,B,-0.1977725,-0.20905596,0.031106094
http://arxiv.org/pdf/2201.11582v1,GUDN A novel guide network for extreme multi-label text classification,"In Section 5, we summarize this paper and indicate      to improve it to obtain a light and fast model. LightXML has
future work. reached the most advanced level. ",cs.CL,C,0.28123808,-0.26139906,-0.008172039
http://arxiv.org/pdf/2201.11582v2,GUDN: A novel guide network with label reinforcement strategy for extreme multi-label text classification,"Section 5 summarizes the
between text and label. Additionally, more than just using      paper and indicates future work. sparse linear layers to ﬁnd latent space is required. ",cs.CL,A,0.08864474,0.051360786,-0.04939539
http://arxiv.org/pdf/2201.11732v1,"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages","This suggests that cost-            D. Further Analyses
efﬁciently hand-labelling ∼ 100 shots in order to improve
the models performance on the target language, unlike in           We now provide a series of further and ﬁner-grained analy-
text-only setups, is insufﬁcient to make meaningful progress       ses, which include performance of the ‘translate train’ trans-
on V&L tasks such as XVNLI, MaRVL and xFlickr&Co. fer approach and domain-speciﬁc results in xFlickr&CO, as
This also stresses the importance of developing methods for        well as an analysis of performance over structurally different
improved zero-shot transfer in future work, as V&L models          question types of xGQA in few-shot setups. still require sufﬁcient amounts of expensive task-annotated
data to beneﬁt from in ‘non-zero-shot’ setups (Table 19). ",cs.CL,C,-0.02119851,-0.16737467,-0.0108980145
http://arxiv.org/pdf/2201.11732v2,"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages","This suggests that cost-            D. Further Analyses
efﬁciently hand-labelling ∼ 100 shots in order to improve
the models performance on the target language, unlike in           We now provide a series of further and ﬁner-grained analy-
text-only setups, is insufﬁcient to make meaningful progress       ses, which include performance of the ‘translate train’ trans-
on V&L tasks such as XVNLI, MaRVL and xFlickr&Co. fer approach and domain-speciﬁc results in xFlickr&CO, as
This also stresses the importance of developing methods for        well as an analysis of performance over structurally different
improved zero-shot transfer in future work, as V&L models          question types of xGQA in few-shot setups. still require sufﬁcient amounts of expensive task-annotated
data to beneﬁt from in ‘non-zero-shot’ setups (Table 19). ",cs.CL,C,-0.02119851,-0.16737467,-0.0108980145
http://arxiv.org/pdf/2201.11826v1,Sentiment-Aware Automatic Speech Recognition pre-training for enhanced Speech Emotion Recognition,"3-4, pp. 169–200, 1992.
sentiment classiﬁcation task can be combined with other pre-
training techniques, such as APC [22], CPC [23], which we       [4] Alan S. Cowen and Dacher Keltner, “Self-report cap-
will address in the future work. tures 27 distinct categories of emotion bridged by con-
                                                                     tinuous gradients,” PNAS, published online September
                                                                     5, 2017. ",cs.CL,A,-0.016057357,0.28488302,0.089988545
http://arxiv.org/pdf/2201.11838v1,Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences,"Second, another approach
developed to address the memory problem of long sequences is to simplify or compress the transformer architecture. In our future work, we will compare this genre of transformers, e.g. TinyBERT to our current long sequence models. ",cs.CL,C,-0.014964848,-0.42015147,-0.16200817
http://arxiv.org/pdf/2201.11838v2,Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences,"Second, another approach
developed to address the memory problem of long sequences is to simplify or compress the transformer architecture. In our future work, we will compare this genre of transformers, e.g. TinyBERT to our current long sequence models. ",cs.CL,C,-0.014964848,-0.42015147,-0.16200817
http://arxiv.org/pdf/2201.11838v3,Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences,"Second, another approach
developed to address the memory problem of long sequences is to simplify or compress the transformer architecture. In our future work, we will compare this genre of transformers, e.g. TinyBERT to our current long sequence models. ",cs.CL,C,-0.014964848,-0.42015147,-0.16200817
http://arxiv.org/pdf/2201.11867v1,Neural-FST Class Language Model for End-to-End Speech Recognition,"We demonstrated that the
                                                                      proposed method, when used in end-to-end ASR, signiﬁcantly out-
                                                                      performs vanilla NNLM in modeling rare named entities, and is less
                                                                      prone to overbiasing and much more compact than the traditional
                                                                      FST shallow fusion approach. For future work, we plan on carrying out more in-depth studies
                                                                      to better understand how robust NFCLM is to more varied use cases
                                                                      in terms of entity types, entity test set sizes, and train/test data sets. We also plan to expand the data on which the decider is trained. ",cs.CL,C,0.01282334,-0.131604,-0.09804045
http://arxiv.org/pdf/2201.11867v2,Neural-FST Class Language Model for End-to-End Speech Recognition,"We demonstrated that the
                                                                      proposed method, when used in end-to-end ASR, signiﬁcantly out-
                                                                      performs vanilla NNLM in modeling rare named entities, and is less
                                                                      prone to overbiasing and much more compact than the traditional
                                                                      FST shallow fusion approach. For future work, we plan on carrying out more in-depth studies
                                                                      to better understand how robust NFCLM is to more varied use cases
                                                                      in terms of entity types, entity test set sizes, and train/test data sets. We also plan to expand the data on which the decider is trained. ",cs.CL,C,0.01282334,-0.131604,-0.09804045
http://arxiv.org/pdf/2201.11885v1,Boosting Entity Mention Detection for Targetted Twitter Streams with Global Contextual Embeddings,"The idea of generating global contextual                            be applied for different EMD application settings, not just
  embeddings guided by a candidate mention extraction pro-                          microblog streams. In future work, we aim to expand the idea
  cess speciﬁcally relies on the recurrence of entities across                      of collective processing for the entire NER pipeline. messages – a phenomenon more typical of social media                              Acknowledgements: This work was supported in part by U.S.
  message streams. ",cs.CL,B,-0.22527269,0.15148528,-0.060544234
http://arxiv.org/pdf/2201.11903v1,Chain of Thought Prompting Elicits Reasoning in Large Language Models,"The emergence of chain        elicited in off-the-shelf language models of sufﬁcient scale. of thought reasoning only at large model scale makes it
costly to serve in real-world applications; future work could     Two recent works perhaps relate most closely to the method
explore how to induce reasoning at smaller model scales. in this paper. ",cs.CL,A,-0.022436932,0.029779872,0.3840677
http://arxiv.org/pdf/2201.11903v2,Chain of Thought Prompting Elicits Reasoning in Large Language Models,"The emergence of chain of thought reason-        improved by using a separate veriﬁer to check the reasoning
ing only at large model scale makes it costly to serve in         steps. The approach in our paper is far simpler—instead
real-world applications; future work could explore how to         of ﬁnetuning, we use prompting, which does not modify
induce reasoning at smaller model scales. the model and thus maintains the ability of a single large
                                                                  language model to perform many tasks. ",cs.CL,A,-0.03660908,0.027124995,0.41029385
http://arxiv.org/pdf/2201.11903v3,Chain of Thought Prompting Elicits Reasoning in Large Language Models,"For multiple choice or binary classiﬁcation problems, it was much more

                                                          24
likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the
commmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform
an analysis of the factuality of such chains of thought. Table 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of
thought the model generated was imperfect. ",cs.CL,A,0.31453592,0.14240064,0.42320782
http://arxiv.org/pdf/2201.11903v4,Chain of Thought Prompting Elicits Reasoning in Large Language Models,"For multiple choice or binary classiﬁcation problems, it was much more

                                                          24
likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the
commmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform
an analysis of the factuality of such chains of thought. Table 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of
thought the model generated was imperfect. ",cs.CL,A,0.31453592,0.14240064,0.42320782
http://arxiv.org/pdf/2201.11903v5,Chain of Thought Prompting Elicits Reasoning in Large Language Models,"For multiple choice or binary classiﬁcation problems, it was much more

                                                          25
likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the
commmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform
an analysis of the factuality of such chains of thought. Table 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of
thought the model generated was imperfect. ",cs.CL,A,0.317452,0.14591727,0.421125
http://arxiv.org/pdf/2201.11990v1,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","[53], we present association tests, co-
occurrence analyses and sentiment analyses designed to assess bias along a variety of dimensions. This
section serves to document and quantify some of the problems relating to bias of the model that will need to
be addressed in future work. 5.2 Gender and Occupation Analysis

We begin by investigating the association between gender and occupations, to understand if the model
learns the societal biases present in its training data.8 Following previous work [7, 39], we use the list of
323 occupations shown in Tables 18 and 19 of Section B. ",cs.CL,A,0.20396379,0.4819957,0.06710926
http://arxiv.org/pdf/2201.11990v2,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","[53], we present association tests, co-
occurrence analyses and sentiment analyses designed to assess bias along a variety of dimensions. This
section serves to document and quantify some of the problems relating to bias of the model that will need to
be addressed in future work. 5.2 Gender and Occupation Analysis

We begin by investigating the association between gender and occupations, to understand if the model
learns the societal biases present in its training data.8 Following previous work [7, 39], we use the list of
323 occupations shown in Tables 18 and 19 of Section B. ",cs.CL,A,0.20396379,0.4819957,0.06710926
http://arxiv.org/pdf/2201.11990v3,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","[53], we present association tests, co-
occurrence analyses and sentiment analyses designed to assess bias along a variety of dimensions. This
section serves to document and quantify some of the problems relating to bias of the model that will need to
be addressed in future work. 5.2 Gender and Occupation Analysis

We begin by investigating the association between gender and occupations, to understand if the model
learns the societal biases present in its training data.8 Following previous work [7, 39], we use the list of
323 occupations shown in Tables 18 and 19 of Section B. ",cs.CL,A,0.20396379,0.4819957,0.06710926
http://arxiv.org/pdf/2201.12109v1,"Protum: A New Method For Prompt Tuning Based on ""[MASK]""","What we actually do is that using the hidden states
                                     S                                  sufﬁciently by the residual unit make classiﬁcation. The
                                                                        further research of the residual unit will also be included in
Figure 5. The accuracy of the validation set of RTE when the dif-       our plan. ",cs.CL,C,0.21688837,-0.15100998,-0.15771295
http://arxiv.org/pdf/2201.12155v1,Reducing language context confusion for end-to-end code-switching automatic speech recognition,"It is an effective strategy for code-switching ASR tasks. In
the baseline achieves 5.30% relative MER reduction compared with        future work, we will explore more attention adjustment strategies,
only code-switching training data. such as whether cross attention is shared or not. ",cs.CL,C,0.053777143,-0.11537017,-0.09497627
http://arxiv.org/pdf/2201.12219v1,Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages,"that one can use our resource to enrich BabelNet further. In future work, we plan to adopt an iterative strategy     Since CLC-BN covers many more languages than Ba-
that considers gradually more and more named entities,     belNet, we can simply extend BabelNet by adding more
starting with the most conﬁdent ones. languages like Burarra, North Junín Quechua, and Mian
                                                           to it. ",cs.CL,B,-0.23143095,-0.078151524,-0.08340451
http://arxiv.org/pdf/2201.12219v2,Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages,"This type of error in-
dicates that alignment performance should be improved. In future work, we plan to explore neural architectures
that more explicitly model the problem as alignment. (2) For some low-resource languages, the output of CLC-
B has a high level of noise, so the neural model fails to
Lang. ",cs.CL,B,-0.1975319,-0.2479426,-0.15408191
http://arxiv.org/pdf/2201.12409v1,A Unified Approach to Entity-Centric Context Tracking in Social Conversations,"Enriching word vectors with subword infor-
and outputs from the previous turn and is thus prone
to error propagation. However, future work may show            1The publicly available code for the system described in
that one can transfer some of the techniques which         (Zhou and Choi, 2018) is missing some parts, so we were not
make E2ECoref successful to context tracking. able to train it on Contrack data. ",cs.CL,B,-0.28446388,0.028187972,-0.09637357
http://arxiv.org/pdf/2201.12409v2,A Unified Approach to Entity-Centric Context Tracking in Social Conversations,"As mentioned above the Contrack baseline
                                                           makes its decisions based only on the current utterance
          Precision Recall              F1                 and outputs from the previous turn and is thus prone
                                                           to error propagation. However, future work may show
Coach     98.2  97.8                  98.0                 that one can transfer some of the techniques which
                                                           make E2ECoref successful to context tracking. Coach-B   98.2  98.0                  98.1
                                                           Plural Coreference Resolution. ",cs.CL,B,-0.046087563,-0.08813746,-0.017438423
http://arxiv.org/pdf/2201.12431v1,Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval,"still computes the interpolation of pauto with pLM . As an
interesting direction for future work, we expect that learning  9. Conclusion
a dynamic interpolation factor λ, similarly to ADAPTRET,
will even further improve RETOMATON’s results. ",cs.CL,C,0.18547213,-0.31164068,-0.19037
http://arxiv.org/pdf/2201.12431v2,Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval,"In contrast, RETOMATON still computes the
become more speciﬁc; for example, two of them contain           interpolation of pauto with pLM , thanks to its pointers. As an
part of the name of the song “Where the Streets Have No         interesting direction for future work, we expect that learning
Name” by the band U2. Nevertheless, we selected sequences       a dynamic interpolation factor λ, similarly to ADAPTRET,
into the list of length=10 such that none of them appeared      will even further improve RETOMATON’s results. ",cs.CL,C,0.08747369,-0.1882273,-0.20291093
http://arxiv.org/pdf/2201.12502v2,Unsupervised Multi-Granularity Summarization,"In pursuit of a more abstractive summary,
mation well into the generated summaries and thus       rephrasing events into different forms may be a
boosts performance. Furthermore, GRANUSUM               viable option, and we leave it as future work. outperforms Selector, which is a strong extractive
baseline, and extractive approaches usually domi-          4) In this paper we focus on three different lev-
nate unsupervised summarization tasks. ",cs.CL,B,-0.16837013,-0.020591564,-0.015249547
http://arxiv.org/pdf/2201.12502v3,Unsupervised Multi-Granularity Summarization,"tent. In pursuit of a more abstractive summary,
Overall, GRANUSUM improves R-1 score by 1.0           rephrasing events into different forms may be a
on average compared to the previous best results,     viable option, and we leave it as future work. indicating that it is sufﬁcient to generate quality
summaries besides the multi-granularity ability. ",cs.CL,A,0.061552927,0.07840808,-0.0048126252
http://arxiv.org/pdf/2201.12507v1,AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,"We compare our method with
layer of the teacher model to the last layer of the student        the baseline methods on two single-sentence classiﬁcation
model. Automatically selecting which layers to align is an         tasks (CoLA (Warstadt et al., 2018), SST-2 (Socher et al.,
interesting research direction that we defer to future work. 2013)), two similarity and paraphrase tasks (MRPC (Dolan
                                                                   & Brockett, 2005), QQP (Chen et al., 2018)), and three infer-
Formally, the SuperLM for sub-space Ak is trained as:              ence tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar
                                                                   et al., 2016), RTE (Dagan et al., 2005; Haim et al., 2006;
        WA∗k = arg min Eα∈A[L(Wα; U ; Dtrain)], (9)                Giampiccolo et al., 2007; Bentivogli et al., 2009))1. ",cs.CL,B,-0.21262494,-0.038079187,0.18143825
http://arxiv.org/pdf/2201.12507v2,AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,"In addition, we only transfer the self-attention knowledge from the
last layer of the teacher model to the last layer of the student model. Automatically selecting which layers to align is an
interesting research direction that we defer to future work. Formally, the SuperLM for sub-space Ak is trained as:

                                            WA∗k = arg min Eα∈A[L(Wα; U ; Dtrain)], (9)

                                                                                   W

where, K is the number of sub-space partitions; W are the weights of the SuperLM; Wα are the weights in W speciﬁed
by the architecture α; U are the weights of the teacher model including the self-attention module used for distillation;
Dtrain is the training data set, and L(·) is the self-attention loss function from Eqn. ",cs.CL,C,0.12756208,-0.21818957,0.17645057
http://arxiv.org/pdf/2201.12549v1,A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis,"This leads
                                                      to the correct ﬁnal predictions. Error Analysis We further study the errors that
                                                      our approach still makes to provide some sugges-
tions for future research. As shown in Table 10,        Ting Chen, Simon Kornblith, Mohammad Norouzi,
there are three main error types. ",cs.CL,C,0.31073114,-0.076885425,-0.044152074
http://arxiv.org/pdf/2201.12664v1,A Deep CNN Architecture with Novel Pooling Layer Applied to Two Sudanese Arabic Sentiment Datasets,"Finally,
the proposed MMA pooling was compared to Max, Avg and Min baselines and shown to perform better than them in
both 2-way and 3-way classiﬁcation. In future work, we plan to use an attention mechanism as part of a more complex deep learning method, to extract
features from a huge corpus covering all Arabic sentiment dialects. 7 Data Availability

The SudSenti2 and SudSenti3 datasets are publicly available15. ",cs.CL,B,-0.3088498,0.043974306,-0.05198489
http://arxiv.org/pdf/2201.12793v1,Part of Speech Tagging (POST) of a Low-resource Language using another Language (Developing a POS-Tagged Lexicon for Kurdish (Sorani) using a Tagged Persian (Farsi) Corpus),"We then organize the result into three lists
according to the mentioned labels. In this paper, we focus on the correct list and leave the rest for future work. We manually
process the correct list further to check the accuracy and the relevance of the tags for the trans-
lated lexicon. ",cs.CL,B,-0.17080592,0.10305937,-0.24422967
http://arxiv.org/pdf/2201.12799v1,Recognition of Implicit Geographic Movement in Text,"(2002). Supercell Outbreak - 8th February
identify the entity types in our future work using a POS          2002 - including video footage. Tagger and a rules-based approach. ",cs.CL,A,0.18333569,0.06194499,-0.0513266
http://arxiv.org/pdf/2201.12833v1,Word Segmentation and Morphological Parsing for Sanskrit,"2020. Evaluating Neural Morpho-
   One line of future work therefore could enhance         logical Taggers for Sanskrit. In Proceedings of the
our approach by including more explicit linguistic         17th SIGMORPHON Workshop on Computational
information, for example by consulting a dictio-           Research in Phonetics, Phonology, and Morphology,
nary when decoding to rule out stemming rules or           pages 198–203, Online. ",cs.CL,B,-0.21072632,0.08466157,-0.12429052
http://arxiv.org/pdf/2201.13242v1,Correcting diacritics and typos with ByT5 transformer model,"Similarly, one can build a model of multiple languages
to gain beneﬁts by overlapping vocabularies and semantics of related under-represented lan-
guages, although studies report contradictory results [48, 46]. We think that all these scaling
approaches are promising as future work. In our work, we generated the typos for the entire datasets just once, but in principle,
we could generate diﬀerent typos each time we pass through the dataset. ",cs.CL,B,-0.23624219,0.014588048,-0.110931374
http://arxiv.org/pdf/2201.13242v2,Correcting diacritics and typos with a ByT5 transformer model,"Similarly, one can build a model of multiple languages
to gain beneﬁts by overlapping vocabularies and semantics of related under-represented lan-
guages, although studies report contradictory results [48, 46]. We think that all these scaling
approaches are promising as future work. In our work, we generated the typos for the entire datasets just once, but, in principle,
we could generate diﬀerent typos each time we pass through the dataset. ",cs.CL,B,-0.23816815,0.014226695,-0.10962949
http://arxiv.org/pdf/2201.13405v1,Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,"(2021). We leave experimentation with more so-
phisticated model variants (Liu et al., 2020b) and sampling
methods such as nucleus sampling (Holtzman et al., 2020)
for future work. For brevity, we do not report results with
other automatic metrics relevant to E2E modelling such as
Task Success Rate or Dialogue Success Rate (Budzianowski
and Vulic´, 2019). ",cs.CL,C,0.14843674,-0.042167433,0.08426624
http://arxiv.org/pdf/2201.13429v1,Constrained Density Matching and Modeling for Cross-lingual Alignment of Contextualized Representations,"(2020) and Zhao et al. (2021)
      (see §4.3), which calls for an improvement in     ﬁnd that applying vector space normalization is
      future work. helpful to yield language-neural representations,
                                                        but there lacks a thorough study on unsupervised,
   • Not only are validation criteria important for     contextualized alignment for multilingual represen-
      guiding unsupervised training, but also for       tations. ",cs.CL,B,-0.3200032,-0.025797725,-0.10216488
http://arxiv.org/pdf/2201.13429v2,Constrained Density Matching and Modeling for Cross-lingual Alignment of Contextualized Representations,"Second, our unsupervised alignment integrated in bootstrapping
       procedure rivals supervised counterparts, showing that parallel data can be removed without
       sacriﬁcing performance. But we admit that these alignments, be it supervised or not, are poor
       in generalization (see §4.3), calling for an improvement in future work. • Not only are validation criteria crucial for guiding unsupervised training, but also for super-
       vised training. ",cs.CL,C,0.019346882,-0.14640272,-0.064624146
http://arxiv.org/pdf/2202.00254v1,Active Learning Over Multiple Domains in Natural Language Tasks,"Our analysis shows the importance
of example selection in existing methods, and also
the surprising potential of domain budget alloca-
tion strategies. Combining families of methods,
or trying domain adaptation techniques on top of
selected example sets, offer promising directions
for future work. References                                                Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur
                                                             Guney, Volkan Cirik, and Kyunghyun Cho. ",cs.CL,C,0.07563729,-0.1537588,0.06316196
http://arxiv.org/pdf/2202.00254v2,Active Learning Over Multiple Domains in Natural Language Tasks,"Our analysis shows the importance
of example selection in existing methods, and also
the surprising potential of domain budget alloca-
tion strategies. Combining families of methods,
or trying domain adaptation techniques on top of
selected example sets, offer promising directions
for future work. Acknowledgements

The authors would like to thank Moises Gold-
szmidt, Ehsan Mousavi, and Stephen Pulman for
helpful discussions and feedback. ",cs.CL,C,0.07572016,-0.17759651,0.041279607
http://arxiv.org/pdf/2202.00291v2,XAlign: Cross-lingual Fact-to-Text Alignment and Generation for Low-Resource Languages,"We make our code            ters, sentence delimiters and non-breaking preﬁxes. and dataset publicly available1, and hope that this     We prune out (1) other language sentences using
will help advance further research in this critical     Polyglot language detector3, (2) sentences with less
area. than 5 words or more than 100 words, (3) sentences
                                                        which could potentially have no factual information
3 Data Collection & Pre-processing                      (sentences with no noun or verb4). ",cs.CL,B,-0.30555245,0.1295068,-0.05821189
http://arxiv.org/pdf/2202.00298v1,Research on Question Classification Methods in the Medical Field,"Section 4 describes the experimental design and
analyzes the experimental results. Section 5 brieﬂy summarizes the conclusions and future work. 2 Related work

At present, the experimental methods of problem classiﬁcation are mainly divided into two types:
statistical-based machine learning methods and neural network-based deep learning methods. ",cs.CL,C,0.21043542,-0.10840972,0.15601373
http://arxiv.org/pdf/2202.00396v2,"Politics, Sentiment and Virality: A Large-Scale Multilingual Twitter Analysis in Greece, Spain and United Kingdom","By performing an exhaustive search for             tive, neutral). Finally, we hope that both our methodol-
a successful sentiment classiﬁer we obtained a robust             ogy and released multilingual models can be leveraged
multilingual model capable of accurately identifying              in future work for subsequent large-scale sociological
sentiment in politicians’ tweets. This is achieved by uti-        studies, including other topics. ",cs.CL,A,-0.14028916,0.41545916,-0.075076856
http://arxiv.org/pdf/2202.00399v1,Language Dependencies in Adversarial Attacks on Speech Recognition Systems,"Instead, we only observe how the parameters change
and act for each phoneme type. In future work, this experiment            The German dataset is smaller than the English one but it
should be expanded to a broader dataset covering more cases to       is also cleaner with respect to the sound quality. We can as-
deliver statistically relevant results. ",cs.CL,B,-0.09029164,0.024223043,-0.24008963
http://arxiv.org/pdf/2202.00399v2,Language Dependencies in Adversarial Attacks on Speech Recognition Systems,"Instead, we only observe how the parameters change
and act for each phoneme type. In future work, this experiment            The German dataset is smaller than the English one but it
should be expanded to a broader dataset covering more cases to       is also cleaner with respect to the sound quality. We can as-
deliver statistically relevant results. ",cs.CL,B,-0.09029164,0.024223043,-0.24008963
http://arxiv.org/pdf/2202.00436v1,Causal Inference Principles for Reasoning about Commonsense Causality,"We     founded and demonstrates great potential as shown by em-
observe that in general, increasing covariate set size im-    pirical studies of various datasets. There are several possible
proves performances if is reasonable: if is too small,        avenues for future works. (i) Prompt engineering for bet-
added covariates may have little impacts while they may       ter temporal predictors and event sampler will likely beneﬁt
introduce more noises if is too large. ",cs.CL,C,0.2835166,-0.016568279,0.020268898
http://arxiv.org/pdf/2202.00436v2,ROCK: Causal Inference Principles for Reasoning about Commonsense Causality,"Temporality Fine-Tuning. Shaded rows in Table 1 show           There are several possible avenues for future works. (i)
that when we use the pretrained RoBERTa-BASE without           Prompt engineering for better temporal predictors and
temporality ﬁne-tuning (we increase k to 30), almost all es-   event sampler will likely beneﬁt ROCK. ",cs.CL,C,0.09594413,-0.20140462,0.06570551
http://arxiv.org/pdf/2202.00443v1,The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,"2008). As future work, we plan to extend our benchmark
by incorporating privacy-preserving replacements for masked terms. This is certainly
challenging, because multiple combinations of replacements could be equally valid to

30
Ildikó Pilán et al. ",cs.CL,A,0.091887295,0.02956098,-0.29283446
http://arxiv.org/pdf/2202.00443v2,The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,"2008). As future work, we plan to extend our benchmark
by incorporating privacy-preserving replacements for masked terms. This is certainly
challenging, because multiple combinations of replacements could be equally valid to
prevent disclosure, but only one would be optimal from the perspective of data utility
preservation. ",cs.CL,A,0.13910384,0.016726427,-0.25828508
http://arxiv.org/pdf/2202.00466v1,Learning Invariable Semantical Representation from Language for Extensible Policy Generalization,"And our method also has some                 [10] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba,
limits which are caused by adapting to the environment. In                  and P. Abbeel, “Domain randomization for transferring
future work, we will attempt to break through the shortage. deep neural networks from simulation to the real world,”
Besides, we consider that if we can let the agent learn plenty              in 2017 IEEE/RSJ international conference on intelligent
of linguistic semantics, we can also teach it to think and                  robots and systems (IROS). ",cs.CL,B,-0.13338184,-0.086503975,0.23954126
http://arxiv.org/pdf/2202.00484v1,Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis,"However, currently our model can only process a single target as input, and
we will conduct further work on multiple targets in the future. We hope that this study will inspire
the community to investigate further research into cross-domain aspect-based sentiment analysis to
greatly reduce the time and economic cost of retraining. References

[Alhuzali and Ananiadou, 2021] Alhuzali, H. and Ananiadou, S. (2021). ",cs.CL,A,-0.009041407,0.24684957,0.1096385
http://arxiv.org/pdf/2202.00484v2,Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis,"However, currently our model can only process a single target as
input, and we will conduct further work on multiple targets in the future. We hope that this study
will inspire the community to investigate further research into cross-domain aspect-based sentiment
analysis to greatly reduce the time and economic cost of retraining. References

[Alhuzali and Ananiadou, 2021] Alhuzali, H. and Ananiadou, S. (2021). ",cs.CL,A,-0.009041407,0.24684957,0.1096385
http://arxiv.org/pdf/2202.00486v1,Towards a Theoretical Understanding of Word and Relation Representation,"Bringing
together text and knowledge graphs may also lead to improved methods for extracting
relations from text. We hope that future work can build more generally on the insights into latent seman-
tic structure presented in this thesis to develop algorithms that perform better, oﬀer
greater interpretability and allow unwanted statistical biases in the data to be miti-
gated. Future work may also extend the understanding of un-contextualised embeddings
to contextualised word embeddings that presently achieve impressive performance in
many downstream NLP tasks (Devlin et al., 2019; Brown et al., 2020) and may become
increasingly pervasive across numerous applications. ",cs.CL,B,-0.2991606,0.13075233,0.07829817
http://arxiv.org/pdf/2202.00535v1,Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning,"the effectiveness of our methods. As future work, we are in-
terested in applying our proposed approaches to other large                     Acknowledgments
pre-trained language models to investigate how performance
varies with different model architectures and size. We would  We would like to thank Edgar Meij, Srivas Prasad, Nimesh
also like to explore RAPT for other downstream tasks like     Ghelani and the anonymous reviewers for their constructive
semantic parsing, natural language inference, named entity    feedback and suggestions. ",cs.CL,B,-0.28684768,0.013269732,0.24882525
http://arxiv.org/pdf/2202.00535v2,Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning,"the effectiveness of our methods. As future work, we are in-
terested in applying our proposed approaches to other large    putational Linguistics and the 11th International Joint Con-
pre-trained language models to investigate how performance     ference on Natural Language Processing (Volume 1: Long
varies with different model architectures and size. We would   Papers), 3816–3830. ",cs.CL,B_centroid,-0.37790844,-0.016548641,0.12495261
http://arxiv.org/pdf/2202.00540v1,Dominant Set-based Active Learning for Text Classification and its Application to Online Social Media,"However, in the second phase, later stages of selection, taking
the model uncertainty into the account can improve the selection performance. Such hybrid methods are not currently
well-studied and we believe that our encouraging results will stimulate further research on similar active learning
strategies. 7 Ethics Statement

Our proposed active learning approach has the potential to mitigate the difﬁculties associated with the annotation, and
classiﬁcation of textual content, e.g. ",cs.CL,A,0.079419516,0.19296773,0.21274081
http://arxiv.org/pdf/2202.00609v1,Semantic of Cloud Computing services for Time Series workflows,"7, pp. 1645–
     Finally, as a future work, we propose the implementation of a Cloud        1660, 2013. Computing services broker for TS that has the ability to optimize a
workﬂow written in tswf-schema taking advantage of algorithm imple-       [2] “Capture the value in iot - ericsson,” Aug
mentations and computing capabilities over different CC platforms. ",cs.CL,C,0.26817006,-0.09280461,-0.07249716
http://arxiv.org/pdf/2202.00794v1,Learning to pronounce as measuring cross lingual joint orthography-phonology complexity,"In order to further
pursue this, work a few interesting notes should be made. First, that as seen with logographic
languages and languages with high phoneme-source letter ambiguity the number of training
samples must be higher for effective learning therefore in future work more comprehensive
datasets should be used such as Wiktionary and comparison based not on absolute number of
samples but on number of samples per class should be made. Additionally, in order to prove out
the theory that the simplest (closest to a 1:1 mapping between phoneme and grapheme) or most
expressive phonetic orthographies are the easiest to pronounce we should sample many different
orthographies and devise a metric for complexity that isn’t simply a ratio of grapheme over IPA
letter. ",cs.CL,B,-0.17324054,0.011337411,-0.26824188
http://arxiv.org/pdf/2202.00794v2,Learning to pronounce as measuring cross-lingual joint orthography-phonology complexity,"Additionally, in order to prove out
the theory that the simplest (closest to a 1:1 mapping between phoneme and grapheme) or most
expressive phonetic orthographies are the easiest to pronounce we should sample many different
orthographies and devise a metric for complexity that isn’t simply a ratio of grapheme over IPA
letter. Due to the simplicity of the ratio of absolute number in grapheme and phoneme inventories,
future work should develop measures based on theorical results from linguistics on how each
phoneme maps to each grapheme. Finally, once a fairer complexity measure and task is developed
for cross-lingual comparison, we should look at performing an ablation study that illustrates how
complexity of learning to pronounce a language change under training and modeling strategies
that are known to improve the ability to transliterate such as encoding phonetic similarity in
features, self-supervision, data augmentation, and joint or cross language modeling. ",cs.CL,B,-0.14414246,0.010640064,-0.22549595
http://arxiv.org/pdf/2202.00795v1,Disaster Tweets Classification using BERT-Based Language Model,"We have also made comparision about the effectiveness
of some text encoders. For future work, we would like to
explore different classiﬁcation head architectures. Instead of
only using BERT, we will experiment with other improved
version of it. ",cs.CL,B,-0.14157966,-0.05549278,0.03316771
http://arxiv.org/pdf/2202.00828v1,Co-training Improves Prompt-based Learning for Large Language Models,"(2021). for future work. For RTE, the label words were Yes, No. ",cs.CL,A,0.027066352,0.24051347,-0.2375624
http://arxiv.org/pdf/2202.00901v1,Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing,"Despite retrieval having the most room for im-
Classify and Fill         84.80  87.16
RAFBASE                   87.03  89.34                   provement, we also see some evidence ﬁlling strug-
                          83.69  86.00                   gles in certain multilingual transfer cases; for ex-
   - Hard Negatives       85.87  88.23                   ample, providing gold spans can improve en→th
   - Identity Masking     86.26  88.69                   transfer by +16.7%. As such, there is ample op-
   - Scenario Fusion      86.76  89.10                   portunity for optimizing the retrieval and ﬁlling
   - Parameter sharing    86.70  89.05                   modules in future work. - Repr. ",cs.CL,C,0.039447192,-0.14360009,-0.21542314
http://arxiv.org/pdf/2202.00964v1,Understanding Knowledge Integration in Language Models with Graph Convolutions,"In summary, to support relation information in our GCS model, users should
carefully select powerful graph ﬁlters (i.e., graph neural network models) based on the speciﬁc scenario. We would like to
leave the exploration for future work. 19Entities with 0.4 < ai,i < 0.6 on self-loops as well-learned (WL) entities. ",cs.CL,C,0.23629482,-0.159458,0.22419424
http://arxiv.org/pdf/2202.00964v2,Understanding Knowledge Integration in Language Models with Graph Convolutions,"In summary, to support relation information in our GCS model, users should
carefully select powerful graph ﬁlters (i.e., graph neural network models) based on the speciﬁc scenario. We would like to
leave the exploration for future work. 19Entities with 0.4 < ai,i < 0.6 on self-loops as well-learned (WL) entities. ",cs.CL,C,0.23629482,-0.159458,0.22419424
http://arxiv.org/pdf/2202.00964v3,Understanding Knowledge Integration in Language Models with Graph Convolutions,"In summary, to support relation information in our GCS model, users should
carefully select powerful graph ﬁlters (i.e., graph neural network models) based on the speciﬁc scenario. We would like to
leave the exploration for future work. 19Entities with 0.4 < ai,i < 0.6 on self-loops as well-learned (WL) entities. ",cs.CL,C,0.23629482,-0.159458,0.22419424
http://arxiv.org/pdf/2202.00964v4,Understanding Knowledge Integration in Language Models with Graph Convolutions,"In summary, to support relation information in our GCS model, users should
carefully select powerful graph ﬁlters (i.e., graph neural network models) based on the speciﬁc scenario. We would like to
leave the exploration for future work. 19Entities with 0.4 < ai,i < 0.6 on self-loops as well-learned (WL) entities. ",cs.CL,C,0.23629482,-0.159458,0.22419424
http://arxiv.org/pdf/2202.00964v5,What Has Been Enhanced in my Knowledge-Enhanced Language Model?,"On bayesian bounds. In
ered by future works. Besides, GCS only provides          ICML, volume 148 of ACM International Confer-
a way to interpret the knowledge integration. ",cs.CL,C,0.23197904,-0.085819386,0.17501795
http://arxiv.org/pdf/2202.00964v6,What Has Been Enhanced in my Knowledge-Enhanced Language Model?,"On bayesian bounds. In
ered by future works. Besides, GCS only provides          ICML, volume 148 of ACM International Confer-
a way to interpret the knowledge integration. ",cs.CL,C,0.23197904,-0.085819386,0.17501795
http://arxiv.org/pdf/2202.00964v7,What Has Been Enhanced in my Knowledge-Enhanced Language Model?,"On bayesian bounds. In
ered by future works. Besides, GCS only provides          ICML, volume 148 of ACM International Confer-
a way to interpret the knowledge integration. ",cs.CL,C,0.23197904,-0.085819386,0.17501795
http://arxiv.org/pdf/2202.01107v1,Keyword localisation in untranscribed speech using visually grounded speech models,"Black, M. Hasegawa-Johnson, F. Metze,
future work. We also showed that many incorrect localisations                           G. Neubig, S. Stu¨ker, P. Godard, M. Mu¨ller, L. Ondel, S. Palaskar,
is because semantically related words are located—seeing what                           P. Arthur, F. Ciannella, M. Du, E. Larsen, D. Merkx, R. Riad, L. Wang,
beneﬁt this holds is another avenue for future work. and E. Dupoux, “Speech technology for unwritten languages,” IEEE/ACM
                                                                                        TASLP, 2020. ",cs.CL,B,-0.29571548,0.07735516,-0.2528122
http://arxiv.org/pdf/2202.01145v1,Relative Position Prediction as Pre-training for Text Encoders,"Rewiring with positional encodings for graph neural networks. In future work, we hope to investigate how our
model responds to increased scale. In particular,     Xinlei Chen and Kaiming He. ",cs.CL,C,0.11903942,-0.20710479,0.041299067
http://arxiv.org/pdf/2202.01155v1,The slurk Interaction Server Framework: Better Data for Better Dialog Models,"Bots can change, in-
                                                                         sert or delete text messages and pretend to be another
       ""type"": ""add""                                                     user. In the same way, it is possible to intercept the au-
                                                                         dio and video channels, although we are leaving such
   },                                                                    tests to future work. ""date_created"": ""2022-01-04T17:02:23"",                                Multimodal context: The display area serves as di-
                                                                         alog context, controlled by JAVASCRIPT, and can con-
   ""date_modified"": null,                                                tain arbitrary HTML elements. ",cs.CL,A,0.1420407,0.21518473,0.020599335
http://arxiv.org/pdf/2202.01157v1,Error Correction in ASR using Sequence-to-Sequence Models,"rescoring is shown to yield further improvements. As future work, we will try to incorporate speech         Jongseok Park, Kyubyong & Kim. 2019. g2pe. ",cs.CL,A,-0.008491449,0.098792166,-0.083839804
http://arxiv.org/pdf/2202.01157v2,Error Correction in ASR using Sequence-to-Sequence Models,"In
error correction. As future work, we would try               Proceedings of the 58th Annual Meeting of the Asso-
to incorporate speech features from audio instead            ciation for Computational Linguistics, pages 4248–
of just phoneme sequences, and jointly learn to              4254.
correct the errors in the transcription. Shun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizu-
References                                                   moto, and Kentaro Inui. ",cs.CL,B,-0.19727214,-0.051980328,-0.22622031
http://arxiv.org/pdf/2202.01159v2,"L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources","Marathi Corpus. The dataset and resources are              Another data set for Marathi NER was introduced
publicly shared to facilitate further research in          in (Murthy et al., 2018). It consists of 5591 sen-
Marathi NLP. ",cs.CL,B,-0.1471031,0.1767531,-0.37092674
http://arxiv.org/pdf/2202.01169v1,Unified Scaling Laws for Routed Language Models,"While the differences between this model and those analyzed in the paper make concrete extrapolation impossible, it
shows that routing techniques still maintain competitive improvements at almost an order of magnitude larger value of
N than analyzed and it is unlikely the scaling coefﬁcients measured in this work substantially overestimate the routing
technique’s scalability. We encourage future work probing the limits of routing networks, both in N and E, to better
understand their properties and provide more accurate predictions of their scaling coefﬁcients. H Extra Plots and Tables

This section contains some helpful visualizations and data which are not included in the main text. ",cs.CL,C,0.31247747,-0.13019234,-0.15188913
http://arxiv.org/pdf/2202.01169v2,Unified Scaling Laws for Routed Language Models,"While the differences between this model and those analyzed in the paper make concrete extrapolation impossible, it
shows that routing techniques still maintain competitive improvements at almost an order of magnitude larger value of
N than analyzed and it is unlikely the scaling coefﬁcients measured in this work substantially overestimate the routing
technique’s scalability. We encourage future work probing the limits of routing networks, both in N and E, to better
understand their properties and provide more accurate predictions of their scaling coefﬁcients. H Extra Plots and Tables

This section contains some helpful visualizations and data which are not included in the main text. ",cs.CL,C,0.31247747,-0.13019234,-0.15188913
http://arxiv.org/pdf/2202.01286v1,ASR-Aware End-to-end Neural Diarization,"for the phone and word boundaries. We do not perform multi-task
learning with speaker change features as the speaker change infor-          In future work, we hope to explore similar techniques with a
mation is already subsumed in the diarization loss function. system like [24] that does not require prior knowledge of the number
                                                                        of speakers and allows inference with low latency. ",cs.CL,B,-0.11686392,-0.109973244,-0.08051333
http://arxiv.org/pdf/2202.01302v1,A Comparison of Online Hate on Reddit and 4chan: A Case Study of the 2020 US Election,"Finally, our
domains in both datasets have been listed in Table 3. The difference   findings are only representative of one case study (the US election),
in the number of URLs used within the Reddit and 4chan posts is        therefore future work should compare these across other case stud-
quite significant. The 4chan dataset contained 226,394 URLs in total,  ies or events to gain a more comprehensive understanding of how
where 40,256 of these were unique URLs and 21% of all the posts con-   these platforms are used in distinct ways. ",cs.CL,A,0.20241316,0.26179376,-0.18960917
http://arxiv.org/pdf/2202.01374v1,mSLAM: Massively multilingual joint pre-training for speech and text,"We hope that this work catalyzes        https://aclanthology.org/W19-5301. further research towards improving and understanding uni-
versal, multi-modal pre-trained models. Barrault, L., Biesialska, M., Bojar, O., Costa-jussa`, M. R.,
                                                                  Federmann, C., Graham, Y., Grundkiewicz, R., Had-
References                                                        dow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P.,
                                                                  Lo, C.-k., Ljubesˇic´, N., Monz, C., Morishita, M., Nagata,
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler,         M., Nakazawa, T., Pal, S., Post, M., and Zampieri, M.
   M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,         Findings of the 2020 conference on machine translation
   and Weber, G. Common voice: A massively-multilingual          (WMT20). ",cs.CL,B,-0.08016475,-0.14571217,0.052293185
http://arxiv.org/pdf/2202.01709v1,Towards Coherent and Consistent Use of Entities in Narrative Generation,"However, we leave that          (MNEMELM). For attending to the entity memory, we add
                                                                      new, randomly initialized cross-attention blocks in paral-
to future work, since dependency parsing is as yet imperfect          lel with self-attention per layer resembling the architecture
                                                                      of adapters3 (Houlsby et al., 2019). We propose using the
    2We also experimented with variants of the metric, where all in-  entity memory together with the prompt for richer entity
termediate entity mentions are considered, but we ﬁnd that results    representations and to better preserve entity-related informa-
are similar and the maximum span better quantiﬁes the problem         tion over a long time horizon. ",cs.CL,B,-0.21877944,-0.09225615,0.09721643
http://arxiv.org/pdf/2202.01764v1,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,"The baseline reaches promising results, but there is plenty
of room for improvement. Extension of the dataset, such as covering longer answers, is left for future work. The
dataset and our experiments are available at https://github.com/SkelterLabsInc/JaQuAD. ",cs.CL,C,0.10457645,-0.0907809,-0.09148313
http://arxiv.org/pdf/2202.01855v1,Self-supervised Learning with Random-projection Quantizer for Speech Recognition,"The quantizer uses random initialization and
                                                                      does not update the parameters, and therefore the initializa-
                                                                      tion algorithm can play an important role on the results. In
                                                                      this paper we showed results with Xavier initialization, and
                                                                      further comparisons on different initialization algorithms
                                                                      can be conduct in the future work. 6. ",cs.CL,C,0.17085864,-0.3255045,-0.1736637
http://arxiv.org/pdf/2202.01855v2,Self-supervised Learning with Random-projection Quantizer for Speech Recognition,"Understanding the difﬁculty
for the projection matrix and the standard normal distribu-       of training deep feedforward neural networks. In Pro-
tion for the codebook, and further comparisons on different       ceedings of the Thirteenth International Conference on
initialization algorithms can be conduct in the future work. Artiﬁcial Intelligence and Statistics, volume 9 of Proceed-
                                                                  ings of Machine Learning Research, pp. ",cs.CL,C,0.05738342,-0.26399064,0.026963983
http://arxiv.org/pdf/2202.02013v1,A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications,"The higher    of detecting artiﬁcially generated academic content. scores are obtained by DistilBERT model on both the        As a future work, we plan to develop original methods
datasets: 62.5% and 70.2% for the fully generated          oriented to the task of detecting automatically gener-
dataset and the hybrid dataset, respectively. Interest-    ated texts and fragments, possibly leveraging knowl-
ingly, if most models perform slightly better on the Hy-
brid Dataset, it is not the case of LSTM model which
achieves a much better score on the Fully Generated
Dataset and of BERT and passive aggressive classiﬁer
Model used                                                       Accuracy
Bag of words, Multinomial Naive Bayes Algorithm                  19.7
Bag of words, Passive Aggressive Classiﬁer Algorithm             31.8
Bag of words, Multinomial Classiﬁer with Hyperparameter (alpha)  19.7
Bag of words, SVM                                                37.9
LSTM model                                                       59.1
Bi-LSTM (Latest Paper)                                           40.9
BERT                                                             52.5
DistilBERT                                                       62.5

                   Table 5: Classiﬁcation Results for Fully Generated Dataset

Model used                                                       Accuracy
Bag of words, Multinomial Naive Bayes Algorithm                  24.2
Bag of words, Passive Aggressive Classiﬁer Algorithm             30.3
Bag of words, Multinomial Classiﬁer with Hyperparameter (alpha)  22.7
Bag of words, SVM                                                37.9
LSTM model                                                       50.0
Bi-LSTM (Latest Paper)                                           47.0
BERT                                                             50.0
DistilBERT                                                       70.2

                   Table 6: Classiﬁcation Results for Hybrid Dataset

Metric  Full-text  Hybrid   Maroniko                        sentiment-preserving fake online reviews using neu-
        Dataset    Dataset  -lakis et                       ral language models and their human-and machine-
                            al., 2020                       based detection. ",cs.CL,B,-0.13057984,0.024531057,-0.042281438
http://arxiv.org/pdf/2202.02013v2,A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications,"of detecting artiﬁcially generated academic content. The classiﬁcation results are presented in Table 5 and       As a future work, we plan to develop original methods
Table 6 for fully generated and hybrid datasets, respec-     oriented to the task of detecting automatically gener-
tively. As per the results, the highest classiﬁcation        ated texts and fragments, possibly leveraging knowl-
score was obtained by the DistilBERT model regard-           edge to detect contradictions and out-of-context sen-
ing both the datasets: the scores are 62.5% and 70.2%        tences. ",cs.CL,B,-0.11698867,0.16821703,-0.037080243
http://arxiv.org/pdf/2202.02093v1,Temporal Attention for Language Models,"on all but the English dataset it hurts performance. We wish to study how to best combine the two
7 Conclusion                                             approaches in future work. Additionally, for fu-
                                                         ture work, we plan to extend this work by apply-
In this paper, we presented a time-aware self-           ing temporal attention to other tasks, such as web
attention mechanism as an extension of the original      search and sentence time prediction, as well as ex-
mechanism of the transformer. ",cs.CL,B,-0.16429591,0.06744277,0.12181242
http://arxiv.org/pdf/2202.02093v2,Temporal Attention for Language Models,"We conduct an experiment evaluat-
ing the marginal addition of time token prepending
along with temporal attention and conclude that
on all but the English dataset it hurts performance. We wish to study how to best combine the two
approaches in future work. Additionally, for fu-
ture work, we plan to extend this work by apply-
ing temporal attention to other tasks, such as web
search and sentence time prediction, as well as ex-
Hila Gonen, Ganesh Jawahar, Djamé Seddah, and             Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and
   Yoav Goldberg. ",cs.CL,B,-0.20845436,0.13642585,0.06406062
http://arxiv.org/pdf/2202.02170v1,The Ecological Footprint of Neural Machine Translation Systems,"Based on the results in Table 9 and Table 10, the low quanti-
zation time and the fact that quantized models can easily be run on CPU, we would
recommend quantized models at translation time for large-scale translation projects
in the pursuit of greener MT. 7 Conclusions and future work

In the era of deep learning, neural models are continuously pushing the boundaries
in NLP, MT including. The ever growing volumes of data and the advanced, larger
models keep delivering new state-of-the-art results. ",cs.CL,B,-0.16800143,-0.26355278,0.02586915
http://arxiv.org/pdf/2202.02312v1,Interactive Mobile App Navigation with Uncertain or Under-specified Natural Language Commands,"Ideally, a vision-language model
should recognize what portion of a task has already been                      In addition to needing improved representations, models
completed. In future work, it may be useful to add explicit                for completing mobile app tasks should be able to recog-
losses to predict how much of a task has been completed                    nize when subtasks have been completed, and current meth-
given the current observed state, or predicting whether we                 ods only succeed with step-by-step instruction. A necessary
are ‘before’ or ‘after’ a particular subtask of our ﬁnal goal. ",cs.CL,A,0.011711452,-0.01298582,0.17770275
http://arxiv.org/pdf/2202.02312v2,A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility,"For both test splits, the action and grounding accuracy is sig-
nificantly higher with low-level input. As the VLN methods showed having both
high-level and low-level inputs can improve performance, adapting Seq2Act to
take both as input would be important in future work. 11.4 Generalization of Natural Language Commands across Apps

We lastly evaluate generalization of our task automation methods to natural
language tasks. ",cs.CL,B,-0.14039272,-0.092819706,0.16192947
http://arxiv.org/pdf/2202.02312v3,A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility,"For both test splits, the action and grounding accuracy is sig-
nificantly higher with low-level input. As the VLN methods showed having both
high-level and low-level inputs can improve performance, adapting Seq2Act to
take both as input would be important in future work. 11.4 Generalization of Natural Language Commands across Apps

We lastly evaluate generalization of our task automation methods to natural
language tasks. ",cs.CL,B,-0.14039272,-0.092819706,0.16192947
http://arxiv.org/pdf/2202.02398v1,Pirá: A Bilingual Portuguese-English Dataset for Question-Answering about the Ocean,"The
Pirá dataset, as well as to a discussion of some of its limitations. We  ENEM-Challenge is based on ENEM, the entrance examination
then conclude the paper in Section 7 with plans for future work. valid for almost all universities in Brazil. ",cs.CL,C,0.1878689,-0.021153051,-0.03801074
http://arxiv.org/pdf/2202.02522v1,LEAPMood: Light and Efficient Architecture to Predict Mood with Genetic Algorithm driven Hyperparameter Tuning,"Graph showing Different Hyper parameter values vs Fitness Function
MODEL PERFORMANCE ON MOOD PREDICTION                             during the process of tuning using GA

Mood Category      Precision  Recall  F1-Score                   recent emotions compared to those felt long before which
                                                                 is not captured by our current model. Good Mood           75.60   73.80     74.68
   Bad Mood           68.57   70.58     69.56                       We aim to address these as future work of this proposal. Further, we test our model on our custom data explained       B. Ablation Study
in Section III-A to evaluate its performance on real-world
chat scenarios and observe that our model performs well on          We perform an ablation study to analyze the effect of
the basis that these contain acronyms, spelling mistakes, etc. ",cs.CL,A,0.20848319,0.1559206,-0.11311047
http://arxiv.org/pdf/2202.02522v2,LEAPMood: Light and Efficient Architecture to Predict Mood with Genetic Algorithm driven Hyperparameter Tuning,"Graph showing Different Hyper parameter values vs Fitness Function
MODEL PERFORMANCE ON MOOD PREDICTION                             during the process of tuning using GA

Mood Category      Precision  Recall  F1-Score                   recent emotions compared to those felt long before which
                                                                 is not captured by our current model. Good Mood           75.60   73.80     74.68
   Bad Mood           68.57   70.58     69.56                       We aim to address these as future work of this proposal. Further, we test our model on our custom data explained       B. Ablation Study
in Section III-A to evaluate its performance on real-world
chat scenarios and observe that our model performs well on          We perform an ablation study to analyze the effect of
the basis that these contain acronyms, spelling mistakes, etc. ",cs.CL,A,0.20848319,0.1559206,-0.11311047
http://arxiv.org/pdf/2202.02617v1,Adaptive Fine-Tuning of Transformer-Based Language Models for Named Entity Recognition,"Note that adaptive ﬁne-tuning can be employed for other downstream tasks as well. We leave it for future work to
follow up on this. Acknowledgements

The author would like to thank Magnus Sahlgren for his helpful comments on an early draft of this paper. ",cs.CL,C,0.22178571,-0.32707208,-0.027202323
http://arxiv.org/pdf/2202.02635v1,Multilingual Hate Speech and Offensive Content Detection using Modified Cross-entropy Loss,"We used large language models which are pre-trained on large corpora for hate
speech detection tasks and to evaluate predictions by different models a validation dataset was
created. In future work, we hope to try out more different fine tuned models. Acknowledgments

The authors would like to thank the organizers of Hate Speech and Offensive Content Iden-
tification in Indo-Aryan Languages 2021 [5] for conducting this data challenge. ",cs.CL,B,-0.24998192,0.17759287,-0.116437495
http://arxiv.org/pdf/2202.02639v1,Classification on Sentence Embeddings for Legal Assistance,"CEUR         http://ceur-ws.org  CEUR Workshop Proceedings (CEUR-WS.org)
                                       Workshop     ISSN 1613-0073
                                       Proceedings
the results that have been obtained; 6 discusses the results and provides insights on the different
models that have been used. 7 talks of the future work that would be done and 8 concludes the
paper. 2. ",cs.CL,C,0.2971177,-0.23940971,-0.0041539697
http://arxiv.org/pdf/2202.02976v1,Measuring and Reducing Model Update Regression in Structured Prediction for NLP,"We can see that in all kinds of model updates,       the golden dependency tree (i.e., the dependency distance
                          Measuring and Reducing Model Update Regression in Structured Prediction for NLP
is 1.). We further study the relation of NFIR and depen-
dency relation for two directly connected words. The results
of stackptr⇒stackptr are shown in Table 9. ",cs.CL,B,-0.10894568,-0.05580433,-0.050267488
http://arxiv.org/pdf/2202.02976v2,Measuring and Reducing Model Update Regression in Structured Prediction for NLP,"However, the increase in
memory footprint does entail an increase in the inference hosting cost. One remedy could be to use
knowledge distillation to distill the old model(s) into a smaller one, which we leave for future work. 6 Experiments

6.1 Setup
For the ensembles of biafﬁne parsers, we ﬁrst average the edge scores from each model, then run
MST on the average scores. ",cs.CL,C,0.033437997,-0.23489846,0.053341646
http://arxiv.org/pdf/2202.02990v1,Comparison and Combination of Sentence Embeddings Derived from Different Supervision Signals,"Su-     of SentEval. pervised SimCSE ﬁne-tunes BERT by contrastive
learning using entailment pairs in NLI datasets as       In future work, we will investigate and analyze
positive examples and contradiction pairs as hard     how combination methods affect the properties of
                                                      sentence embeddings. References                                                Alexis Conneau and Douwe Kiela. ",cs.CL,B,-0.29513532,0.075324476,0.08651771
http://arxiv.org/pdf/2202.02990v2,Comparison and Combination of Sentence Embeddings Derived from Different Supervision Signals,"Supervised methods use labeled text to encode
higher-level semantic information. Supervised            For future work, we will expand the scope of
methods generally produce more sophisticated sen-     our analysis to other pre-trained language mod-
tence embeddings than unsupervised methods. In        els and sentence embedding methods to obtain in-
addition to SBERT and DefSent, supervised Sim-        sights for better sentence embeddings. ",cs.CL,B,-0.34433514,0.101761445,0.07657401
http://arxiv.org/pdf/2202.03086v1,Machine Translation from Signed to Spoken Languages: State of the Art and Challenges,"In other words, SLT is a low-
resource MT task. 12 papers (37.5%) use custom datasets that are not publicly available [38, 33, 34, 37, 43, 62, 45, 56,
41, 7, 57, 44], limiting further analysis of their results as they cannot be compared directly to other
papers. 5.6 Evaluation

The majority of evaluation of the quality of SLT models is based on quantitative metrics. ",cs.CL,C,0.28709123,0.057708573,-0.13416827
http://arxiv.org/pdf/2202.03086v2,Machine Translation from Signed to Spoken Languages: State of the Art and Challenges,"In other words, SLT is a low-
resource MT task. 12 papers (37.5%) use custom datasets that are not publicly available [38, 33, 34, 37, 43, 62, 45, 56,
41, 7, 57, 44], limiting further analysis of their results as they cannot be compared directly to other
papers. 5.6 Evaluation

The majority of evaluation of the quality of SLT models is based on quantitative metrics. ",cs.CL,C,0.28709123,0.057708573,-0.13416827
http://arxiv.org/pdf/2202.03086v3,Machine Translation from Signed to Spoken Languages: State of the Art and Challenges,"In other words, SLT is a low-
resource MT task. 12 papers (37.5%) use custom datasets that are not publicly available [32, 33, 37, 39, 41, 49–52, 54,
56,59], limiting further analysis of their results as they cannot be compared directly to other papers. 5.6 Evaluation

The majority of evaluation of the quality of SLT models is based on quantitative metrics. ",cs.CL,C,0.29254752,0.061191455,-0.13448276
http://arxiv.org/pdf/2202.03119v1,Moving Other Way: Exploring Word Mover Distance Extensions,"dimensional spaces better than other embeddings’
types. We can also notice that on TWITTER the classi-
    It seems that taking into account the frequency of               in neural information processing systems, 30:6338–
words and improving the mechanism of optimal trans-                  6347.
port in application to semantics could be promising
directions for further research. However, additional           Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. ",cs.CL,B,-0.16401261,-0.07449773,-0.040876783
http://arxiv.org/pdf/2202.03119v2,Moving Other Way: Exploring Word Mover Distance Extensions,"one could suggest that they capture semantics in low-
dimensional spaces better than other embeddings’            It seems that taking into account the frequency of
types. words and improving the mechanism of optimal trans-
                                                        port in application to semantics could be promising
    We can also notice that on TWITTER the classi-      directions for further research. However, additional
ﬁcation error is almost the same, whereas on IMDB       work on this problem is necessary. ",cs.CL,B,-0.16271892,0.037454028,-0.03928799
http://arxiv.org/pdf/2202.03164v1,Conversational Agents: Theory and Applications,"also attempted to ﬁnd a correlation between human judg-
ment and automatic metrics, but only small correlations were found. The recent survey by Chaves and Gerrosa154 reports on further research
endeavors to elucidate the aspects that account for the perception of suc-
cessful and engaging interactions with CAs. These aspects include, for ex-
ample, conscientiousness: how aware of and attentive to the context the
CA appears to be; communicability: how transparent the CA’s interaction
capabilities are, e.g., for the user to know how to best query the agent;155
damage control : how capable the CA is to recover from failure, handle un-
known concepts; etc.156 Some apparent qualities can actually be perceived
to be negative, when excessive. ",cs.CL,A,0.38438565,0.2782251,0.0073853247
http://arxiv.org/pdf/2202.03291v1,Mental Disorders on Online Social Media Through the Lens of Language and Behaviour: Analysis and Visualisation,"Similar scenarios could be described considering the rest of the dimensions
studied in this work. We plan to investigate various directions as future work. Firstly, we aim to ex-
pand our study to multiple datasets collected from diﬀerent social media platforms
to provide more insights into the behaviour of users with various mental issues
on diﬀerent social media platforms. ",cs.CL,A,0.23232372,0.409805,-0.06524483
http://arxiv.org/pdf/2202.03354v1,Robust Dialogue State Tracking with Weak Supervision and Sparse Data,"On 2.2, the method of Tian et al. (2021)
performs best with a two-pass generative approach         For future work we plan a closer inspection of
that utilises a mechanism to recover from previ-       the ability to learn from non-dialogue data, poten-
ously made errors. Cho et al. ",cs.CL,C,-0.070226386,-0.167447,0.16799363
http://arxiv.org/pdf/2202.03354v2,Robust Dialogue State Tracking with Weak Supervision and Sparse Data,"We achieve either competitive or state-of-
presented in Sections 4.3 and 4.4 and continue        the-art performance on all tested benchmarks. For
ﬁne-tuning with limited—and potentially unstruc-      future work we continue to investigate learning
                                                      from non-dialogue data, potentially in a continu-
                                                      ous fashion over the lifetime of a dialogue system. Acknowledgements                                     Guan-Lin Chao and Ian Lane. ",cs.CL,B,-0.09183483,-0.05627221,0.09085298
http://arxiv.org/pdf/2202.03611v1,Do Language Models Learn Position-Role Mappings?,"We have demonstrated here that the third option is a feasi-
ble explanation: three language models that contain no explicit linguistic biases
regarding possible position-role mappings nevertheless successfully demonstrate
knowledge of position-role mappings that largely generalizes across verbs and
syntactic structures. The limitations we ﬁnd do not invalidate this larger conclusion,
though they do suggest the importance of further research in this area. We have shown that pretrained language models (BERT, RoBERTa, and Dis-
tilBERT) recognize distributional differences between THEME- and RECIPIENT-
expecting positions. ",cs.CL,B,-0.1775492,0.10520156,0.17559543
http://arxiv.org/pdf/2202.03629v1,Survey of Hallucination in Natural Language Generation,"11.4 Future Direction in Translation

The future work in hallucinations in NMT will be to deﬁne hallucinations in the way that is clear
and quantiﬁable (i.e., deﬁning the metric that would specify how much the hallucinations have
to be disconnected from the source to be considered hallucination and not only traditional error). Another direction for future work on hallucinations is improving existing methods of searching for
hallucinatory content, such as algorithms proposed by Feng et al. [42], Lee et al. ",cs.CL,A,0.14380667,0.0117284255,-0.13124397
http://arxiv.org/pdf/2202.03629v2,Survey of Hallucination in Natural Language Generation,"[32, 70, 140, 161, 166,
197, 198] use constrained decoding to incorporate speciﬁc terminology to machine translation, but
the above methods can be repurposed to mitigate hallucinations. Another direction for future work on hallucinations is improving existing methods of searching
for hallucinatory content, such as algorithms proposed by Feng et al. [48], Lee et al. ",cs.CL,A,-0.071338445,-0.014179802,-0.17870958
http://arxiv.org/pdf/2202.03629v3,Survey of Hallucination in Natural Language Generation,"34                                                                            Ziwei Ji, et al. Another direction for future work on hallucinations is improving existing methods of search-
ing for hallucinatory content, such as the algorithms proposed by Feng et al. [47], Lee et al. ",cs.CL,A,0.1915265,0.042047374,-0.12493004
http://arxiv.org/pdf/2202.03629v4,Survey of Hallucination in Natural Language Generation,"[31, 69, 140, 161, 167, 202]
and [201] use constrained decoding to incorporate speciﬁc terminology into MT, but the above
methods can be repurposed to mitigate hallucinations. Another direction for future work on hallucinations is improving existing methods of search-
ing for hallucinatory content, such as the algorithms proposed by Feng et al. [47], Lee et al. ",cs.CL,A,0.06026648,-0.025738463,-0.19211353
http://arxiv.org/pdf/2202.03629v5,Survey of Hallucination in Natural Language Generation,"[33, 70, 146, 171, 177, 214]
and [213] use constrained decoding to incorporate specific terminology into MT, but the above
methods can be repurposed to mitigate hallucinations. Another direction for future work on hallucinations is improving existing methods of searching
for hallucinatory content, such as the algorithms proposed by Feng et al. [49], Lee et al. ",cs.CL,A,0.017702542,0.0048693316,-0.21989161
http://arxiv.org/pdf/2202.03792v1,Counterfactual Multi-Token Fairness in Text Classification,"With the multi-token setup proposed by us in place, a new dimension of future work opens up. The proposition of methods for generating factually correct counterfactuals will be taken in future work. We have
showcased Fairness-Utility trade-oﬀ as well. ",cs.CL,A,0.0702075,0.2503356,0.08815121
http://arxiv.org/pdf/2202.03792v2,Counterfactual Multi-Token Fairness in Text Classification,"With the multi-token setup proposed by us in place, a new dimension of future work opens up. The proposition of methods for generating factually correct counterfactuals will be taken in future work. We have
showcased Fairness-Utility trade-oﬀ as well. ",cs.CL,A,0.0702075,0.2503356,0.08815121
http://arxiv.org/pdf/2202.03829v1,TimeLMs: Diachronic Language Models from Twitter,"Association for Computational Linguistics. As future work, it would be interesting to inte-
grate the time span variable more prominently in         Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
the language models, i.e., introducing string pre-          Kristina Toutanova. 2018. ",cs.CL,B,-0.12951648,0.041694514,-0.009575294
http://arxiv.org/pdf/2202.03829v2,TimeLMs: Diachronic Language Models from Twitter,""", it              mance when models are used for future data, which
is only in 2021-Q4 that predictions change sub-            is one of the most common settings in practice. stantially, when the model has been exposed to
                                                              As future work, we are planning to explicitly in-
tegrate the time span variable in the language mod-      Marco Del Tredici, Raquel Fernández, and Gemma
els, i.e., introducing string preﬁxes, along the lines      Boleda. 2019. ",cs.CL,C,0.111949265,-0.04677573,0.043927714
http://arxiv.org/pdf/2202.04003v1,Differentiable N-gram Objective on Abstractive Summarization,"Available:
bag-of-ngram objective. Meanwhile, applicating our pro-                   http://arxiv.org/abs/1910.10683
posed algorithm on other seq2seq tasks is simple and worth
examining in future work. [9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
                                                                          hamed, O. ",cs.CL,C,0.10495904,-0.2897216,-0.11267887
http://arxiv.org/pdf/2202.04003v2,Differentiable N-gram Objective on Abstractive Summarization,"Meanwhile, applicating our pro-              [9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
posed algorithm on other seq2seq tasks is simple and worth                hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART:
examining in future work. Denoising sequence-to-sequence pre-training for natural
                                                                          language generation, translation, and comprehension.” in
References                                                                Proceedings of the 58th Annual Meeting of the Association
                                                                          for Computational Linguistics. ",cs.CL,B,-0.34502512,-0.21992621,0.023752399
http://arxiv.org/pdf/2202.04003v3,Differentiable N-gram Objective on Abstractive Summarization,"Meanwhile, applicating our pro-              [9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
posed algorithm on other seq2seq tasks is simple and worth                hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART:
examining in future work. Denoising sequence-to-sequence pre-training for natural
                                                                          language generation, translation, and comprehension.” in
References                                                                Proceedings of the 58th Annual Meeting of the Association
                                                                          for Computational Linguistics. ",cs.CL,B,-0.34502512,-0.21992621,0.023752399
http://arxiv.org/pdf/2202.04003v4,Differentiable N-gram Objective on Abstractive Summarization,"Meanwhile, applicating our pro-              [9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
posed algorithm on other seq2seq tasks is simple and worth                hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART:
examining in future work. Denoising sequence-to-sequence pre-training for natural
                                                                          language generation, translation, and comprehension.” in
References                                                                Proceedings of the 58th Annual Meeting of the Association
                                                                          for Computational Linguistics. ",cs.CL,B,-0.34502512,-0.21992621,0.023752399
http://arxiv.org/pdf/2202.04003v5,Differentiable N-gram Objective on Abstractive Summarization,"Meanwhile, applicating our pro-                  hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART:
posed algorithm on other seq2seq tasks is simple and worth               Denoising sequence-to-sequence pre-training for natural
examining in future work. language generation, translation, and comprehension.” in
                                                                         Proceedings of the 58th Annual Meeting of the Association
References                                                               for Computational Linguistics. ",cs.CL,B,-0.3680668,-0.10209717,0.03723692
http://arxiv.org/pdf/2202.04003v6,Differentiable N-gram Objective on Abstractive Summarization,"Additionally, BART was pre-
trained on Wikipedia and books corpus, and we only investi-
gate our method on news summarization datasets which have
moderate text length. Applying the objectives on different pre-
trained language models and conducting test on different cat-
egories of summarization datasets need further study. In this paper, we propose two differentiable N -gram ob-
jectives based on probabilistic sub-sequence matching that no
longer ceil the matched number of sub-sequences and can
value the matched probabilistic sub-sequences equally and
produce fairly signiﬁcant improvement on abstractive summa-
rization. ",cs.CL,B,-0.32932943,0.036347803,-0.06585127
http://arxiv.org/pdf/2202.04161v1,Logical Reasoning for Task Oriented Dialogue Systems,"We showed that these
                                                       models can learn to do logical reasoning to 1) an-
                                                       swer questions form the dialogue context when all
                                                       the information is available, 2) extract constraints
                                                       when partial information is available, and 3) dele-
                                                       gate to the dialogue policy when no reasoning is
                                                       required. For future work, we plan to investigate
                                                       the application of our method to other reasoning
                                                       tasks (e.g., temporal and spatial reasoning). Fur-
thermore, we plan to explore how logical reasoning       Xiaoxiao Guo, Tim Klinger, Clemens Rosenbaum,
can be used to disambiguate with the user when              Joseph P Bigus, Murray Campbell, Ban Kawas,
multiple conclusions can be made. ",cs.CL,A,-0.051592834,0.04210595,0.4850188
http://arxiv.org/pdf/2202.04173v1,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,"Second, in this paper, we mainly focus on the intrinsic quality of LMs and analyze the trade-off between toxicity and quality. While recent work demonstrates that detoxiﬁcation methods may amplify social biases, we leave it as a future work to
analyze the bias impact after detoxiﬁcation. A.10.3. ",cs.CL,A,0.45648283,0.28586066,-0.030932683
http://arxiv.org/pdf/2202.04350v1,pNLP-Mixer: an Efficient all-MLP Architecture for Language,"This difference of over 3% highlights            quadratic. Overall, this evaluation demonstrates that the
the importance of carefully designing the projection layer         MLP-Mixer is a weight-efﬁcient architecture for processing
and justiﬁes an effort for further research on projection al-      the projection output, i.e., it reaches a higher performance
gorithms. Given these results, in the remaining experiments        than the alternatives with a smaller amount of parameters. ",cs.CL,C,0.30808598,-0.23466855,-0.11083792
http://arxiv.org/pdf/2202.04538v1,Generating Training Data with Language Models: Towards Zero-Shot Language Understanding,"seven classiﬁcation tasks of the GLUE benchmark, even
However, we indeed observed nontrivial improvements (e.g.,      yielding comparable or better results than sophisticated few-
1+ points on MNLI and SST-2; 2+ points on RTE) when             shot learning methods and offering better stability. This is
a small amount of task-speciﬁc samples (16 per class) are       large room for future work, including but not limited to:
used only for hyperparameter tuning and early stopping. We      Extension to few-shot learning settings, exploring larger
do not report these results as they do not reﬂect the true      generator models, better ﬁne-tuning techniques to leverage
zero-shot performance, but it serves as a simply way for        generated data and better strategies for selecting quality
achieving better results without any changes to SuperGen’s      training data. ",cs.CL,C,0.13165557,-0.2702878,0.06294698
http://arxiv.org/pdf/2202.04538v2,Generating Training Data with Language Models: Towards Zero-Shot Language Understanding,"SuperGen achieves strong
performance on seven classiﬁcation tasks of the GLUE benchmark, even yielding comparable or
better results than sophisticated few-shot learning methods and offering better stability. There is large
room for future work, including but not limited to: Extension to few-shot learning settings, exploring
larger generator models [25, 68], better ﬁne-tuning techniques to leverage generated data and better
strategies for selecting quality training data. Acknowledgments

Research was supported in part by US DARPA KAIROS Program No. ",cs.CL,C,0.115867645,-0.32181042,0.04671062
http://arxiv.org/pdf/2202.04742v1,FedQAS: Privacy-aware machine reading comprehension with federated learning,"The system actively supports end-users in joining training and improving the performance
through incremental learning on a various range of local clients. In future work, we aim to extend the FedQAS to cover
more question answering datasets and allow federated learning for large private documents. Availability

FedQAS is publicly available under the Apache2 license at https://github.com/aitmlouk/
FEDn-client-FedQAS-tf. ",cs.CL,C,0.018958645,-0.048918214,0.1457061
http://arxiv.org/pdf/2202.04824v1,AdaPrompt: Adaptive Model Training for Prompt-based NLP,"This also suggests
ual pretraining using prompt-aware data is highly         that more sophisticated strategy that cares of task
beneﬁcial to zero-shot prompt-based NLP. and prompt information can be useful, which we
                                                          leave for future work. 4.3 Analysis
                                                          4.3.3 Generalization Capability
4.3.1 Size of Retrieved Data
                                                          For experiments in section 4.2.1 and section 4.2.2,
As stated, Elasticsearch returns data in the order of     we use task testset as the sources to build queries
matching scores. ",cs.CL,A,0.021134216,0.04426246,0.21468036
