url,title,further research,primary category,label,x,y,z
http://arxiv.org/pdf/2201.00095v1,Computer Vision Based Parking Optimization System,"This solution fails in places with very low visibility such      [4] Janak Parmar, Pritikana Das, Sanjaykumar M. Dave, “Study on demand
as poor weather conditions snow, rain etc. The future work               and characteristics of parking systems in urban areas”, 2014.
can be used to resolve these above mentioned drawbacks. We
leave it to the future to test the application on indoor parking    [5] Abhirup Khanna, Rishi Anand, “IoT based Smart Parking System”,
lots as the problem with the indoor parking spaces is, in these          2016.
places the video is hindered by beams, pillars and low contrast
markings. ",cs.CV,B,0.20106705,-0.23479445,0.1434876
http://arxiv.org/pdf/2201.00132v1,SAFL: A Self-Attention Scene Text Recognizer with Focal Loss,"[7] [8] addressed irregular
proposed model in Section III and present the evaluation           text problems with a rectiﬁcation network based on Spatial
results in Section IV. Finally, we conclude the paper and          Transformer Network (STN), which transform input image for
discuss the future works in Section V.                             better recognition. Zhan et al. ",cs.CV,C,-0.07558787,-0.069527015,-0.0101681985
http://arxiv.org/pdf/2201.00239v1,SporeAgent: Reinforced Scene-level Plausibility for Object Pose Refinement,"Section 6 concludes the paper and             introduce terms used throughout the paper. We additionally
gives an outlook to future work. discuss how registration – and by extension refinement – is
                                                                     approached as a reinforcement learning task. ",cs.CV,A,0.017015994,0.15657501,0.24290004
http://arxiv.org/pdf/2201.00267v1,On the Cross-dataset Generalization for License Plate Recognition,"is precisely what caused a leave-one-dataset-out eval-
uation to have not yet been performed in the literature. As future work, we plan to gather images from the
                                                               internet to build a novel dataset for end-to-end ALPR
6 CONCLUSIONS                                                  with images acquired in various countries/regions, by
                                                               many different cameras, both static or mobile, with a
As the performance of traditional-split LP recognition         well-deﬁned evaluation protocol for both within- and
is rapidly improving, researchers should pay more at-          cross-dataset LP detection and LP recognition. In ad-
tention to cross-dataset LP recognition since it better        dition, we intend to leverage the potential of Gener-
simulates real-world ALPR applications, where new              ative Adversarial Networks (GANs) to generate hun-
cameras are regularly being installed in new locations         dreds of thousands of synthetic LP images with dif-
without existing systems being retrained every time. ",cs.CV,C,-0.25323868,-0.055732958,-0.16107066
http://arxiv.org/pdf/2201.00267v2,On the Cross-dataset Generalization in License Plate Recognition,"is precisely what caused a leave-one-dataset-out eval-
uation to have not yet been performed in the literature. As future work, we plan to gather images from the
                                                               internet to build a novel dataset for end-to-end ALPR
6 CONCLUSIONS                                                  with images acquired in various countries/regions, by
                                                               many different cameras, both static or mobile, with a
As the performance of traditional-split LP recognition         well-deﬁned evaluation protocol for both within- and
is rapidly improving, researchers should pay more at-          cross-dataset LP detection and LP recognition. In ad-
tention to cross-dataset LP recognition since it better        dition, we intend to leverage the potential of Gener-
simulates real-world ALPR applications, where new              ative Adversarial Networks (GANs) to generate hun-
cameras are regularly being installed in new locations         dreds of thousands of synthetic LP images with dif-
without existing systems being retrained every time. ",cs.CV,C,-0.25323868,-0.055732958,-0.16107066
http://arxiv.org/pdf/2201.00267v3,On the Cross-dataset Generalization in License Plate Recognition,"uation to have not yet been performed in the literature. As future work, we plan to gather images from the
6 CONCLUSIONS                                                  internet to build a novel dataset for end-to-end ALPR
                                                               with images acquired in various countries/regions, by
As the performance of traditional-split LP recognition         many different cameras, both static or mobile, with a
is rapidly improving, researchers should pay more at-          well-deﬁned evaluation protocol for both within- and
tention to cross-dataset LP recognition since it better        cross-dataset LP detection and LP recognition. In ad-
simulates real-world ALPR applications, where new              dition, we intend to leverage the potential of Gener-
cameras are regularly being installed in new locations         ative Adversarial Networks (GANs) to generate hun-
without existing systems being retrained every time. ",cs.CV,C,-0.28036207,-0.100605406,-0.14338571
http://arxiv.org/pdf/2201.00267v4,On the Cross-dataset Generalization in License Plate Recognition,"uation to have not yet been performed in the literature. As future work, we plan to gather images from the
6 CONCLUSIONS                                                  internet to build a novel dataset for end-to-end ALPR
                                                               with images acquired in various countries/regions, by
As the performance of traditional-split LP recognition         many different cameras, both static or mobile, with a
is rapidly improving, researchers should pay more at-          well-deﬁned evaluation protocol for both within- and
tention to cross-dataset LP recognition since it better        cross-dataset LP detection and LP recognition. In ad-
simulates real-world ALPR applications, where new              dition, we intend to leverage the potential of Gener-
cameras are regularly being installed in new locations         ative Adversarial Networks (GANs) to generate hun-
without existing systems being retrained every time. ",cs.CV,C,-0.28036207,-0.100605406,-0.14338571
http://arxiv.org/pdf/2201.00377v1,Parkour Spot ID: Feature Matching in Satellite and Street view images using Deep Learning,"We presented a scalable framework because its performance
improves by enhancing its parts. One future work direction can            [4] ——, “Google Maps Street View Static API,” https://developers.google. be to evolve the proposed system by enhancing the satellite                    com/maps/documentation/streetview/overview, [Online; Dec 2021]. ",cs.CV,B,0.12322505,-0.25508672,0.08245359
http://arxiv.org/pdf/2201.00392v1,Fast and High-Quality Image Denoising via Malleable Convolutions,"We evaluate the runtime         may also capture heterogenous image statistics in a less ef-
speed, FLOPs cost, and PSNR value of four variants with              ﬁcient way. A future work is to augment MalleConv by
the size of the AvgPooling layer equal to {0, 2, 4, 8}. As           other architectures, e.g., attention mechanism [38] or de-
shown in Table 2, by processing a 4× downsampled fea-                formable shape [14], to further improve its quality in appli-
ture map, our proposed efﬁcient predictor network achieves           cations with less computational constraints. ",cs.CV,B,-0.017582815,-0.043050498,-0.27710524
http://arxiv.org/pdf/2201.00392v2,Fast and High-Quality Image Denoising via Malleable Convolutions,"We evaluate the runtime         may also capture heterogenous image statistics in a less ef-
speed, FLOPs cost, and PSNR value of four variants with              ﬁcient way. A future work is to augment MalleConv by
the size of the AvgPooling layer equal to {0, 2, 4, 8}. As           other architectures, e.g., attention mechanism [38] or de-
shown in Table 2, by processing a 4× downsampled fea-                formable shape [14], to further improve its quality in appli-
ture map, our proposed efﬁcient predictor network achieves           cations with less computational constraints. ",cs.CV,B,-0.017582815,-0.043050498,-0.27710524
http://arxiv.org/pdf/2201.00443v1,Scene Graph Generation: A Comprehensive Survey,"6.2 Opportunities                                                REFERENCES

The community has published hundreds of scene graph              [1] J. Johnson, R. Krishna, M. Stark, L. J. Li, D. A. Shamma, M. S.
models and has obtained a wealth of research result. We                  Bernstein, and F. F. Li, “Image retrieval using scene graphs,” in
think there are several avenues for future work. On the                  2015 IEEE Conference on Computer Vision and Pattern Recognition
                                                                         (CVPR), 2015, pp. ",cs.CV,C,-0.105194256,-0.22663751,0.047921598
http://arxiv.org/pdf/2201.00443v2,Scene Graph Generation: A Comprehensive Survey,"We         can promote more in-depth ideas used on SGG. think there are several avenues for future work. Researchers
will be motivated to explore more models as a result of         REFERENCES
the above challenges. ",cs.CV,A,0.091382004,0.29582846,0.21455964
http://arxiv.org/pdf/2201.00467v1,maskGRU: Tracking Small Objects in the Presence of Large Background Motions,"We speculate that incorporating a mask into the hidden
state helps mitigate the eﬀects of vanishing gradients and acts as an attention-
like mechanism by indicating the region of interest in the image. In future work
we plan to further explore the reasons behind maskGRU’s eﬀectiveness through
ablation experiments. We also plan to compare it with other models beyond
convGRU such as COMET and TrackNetv2. ",cs.CV,B,0.10494501,-0.13627721,0.032663718
http://arxiv.org/pdf/2201.00520v2,Vision Transformer with Deformable Attention,"How-
ever, replacing with more deformable attention at the early            ,PDJH1HW.$FF#         $EODWLRQVWXG\RQGLIIHUHQWRIIVHWUDQJHIDFWRUV
stages slightly decreases the accuracy.           2IIVHWUDQJHIDFWRUs   
Ablation on different s. We go on the further study of the                                
impact of different maximum offsets, i.e., the offset range                               
scale factor s in the paper. We conduct an ablation exper-                                
iment of s ranging from 0 to 16 where 14 corresponds to                                   
the largest reasonable offset given the size of the feature                               
map (14 × 14 at stage 3). ",cs.CV,A,0.32705945,0.16168374,0.09582813
http://arxiv.org/pdf/2201.00520v3,Vision Transformer with Deformable Attention,"How-
ever, replacing with more deformable attention at the early            ,PDJH1HW.$FF#         $EODWLRQVWXG\RQGLIIHUHQWRIIVHWUDQJHIDFWRUV
stages slightly decreases the accuracy.           2IIVHWUDQJHIDFWRUs   
Ablation on different s. We go on the further study of the                                
impact of different maximum offsets, i.e., the offset range                               
scale factor s in the paper. We conduct an ablation exper-                                
iment of s ranging from 0 to 16 where 14 corresponds to                                   
the largest reasonable offset given the size of the feature                               
map (14 × 14 at stage 3). ",cs.CV,A,0.32705945,0.16168374,0.09582813
http://arxiv.org/pdf/2201.00531v1,Novelty-based Generalization Evaluation for Traffic Light Detection,"1, 2, 5
the framework on non-standard shapes like pedestrian or
vehicles is still an open challenge that we intend to pursue                    [16] M. Johanson, S. Belenki, J. Jalminger, M. Fant, and M. Gjertz. Big
as future work. automotive data: Leveraging large volumes of data for knowledge-driven
                                                                                      product development. ",cs.CV,C,0.057388417,-0.047668047,0.14558357
http://arxiv.org/pdf/2201.00625v1,GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings,"The CEE module. Our CEE module is the orange branch                                                                                   Limitation and future work. It is undeniable that our
in Fig. ",cs.CV,A,0.26804155,0.1950644,0.14403233
http://arxiv.org/pdf/2201.00625v2,GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings,"International Journal on Document Analysis
                                                                         and Recognition (IJDAR), 13(3):187–207, 2010. 2, 5
Limitation and future work. It is undeniable that our
method is still far from perfection, and the panoptic sym-         [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
bol spotting remains an open problem. ",cs.CV,C,0.015353565,0.06739851,0.1967122
http://arxiv.org/pdf/2201.00770v1,FaceQgen: Semi-Supervised Deep Learning for Face Image Quality Assessment,"[16] A. Khodabakhsh, M. Pedersen, and C. Busch. Subjective versus
   Finally, for future work we also suggest to investigate: how                     objective face image quality evaluation for face recognition. ICBEA,
the proposed GAN-based methods can contribute to disentan-                          pages 36–42, 2019.
gle individual quality factors [10] (as required in related work
around biometric quality, e.g., ISO/IEC WD 29794-5); appli-                   [17] C. Lennan, H. Nguyen, and D. Tran. ",cs.CV,C,0.043021895,-0.003924018,-0.032294825
http://arxiv.org/pdf/2201.00785v1,Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,"Intuitively,
such sampling variations drop when increasing the sampling resolution. To further study this problem, we conduct the following experiment. According
to the deﬁnition in Section 4.1, ﬁrst, on ShapeNet, we generate four datasets by
sampling diﬀerent number points of the input point cloud, ranging from n0 = 256
to 2048 points. ",cs.CV,B,0.14067258,0.05707116,-0.29440826
http://arxiv.org/pdf/2201.00785v2,Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,"Intuitively,
such sampling variations drop when increasing the sampling resolution. To further study this problem, we conduct the following experiment. According
to the deﬁnition in Section 4.1, ﬁrst, on ShapeNet, we generate four datasets
by sampling diﬀerent number points of the input point cloud, ranging from
n0 = 256 to 2048 points. ",cs.CV,B,0.14067258,0.05707116,-0.29440826
http://arxiv.org/pdf/2201.00785v3,IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,"The model with a              sampling variations drop when increasing the sampling
maximum cropping size of 50% achieves the best transfer                resolution. To further study this problem, we conduct
learning performance on the SUN RGB-D dataset. Note that               the following experiment. ",cs.CV,B,-0.01493549,-0.08630811,-0.3159243
http://arxiv.org/pdf/2201.00893v1,Rice Diseases Detection and Classification Using Attention Based Neural Network and Bayesian Optimization,Approaches used to extract samples and other image processing steps are also discussed in Section 4. The research ﬁndings and future work are concluded in Section 5. 2. ,cs.CV,B,0.13857122,-0.14534575,-0.044933643
http://arxiv.org/pdf/2201.00969v1,Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety,"Thus an Interactive Image Captioning AI for Explainable night scene
understanding is demonstrated experimentally. To encourage further research by inter-
ested researchers in Vision-Language modelling for promoting safety of women, the
source code is made available in open-source. 15

References

1. ",cs.CV,C,-0.13737085,-0.19015445,0.38690174
http://arxiv.org/pdf/2201.00978v1,PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture,"models. We hope this new baseline will be helpful to the
                                                                                                          further research and application of vision transformer. 1. ",cs.CV,B,0.21983215,-0.25167644,0.032816984
http://arxiv.org/pdf/2201.01046v1,Sound and Visual Representation Learning with Multiple Pretraining Tasks,"Further we notice in Table 4           5.6. Limitations and future works
that IL approach has signiﬁcant advantages over MoCov2
and other recent methods e.g. DenseCL [66] and DetCo [70]               Our paper investigates a handful of approaches of Multi-
by +2.83, +1.56 and +1.61 AP on COCO detection. ",cs.CV,C,0.03514114,-0.15459004,-0.097889856
http://arxiv.org/pdf/2201.01073v1,Towards Unsupervised Open World Semantic Segmentation,"This performance gap is also re-
                                                                         ﬂected in the model’s ability to learn the novel class, thus we
initial DNN                           66.94   00.00      63.42           conclude that our method beneﬁts signiﬁcantly from high
extended DNN (ours)                   67.05   41.85      65.72           performance networks. extended DNN (baseline)               66.74   41.40      65.41
oracle                                69.48   76.66      69.86              For future work, we plan to extend a neural network by
                                                                         multiple classes at once. On that account, suitable datasets
3. experiment (a): A2D2, guardrail    DeepLabV3+ (ﬁne-tuned)             are in demand. ",cs.CV,A,-0.14282958,0.2818508,-0.23506105
http://arxiv.org/pdf/2201.01073v2,Towards Unsupervised Open World Semantic Segmentation,"We report the IoU,                 beneﬁts signiﬁcantly from high performance networks. precision and recall values for the novel class (highlighted
with gray rows), respectively, as well as averaged over the            For future work, we plan to improve the extension of a neu-
previously-known and the extended class spaces C and C+. ral network by multiple classes at once. ",cs.CV,A,-0.082642436,0.27734327,-0.2334713
http://arxiv.org/pdf/2201.01115v1,Data Augmentation for Depression Detection Using Skeleton-Based Gait Information,"At the same                 augmentation for deep learning. Journal of Big Data, 6(1):1–48, 2019.
time, we will further study the relationship between gait
                                                                       [11] Zeshan Hussain, Francisco Gimenez, Darvin Yi, and Daniel Rubin. Dif-
                                                                             ferential data augmentation techniques for medical imaging classiﬁcation
                                                                             tasks. ",cs.CV,C,-0.16669023,0.0059953397,-0.118450835
http://arxiv.org/pdf/2201.01283v1,Self-supervised Learning from 100 Million Medical Images,"The paper is organized as follows: Section II provides an overview of related work, with the last subsection focusing on
recent developments for self-supervised learning in the medical imaging domain; Section III describes the proposed method
followed by Section IV in which we present the experiments on various abnormality detection problems based on different
2D/3D image modalities. Finally, Section V concludes the paper with a summary and outlook on future work. II. ",cs.CV,C,-0.10876592,-0.16127029,-0.037636574
http://arxiv.org/pdf/2201.01408v1,Fusing Convolutional Neural Network and Geometric Constraint for Image-based Indoor Localization,"[9] address the geometric constraint,
                   1.00 m,         0.62 m,         1.35 m,      1.14 m,      experiments show that our method is more efﬁcient given the
                   8.53◦           7.69◦           8.48◦        8.41◦        correct camera parameter. It remains unclear which method is
                                                                             more suitable when the camera intrinsic parameter is unknown,
                   0.58 m,         0.41 m,         0.38 m,      0.27 m,      and this problem is an interesting future work direction. 9.11◦           5.07◦           3.55◦        3.62◦
                                                                                                      VII. ",cs.CV,B,0.42555684,-0.24856442,-0.0633711
http://arxiv.org/pdf/2201.01415v1,Problem-dependent attention and effort in neural networks with an application to image resolution,"In a digit classiﬁcation application, for example, if
the model assigns equal likelihood to a test image being a one or a seven, then
the propensity score is relatively low (50% or less); thus, a closer examination
with a more complex model may be warranted. But if the model projects a
classiﬁcation of three with 95% likelihood, then it is likely that further analysis
would not be worth the cost. This multi-stage strategy of problem-dependent eﬀort is applied to a digit
classiﬁcation exercise using Google’s well-known Street View House Numbers
(SVHN) dataset [4]. ",cs.CV,A,0.017949823,0.17398554,0.095211394
http://arxiv.org/pdf/2201.01415v2,Problem-dependent attention and effort in neural networks with an application to image resolution,"(2019). By contrast, many   that further analysis would not be worth the cost. artiﬁcial neural networks—and machine learning systems
                                        more generally—apply a standardized and uniform process            In the current study, ﬁve image recognition datasets
                                        to all observations without regard to the information value    are considered that are standard in the Computer Vision
                                        of diﬀerent training cases or the diﬃculty or conﬁdence of     literature and involve identifying digits from handwriting or
                                        classifying diﬀerent test cases. ",cs.CV,C,-0.23437819,0.016168598,-0.110783465
http://arxiv.org/pdf/2201.01486v1,Sign Language Recognition System using TensorFlow Object Detection API,"In continuous SLR, the system is able to recognize and
 translate whole sentences instead of a single gesture [33][34]. Even with all the research that has been done in SLR, many inadequacies need to be
 dealt with by further research. Some of the issues and challenges that need to be worked
 on are as follows [33][2][4][6]. ",cs.CV,A,0.06837815,0.15852478,0.4079585
http://arxiv.org/pdf/2201.01486v2,Sign Language Recognition System using TensorFlow Object Detection API,"In continuous SLR, the system is able to recognize and
 translate whole sentences instead of a single gesture [33][34]. Even with all the research that has been done in SLR, many inadequacies need to be
 dealt with by further research. Some of the issues and challenges that need to be worked
 on are as follows [33][2][4][6]. ",cs.CV,A,0.06837815,0.15852478,0.4079585
http://arxiv.org/pdf/2201.01494v1,"Improving Object Detection, Multi-object Tracking, and Re-Identification for Disaster Response Drones","Plus, further improvements in the output refinement block may lead to a
greater impact on overall performance. Also, proposing a one-stage Online Multi-Camera
Tracking model is one of our future works. It is more challenging, but makes Multi-
Camera-Multi-Object-Tracking lighter and faster in terms of efficiency. ",cs.CV,B,0.13263945,-0.2891386,-0.02970247
http://arxiv.org/pdf/2201.01565v1,Culture-to-Culture Image Translation with Generative Adversarial Networks,"However – given
   Third, both in the ﬁrst and the second section younger           current GAN technology – this is likely the best we can do:
respondents < 30 tend to prefer EU and OTHER → EU                   we plan to test more advanced techniques as future work. objects: this traditionalist attitude looks coherent with the
lesser experience with other cultures and the limited interest         Finally, future work will include human-robot interaction
younger people may have in items of furniture. Many of them         with culturized images, not yet systematically tested with
have never faced the problem of furnishing a house, possibly        recruited participants due to the Covid-19 pandemic. ",cs.CV,C,0.025903119,-0.060220152,0.29370564
http://arxiv.org/pdf/2201.01565v2,Culture-to-Culture Image Translation with Generative Adversarial Networks,"However – given
                                                                   current GAN technology – this is likely the best we can do:
   Third, both in the ﬁrst and the second section younger          we plan to test more advanced techniques as future work. respondents < 30 tend to prefer E and O → E objects: this
traditionalist attitude looks coherent with the lesser experience     Finally, future work will include human-robot interaction
with other cultures and the limited interest younger people may    with culturized images, not yet systematically tested with
have in items of furniture. Many of them have never faced the      recruited participants due to the Covid-19 pandemic. ",cs.CV,C,-0.015234238,-0.06333563,0.27776557
http://arxiv.org/pdf/2201.01565v3,User Evaluation of Culture-to-Culture Image Translation with Generative Adversarial Nets,"We        I, showing pictures and asking related questions4. will consider other nationalities in future works. The general
idea was to show participants images of different objects            1) Personal questions: the respondents had to declare their
and environments belonging to European and non-European                  age (less than 20, 20–29, 30–39, 40–49, 50–59, 60–69,
cultural domains and ask them some questions. ",cs.CV,C,-0.027872834,-0.07705882,0.4266948
http://arxiv.org/pdf/2201.01636v1,Tackling the Class Imbalance Problem of Deep Learning Based Head and Neck Organ Segmentation,"The patch-size optimization and the class adaptive Dice loss can both easily be integrated into current DL based
segmentation approaches. In future work we want to improve the state-of-the-art performance of the recently presented
hybrid 2D-3D, single network approach of Chen et al. [11] by integrating our adaptations. ",cs.CV,B,0.04427762,-0.08250728,-0.33096907
http://arxiv.org/pdf/2201.01636v2,Tackling the Class Imbalance Problem of Deep Learning Based Head and Neck Organ Segmentation,"The patch-size optimization and the class adaptive Dice loss can both easily be integrated into current DL based
segmentation approaches. In future work we want to improve the state-of-the-art performance of the recently presented
hybrid 2D-3D, single network approach of Chen et al. [13] by integrating our adaptations. ",cs.CV,B,0.04492598,-0.08157386,-0.33150524
http://arxiv.org/pdf/2201.01654v1,TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets,"With spreadsheets as weak supervision, this pa-
                                             parsing. We share these resources with the research com-     per proposes a pipeline to provide an automated process
                                             munity to facilitate further research in this interesting    of reading tables from PDFs and utilize them as a weak
                                             direction. supervision source for DL systems. ",cs.CV,A,0.10262744,0.1743301,0.22137183
http://arxiv.org/pdf/2201.01699v1,An Investigation Of Ben-ford's Law Divergence And Machine Learning Techniques For Separability Of Fingerprint Images,"This shows that our proposed method can effectively reduce features and achieve high intra-class
separability results especially using Decision Tree and CNN algorithms. For future work, we plan
to investigate other classification techniques such as Long Short-Term Memory (LSTM) for the
intra-class classification and source identification of fingerprint images. References

[1] Hildebrandt, M., Sturm, J., Dittmann, J., and Vielhauer, C. (2013, September). ",cs.CV,C,-0.24436036,0.09024547,-0.12531221
http://arxiv.org/pdf/2201.01699v2,"An Investigation of ""Benford's"" Law Divergence and Machine Learning Techniques for ""Intra-Class"" Separability of Fingerprint Images","This shows that our proposed method can effectively reduce features and achieve high intra-class
separability results especially using Decision Tree and CNN algorithms. For future work, we plan
to investigate other classification techniques such as Long Short-Term Memory (LSTM) for the
intra-class classification and source identification of fingerprint images. References

[1] Hildebrandt, M., Sturm, J., Dittmann, J., and Vielhauer, C. (2013, September). ",cs.CV,C,-0.24436036,0.09024547,-0.12531221
http://arxiv.org/pdf/2201.01850v1,On the Real-World Adversarial Robustness of Real-Time Semantic Segmentation Models for Autonomous Driving,"Since many variables of a realistic driving                       [12] Y. Hong, H. Pan, W. Sun, S. Member, IEEE, and Y. Jia, “Deep Dual-
environment are not controllable (e.g., weather, geometric                            resolution Networks for Real-time and Accurate Semantic Segmentation
information, external objects), assessing the robustness of a                         of Road Scenes,” arXiv e-prints, p. arXiv:2101.06085, Jan. 2021.
model directly in the real world appears to be practically infea-
sible. As such, future work aims at studying the transferability                [13] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, “Bisenet:
of robust assessments derived from virtual scenarios (e.g.,                           Bilateral segmentation network for real-time semantic segmentation,” in
CARLA simulator) to real-world environments. Furthermore,                             Proceedings of the European conference on computer vision (ECCV),
the experiments conducted on the detection mechanisms high-                           2018, pp. ",cs.CV,B,-0.030681642,-0.16458969,-0.1190633
http://arxiv.org/pdf/2201.01857v1,Multi-Grid Redundant Bounding Box Annotation for Accurate Object Detection,"As seen from the ﬁgure, almost        object detection training sets without needing additional ex-
in all cases, the predicted unﬁltered bounding boxes overlap       ternal dataset. Finally, as future work, we would like to tackle
perfectly, proving tight-ﬁt bounding box prediction. small object detection challenges and try to use MultiGridDet
                                                                   on object tracking and segmentation challenges. ",cs.CV,C,-0.14333226,-0.085388765,-0.11595827
http://arxiv.org/pdf/2201.01883v1,Memory-guided Image De-raining Using Time-Lapse Data,"Extensive experiments also show that the proposed method
                                                                               achieved an improved generalization ability on both real-world
F. Visualization of Learned Features                                           and synthetic data. In future work, we will explore video de-
                                                                               raining by leveraging a memory network. In this section, we present the results of a visualization of
learned features within our proposed networks. ",cs.CV,C,-0.27275422,-0.14358258,-0.022219645
http://arxiv.org/pdf/2201.01953v1,Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling,"result for pixel-wise semantic labeling, which is an proﬁtable          236, p. 111402, 2020.
attempt to bridge the tile-level scene classiﬁcation toward
pixel-wise semantic parsing for aerial image interpretation. [2] T. Wellmann, A. Lausch, E. Andersson, S. Knapp, C. Cortinovis,
                                                                        J. Jache, S. Scheuer, P. Kremer, A. Mascarenhas, R. Kraemer
    In the future work, we will dedicate our efforts to enrich          et al., “Remote sensing in urban planning: Contributions towards
the Million-AID with more semantic categories and expand                ecologically sound policies?” Landsc. Urban Plan., vol. ",cs.CV,C,-0.1698559,-0.29755747,0.092341155
http://arxiv.org/pdf/2201.01953v2,Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling,"result for pixel-wise semantic labeling, which is an proﬁtable          236, p. 111402, 2020.
attempt to bridge the tile-level scene classiﬁcation toward
pixel-wise semantic parsing for aerial image interpretation. [2] T. Wellmann, A. Lausch, E. Andersson, S. Knapp, C. Cortinovis,
                                                                        J. Jache, S. Scheuer, P. Kremer, A. Mascarenhas, R. Kraemer
    In the future work, we will dedicate our efforts to enrich          et al., “Remote sensing in urban planning: Contributions towards
the Million-AID with more semantic categories and expand                ecologically sound policies?” Landsc. Urban Plan., vol. ",cs.CV,C,-0.1698559,-0.29755747,0.092341155
http://arxiv.org/pdf/2201.01984v1,Compact Bidirectional Transformer for Image Captioning,"2017; Huang et al. 2019), which     anisms, and we leave integrating these methods for better
averaging the word-level output probability distributions of    performance as our future work. four independently trained instances with different parame-
ter initialization. ",cs.CV,A,-0.030826554,0.3845381,0.099076614
http://arxiv.org/pdf/2201.02001v1,TransVPR: Transformer-based place recognition with multi-level attention aggregation,"Instead               are sparse. Therefore, one direction for future work is to
      of estimating three attention maps separately, a single            estimate camera poses in a regression framework exploiting
      attention map A is computed based on concatenated                  the TransVPR descriptors. patch tokens P. The global feature is expressed by the
      summation of P weighted by A. ",cs.CV,C,-0.034312587,-0.2364054,0.18800505
http://arxiv.org/pdf/2201.02001v2,TransVPR: Transformer-based place recognition with multi-level attention aggregation,"Instead               are sparse. Therefore, one direction for future work is to
      of estimating three attention maps separately, a single            estimate camera poses in a regression framework exploiting
      attention map A is computed based on concatenated                  the TransVPR descriptors. patch tokens P. The global feature is expressed by the
      summation of P weighted by A. ",cs.CV,C,-0.034312587,-0.2364054,0.18800505
http://arxiv.org/pdf/2201.02001v3,TransVPR: Transformer-based place recognition with multi-level attention aggregation,"Instead               are sparse. Therefore, one direction for future work is to
      of estimating three attention maps separately, a single            estimate camera poses in a regression framework exploiting
      attention map A is computed based on concatenated                  the TransVPR descriptors. patch tokens P. The global feature is expressed by the
      summation of P weighted by A. ",cs.CV,C,-0.034312587,-0.2364054,0.18800505
http://arxiv.org/pdf/2201.02001v4,TransVPR: Transformer-based place recognition with multi-level attention aggregation,"ization would be not precise enough when reference images
                                                                      are sparse. Therefore, one research topic of future work is
   • Multi-level & single attention map (mL-sATT). Instead            to estimate camera poses in a regression framework by ex-
      of estimating three attention maps separately, a single         ploiting the TransVPR descriptors. ",cs.CV,C,-0.08196387,-0.27105838,0.09048187
http://arxiv.org/pdf/2201.02010v1,Self-Training Vision Language BERTs with a Unified Conditional Model,"D. Ablation Experiments                                              3) Conditional vs Unconditional: The experiments in the
                                                                  last section show the effectiveness of the conditional model in
   1) Step by Step Ablation Studies: In this section, we provide  ﬁnetuning downstream tasks. In this section, we further study
step by step ablation studies of our proposed system. The         how the condition ﬂag affects the generation performance. ",cs.CV,A,0.3132152,0.36411202,0.17587931
http://arxiv.org/pdf/2201.02011v1,An unambiguous cloudiness index for nonwovens,"In this case, the parameters λ and ν depend on the angle ϕ. Characterization of
                  anisotropy is however subject of further research. In inspection systems for industrial quality control, line cameras are often used to
                  scan the material continuously in the production line, see e. g. [65, 66]. ",cs.CV,B,0.45092803,-0.07778941,-0.046423636
http://arxiv.org/pdf/2201.02011v2,An unambiguous cloudiness index for nonwovens,"In this case, the parameters λ and ν depend on the angle ϕ. Characterization of

                  anisotropy is however subject of further research. In inspection systems for industrial quality control, line cameras are often used to

                  scan the material continuously in the production line, see e. g. [65, 66]. ",cs.CV,B,0.45092803,-0.07778941,-0.046423636
http://arxiv.org/pdf/2201.02028v1,A Light in the Dark: Deep Learning Practices for Industrial Computer Vision,"Subsequently, we
present our results in a comprehensive manner in section 5. To conclude our paper, we
provide summarizing remarks and derive an outlook for future work in section 6. 2 Conceptual Background and Related Work

2.1 Computer Vision and Deep Neural Networks

The ﬁeld of CV is concerned with the acquisition, processing, analysis, and understand-
ing of digital images to generate symbolic or numerical information that can be used to
support automated decision-making. ",cs.CV,C,-0.1920737,-0.069570646,-0.05412129
http://arxiv.org/pdf/2201.02052v1,A Unified Framework for Attention-Based Few-Shot Object Detection,"The experiments
carried out in this work demonstrate the ﬂexibility of the proposed frame-
work and prove that it is convenient for comparing attention techniques. More
comparisons are planned as future work, along with in-depth ablation studies
and the design of new attention techniques for FSOD. To help the develop-
ment of such methods and future comparisons, the code of the proposed AAF
framework will be made available. ",cs.CV,A,0.24599415,0.06547288,0.33758926
http://arxiv.org/pdf/2201.02074v2,EM-driven unsupervised learning for efficient motion segmentation,"We plan to work on mask selec-     ference stage. In addition, they engage by design a
tion in future works. form of implicit consistency pushing the network to
                                                        be robust to corrupted observations. ",cs.CV,A,0.2473552,0.20427771,-0.10309313
http://arxiv.org/pdf/2201.02093v1,Deep Learning Based Classification System For Recognizing Local Spinach,"We trained those
models with noiseless or few noise images so this model will be unable to give the best
performance if the images have noise. As those models gives high performance, we
expect that these results will help for further research. Moreover, it will help to develop
an image application through the application, general people can classify the edible
local spinach. ",cs.CV,C,-0.04713566,0.0059747305,-0.055179495
http://arxiv.org/pdf/2201.02110v1,Eye Know You Too: A DenseNet Architecture for End-to-end Biometric Authentication via Eye Movements,"Afterward, we propose a new methodology for training and evaluating eye movement biometrics models in
Section 6. Finally, we close with a discussion of our results and ideas for future work in Section 7. 2 Prior Work

2.1 Convolutional neural networks (CNNs)

Since the seminal works of AlexNet [8] and VGGNet [9], CNNs have quickly become some of the most popular types
of neural networks for image processing tasks. ",cs.CV,C,-0.14139593,-0.1271757,0.08156565
http://arxiv.org/pdf/2201.02260v1,CitySurfaces: City-Scale Semantic Segmentation of Sidewalk Materials,"As                  used as an image-level uncertainty measure. another line for our future work, we would like to explore auto-
mated sample selection procedures and self-supervised learning                  To select new samples, we feed the pool of unlabeled images
techniques and tailor them to sidewalk and pedestrian facility               to our network, obtain the segmentation and calculate image-
analysis. We chose a simple (yet eﬀective) uncertainty measure               level uncertainty to select images with the highest uncertainty. ",cs.CV,C,-0.1166151,-0.15902388,-0.08152242
http://arxiv.org/pdf/2201.02280v1,Repurposing Existing Deep Networks for Caption and Aesthetic-Guided Image Cropping,"However, despite our method not
being real-time, it was clearly demonstrated that it performs better and it is ﬂexible. Our future work would be to resolve this, similar to how style transfer [43] started off
taking minutes per image, but is now able to run real-time [44]. 5. ",cs.CV,B,0.07685005,-0.14155109,-0.08936531
http://arxiv.org/pdf/2201.02304v1,Budget-aware Few-shot Learning via Graph Convolutional Network,"g is a

feedforward fusion network. We can perform an iterative intra-and-inter graph mes-

sage passing with multi-layer graph convolutional network,                     2We note that extending our method to other types of few-shot clas-
                                                                           siﬁers that requires meta-training is straightforward in the meta-learning
1The unlabeled data use the zero vector as the label encoding o.           framework, which is not our focus and left for future work. 5
   To effectively train the policy network, we propose a su-          set starts with an empty set and continuously enlarges with
pervised method by introducing an expert policy and ap-               newly added instances, thus requires the policy network be
plying dense supervision at each step t. More speciﬁcally,            able to identify the missing classes without any information
given a moderate-sized support set in a task, we are able to          of the novel classes. ",cs.CV,C,-0.30916092,0.13173082,-0.08784474
http://arxiv.org/pdf/2201.02495v1,Sign Language Video Retrieval with Free-Form Textual Queries,"on the 642 sign-sentence pairs of the test set. T2V           V2T

                                                                            Text Embedding R@1↑ R@5↑ R@10↑ MedR↓ R@1↑ R@5↑ R@10↑ MedR↓

               Sign Recognition  Cross-Modal Embeddings                     Translation [9] 30.2 53.1 63.4  4.5 28.8 52.0 60.8 56.1

Alignment R@1↑ R@5↑ R@10↑ MedR↓ R@1↑ R@5↑ R@10↑ MedR↓                       Cross-modal  48.6 76.5 84.6     2.0 50.3 78.4 84.4  1.0

Speech   9.5   16.1  19.0        418.0 5.60.4 13.60.4 17.80.1 202.710.3     Combination  55.8 79.6 87.2     1.0 53.1 79.4 86.1  1.0
Signing
         18.9 32.1 36.5          62.0 24.20.3 40.40.5 46.90.9 15.01.7

lations to future work, which can potentially provide further               How2Sign dataset. Some qualitative examples of videos re-
improvements, and use GrOVLE embeddings for the rest of                     trieved by our system are provided in Fig. ",cs.CV,A,0.20514736,0.28778225,0.30525827
http://arxiv.org/pdf/2201.02495v2,Sign Language Video Retrieval with Free-Form Textual Queries,"We leave the end-to-end fine-
from [2] (pretrained on the BOBSL data) that was used to                                           tuning of the language models with our sign language trans-
initialise our I3D sign video embedding. The sensitivity of                                        lations to future work, which can potentially provide further
our model to different initialisations is demonstrated in Ap-                                      improvements, and use GrOVLE embeddings for the rest of
pendix D. While strongly outperforming Kinetics features,                                          the experiments. BOBSL features remain substantially weaker than the end-                                           (iv) Sign recognition probabilities. ",cs.CV,C,-0.0560366,0.020955306,0.17556626
http://arxiv.org/pdf/2201.02503v1,A Review of Deep Learning Techniques for Markerless Human Motion on Synthetic Datasets,"We hope that these datasets will lead to further advances in human joint
motion estimation and provide an opportunity to determine the performance of current state-of-
the-art algorithms. In future work, we would like to extend our experiments to use a multi-view approach with
multi cameras and expand the scope to include complex and continuous human movements such
as ballet and martial arts. We are also interested in 3D reconstruction scenarios that reproduce 3D
human movements on one side. ",cs.CV,B,0.117526345,-0.381329,0.14080822
http://arxiv.org/pdf/2201.02526v1,Learning Target-aware Representation for Visual Tracking via Informative Interactions,"Matching inside the backbone may be a worth choice     resentation and surrounding context. One possible solution
in future work. for this circumstance is using global search when the target
                                                                 disappears for a long time. ",cs.CV,A,0.14056173,0.0727303,0.03048819
http://arxiv.org/pdf/2201.02533v1,NeROIC: Neural Rendering of Objects from Online Image Collections,"While some
                                                                           works [37,49] uses terms such as light visibility to represent
                                                                           these effects in a physically-based way, applying these tech-
                                                                           niques to data with varying unknown illumination is much
                                                                           more challenging, and requires further investigation. We
                                                                           are planning to explore this topic in our future work. Ethical Implications. ",cs.CV,B,0.25328043,-0.16191737,0.20246294
http://arxiv.org/pdf/2201.02533v2,NeROIC: Neural Rendering of Objects from Online Image Collections,"We are planning to explore these                               Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes. In
topics in our future work. IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,
                                                                                               virtual, June 19-25, 2021. ",cs.CV,B,0.057663582,-0.39785656,0.044467524
http://arxiv.org/pdf/2201.02560v1,A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items,"Devising proper models
MS R-CNN [96], Mask R-CNN [53], and HTC [97]. It also               for this category of objects is our potential future work. showed a modest performance compared to YOLOv3 [105],
CST [10], RetinaNet [35] and YOLCAT [98]. ",cs.CV,C,-0.16144547,-0.15382111,-0.18054242
http://arxiv.org/pdf/2201.02560v2,A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items,"Devising proper models
MS R-CNN [47], Mask R-CNN [12], and HTC [48]. It also             for this category of objects is our potential future work. showed a modest performance compared to YOLOv3 [57],
CST [25], RetinaNet [2] and YOLCAT [49]. ",cs.CV,C,-0.16920249,-0.16883504,-0.17066891
http://arxiv.org/pdf/2201.02605v1,Detecting Twenty-thousand Classes using Image-level Supervision,"Top: our                 region and does not consider overall dataset statistics. We
method using ImageNet-1K as pretraining and ImageNet-21K as              leave incorporating such information for future work. The
co-training; Bottom: using ImageNet-21K for both pretraining and         generalization capabilities of Detic also beneﬁt from the
co-training. ",cs.CV,C_centroid,-0.30937892,0.09844098,-0.1866073
http://arxiv.org/pdf/2201.02605v2,Detecting Twenty-thousand Classes using Image-level Supervision,"Top: our                 region and does not consider overall dataset statistics. We
method using ImageNet-1K as pretraining and ImageNet-21K as              leave incorporating such information for future work. The
co-training; Bottom: using ImageNet-21K for both pretraining and         generalization capabilities of Detic also beneﬁt from the
co-training. ",cs.CV,C,-0.30937892,0.09844098,-0.1866073
http://arxiv.org/pdf/2201.02605v3,Detecting Twenty-thousand Classes using Image-level Supervision,"While Detic is simpler than prior assignment-based
weakly-supervised detection methods, it supervises all image labels to the same
region and does not consider overall dataset statistics. We leave incorporating
such information for future work. Moreover, open vocabulary generalization
has no guarantees on extreme domains. ",cs.CV,C,-0.28246617,-0.025702005,0.051196236
http://arxiv.org/pdf/2201.02609v1,Generalized Category Discovery,"time may belong to new classes. However, OSR aims only
to detect test-time images which do not belong to one of              We hope that this paper will serve as a springboard for
the classes in the labelled set, but does not require any fur-     future work on this more realistic version of open world
ther classiﬁcation amongst these detected images. Mean-            image recognition. ",cs.CV,C,-0.03266949,-0.09551821,0.112415686
http://arxiv.org/pdf/2201.02639v1,MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound,"With access to compute beyond                Audio-Subtitle Finetuning. To evaluate how much audio
our current capacity, future work would be well-suited to consider this and          can help for TVQA, we ﬁnetune Reserve jointly between
other pre-training modiﬁcations. Here, we use FLOPs as our key eﬃciency metric, as they are a critical
                                                                                     bottleneck in model scaling [65, 34, 128]. ",cs.CV,A,0.013910604,0.2406295,-0.04716327
http://arxiv.org/pdf/2201.02639v2,MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound,"This
continue to increase, though a confounding factor might be the learning-rate                                     enables it to outperform other large models, including VATT
schedule. With access to compute beyond our current capacity, future work
would be well-suited to consider this and other pre-training modifications. 8Here, we use FLOPs as our key efficiency metric, as they are a critical
bottleneck in model scaling [66, 34, 130]. ",cs.CV,A,0.02512617,0.43408352,-0.26834854
http://arxiv.org/pdf/2201.02639v3,MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound,"This
continue to increase, though a confounding factor might be the learning-rate                                     enables it to outperform other large models, including VATT
schedule. With access to compute beyond our current capacity, future work
would be well-suited to consider this and other pre-training modiﬁcations. Here, we use FLOPs as our key eﬃciency metric, as they are a critical
bottleneck in model scaling [66, 34, 130]. ",cs.CV,A,0.022189485,0.4359724,-0.24356636
http://arxiv.org/pdf/2201.02639v4,MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound,"This
continue to increase, though a confounding factor might be the learning-rate                                     enables it to outperform other large models, including VATT
schedule. With access to compute beyond our current capacity, future work
would be well-suited to consider this and other pre-training modiﬁcations. Here, we use FLOPs as our key eﬃciency metric, as they are a critical
bottleneck in model scaling [66, 34, 130]. ",cs.CV,A,0.022189485,0.4359724,-0.24356636
http://arxiv.org/pdf/2201.02714v1,Pseudo-labelling and Meta Reweighting Learning for Image Aesthetic Quality Assessment,"In Section IV, we will              propose a multistream network, composed by spatial, motion,
describe the experiments in detail and analyze the results. and structural streams, to gain the multimodal features of path
Finally, Section V will focus on conclusion and future work. planning. ",cs.CV,B,0.09315109,-0.12615287,0.035174426
http://arxiv.org/pdf/2201.02726v1,Real-time Rail Recognition Based on 3D Point Clouds,"and M. Pollefeys, ""Semantic3d. net: A new large-scale point
                                                                              cloud classification benchmark,"" arXiv preprint
   In future works, the point clouds scanned by LiDAR may                     arXiv:1704.03847, 2017.
cause motion distortion when the train runs at high speed. The
time dimension information can be fused into point clouds to            [12] B. Yi, Y. Yang, Q. Yi, W. Dai, and X. Li, ""Novel method for
solve the motion distortion problem and further increase                      rail wear inspection based on the sparse iterative closest point
features for rail recognition. ",cs.CV,B,0.15482765,-0.16387281,-0.19964644
http://arxiv.org/pdf/2201.02772v1,A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval,"From the results, we can see that the overall
precision-recall results are consistent with the mAP scores, where      performance of the prototype contrastive loss on the four datasets
our CLIP4CMR obviously outperforms all the baseline methods. is significantly better than other loss functions, although its per-
                                                                        formance on Wikipedia and NUS-WIDE datasets is slightly lower
   To further study how the superiority of CLIP4CMR is gener-           than the linear regression loss. For the prir-wise losses, we can
ated, we further show the distributions of the intra-class distances    observe that the performance of modality-invariant loss is very
and inter-class distances across different modalities in the test       poor, which shows the necessity of considering negative samples
set, as shown in Figure (3). ",cs.CV,A,0.009785822,0.16133484,-0.13031322
http://arxiv.org/pdf/2201.02772v2,A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval,"Specifically, we unify the model architecture of CLIP4CMR,
4.2.2 Visualization Analysis. To further study how the superiority                                                                                                                                 training protocol, parameter choice and random seed for a relatively
of CLIP4CMR is generated, we further examine the distributions                                                                                                                                     objective comparison. We compare three popular pair-wise losses
of the intra-class image-text distances and inter-class image-text                                                                                                                                 namely modality-invariant loss (i.e., ML), contrastive loss (i.e., CL)
distances in the test set. ",cs.CV,C,-0.11839415,0.07554775,-0.057045434
http://arxiv.org/pdf/2201.02779v1,A Baseline Statistical Method For Robust User-Assisted Multiple Segmentation,"as in [28], in the DGL test may improve the
annotated differently due to the presence of a contour e.g. performance of the proposed method and is the topic of our
the sky in the top image and tree branches in the bottom               future work. image ii) the intensity distributions mix while construction
Ti in DGLtGpTts, DGLfB%B and DGLBp%Bpt e.g. ",cs.CV,B,0.2891838,-0.10195705,-0.07284861
http://arxiv.org/pdf/2201.02837v1,"Mushrooms Detection, Localization and 3D Pose Estimation using RGB-D Sensor for Robotic-picking Applications","The ZED Stereo Camera, a depth       and compared in section VII. The main conclusions and
sensor based on passive stereovision, observes objects from       suggestions for future work are summarized in section VIII. 0.5 to 20m. ",cs.CV,B,0.30440277,-0.36492842,0.097004645
http://arxiv.org/pdf/2201.02850v1,Image-based Automatic Dial Meter Reading in Unconstrained Scenarios,"certainly be compensated in a subsequent reading. TABLE II                                   As future work, we intend to develop a rejection system in
             DISTRIBUTION OF ERRORS BY DIAL POSITION. order to automatically request new image samples in cases of
                                                                            poorly taken images. ",cs.CV,A,0.15556918,0.043984618,0.03845678
http://arxiv.org/pdf/2201.02850v2,Image-based Automatic Dial Meter Reading in Unconstrained Scenarios,"302–318, 2016. As future work, we intend to develop a rejection system in      [5] G. Salomon, R. Laroca, and D. Menotti, “Deep learning for image-based
order to automatically request new image samples in cases of            automatic dial meter reading: Dataset and baselines,” in International
poorly taken images. Additionally, we plan to leverage Gener-           Joint Conference on Neural Networks (IJCNN), July 2020, pp. ",cs.CV,C,-0.2833635,0.024806986,-0.104242414
http://arxiv.org/pdf/2201.02885v1,Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision,"However,
that the images are linked to individual plants at multiple    the discrimination between the ﬁrst 5 degrees seems to be
dates enables this dataset to be used not only for spatially   challenging. Certainly, further research can be done, since
related but also for time series analyses. Figure 14 shows a   this is beyond the scope of this work. ",cs.CV,B,0.08971137,-0.16064501,0.0731321
http://arxiv.org/pdf/2201.02885v2,Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision,"However,
that the images are linked to individual plants at multiple    the discrimination between the ﬁrst 5 degrees seems to be
dates enables this dataset to be used not only for spatially   challenging. Certainly, further research can be done, since
related but also for time series analyses. Figure 14 shows a   this is beyond the scope of this work. ",cs.CV,B,0.08971137,-0.16064501,0.0731321
http://arxiv.org/pdf/2201.02980v3,Invariance encoding in sliced-Wasserstein space for image classification with limited training data,"sign lower weights to the background and discard them auto-
matically. However, these analyses require reformulation of the                      [12] R. F. Murphy, M. V. Boland, M. Velliste, et al., Towards a systematics for
problem, which we leave to future work. Another potential so-                              protein subcellular location: quantitative description of protein localiza-
lution is to use an object detection and segmentation method                               tion patterns and automated analysis of ﬂuorescence microscope images.,
along with our proposed classiﬁcation method. ",cs.CV,C,0.018964892,-0.06540312,-0.04318599
http://arxiv.org/pdf/2201.03043v1,Semantics-driven Attentive Few-shot Learning over Clean and Noisy Samples,"We also study the approach in a noisy FSL
setting, where some of the few-shot support samples are stochastically corrupted. While we
focus on a single semantic modality throughout our experiments, we believe that incorporating
multiple modalities, as in [36], and integration into a generative FSL model based approach, as
in [37], are promising future work directions. Acknowledgements

    This work was supported in part by the TUBITAK Grants 116E445 and 119E597. ",cs.CV,C,-0.11092633,0.06937246,0.13549921
http://arxiv.org/pdf/2201.03045v1,Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations,"In order to maximise the knowledge gain by researchers,
recent studies [68, 81] are reviewing options to apply blockchain technology to the secure distribution
of scientiﬁc datasets. Furthermore, there is remaining opportunity for further research in the area of coalitions against
CSAE. A promising approach is presented by The Technology Coalition (2020). ",cs.CV,A,0.07521109,0.27241033,0.0141929425
http://arxiv.org/pdf/2201.03080v1,The State of Aerial Surveillance: A Survey,"There are a wide range of resources, including      7.2.2 Model Development
public datasets, papers and even competitions available
addressing these tasks. In contrast, the human identiﬁcation      From a model perspective, there are three key aspects
task is the least investigated task arising from the small size   that need further research: model architecture, effects of
of face biometric when captured from an aerial platform. unconstrained data on model trustworthiness, prediction
The task of recognizing faces from long distance with cam-        uncertainties and model explainability. ",cs.CV,C,-0.08922158,-0.07488522,0.20254457
http://arxiv.org/pdf/2201.03080v2,The State of Aerial Surveillance: A Survey,"There are a wide range of resources, including      7.2.2 Model Development
public datasets, papers and even competitions available
addressing these tasks. In contrast, the human identiﬁcation      From a model perspective, there are three key aspects
task is the least investigated task arising from the small size   that need further research: model architecture, effects of
of face biometric when captured from an aerial platform. unconstrained data on model trustworthiness, prediction
The task of recognizing faces from long distance with cam-        uncertainties and model explainability. ",cs.CV,C,-0.08922158,-0.07488522,0.20254457
http://arxiv.org/pdf/2201.03101v1,ImageSubject: A Large-scale Dataset for Subject Detection,"We
                                                                   give the baseline result by using the popular transformer-
                                                                   based model DETR. We believe there is still a lot of space
                                                                   to improve for future work, not only for single frame detec-
                                                                   tion models but also for temporal subject detection models. We conduct detailed analysis for the proposed dataset
                                                                   and also implement experiments to show the difference be-
                                                                   tween our subject detection and other related tasks like
saliency detection. ",cs.CV,C,-0.12176263,-0.12272412,0.2786846
http://arxiv.org/pdf/2201.03101v2,ImageSubject: A Large-scale Dataset for Subject Detection,"We
                                                                   give the baseline result by using the popular transformer-
                                                                   based model DETR. We believe there is still a lot of space
                                                                   to improve for future work, not only for single frame detec-
                                                                   tion models but also for temporal subject detection models. We conduct detailed analysis for the proposed dataset
                                                                   and also implement experiments to show the difference be-
tween our subject detection and other related tasks like               [12] Michael L. Gleicher and Feng Liu. ",cs.CV,C,-0.078275904,-0.049675137,0.28846684
http://arxiv.org/pdf/2201.03176v1,"Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond","the-art pedestrian detectors (except for BGCNet [23]) as well in
cross dataset evaluation, presented in Table 5. We again attribute          Secondly, we train CSP and Cascade RCNN on CityPersons
this to the bias present in the design of current state-of-the-art      and evaluate them on ECP [4] to further study the generaliza-
pedestrian detectors, which are tailored for speciﬁc datasets and       tion abilities of them under different diverse degrees of training
therefore limit their generalization ability. Moreover, a signiﬁcant    datasets. ",cs.CV,C,-0.23803154,-0.07184851,0.0223781
http://arxiv.org/pdf/2201.03176v2,"Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond","the-art pedestrian detectors (except for BGCNet [23]) as well in
cross dataset evaluation, presented in Table 5. We again attribute          Secondly, we train CSP and Cascade RCNN on CityPersons
this to the bias present in the design of current state-of-the-art      and evaluate them on ECP [4] to further study the generaliza-
pedestrian detectors, which are tailored for speciﬁc datasets and       tion abilities of them under different diverse degrees of training
therefore limit their generalization ability. Moreover, a signiﬁcant    datasets. ",cs.CV,C,-0.23803149,-0.071848415,0.022378176
http://arxiv.org/pdf/2201.03243v1,Small Object Detection using Deep Learning,"This architecture now performs
detection at 52*52 scale. Regarding future work, a large dataset containing different types and sizes of drones will be acquired and prepared
for real time object detection. For real time object detection, this task has an extension of hardware implementation which will be carried out in the
future. ",cs.CV,B,0.058616526,-0.24447057,-0.13179474
http://arxiv.org/pdf/2201.03246v1,Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing,"State-of-the-art on-board hardware will further improve the
performance of all detectors. 7 Conclusions and future work

CycleGAN-based style transfer can be efﬁciently used to generate synthetic (fake) adverse condition data that can in
turn be used to improve the performance of object detection models for autonomous racing in real adverse conditions. In this paper, training on synthetic data has improved the performance of four out of ﬁve state-of-the-art detection
models by 42.7 and 4.4 percentage points when tested on real night and real droplet frames, respectively. ",cs.CV,C,-0.10480451,-0.19505405,-0.13997574
http://arxiv.org/pdf/2201.03246v2,Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing,"State-of-the-art on-board hardware will further improve the
performance of all detectors. 7 Conclusions and future work

CycleGAN-based style transfer can be efﬁciently used to generate synthetic (fake) adverse condition data that can in
turn be used to improve the performance of object detection models for autonomous racing in real adverse conditions. In this paper, training on synthetic data has improved the performance of four out of ﬁve state-of-the-art detection
models by 42.7 and 4.4 percentage points when tested on real night and real droplet frames, respectively. ",cs.CV,C,-0.10480451,-0.19505405,-0.13997574
http://arxiv.org/pdf/2201.03323v1,Gait Recognition Based on Deep Learning: A Survey,"Datasets employed for the task are presented in Section 4. Finally, Section 5 presents the
conclusions and future works. 2 THEORETICAL BACKGROUND

This section presents a theoretical background introducing the problem of biometric identification,
describing the deep learning approaches employed for gait recognition, and a detailed introduction
regarding the problem of gait recognition. ",cs.CV,C,-0.18635288,0.00059295446,0.02326556
http://arxiv.org/pdf/2201.03342v1,COIN: Counterfactual Image Generation for VQA Interpretation,"Human attention in visual question answering: Do humans
                                                                                     and deep networks look at the same regions? Computer Vision and
   For future work, we will train COIN on a larger, more                             Image Understanding 163 (2017), 90–100. diverse dataset such as VQAv2 dataset, which contains mul-                      [8] Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, and
tiple images per question rather than only a single one as in                        Andrew Zaldivar. ",cs.CV,C,-0.31372958,-0.070732206,0.19836247
http://arxiv.org/pdf/2201.03454v1,"3D Face Morphing Attacks: Generation, Vulnerability and Detection","However, future work can benchmark the proposed
                                                      4 00.1       method on a large-scale dataset. As a third aspect, cleaning
                                                                   noise from 3D scans is tedious and sometimes requires manual
                              APCER (in %)                         intervention; thus, future work can develop a fully automated
                                                                   noise removal in 3D point clouds to easily the 3D morph
Fig. 9. ",cs.CV,B,0.2743683,-0.274887,-0.17783545
http://arxiv.org/pdf/2201.03454v2,"3D Face Morphing Attacks: Generation, Vulnerability and Detection","3, pp. 365–383,
future work can develop a fully automated noise removal in
3D point clouds to easily the 3D morph generation. 2021. ",cs.CV,B,0.3564924,-0.29669496,-0.10485567
http://arxiv.org/pdf/2201.03546v1,Language-driven Semantic Segmentation,"In this speciﬁc example, the windows of the house are

labeled as ""house"" instead of window, even thought the label ""window"" is available as a choice. We

hope that these failure cases can inform future work, which could involve augmenting training with

negative samples or building ﬁne-grained language-driven semantic segmentation models that can

potentially assign multiple labels when multiple explanations ﬁt the data well. 6 CONCLUSION

We introduced LSeg, a novel method and architecture for training language-driven semantic segmen-
tation models. ",cs.CV,C,-0.263502,0.07973185,0.21426347
http://arxiv.org/pdf/2201.03546v2,Language-driven Semantic Segmentation,"In this speciﬁc example, the windows of the house are labeled as ""house""

instead of window, even thought the label ""window"" is available as a choice. We hope that these

failure cases can inform future work, which could involve augmenting training with negative samples

or building ﬁne-grained language-driven semantic segmentation models that can potentially assign

multiple labels when multiple explanations ﬁt the data well. 6 CONCLUSION

We introduced LSeg, a novel method and architecture for training language-driven semantic segmen-
tation models. ",cs.CV,C,-0.263502,0.07973185,0.21426347
http://arxiv.org/pdf/2201.03639v1,Multi-query Video Retrieval,"In ICCV, 2017. 2
future work. [6] Max Bain, Arsha Nagrani, Gu¨l Varol, and Andrew Zisser-
6. ",cs.CV,A,0.30061084,0.20563479,0.09974206
http://arxiv.org/pdf/2201.03686v1,NFANet: A Novel Method for Weakly Supervised Water Extraction from High-Resolution Remote Sensing Imagery,"By using
                                                                 only point labels, the proposed method obtains comparable
                                                                 results with that of full supervision. As a possible future work, we will conduct research
                                                                 on weakly supervised or semi-supervised methods of self-
                                                                 correction. Moreover, we will also consider the characteristics
                                                                 of other different ground objects and design a dedicated
                                                                 module for feature extraction to further improve the ground
                                                                 object extraction performance of CNN. ",cs.CV,C,-0.17857313,-0.14119306,-0.13864909
http://arxiv.org/pdf/2201.03786v1,Drone Object Detection Using RGB/IR Fusion,"on both synthetically generated IR data and real-world datasets. Conclusions and future work are included in Section 5. To apply deep learning methodologies to solve this problem,
                                        we need pairs of IR/RGB imagery to train deep networks. ",cs.CV,C,-0.24314925,-0.14723572,-0.115421936
http://arxiv.org/pdf/2201.03891v1,Emotion Estimation from EEG -- A Dual Deep Learning Approach Combined with Saliency,"For this      methods. In future work, an emotion estimation approach from
reason, it has been chosen to consider the combination of these  EEG could be used in various applications covering several
two models to merge their advantages. ﬁelds, e.g. ",cs.CV,A,0.18031308,0.098531485,0.37074965
http://arxiv.org/pdf/2201.03902v1,Where Is My Mind (looking at)? Predicting Visual Attention from Brain Activity,"A non-exhaustive list of     average position of the centre of interest in the video. The pre-
works considering DL algorithms with EEG is the following:        sented results were encouraging for further study and indicate
                                                                  the existence of a relationship between visual attention and
   • The use of convolutional neural networks (CNN) has           brain activity. On the other hand, different datasets aiming to
      been considered to extract feature from EEG signals. ",cs.CV,C,-0.03967343,-0.14564672,0.16938227
http://arxiv.org/pdf/2201.04011v1,Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition,"1979–1988. In future work, we intend to extend the similarity-based                   [18] L. Yang, Q. Song, and Y. Wu, “Attacks on state-of-the-art face
method to attack multiple target users or conduct studies on                        recognition using attentional adversarial attack generative network,”
transferability. ",cs.CV,C,-0.26493716,0.05793222,0.13809815
http://arxiv.org/pdf/2201.04011v2,Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition,"1979–1988. In future work, we intend to extend the similarity-based                   [18] L. Yang, Q. Song, and Y. Wu, “Attacks on state-of-the-art face
method to attack multiple target users or conduct studies on                        recognition using attentional adversarial attack generative network,”
transferability. ",cs.CV,C,-0.26493716,0.05793222,0.13809815
http://arxiv.org/pdf/2201.04019v1,Pyramid Fusion Transformer for Semantic Segmentation,"We choose MaskFormer [6]
semantic segmentation map is obtained by probability-                           as our baseline model because of its strong performance
                                                                                among the mask-level classiﬁcation methods [6, 20, 38]. mask marginalization as that in MaskFormer [6], where the                       In the ablations, we further study the effectiveness of our
                                                                                proposed components, including usage of multi-scale fea-
category prediction for pixel (h, w) is computed as                             tures, cross-scale inter-query attention, loss designs, and
                                                                                model parameter sharing. Experimental results demonstrate
               argmaxi∈{1,...,K} pi · mi(h, w)                          (6)     that our model can learn useful information from multi-
                                                                                scale feature maps to deliver high quality segmentation
3.2. ",cs.CV,C,-0.19537507,-0.089871354,-0.05859035
http://arxiv.org/pdf/2201.04019v2,Pyramid Fusion Transformer for Semantic Segmentation,"2 row 2,
using our attention weight loss helps the cross-attention to concentrate more on
the regions corresponding to the categories. To further study the improvements
from the proposed loss, we ablate the loss weight and verify our design through
14     Qin et al. Table 5: Analysis of query-pixel cross-                    49.0            48.7     48.7
attention weight loss. ",cs.CV,C,-0.0027375184,0.078562416,0.11627305
http://arxiv.org/pdf/2201.04024v1,Smart Director: An Event-Driven Directing System for Live Broadcasting,"Smart Director has been tested on real-world soccer datasets, and it has produced
high quality broadcast videos which are comparable to the ones generated by human directors. Our future works are as follows. First, in current version of our Smart Director, the directing
system mainly mimics the human-in-the-loop broadcasting process (e.g., camera view selection
and slow-motion replays), while ignoring the interaction between directors and camera operators
(e.g., instructing a camera operator how to react to major events). ",cs.CV,B,0.023571312,-0.21422295,0.25118816
http://arxiv.org/pdf/2201.04039v1,MobilePhys: Personalized Mobile Camera-Based Contactless Physiological Sensing,"We collected a large multi-modality mobile physiological sensing PPG dataset, which will be released with this
paper. We would expect future work to explore novel contactless or contact physiological sensing methods and
applications using our dataset. We foresee the opportunities of a multi-modality sensing approach (e.g., IMU,
Audio, Ambient light and RGB videos, etc. ",cs.CV,C,0.06789104,-0.09578288,0.17351846
http://arxiv.org/pdf/2201.04039v2,MobilePhys: Personalized Mobile Camera-Based Contactless Physiological Sensing,"We collected a large multi-modality mobile physiological sensing PPG dataset, which will be released with this
paper. We would expect future work to explore novel contactless or contact physiological sensing methods and
applications using our dataset. We foresee the opportunities of a multi-modality sensing approach (e.g., IMU,
Audio, Ambient light and RGB videos, etc. ",cs.CV,C,0.06789104,-0.09578288,0.17351846
http://arxiv.org/pdf/2201.04042v1,Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models,"(2020). open the way to further research in the context of             Moglow: Probabilistic and controllable motion syn-
lightweight neural animation. thesis using normalising ﬂows. ",cs.CV,B,0.10342887,-0.02447344,0.17705017
http://arxiv.org/pdf/2201.04042v2,Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models,"(2020). open the way to further research in the context of             Moglow: Probabilistic and controllable motion syn-
lightweight neural animation. thesis using normalising ﬂows. ",cs.CV,B,0.10342887,-0.02447344,0.17705017
http://arxiv.org/pdf/2201.04114v1,DM-VIO: Delayed Marginalization Visual-Inertial Odometry,"The foundation of       [14] C. Kerl, J. Sturm, and D. Cremers, “Robust odometry estimation for
our IMU initialization is delayed marginalization, which also         RGB-D cameras,” in ICRA, 2013.
enables the pose graph bundle adjustment. We anticipate that this method will spark further research   [15] R. Newcombe, S. Lovegrove, and A. Davison, “DTAM: Dense tracking
in this direction. The idea of delayed marginalization could          and mapping in real-time,” in ICCV, 2011.
be applied to more use cases, e.g. ",cs.CV,B,0.22178972,-0.34185678,0.006443763
http://arxiv.org/pdf/2201.04127v1,HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video,"These limita-      izing the optimization process. We hope the result points
tions point to a range of interesting avenues for future work. in a promising direction toward modeling humans in mo-
                                                                tion and, eventually, achieving fully photorealistic, free-
   Conclusion. ",cs.CV,A,0.054518923,0.036067102,0.064516485
http://arxiv.org/pdf/2201.04127v2,HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video,"Finally, for in-the-wild videos, we rely on manual      in a promising direction toward modeling humans in mo-
intervention to correct segmentation errors. These limita-      tion and, eventually, achieving fully photorealistic, free-
tions point to a range of interesting avenues for future work. viewpoint rendering of people from casual captures. ",cs.CV,B,-0.12683769,-0.32095802,0.1464501
http://arxiv.org/pdf/2201.04214v1,Region-based Layout Analysis of Music Score Images,"As the aforementioned results show, no model detects all the regions of in-
terest in music score images. A speciﬁc object-detection model for LA in OMR
could be a promising avenue for further research, in which speciﬁc characteris-
tics of this type of documents could be exploited. For example, the fact that
their general structure is regular or that their regions are usually wider than
taller. ",cs.CV,C,-0.027595293,-0.019695068,0.19819249
http://arxiv.org/pdf/2201.04236v1,"Incidents1M: a large-scale dataset of images with natural disasters, damage, and incidents","We provide details of the dataset construction, statistics and potential biases; introduce and train a model for incident
                                        detection; and perform image-ﬁltering experiments on millions of images on Flickr and Twitter. We also present some applications on
                                        incident analysis to encourage and enable future work in computer vision for humanitarian aid. Code, data, and models are available at
                                        http://incidentsdataset.csail.mit.edu. ",cs.CV,C,-0.24327317,-0.18076956,0.12392466
http://arxiv.org/pdf/2201.04288v1,Multiview Transformers for Video Recognition,"a new model and the kinetics dataset. In CVPR,
Limitations and future work. Although we have improved                   2017. ",cs.CV,A,0.22116114,0.20015626,-0.01587982
http://arxiv.org/pdf/2201.04288v2,Multiview Transformers for Video Recognition,"a new model and the kinetics dataset. In CVPR,
Limitations and future work. Although we have improved                   2017. ",cs.CV,A,0.22116114,0.20015626,-0.01587982
http://arxiv.org/pdf/2201.04288v3,Multiview Transformers for Video Recognition,"biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
                                                                         Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
Limitations and future work. Although we have improved                   guage models are few-shot learners. ",cs.CV,A,0.0499714,0.25455928,0.021474928
http://arxiv.org/pdf/2201.04288v4,Multiview Transformers for Video Recognition,"biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
                                                                         Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
Limitations and future work. Although we have improved                   guage models are few-shot learners. ",cs.CV,A,0.0499714,0.25455928,0.021474928
http://arxiv.org/pdf/2201.04329v1,Neural Residual Flow Fields for Efficient Video Representations,"We believe that searching through pos-         and make implicit representations more accessible in a va-
sible combinations of input feature encoding and activation          riety of cases. Further explorations on unsupervised optical
functions might yield even better results, and we leave this         ﬂow estimation with the implicit representations would also
for future work. be an interesting research direction to improve the perfor-
                                                                     mance. ",cs.CV,C,0.029926553,-0.09734686,0.016151328
http://arxiv.org/pdf/2201.04388v1,OCSampler: Compressing Videos to One Clip with Single-step Sampling,"rate annealing for 50 epochs and an initial learning rate of
                                                                    0.001. We conduct all experiments on 8 TITAN XPs and
   • FrameExit [9] adopts a deterministic policy function           will release our codes public to facilitate future works. and gating modules to determine the earliest exiting
      point for inference. ",cs.CV,A,0.15716463,0.25114816,-0.15444785
http://arxiv.org/pdf/2201.04402v1,MoViDNN: A Mobile Platform for Evaluating Video Quality Enhancement with Deep Neural Networks,"Finally, a real-world demonstration of the
MoViDNN can be seen in the provided video 3. As future work, we plan to improve the subjective test part of MoViDNN
by including a crowdsourcing option. Moreover, extending the platform to sup-
port additional DNN based video quality enhancement methods such as video
frame interpolation can also be done. ",cs.CV,B,-0.04015013,-0.11713794,-0.054431885
http://arxiv.org/pdf/2201.04494v1,SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,"Finally, to further facili-
For simplicity, we faithfully follow the three baseline net-   tate the research in this research area, we also release the
works used in their original paper Wang et al. (2020); Sauder  unlabeled York point clouds, encouraging further research
and Sievers (2019), including PointNet Qi et al. (2017a),      exploration on this part of the data. ",cs.CV,B,0.24895033,-0.0798621,-0.12915534
http://arxiv.org/pdf/2201.04623v1,Virtual Elastic Objects,"moved resulting background clutter in the point clouds. The      We hope that the presented results and the accompanying
physical simulator turned out to be remarkably robust to-        dataset will inspire and enable future work on reconstruct-
wards noise and can run with any reconstructed point cloud       ing and re-rendering interactive objects. with temporal correspondences. ",cs.CV,B,0.23789535,-0.37560403,-0.09357939
http://arxiv.org/pdf/2201.04706v1,Semantic Labeling of Human Action For Visually Impaired And Blind People Scene Interaction,"Once an action is recognized, the corresponding
label will be generated on the output device by raised cuboids. Our future works
include two main tasks: - Improving the action recognition performance. – Pro-
viding more informative semantic labeling that will take into consideration face
expressions. ",cs.CV,C,-0.1328291,-0.09517522,0.31345904
http://arxiv.org/pdf/2201.04755v1,Spatial-Temporal Map Vehicle Trajectory Detection Using Dynamic Mode Decomposition and Res-UNet+ Neural Networks,"The final section concludes the paper and
operations, management, and design. One of the most impactful                 addresses future works. video-based trajectory datasets is the next generation simulation
(NGSIM) trajectory dataset [1], which has significantly boosted                                        II. ",cs.CV,B,0.12476957,-0.11342879,0.08325446
http://arxiv.org/pdf/2201.04756v3,Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction,"As an unsupervised learning method for roadside LiDAR
application, this method has the better explanatory capability and does not need any labeled data. Compared to earlier roadside LiDAR methods that rely on observational indicators, the data-
driven algorithm was built on a solid theoretical foundation and required few parameters, making
it suitable to be used as the benchmark for future works. After a thorough evaluation, this method
has achieved the new state-of-the-art by relating the rich body of background modeling
techniques to the LiDAR point clouds. ",cs.CV,B,0.09299796,-0.17934945,-0.16015068
http://arxiv.org/pdf/2201.04756v4,Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction,"The data-driven algorithm
was built on a solid theoretical foundation than earlier roadside LiDAR methods that rely on
observational indicators. This method required few parameters, making it suitable to be used as
the benchmark for future works. After a thorough evaluation, this method has achieved the new
state-of-the-art by relating the rich body of background modeling techniques to the LiDAR point
clouds. ",cs.CV,B,0.31426722,-0.25930867,-0.11144447
http://arxiv.org/pdf/2201.04756v5,Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction,"The data-driven algorithm
was built on a solid theoretical foundation than earlier roadside LiDAR methods that rely on
observational indicators. This method required few parameters, making it suitable to be used as
the benchmark for future works. After a thorough evaluation, this method has achieved the new
state-of-the-art by relating the rich body of background modeling techniques to the LiDAR point
clouds. ",cs.CV,B,0.31426722,-0.25930867,-0.11144447
http://arxiv.org/pdf/2201.04771v1,Unlocking large-scale crop field delineation in smallholder farming systems with transfer learning and weak supervision,"In such settings, aerial imagery could be an alternative source
of inputs, since airplanes and drones ﬂy beneath clouds and aerial imagery
has been used to delineate ﬁelds in the past [24]. We leave an investigation
of how well ﬁeld delineation models can transfer between satellite and aerial
imagery to future work. Second, in some regions it may be diﬃcult even for humans to annotate
ﬁelds. ",cs.CV,B,0.0247578,-0.19648528,0.054310128
http://arxiv.org/pdf/2201.04777v1,A Survey on Masked Facial Detection Methods and Datasets for Fighting Against COVID-19,"How to process various resolutions of images    reviewed. Thirteen open datasets are concluded from various
   in a feasible manner is an important issue in future work. aspects and their valid links are provided. ",cs.CV,B,0.05770651,-0.14664745,-0.026934259
http://arxiv.org/pdf/2201.04788v1,AI Singapore Trusted Media Challenge Dataset,"Section 4 introduces the
experimental setting and results for the user study. Section 5 concludes the whole paper and gives some directions for
future work. 2 Related Work

This section gives an overview on existing deepfake datasets as well as user studies on these datasets. ",cs.CV,A,-0.09456731,0.18966413,-0.053928543
http://arxiv.org/pdf/2201.04788v2,AI Singapore Trusted Media Challenge Dataset,"They also used 3 different models to detect audio manipulation
(ResNet50), face manipulation (WSDAN [26]) and lip-sync errors (SyncNet) respectively and aggregate at the score
level using a random forest. 5.2 Analysis

Similar to user studies describe in Section 4, we further study the performance of winners’ models on different types of
fake media and generation methods. The results are in shown in the following tables. ",cs.CV,A,-0.01698551,0.12491183,0.17205
http://arxiv.org/pdf/2201.04788v3,Trusted Media Challenge Dataset and User Study,"In particular, we focused on
                                                                                                                                     the detection of audiovisual fake media, where either or both video
                                        CCS CONCEPTS                                                                                 and audio modalities may be modified. For further research by the
                                                                                                                                     community, we are releasing the TMC dataset which includes 4,380
                                        • Computing methodologies → Computer vision tasks; • Ap-                                     fake and 2,563 real videos that were part of the challenge. plied computing → Computer forensics. ",cs.CV,C,-0.08254461,-0.14101501,0.10688603
http://arxiv.org/pdf/2201.04797v1,Scalable Cluster-Consistency Statistics for Robust Multi-Object Matching,"            

A.8. A Simple Demonstrating Example                                 1 0 1 0 1 0 0 0

   The following example heuristically illustrates the usefulness      01010100
of the S statistic in emphasizing the good edges, while rigorous
theorems are left for future work. The top part of Figure 7 demonstrates the corresponding
                                                                    keypoint graph. ",cs.CV,A,0.27807838,0.17909089,-0.0049133226
http://arxiv.org/pdf/2201.04833v1,SnapshotNet: Self-supervised Feature Learning for Point Cloud Data Segmentation Using Minimal Labeled Data,"Section 4 details the experimental
   Therefore, based on the two pieces of our previous work, we      results, including the designs and evaluations of data capturing,
further propose the SnapshotNet, which integrates multi-FOV         feature learning, and segmentation. Finally Section 5 concludes
snapshot generation, contrastive feature learning, and a weakly-    the work with discussions of a few ideas for future work. supervised technique for point-wise scene segmentation using
a voting mechanism. ",cs.CV,B,-0.19435866,-0.3577218,-0.090191096
http://arxiv.org/pdf/2201.04866v1,Weakly Supervised Scene Text Detection using Deep Reinforcement Learning,"3: Examples of assessor predictions on background; IoU
on ICDAR2013; red rectangles are predicted bounding boxes          prediction on red square, ground-truth IoU on green square

                                                                   feasible as we were able to get reasonable results without any
                                                                   human data annotation required. Concerning the supervised model there are several
                                                                   improvements with a lot of potential, which we propose for
                                                                   future work. First of all, our feature extractor only relies on
                                                                   convolutional ﬁlters of ﬁxed scales. ",cs.CV,C,-0.18092108,-0.07294759,-0.07433751
http://arxiv.org/pdf/2201.04898v1,Flexible Style Image Super-Resolution using Conditional Objective,"Also, we can ﬁnd many solutions by controlling a single parameter at the
inference phase. We will release our code for further research and comparisons. References

 [1] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, “Deep learning for single image super-resolution:
      A brief review,” IEEE Transactions on Multimedia, vol. ",cs.CV,B,-0.08140305,-0.12176452,-0.22949892
http://arxiv.org/pdf/2201.04898v2,Flexible Style Image Super-Resolution using Conditional Objective,"Also, we can ﬁnd many solutions by controlling a single parameter at the
inference phase. We will release our code for further research and comparisons. References

 [1] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, “Deep learning for single image super-resolution:
      A brief review,” IEEE Transactions on Multimedia, vol. ",cs.CV,B,-0.08140305,-0.12176452,-0.22949892
http://arxiv.org/pdf/2201.04898v3,Flexible Style Image Super-Resolution using Conditional Objective,"Also, we can ﬁnd many solutions by controlling a single parameter at the
inference phase. We will release our code for further research and comparisons. References

 [1] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, “Deep learning for single image super-resolution:
      A brief review,” IEEE Transactions on Multimedia, vol. ",cs.CV,B,-0.08140305,-0.12176452,-0.22949892
http://arxiv.org/pdf/2201.04945v1,Learning Semantic Abstraction of Shape via 3D Region of Interest,"tion of each shape. We conclude and discuss ideas for future work in Section 5. At the same time, with the rapid development of neural net-
2. ",cs.CV,A,-0.035237208,0.15783772,0.010801498
http://arxiv.org/pdf/2201.05047v1,TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,"Our
                                        proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a
                                        single V100 GPU device. Code and models will be available for further research. Index Terms—Video Object Detection, Vision Transformers, Scene Understanding, Video Understanding. ",cs.CV,B,0.11198675,-0.33462322,-0.09018352
http://arxiv.org/pdf/2201.05047v2,TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,"Our
                                        proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a
                                        single V100 GPU device. Code and models will be available for further research. Index Terms—Video Object Detection, Vision Transformers, Scene Understanding, Video Understanding. ",cs.CV,B,0.11198675,-0.33462322,-0.09018352
http://arxiv.org/pdf/2201.05047v3,TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,"Our
                                           proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a
                                           single V100 GPU device. Code and models will be available for further research. Index Terms—Video Object Detection, Vision Transformers, Scene Understanding, Video Understanding. ",cs.CV,B,0.11198675,-0.33462322,-0.09018352
http://arxiv.org/pdf/2201.05057v1,On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles,"If we apply data augmentation and trajectory smoothing at                                                               worse performance in some normal cases. A complete de-
train time simultaneously, the prediction error under attacks                                                           fense of adversarial trajectories is a promising future work. is reduced by 26% on average while the prediction error of
normal cases is increased by 11%. ",cs.CV,A,-0.03595509,0.14380182,-0.17478281
http://arxiv.org/pdf/2201.05057v2,On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles,"Al-
though we showed different values for the two parameters
in Tab.1, we did not observe a clear correlation between the
parameters and prediction accuracy. We can openly discuss
this question as future work. 13 ",cs.CV,A,0.42397922,0.32082352,-0.11055214
http://arxiv.org/pdf/2201.05057v3,On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles,"Al-
though we showed different values for the two parameters
in Tab.1, we did not observe a clear correlation between the
parameters and prediction accuracy. We can openly discuss
this question as future work. ",cs.CV,A,0.41952848,0.3231383,-0.095426366
http://arxiv.org/pdf/2201.05119v2,Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?,"This        RELICv2 (ours)                           77.1

model is trained with a cross-entropy loss, a cosine learn-

ing rate schedule and full access to labels using the same set of data augmentations as proposed by

[Chen et al., 2020a]. While the recent work of [Wightman et al., 2021] proposes a series of elaborate

optimization and data augmentation tricks to improve the performance of the supervised ResNet50

on ImageNet, we leave the application of these heuristics for future work. Instead, in this work, we

focus on a fair like-for-like comparison between self-supervised methods and supervised learning

7. ",cs.CV,C,-0.35763252,0.14330246,-0.123999745
http://arxiv.org/pdf/2201.05131v1,SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation,"Thus, we use a prediction MLP head only for optimizing the distillation ob-
jective and achieve boosts in performance with just the backbone network during inference. We believe studying the reasoning for this effect is an interesting future work. Our work also
serves as an improved benchmark for future self-supervised distillation works. ",cs.CV,A,-0.09116669,0.2940079,-0.087245904
http://arxiv.org/pdf/2201.05151v1,"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning","ImageNet and tiered-ImageNet benchmarks. We further study our         [11] A. R. Feyjie, R. Azad, M. Pedersoli, C. Kauffman, I. B.
methods in paradigms of active and continual learning, proposing              Ayed, and J. Dolz, “Semi-supervised few-shot learning for
extensions for undertaking such tasks. ",cs.CV,C,-0.33957282,0.058272615,-0.10482273
http://arxiv.org/pdf/2201.05151v2,"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning","These models are able to accomplish
strong performance on Meta-Dataset, mini-ImageNet and tiered-ImageNet bench-
marks. We further study our methods in paradigms of active and continual learn-
ing, proposing extensions for undertaking such tasks. Future research on better continual learning of the task-wise feature manifolds
in both models can be beneﬁcial in boosting the performance. ",cs.CV,C,-0.35387555,0.08900575,-0.16602737
http://arxiv.org/pdf/2201.05299v1,A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering,"Our work has limitations
                                                                    that the dense passage retrieval is not optimized for the OK-
                                                                    VQA task, due to the unavailability of ground-truth support-
                                                                    ing facts. We consider this as one of our future work, as well
                                                                    as improving the quality of image-to-text transformation. References                                                          [14] Xiaotian Han, Jianwei Yang, Houdong Hu, Lei Zhang, Jian-
                                                                          feng Gao, and Pengchuan Zhang. ",cs.CV,C,0.014093367,-0.04858868,0.12152833
http://arxiv.org/pdf/2201.05307v1,Unsupervised Temporal Video Grounding with Deep Semantic Clustering,"Therefore, we choose 4 as the neck number        lines. The future work includes applying DSCNet to other
on Charades-STA dataset in our experiments. tasks/datasets (Li et al. ",cs.CV,A,0.039314196,0.19519038,0.07167247
http://arxiv.org/pdf/2201.05346v4,Arbitrary Handwriting Image Style Transfer,"On the other hand, our network model has some flaws when training the data set, such as the size of the
text, different size of text may also cause bad results. If the network structure can be optimized in the
future work to improve this situation or solve it, then it will be a great progress for deep learning
handwriting imitation or text style transfer. 7 ACKNOWLEDGEMENTS

This work was supported by National Natural Science Foundation of China (61772179),Hunan
Provincial Natural Science Foundation of China(2020JJ4152), the Science and Technology Plan
Project of Hunan Province(2016TP1020),Double First-Class University Project of Hunan
Province(Xiangjiaotong [2018]469), Postgraduate Scientific Research Innovation Project of Hunan
Province(CX20190998),Degree & Postgraduate Education Reform Project of Hunan Province
(2019JGYB266,2020JGZD072),Industry University Research Innovation Foundation of Ministry of
Education Science and Technology Development Center (2020QT09), Hengyang technology
innovation guidance projects(Hengcaijiaozhi [2020]-67), Postgraduate Teaching Platform Project of
Hunan Province(Xiangjiaotong [2019]370-321). ",cs.CV,A,-0.19298023,0.35226238,0.013849879
http://arxiv.org/pdf/2201.05479v1,HardBoost: Boosting Zero-Shot Learning with Hard Classes,"We will leave this problem   samples per class. as a future work. Last but not least, as shown in Table 3, the
proposed HarS-VAEGAN achieves the new state-of-the-art              In the second setting, since DU and DS are compound
performances, outperforming the previous one [19] by 2.0%       in the training, models have to predict pseudo labels in
and 0.4% on AWA2 and SUN respectively. ",cs.CV,A,0.017160522,0.2827178,-0.113536656
http://arxiv.org/pdf/2201.05489v1,Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks,"In terms of generalization, we discussed the ability of language to transfer knowledge to new categories, because
language can describe unseen objects through the reorganization of symbols. However, these experiments are only a few initial
attempts, and their practical versatility needs further research on more datasets and tasks. Our model achieved good results on simple datasets, and performance on more complex datasets still needs to improve. ",cs.CV,C,-0.31354493,0.23692104,0.287507
http://arxiv.org/pdf/2201.05514v1,Determination of building flood risk maps from LiDAR mobile mapping data,": Preprint submitted to Elsevier                                       Page 14 of 17
Determination of building ﬂood risk maps from LiDAR mobile mapping data

trians, has always been the greatest challenge. This can lead       As for future work, image-based detection can be used
to the missing and incompleteness of facade openings at lower   to validate the detections from LiDAR data and complement
parts of the facades. In the data used in this study, many fa-  facade openings that are not identiﬁed. ",cs.CV,B,0.16765285,-0.27382427,-0.07022224
http://arxiv.org/pdf/2201.05585v2,Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip Connections and Hybrid Learning,"Nevertheless,
these numbers confirm the effectiveness of the domain adaptation performed by
HYLDA. The effects of different data set sizes and number of class instances
needs to be investigated in future work. 5.4 Ablation Study

We conduct an ablation study on the N → K direction with the X250 labeled tar-
get domain semi-supervision data set. ",cs.CV,A,0.083908215,0.32759607,0.04681943
http://arxiv.org/pdf/2201.05729v1,CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks,"As in
results (8.1), additional metrics for internal validation re-    Tab. 9, the top two rows represent statistics based on the
sults (8.2), information about question sub-types (8.3), de-     full standard validation set (without Implicit Mitigation ﬁl-
tails about language mitigation (8.4), and further analysis of   tering). It is obvious to observe that the average number of
model behavior before and after distillation (8.5). ",cs.CV,A,0.22099832,0.4579826,0.02979275
http://arxiv.org/pdf/2201.05816v1,A Critical Analysis of Image-based Camera Pose Estimation Techniques,"Since Krizhevsky et al. [46] show the advantage
of learning-based features, further research followed that used CNN layer ac-
tivations as oﬀ-the-shelf image descriptors that appear as objective results in
retrieval tasks [47, 48]. Following classic retrieval approaches, such work uses
CNN to aggregate local features [49, 50]. ",cs.CV,C,-0.3198883,-0.07818758,-0.025501683
http://arxiv.org/pdf/2201.05829v1,Multi-View representation learning in Multi-Task Scene,"In section 6, we designed a substantial
number of experiments to demonstrate the performance of the algorithms. In last section, we made a
conclusion and discussed some future works about our proposed methods. 3
2. ",cs.CV,A,0.25367433,0.17610092,-0.13006428
http://arxiv.org/pdf/2201.05858v1,Smart Parking Space Detection under Hazy conditions using Convolutional Neural Networks: A Novel Approach,"[41] F. Al-Turjman, A. Malekloo, Smart parking in iot-enabled cities: A survey, Sustainable Cities and
      Society 49 (2019) 101608. [42] M. Khalid, K. Wang, N. Aslam, Y. Cao, N. Ahmad, M. K. Khan, From smart parking towards
      autonomous valet parking: A survey, challenges and future works, Journal of Network and Computer
      Applications (2020) 102935. [43] Z. Wang, Z. Deng, S. Wang, Accelerating convolutional neural networks with dominant convolutional
      kernel and knowledge pre-regression, in: B. Leibe, J. Matas, N. Sebe, M. Welling (Eds. ",cs.CV,C,-0.13892208,-0.029887998,-0.1136693
http://arxiv.org/pdf/2201.05986v1,Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels,"abs/2001.05201, pp. 1–13, 2020.
modules, which we will investigate in future work. [11] J. Thies, M. Elgharib, A. Tewari, C. Theobalt, and M. Nießner, “Neural
                        ACKNOWLEDGMENT                                                 voice puppetry: Audio-driven facial reenactment,” in European Confer-
                                                                                       ence on Computer Vision. ",cs.CV,C,-0.07521323,-0.12720507,0.26681712
http://arxiv.org/pdf/2201.05989v1,Instant Neural Graphics Primitives with a Multiresolution Hash Encoding,"https://doi.org/10.2312/EGWR/
tracer. The initial results are promising, as shown in Figure 13, and      EGSR07/051- 060
we intend to pursue this direction further in future work. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-
7 CONCLUSION                                                               Brualla, and Pratul P. Srinivasan. ",cs.CV,A,0.3506035,0.23842831,0.05476538
http://arxiv.org/pdf/2201.05989v2,Instant Neural Graphics Primitives with a Multiresolution Hash Encoding,"Our
    up is small in practice with significant detriment to quality. multi-resolution hash encoding provides a practical learning-based

   Alternatively to hand-crafted hash functions, it is conceivable to
optimize the hash function in future work, turning the method into
a dictionary-learning approach. Two possible avenues are (1) de-
veloping a continuous formulation of indexing that is amenable to
analytic differentiation or (2) applying an evolutionary optimization
algorithm that can efficiently explore the discrete function space. ",cs.CV,A,-0.0128939375,0.17060225,-0.08945828
http://arxiv.org/pdf/2201.05991v1,Video Transformers: A Survey,"other settings [50], [52], [71], [88]. While we expect them to
In a similar fashion, video models could greatly beneﬁt from      follow the same trend as other modalities, further research
longer time spans by leveraging memory modules (such as           is needed. the one seen in [58]) to create recurrent Transformers and
extend VTs receptive ﬁeld even further. ",cs.CV,B,0.030500615,-0.11442129,0.104345
http://arxiv.org/pdf/2201.05991v2,Video Transformers: A Survey,"We hope
spaces that exhibit better generalization capabilities. When        our contributions in this paper will entice further research
targeting video-only tasks (e.g., tracking, segmentation, clas-     in many different areas of application and boost our current
siﬁcation) we see potential in multi-modal SSL to learn             understanding of Video Transformers. such spaces. ",cs.CV,B,-0.064841166,-0.18850476,0.18384416
http://arxiv.org/pdf/2201.06030v1,"Fully Convolutional Change Detection Framework with Generative Adversarial Network for Unsupervised, Weakly Supervised and Regional Supervised Change Detection","In the basic
erence, (d) density of FCD-AN, (e) density of FCD-GAN, (f) density of     format, we only train a unidirectional generator. The bidi-
                                                                          rectional generator is worth exploring in the future work. FSCD, (g) binary change map of FCD-AN, (h) binary change map of
                                                                             For weakly and regional supervised change detection,
FCD-GAN, and (i) binary change map of FSCD, where white indicates         since semantic supervision is added to define what is actu-
                                                                          ally the changes, discriminator and adversarial process are
true detection, red indicates false alarms, and blue indicates omission   utilized as the constraints to fuse the semantic information. ",cs.CV,C,-0.17206696,0.11074971,0.098008946
http://arxiv.org/pdf/2201.06192v1,Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems,"adversarial attack in both digital and physical domains. In
                                                                      future work, we will launch the physical adversarial attack
D. Comparison and Discussion                                          against the black-box DNNs model with the help of the side-
                                                                      channel attack [43]. There are few works focusing on the physical AEs against
the object detectors. ",cs.CV,C,-0.15304714,0.04906129,-0.13142791
http://arxiv.org/pdf/2201.06289v1,The CLEAR Benchmark: Continual LEArning on Real-World Imagery,"However, Finetuning a linear layer
on top of unsupervised feature representations
(YFCC-B0) pre-trained on bucket 0th of
CLEAR improves performance to 91.9%
In-domain acc (under iid protocol) and 91.4%
Next-domain acc (under streaming protocol). This suggests that CLEAR is still challenging
without unsupervised pre-training even in the
simplest incremental domain learning setup,
and future works should embrace unlabeled
data for continual semi-supervised learning to
maximize performances. Figure 6: Accuracy matrix under iid proto-
GDumb falls short compared to other base- col with Linear, YFCC-B0, Biased Reservoir,
lines: GDumb [44] as a degenerate solution is Finetuning strategy. ",cs.CV,C,-0.21232042,0.22426501,-0.20247169
http://arxiv.org/pdf/2201.06289v2,The CLEAR Benchmark: Continual LEArning on Real-World Imagery,"However, Finetuning a linear layer
on top of unsupervised feature representations
(YFCC-B0) pre-trained on bucket 0th of
CLEAR improves performance to 91.9%
In-domain acc (under iid protocol) and 91.4%
Next-domain acc (under streaming protocol). This suggests that CLEAR is still challenging
without unsupervised pre-training even in the
simplest incremental domain learning setup,
and future works should embrace unlabeled
data for continual semi-supervised learning to
maximize performances. Figure 6: Accuracy matrix under iid proto-
GDumb falls short compared to other base- col with Linear, YFCC-B0, Biased Reservoir,
lines: GDumb [44] as a degenerate solution is Finetuning strategy. ",cs.CV,C,-0.21232042,0.22426501,-0.20247169
http://arxiv.org/pdf/2201.06289v3,The CLEAR Benchmark: Continual LEArning on Real-World Imagery,"However, Finetuning a linear layer
on top of unsupervised feature representations
(YFCC-B0) pre-trained on bucket 0th of
CLEAR improves performance to 91.9%
In-domain acc (under iid protocol) and 91.4%
Next-domain acc (under streaming protocol). This suggests that CLEAR is still challenging
without unsupervised pre-training even in the
simplest incremental domain learning setup,
and future works should embrace unlabeled
data for continual semi-supervised learning to
maximize performances. Figure 6: Accuracy matrix under iid proto-
GDumb falls short compared to other base- col with Linear, YFCC-B0, Biased Reservoir,
lines: GDumb [44] as a degenerate solution is Finetuning strategy. ",cs.CV,C,-0.21232042,0.22426501,-0.20247169
http://arxiv.org/pdf/2201.06357v1,Disentangled Latent Transformer for Interpretable Monocular Height Estimation,"The            tation and height estimation. This work provides novel
aforementioned experimental results have demonstrated the        insights for future works to better understand and design
effectiveness of our proposed DLT model on both height           MHE models. estimation and unsupervised segmentation tasks. ",cs.CV,B,0.23596399,-0.08890408,-0.012055883
http://arxiv.org/pdf/2201.06357v2,Disentangled Latent Transformer for Interpretable Monocular Height Estimation,"The            tation and height estimation. This work provides novel
aforementioned experimental results have demonstrated the        insights for future works to better understand and design
effectiveness of our proposed DLT model on both height           MHE models. estimation and unsupervised segmentation tasks. ",cs.CV,B,0.23596399,-0.08890408,-0.012055883
http://arxiv.org/pdf/2201.06435v1,FourierNet: Shape-Preserving Network for Henle's Fiber Layer Segmentation in Optical Coherence Tomography Images,"This suggests             for other applications. This possibility can be investigated as
that deep learning has a great potential to analyze OCT                future work. images in more detail than the common practice. ",cs.CV,C,-0.071507215,-0.01002478,-0.0706992
http://arxiv.org/pdf/2201.06459v2,A Novel Framework to Jointly Compress and Index Remote Sensing Images for Efficient Content-Based Retrieval,"It is worth noting that the encoder of the
proposed framework relies on the 2D convolutional layers,               [6] Z. Zhang, Q. Zou, Y. Lin, L. Chen, and S. Wang, “Improved
and thus gives equal importance to each image band. As a                     deep hashing with soft pairwise similarity for multi-label im-
future work, we plan to integrate 3D convolutional layers to                 age retrieval,” IEEE Trans. Multimedia, vol. ",cs.CV,C,-0.19564334,-0.058633238,0.037658133
http://arxiv.org/pdf/2201.06569v1,Automatic Quantification and Visualization of Street Trees,"The entire pipeline is showcased in Figure 4.
trunks are visually similar. However, we have not conducted any
experiments on this as it requires additional annotation of tree         4.1 Detection and Counting
classes by experts, and hence would be part of our future work. In this work, we focus on counting the trees, irrespective of their      In the detection module, the model tries to predict bounding boxes
types. ",cs.CV,B,0.093170196,0.021775689,0.0015461942
http://arxiv.org/pdf/2201.06644v1,HydraFusion: Context-Aware Selective Sensor Fusion for Robust and Efficient Autonomous Vehicle Perception,"minimum variance unbiased estimator can be derived in a batch
                                                                         manner as:
   (4) We implement our approach on an industry-standard AV
        hardware platform, the Nvidia Drive PX2, to demonstrate          𝑛                      𝑛
        that our approach can be practically deployed in a real AV
        with comparable energy consumption, latency, and memory          𝑦ˆ(𝑛) = [∑︁ 𝐻𝑇 𝑅−1𝐻𝑖 ]−1 · ∑︁ 𝐻𝑇 𝑅−1𝑥𝑖                                (5)
        usage to state-of-the-art methods. 𝑖𝑖                  𝑖𝑖

   (5) We open-source our algorithmic implementation and ar-             𝑖 =1              𝑖 =1
        chitecture1 to benefit the research community and enable
        further study of selective sensor fusion approaches for CPS         Typically, the more information a fusion filter has, the better the
        problems. performance will be. ",cs.CV,B,0.29789743,-0.13941647,-0.14353347
http://arxiv.org/pdf/2201.06696v1,ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues,"Meanwhile,
                                                                        the initial proposal generation model is also hard to capture
Edge boxes [46]     15.2 42.5 58.3 64.7 72.5                            small objects. We leave the exploration of super-resolution
                                                                        techniques to solve this problem for future work. Initial score       19.1 46.5 61.2 65.7 74.4
                                                                        5. ",cs.CV,B,0.22922674,-0.12029274,-0.14260602
http://arxiv.org/pdf/2201.06740v1,Convolutional Cobweb: A Model of Incremental Learning from 2D Images,"These ﬁndings suggest that integrating convolutional processing with incremental concept
learning is a promising research direction; however, more work is needed to develop approaches
at the intersection of these two lines of research. In particular, future work should explore variants
of Convolutional Cobweb that utilize multiple convolutional layers (similar to our more complex
CNN model). Our hope is that by utilizing more complex convolutional structures we can exceed
the performance of the more complex CNN. ",cs.CV,C,-0.32489493,0.0800555,0.047358286
http://arxiv.org/pdf/2201.06799v1,"Pistol: Pupil Invisible Supportive Tool to extract Pupil, Iris, Eye Opening, Eye Movements, Pupil and Iris Gaze Vector, and 2D as well as 3D Gaze","Here it is clear that there are some outliers, but
the most common estimates have an error below 50 cm. This clearly shows that the depth estimation especially for a
lateral camera placement needs further research, and we hope to improve it in future versions. 4.5 Marker detection

Table 7: Results of the marker detection. ",cs.CV,B,0.28333715,-0.29260325,-0.050503597
http://arxiv.org/pdf/2201.06889v1,Boosting Robustness of Image Matting with Context Assembling and Strong Data Augmentation,"A
possible solution is to learn the structure of the foreground
objects. We leave it as a future work. Figure 14. ",cs.CV,B,0.04320584,-0.23681165,0.060913403
http://arxiv.org/pdf/2201.06945v2,It's All in the Head: Representation Knowledge Distillation through Classifier Sharing,"Finally, training with SH-KD and TH-KD increases the
similarity between the teacher and student embeddings and leads to the desired
improvement in the student accuracy. For future work, we would like to inves-
tigate ways to apply the proposed approaches to an ensemble of teachers. In
addition, a theoretical formulation based on tighter upper bounds may yield a
better understanding of the possible beneﬁts and limitations of the proposed
methods. ",cs.CV,A,-0.078359224,0.32536775,0.032072484
http://arxiv.org/pdf/2201.07021v1,MuSCLe: A Multi-Strategy Contrastive Learning Framework for Weakly Supervised Semantic Segmentation,"Notably, this is achieved on a single
                                                                     GPU, unlike most existing work in the area. In future work,
   We show some representative qualitative segmentation              we will investigate extending our model to multi-GPU set-
results obtained from MuSCLe-b7 in Figure 6, from which              tings to further boost the performance. we can observe that detailed object boundaries are prop-
erly recovered (columns 1-2). ",cs.CV,B,-0.016544389,-0.11426561,-0.115818225
http://arxiv.org/pdf/2201.07070v2,Attention-based Proposals Refinement for 3D Object Detection,"KITTI and NuScenes dataset validate the effectiveness of
our method. As for future work, we would like to extend the                   [17] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
RFE module to enable fusion of LiDAR with other sensing                             driving? the kitti vision benchmark suite,” in Conference on Computer
modalities such as cameras or radars. ",cs.CV,B,0.2608267,-0.22080028,-0.17109439
http://arxiv.org/pdf/2201.07070v3,Attention-based Proposals Refinement for 3D Object Detection,"Experiments on
                               84.66 75.34  56.15                 KITTI and NuScenes datasets validate the effectiveness of
                                                                  our method. 84.85 73.35  57.41
                                                                     As for future work, we would like to extend the RFE
   Finally, the versatility of our method is demonstrated         module to enable the fusion of LiDAR with other sensing
by its integration with different RPNs: SECOND, PartA2,           modalities such as cameras or radars. Another possibility is
PointRCNN. ",cs.CV,B,0.3273567,-0.16001847,-0.19667706
http://arxiv.org/pdf/2201.07106v1,Variational Inference for Quantifying Inter-observer Variability in Segmentation of Anatomical Structures,"The quantiﬁed inter-observer variability, i.e.,
the averaged segmentation, aligns well with the manual average, when evaluated on the QUBIQ datasbase,
which indicates that the aleatoric uncertainty can be accurately measured. In this work, we only explored the
brain growth task using the QUBIQ challenge database, while the method can be easily generalized for any
inter-observer variability quantiﬁcation tasks, which is subject to our future work. Figure 3. ",cs.CV,A,0.23418891,0.06350839,0.05224929
http://arxiv.org/pdf/2201.07131v1,Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection,"indicate that our proposed cross-modal task strongly bene-
                                                                  ﬁts generalisation. We leave for future work the investiga-
Visual-only representation learning. Although it is natu-         tion of more effective video augmentations that could fur-
ral to use the correspondence between the visual and audi-        ther improve the visual-only baseline. ",cs.CV,C,-0.20971055,-0.14044636,0.19993515
http://arxiv.org/pdf/2201.07189v1,MUSE-VAE: Multi-Scale VAE for Environment-Aware Long Term Trajectory Prediction,"MUSE-VAE processes each agent independently,
which cannot reﬂect agent-interaction. In the future work,
we will take into consideration of multi agent-aware model
that can avoid collisions with neighboring agents. Supplementary Materials for                              colored) area that nuScenes dataset provides. ",cs.CV,A,0.15640652,0.07991661,0.07038842
http://arxiv.org/pdf/2201.07264v1,Exploring Kervolutional Neural Networks,"VII. ANALYSIS AND DISCUSSION
   This section contains further analysis on the results shown
from section VI along with additional theoretical analysis of
KNNs absent from the original KNN paper. A. ",cs.CV,A,0.36033246,0.26531738,-0.07490057
http://arxiv.org/pdf/2201.07309v1,OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation,"observation. This leads to slower inference speed on average     However, such methods require either large-scale manual
but better performance, as we can see in Table I.                data annotation or synthetic data generation to work on
                                                                 new objects, with long data generation and training times,
   In addition, we further study the potential negative effects  whereas our method can quickly adapt online to new objects. of low-accuracy pose estimates in the pseudo ground truth        Further, our method can train directly on real data in a
used in the self-supervised training process, as these bad       self-supervised way, without requiring manual annotations
pose estimates may mislead the detector. ",cs.CV,C,-0.1848576,-0.06754423,-0.022818051
http://arxiv.org/pdf/2201.07309v2,OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation,"observation. This leads to slower inference speed on average     However, such methods require either large-scale manual
but better performance, as we can see in Table I.                data annotation or synthetic data generation to work on
                                                                 new objects, with long data generation and training times,
   In addition, we further study the potential negative effects  whereas our method can quickly adapt online to new objects. of low-accuracy pose estimates in the pseudo ground truth        Further, our method can train directly on real data in a
used in the self-supervised training process, as these bad       self-supervised way, without requiring manual annotations
pose estimates may mislead the detector. ",cs.CV,C,-0.1848576,-0.06754423,-0.022818051
http://arxiv.org/pdf/2201.07422v1,Self-Supervised Deep Blind Video Super-Resolution,"gap between the estimated blur kernel and the ground-truth
Moreover, using the proposed self-supervised approach to            blur kernel. Although the proposed method only focuses on
ﬁne-tune our method with degraded videos is able to gen-            the accuracy of the generated auxiliary paired data but not
erate better results on these real scenarios (see “w/ ﬁne-          the estimated blur kernels, future work will study how to
tuning” in Table 8). Figure 12 shows some visual results,           estimate more accurate blur kernels so that the performance
where our method with ﬁne-tuning using real degraded                of the proposed self-supervised method can be further im-
videos generates better results with ﬁner structures (see           proved. ",cs.CV,B,-0.039617825,-0.18597752,-0.0881826
http://arxiv.org/pdf/2201.07428v1,Variable Augmented Network for Invertible MR Coil Compression,"Section V concludes with topics and
as in GCC, more virtual coils are often demanded in SCC. future works are given. Following the success of deep learning in a wide range of                   II. ",cs.CV,A,-0.051037773,0.19778696,-0.14153694
http://arxiv.org/pdf/2201.07428v2,Variable Augmented Network for Invertible MR Coil Compression,"critical role in conducting the major experiments. Section V
[18] raised an innovation of correlation with coil sensitivity    concludes with topics and gives the future work. The work
information, in which the optimal subset of receiving coil        opens a brand new opportunity for invertible MR coil com-
channels is determined by the contribution of each coil           pression without overheads seen in clinical practice. ",cs.CV,A,0.36945254,0.21137612,0.08270472
http://arxiv.org/pdf/2201.07451v1,TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning,"Second, we do not pro-
pose new fusion rules for diﬀerent fusion tasks and more eﬀective task-speciﬁc
fusion rules can be specially designed in the future to further improve the fusion
performance. 35
    In future work, more eﬀective Transformer-based feature extraction meth-
ods can be explored for image fusion tasks. Moreover, since both CNN-based
architectures and Transformer-based architectures have their advantages, how
to further combine the two architectures is another promising research direc-
tion. ",cs.CV,C,-0.08109547,-0.14948422,-0.12485187
http://arxiv.org/pdf/2201.07495v1,Weakly Supervised Semantic Segmentation of Remote Sensing Images for Tree Species Classification Based on Explanation Methods,"The theoretical and experimental analysis show that
                                                                   for tree species segmentation problems, SEM can be chosen
                                                                   as it: i) yields the highest segmentation accuracy; ii) provides
                                                                   the lowest model complexity; and iii) requires a low segmen-
                                                                   tation time. As a future work, we plan to assess the ability
    Image Reference Map CAM [3] GradCAM [4] PCM [5]                                          SEM [6]

a)

b)

c)

    Pine  Spruce                                                        Beech  Oak  Cleared

Fig. 1. ",cs.CV,B,0.09047002,-0.10331024,-0.062440142
http://arxiv.org/pdf/2201.07572v1,Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation,"The practical
annotation eﬀort depends on a combination of boundary accuracy and the number of
superpixels, but the importance of each metric can not be quantiﬁed yet. These aspects
will be further explored in future work to evaluate the downstream utility for reducing
annotation time with the proposed methods. (a) Ground truth                 (b) SLIC   (c) Adjusted SLIC

                  (d) ResNet-50             (e) Autoencoder

Fig. ",cs.CV,B,-0.04014793,-0.11383217,-0.12488526
http://arxiv.org/pdf/2201.07594v1,Real-time Recognition of Yoga Poses using computer Vision for Smart Health Care,"It is required
to focus on the identification of the postures in other yoga-like practices such as ballet, climbing, Tai-Chi. In our
future work, it is required to consider the multi-users in single frame scenarios such as classrooms and practice
halls. To improve the wearable system, there is a need to integrate features like blood pressure monitoring and body
temperature through other wearable devices, which can be connected to an existing application program. ",cs.CV,A,0.21052262,-0.00570946,0.3207559
http://arxiv.org/pdf/2201.07609v1,A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo,"take the minimum conﬁdence over all reference frames to              Acknowledgements: This work was partially supported by
get a “strict” depth conﬁdence, adding views will only re-           the Natural Science Foundation of China (61725204), BN-
sult in slight performance gain. More advanced conﬁdence             Rist and Tsinghua University (CS Dept) - DeepBlue Tech-
based on multi-view depth fusion is left for future work. nology (Shanghai) Company Limited Joint Research Center
                                                                     for Machine Vision (JCMV). ",cs.CV,B,0.06247967,-0.28843012,-0.04660649
http://arxiv.org/pdf/2201.07619v1,CAST: Character labeling in Animation using Self-supervision by Tracking,"Note
that the three datasets do not contain any overlap i.e., no character, episode or series appears in more than one dataset. We plan to publish a labeled dataset for future work comparison. We also used the SAIL AMCD dataset [31] to evaluate CAST for unsupervised discovery of character dictionaries
and compare our results to [31]. ",cs.CV,C,-0.091481,0.1749331,0.19787517
http://arxiv.org/pdf/2201.07661v1,Open Source Handwritten Text Recognition on Medieval Manuscripts using Mixed Models and Document-Specific Finetuning,"To foster this spirit of open collaboration we made our own pretrained models
openly available. Regarding future work, we want to utilize the intrinsic conﬁdence values of
the ATR engine. For example, these conﬁdence values could be used to identify
individual lines the existing model struggled with the most and then transcribe
these lines in a targeted way to maximize the training eﬀect in an active learning-
like approach. ",cs.CV,A,-0.08738301,0.234794,0.09318542
http://arxiv.org/pdf/2201.07692v1,GroupGazer: A Tool to Compute the Gaze per Participant in Groups with integrated Calibration to Map the Gaze Online to a Screen or Beamer Projection,"Based on the gaze signal it is possible to extract further information like the eye movement types [44, 55, 21, 22, 31, 34]. In the ﬁeld of human-machine interaction [20], the gaze signal is used and further researched for interaction with
                                      robots [112] but also other technical devices [111, 18, 23, 45]. This involves not only simple control but also collaboration
                                      in which a human communicates complex behavior to a robot or system [95]. ",cs.CV,C,0.17737216,-0.118849866,0.40067723
http://arxiv.org/pdf/2201.07703v1,Q-ViT: Fully Differentiable Quantization for Vision Transformer,"these observations, we conclude that it is a must to design a            This analysis shows the key aspect for ViT quantiza-
special mixed-precision QAT for low-bit ViT quantization. tion and provide some insights for the future work. In this paper, we propose a fully differentiable quan-             • Based on this analysis, we propose Q-ViT, a fully dif-
tization for vision transformer, named as Q-ViT. ",cs.CV,B,0.21587665,-0.14461009,-0.16026014
http://arxiv.org/pdf/2201.07703v2,Q-ViT: Fully Differentiable Quantization for Vision Transformer,"these observations, we conclude that it is a must to design a            This analysis shows the key aspect for ViT quantiza-
special mixed-precision QAT for low-bit ViT quantization. tion and provide some insights for the future work. In this paper, we propose a fully differentiable quan-             • Based on this analysis, we propose Q-ViT, a fully dif-
tization for vision transformer, named as Q-ViT. ",cs.CV,B,0.21587665,-0.14461009,-0.16026014
http://arxiv.org/pdf/2201.07734v1,Towards holistic scene understanding: Semantic segmentation and beyond,"For both CPP and PPP, part-level classes are only deﬁned for scene-
level things classes. In future work, we anticipate designers of datasets also
opt for deﬁning part classes for stuﬀ classes. If so, this is fully compatible with
our task deﬁnition and metric, since the framework already supports this. ",cs.CV,C,-0.04362799,0.058592018,0.09608424
http://arxiv.org/pdf/2201.07781v1,Towards a General Deep Feature Extractor for Facial Expression Recognition,"1499-1503, 2016.
to labeled data is closer to one to one. We leave determining
whether larger volumes of unlabeled data can further improve         [11] S. Li, W. Deng, “Reliable Crowdsourcing and Deep Locality-Preserving
DeepFEVER’s performance to future work. Learning for Unconstrained Facial Expression Recognition,” IEEE
                                                                           Trans. ",cs.CV,C,-0.29964507,0.07424307,0.11918579
http://arxiv.org/pdf/2201.07786v1,Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation,"Since audio feature is not input              ally [21]. We will address these issues in future work. to the deformation implicit function, it tends to warp the
weakly audio-related torso part, while the strongly audio-             6. ",cs.CV,A,0.23140772,0.106808,0.13823377
http://arxiv.org/pdf/2201.07894v1,Enhanced Performance of Pre-Trained Networks by Matched Augmentation Distributions,"While the experiments herein
8). To this end, even approaches like Meal [5] or Meal-V2 [4]                   have shown that MID using just random crops improves
that do not rely on extra data beyond ImageNet, still require                   performance, future work should examine full distributional
performing inference of 1.2 million training images through                     matching including other augmentations used in training of
several teacher networks to get the softmax soft labels. A                      the pre-trained networks e.g., color-jittering. ",cs.CV,C,-0.30248442,0.032239772,-0.1075701
http://arxiv.org/pdf/2201.07906v1,The Role of Facial Expressions and Emotion in ASL,"tionality, and cognition. For this project, we use emotional features
                                                                        only, although future work might explore the role of semantics
   Multi-label Classification of Emotions. In the second phase, we      more generally. ",cs.CV,C,-0.11718118,0.1270918,0.4305341
http://arxiv.org/pdf/2201.07927v1,Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation,"Single black background tends to                bution in the source dataset. It is an important future work
overﬁt, and it is effectively alleviated by adding random col-          to explore learning-by-synthesis approaches to cover differ-
ors and random scenes. We keep 40% color background                     ent data diversity requirements. ",cs.CV,C,-0.19907826,-0.053623397,-0.037269585
http://arxiv.org/pdf/2201.07927v2,Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation,"Single black background tends to                bution in the source dataset. It is an important future work
overﬁt, and it is effectively alleviated by adding random col-          to explore learning-by-synthesis approaches to cover differ-
ors and random scenes. We keep 40% color background                     ent data diversity requirements. ",cs.CV,C,-0.19907826,-0.053623397,-0.037269585
http://arxiv.org/pdf/2201.07927v3,Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation,"Overall, these domain adap-             in the source dataset. It is important future work to explore
tation methods cannot consistently outperform the baseline          learning-by-synthesis approaches to cover different data di-
model (fourth row). In contrast, our mask-guided model              versity requirements. ",cs.CV,C,-0.12265771,0.16000213,0.013413843
http://arxiv.org/pdf/2201.07929v1,Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision,"In order to demonstrate the effectiveness of                     forms all of the state-of-the-art methods both qualitatively
both modules, we removed the domain classiﬁer Λ in our                         and quantitatively and our method also works well under
                                                                               severe occlusion. As future work, we would like to develop
                                                                               a video-based method for estimating temporally-consistent
                                                                               egocentric poses from an in-the-wild video. Limitations. ",cs.CV,B,0.052054405,-0.36277875,0.23124093
http://arxiv.org/pdf/2201.07931v1,Experimental Large-Scale Jet Flames' Geometrical Features Extraction for Risk Management Using Infrared Images and Deep Learning Segmentation Methods,"This would allow for a more thorough risk analysis of jet ﬁres that evaluates the likelihood
of direct ﬂame impingement and determines the distribution and intensity of radiant heat that is emitted
from the ﬂame to the surrounding equipment. As future work, other improved versions of the Deep Learning architectures could be tested and diﬀerent
real-time algorithms could be explored to further improve the eﬃciency and accuracy of the segmentation. Additionally, the application of edge computing, smart cameras, or 8-bit quantization, could be introduced
to use fewer bits for calculation and storage, which would accelerate the processing time of these algorithms. ",cs.CV,B,-0.042971853,-0.07776112,-0.26764053
http://arxiv.org/pdf/2201.07937v1,GASCN: Graph Attention Shape Completion Network,"In
5.3. Point Cloud Registration                                   our future work, we will investigate combining shape com-
                                                                pletion with grasp pose detection [9] in robotics. Many tasks in robotics could beneﬁt from dense com-
plete point clouds. ",cs.CV,B,0.1972714,-0.278793,-0.03458564
http://arxiv.org/pdf/2201.08027v1,A Joint Morphological Profiles and Patch Tensor Change Detection for Hyperspectral Imagery,"57, no. 12,
which is also the focus of our future work. pp. ",cs.CV,A,0.35413593,0.19976752,0.18974017
http://arxiv.org/pdf/2201.08049v1,Lightweight Salient Object Detection in Optical Remote Sensing Images via Feature Correlation,"as compared with most CNN-based methods, it is still difﬁcult                          1461–1475, Jan. 2021.
to run on the CPU in real time. [9] G. Li, Z. Liu, and H. Ling, “ICNet: Information conversion network for
   Therefore, in future works, we will work in the following                           RGB-D based salient object detection,” IEEE Trans. Image Process.,
two directions: 1) similar to the lightweight ORSI-SOD meth-                           vol. ",cs.CV,C,-0.12768325,-0.2378929,-0.18793431
http://arxiv.org/pdf/2201.08071v1,The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions,"6%                                               Section 5 summarizes current research progress via performance
                                                                                                            comparisons. Section 6 discusses open issues and further research
                                                                ECCV        ICCV                            directions. Section 7 concludes this paper. ",cs.CV,A,0.28215355,0.038475025,-0.027649932
http://arxiv.org/pdf/2201.08122v1,A Computational Model for Machine Thinking,"5
3. Conclusion and future works

    In this paper we propose a new model for machine thinking. This model is
based on three units: The needs, Perceived scene, Imagined scene. ",cs.CV,C,-0.01790554,-0.042404722,0.32336688
http://arxiv.org/pdf/2201.08125v1,Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing,"As an        trained on a different domain (e.g., CV images). To address
example, 3rd retrieved caption by JDSH failed to estimate the      this problem, as a future work, we plan to apply end-to-end
number of airplanes, while the proposed DUCH consistently          training of the feature extraction module by ﬁne-tuning the
demonstrated the quantity correctly. The qualitative cross-        modality-speciﬁc encoders. ",cs.CV,C,-0.27998644,-0.03134119,-0.0002349224
http://arxiv.org/pdf/2201.08131v1,GeoFill: Reference-Based Image Inpainting of Scenes with Complex Geometry,"In the last
case, dynamic objects, e.g., pedestrians, make our optimiza-
tion module estimate inaccurate parameters. We discuss in
the last section of our main paper ways that future work
might address these issues. Additional Visual Results

   We include additional qualitative comparisons of Ge-
oFill against other baselines in Figure 10. ",cs.CV,A,0.25367743,0.03138279,0.02198631
http://arxiv.org/pdf/2201.08131v2,GeoFill: Reference-Based Image Inpainting with Better Geometric Understanding,"In the last case, dynamic ob-
jects, e.g., pedestrians, make our optimization module esti-
mate inaccurate parameters. We discuss in the last section
of our main paper ways that future work might address these
issues. Additional Visual Results

   We include additional qualitative comparisons of Ge-
oFill against other baselines in Fig. ",cs.CV,A,0.29262865,0.08516572,0.06473593
http://arxiv.org/pdf/2201.08217v1,Watermarking Pre-trained Encoders in Contrastive Learning,"The noise brought by the triggered samples in the
training dataset will not harm the model ACC which proves
the robustness of our method. We have the following directions as future work. (1) We
aim to develop an algorithm to optimize our trigger pattern,
to reduce the false positives of the watermark veriﬁcation. ",cs.CV,A,0.054415293,0.1869227,-0.05283332
http://arxiv.org/pdf/2201.08371v1,Revisiting Weakly Supervised Pre-Training of Visual Perception Models,"Overall, our results suggest
more work is needed to train models that perform equally            To conclude, we emphasize that we remain convinced
across the world. In future work, we plan to train multi-        about the potential of weakly-supervised learning ap-
lingual hashtag models [64] as this may lead to models that      proaches. If we resolve the aforementioned issues, we be-
achieve equal recognition accuracies across countries. ",cs.CV,C,-0.19102597,0.2556802,0.28966668
http://arxiv.org/pdf/2201.08371v2,Revisiting Weakly Supervised Pre-Training of Visual Perception Models,"In order to make hashtag-prediction systems
across the world. In future work, we plan to train multi-                     like ours ready for real-world deployment, it is essential that
lingual hashtag models [64] as this may lead to models that                   we improve the quality of our analyses, and that we address
achieve equal recognition accuracies across countries. any issues that those analyses may surface. ",cs.CV,A,-0.12054406,0.31684405,0.30077493
http://arxiv.org/pdf/2201.08377v1,Omnivore: A Single Model for Many Visual Modalities,"We leave such exten-
(RGBT) using its shared representation space. sions to future work. Bridging frame-based and clip-based video models. ",cs.CV,B,0.07022756,-0.2801968,-0.003644459
http://arxiv.org/pdf/2201.08377v2,Omnivore: A Single Model for Many Visual Modalities,"The results illus-   trained using only classification; using structured prediction
trate how OMNIVORE supports retrieval of visual concepts         tasks such as segmentation might yield richer representa-
across images (RGB), single-view 3D (RGBD), and videos           tions. We leave such extensions to future work. (RGBT) using its shared representation space. ",cs.CV,C,-0.29191035,-0.13374382,0.07463811
http://arxiv.org/pdf/2201.08550v1,What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review,"In [179], features are obtained from histology images through techniques in-
cluding multidimensional fractal models and deﬁning convolutional descriptors
with diﬀerent CNN models. The method shows that the feature association
extracted from diﬀerent methods is a potential ﬁeld for further research. 8.4.3 Potential Methods of Classiﬁcation for LHIA

In the summarized papers, in addition to the commonly used classiﬁcation
methods, there are some other potential methods. ",cs.CV,C,-0.09215309,-0.09263505,-0.055772115
http://arxiv.org/pdf/2201.08550v2,What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review,"In [179], features are obtained from histology images through techniques in-
cluding multidimensional fractal models and deﬁning convolutional descriptors
with diﬀerent CNN models. The method shows that the feature association
extracted from diﬀerent methods is a potential ﬁeld for further research. 8.4.3 Potential Methods of Classiﬁcation for LHIA

In the summarized papers, in addition to the commonly used classiﬁcation
methods, there are some other potential methods. ",cs.CV,C,-0.09215309,-0.09263505,-0.055772115
http://arxiv.org/pdf/2201.08619v1,Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World,"9, which do conﬁrm this tendency. interesting future work. Therefore, we can conclude that tuning poisonous data points
                                                                                                   with few images can compensate the ASR in some uncommon
   3) Distributed Machine Learning: Distributed machine                                            settings to make the cloaking attack robust in more diverse and
learning, represented by the federated learning, can greatly                                       complicated real-world scenarios. ",cs.CV,C,-0.07851729,0.16767901,-0.13565823
http://arxiv.org/pdf/2201.08619v2,Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World,"Achieving this poisoning                Video No. 16   17 Average
effect in object detection annotation could be possible and is                  ASR(%) 3.42  5.10 14.26
interesting future work. D. Countermeasures
   3) Distributed Machine Learning: Distributed machine
learning, represented by the federated learning, can greatly                      As this is the ﬁrst work that investigates the object detector’s
reduce systematic privacy leakage of user data without directly                susceptibility to backdoor attacks, there is so far no existing
accessing them but the trained local model parameters. ",cs.CV,C,-0.23685251,0.049992643,-0.023213059
http://arxiv.org/pdf/2201.08663v1,Fast Differentiable Matrix Square Root,"We conjecture this might introduce
some instabilities to the training process. Despite the analysis, developing an effective remedy is a
direction of our future work. A.8 COMPUTATION ANALYSIS ON MATRIX INVERSE VERSUS SOLVING LINEAR SYSTEM

Suppose we want to compute C=A−1B. ",cs.CV,A,0.041527264,0.27868518,-0.12601547
http://arxiv.org/pdf/2201.08779v1,Contrastive and Selective Hidden Embeddings for Medical Image Segmentation,"Noticeably, the model still           ers for the applicability of the receptive ﬁeld concept, and
performs on par with the whole-set baseline with as little as       we leave the extension of this limitation, for example, with
20% of overall training data. This property reveals the great       a generalization of the receptive ﬁeld concept for decoder
potential of proposed methods in handling other especially          layers, to future works. Fig. ",cs.CV,C,-0.21115044,0.03200897,0.0571121
http://arxiv.org/pdf/2201.08779v2,Contrastive and Selective Hidden Embeddings for Medical Image Segmentation,"Noticeably, the model still           ers for the applicability of the receptive ﬁeld concept, and
performs on par with the whole-set baseline with as little as       we leave the extension of this limitation, for example, with
20% of overall training data. This property reveals the great       a generalization of the receptive ﬁeld concept for decoder
potential of proposed methods in handling other especially          layers, to future works. Fig. ",cs.CV,C,-0.21115044,0.03200897,0.0571121
http://arxiv.org/pdf/2201.08815v1,Learning from One and Only One Shot,"We will not be using black-box models, but
maintain model interpretability by modeling “the direct human way”, where we learn (from a small
training set) a particular function to be integrated into our distortion formulas. We will introduce
such functions in future work and continue to improve them. Figure 8 summarizes the pipeline for
generalizing our current distortable canvas model into the future directions sketched above. ",cs.CV,B,0.07529054,-0.01491357,0.10934547
http://arxiv.org/pdf/2201.08893v1,Signal Strength and Noise Drive Feature Preference in CNN Image Classifiers,"We also consider the impact that these experiments might have on the comparisons that have been
made between machine and human vision. When tasks and datasets are carefully controlled for Signal
and Noise, we expect that feature preferences of machines and humans move closer in alignment,
but this must be tested experimentally, and should be explored in future work. Future work should
also test for the extensibility of these results across other tasks (including unsupervised objectives),
datasets, data augmentation strategies, and architectures. ",cs.CV,C,-0.11458561,-0.09144834,0.122338355
http://arxiv.org/pdf/2201.08949v1,Temporal Aggregation for Adaptive RGBT Tracking,"PROPOSED METHOD
greater than 0 into account. After further analysis, it is proved
in our method as shown in Fig. 7 that negative features can           In this section, we will ﬁrstly give a brief description of the
still contribute to the ﬁnal performance. ",cs.CV,A,0.35714692,0.27584416,-0.16306551
http://arxiv.org/pdf/2201.08949v2,Temporal Aggregation for Adaptive RGBT Tracking,"PROPOSED METHOD
greater than 0 into account. After further analysis, it is proved
in our method as shown in Fig. 7 that negative features can           In this section, we will ﬁrstly give a brief description of the
still contribute to the ﬁnal performance. ",cs.CV,A,0.35714692,0.27584416,-0.16306551
http://arxiv.org/pdf/2201.08962v1,Collaborative Representation for SPD Matrices with Application to Image-Set Classification,"Log CRC
                                                                     Experimental results obtained on four different benchmark-
D. Ablation Study for Each Component of the Proposed              ing datasets show that the proposed approach could make a
Framework                                                         considerable improvement in classiﬁcation performance and
                                                                  computational efﬁciency compared to some representative
   Although the effectiveness of the proposed approach has        image set classiﬁcation methods. For future work, we plan to
been demonstrated, to validate the contribution of each com-      integrate dictionary learning into the proposed framework for
ponent of the proposed framework is also highly appealing. further improving the discriminability of the learned geometric
To this end, we make a comparison between LogEK CRC,              features. ",cs.CV,C,-0.11000425,0.07860491,-0.019650146
http://arxiv.org/pdf/2201.08977v1,Semi-Supervised Adversarial Recognition of Refined Window Structures for Inverse Procedural Façade Modeling,"Discussion and limitations
    Based on the evaluations above, we now return to the issues raised at the beginning of

this paper. In addition, we also discuss the limitations of the current strategy that could be
surmounted in future works. 2https://vrlab.org.cn/~hanhu/projects/windows

                                                             20
LOD-2  ResNet+GAN  ResNet+GAN+GI

       Figure 15: Reconstruction results for the Bank dataset. ",cs.CV,B,-0.029347375,-0.037649732,-0.20270118
http://arxiv.org/pdf/2201.08977v2,Semi-Supervised Adversarial Recognition of Refined Window Structures for Inverse Procedural Façade Modeling,"Discussion and limitations

    Based on the evaluations above, we now return to the issues raised at the beginning of
this paper. In addition, we also discuss the limitations of the current strategy that could be
surmounted in future works. 1) Breaking the domain gap in pre-training with few labeled data. ",cs.CV,A,-0.28197756,0.39929223,0.018920483
http://arxiv.org/pdf/2201.09042v1,Uncertainty-aware deep learning methods for robust diabetic retinopathy classification,"In addition to the country distribution shift, our aim is to examine
the within country hospital region distribution shift using this data, in addition to the KSSHP
dataset. Additionally, our future work includes utilization of some of the computationally more
intensive solutions for robust deep learning. Moreover, we will be examining the impact of joint
training from the diﬀerent regions on performance and robustness. ",cs.CV,C,-0.22336748,0.12428824,-0.14213568
http://arxiv.org/pdf/2201.09042v2,Uncertainty-aware deep learning methods for robust diabetic retinopathy classification,"In addition to the country distribution shift, our aim is to examine
the within country hospital region distribution shift using this data, in addition to the KSSHP
dataset. Additionally, our future work includes utilization of some of the computationally more
intensive solutions for robust deep learning. Moreover, we will be examining the impact of joint
training from the diﬀerent regions on performance and robustness. ",cs.CV,C,-0.22336748,0.12428824,-0.14213568
http://arxiv.org/pdf/2201.09049v1,LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN,"902–911. other streaming protocols as future work. [16] P. Varini, G. Serra, and R. Cucchiara, “Personalized egocentric
REFERENCES                                                                        video summarization of cultural tour on user preferences input,”
                                                                                  IEEE Transactions on Multimedia, vol. ",cs.CV,C,-0.005113667,-0.08126548,0.38955963
http://arxiv.org/pdf/2201.09049v2,LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN,"902–911. other streaming protocols as future work. [16] P. Varini, G. Serra, and R. Cucchiara, “Personalized egocentric
REFERENCES                                                                        video summarization of cultural tour on user preferences input,”
                                                                                  IEEE Transactions on Multimedia, vol. ",cs.CV,C,-0.005113667,-0.08126548,0.38955963
http://arxiv.org/pdf/2201.09109v1,Robust Unpaired Single Image Super-Resolution of Faces,"Red Hook, NY, USA: Curran Associates Inc.
SISR network that is more robust than the previous state-        ISBN 9781510860964.
of-the-art networks. In our future works, we intend to study
the loss surfaces of such networks in more depth and see         LeCun, Y.; and Cortes, C. 2010. MNIST handwritten digit
whether there is a way to optimize our method even further. ",cs.CV,A,-0.0026511839,0.18853632,-0.29904142
http://arxiv.org/pdf/2201.09153v1,An Integrated Approach for Video Captioning and Applications,"Even if the generated caption for the video is not
accurate, we can enhance it further by using crowdsourcing. We hope that our
ideas can be reused and help inspire future work. The remaining challenges are best illustrated with an example. ",cs.CV,C,-0.10871965,-0.1096461,0.35979366
http://arxiv.org/pdf/2201.09205v1,Deeply Explain CNN via Hierarchical Decomposition,"The issue of removing individual channels will
[69] and ImageNet [68]. be our future work. native degree D to measure the discriminative information of a                   6 CONCLUSION

channel. ",cs.CV,C,-0.07724549,0.09813963,0.0061271936
http://arxiv.org/pdf/2201.09208v1,Design of Sensor Fusion Driver Assistance System for Active Pedestrian Safety,"environments. [11] C. Premebida, G. Monteiro, U. Nunes, and P.
Our future work will improve on the limitations of the          Peixoto, “A lidar and vision-based approach for
proposed sensor fusion concept. For example, by                 pedestrian and vehicle detection and tracking”
adding Bayes’ theorem to determine the results of the           IEEE Intelligent Transportation Systems Conference,
optical flow, the number of false positives may be              2007, pp. ",cs.CV,B,0.20031577,-0.30777338,-0.009625812
http://arxiv.org/pdf/2201.09246v1,Face recognition via compact second order image gradient orientations,"By employing CRC as the ﬁnal classiﬁer, our proposed method
achieves impressive results in various scenarios and even outperforms some deep neural network
based approaches. In future work, we will introduce SOIGO into other popular subspace learning approaches,
e.g., linear discriminant analysis (LDA), to extract more discriminative features. Moreover, other
variants of CRC will also be investigated to further enhance the performance of recognition. ",cs.CV,C,-0.2787701,0.057478182,-0.07950075
http://arxiv.org/pdf/2201.09246v2,Face recognition via compact second order image gradient orientations,"By employing CRC as the ﬁnal classiﬁer, our proposed method
achieves impressive results in various scenarios and even outperforms some deep neural network
based approaches. In future work, we will introduce SOIGO into other popular subspace learning approaches,
e.g., linear discriminant analysis (LDA), to extract more discriminative features. Moreover, other
variants of CRC will also be investigated to further enhance the performance of recognition. ",cs.CV,C,-0.2787701,0.057478182,-0.07950075
http://arxiv.org/pdf/2201.09271v1,Wavelet-Attention CNN for Image Classification,"The
eﬀectiveness of our method is proved by a series of experiments on CIFAR-
10 and CIFAR-100. In future work, we will devote more attention to the WA
block structure and explore the applications in large-scale datasets. Acknowledgments. ",cs.CV,A,0.15379255,0.22710016,-0.14676006
http://arxiv.org/pdf/2201.09286v1,How to scale hyperparameters for quickshift image segmentation,"Left:           provided that ks is large enough with respect to the
segmentation of a downsized image (by a factor ρ “ 2),        color diﬀerence. quickshift ﬁnds 20 superpixels; middle: segmentation
of the original image with the same hyperparameters           As future work, our main focus is to tackle the original
(87 superpixels); right: segmentation of the original         version of the algorithm, in order to capture better the
image with rescaled hyperparameters (ˆ2), quickshift          behavior for small dm. The main diﬃculty in doing so
ﬁnds 19 superpixels, approximately the same number            seems to be the extension of Lemma 3.2, which seems
as in the downsized image. ",cs.CV,B,0.22917047,-0.12631992,-0.28749177
http://arxiv.org/pdf/2201.09286v2,How to scale hyperparameters for quickshift image segmentation,"Figure 9: Scaling quickshift hyperparameters. Left:                        As future work, our main focus is to tackle the original
segmentation of a downsized image (by a factor ρ “ 2)                      version of the algorithm, in order to capture better the
with ks “ 5 and dm “ `8, quickshift ﬁnds 20 su-                            behavior for small dm. The main diﬃculty in doing so
perpixels; middle: segmentation of the original image                      seems to be the extension of Lemma 3.2, which seems
with the same hyperparameters (87 superpixels); right:                     challenging even considering i.i.d. ",cs.CV,B,0.19254373,-0.08961994,-0.24821529
http://arxiv.org/pdf/2201.09354v1,Survey and Systematization of 3D Object Detection Models and Methods,"Likewise, our systematization aims         face of the scene object. RGB image and depth image hold
at being suﬃciently robust to allow a classiﬁcation of all ex-       ideally a one-to-one correspondence between pixels (Wang and
isting 3DOD pipelines and methods as well as of future works         Ye, 2020). without the need of major adjustments to the general frame-
work. ",cs.CV,B_centroid,0.089657694,-0.352274,-0.016309123
http://arxiv.org/pdf/2201.09381v1,vCLIMB: A Novel Video Class Incremental Learning Benchmark,"We think that explor-
down-sampled videos in the episodic memory. However,              ing non-uniform sampling strategies is another promising
without applying the consistency loss, the model naturally        direction, but we leave that exploration for future work. 8
References                                                             [15] Diederik P. Kingma and Jimmy Ba. ",cs.CV,B,-0.048947178,-0.024421422,0.08332783
http://arxiv.org/pdf/2201.09381v2,vCLIMB: A Novel Video Class Incremental Learning Benchmark,"CIL untrimmed video classification. We think that explor-
When training the model on augmented examples, we ex-          ing non-uniform sampling strategies is another promising
pect the model to learn more robust representations that can   direction, but we leave that exploration for future work. make learning new tasks easier. ",cs.CV,C,-0.29438224,-0.0908184,0.062211398
http://arxiv.org/pdf/2201.09390v1,AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks,"At character level, the proposed method performed comparable
with the state-of-the-art methods and achieved 6.50% test set CER. However,
the character level error can be further reduced by using data augmentation,
language modeling, and a diﬀerent regularization method, which will be inves-
tigated as future work. Our source code and pre-trained models are publicly
available for further ﬁne-tuning or predictions on unseen data at GitHub5. ",cs.CV,A_centroid,-0.060052767,0.38977656,0.03574813
http://arxiv.org/pdf/2201.09390v2,AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks,"At character level, the proposed method performed comparable
with the state-of-the-art methods and achieved 6.50% test set CER. However,
the character level error can be further reduced by using data augmentation,
language modeling, and a diﬀerent regularization method, which will be inves-
tigated as future work. Our source code and pre-trained models are publicly
available for further ﬁne-tuning or predictions on unseen data at GitHub5. ",cs.CV,A,-0.060052767,0.38977656,0.03574813
http://arxiv.org/pdf/2201.09390v3,AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks,"At character level, the proposed method performed comparable
with the state-of-the-art methods and achieved 6.50% test set CER. However,
the character level error can be further reduced by using data augmentation,
language modeling, and a diﬀerent regularization method, which will be inves-
tigated as future work. Our source code and pre-trained models are publicly
available for further ﬁne-tuning or predictions on unseen data at GitHub5. ",cs.CV,A,-0.060052767,0.38977656,0.03574813
http://arxiv.org/pdf/2201.09548v1,Consistent 3D Hand Reconstruction in Video via self-supervised Learning,"Adjusting the        accuracy by further adding T&S consistency loss. However,
conﬁguration of quaternion loss dynamically according to        the enhancement is insigniﬁcant, speciﬁcally only leading to
the input and the output can be a promising direction for       an increase of 0.001 in AUCJ, AUCV and a decrease of 0.001
future work. in shape standard deviation. ",cs.CV,A,0.3205006,0.059864882,-0.15618576
http://arxiv.org/pdf/2201.09594v1,Describe me if you can! Characterized Instance-level Human Parsing,"So we would
each approach family that have available models. Inference              like to address this class imbalance issue as future work to im-
time is averaged after 50 runs on a Titan X GPU, using CIHP             prove HPTR performance. Consistency between characteris-
images containing from 2 to 18 people. ",cs.CV,A,0.114549994,0.13910669,-0.03580996
http://arxiv.org/pdf/2201.09613v1,SEN12MS-CR-TS: A Remote Sensing Data Set for Multi-modal Multi-temporal Cloud Removal,"Sections IV-C and IV-D                                    angle between the bands of two multi-channel images [39]. For
detail the experiments and outcomes for the sequence-to-point                                   further analysis, the pixelwise NRMSE is evaluated in three
and sequence-to-sequence cloud removal tasks, respectively. manners: 1) over all pixels of the target image (as per conven-
                                                                                                tion), 2) only over cloud-covered pixels (visible in neither of
A. Metrics                                                                                      any input optical sample) to measure reconstruction of noisy
                                                                                                information, as well as 3) only over cloud-free pixels (visible
   We evaluate quantitative performance in terms of normal-                                     in at least one input optical patch) quantifying preservation of
ized root mean squares error (NRMSE), Peak Signal-to-Noise                                      information. ",cs.CV,B,0.28456098,-0.17663375,-0.27767718
http://arxiv.org/pdf/2201.09700v1,Feature transforms for image data augmentation,"The best ensemble reported in this work either exceeds the performance of
the state-of-the-art in the literature or achieves similar performance on all the tested datasets. In section 5, we conclude with a few
suggestions for further research in this area. The main contributions of this study can be summarized as follows:
 Presented is a large evaluation across eleven freely available and diverse benchmarks of the performance of common image

     manipulation methods used for data augmentation. ",cs.CV,C,-0.14995366,-0.118808195,-0.12636243
http://arxiv.org/pdf/2201.09701v1,Learning Semantics for Visual Place Recognition through Multi-Scale Attention,"The other is SYNTHIA [24], which contains very dense sequences of images
that are suitable for the visual localization task [20] (≤ 5m from one gps coordinate to
the other) but not for the coarser place recognition task usually considered in literature
(≤ 25m). To develop our data-driven approach for VPR that combines both visual and
semantic knowledge, and also to enable further research in this direction, we created a
new synthetic dataset. This new dataset was inspired by IDDA [1], which was built from the CARLA vir-
tual simulator [7] speciﬁcally for semantic segmentation but without GPS annotations. ",cs.CV,C,-0.14474882,-0.24142724,0.052242775
http://arxiv.org/pdf/2201.09701v2,Learning Semantics for Visual Place Recognition through Multi-Scale Attention,"The other is SYNTHIA [24], which contains very dense sequences of images
that are suitable for the visual localization task [20] (≤ 5m from one gps coordinate to
the other) but not for the coarser place recognition task usually considered in literature
(≤ 25m). To develop our data-driven approach for VPR that combines both visual and
semantic knowledge, and also to enable further research in this direction, we created a
new synthetic dataset. This new dataset was inspired by IDDA [1], which was built from the CARLA vir-
tual simulator [7] speciﬁcally for semantic segmentation but without GPS annotations. ",cs.CV,C,-0.14474882,-0.24142724,0.052242775
http://arxiv.org/pdf/2201.09846v1,A Novel Mix-normalization Method for Generalizable Multi-source Person Re-identification,"not only in each DMN but also inter the different DMNs. This way can bring more diversity of features during training,             3: Randomly produce the number of mixing domains as C.
so as to promote the generalization to the unseen domain, as
validated in the further analysis of the experimental section. 4: while |S| = 0 do

   Besides, when using the DMN to train the model, we set                  5: if |S| =< C then
the maximum numbers of the randomly selected domains as
D − 1. ",cs.CV,A,-0.036325525,0.28287742,-0.07207324
http://arxiv.org/pdf/2201.09846v2,A Novel Mix-normalization Method for Generalizable Multi-source Person Re-identification,"This way can bring more diversity of features
and D − 1 (i.e., [1, D − 1]) in our method. Then we randomly       during training so as to promote the generalization to the
choose C domains (i.e., φ) from S and remove them from             unseen domain, as validated in the further analysis of the
S. For the selected domain set φ, we compute the statistics        experimental section. as Eqs. ",cs.CV,A,-0.05562164,0.23633699,-0.0575205
http://arxiv.org/pdf/2201.09933v1,Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices,"However, we also observe several limitations from its applications to real-world scenarios and from users’
feedback. In this section, we briefly discuss some potential future works that will further improve EMOShip
system. 5.4.1 Personalized Emotional Management. ",cs.CV,A,0.17820331,0.19073379,0.47861707
http://arxiv.org/pdf/2201.09933v2,Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices,"However, we also observe several limitations from its applications to real-world scenarios and from users’
feedback. In this section, we briefly discuss some potential future works that will further improve EMOShip
system. 5.4.1 Personalized Emotional Management. ",cs.CV,A,0.17820331,0.19073379,0.47861707
http://arxiv.org/pdf/2201.09967v1,Attacks and Defenses for Free-Riders in Multi-Discriminator GAN,"The various architectures
of MD-GAN differ with regard to model exchange between
discriminators. AsynDGAN [4] and GMAN [6] are elementary
It is future work to verify this expectation and ensure that                    [19] X. Liang, Z. Hu, H. Zhang, C. Gan, and E. P. Xing. Recurrent topic-
                                                                                      transition GAN for visual paragraph generation. ",cs.CV,C,-0.10962692,0.09059049,0.041987054
http://arxiv.org/pdf/2201.09973v1,The Vehicle Trajectory Prediction Based on ResNet and EfficientNet Model,"Finally, in the fifth part,
the research work of this paper is summarized and the       loss would be caused every time from input to output. future work is prospected. Therefore, to keep the network from degradation, the
                    II. ",cs.CV,A,0.22835352,0.29606432,-0.21141464
http://arxiv.org/pdf/2201.10047v1,Are Commercial Face Detection Models as Biased as Academic Models?,"We have done this due to a potential interpretation
of the data use license agreement which limits modiﬁcations and could preclude corrupting the images
in their dataset. This license inhibits further research on algorithmic fairness in this way, which
is especially unfortunate because the dataset has actor-provided age and gender labels as well as
human-reviewer-provided Fitzpatrick skin types and ambient lighting conditions. Yet, due to potential
legal liability, we choose not to compare to the CCD results of Dooley et al. ",cs.CV,C,-0.012170911,-0.027671078,0.068622336
http://arxiv.org/pdf/2201.10060v1,ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals,"Therefore,       the ﬁnal result is presented based on the average accuracy
the proposed ViT-HGR architecture converts the main signal       for each fold. into smaller portions using a speciﬁc window size and then
feeds each of these portions to the ViT for further analysis. B. ",cs.CV,B,0.43264738,0.0012025107,-0.07356295
http://arxiv.org/pdf/2201.10138v1,SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification,"Furthermore, the proposed pre-training
                                                                                   pipeline may also be employed to other domains of computer
                                     TABLE III                                     vision, especially where region-wise focusing is crucial. We
CROSS-DATASET EVALUATION RESULTS OBTAINED BY THE PROPOSED                          intend to work further in these directions in our future works. METHOD. ",cs.CV,C,-0.14256996,-0.08194354,-0.13247162
http://arxiv.org/pdf/2201.10138v2,SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification,"Furthermore, the proposed pre-training
                                                                                               pipeline may also be employed to other domains of computer
                     69.59     41.87                         73.58  30.03                      vision, especially where region-wise focusing is crucial. We
                                                                                               intend to work further in these directions in our future works. D. Cross-Script Evaluation

   Finally, we also probe into a cross-script OSV setup, where
the model trained on a signature corpus of a given script is
evaluated on a dataset of a different script. ",cs.CV,C,-0.16135421,0.033802263,-0.019347329
http://arxiv.org/pdf/2201.10162v1,Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks,"933–946,
consistent with the “true motion” of video objects [91], leading         2014.
inaccurate segmentation results. We leave these challenges for
our future work. [10] G. J. Sullivan, J.-R. Ohm, W.-J. ",cs.CV,B,0.07043749,-0.3874573,0.016277326
http://arxiv.org/pdf/2201.10162v2,Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks,"For
                                                                                                             Decoder
                        Decoder
             Encoder                                                                                                                45  example, in order to support the machine learning based
                                                                                                  Encoder                               multimedia algorithms, e.g., detection, recognition, track-
                                                                                                                        Human           ing, etc, the traditional coding frameworks typically need
   Video     Video Codec    Human        Video                                                                        Perception
Acquisition        (a)    Perception  Acquisition New Video Codec

                                                              (b)

Figure 1: Motivation illustration: (a) the traditional coding frame-                                                                    ﬁrst decompresses all the encoded bitstream into the raw
works typically only focus on satisfying human perception. (b) as a                                                                     RGB/YUV format, and then feed the decompressed video
high-eﬃcient coding framework that serves for the AI era, it should 50                                                                  content into downstream tasks for the further analysis,
satisfy both human perception and machine analytics. which inevitably consumes a large amount of decoding

15 more and more attention. ",cs.CV,C,-0.12958542,-0.13367239,0.18145207
http://arxiv.org/pdf/2201.10175v1,RFMask: A Simple Baseline for Human Silhouette Segmentation with Radio Signals,"Such failure is due to
                                        based on millimeter wave signals. We hope that our work can                  the fact that visual perception is fundamentally an ill-posed
                                        serve as a baseline and inspire further research that perform                problem under occlusion or low illumination conditions. vision tasks with radio signals. ",cs.CV,B,0.31710672,-0.19292146,-0.019307358
http://arxiv.org/pdf/2201.10184v1,Estimating the Direction and Radius of Pipe from GPR Image by Ellipse Inversion Model,"Experiments on real-world datasets are conducted,
                                                                                and the existing pipeline map is modiﬁed, which validated the
                                                                                effectiveness of the proposed model. In future work, we will
                                                                                study how to map the pipelines of an area where there is no
                                                                                existing pipeline map. TABLE I                                                                                REFERENCES
   THE ERRORS OF THE PROPOSED MODEL
                                                                                 [1] S. W. Jaw and M. Hashim, “Locational accuracy of underground utility
   The average error     The max error                                                mapping using ground penetrating radar,” Tunnelling and Underground
                                                                                      Space Technology, vol. ",cs.CV,B,0.21779485,-0.016780794,-0.10276443
http://arxiv.org/pdf/2201.10252v1,DocEnTr: An End-to-End Document Image Enhancement Transformer,"It is a simple and ﬂexible framework that can also be easily                      [1] B. Megyesi, N. Blomqvist, and E. Pettersson, “The decode database:
applied to enhance other kinds of degradation occurring in                             Collection of historical ciphers and keys,” in The 2nd International
document images (like blur, shadow, warps, stains etc). These                          Conference on Historical Cryptology, HistoCrypt 2019, June 23-26
aspects will be investigated in a future work. We also wish to                         2019, Mons, Belgium, 2019, pp. ",cs.CV,A,0.20729417,0.10307007,0.102916166
http://arxiv.org/pdf/2201.10271v1,Convolutional Xformers for Vision,"DualOpT training needs to be investigated more using different
initialisations, hyper-parameter tuning, and learning rates to shed more light into the differences in loss surfaces near
and far away from the solution. Additionally, further study on which optimizers are better for initial and ﬁnal phases
also needs to be conducted. Among the token-mixing architectures, we found that FNet and MLP-Mixer were not suitable for vision applications
but WaveMix and ConvMixer perform at par with the transformer models while consuming less GPU RAM. ",cs.CV,B,-0.026756784,-0.0014178045,-0.24659571
http://arxiv.org/pdf/2201.10366v1,ADAPT: An Open-Source sUAS Payload for Real-Time Disaster Prediction and Response with AI,"It is the duty of those contributing to the democrati-
sult and build more features and payload conﬁgurations into  zation of AI and sUAS capabilities to be aware of potential
the ADAPT system. For future work, we plan further feature   blind spots in our society’s technological understanding of
development of the open-source system in support of ﬂight    these elements and advocate for policies, rules, and regula-
odometery and multi-sensor fusion. Additionally, we plan to  tions which protect those most vulnerable to the potential for
continue development of the river ice segmentation dataset   abuse. ",cs.CV,B,0.025511866,-0.11373605,-0.054967888
http://arxiv.org/pdf/2201.10394v1,Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition,"Table 6 and 7 show such
Despite the positive results, we also ﬁnd some limitations of  cases on Diving-48 and UCF-101 datasets. the proposed approaches, which we hope can lead to inter-
esting future work. Interestingly, Diving-48 V2 shows a decrease in perfor-
                                                               mance with the GrayST input. ",cs.CV,A,0.2696469,0.21142335,0.014509552
http://arxiv.org/pdf/2201.10394v2,Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition,"Table 6 and 7 show such
Despite the positive results, we also ﬁnd some limitations of  cases on Diving-48 and UCF-101 datasets. the proposed approaches, which we hope can lead to inter-
esting future work. Interestingly, Diving-48 V2 shows a decrease in perfor-
                                                               mance with the GrayST input. ",cs.CV,A,0.2696469,0.21142335,0.014509552
http://arxiv.org/pdf/2201.10394v3,Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition,"Our methods outperformed the baselines by a signiﬁcant
margin on this dataset. D Limitations

Despite the positive results, we also ﬁnd some limitations of the proposed approaches, which
we hope can lead to interesting future work. KIM ET AL. ",cs.CV,A,0.014698116,0.2320181,0.009868503
http://arxiv.org/pdf/2201.10410v1,Comparison of Evaluation Metrics for Landmark Detection in CMR Images,"However, this yields diﬀerent values for a heterogeneous image
collection of diﬀerent image sizes. In future work, we will investigate these diﬀerences
and compare them with a global, image-size independent, upper bound. Acknowledgement. ",cs.CV,B,0.10054128,-0.058077347,-0.21166366
http://arxiv.org/pdf/2201.10410v2,Comparison of Evaluation Metrics for Landmark Detection in CMR Images,"However, this yields diﬀerent values for a heterogeneous image
collection of diﬀerent image sizes. In future work, we will investigate these diﬀerences
and compare them with a global, image-size independent, upper bound. Acknowledgement. ",cs.CV,B,0.100541234,-0.05807741,-0.21166365
http://arxiv.org/pdf/2201.10423v1,Rayleigh EigenDirections (REDs): GAN latent space traversals for multidimensional features,"As a result, we           could be extended to use both global and local directions
found the face latent space and traversals to be smoother. per traversal step, which we leave for future work. Our living room traversals often exhibit large perceptual
“jumps” due to discontinuities in latent space (see Supple-              Though our method can theoretically work with any deep
mentary). ",cs.CV,B,0.15625568,-0.06679071,0.050470408
http://arxiv.org/pdf/2201.10439v1,Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition,"Finally, we did not observe any beneﬁt in ﬁne-tuning our lip
reading models. We hypothesise that the domain shift between our             The future work includes the use of the ViT video front-end for
train data and the LRS3-TED test set is the greatest for the audio      the multiple speaker audio-visual recognition [31, 32]. modality. ",cs.CV,A,0.050691582,0.08186649,0.1640694
http://arxiv.org/pdf/2201.10448v1,How Low Can We Go? Pixel Annotation for Semantic Segmentation,"keeping the correct training order is also important. We be-
                                                                  lieve that additional research can be done on this interest-
   The performance of OracleNet can be understood by              ing phenomena. No optimization of hyper-parameters was
comparing the pixels it selected against those selected by        done in our experiments (architecture, forward-backward it-
Pseudo-Oracle. ",cs.CV,A,-0.011840313,0.21435387,-0.21315673
http://arxiv.org/pdf/2201.10448v2,How Low Can We Go? Pixel Annotation for Semantic Segmentation,"Furthermore, we see
   The performance of OracleNet can be understood by              that keeping the correct training order is also important. We
comparing the pixels it selected against those selected by        believe that additional research can be done on this interest-
PixelLabeling algorithm. An example is shown in Figure 7.         ing phenomenon. ",cs.CV,C,-0.12003583,0.03874806,-0.18770868
http://arxiv.org/pdf/2201.10521v1,A Review of Deep Learning Based Image Super-resolution Techniques,"RTSR improves the performance of the super sub network by enhancing
the quality of supervision pictures and repeatedly training the network. However, whether using
enhanced supervision pictures to continuously improve the quality of network training will cause
more artiﬁcial traces in the ﬁnal generated pictures is worth further research, In addition, it is also
worth exploring whether the supervision picture enhanced in the last cycle can be used to directly
generate the supervision picture required in the next cycle instead of continuously using the original
high score picture to generate the supervision picture required in the next cycle. 6.2 Texture and perceptual cost function

Texture loss is used to describe the difference in texture style between the generated image and the
reference image. ",cs.CV,C,-0.11086792,0.0016799979,-0.161149
http://arxiv.org/pdf/2201.10523v1,Interpretability in Convolutional Neural Networks for Building Damage Classification in Satellite Imagery,"This research is especially important now,
when climate change is ramping up the frequency and intensity of these devastating events. 4
Building upon our work with improving interpretability, future work includes investigations of the
prediction performance of deep learning models with other types of input added, such as neighboring
building ground truths. Additionally, other ways to combine information such as pre-disaster and
post-disaster images (instead of a simple concatenation like we did here) with the goal of improving
interpretability should yield valuable results. ",cs.CV,C,-0.25158995,0.07387,0.11259034
http://arxiv.org/pdf/2201.10602v1,Jacobian Computation for Cumulative B-splines on SE(3) and Application to Continuous-Time Object Tracking,"Our
                                 0.40 169.58               0.76 155.55  results are on par with the state of the art, showing signiﬁcant
                                                                        improvements in certain aspects like global consistency and
                                 0.30 126.11               0.47 169.92  velocity estimation. 0.38 107.28               0.32 61.28      As future work, we ﬁnd interesting to explore higher order
                                                                        continuity curves. This could increase the ﬂexibility of the
TABLE II: Maximum component of translation error [m]                    trajectories as the recent work [52] suggests. ",cs.CV,A,0.43304566,0.13351902,0.040526684
http://arxiv.org/pdf/2201.10602v2,Jacobian Computation for Cumulative B-Splines on SE(3) and Application to Continuous-Time Object Tracking,"Our results are on par with the
         System           Box 1       Box 2        Box 3       Box 4   state of the art, showing signiﬁcant improvements in certain
                      xyz rpy                  xyz rpy                 aspects like global consistency and velocity estimation. [11] MVO (Pose)      0.09 11.21  xyz  rpy     0.13 5.40   xyz  rpy
[9] ClusterVO        0.24 6.09                0.24 15.03                  As future work, we ﬁnd interesting to explore higher order
[8] VDO              1.06 56.77  0.31 68.20   1.30 19.12  0.55 93.14   continuity curves. This could increase the ﬂexibility of the
Ours (w/ Local BA)   0.39 42.27               0.77 33.33               trajectories as the recent work [52] suggests. ",cs.CV,A,0.42372167,0.12966701,-0.08652003
http://arxiv.org/pdf/2201.10650v1,Beyond Visual Image: Automated Diagnosis of Pigmented Skin Lesions Combining Clinical Image Features with Patient Data,"In addition, a new method of interactive
segmentation is presented that takes into account the similarity of color and proximity of pixels. The remainder of this paper is organized as follows: Section 2 introduces the segmentation framework;
Section 3 describes the feature extraction stage; Section 4 explains the classiﬁcation method used; Section
5 describes the proposed approach; In Section 6 the experiments and results are presented; Finally, section
7 presents a brief conclusion with directions for future work. 4
2. ",cs.CV,C,-0.05324533,-0.25517645,0.07676336
http://arxiv.org/pdf/2201.10654v1,SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering,"7. Conclusion and future work
6.5. Performance on semantic graphs with different
      levels of accuracy                                              In this work, we present a novel model named SA-VQA. ",cs.CV,A,-0.019931957,0.2576441,0.15872616
http://arxiv.org/pdf/2201.10675v1,Virtual Adversarial Training for Semi-supervised Breast Mass Classification,"In addition, this study treats the importance/ influence of each labeled or unlabeled
sample equally, which may affect the performance of VAT. We expect to extend VAT by
incorporating the importance of training samples in future work. In summary, our study initially verified the feasibility of using VAT to improve the performance
of classifying benign and malignant breast masses from mammograms in a semi-supervised manner. ",cs.CV,C,-0.10854025,0.178109,-0.01874865
http://arxiv.org/pdf/2201.10695v1,Estimation of Spectral Biophysical Skin Properties from Captured RGB Albedo,"Nonetheless our model performs well in these conditions. In fact, the error can be used as a mask revealing the regions of still
signiﬁcant lighting, which in turn could be fed to a semi-automatic process of delighting (future work). 3
              MELANIN  HEMOGLOBIN(D) HEMOGLOBIN(E) MELANIN RATIO OXYGENATION  THICKNESS

C1 [GGD’20]

C1 OURS D65

C1 OURS D65*

C2 [GGD’20]

C2 OURS D65

C2 OURS D65*

Figure 6: Skin parameter estimation comparison. ",cs.CV,A,0.3099724,0.021440089,0.10120603
http://arxiv.org/pdf/2201.10728v1,Training Vision Transformers with Only 2040 Images,"However, there is still room for improve-
                                                                    ment when training from scratch on these small datasets
                                                                    for architectures like DeiT. Furthermore, it is still unknown
                                                                    what properties matter for pretraining on small datasets when
                                                                    transferring and we leave them to future work. 8
References                                                                     Scale. ",cs.CV,A,-0.104714386,0.3280987,-0.119033724
http://arxiv.org/pdf/2201.10737v3,Class-Aware Generative Adversarial Transformers for Medical Image Segmentation,"We conduct extensive analyses to study the robustness of our
approach, and form a more detailed understanding of desirable properties in the medical domain (i.e.,
transparency and data efﬁciency). Overall, we hope that this model can serve as a solid baseline for 2D medical image segmentation
and motivate further research in medical image analysis tasks. It also provides a new perspective on
transfer learning in medical domain, and initially shed novel insights towards understanding neural
network behavior. ",cs.CV,C,-0.26484013,-0.031599157,-0.171353
http://arxiv.org/pdf/2201.10737v4,Class-Aware Adversarial Transformers for Medical Image Segmentation,"We conduct extensive analyses to study the robustness of our approach, and
form a more detailed understanding of desirable properties in the medical domain (i.e., transparency
and data efﬁciency). Overall, we hope that this model can serve as a solid baseline for 2D medical image segmentation
and motivate further research in medical image analysis tasks. It also provides a new perspective on
transfer learning in medical domain, and initially shed novel insights towards understanding neural
network behavior. ",cs.CV,C,-0.26484013,-0.031599157,-0.171353
http://arxiv.org/pdf/2201.10737v5,Class-Aware Adversarial Transformers for Medical Image Segmentation,"We conduct extensive analyses to study the robustness of our approach, and
form a more detailed understanding of desirable properties in the medical domain (i.e., transparency
and data efﬁciency). Overall, we hope that this model can serve as a solid baseline for 2D medical image segmentation
and motivate further research in medical image analysis tasks. It also provides a new perspective on
transfer learning in medical domain, and initially shed novel insights towards understanding neural
network behavior. ",cs.CV,C,-0.26484013,-0.031599157,-0.171353
http://arxiv.org/pdf/2201.10753v1,Interactive Image Inpainting Using Semantic Guidance,"With the help of external           including face images and natural scenes demonstrate the
attention module which supply additional contextual infor-              superiority of our approach in terms of inpainting quality
                                                                        and controllability. In future work, we plan to add reference
                                                                        image in our inpainting approach, so that user could not only
                                                                        customize the content of the inpainting result, but also choose
                                                                        their prefer reference image to supply additional color and
                                                                        texture to the inpainting result. REFERENCES

                                                                         [1] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, “Patch-
                                                                              match: a randomized correspondence algorithm for structural image
                                                                              editing,” ACM Trans. ",cs.CV,C,-0.09498492,-0.18888286,0.112112045
http://arxiv.org/pdf/2201.10766v1,"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes","Limitations                                                      a more difﬁcult classiﬁcation task. In future work, we may
                                                                    build on RIVAL10 to craft more ﬁnegrained classiﬁcation
   The central challenge of our work is performing com-             tasks, perhaps leveraging attribute insertion and removal. parisons across diverse model types. ",cs.CV,A,-0.08454126,0.28268486,0.17172557
http://arxiv.org/pdf/2201.10801v1,When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism,"In Proceedings
performance must come from the remaining components of          of the IEEE/CVF International Conference on Computer Vi-
ViT, e.g., the FFN and the training scheme. In future work,     sion, 7083–7093. we plan to have more analysis on such factors and investi-
gate more ViT variants. ",cs.CV,A,0.23618612,0.334163,-0.07864769
http://arxiv.org/pdf/2201.10830v1,MonoDistill: Learning Spatial Features for Monocular 3D Object Detection,"However, generalizing
to the new scenes with different statistical characteristics is a hard task for existing 3D detectors
(Yang et al. ; Wang et al., 2020b), including the image-based models and LiDAR-based models,
and deserves further investigation by future works. We also argue that the proposed method can
generalize to the new scenes better than other monocular models because ours model learns the
stronger features from the teacher net. ",cs.CV,B,-0.12720314,-0.2896145,-0.06632349
http://arxiv.org/pdf/2201.10836v1,PARS: Pseudo-Label Aware Robust Sample Selection for Learning with Noisy Labels,"Comparison of this baseline and the ﬁnal PARS can demonstrate the
importance of different components of our proposed method, PARS, independently of the choice of a
robust loss function. Note that studying which noise robust loss is best suited for each dataset and/or
noise setting is beyond the scope of this paper, which we leave for future work. A.4 ADDITIONAL EXPERIMENTS

Convergence of Test Accuracy. ",cs.CV,A,0.1373287,0.17015591,-0.18586168
http://arxiv.org/pdf/2201.10963v2,Learning to Compose Diversified Prompts for Image Emotion Classification,"arXiv preprint
results like the second image, which seems acceptable. So it              arXiv:1410.8586, 2014.
could be considered to further research about the interclass
relationships or LDL-related studies by PT-DPC or to exploit           [Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li,
a more large-scale pre-trained model beyond CLIP. Kai Li, and Li Fei-Fei. ",cs.CV,A,0.054441236,0.25402543,-0.021260673
http://arxiv.org/pdf/2201.10972v2,How Robust are Discriminatively Trained Zero-Shot Learning Models?,"Our baseline results, spanning a wide range of corruptions and defenses, show that
the discriminative ZSL models are not robust, primarily due to the severe class imbalance
and model weakness inherent to ZSL. Our results indicate that although some defense
methods work, they fail to do so in a tangible manner, which highlights the necessity
of further research. Our results also show that although these defense methods fail to
work, they set new high accuracies for our ZSL models. ",cs.CV,A,0.02612901,0.32425857,0.093746915
http://arxiv.org/pdf/2201.11006v1,An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications,"In addition, conventional studies have focused
contrast, τ dropped signiﬁcantly under the use of K . on image classiﬁcation, so other applications such as object
                                                               detection and semantic segmentation should be discussed as
   A watermarked model is pirated if the original watermark    future work. is removed or a new veriﬁable watermark is injected while
maintaining a model’s accuracy. ",cs.CV,C,-0.03000493,-0.08360513,0.064720504
http://arxiv.org/pdf/2201.11006v2,An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications,"In addition, conventional studies have focused
contrast, τ dropped signiﬁcantly under the use of K . on image classiﬁcation, so other applications such as object
                                                               detection and semantic segmentation should be discussed as
   A watermarked model is pirated if the original watermark    future work. is removed or a new veriﬁable watermark is injected while
maintaining a model’s accuracy. ",cs.CV,C,-0.03000493,-0.08360513,0.064720504
http://arxiv.org/pdf/2201.11014v1,Evaluating language-biased image classification based on semantic representations,"A remaining question is how the CLIP model acquires the biased representations in the image encoder. The presented
results may be improved by further studying them as a function of potential biases in the training data of CLIP itself:
for example, it would be useful to understand in the ﬁrst place how CLIP acquired the ability to visually recognize
superimposed words and why we observed a bias favoring the superimposed words over the whole image. In particular,
we can ask whether the written words are in the training data and if they are, whether there are adversarial examples in
the training data like our word-superimposed images. ",cs.CV,C,-0.18815255,0.024258267,0.017270185
http://arxiv.org/pdf/2201.11014v2,Language-biased image classification: evaluation based on semantic representations,"A remaining question is how the CLIP model acquires the biased representations in the image en-
coder. The presented results may be improved by further studying them as a function of potential
biases in the training data of CLIP itself: for example, it would be useful to understand in the ﬁrst
place how CLIP acquired the ability to visually recognize superimposed words and why we observed
a bias favoring the superimposed words over the whole image. In particular, we can ask whether the
written words are in the training data and if they are, whether there are adversarial examples in the
training data like our word-superimposed images. ",cs.CV,C,-0.1680884,0.007184601,0.027037818
http://arxiv.org/pdf/2201.11091v1,Momentum Capsule Networks,"For completeness, we included a
hyperparameter analysis that evaluated diﬀerent values of learning rates, num-
ber of capsules and batch sizes, increasing the number of capsules had a positive
eﬀect on MoCapsNet, while changing the batch size or learning rate did not have
a big inﬂuence or worsened the results. For future work, it would be of interest to analyze replacing the fully con-
nected capsule layers with convolutional capsule layers, which would enable us
to save even more memory. Such modiﬁcation could allow for the application of
capsule networks to more complex tasks and/or datasets, such as ImageNet. ",cs.CV,C,-0.28542438,0.15379746,-0.17839557
http://arxiv.org/pdf/2201.11091v2,Momentum Capsule Networks,"For completeness, we included
a hyperparameter analysis that evaluated diﬀerent values of learning rates, number of capsules and batch
sizes, increasing the number of capsules had a positive eﬀect on MoCapsNet, while changing the batch size
or learning rate did not have a big inﬂuence or worsened the results. For future work, it would be of interest to analyze replacing the fully connected capsule layers with convo-
lutional capsule layers, which would enable us to save even more memory. Such modiﬁcation could allow for
the application of capsule networks to more complex tasks and/or datasets, such as ImageNet. ",cs.CV,C,-0.28509703,0.15106034,-0.17430958
http://arxiv.org/pdf/2201.11103v1,Auto-Compressing Subset Pruning for Semantic Image Segmentation,"(2020) as baselines,
which motivates our choice of datasets and architectures. Most notably, this excludes state-of-the-art semantic
segmentation architectures, and we leave the study of these architectures to future work. We note that we do not have
access to either implementation or pretrained checkpoints of the references we compare ourselves to. ",cs.CV,C,-0.21824387,-0.044836,0.036522817
http://arxiv.org/pdf/2201.11114v1,Natural Language Descriptions of Deep Visual Features,"Discrete categorization is also possible for directions in rep-
resentation space (Kim et al., 2018; Andreas et al., 2017; Schwettmann et al., 2021) and for clusters
of images induced by visual representations (Laina et al., 2020); in the latter, an off-the-shelf im-
age captioning model is used to obtain language descriptions of the unifying visual concept for the
cluster, although the descriptions miss low-level visual commonalities. As MILAN requires only a
primitive procedure for generating model inputs maximally associated with the feature or direction
of interest, future work might extend it to these settings as well. 2
Published as a conference paper at ICLR 2022

Natural language explanations of decisions Previous work aimed at explaining computer vision
classiﬁers using natural language has focused on generating explanations for individual classiﬁcation
decisions (e.g., Hendricks et al., 2016; Park et al., 2018; Hendricks et al., 2018; Zellers et al., 2019). ",cs.CV,C,-0.31466618,-0.061238073,0.39604288
http://arxiv.org/pdf/2201.11114v2,Natural Language Descriptions of Deep Visual Features,"Discrete categorization is also possible for directions in rep-
resentation space (Kim et al., 2018; Andreas et al., 2017; Schwettmann et al., 2021) and for clusters
of images induced by visual representations (Laina et al., 2020); in the latter, an off-the-shelf im-
age captioning model is used to obtain language descriptions of the unifying visual concept for the
cluster, although the descriptions miss low-level visual commonalities. As MILAN requires only a
primitive procedure for generating model inputs maximally associated with the feature or direction
of interest, future work might extend it to these settings as well. 2
Published as a conference paper at ICLR 2022

Natural language explanations of decisions Previous work aimed at explaining computer vision
classiﬁers using natural language has focused on generating explanations for individual classiﬁcation
decisions (e.g., Hendricks et al., 2016; Park et al., 2018; Hendricks et al., 2018; Zellers et al., 2019). ",cs.CV,C,-0.31466618,-0.061238073,0.39604288
http://arxiv.org/pdf/2201.11197v1,Challenges and Opportunities for Machine Learning Classification of Behavior and Mental State from Images,"6
Opportunities
Having covered the challenges for complex CV model development, we now review current
methods which address these challenges. We then outline opportunities for future work and
innovation. We examine the present state-of-the-art methodologies in automatic data labeling,
manual feature engineering, feature representation learning including self-supervised learning,
active learning, generative methods for data augmentation, label representation, crowd worker
selection, federated learning, and meta-learning (Figure 1). ",cs.CV,C,-0.33509615,0.2017541,0.10500548
http://arxiv.org/pdf/2201.11279v1,Revisiting RCAN: Improved Training for Image Super-Resolution,"This section shows an even longer train-
ing schedule, a comparison between training from scratch            Rejection Sampling. Existing SR frameworks generate
and warm-start, and two additional techniques we tested,            training data by randomly sampling patches uniformly from
which can be helpful data points for future work. all positions. ",cs.CV,A,-0.044044405,0.23869187,-0.12650844
http://arxiv.org/pdf/2201.11307v1,Dissecting the impact of different loss functions with gradient surgery,"P lin−ms is performance almost as            1+S  Euclidean direction  (20)
same as P lin, but P sig−ms shows some computation instability
effect. We put a further analysis of this effect in the appendix. √2
                                                                      1−S2 cosine direction
   In summary, Table 3 shows several features related to the pair
weight. ",cs.CV,A,0.40661168,0.22719112,-0.09137917
http://arxiv.org/pdf/2201.11345v2,Exploring Global Diversity and Local Context for Video Summarization,"the partition of video shots still needs to be improved since
In addition, the L2 similarity outperforms the rest of the                        the quality of the video shots is also important for video
two similarity measurements by at least 2.8% on SumMe                             summary performance. In future work, we aim to improve
and 0.6% on TVSum. This is consistent with the content of                         the model by incorporating the audio information since we
Section III-A - the L2 similarity can lead to more diversiﬁed                     only utilize the visual feature of the video. ",cs.CV,B,0.0238499,-0.1333014,0.091088615
http://arxiv.org/pdf/2201.11351v1,Effective Shortcut Technique for GAN,"448–456. PMLR (2015)
As our future work, we plan to further investigate the
eﬀectiveness of the gated shortcut in other various ap-          15. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-
plications. ",cs.CV,A,0.25318143,0.04091849,0.076310314
http://arxiv.org/pdf/2201.11379v1,Deep Confidence Guided Distance for 3D Partial Shape Registration,"Using such a soft map is sometimes unfeasi-
                          2.90           3.81                    ble, as in the case of depth sensors for autonomous driving,
    Full method                                                  acquiring ≈ 100, 000 points per scene. Accordingly, one in-
                                                                 teresting future work is to produce a hierarchical soft align-
focus on 3D objects, we present visual results on 2.5D in-       ment scheme, where P is evaluated only for sampled clus-
door scans from the 3DMATCH dataset. 3DMATCH sam-                ters, and propagated downwards to the dense point cloud. ",cs.CV,B,0.33006674,-0.343728,-0.07740582
http://arxiv.org/pdf/2201.11403v1,Generalised Image Outpainting with U-Transformer,"We      backbone in the variant of CNN with SC&TSP suﬀers
could see that removing any design of our full model  from blur and has an obvious boundary between the
leads to worse performance, proving that our de-      input and predicted region due to long-range depen-
signs could boost the proposed model learning and     dencies not being well extracted. When only using
improve the quality of the generated extrapolation    Swin Transformer blocks, the extended image borders
images.We further study the eﬀectiveness of our de-   have plausible structure but obvious blunt colours in
signs from qualitative results, with some examples    column (c). When further utilizing the SC, more re-
shown in Fig. ",cs.CV,C,-0.20958939,-0.02510635,-0.1740354
http://arxiv.org/pdf/2201.11403v2,Generalised Image Outpainting with U-Transformer,"We       from blur and has an obvious boundary between the
could see that removing any design of our full model   input and predicted region due to long-range depen-
leads to worse performance, proving that our designs   dencies not being well extracted. When only using
could boost the proposed model learning and im-        Swin Transformer blocks, the extended image borders
prove the quality of the generated extrapolation im-   have plausible structure but obvious blunt colours in
ages.We further study the eﬀectiveness of our designs  column (c). When further utilizing the SC, more re-
from qualitative results, with some examples shown     alistic and smooth colours are generated in the ex-
in Fig. ",cs.CV,B,0.0023136865,-0.051057298,-0.16213796
http://arxiv.org/pdf/2201.11403v3,Generalised Image Outpainting with U-Transformer,"The edges be-

                                                          9
                                                                                               Transformer w/o SC & TSP

                                                                                               Transformer w/o SC

                                                                                               Full model

                                                                                                Transformer Transformer
                                                                                               w/o SC-& TSP w/o SC

Test          (a)     (b)            (c)              (d)          (e)          (f)

Input         GT      CNN    Transformer              Transformer  Transformer                 Full model

                      with SC & TSP w/o SC & TSP w/o TSP           w/o SC       U-Transformer

                   Figure 7: Visual results of ablation study on Scenery dataset. 4.4 Ablation Study                                    leads to worse performance, proving that our designs
                                                      could boost the proposed model learning and im-
  w/o Trans   FID↓     IS↑   PSNR↑   SSIM↑            prove the quality of the generated extrapolation im-
w/o SC&TSP    37.934  2.991  22.658  0.743            ages.We further study the eﬀectiveness of our designs
              29.390  3.092  23.333  0.768            from qualitative results, with some examples shown
    w/o SC    92.038  2.345  20.895  0.554            in Fig. 7. ",cs.CV,A,0.1447532,0.099063374,-0.097044095
http://arxiv.org/pdf/2201.11403v4,Generalised Image Outpainting with U-Transformer,"ated extrapolation images. We further study the effectiveness
the trees, rocks, and rivers predicted in column (e) of Fig. 5.            of our designs from qualitative results, with some examples
The ﬁne-art paintings are more complicated, having a broader               shown in Fig. ",cs.CV,B,0.13091522,-0.09328094,0.12571195
http://arxiv.org/pdf/2201.11403v5,Generalised Image Outpainting with U-Transformer,"We could see that removing any design of our full model leads to
worse performance, proving that our designs could boost the proposed model
learning and improve the quality of the generated extrapolation images. We
further study the eﬀectiveness of our designs from qualitative results, with
some examples shown in Fig. 8. ",cs.CV,A,0.020275448,0.16044982,-0.07699747
http://arxiv.org/pdf/2201.11407v1,Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions,"Our method
                                                                       achieves state-of-the-art results in multiple datasets. Since ﬂow
    Intermediate ﬂow visualizations: We visualize the backward         and occlusion estimates from PWCNet-Bi-Occ are often not accu-
ﬂow Ftr→0 estimated by QVI [50] and our approach in Figure 9.          rate and hence can create a performance bottleneck in interpola-
We notice that erroneous results in QVI’s [50] interpolated frame      tion task, further research can explore whether inclusion of RGB
is caused by incorrect estimation of the backward ﬂow. However,        frames as input to 3D CNN can improve the performance. ",cs.CV,B,-0.05256472,-0.2673796,-0.24179322
http://arxiv.org/pdf/2201.11407v2,Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions,"Our method
                                                                       achieves state-of-the-art results in multiple datasets. Since ﬂow
    Intermediate ﬂow visualizations: We visualize the backward         and occlusion estimates from PWCNet-Bi-Occ are often not accu-
ﬂow Ftr→0 estimated by QVI [50] and our approach in Figure 9.          rate and hence can create a performance bottleneck in interpola-
We notice that erroneous results in QVI’s [50] interpolated frame      tion task, further research can explore whether inclusion of RGB
is caused by incorrect estimation of the backward ﬂow. However,        frames as input to 3D CNN can improve the performance. ",cs.CV,B,-0.05256472,-0.2673796,-0.24179322
http://arxiv.org/pdf/2201.11440v1,An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks,"As future research, we plan to further analyze the impact of the
number of folds in cross-validation based Bagging techniques and extend our analysis on deep learning Boosting
approaches. Furthermore, the analysis and applicability of popular explainable artificial intelligence techniques
for ensemble learning based medical image classification pipelines with multiple models is still an open research
field and requires further research. Acknowledgments
We want to thank Edmund Müller, Dennis Hartmann, Philip Meyer, and Peter Parys for their useful comments and
support. ",cs.CV,C,-0.23942155,0.0943854,-0.14714803
http://arxiv.org/pdf/2201.11440v2,An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks,"Furthermore, the applicability of explainable artificial intelligence techniques for ensemble learning

Preprint - April 2022  Page 12 / 16
  An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks - Müller et al. based medical image classification pipelines with multiple models is still an open research field and requires
further research. Acknowledgments
We want to thank Edmund Müller, Dennis Hartmann, Philip Meyer, and Peter Parys for their useful comments and
support. ",cs.CV,C,-0.26333278,0.070015825,-0.07761605
http://arxiv.org/pdf/2201.11523v1,ResiDualGAN: Resize-Residual DualGAN for Cross-Domain Remote Sensing Images Semantic Segmentation,"PotsdamIRRG to Vaihingen and 2.27% from PotsdamRGB to
Vaihingen. The OSA can also be replaced with other methods,        [6] X. Yuan, J. Shi, and L. Gu, “A review of deep learning methods
like self-training, to reach higher accuracy performance in the         for semantic segmentation of remote sensing imagery,” Expert Systems
future works. with Applications, vol. ",cs.CV,C,-0.22528635,-0.0070138816,-0.19031219
