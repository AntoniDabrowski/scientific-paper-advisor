Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
1339,Our results raise several questions for further study.,"We found that in more complex settings, a parametric bootstrap based
on a single set of learned features does not reﬂect the degree of uncertainty present when comparing features
derived from independently trained models.","It is natural to ask to what extent similar behaviors
are exhibited across other data domains, model types, or training regimes.",2022-02-01 01:43:19+00:00,Bootstrap Confidence Regions for Learned Feature Embeddings,stat.CO,['stat.CO'],[arxiv.Result.Author('Kris Sankaran')],"Algorithmic feature learners provide high-dimensional vector representations
for non-matrix structured signals, like images, audio, text, and graphs.
Low-dimensional projections derived from these representations can be used to
explore variation across collections of these data. However, it is not clear
how to assess the uncertainty associated with these projections. We adapt
methods developed for bootstrapping principal components analysis to the
setting where features are learned from non-matrix data. We empirically compare
the derived confidence regions in simulations, varying factors that influence
both feature learning and the bootstrap. Approaches are illustrated on spatial
proteomic data. Code, data, and trained models are released as an R compendium.",-0.029716503,0.35534585,-0.40413165,A
5214,"These theoretical analyses have inspired much further research as they give useful guidelines for

implementation of MALA in high dimensions: in addition to employing an explicit scale in the proposal

variance as predicted by the theory, one should “tune” the proposal variance of the RWM and MALA

algorithms so as to have acceptance probabilities of 0.234 and 0.574 respectively.","This          quantiﬁes   the  eﬃciency  gained  by  use  of  MALA  over

RWM, and in particular from employing local moves informed by the gradient of the logarithm of the target

density.",1.1.,2022-04-21 01:08:05+00:00,Optimal Scaling for the Proximal Langevin Algorithm in High Dimensions,stat.CO,"['stat.CO', 'math.PR', 'stat.ME', 'stat.ML']",[arxiv.Result.Author('Natesh S. Pillai')],"The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm
that incorporates the gradient of the logarithm of the target density in its
proposal distribution. In an earlier joint work \cite{pill:stu:12}, the author
had extended the seminal work of \cite{Robe:Rose:98} and showed that in
stationarity, MALA applied to an $N$-dimensional approximation of the target
will take ${\cal O}(N^{\frac13})$ steps to explore its target measure. It was
also shown in \cite{Robe:Rose:98,pill:stu:12} that, as a consequence of the
diffusion limit, the MALA algorithm is optimized at an average acceptance
probability of $0.574$. In \cite{pere:16}, Pereyra introduced the proximal MALA
algorithm where the gradient of the log target density is replaced by the
proximal function (mainly aimed at implementing MALA non-differentiable target
densities). In this paper, we show that for a wide class of twice
differentiable target densities, the proximal MALA enjoys the same optimal
scaling as that of MALA in high dimensions and also has an average optimal
acceptance probability of $0.574$. The results of this paper thus give the
following practically useful guideline: for smooth target densities where it is
expensive to compute the gradient while implementing MALA, users may replace
the gradient with the corresponding proximal function (that can be often
computed relatively cheaply via convex optimization) \emph{without} losing any
efficiency. This confirms some of the empirical observations made in
\cite{pere:16}.",-0.1177312,-0.15560806,0.19208777,C
5893,"Therefore, we recommend treating proﬁle CIs for the elements of the
TPM with care, in particular for models with more than two states, and further research in this direction
is needed.","This may be due to the strong dependence of all elements of the TPM in the same
row, which is problematic for a proper deﬁnition of the proﬁle likelihood function Lp(γij) difﬁcult (see,
e.g., Fischer and Lewis, 2021).","From an application perspective, the use of TMB allows executing estimation procedures at a signif-
icantly reduced cost compared to the execution of plain R. Such a performance gain could be of interest
when repeatedly executing statistical procedures on mobile devices.",2022-05-07 17:09:45+00:00,A gentle tutorial on accelerated parameter and confidence interval estimation for hidden Markov models using Template Model Builder,stat.CO,"['stat.CO', '62-04 (Primary), 62-08 (Secondary), 62F40, 91-04, 92-04', 'G.3; I.6.8']","[arxiv.Result.Author('Timothée Bacri'), arxiv.Result.Author('Geir D. Berentsen'), arxiv.Result.Author('Jan Bulla'), arxiv.Result.Author('Sondre Hølleland')]","A very common way to estimate the parameters of a hidden Markov model (HMM)
is the relatively straightforward computation of maximum likelihood (ML)
estimates. For this task, most users rely on user-friendly implementation of
the estimation routines via an interpreted programming language such as the
statistical software environment R (R Core Team, 2021). Such an approach can
easily require time-consuming computations, in particular for longer sequences
of observations. In addition, selecting a suitable approach for deriving
confidence intervals for the estimated parameters is not entirely obvious (see,
e.g., Zucchini et al., 2016; Lystig and Hughes, 2002; Visser et al., 2000), and
often the computationally intensive bootstrap methods have to be applied.
  In this tutorial, we illustrate how to speed up the computation of ML
estimates significantly via the R package TMB. Moreover, this approach permits
simple retrieval of standard errors at the same time. We illustrate the
performance of our routines using different data sets. First, two smaller
samples from a mobile application for tinnitus patients and a well-known data
set of fetal lamb movements with 87 and 240 data points, respectively. Second,
we rely on larger data sets of simulated data of sizes 2000 and 5000 for
further analysis. This tutorial is accompanied by a collection of scripts which
are all available on GitHub. These scripts allow any user with moderate
programming experience to benefit quickly from the computational advantages of
TMB.",-0.11383787,-0.2663256,0.06454587,C
6088,"Our approach using the
tempered algorithm was able to account for bifurcations in parameter space and also
highlight alternative parameter combinations consistent with the data that warranted
further study.","We compared several MCMC algorithms for conducting inference and found that
despite improvements in eﬀective sdosele sizes, the random walk Metropolis and geo-
metric algorithms struggled to bypass bifurcations in parameter space and hence un-
derestimate posterior uncertainty in marginal distributions.","Girolami and Calderhead (2011) highlight that extending the manifold
MALA algorithm to a manifold Hamiltonian Monte Carlo (HMC) algorithm facilitates
implementation within a parallel tempered algorithm and this may further improve
the results found here.",2022-05-12 08:50:11+00:00,Bayesian inference for stochastic oscillatory systems using the phase-corrected Linear Noise Approximation,stat.CO,"['stat.CO', 'stat.ME']","[arxiv.Result.Author('Ben Swallow'), arxiv.Result.Author('David A. Rand'), arxiv.Result.Author('Giorgos Minas')]","Likelihood-based inference in stochastic non-linear dynamical systems, such
as those found in chemical reaction networks and biological clock systems, is
inherently complex and has largely been limited to small and unrealistically
simple systems. Recent advances in analytically tractable approximations to the
underlying conditional probability distributions enable long-term dynamics to
be accurately modelled, and make the large number of model evaluations required
for exact Bayesian inference much more feasible. We propose a new methodology
for inference in stochastic non-linear dynamical systems exhibiting oscillatory
behaviour and show the parameters in these models can be realistically
estimated from simulated data. Preliminary analyses based on the Fisher
Information Matrix of the model can guide the implementation of Bayesian
inference. We show that this parameter sensitivity analysis can predict which
parameters are practically identifiable. Several Markov chain Monte Carlo
algorithms are compared, with our results suggesting a parallel tempering
algorithm consistently gives the best approach for these systems, which are
shown to frequently exhibit multi-modal posterior distributions.",-0.25066212,-0.27114043,-0.05622283,C
6134,"Introduction
Value of Information (VoI) analysis is a set of concepts and methods rooted in decision theory
that quantifies the expected utility loss due to uncertainty associated with decisions, or the
expected utility gain by conducting further research.","An R implementation of this method is provided as part of the predtools package
(https://github.com/resplab/predtools/).","(1) VoI has been applied in decision analysis
across areas including health technology assessment,(2) environmental risk analysis,(3) and
clinical prediction modeling.",2022-05-12 21:05:19+00:00,Closed-Form Solution of the Unit Normal Loss Integral in Two-Dimensions,stat.CO,"['stat.CO', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Tae Yoon Lee'), arxiv.Result.Author('Paul Gustafson'), arxiv.Result.Author('Mohsen Sadatsafavi')]","In Value of Information (VoI) analysis, the unit normal loss integral (UNLI)
frequently emerges as a solution for the computation of various VoI metrics.
However, one limitation of the UNLI has been that its closed-form solution is
available for only one dimension, and thus can be used for comparisons
involving only two strategies (where it is applied to the scalar incremental
net benefit). We derived a closed-form solution for the two-dimensional UNLI,
enabling closed-form VoI calculations for three strategies. We verified the
accuracy of this method via simulation studies. A case study based on a
three-arm clinical trial was used as an example. VoI methods based on the
closed-form solutions for the UNLI can now be extended to three-decision
comparisons, taking a fraction of a second to compute and not being subject to
Monte Carlo error. An R implementation of this method is provided as part of
the predtools package (https://github.com/resplab/predtools/).",0.23474939,0.2505777,0.20344129,B
6807,"Also, additional       and it opens some promising new directions for further research.","First, we only tested speciﬁc       To summarize, the adding of ”redundant visual information”
line charts and with speciﬁc data perturbations, and a wider range     can provide signiﬁcant beneﬁts when analysing patterns in the data,
of displays and data changes should be studied.",distance metrics should be studied.,2022-05-27 09:39:54+00:00,Finding Patterns in Visualized Data by Adding Redundant Visual Information,stat.CO,"['stat.CO', 'cs.CV']","[arxiv.Result.Author('Salomon Eisler'), arxiv.Result.Author('Joachim Meyer')]","We present ""PATRED"", a technique that uses the addition of redundant
information to facilitate the detection of specific, generally described
patterns in line-charts during the visual exploration of the charts. We
compared different versions of this technique, that differed in the way
redundancy was added, using nine distance metrics (such as Euclidean, Pearson,
Mutual Information and Jaccard) with judgments from data scientists which
served as the ""ground truth"". Results were analyzed with correlations (R2), F1
scores and Mutual Information with the average ranking by the data scientists.
Some distance metrics consistently benefit from the addition of redundant
information, while others are only enhanced for specific types of data
perturbations. The results demonstrate the value of adding redundancy to
improve the identification of patterns in time-series data during visual
exploration.",0.1436181,0.46368212,0.32425728,C
8354,"Finally, Section 6 concludes the paper with ﬁnal practical recommendations and
further research directions.","We observe that our procedure can indeed prevent
degeneracy as T → ∞, provided that some care is taken to build couplings with
good performance.",1.3.,2022-07-03 08:06:23+00:00,On the complexity of backward smoothing algorithms,stat.CO,"['stat.CO', 'stat.ME']","[arxiv.Result.Author('Hai-Dang Dau'), arxiv.Result.Author('Nicolas Chopin')]","In the context of state-space models, backward smoothing algorithms rely on a
backward sampling step, which by default has a O(N^2) complexity (where N is
the number of particles). An alternative implementation relying on rejection
sampling has been proposed in the literature, with stated O(N) complexity. We
show that the running time of such algorithms may have an infinite expectation.
We develop a general framework to establish the convergence and stability of a
large class of backward smoothing algorithms that may be used as more reliable
alternatives. We propose three novel algorithms within this class. The first
one mixes rejection with multinomial sampling; its running time has finite
expectation, and close-to-linear complexity (in a certain class of models). The
second one relies on MCMC, and has deterministic O(N) complexity. The third one
may be used even when the transition of the model is intractable. We perform
numerical experiments to confirm the good properties of these novel algorithms.",-0.34716052,-0.35089225,0.1433143,C
8507,"The development of a robust optimisation technique in this
context requires care and a detailed empirical assessment, and is left as a promising avenue
for further research.","It is interesting to observe
that gradient-free Stein discrepancy leads to a similar performance in both examples, with
the caveat that stochastic optimisation was more prone to occasional failure when gradient-
free Stein discrepancy was used.","14
5 Conclusion

In this paper a gradient-free kernel Stein discrepancy was proposed and studied.",2022-07-06 12:55:53+00:00,Gradient-Free Kernel Stein Discrepancy,stat.CO,"['stat.CO', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Matthew A Fisher'), arxiv.Result.Author('Chris. J Oates')]","Stein discrepancies have emerged as a powerful statistical tool, being
applied to fundamental statistical problems including parameter inference,
goodness-of-fit testing, and sampling. The canonical Stein discrepancies
require the derivatives of a statistical model to be computed, and in return
provide theoretical guarantees of convergence detection and control. However,
for complex statistical models, the stable numerical computation of derivatives
can require bespoke algorithmic development and render Stein discrepancies
impractical. This paper focuses on posterior approximation using Stein
discrepancies, and introduces a collection of non-canonical Stein discrepancies
that are gradient free, meaning that derivatives of the statistical model are
not required. Sufficient conditions for convergence detection and control are
established, and applications to sampling and variational inference are
presented.",-0.29853964,-0.015533924,-0.32014227,C
9491,The method has several limitations that require further research.,"These scores are then shown to provide additional insights into heterogeneity of immune
cell subtype proﬁles, and their contribution to clinical prognosis in patients with localized but
high-risk prostate cancer.","First, while the model (1)
and optimization (3) are formulated for general case of exponential family, our implementation and
numerical results are limited to Gaussian and Binomial proportion cases, as those were suﬃcient for
motivating datasets.",2022-07-29 19:32:26+00:00,Exponential canonical correlation analysis with orthogonal variation,stat.CO,"['stat.CO', 'stat.AP', 'stat.ME']","[arxiv.Result.Author('Dongbang Yuan'), arxiv.Result.Author('Yunfeng Zhang'), arxiv.Result.Author('Shuai Guo'), arxiv.Result.Author('Wenyi Wang'), arxiv.Result.Author('Irina Gaynanova')]","Canonical correlation analysis (CCA) is a standard tool for studying
associations between two data sources; however, it is not designed for data
with count or proportion measurement types. In addition, while CCA uncovers
common signals, it does not elucidate which signals are unique to each data
source. To address these challenges, we propose a new framework for CCA based
on exponential families with explicit modeling of both common and
source-specific signals. Unlike previous methods based on exponential families,
the common signals from our model coincide with canonical variables in Gaussian
CCA, and the unique signals are exactly orthogonal. These modeling differences
lead to a non-trivial estimation via optimization with orthogonality
constraints, for which we develop an iterative algorithm based on a splitting
method. Simulations show on par or superior performance of the proposed method
compared to the available alternatives. We apply the method to analyze
associations between gene expressions and lipids concentrations in nutrigenomic
study, and to analyze associations between two distinct cell-type deconvolution
methods in prostate cancer tumor heterogeneity study.",-0.1534957,-0.29451245,0.15385932,C
10191,"4
Finally, in Section 6 we conclude by discussing potential advantages and disadvantages of
our approach, as well as fruitful avenues for further research.",In Section 5 we demonstrate our new methods on a case study.,"2 Bayesian Additive Regression Trees and Variable
     Importance

We begin with a short review of the concepts of Bayesian additive regression trees and
follow with a a review of both variable importance and variable interactions in a BART
model.",2022-08-18 17:14:14+00:00,Visualizations for Bayesian Additive Regression Trees,stat.CO,['stat.CO'],"[arxiv.Result.Author('Alan Inglis'), arxiv.Result.Author('Andrew Parnell'), arxiv.Result.Author('Cathrine Hurley')]","Tree-based regression and classification has become a standard tool in modern
data science. Bayesian Additive Regression Trees (BART) has in particular
gained wide popularity due its flexibility in dealing with interactions and
non-linear effects. BART is a Bayesian tree-based machine learning method that
can be applied to both regression and classification problems and yields
competitive or superior results when compared to other predictive models. As a
Bayesian model, BART allows the practitioner to explore the uncertainty around
predictions through the posterior distribution. In this paper, we present new
visualization techniques for exploring BART models. We construct conventional
plots to analyze a model's performance and stability as well as create new
tree-based plots to analyze variable importance, interaction, and tree
structure. We employ Value Suppressing Uncertainty Palettes (VSUP) to construct
heatmaps that display variable importance and interactions jointly using color
scale to represent posterior uncertainty. Our new visualizations are designed
to work with the most popular BART R packages available, namely BART, dbarts,
and bartMachine. Our approach is implemented in the R package bartMan (BART
Model ANalysis).",0.7117084,-0.24313536,-0.14568676,B
10192,"Finally, in Section 6 we conclude by
discussing potential advantages and disadvantages of our approach, as well as potential
avenues for further research.","In Section 5
we demonstrate our new methods on a case study.","2 Bayesian Additive Regression Trees and Variable
    Importance

We begin with by reviewing Bayesian additive regression trees and follow with a review
of both variable importance and variable interactions in a BART model.",2022-08-18 17:14:14+00:00,Visualizations for Bayesian Additive Regression Trees,stat.CO,['stat.CO'],"[arxiv.Result.Author('Alan Inglis'), arxiv.Result.Author('Andrew Parnell'), arxiv.Result.Author('Catherine Hurley')]","Tree-based regression and classification has become a standard tool in modern
data science. Bayesian Additive Regression Trees (BART) has in particular
gained wide popularity due its flexibility in dealing with interactions and
non-linear effects. BART is a Bayesian tree-based machine learning method that
can be applied to both regression and classification problems and yields
competitive or superior results when compared to other predictive models. As a
Bayesian model, BART allows the practitioner to explore the uncertainty around
predictions through the posterior distribution. In this paper, we present new
visualization techniques for exploring BART models. We construct conventional
plots to analyze a model's performance and stability as well as create new
tree-based plots to analyze variable importance, interaction, and tree
structure. We employ Value Suppressing Uncertainty Palettes (VSUP) to construct
heatmaps that display variable importance and interactions jointly using color
scale to represent posterior uncertainty. Our new visualizations are designed
to work with the most popular BART R packages available, namely BART, dbarts,
and bartMachine. Our approach is implemented in the R package bartMan (BART
Model ANalysis).",0.7101779,-0.24238703,-0.12818888,B_centroid
10472,"How it may affect the accuracy of solutions, or whether modification of
backpropagation, SSGD and ALS/HGDALS algorithms can consider nonlinear generative models, are
attractive ideas that deserve further research.","For the latter, a possible criticism is that
neural mass3,63 or spiking neuronal modeling64 may better represent the nonlinear nature of
neuronal dynamics.","Moreover, it might be interesting to speculate about what to expect if either ALS/HGDALS or any
other algorithm becomes a proper Oracle method for MEG/EEG signals — as with the MRI algorithm
for MRI/fMRI signals, which provides accurate pictures of in-vivo brains using radio frequency pulses
and acquired spin echo signals.",2022-08-26 19:54:12+00:00,Solving large-scale MEG/EEG source localization and functional connectivity problems simultaneously using state-space models,stat.CO,"['stat.CO', 'cs.NA', 'eess.SP', 'math.DS', 'math.NA', 'stat.AP']","[arxiv.Result.Author('Jose M. Sanchez-Bornot'), arxiv.Result.Author('Roberto C. Sotero'), arxiv.Result.Author('Scott Kelso'), arxiv.Result.Author('Damien Coyle')]","State-space models are used in many fields when dynamics are unobserved.
Popular methods such as the Kalman filter and expectation maximization enable
estimation of these models but pay a high computational cost in large-scale
analysis. In these approaches, sparse inverse covariance estimators can reduce
the cost; however, a trade-off between enforced sparsity and increased
estimation bias occurs, which demands careful consideration in low
signal-to-noise ratio scenarios. We overcome these limitations by 1)
Introducing multiple penalized state-space models based on data-driven
regularization; 2) Implementing novel algorithms such as backpropagation,
state-space gradient descent, and alternating least squares; 3) Proposing an
extension of K-fold cross-validation to evaluate the regularization parameters.
Finally, we solve the simultaneous brain source localization and functional
connectivity problems for simulated and real MEG/EEG signals for thousands of
sources on the cortical surface, demonstrating a substantial improvement over
state-of-the-art methods.",-0.43176574,0.14594182,-0.24114224,A
11893,"These ob-
servations suggest a few seeds are very useful in rendering the data learnable without
sacriﬁcing the anonimity of the rest, and therefore further study of algorithms that
exploit the seed information could be valuable in the practical applications of the
shuﬄed regression problem.","Train error increases in some cases
with the seeds which suggest their role in ameliorating the overﬁtting observed
previously in experiments on real datasets without the seed information.",5.,2022-09-30 17:33:48+00:00,Shuffled linear regression through graduated convex relaxation,stat.CO,"['stat.CO', 'cs.LG']","[arxiv.Result.Author('Efe Onaran'), arxiv.Result.Author('Soledad Villar')]","The shuffled linear regression problem aims to recover linear relationships
in datasets where the correspondence between input and output is unknown. This
problem arises in a wide range of applications including survey data, in which
one needs to decide whether the anonymity of the responses can be preserved
while uncovering significant statistical connections. In this work, we propose
a novel optimization algorithm for shuffled linear regression based on a
posterior-maximizing objective function assuming Gaussian noise prior. We
compare and contrast our approach with existing methods on synthetic and real
data. We show that our approach performs competitively while achieving
empirical running-time improvements. Furthermore, we demonstrate that our
algorithm is able to utilize the side information in the form of seeds, which
recently came to prominence in related problems.",-0.07376577,0.39991346,-0.33378437,A_centroid
12508,"Overall, the associations among TIR and AIR, and their
quantiﬁcation based on ordinal patterns and physical implications, require further study.","Original and amplitude permutations both convey basic vector structures [38], but could they
be further improved for TIR and AIR analysis?",This paper explores the relationships between TIR and AIR based on joint ordinal patterns.,2022-10-15 09:06:27+00:00,Comparative analysis of time irreversibility and amplitude irreversibility based on joint permutation,stat.CO,"['stat.CO', 'nlin.CD']","[arxiv.Result.Author('Wenpo Yao'), arxiv.Result.Author('Wenli Yao'), arxiv.Result.Author('Rongshuang Xu'), arxiv.Result.Author('Jun Wang')]","Although time irreversibility (TIR) and amplitude irreversibility (AIR) are
relevant concepts for nonequilibrium analysis, their association has received
little attention. This paper conducts a systematic comparative analysis of the
relationship between TIR and AIR based on statistical descriptions and
numerical simulations. To simplify the quantification of TIR and AIR, the
amplitude permutation and global information of the associated vector are
combined to produce a joint probability estimation. Chaotic logistic, Henon,
and Lorenz series are generated to evaluate TIR and AIR according to surrogate
theory, and the distributions of joint permutations for these model series are
measured to compare the degree of TIR and AIR. The joint permutation TIR and
AIR are then used to investigate nonequilibrium pathological features in
epileptic electroencephalography data. Test results suggest that epileptic
brain electrical activities, particular those during the onset of seizure,
manifest higher nonequilibrium characteristics. According to the statistical
definitions and targeted pairs of joint permutations in the chaotic model data,
TIR and AIR are fundamentally different nonequilibrium descriptors from time-
and amplitude-reversibility, respectively, and thus require different forms of
numerical analysis. At the same time, TIR and AIR both provide measures for
fluctuation theorems associated with nonequilibrium processes, and have similar
probabilistic differences in the pairs of joint permutations and consistent
outcomes when used to analyze both the model series and real-world signals.
Overall, comparative analysis of TIR and AIR contributes to our understanding
of nonequilibrium features and broadens the scope of quantitative
nonequilibrium measures. Additionally, the construction of joint permutations
contributes to the development of symbolic time series analysis.",0.02170125,0.43854102,0.5613707,C
12960,"Also the scaling
dependence on dimension in multivariate settings is left for further research.","For an exponential family the analysis breaks down into easily manageable parts
but it may well be possible to generalize these results beyond this setting.",Remark 4.3.,2022-10-25 07:44:01+00:00,Federated Bayesian Computation via Piecewise Deterministic Markov Processes,stat.CO,"['stat.CO', 'cs.DC', '62F15 (primary) 65C06, 68W15', 'G.3']","[arxiv.Result.Author('Joris Bierkens'), arxiv.Result.Author('Andrew Duncan')]","When performing Bayesian computations in practice, one is often faced with
the challenge that the constituent model components and/or the data are only
available in a distributed fashion, e.g. due to privacy concerns or sheer
volume. While various methods have been proposed for performing posterior
inference in such federated settings, these either make very strong assumptions
on the data and/or model or otherwise introduce significant bias when the local
posteriors are combined to form an approximation of the target posterior. By
leveraging recently developed methods for Markov Chain Monte Carlo (MCMC) based
on Piecewise Deterministic Markov Processes (PDMPs), we develop a computation
-- and communication -- efficient family of posterior inference algorithms
(Fed-PDMC) which provides asymptotically exact approximations of the full
posterior over a large class of Bayesian models, allowing heterogenous model
and data contributions from each client. We show that communication between
clients and the server preserves the privacy of the individual data sources by
establishing differential privacy guarantees. We quantify the performance of
Fed-PDMC over a class of illustrative analytical case-studies and demonstrate
its efficacy on a number of synthetic examples along with realistic Bayesian
computation benchmarks.",-0.09717333,-0.23676664,0.3047708,C_centroid
13878,"While it is technically possible to allow
                                  Mixture of Experts Distributional Regression 11

feature-dependency of the conditional mixture weights which depend on x in
(8), further research is required to investigate the eﬀect of the entropy-based
penalty in this case.","We investigate the eﬀects of this

tuning parameter ξ in Section 4.4.","4 Numerical Experiments

We now investigate our framework in terms of predictive and estimation
performance.",2022-11-17 20:28:41+00:00,Mixture of Experts Distributional Regression: Implementation Using Robust Estimation with Adaptive First-order Methods,stat.CO,['stat.CO'],"[arxiv.Result.Author('David Rügamer'), arxiv.Result.Author('Florian Pfisterer'), arxiv.Result.Author('Bernd Bischl'), arxiv.Result.Author('Bettina Grün')]","In this work, we propose an efficient implementation of mixtures of experts
distributional regression models which exploits robust estimation by using
stochastic first-order optimization techniques with adaptive learning rate
schedulers. We take advantage of the flexibility and scalability of neural
network software and implement the proposed framework in mixdistreg, an R
software package that allows for the definition of mixtures of many different
families, estimation in high-dimensional and large sample size settings and
robust optimization based on TensorFlow. Numerical experiments with simulated
and real-world data applications show that optimization is as reliable as
estimation via classical approaches in many different settings and that results
may be obtained for complicated scenarios where classical approaches
consistently fail.",0.091893524,0.022299822,-0.31834817,A
