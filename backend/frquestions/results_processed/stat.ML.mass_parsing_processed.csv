Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
19,"‚Ä¢ We shed light on the application of the pseudo-barycenter to L2-objective unsupervised learn-
   ing to achieve diverse data allocation and representation, which provides potential access to
   fairness in unsupervised learning and deserves further study, see Figure 1.","‚Ä¢ We design an algorithm that is computationally eÔ¨Écient in high-dimensional data space,
   by proving a (nearly) closed-form solution of the pseudo-barycenter and the corresponding
   optimal transport maps in both the Gaussian case and the general marginal distribution case.","3
Figure 1: In the upper-left panel, the three distributions are sampled from isotropic Gaussian distribution with diÔ¨Äerent Ô¨Årst
two moments.",2022-01-02 05:05:26+00:00,Fair Data Representation for Machine Learning at the Pareto Frontier,stat.ML,"['stat.ML', 'cs.LG', 'math.PR']","[arxiv.Result.Author('Shizhou Xu'), arxiv.Result.Author('Thomas Strohmer')]","As machine learning powered decision making is playing an increasingly
important role in our daily lives, it is imperative to strive for fairness of
the underlying data processing and algorithms. We propose a pre-processing
algorithm for fair data representation via which L2- objective supervised
learning algorithms result in an estimation of the Pareto frontier between
prediction error and statistical disparity. In particular, the present work
applies the optimal positive definite affine transport maps to approach the
post-processing Wasserstein barycenter characterization of the optimal fair
L2-objective supervised learning via a pre-processing data deformation. We call
the resulting data Wasserstein pseudo-barycenter. Furthermore, we show that the
Wasserstein geodesics from the learning outcome marginals to the barycenter
characterizes the Pareto frontier between L2-loss and total Wasserstein
distance among learning outcome marginals. Thereby, an application of McCann
interpolation generalizes the pseudo-barycenter to a family of data
representations via which L2-objective supervised learning algorithms result in
the Pareto frontier. Numerical simulations underscore the advantages of the
proposed data representation: (1) the pre-processing step is compositive with
arbitrary L2-objective supervised learning methods and unseen data; (2) the
fair representation protects data privacy by preventing the training machine
from direct or indirect access to the sensitive information of the data; (3)
the optimal affine map results in efficient computation of fair supervised
learning on high-dimensional data; (4) experimental results shed light on the
fairness of L2-objective unsupervised learning via the proposed fair data
representation.",0.2527148,0.063772395,0.124652706,A
443,"We view this work as a very Ô¨Årst step to explore loss surfaced based implicit regularization and
hope that the presented ideas spark further research into this direction.","Both quantities depend on the learning rate
ùúÇùë° and the switch between reaching and non-escaping of local minima is driven by the decay of the
learning rate.","We want to stress that in

                                                                             14
many respects, the obtained results in this work are preliminary.",2022-01-12 16:41:34+00:00,On generalization bounds for deep networks based on loss surface implicit regularization,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Masaaki Imaizumi'), arxiv.Result.Author('Johannes Schmidt-Hieber')]","The classical statistical learning theory says that fitting too many
parameters leads to overfitting and poor performance. That modern deep neural
networks generalize well despite a large number of parameters contradicts this
finding and constitutes a major unsolved problem towards explaining the success
of deep learning. The implicit regularization induced by stochastic gradient
descent (SGD) has been regarded to be important, but its specific principle is
still unknown. In this work, we study how the local geometry of the energy
landscape around local minima affects the statistical properties of SGD with
Gaussian gradient noise. We argue that under reasonable assumptions, the local
geometry forces SGD to stay close to a low dimensional subspace and that this
induces implicit regularization and results in tighter bounds on the
generalization error for deep neural networks. To derive generalization error
bounds for neural networks, we first introduce a notion of stagnation sets
around the local minima and impose a local essential convexity property of the
population risk. Under these conditions, lower bounds for SGD to remain in
these stagnation sets are derived. If stagnation occurs, we derive a bound on
the generalization error of deep neural networks involving the spectral norms
of the weight matrices but not the number of network parameters. Technically,
our proofs are based on controlling the change of parameter values in the SGD
iterates and local uniform convergence of the empirical loss functions based on
the entropy of suitable neighborhoods around local minima. Our work attempts to
better connect non-convex optimization and generalization analysis with uniform
convergence.",0.45900568,0.02790752,0.009644605,A
444,"We view this work as a very Ô¨Årst step to explore loss surfaced based implicit regularization and
hope that the presented ideas spark further research into this direction.","The switch between reaching and non-escaping
of local minima is driven by the decay of the learning rate.","We want to stress that in
many respects, the obtained results in this work are preliminary.",2022-01-12 16:41:34+00:00,On generalization bounds for deep networks based on loss surface implicit regularization,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Masaaki Imaizumi'), arxiv.Result.Author('Johannes Schmidt-Hieber')]","The classical statistical learning theory implies that fitting too many
parameters leads to overfitting and poor performance. That modern deep neural
networks generalize well despite a large number of parameters contradicts this
finding and constitutes a major unsolved problem towards explaining the success
of deep learning. While previous work focuses on the implicit regularization
induced by stochastic gradient descent (SGD), we study here how the local
geometry of the energy landscape around local minima affects the statistical
properties of SGD with Gaussian gradient noise. We argue that under reasonable
assumptions, the local geometry forces SGD to stay close to a low dimensional
subspace and that this induces another form of implicit regularization and
results in tighter bounds on the generalization error for deep neural networks.
To derive generalization error bounds for neural networks, we first introduce a
notion of stagnation sets around the local minima and impose a local essential
convexity property of the population risk. Under these conditions, lower bounds
for SGD to remain in these stagnation sets are derived. If stagnation occurs,
we derive a bound on the generalization error of deep neural networks involving
the spectral norms of the weight matrices but not the number of network
parameters. Technically, our proofs are based on controlling the change of
parameter values in the SGD iterates and local uniform convergence of the
empirical loss functions based on the entropy of suitable neighborhoods around
local minima.",0.47809613,0.04742686,0.02830551,A_centroid
445,"Training is             We view this work as a very Ô¨Årst step to explore loss surfaced
                                                                          based implicit regularization and hope that the presented ideas spark
done using Gaussian SGD (2) with multivariate standard normal             further research into this direction.","The number of parameters
ùê∑ = ‚Ñé + ‚Ñé2 lies therefore in the interval [30, 930].","We want to stress that in many
                                                                          respects, the obtained results in this work are preliminary.",2022-01-12 16:41:34+00:00,On generalization bounds for deep networks based on loss surface implicit regularization,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Masaaki Imaizumi'), arxiv.Result.Author('Johannes Schmidt-Hieber')]","The classical statistical learning theory implies that fitting too many
parameters leads to overfitting and poor performance. That modern deep neural
networks generalize well despite a large number of parameters contradicts this
finding and constitutes a major unsolved problem towards explaining the success
of deep learning. While previous work focuses on the implicit regularization
induced by stochastic gradient descent (SGD), we study here how the local
geometry of the energy landscape around local minima affects the statistical
properties of SGD with Gaussian gradient noise. We argue that under reasonable
assumptions, the local geometry forces SGD to stay close to a low dimensional
subspace and that this induces another form of implicit regularization and
results in tighter bounds on the generalization error for deep neural networks.
To derive generalization error bounds for neural networks, we first introduce a
notion of stagnation sets around the local minima and impose a local essential
convexity property of the population risk. Under these conditions, lower bounds
for SGD to remain in these stagnation sets are derived. If stagnation occurs,
we derive a bound on the generalization error of deep neural networks involving
the spectral norms of the weight matrices but not the number of network
parameters. Technically, our proofs are based on controlling the change of
parameter values in the SGD iterates and local uniform convergence of the
empirical loss functions based on the entropy of suitable neighborhoods around
local minima.",0.43372077,0.1474472,0.09863022,A
495,"Hence, further research can focus on determining optimal ways to estimate epidemic
state from noisy data due to delays and testing variance, as well as understanding the impact
that policy levers such as lockdowns and masks have on the spread of infectious disease.","This will require reÔ¨Åned statis-
tical algorithms for estimating the state of an epidemic subject to noisy observations, as well
as an understanding of the mechanisms that policy makers can utilize to inhibit spread of the
disease.","By
unifying the disparate approaches to epidemic forecasting, we hope to take a step towards
reaping the beneÔ¨Åts of both approaches in the design of public health policy for infectious
disease.",2022-01-07 19:42:05+00:00,Unifying Epidemic Models with Mixtures,stat.ML,"['stat.ML', 'cs.LG', 'stat.AP']","[arxiv.Result.Author('Arnab Sarker'), arxiv.Result.Author('Ali Jadbabaie'), arxiv.Result.Author('Devavrat Shah')]","The COVID-19 pandemic has emphasized the need for a robust understanding of
epidemic models. Current models of epidemics are classified as either
mechanistic or non-mechanistic: mechanistic models make explicit assumptions on
the dynamics of disease, whereas non-mechanistic models make assumptions on the
form of observed time series. Here, we introduce a simple mixture-based model
which bridges the two approaches while retaining benefits of both. The model
represents time series of cases and fatalities as a mixture of Gaussian curves,
providing a flexible function class to learn from data compared to traditional
mechanistic models. Although the model is non-mechanistic, we show that it
arises as the natural outcome of a stochastic process based on a networked SIR
framework. This allows learned parameters to take on a more meaningful
interpretation compared to similar non-mechanistic models, and we validate the
interpretations using auxiliary mobility data collected during the COVID-19
pandemic. We provide a simple learning algorithm to identify model parameters
and establish theoretical results which show the model can be efficiently
learned from data. Empirically, we find the model to have low prediction error.
The model is available live at covidpredictions.mit.edu. Ultimately, this
allows us to systematically understand the impacts of interventions on
COVID-19, which is critical in developing data-driven solutions to controlling
epidemics.",-0.1975843,-0.0016963433,-0.23148337,C
558,"Moreover, as in this study only rather low sample sizes were investigated, the behavior of these
approaches in big data settings with larger sample sizes should also be further researched.","Also, mixed problems with numeric and categorical outputs at the same time have to be inves-
tigated.","References

Breiman, L., 2001.",2022-01-14 08:44:25+00:00,Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Lena Schmid'), arxiv.Result.Author('Alexander Gerharz'), arxiv.Result.Author('Andreas Groll'), arxiv.Result.Author('Markus Pauly')]","Tree-based ensembles such as the Random Forest are modern classics among
statistical learning methods. In particular, they are used for predicting
univariate responses. In case of multiple outputs the question arises whether
we separately fit univariate models or directly follow a multivariate approach.
For the latter, several possibilities exist that are, e.g. based on modified
splitting or stopping rules for multi-output regression. In this work we
compare these methods in extensive simulations to help in answering the primary
question when to use multivariate ensemble techniques.",-0.33754522,-0.02144051,0.046641454,C
996,"The authors encourage further research devoted to these topics, but
this is outside the scope of the present work.","It should be pointed out here that only predic-
tive performance of the algorithms in terms of prediction error will be taken
into account, and other aspects, such as the ability to quantify uncertainty
of predictions and the possibility of investigating causality, are features that
should be considered when choosing an algorithm for prediction or optimiza-
tion2.","The paper is organized as follows: section 2 presents the literature back-
ground to the application of ML for the analysis of DOE data; section 3
presents the methodology and details of the simulation study conducted;
section 4 shows the results and section 5 discusses the Ô¨Åndings in depth.",2022-01-25 11:14:36+00:00,Design choice and machine learning model performances,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Rosa Arboretti'), arxiv.Result.Author('Riccardo Ceccato'), arxiv.Result.Author('Luca Pegoraro'), arxiv.Result.Author('Luigi Salmaso')]","An increasing number of publications present the joint application of Design
of Experiments (DOE) and machine learning (ML) as a methodology to collect and
analyze data on a specific industrial phenomenon. However, the literature shows
that the choice of the design for data collection and model for data analysis
is often driven by incidental factors, rather than by statistical or
algorithmic advantages, thus there is a lack of studies which provide
guidelines on what designs and ML models to jointly use for data collection and
analysis. This is the first time in the literature that a paper discusses the
choice of design in relation to the ML model performances. An extensive study
is conducted that considers 12 experimental designs, 7 families of predictive
models, 7 test functions that emulate physical processes, and 8 noise settings,
both homoscedastic and heteroscedastic. The results of the research can have an
immediate impact on the work of practitioners, providing guidelines for
practical applications of DOE and ML.",-0.26400757,0.23654881,-0.008917647,B
997,"The authors encourage further research devoted to these topics, but this is outside the scope
of the present work.","It should be pointed
                                          out here that only predictive performance of the algorithms in terms of prediction error will be taken into
                                          account, and other aspects, such as the ability to quantify uncertainty of predictions and the possibility of
                                          investigating causality, are features that should be considered when choosing an algorithm for prediction or
                                      Published OA in QREI - DOI: 10.1002/qre.3123

optimization 2.","The paper is organized as follows: section 2 presents the literature background to the application of
ML for the analysis of DOE data; section 3 presents the methodology and details of the simulation study
conducted; section 4 shows the results and section 5 discusses the Ô¨Åndings in depth.",2022-01-25 11:14:36+00:00,Design choice and machine learning model performances,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Rosa Arboretti'), arxiv.Result.Author('Riccardo Ceccato'), arxiv.Result.Author('Luca Pegoraro'), arxiv.Result.Author('Luigi Salmaso')]","An increasing number of publications present the joint application of Design
of Experiments (DOE) and machine learning (ML) as a methodology to collect and
analyze data on a specific industrial phenomenon. However, the literature shows
that the choice of the design for data collection and model for data analysis
is often not driven by statistical or algorithmic advantages, thus there is a
lack of studies which provide guidelines on what designs and ML models to
jointly use for data collection and analysis. This article discusses the choice
of design in relation to the ML model performances. A study is conducted that
considers 12 experimental designs, 7 families of predictive models, 7 test
functions that emulate physical processes, and 8 noise settings, both
homoscedastic and heteroscedastic. The results of the research can have an
immediate impact on the work of practitioners, providing guidelines for
practical applications of DOE and ML.",-0.2346119,0.24093574,-0.044823706,B
1446,"This exploratory work aims to share
a potential path forward for further research that may address these concerns with the community.","Therefore, we as a community,
must begin addressing how we can ameliorate our own carbon footprint.","While Ws
is still not ready for commercial usage, we sincerely hope that the community begins to build novel algorithms
over our work on H-Sequence and identify a simpler and cheaper path to train our networks.",2022-02-01 20:30:43+00:00,Deep Layer-wise Networks Have Closed-Form Weights,stat.ML,"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Chieh Wu'), arxiv.Result.Author('Aria Masoomi'), arxiv.Result.Author('Arthur Gretton'), arxiv.Result.Author('Jennifer Dy')]","There is currently a debate within the neuroscience community over the
likelihood of the brain performing backpropagation (BP). To better mimic the
brain, training a network \textit{one layer at a time} with only a ""single
forward pass"" has been proposed as an alternative to bypass BP; we refer to
these networks as ""layer-wise"" networks. We continue the work on layer-wise
networks by answering two outstanding questions. First, $\textit{do they have a
closed-form solution?}$ Second, $\textit{how do we know when to stop adding
more layers?}$ This work proves that the Kernel Mean Embedding is the
closed-form weight that achieves the network global optimum while driving these
networks to converge towards a highly desirable kernel for classification; we
call it the $\textit{Neural Indicator Kernel}$.",-0.10637114,0.052761216,0.1338373,B
1821,We hope that our framework can spur further research on integrative data analysis.,"We also veriÔ¨Åed the theories by extensive nu-
merical experiments.","It
would be interesting to extend our methods to high-dimensional problems with sparsity or other structures,
and develop inferential tools for uncertainty quantiÔ¨Åcation.",2022-02-10 18:55:58+00:00,Adaptive and Robust Multi-task Learning,stat.ML,"['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH', '62F10, 62R07']","[arxiv.Result.Author('Yaqi Duan'), arxiv.Result.Author('Kaizheng Wang')]","We study the multi-task learning problem that aims to simultaneously analyze
multiple datasets collected from different sources and learn one model for each
of them. We propose a family of adaptive methods that automatically utilize
possible similarities among those tasks while carefully handling their
differences. We derive sharp statistical guarantees for the methods and prove
their robustness against outlier tasks. Numerical experiments on synthetic and
real datasets demonstrate the efficacy of our new methods.",-0.06935513,-0.06916457,-0.07156877,C
1822,"ARMUL                           Benchmarks

Vanilla Clustered Low-rank  STL Data pooling Clustered Low-rank

1.12 (0.25) 0.84 (0.22) 0.80 (0.19) 1.95 (0.32) 3.48 (0.39) 2.15 (0.33) 1.30 (0.23)

further research on integrative data analysis.","We hope that our framework can spur

                         23
Table 1: Testing errors of ARMUL and benchmarks on the HAR dataset.","It would be interesting to extend our methods to
high-dimensional problems with sparsity or other structures, and develop inferential tools for uncer-
tainty quantiÔ¨Åcation.",2022-02-10 18:55:58+00:00,Adaptive and Robust Multi-task Learning,stat.ML,"['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH', '62F10, 62R07']","[arxiv.Result.Author('Yaqi Duan'), arxiv.Result.Author('Kaizheng Wang')]","We study the multi-task learning problem that aims to simultaneously analyze
multiple datasets collected from different sources and learn one model for each
of them. We propose a family of adaptive methods that automatically utilize
possible similarities among those tasks while carefully handling their
differences. We derive sharp statistical guarantees for the methods and prove
their robustness against outlier tasks. Numerical experiments on synthetic and
real datasets demonstrate the efficacy of our new methods.",-0.1521274,-0.039562747,0.15692517,C
2156,"(2021) further study the bias and consistency of IJ estimator and propose
alternative variance estimators by classical jackknife and regression approaches.",Peng et al.,"An important line of research that underlies statistical inference for random forests is the
asymptotic normality of the forests estimator.",2022-02-18 03:35:47+00:00,On Variance Estimation of Random Forests,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Tianning Xu'), arxiv.Result.Author('Ruoqing Zhu'), arxiv.Result.Author('Xiaofeng Shao')]","Ensemble methods based on subsampling, such as random forests, are popular in
applications due to their high predictive accuracy. Existing literature views a
random forest prediction as an infinite-order incomplete U-statistic to
quantify its uncertainty. However, these methods focus on a small subsampling
size of each tree, which is theoretically valid but practically limited. This
paper develops an unbiased variance estimator based on incomplete U-statistics,
which allows the tree size to be comparable with the overall sample size,
making statistical inference possible in a broader range of real applications.
Simulation results demonstrate that our estimators enjoy lower bias and more
accurate confidence interval coverage without additional computational costs.
We also propose a local smoothing procedure to reduce the variation of our
estimator, which shows improved numerical performance when the number of trees
is relatively small. Further, we investigate the ratio consistency of our
proposed variance estimator under specific scenarios. In particular, we develop
a new ""double U-statistic"" formulation to analyze the Hoeffding decomposition
of the estimator's variance.",-0.11304577,-0.033516414,-0.38142198,C
2157,"(2021) further study the bias and consistency of the IJ
estimator and propose alternative variance estimators with classical jackknife and regression approaches.",Peng et al.,"An important line of research that underlies statistical inference for random forests is the asymptotic
normality of the forests estimator.",2022-02-18 03:35:47+00:00,On Variance Estimation of Random Forests,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Tianning Xu'), arxiv.Result.Author('Ruoqing Zhu'), arxiv.Result.Author('Xiaofeng Shao')]","Ensemble methods, such as random forests, are popular in applications due to
their high predictive accuracy. Existing literature views a random forest
prediction as an infinite-order incomplete U-statistic to quantify its
uncertainty. However, these methods focus on a small subsampling size of each
tree, which is theoretically valid but practically limited. This paper develops
an unbiased variance estimator based on incomplete U-statistics, which allows
the tree size to be comparable with the overall sample size, making statistical
inference possible in a broader range of real applications. Simulation results
demonstrate that our estimators enjoy lower bias and a more accurate coverage
rate without additional computational costs. We also propose a local smoothing
procedure to reduce the variation of our estimator, which shows improved
numerical performance when the number of trees is relatively small. Further, we
investigate the ratio consistency of our proposed variance estimator under
specific scenarios. In particular, we develop a new ""double U-statistic""
formulation to analyze the Hoeffding decomposition of the estimator's variance.",-0.11289424,-0.034438618,-0.373765,C
2158,"(2021) further study the bias and consistency
of the IJ estimator and propose alternative variance estimators with classical jackknife and
regression approaches.",Peng et al.,Many questions remain unanswered for variance estimation of random forests.,2022-02-18 03:35:47+00:00,On Variance Estimation of Random Forests,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Tianning Xu'), arxiv.Result.Author('Ruoqing Zhu'), arxiv.Result.Author('Xiaofeng Shao')]","Ensemble methods, such as random forests, are popular in applications due to
their high predictive accuracy. Existing literature views a random forest
prediction as an infinite-order incomplete U-statistic to quantify its
uncertainty. However, these methods focus on a small subsampling size of each
tree, which is theoretically valid but practically limited. This paper develops
an unbiased variance estimator based on incomplete U-statistics, which allows
the tree size to be comparable with the overall sample size, making statistical
inference possible in a broader range of real applications. Simulation results
demonstrate that our estimators enjoy lower bias and more accurate coverage
rate without additional computational costs. We also propose a local smoothing
procedure to reduce the variation of our estimator, which shows improved
numerical performance when the number of trees is relatively small. Further, we
investigate the ratio consistency of our proposed variance estimator under
specific scenarios. In particular, we develop a new ""double U-statistic""
formulation to analyze the Hoeffding decomposition of the estimator's variance.",-0.083216734,0.041604966,-0.32330012,C
2175,"As an outlook and extension to this work, two major steps can be consid-
ered in further research: Incorporating additional data sources or using other
models, such as e.g.","Since
the values of the model‚Äôs goodness-of-Ô¨Åt are neither particularly bad or good,
these Ô¨Åndings should be viewed with some caution.","boosting approaches or deep learning methods such as
neural networks.",2022-02-18 13:20:47+00:00,Churn modeling of life insurance policies via statistical and machine learning methods -- Analysis of important features,stat.ML,"['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO']","[arxiv.Result.Author('Andreas Groll'), arxiv.Result.Author('Carsten Wasserfuhr'), arxiv.Result.Author('Leonid Zeldin')]","Life assurance companies typically possess a wealth of data covering multiple
systems and databases. These data are often used for analyzing the past and for
describing the present. Taking account of the past, the future is mostly
forecasted by traditional statistical methods. So far, only a few attempts were
undertaken to perform estimations by means of machine learning approaches. In
this work, the individual contract cancellation behavior of customers within
two partial stocks is modeled by the aid of various classification methods.
Partial stocks of private pension and endowment policy are considered. We
describe the data used for the modeling, their structured and in which way they
are cleansed. The utilized models are calibrated on the basis of an extensive
tuning process, then graphically evaluated regarding their goodness-of-fit and
with the help of a variable relevance concept, we investigate which features
notably affect the individual contract cancellation behavior.",-0.110190466,0.38072073,0.03618432,B
2285,"We hope these developments prompt further study of other
possible choices for approximate priors.","For
stationary kernels, random Fourier feature methods are particularly attractive
due to their good behavior with respect to dimension, but are by no means
the only choice.","Pathwise approximations are also of interest as a potential organizing prin-
ciple for designing Gaussian process software packages.",2022-02-22 01:42:57+00:00,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Alexander Terenin')],"Bayesian learning using Gaussian processes provides a foundational framework
for making decisions in a manner that balances what is known with what could be
learned by gathering data. In this dissertation, we develop techniques for
broadening the applicability of Gaussian processes. This is done in two ways.
Firstly, we develop pathwise conditioning techniques for Gaussian processes,
which allow one to express posterior random functions as prior random functions
plus a dependent update term. We introduce a wide class of efficient
approximations built from this viewpoint, which can be randomly sampled once in
advance, and evaluated at arbitrary locations without any subsequent
stochasticity. This key property improves efficiency and makes it simpler to
deploy Gaussian process models in decision-making settings. Secondly, we
develop a collection of Gaussian process models over non-Euclidean spaces,
including Riemannian manifolds and graphs. We derive fully constructive
expressions for the covariance kernels of scalar-valued Gaussian processes on
Riemannian manifolds and graphs. Building on these ideas, we describe a
formalism for defining vector-valued Gaussian processes on Riemannian
manifolds. The introduced techniques allow all of these models to be trained
using standard computational methods. In total, these contributions make
Gaussian processes easier to work with and allow them to be used within a wider
class of domains in an effective and principled manner. This, in turn, makes it
possible to potentially apply Gaussian processes to novel decision-making
settings.",0.27248454,0.084104024,0.03349179,A
2286,"We hope these developments prompt further study of other
possible choices for approximate priors.","For
stationary kernels, random Fourier feature methods are particularly attractive
due to their good behavior with respect to dimension, but are by no means
the only choice.","Pathwise approximations are also of interest as a potential organizing prin-
ciple for designing Gaussian process software packages.",2022-02-22 01:42:57+00:00,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Alexander Terenin')],"Bayesian learning using Gaussian processes provides a foundational framework
for making decisions in a manner that balances what is known with what could be
learned by gathering data. In this dissertation, we develop techniques for
broadening the applicability of Gaussian processes. This is done in two ways.
Firstly, we develop pathwise conditioning techniques for Gaussian processes,
which allow one to express posterior random functions as prior random functions
plus a dependent update term. We introduce a wide class of efficient
approximations built from this viewpoint, which can be randomly sampled once in
advance, and evaluated at arbitrary locations without any subsequent
stochasticity. This key property improves efficiency and makes it simpler to
deploy Gaussian process models in decision-making settings. Secondly, we
develop a collection of Gaussian process models over non-Euclidean spaces,
including Riemannian manifolds and graphs. We derive fully constructive
expressions for the covariance kernels of scalar-valued Gaussian processes on
Riemannian manifolds and graphs. Building on these ideas, we describe a
formalism for defining vector-valued Gaussian processes on Riemannian
manifolds. The introduced techniques allow all of these models to be trained
using standard computational methods. In total, these contributions make
Gaussian processes easier to work with and allow them to be used within a wider
class of domains in an effective and principled manner. This, in turn, makes it
possible to potentially apply Gaussian processes to novel decision-making
settings.",0.27248454,0.084104024,0.03349179,A
2287,"We hope these developments prompt further study of other
possible choices for approximate priors.","For
stationary kernels, random Fourier feature methods are particularly attractive
due to their good behavior with respect to dimension, but are by no means
the only choice.","Pathwise approximations are also of interest as a potential organizing prin-
ciple for designing Gaussian process software packages.",2022-02-22 01:42:57+00:00,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Alexander Terenin')],"Bayesian learning using Gaussian processes provides a foundational framework
for making decisions in a manner that balances what is known with what could be
learned by gathering data. In this dissertation, we develop techniques for
broadening the applicability of Gaussian processes. This is done in two ways.
Firstly, we develop pathwise conditioning techniques for Gaussian processes,
which allow one to express posterior random functions as prior random functions
plus a dependent update term. We introduce a wide class of efficient
approximations built from this viewpoint, which can be randomly sampled once in
advance, and evaluated at arbitrary locations without any subsequent
stochasticity. This key property improves efficiency and makes it simpler to
deploy Gaussian process models in decision-making settings. Secondly, we
develop a collection of Gaussian process models over non-Euclidean spaces,
including Riemannian manifolds and graphs. We derive fully constructive
expressions for the covariance kernels of scalar-valued Gaussian processes on
Riemannian manifolds and graphs. Building on these ideas, we describe a
formalism for defining vector-valued Gaussian processes on Riemannian
manifolds. The introduced techniques allow all of these models to be trained
using standard computational methods. In total, these contributions make
Gaussian processes easier to work with and allow them to be used within a wider
class of domains in an effective and principled manner. This, in turn, makes it
possible to potentially apply Gaussian processes to novel decision-making
settings.",0.27248454,0.084104024,0.03349179,A
2423,"For example, snapshot en-
                                                and further study empirically various eÔ¨Äects          sembles [Huang et al., 2017, Garipov et al., 2018] re-
                                                such as transition between the two regimes,           duce the ensemble training time (without signiÔ¨Åcantly
                                                scaling of ensemble performance with the net-         aÔ¨Äecting the storage and inference time).","We con-               A lightweight ensemble attempts to retain the accu-
                                                Ô¨Årm our theoretical prediction with a wide            racy gain from ensembling while relaxing requirements
                                                range of experiments with Ô¨Ånite networks,             for a particular resource.","Lightweight
                                                work width and number of models, and de-              ensembles typically have a lower accuracy than the
                                                pendence of performance on a number of ar-            standard independent ensembles of the same size, be-
                                                chitecture and hyperparameter choices.",2022-02-24 18:55:41+00:00,Embedded Ensembles: Infinite Width Limit and Operating Regimes,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Maksim Velikanov'), arxiv.Result.Author('Roman Kail'), arxiv.Result.Author('Ivan Anokhin'), arxiv.Result.Author('Roman Vashurin'), arxiv.Result.Author('Maxim Panov'), arxiv.Result.Author('Alexey Zaytsev'), arxiv.Result.Author('Dmitry Yarotsky')]","A memory efficient approach to ensembling neural networks is to share most
weights among the ensembled models by means of a single reference network. We
refer to this strategy as Embedded Ensembling (EE); its particular examples are
BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a
systematic theoretical and empirical analysis of embedded ensembles with
different number of models. Theoretically, we use a Neural-Tangent-Kernel-based
approach to derive the wide network limit of the gradient descent dynamics. In
this limit, we identify two ensemble regimes - independent and collective -
depending on the architecture and initialization strategy of ensemble models.
We prove that in the independent regime the embedded ensemble behaves as an
ensemble of independent models. We confirm our theoretical prediction with a
wide range of experiments with finite networks, and further study empirically
various effects such as transition between the two regimes, scaling of ensemble
performance with the network width and number of models, and dependence of
performance on a number of architecture and hyperparameter choices.",-0.025699943,0.2417772,-0.0988605,B
2508,"Also, we leave it further study whether our method can be
improved to achieve a faster rate, or whether there is other quantization schemes that leads to
faster estimator.","Based on similar
underlying rationale, it is consistent that our heavy-tailed results on the all three models are
square root of the minimax rates.","3 Sparse Linear Regression

We intend to establish our results for sparse linear regression (Section 3) and low-rank matrix

completion (Section 4) under the uniÔ¨Åed framework of trace regression.",2022-02-26 15:13:04+00:00,High Dimensional Statistical Estimation under One-bit Quantization,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Junren Chen'), arxiv.Result.Author('Cheng-Long Wang'), arxiv.Result.Author('Michael K. Ng'), arxiv.Result.Author('Di Wang')]","Compared with data with high precision, one-bit (binary) data are preferable
in many applications because of the efficiency in signal storage, processing,
transmission, and enhancement of privacy. In this paper, we study three
fundamental statistical estimation problems, i.e., sparse covariance matrix
estimation, sparse linear regression, and low-rank matrix completion via binary
data arising from an easy-to-implement one-bit quantization process that
contains truncation, dithering and quantization as typical steps. Under both
sub-Gaussian and heavy-tailed regimes, new estimators that handle
high-dimensional scaling are proposed. In sub-Gaussian case, we show that our
estimators achieve minimax rates up to logarithmic factors, hence the
quantization nearly costs nothing from the perspective of statistical learning
rate. In heavy-tailed case, we truncate the data before dithering to achieve a
bias-variance trade-off, which results in estimators embracing convergence
rates that are the square root of the corresponding minimax rates. Experimental
results on synthetic data are reported to support and demonstrate the
statistical properties of our estimators under one-bit quantization.",0.07633846,0.06944044,0.045392394,A
2509,"Finally, we point out several potential directions for further study.","Besides, some technical proofs may be interesting themselves, for example,
the exponentially-decaying probability tail obtained in Theorem 3, see Remark 1.","First of all, essential
degradation occurs in the obtained statistical rates of our estimators under heavy-tailed data.",2022-02-26 15:13:04+00:00,High Dimensional Statistical Estimation under One-bit Quantization,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Junren Chen'), arxiv.Result.Author('Cheng-Long Wang'), arxiv.Result.Author('Michael K. Ng'), arxiv.Result.Author('Di Wang')]","Compared with data with high precision, one-bit (binary) data are preferable
in many applications because of the efficiency in signal storage, processing,
transmission, and enhancement of privacy. In this paper, we study three
fundamental statistical estimation problems, i.e., sparse covariance matrix
estimation, sparse linear regression, and low-rank matrix completion via binary
data arising from an easy-to-implement one-bit quantization process that
contains truncation, dithering and quantization as typical steps. Under both
sub-Gaussian and heavy-tailed regimes, new estimators that handle
high-dimensional scaling are proposed. In sub-Gaussian case, we show that our
estimators achieve minimax rates up to logarithmic factors, hence the
quantization nearly costs nothing from the perspective of statistical learning
rate. In heavy-tailed case, we truncate the data before dithering to achieve a
bias-variance trade-off, which results in estimators embracing convergence
rates that are the square root of the corresponding minimax rates. Experimental
results on synthetic data are reported to support and demonstrate the
statistical properties of our estimators under one-bit quantization.",0.006393306,-0.24541989,-0.294688,C
2510,"It needs further research how
to address these issues so that the proposed methods are more practical in real problems.","Moreover, we report
that our estimation based on one-bit quantized data may not be satisfactory in some situations,
for example, when the underlying entries vary a lot in magnitude (i.e., absolute value), since a
common dithering scale Œ≥ cannot well handle the quantization.","References

 [1] Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin.",2022-02-26 15:13:04+00:00,High Dimensional Statistical Estimation under One-bit Quantization,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Junren Chen'), arxiv.Result.Author('Cheng-Long Wang'), arxiv.Result.Author('Michael K. Ng'), arxiv.Result.Author('Di Wang')]","Compared with data with high precision, one-bit (binary) data are preferable
in many applications because of the efficiency in signal storage, processing,
transmission, and enhancement of privacy. In this paper, we study three
fundamental statistical estimation problems, i.e., sparse covariance matrix
estimation, sparse linear regression, and low-rank matrix completion via binary
data arising from an easy-to-implement one-bit quantization process that
contains truncation, dithering and quantization as typical steps. Under both
sub-Gaussian and heavy-tailed regimes, new estimators that handle
high-dimensional scaling are proposed. In sub-Gaussian case, we show that our
estimators achieve minimax rates up to logarithmic factors, hence the
quantization nearly costs nothing from the perspective of statistical learning
rate. In heavy-tailed case, we truncate the data before dithering to achieve a
bias-variance trade-off, which results in estimators embracing convergence
rates that are the square root of the corresponding minimax rates. Experimental
results on synthetic data are reported to support and demonstrate the
statistical properties of our estimators under one-bit quantization.",0.056625813,-0.16055502,0.04026552,A
2679,"For this kind of dataset, it is necessary to further study its
(3) FSDPC and DGDPC algorithms can maintain similar                  inherent data structure characteristics, to Ô¨Ånd a more effective

14                                                                                                    VOLUME 4, 2016
Author et al.",applicability of DPC series algorithm is stronger relatively.,": Preparation of Papers for IEEE TRANSACTIONS and JOURNALS

FIGURE 9.",2022-03-02 09:29:40+00:00,A density peaks clustering algorithm with sparse search and K-d tree,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Yunxiao Shan'), arxiv.Result.Author('Shu Li'), arxiv.Result.Author('Fuxiang Li'), arxiv.Result.Author('Yuxin Cui'), arxiv.Result.Author('Shuai Li'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Xiang Li')]","Density peaks clustering has become a nova of clustering algorithm because of
its simplicity and practicality. However, there is one main drawback: it is
time-consuming due to its high computational complexity. Herein, a density
peaks clustering algorithm with sparse search and K-d tree is developed to
solve this problem. Firstly, a sparse distance matrix is calculated by using
K-d tree to replace the original full rank distance matrix, so as to accelerate
the calculation of local density. Secondly, a sparse search strategy is
proposed to accelerate the computation of relative-separation with the
intersection between the set of $k$ nearest neighbors and the set consisting of
the data points with larger local density for any data point. Furthermore, a
second-order difference method for decision values is adopted to determine the
cluster centers adaptively. Finally, experiments are carried out on datasets
with different distribution characteristics, by comparing with other six
state-of-the-art clustering algorithms. It is proved that the algorithm can
effectively reduce the computational complexity of the original DPC from
$O(n^2K)$ to $O(n(n^{1-1/K}+k))$. Especially for larger datasets, the
efficiency is elevated more remarkably. Moreover, the clustering accuracy is
also improved to a certain extent. Therefore, it can be concluded that the
overall performance of the newly proposed algorithm is excellent.",-0.08950204,-0.07187766,0.24036922,B
2950,"Exploring this idea
in more detail will be another topic of further research.","It should not be prohibitive to incorporate
exogenous features in the statistics t(x) in AgraSSt, for example using ideas
from graph attention networks [VeliÀáckovi¬¥c et al., 2018].","21
Acknowledgements

G.R.",2022-03-07 19:12:40+00:00,AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Wenkai Xu'), arxiv.Result.Author('Gesine Reinert')]","We propose and analyse a novel statistical procedure, coined AgraSSt, to
assess the quality of graph generators that may not be available in explicit
form. In particular, AgraSSt can be used to determine whether a learnt graph
generating process is capable of generating graphs that resemble a given input
graph. Inspired by Stein operators for random graphs, the key idea of AgraSSt
is the construction of a kernel discrepancy based on an operator obtained from
the graph generator. AgraSSt can provide interpretable criticisms for a graph
generator training procedure and help identify reliable sample batches for
downstream tasks. Using Stein`s method we give theoretical guarantees for a
broad class of random graph models. We provide empirical results on both
synthetic input graphs with known graph generation procedures, and real-world
input graphs that the state-of-the-art (deep) generative models for graphs are
trained on.",-0.15752381,-0.18172117,-0.13426287,C
2951,"Exploring this idea in more detail will
be another topic of further research.","It would
also be possible to incorporate exogenous features in the statistics t(x) in AgraSSt, for example using
ideas from graph attention networks [VelicÀákovic¬¥ et al., 2018].","As AgraSSt depends on the chosen summary statistic t(x), results have to be interpreted with regards
to the respective conditional distributions.",2022-03-07 19:12:40+00:00,AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators,stat.ML,"['stat.ML', 'cs.LG', '60E05, 62E17, 60B20, 05C80']","[arxiv.Result.Author('Wenkai Xu'), arxiv.Result.Author('Gesine Reinert')]","We propose and analyse a novel statistical procedure, coined AgraSSt, to
assess the quality of graph generators that may not be available in explicit
form. In particular, AgraSSt can be used to determine whether a learnt graph
generating process is capable of generating graphs that resemble a given input
graph. Inspired by Stein operators for random graphs, the key idea of AgraSSt
is the construction of a kernel discrepancy based on an operator obtained from
the graph generator. AgraSSt can provide interpretable criticisms for a graph
generator training procedure and help identify reliable sample batches for
downstream tasks. Using Stein`s method we give theoretical guarantees for a
broad class of random graph models. We provide empirical results on both
synthetic input graphs with known graph generation procedures, and real-world
input graphs that the state-of-the-art (deep) generative models for graphs are
trained on.",-0.17190665,-0.25668007,-0.15279347,C
2952,"Exploring this idea in more detail will
be another topic of further research.","It would
also be possible to incorporate exogenous features in the statistics t(x) in AgraSSt, for example using
ideas from graph attention networks [VelicÀákovic¬¥ et al., 2018].","As AgraSSt depends on the chosen summary statistic t(x), results have to be interpreted with regards
to the respective conditional distributions.",2022-03-07 19:12:40+00:00,AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators,stat.ML,"['stat.ML', 'cs.LG', '60E05, 62E17, 60B20, 05C80']","[arxiv.Result.Author('Wenkai Xu'), arxiv.Result.Author('Gesine Reinert')]","We propose and analyse a novel statistical procedure, coined AgraSSt, to
assess the quality of graph generators that may not be available in explicit
form. In particular, AgraSSt can be used to determine whether a learnt graph
generating process is capable of generating graphs that resemble a given input
graph. Inspired by Stein operators for random graphs, the key idea of AgraSSt
is the construction of a kernel discrepancy based on an operator obtained from
the graph generator. AgraSSt can provide interpretable criticisms for a graph
generator training procedure and help identify reliable sample batches for
downstream tasks. Using Stein`s method we give theoretical guarantees for a
broad class of random graph models. We provide empirical results on both
synthetic input graphs with known graph generation procedures, and real-world
input graphs that the state-of-the-art (deep) generative models for graphs are
trained on.",-0.17190665,-0.25668007,-0.15279347,C
3109,"Keywords: Nonlinear Filtering, Deep Learning, Stochastic PDE Ap-
                                                  proximation

                                         1 Introduction

                                         In this paper we present a further study of the deep learning method devel-
                                         oped in [4] on the example of the Benes Ô¨Ålter.","Further we present the Ô¨Årst study of the neural
                                                  network method with an adaptive domain for the Benes model.","The algorithm is derived from
                                         the splitting method for SPDEs and replaces the PDE approximation step by
                                         a neural network representation and learning algorithm.",2022-03-09 14:08:38+00:00,Deep Learning for the Benes Filter,stat.ML,"['stat.ML', 'cs.AI', 'cs.NA', 'math.NA', 'math.PR', '60C30']",[arxiv.Result.Author('Alexander Lobbe')],"The Benes filter is a well-known continuous-time stochastic filtering model
in one dimension that has the advantage of being explicitly solvable. From an
evolution equation point of view, the Benes filter is also the solution of the
filtering equations given a particular set of coefficient functions. In
general, the filtering stochastic partial differential equations (SPDE) arise
as the evolution equations for the conditional distribution of an underlying
signal given partial, and possibly noisy, observations. Their numerical
approximation presents a central issue for theoreticians and practitioners
alike, who are actively seeking accurate and fast methods, especially for such
high-dimensional settings as numerical weather prediction, for example. In this
paper we present a brief study of a new numerical method based on the mesh-free
neural network representation of the density of the solution of the Benes model
achieved by deep learning. Based on the classical SPDE splitting method, our
algorithm includes a recursive normalisation procedure to recover the
normalised conditional distribution of the signal process. Within the
analytically tractable setting of the Benes filter, we discuss the role of
nonlinearity in the filtering model equations for the choice of the domain of
the neural network. Further we present the first study of the neural network
method with an adaptive domain for the Benes model.",0.31426942,0.25507057,0.08373159,A
3249,"We further study the local stability and global
                                                  convergence of gradient descent-ascent methods towards consistent equilibrium.","The discriminator deÔ¨Åned from second-order statistical moments
                                                  can result in non-existence of Nash equilibrium, existence of consistent non-Nash equilibrium,
                                                  or existence and uniqueness of consistent Nash equilibrium, depending on whether symmetry
                                                  properties of the generator family are respected.","Keywords: GANs, Nash equilibrium, moment-matching, stationary process, statistical consis-
                                          tency

                                          1 Introduction

                                          Estimating the probability distribution of data samples is a classical problem in statistics and ma-
                                          chine learning.",2022-03-14 14:30:23+00:00,On the Nash equilibrium of moment-matching GANs for stationary Gaussian processes,stat.ML,"['stat.ML', 'cs.GT', 'cs.LG']",[arxiv.Result.Author('Sixin Zhang')],"Generative Adversarial Networks (GANs) learn an implicit generative model
from data samples through a two-player game. In this paper, we study the
existence of Nash equilibrium of the game which is consistent as the number of
data samples grows to infinity. In a realizable setting where the goal is to
estimate the ground-truth generator of a stationary Gaussian process, we show
that the existence of consistent Nash equilibrium depends crucially on the
choice of the discriminator family. The discriminator defined from second-order
statistical moments can result in non-existence of Nash equilibrium, existence
of consistent non-Nash equilibrium, or existence and uniqueness of consistent
Nash equilibrium, depending on whether symmetry properties of the generator
family are respected. We further study the local stability and global
convergence of gradient descent-ascent methods towards consistent equilibrium.",0.36196953,-0.043497473,-0.18559118,A
3250,"We further study empirically the local stability and global convergence
                                               of gradient descent-ascent methods towards consistent equilibrium.","The discriminator deÔ¨Åned from second-order statistical moments can result in
                                               non-existence of Nash equilibrium, existence of consistent non-Nash equilibrium, or existence and
                                               uniqueness of consistent Nash equilibrium, depending on whether symmetry properties of the gen-
                                               erator family are respected.","Keywords: GANs, Nash equilibrium, moment-matching, stationary process, statistical consistency

                                         1.",2022-03-14 14:30:23+00:00,On the Nash equilibrium of moment-matching GANs for stationary Gaussian processes,stat.ML,"['stat.ML', 'cs.GT', 'cs.LG']",[arxiv.Result.Author('Sixin Zhang')],"Generative Adversarial Networks (GANs) learn an implicit generative model
from data samples through a two-player game. In this paper, we study the
existence of Nash equilibrium of the game which is consistent as the number of
data samples grows to infinity. In a realizable setting where the goal is to
estimate the ground-truth generator of a stationary Gaussian process, we show
that the existence of consistent Nash equilibrium depends crucially on the
choice of the discriminator family. The discriminator defined from second-order
statistical moments can result in non-existence of Nash equilibrium, existence
of consistent non-Nash equilibrium, or existence and uniqueness of consistent
Nash equilibrium, depending on whether symmetry properties of the generator
family are respected. We further study the local stability and global
convergence of gradient descent-ascent methods towards consistent equilibrium.",0.33779675,-0.095957294,-0.2363022,A
3251,"We further study empirically the local stability and global convergence
                                                of gradient descent-ascent methods towards consistent equilibrium.","The discriminator deÔ¨Åned from second-order statistical moments can result in
                                                non-existence of Nash equilibrium, existence of consistent non-Nash equilibrium, or existence and
                                                uniqueness of consistent Nash equilibrium, depending on whether symmetry properties of the gen-
                                                erator family are respected.","Keywords: GANs, Nash equilibrium, moment-matching, stationary process, statistical consistency

                                          1.",2022-03-14 14:30:23+00:00,On the Nash equilibrium of moment-matching GANs for stationary Gaussian processes,stat.ML,"['stat.ML', 'cs.GT', 'cs.LG']",[arxiv.Result.Author('Sixin Zhang')],"Generative Adversarial Networks (GANs) learn an implicit generative model
from data samples through a two-player game. In this paper, we study the
existence of Nash equilibrium of the game which is consistent as the number of
data samples grows to infinity. In a realizable setting where the goal is to
estimate the ground-truth generator of a stationary Gaussian process, we show
that the existence of consistent Nash equilibrium depends crucially on the
choice of the discriminator family. The discriminator defined from second-order
statistical moments can result in non-existence of Nash equilibrium, existence
of consistent non-Nash equilibrium, or existence and uniqueness of consistent
Nash equilibrium, depending on whether symmetry properties of the generator
family are respected. We further study empirically the local stability and
global convergence of gradient descent-ascent methods towards consistent
equilibrium.",0.33779675,-0.095957294,-0.2363022,A
3327,"In the following, we further study the scores of the algorithms with statistical
tests, the simplex-plot, and the cumulative diÔ¨Äerence-plot.","However,
in this case, the three visualization methods considered (histogram, box-plot, and violin-plot)
have not been able to summarize the scores obtained with the algorithms in a way that enables an
easy comparison.","6.2.1 Mann-Whitney test

Applying the Mann-Whitney test we obtain a p-value of p = 0.035, lower than the usually used
0.05 threshold.",2022-03-15 13:37:03+00:00,Comparing two samples through stochastic dominance: a graphical approach,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Etor Arza'), arxiv.Result.Author('Josu Ceberio'), arxiv.Result.Author('Ekhi√±e Irurozki'), arxiv.Result.Author('Aritz P√©rez')]","Non-deterministic measurements are common in real-world scenarios: the
performance of a stochastic optimization algorithm or the total reward of a
reinforcement learning agent in a chaotic environment are just two examples in
which unpredictable outcomes are common. These measures can be modeled as
random variables and compared among each other via their expected values or
more sophisticated tools such as null hypothesis statistical tests. In this
paper, we propose an alternative framework to visually compare two samples
according to their estimated cumulative distribution functions. First, we
introduce a dominance measure for two random variables that quantifies the
proportion in which the cumulative distribution function of one of the random
variables scholastically dominates the other one. Then, we present a graphical
method that decomposes in quantiles i) the proposed dominance measure and ii)
the probability that one of the random variables takes lower values than the
other. With illustrative purposes, we re-evaluate the experimentation of an
already published work with the proposed methodology and we show that
additional conclusions (missed by the rest of the methods) can be inferred.
Additionally, the software package RVCompare was created as a convenient way of
applying and experimenting with the proposed framework.",-0.33667195,-0.39646828,0.57500774,C
3328,"In the following, we further study the scores of the algorithms with statistical
tests, the simplex-plot, and the cumulative diÔ¨Äerence-plot.","in this case, the three visualization methods considered (histogram, box-plot, and violin-plot)
have not been able to summarize the scores obtained with the algorithms in a way that enables an
easy comparison.","6.2.1 Mann-Whitney test

Applying the Mann-Whitney test we obtain a p-value of p = 0.035, lower than the usually used
0.05 threshold.",2022-03-15 13:37:03+00:00,Comparing two samples through stochastic dominance: a graphical approach,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Etor Arza'), arxiv.Result.Author('Josu Ceberio'), arxiv.Result.Author('Ekhi√±e Irurozki'), arxiv.Result.Author('Aritz P√©rez')]","Non-deterministic measurements are common in real-world scenarios: the
performance of a stochastic optimization algorithm or the total reward of a
reinforcement learning agent in a chaotic environment are just two examples in
which unpredictable outcomes are common. These measures can be modeled as
random variables and compared among each other via their expected values or
more sophisticated tools such as null hypothesis statistical tests. In this
paper, we propose an alternative framework to visually compare two samples
according to their estimated cumulative distribution functions. First, we
introduce a dominance measure for two random variables that quantifies the
proportion in which the cumulative distribution function of one of the random
variables scholastically dominates the other one. Then, we present a graphical
method that decomposes in quantiles i) the proposed dominance measure and ii)
the probability that one of the random variables takes lower values than the
other. With illustrative purposes, we re-evaluate the experimentation of an
already published work with the proposed methodology and we show that
additional conclusions (missed by the rest of the methods) can be inferred.
Additionally, the software package RVCompare was created as a convenient way of
applying and experimenting with the proposed framework.",-0.32934386,-0.38822538,0.5762646,C
3329,"In the following, we further study the scores of the algorithms with statistical
tests, the simplex-plot, and the cumulative diÔ¨Äerence-plot.","in this case, the three visualization methods considered (histogram, box-plot, and violin-plot)
have not been able to summarize the scores obtained with the algorithms in a way that enables an
easy comparison.","6.2.1 Mann-Whitney test

Applying the Mann-Whitney test we obtain a p-value of p = 0.035, lower than the usually used
0.05 threshold.",2022-03-15 13:37:03+00:00,Comparing two samples through stochastic dominance: a graphical approach,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Etor Arza'), arxiv.Result.Author('Josu Ceberio'), arxiv.Result.Author('Ekhi√±e Irurozki'), arxiv.Result.Author('Aritz P√©rez')]","Non-deterministic measurements are common in real-world scenarios: the
performance of a stochastic optimization algorithm or the total reward of a
reinforcement learning agent in a chaotic environment are just two examples in
which unpredictable outcomes are common. These measures can be modeled as
random variables and compared among each other via their expected values or
more sophisticated tools such as null hypothesis statistical tests. In this
paper, we propose an alternative framework to visually compare two samples
according to their estimated cumulative distribution functions. First, we
introduce a dominance measure for two random variables that quantifies the
proportion in which the cumulative distribution function of one of the random
variables scholastically dominates the other one. Then, we present a graphical
method that decomposes in quantiles i) the proposed dominance measure and ii)
the probability that one of the random variables takes lower values than the
other. With illustrative purposes, we re-evaluate the experimentation of an
already published work with the proposed methodology and we show that
additional conclusions (missed by the rest of the methods) can be inferred.
Additionally, the software package RVCompare was created as a convenient way of
applying and experimenting with the proposed framework.",-0.32934386,-0.38822538,0.5762646,C
3330,"In the following, we further study the scores of the algorithms with statistical
tests, the simplex-plot, and the cumulative diÔ¨Äerence-plot.","in this case, the three visualization methods considered (histogram, box-plot, and violin-plot)
have not been able to summarize the scores obtained with the algorithms in a way that enables an
easy comparison.","6.2.1 Mann-Whitney test

Applying the Mann-Whitney test we obtain a p-value of p = 0.035, lower than the usually used
0.05 threshold.",2022-03-15 13:37:03+00:00,Comparing Two Samples Through Stochastic Dominance: A Graphical Approach,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Etor Arza'), arxiv.Result.Author('Josu Ceberio'), arxiv.Result.Author('Ekhi√±e Irurozki'), arxiv.Result.Author('Aritz P√©rez')]","Non-deterministic measurements are common in real-world scenarios: the
performance of a stochastic optimization algorithm or the total reward of a
reinforcement learning agent in a chaotic environment are just two examples in
which unpredictable outcomes are common. These measures can be modeled as
random variables and compared among each other via their expected values or
more sophisticated tools such as null hypothesis statistical tests. In this
paper, we propose an alternative framework to visually compare two samples
according to their estimated cumulative distribution functions. First, we
introduce a dominance measure for two random variables that quantifies the
proportion in which the cumulative distribution function of one of the random
variables stochastically dominates the other one. Then, we present a graphical
method that decomposes in quantiles i) the proposed dominance measure and ii)
the probability that one of the random variables takes lower values than the
other. With illustrative purposes, we re-evaluate the experimentation of an
already published work with the proposed methodology and we show that
additional conclusions (missed by the rest of the methods) can be inferred.
Additionally, the software package RVCompare was created as a convenient way of
applying and experimenting with the proposed framework.",-0.32934386,-0.38822538,0.5762646,C
3469,"In this subsection, we further study the risk bounds of IHT in well-speciÔ¨Åed scenarios where the
data is assumed to be generated according to a truly sparse model.","4.2 Fast rates for well-speciÔ¨Åed sparse learning models

The sparse excess risk bounds derived so far are essentially for misspeciÔ¨Åed sparse learning models.","Such a statistical treatment is
conventional in the theoretical analysis of high-dimensional sparsity recovery approaches [2, 41, 64].",2022-03-17 16:12:56+00:00,Stability and Risk Bounds of Iterative Hard Thresholding,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Xiao-Tong Yuan'), arxiv.Result.Author('Ping Li')]","In this paper, we analyze the generalization performance of the Iterative
Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The
parameter estimation and sparsity recovery consistency of IHT has long been
known in compressed sensing. From the perspective of statistical learning,
another fundamental question is how well the IHT estimation would predict on
unseen data. This paper makes progress towards answering this open question by
introducing a novel sparse generalization theory for IHT under the notion of
algorithmic stability. Our theory reveals that: 1) under natural conditions on
the empirical risk function over $n$ samples of dimension $p$, IHT with
sparsity level $k$ enjoys an $\mathcal{\tilde
O}(n^{-1/2}\sqrt{k\log(n)\log(p)})$ rate of convergence in sparse excess risk;
2) a tighter $\mathcal{\tilde O}(n^{-1/2}\sqrt{\log(n)})$ bound can be
established by imposing an additional iteration stability condition on a
hypothetical IHT procedure invoked to the population risk; and 3) a fast rate
of order $\mathcal{\tilde O}\left(n^{-1}k(\log^3(n)+\log(p))\right)$ can be
derived for strongly convex risk function under proper strong-signal
conditions. The results have been substantialized to sparse linear regression
and sparse logistic regression models to demonstrate the applicability of our
theory. Preliminary numerical evidence is provided to confirm our theoretical
predictions.",0.2307668,0.11347099,-0.12366328,A
3470,"In view of the recent progresses achieved towards understanding
the beneÔ¨Åt of overparametrization for the optimization and generalization of gradient-based deep
learning algorithms [3, 46], it is interesting to further study the impact of overparametrization on
the generalization performance of IHT-style algorithms for deep learning with sparsity.","In view of the stan-
dard OÀú n‚àí1/2‚àöp uniform convergence bound for dense models (see, e.g., [49]), it is more or less
straightforward to derive a generalization bound of order OÀú n‚àí1/2 k log(p) for IHT which is ap-
plicable to the non-convex regime.","References

 [1] Felix Abramovich and Vadim Grinshtein.",2022-03-17 16:12:56+00:00,Stability and Risk Bounds of Iterative Hard Thresholding,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Xiao-Tong Yuan'), arxiv.Result.Author('Ping Li')]","In this paper, we analyze the generalization performance of the Iterative
Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The
parameter estimation and sparsity recovery consistency of IHT has long been
known in compressed sensing. From the perspective of statistical learning,
another fundamental question is how well the IHT estimation would predict on
unseen data. This paper makes progress towards answering this open question by
introducing a novel sparse generalization theory for IHT under the notion of
algorithmic stability. Our theory reveals that: 1) under natural conditions on
the empirical risk function over $n$ samples of dimension $p$, IHT with
sparsity level $k$ enjoys an $\mathcal{\tilde
O}(n^{-1/2}\sqrt{k\log(n)\log(p)})$ rate of convergence in sparse excess risk;
2) a tighter $\mathcal{\tilde O}(n^{-1/2}\sqrt{\log(n)})$ bound can be
established by imposing an additional iteration stability condition on a
hypothetical IHT procedure invoked to the population risk; and 3) a fast rate
of order $\mathcal{\tilde O}\left(n^{-1}k(\log^3(n)+\log(p))\right)$ can be
derived for strongly convex risk function under proper strong-signal
conditions. The results have been substantialized to sparse linear regression
and sparse logistic regression models to demonstrate the applicability of our
theory. Preliminary numerical evidence is provided to confirm our theoretical
predictions.",0.38756427,0.036882717,-0.0656348,A
3649,"This indicates that the approach is not untrustworthy
per se, but that it requires further research on methods to approximate the posterior
probabilities of models (Robert et al., 2011).","However, the authors further remark that although the
existing methods are usually applied to problems with insuÔ¨Écient statistics, they seem
to select the correct models.","2.3 Summary

I presented an introduction to Bayesian model comparison in the context of ABC.",2022-03-15 10:24:16+00:00,Model Comparison in Approximate Bayesian Computation,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Jan Boelts')],"A common problem in natural sciences is the comparison of competing models in
the light of observed data. Bayesian model comparison provides a statistically
sound framework for this comparison based on the evidence each model provides
for the data. However, this framework relies on the calculation of likelihood
functions which are intractable for most models used in practice. Previous
approaches in the field of Approximate Bayesian Computation (ABC) circumvent
the evaluation of the likelihood and estimate the model evidence based on
rejection sampling, but they are typically computationally intense. Here, I
propose a new efficient method to perform Bayesian model comparison in ABC.
Based on recent advances in posterior density estimation, the method
approximates the posterior over models in parametric form. In particular, I
train a mixture-density network to map features of the observed data to the
posterior probability of the models. The performance is assessed with two
examples. On a tractable model comparison problem, the underlying exact
posterior probabilities are predicted accurately. In a use-case scenario from
computational neuroscience -- the comparison between two ion channel models --
the underlying ground-truth model is reliably assigned a high posterior
probability. Overall, the method provides a new efficient way to perform
Bayesian model comparison on complex biophysical models independent of the
model architecture.",-0.11834438,-0.12787217,-0.1635107,C
3657,"Finally, even though our method achieved better accuracy levels compared to PI-WGAN in all the cases, in a
few of numerical experiments presented in the paper, the performance levels can still be improved in further research
extensions.","Also, we now assume
that all sensors will give us accurate information while, in practice, measurement noises exist and should be accounted
for.","Future research directions also include quantifying the the sensor uncertainty as well as the modeling
uncertainty into the framework.",2022-03-21 21:51:19+00:00,PI-VAE: Physics-Informed Variational Auto-Encoder for stochastic differential equations,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Weiheng Zhong'), arxiv.Result.Author('Hadi Meidani')]","We propose a new class of physics-informed neural networks, called
physics-informed Variational Autoencoder (PI-VAE), to solve stochastic
differential equations (SDEs) or inverse problems involving SDEs. In these
problems the governing equations are known but only a limited number of
measurements of system parameters are available. PI-VAE consists of a
variational autoencoder (VAE), which generates samples of system variables and
parameters. This generative model is integrated with the governing equations.
In this integration, the derivatives of VAE outputs are readily calculated
using automatic differentiation, and used in the physics-based loss term. In
this work, the loss function is chosen to be the Maximum Mean Discrepancy (MMD)
for improved performance, and neural network parameters are updated iteratively
using the stochastic gradient descent algorithm. We first test the proposed
method on approximating stochastic processes. Then we study three types of
problems related to SDEs: forward and inverse problems together with mixed
problems where system parameters and solutions are simultaneously calculated.
The satisfactory accuracy and efficiency of the proposed method are numerically
demonstrated in comparison with physics-informed generative adversarial network
(PI-WGAN).",-0.052616086,0.031892046,-0.029607603,B
3711,"More information is available from the following locations:

    - R: https://github.com/dhopp1/nowcastLSTM
    - MATLAB: https://github.com/dhopp1/nowcast_lstm_matlab
    - Julia: https://github.com/dhopp1/NowcastLSTM.jl

The remainder of this paper is structured as follows: the next section will provide more
background information on nowcasting, including during the COVID-19 pandemic, and
the LSTM methodology; section three will examine the relative performance of DFMs
and LSTMs in nowcasting three series during the pandemic: global merchandise trade
exports expressed in both values and volumes and global services exports; section
four will introduce and examine a methodology for introducing causal inference to
LSTM predictions, as well as introduce the wrappers for the nowcast_lstm library;
section five will conclude and examine areas of further research.","Finally, in
order to further increase accessibility to the use of LSTMs in economic nowcasting,
wrappers for R, MATLAB, and Julia for the nowcast_lstm library have been introduced,
enabling the use of library from these languages without the need for Python knowledge.","_____________________________________________________________________________________
4              UNCTAD Research Paper No.",2022-03-22 16:48:41+00:00,Performance of long short-term memory artificial neural networks in nowcasting during the COVID-19 crisis,stat.ML,"['stat.ML', 'cs.LG', 'econ.EM']",[arxiv.Result.Author('Daniel Hopp')],"The COVID-19 pandemic has demonstrated the increasing need of policymakers
for timely estimates of macroeconomic variables. A prior UNCTAD research paper
examined the suitability of long short-term memory artificial neural networks
(LSTM) for performing economic nowcasting of this nature. Here, the LSTM's
performance during the COVID-19 pandemic is compared and contrasted with that
of the dynamic factor model (DFM), a commonly used methodology in the field.
Three separate variables, global merchandise export values and volumes and
global services exports, were nowcast with actual data vintages and performance
evaluated for the second, third, and fourth quarters of 2020 and the first and
second quarters of 2021. In terms of both mean absolute error and root mean
square error, the LSTM obtained better performance in two-thirds of
variable/quarter combinations, as well as displayed more gradual forecast
evolutions with more consistent narratives and smaller revisions. Additionally,
a methodology to introduce interpretability to LSTMs is introduced and made
available in the accompanying nowcast_lstm Python library, which is now also
available in R, MATLAB, and Julia.",-0.28104347,0.18308336,-0.07146077,B
3712,"This method should be considered experimental, and further research is necessary to
examine additional and more robust ways of implementing interpretability within the
LSTM framework.","In empirical
testing, this rescaling factor is almost always close to 1.","Further information on usage and methodology is available on the
repository‚Äôs page: https://github.com/dhopp1/nowcast_lstm.",2022-03-22 16:48:41+00:00,Performance of long short-term memory artificial neural networks in nowcasting during the COVID-19 crisis,stat.ML,"['stat.ML', 'cs.LG', 'econ.EM']",[arxiv.Result.Author('Daniel Hopp')],"The COVID-19 pandemic has demonstrated the increasing need of policymakers
for timely estimates of macroeconomic variables. A prior UNCTAD research paper
examined the suitability of long short-term memory artificial neural networks
(LSTM) for performing economic nowcasting of this nature. Here, the LSTM's
performance during the COVID-19 pandemic is compared and contrasted with that
of the dynamic factor model (DFM), a commonly used methodology in the field.
Three separate variables, global merchandise export values and volumes and
global services exports, were nowcast with actual data vintages and performance
evaluated for the second, third, and fourth quarters of 2020 and the first and
second quarters of 2021. In terms of both mean absolute error and root mean
square error, the LSTM obtained better performance in two-thirds of
variable/quarter combinations, as well as displayed more gradual forecast
evolutions with more consistent narratives and smaller revisions. Additionally,
a methodology to introduce interpretability to LSTMs is introduced and made
available in the accompanying nowcast_lstm Python library, which is now also
available in R, MATLAB, and Julia.",-0.24705562,0.17081812,0.06005832,B
3859,"Furthermore, we openly
                                          share the source code of the proposed method to facilitate further research.","We demonstrate the learning behavior of our algorithm on
                                          synthetic data and the signiÔ¨Åcant performance improvements over the con-
                                          ventional methods over various real life datasets.","Keywords: feature extraction, end-to-end learning, online learning,
                                          prediction, long short-term memory (LSTM), soft gradient boosting
                                          decision tree (sGBDT).",2022-03-25 17:13:08+00:00,A Hybrid Framework for Sequential Data Prediction with End-to-End Optimization,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Mustafa E. Aydƒ±n'), arxiv.Result.Author('Suleyman S. Kozat')]","We investigate nonlinear prediction in an online setting and introduce a
hybrid model that effectively mitigates, via an end-to-end architecture, the
need for hand-designed features and manual model selection issues of
conventional nonlinear prediction/regression methods. In particular, we use
recursive structures to extract features from sequential signals, while
preserving the state information, i.e., the history, and boosted decision trees
to produce the final output. The connection is in an end-to-end fashion and we
jointly optimize the whole architecture using stochastic gradient descent, for
which we also provide the backward pass update equations. In particular, we
employ a recurrent neural network (LSTM) for adaptive feature extraction from
sequential data and a gradient boosting machinery (soft GBDT) for effective
supervised regression. Our framework is generic so that one can use other deep
learning architectures for feature extraction (such as RNNs and GRUs) and
machine learning algorithms for decision making as long as they are
differentiable. We demonstrate the learning behavior of our algorithm on
synthetic data and the significant performance improvements over the
conventional methods over various real life datasets. Furthermore, we openly
share the source code of the proposed method to facilitate further research.",0.011640172,0.49916428,0.27460003,B
3860,"Furthermore, we openly
                                         share the source code of the proposed method to facilitate further research.","We demonstrate the learning behavior of our algorithm on
                                         synthetic data and the signiÔ¨Åcant performance improvements over the con-
                                         ventional methods over various real life datasets.","Keywords: feature extraction, end-to-end learning, online learning,
                                         prediction, long short-term memory (LSTM), soft gradient boosting
                                         decision tree (sGBDT).",2022-03-25 17:13:08+00:00,A Hybrid Framework for Sequential Data Prediction with End-to-End Optimization,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Mustafa E. Aydƒ±n'), arxiv.Result.Author('Suleyman S. Kozat')]","We investigate nonlinear prediction in an online setting and introduce a
hybrid model that effectively mitigates, via an end-to-end architecture, the
need for hand-designed features and manual model selection issues of
conventional nonlinear prediction/regression methods. In particular, we use
recursive structures to extract features from sequential signals, while
preserving the state information, i.e., the history, and boosted decision trees
to produce the final output. The connection is in an end-to-end fashion and we
jointly optimize the whole architecture using stochastic gradient descent, for
which we also provide the backward pass update equations. In particular, we
employ a recurrent neural network (LSTM) for adaptive feature extraction from
sequential data and a gradient boosting machinery (soft GBDT) for effective
supervised regression. Our framework is generic so that one can use other deep
learning architectures for feature extraction (such as RNNs and GRUs) and
machine learning algorithms for decision making as long as they are
differentiable. We demonstrate the learning behavior of our algorithm on
synthetic data and the significant performance improvements over the
conventional methods over various real life datasets. Furthermore, we openly
share the source code of the proposed method to facilitate further research.",0.011640172,0.49916428,0.27460003,B
4608,"The earliest
                                         bound on the -entropy of a ball in RKHS was given in [5] and further study of this subject was carried out
                                         in [30].",Literature dedicated to covering numbers of kernel classes includes monographs [6] and [3].,"The case of special types of kernels, such as translation invariant and analytical kernels, was considered
                                         in [29].",2022-04-09 16:45:22+00:00,Spectral bounds of the $\varepsilon$-entropy of kernel classes,stat.ML,"['stat.ML', 'cs.LG', 'math.FA']",[arxiv.Result.Author('Rustem Takhanov')],"We develop new upper and lower bounds on the $\varepsilon$-entropy of a unit
ball in a reproducing kernel Hilbert space induced by some Mercer kernel $K$.
Our bounds are based on the behaviour of eigenvalues of a corresponding
integral operator. In our approach we exploit an ellipsoidal structure of a
unit ball in RKHS and a previous work on covering numbers of an ellipsoid in
the euclidean space obtained by Dumer, Pinsker and Prelov.
  We present a number of applications of our main bound, such as its tightness
for a practically important case of the Gaussian kernel. Further, we develop a
series of lower bounds on the $\varepsilon$-entropy that can be established
from a connection between covering numbers of a ball in RKHS and a quantization
of a Gaussian Random Field that corresponds to the kernel $K$ by the
Kosambi-Karhunen-Lo\`eve transform.",0.21311098,-0.3036499,0.054871663,A
4858,"While there has
FID metric itself, wherein the scores do not correlate with             been a signiÔ¨Åcant amount of research trying to remedy and
visual quality, emphasize the importance of further research            explain this situation (Choi et al., 2018; Ren et al., 2019;
on sample-based evaluation metrics for DGMs (Borji, 2022),              Le Lan & Dinh, 2020; Caterini & Loaiza-Ganem, 2021),
although developing such metrics falls outside our scope.","We believe these failures modes of the           hoods to MNIST and SVHN, respectively.","there is little work achieving good OOD performance using
                                                                        only likelihoods (Caterini et al., 2021).",2022-04-14 18:00:03+00:00,Diagnosing and Fixing Manifold Overfitting in Deep Generative Models,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Gabriel Loaiza-Ganem'), arxiv.Result.Author('Brendan Leigh Ross'), arxiv.Result.Author('Jesse C. Cresswell'), arxiv.Result.Author('Anthony L. Caterini')]","Likelihood-based, or explicit, deep generative models use neural networks to
construct flexible high-dimensional densities. This formulation directly
contradicts the manifold hypothesis, which states that observed data lies on a
low-dimensional manifold embedded in high-dimensional ambient space. In this
paper we investigate the pathologies of maximum-likelihood training in the
presence of this dimensionality mismatch. We formally prove that degenerate
optima are achieved wherein the manifold itself is learned but not the
distribution on it, a phenomenon we call manifold overfitting. We propose a
class of two-step procedures consisting of a dimensionality reduction step
followed by maximum-likelihood density estimation, and prove that they recover
the data-generating distribution in the nonparametric regime, thus avoiding
manifold overfitting. We also show that these procedures enable density
estimation on the manifolds learned by implicit models, such as generative
adversarial networks, hence addressing a major shortcoming of these models.
Several recently proposed methods are instances of our two-step procedures; we
thus unify, extend, and theoretically justify a large class of models.",-0.21033809,-0.10831773,0.036226004,C
5124,"Among the non-BRCA associated genes CST9L is highly
ranked in all methods, so this is a good candidate for further research.","Further, SLC22A5 ranks in the top three genes for all methods, and
TEX14 is always the least important BRCA associated gene.","While there are clearly important similarities
in the results of all methods, the glaring difference is the number of features given 0 importance.",2022-04-21 07:54:58+00:00,Ultra Marginal Feature Importance,stat.ML,"['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'stat.AP']","[arxiv.Result.Author('Joseph Janssen'), arxiv.Result.Author('Vincent Guan')]","Scientists frequently prioritize learning from data rather than training the
best possible model; however, research in machine learning often prioritizes
the latter. The development of marginal feature importance methods, such as
marginal contribution feature importance, attempts to break this trend by
providing a useful framework for explaining relationships in data in an
interpretable fashion. In this work, we generalize the framework of marginal
contribution feature importance to improve performance with regards to
detecting correlated interactions and reducing runtime. To do so, we consider
""information subsets"" of the set of features $F$ and show that our importance
metric can be computed directly after applying fair representation learning
methods from the AI fairness literature. The methods of optimal transport and
linear regression are considered and explored experimentally for removing all
the information of our feature of interest $f$ from the feature set $F$. Given
these implementations, we show on real and simulated data that ultra marginal
feature importance performs at least as well as marginal contribution feature
importance, with substantially faster computation time and better performance
in the presence of correlated interactions and unrelated features.",-0.26740283,0.042439707,0.09851425,B
5245,"An important direction for further research is
the detailed characterization of noise from various electronic components.","While in the Spice simulation, white noise was directly added to the circuit, the ultimate aim is
to leverage unavoidable, inherent noise of the circuit.","It has been demonstrated,
in the context of photonic networks (Roques-Carmes et al., 2019), systems leveraging non-Gaussian
sources of noise can converge to the same distribution as those with white noise.",2022-04-23 23:16:47+00:00,Learning and Inference in Sparse Coding Models with Langevin Dynamics,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Michael Y. -S. Fang'), arxiv.Result.Author('Mayur Mudigonda'), arxiv.Result.Author('Ryan Zarcone'), arxiv.Result.Author('Amir Khosrowshahi'), arxiv.Result.Author('Bruno A. Olshausen')]","We describe a stochastic, dynamical system capable of inference and learning
in a probabilistic latent variable model. The most challenging problem in such
models - sampling the posterior distribution over latent variables - is
proposed to be solved by harnessing natural sources of stochasticity inherent
in electronic and neural systems. We demonstrate this idea for a sparse coding
model by deriving a continuous-time equation for inferring its latent variables
via Langevin dynamics. The model parameters are learned by simultaneously
evolving according to another continuous-time equation, thus bypassing the need
for digital accumulators or a global clock. Moreover we show that Langevin
dynamics lead to an efficient procedure for sampling from the posterior
distribution in the 'L0 sparse' regime, where latent variables are encouraged
to be set to zero as opposed to having a small L1 norm. This allows the model
to properly incorporate the notion of sparsity rather than having to resort to
a relaxed version of sparsity to make optimization tractable. Simulations of
the proposed dynamical system on both synthetic and natural image datasets
demonstrate that the model is capable of probabilistically correct inference,
enabling learning of the dictionary as well as parameters of the prior.",0.14010265,-0.10196259,-0.0161692,A
5356,"In that direction further research might needed in order to utilize
effective sampling algorithms combined with algorithms that aim to learn operators on the data [5].","In
problems in science and engineering data might be given only in a small neighborhood of their entire manifold whose
dimensionality also might not be known in advance.","In addition, the latent coordinates in our case were computed without taking into consideration physical constrains
that may exist in their original model (e.g.",2022-04-26 18:44:17+00:00,Double Diffusion Maps and their Latent Harmonics for Scientific Computations in Latent Space,stat.ML,"['stat.ML', 'cs.LG', 'math.DS']","[arxiv.Result.Author('Nikolaos Evangelou'), arxiv.Result.Author('Felix Dietrich'), arxiv.Result.Author('Eliodoro Chiavazzo'), arxiv.Result.Author('Daniel Lehmberg'), arxiv.Result.Author('Marina Meila'), arxiv.Result.Author('Ioannis G. Kevrekidis')]","We introduce a data-driven approach to building reduced dynamical models
through manifold learning; the reduced latent space is discovered using
Diffusion Maps (a manifold learning technique) on time series data. A second
round of Diffusion Maps on those latent coordinates allows the approximation of
the reduced dynamical models. This second round enables mapping the latent
space coordinates back to the full ambient space (what is called lifting); it
also enables the approximation of full state functions of interest in terms of
the reduced coordinates. In our work, we develop and test three different
reduced numerical simulation methodologies, either through pre-tabulation in
the latent space and integration on the fly or by going back and forth between
the ambient space and the latent space. The data-driven latent space simulation
results, based on the three different approaches, are validated through (a) the
latent space observation of the full simulation through the Nystr\""om Extension
formula, or through (b) lifting the reduced trajectory back to the full ambient
space, via Latent Harmonics. Latent space modeling often involves additional
regularization to favor certain properties of the space over others, and the
mapping back to the ambient space is then constructed mostly independently from
these properties; here, we use the same data-driven approach to construct the
latent space and then map back to the ambient space.",0.24358687,-0.041329645,0.13830788,A
5696,"We further study a subclass of our model described by certain natural parametric assumptions
(Section 4).","These experiments suggest
the existence of sequential bias (Section 3.2), the relative nature of the ratings (Section 3.3), and
speciÔ¨Åc structure in the mistakes made when inferring comparisons from the scores (Section 3.4).","In particular, we motivate this modeling choice by showing that scoring according to
our parametric model is the theoretically optimal response if the evaluator‚Äôs goal is to minimize
the squared error in reporting the (normalized) ranking of the items.",2022-05-03 16:38:13+00:00,Modeling and Correcting Bias in Sequential Evaluation,stat.ML,"['stat.ML', 'cs.GT', 'cs.IT', 'cs.LG', 'math.IT']","[arxiv.Result.Author('Jingyan Wang'), arxiv.Result.Author('Ashwin Pananjady')]","We consider the problem of sequential evaluation, in which an evaluator
observes candidates in a sequence and assigns scores to these candidates in an
online, irrevocable fashion. Motivated by the psychology literature that has
studied sequential bias in such settings -- namely, dependencies between the
evaluation outcome and the order in which the candidates appear -- we propose a
natural model for the evaluator's rating process that captures the lack of
calibration inherent to such a task. We conduct crowdsourcing experiments to
demonstrate various facets of our model. We then proceed to study how to
correct sequential bias under our model by posing this as a statistical
inference problem. We propose a near-linear time, online algorithm for this
task and prove guarantees in terms of two canonical ranking metrics. We also
prove that our algorithm is information theoretically optimal, by establishing
matching lower bounds in both metrics. Finally, we show that our algorithm
outperforms the de facto method of using the rankings induced by the reported
scores.",-0.17653528,-0.039009035,-0.13706955,C
5781,"The impact of the value of p on sequential learning performance is a
potential topic for further study.","For simplicity and because we do not want to be overly skewed by outliers, we set p = 0.683 for the
remainder of this manuscript.","4 Numerical Experiments: How Well Calibrated is the Proposed Prediction
    Interval?",2022-05-04 18:04:10+00:00,Multivariate Prediction Intervals for Random Forests,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Brendan Folie'), arxiv.Result.Author('Maxwell Hutchinson')]","Accurate uncertainty estimates can significantly improve the performance of
iterative design of experiments, as in Sequential and Reinforcement learning.
For many such problems in engineering and the physical sciences, the design
task depends on multiple correlated model outputs as objectives and/or
constraints. To better solve these problems, we propose a recalibrated
bootstrap method to generate multivariate prediction intervals for bagged
models and show that it is well-calibrated. We apply the recalibrated bootstrap
to a simulated sequential learning problem with multiple objectives and show
that it leads to a marked decrease in the number of iterations required to find
a satisfactory candidate. This indicates that the recalibrated bootstrap could
be a valuable tool for practitioners using machine learning to optimize systems
with multiple competing targets.",-0.067566164,0.19024563,-0.06471464,B
5782,"Using different techniques to produce the univariate prediction interval and studying the
impact on SL would be a valuable topic for further study.","In the subsequent sections we show that it also produces well-calibrated
multivariate prediction intervals and that this quality results in signiÔ¨Åcantly more efÔ¨Åcient sequential
learning.","We also performed several sanity checks, some of which are included in the appendices.",2022-05-04 18:04:10+00:00,Multivariate Prediction Intervals for Random Forests,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Brendan Folie'), arxiv.Result.Author('Maxwell Hutchinson')]","Accurate uncertainty estimates can significantly improve the performance of
iterative design of experiments, as in Sequential and Reinforcement learning.
For many such problems in engineering and the physical sciences, the design
task depends on multiple correlated model outputs as objectives and/or
constraints. To better solve these problems, we propose a recalibrated
bootstrap method to generate multivariate prediction intervals for bagged
models and show that it is well-calibrated. We apply the recalibrated bootstrap
to a simulated sequential learning problem with multiple objectives and show
that it leads to a marked decrease in the number of iterations required to find
a satisfactory candidate. This indicates that the recalibrated bootstrap could
be a valuable tool for practitioners using machine learning to optimize systems
with multiple competing targets.",-0.0030497008,0.3496126,-0.06250487,B_centroid
5783,"The impact of the value of p on sequential learning performance is a
potential topic for further study.","For simplicity and because we do not want to be overly skewed by outliers, we set p = 0.683 for the
remainder of this manuscript.","4 Numerical Experiments: How Well Calibrated is the Proposed Prediction
    Interval?",2022-05-04 18:04:10+00:00,Multivariate Prediction Intervals for Random Forests,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Brendan Folie'), arxiv.Result.Author('Maxwell Hutchinson')]","Accurate uncertainty estimates can significantly improve the performance of
iterative design of experiments, as in Sequential and Reinforcement learning.
For many such problems in engineering and the physical sciences, the design
task depends on multiple correlated model outputs as objectives and/or
constraints. To better solve these problems, we propose a recalibrated
bootstrap method to generate multivariate prediction intervals for bagged
models and show that it is well-calibrated. We apply the recalibrated bootstrap
to a simulated sequential learning problem with multiple objectives and show
that it leads to a marked decrease in the number of iterations required to find
a satisfactory candidate. This indicates that the recalibrated bootstrap could
be a valuable tool for practitioners using machine learning to optimize systems
with multiple competing targets.",-0.067566164,0.19024563,-0.06471464,B
5784,"Using different techniques to produce the univariate prediction interval and studying the
impact on SL would be a valuable topic for further study.","In the subsequent sections we show that it also produces well-calibrated
multivariate prediction intervals and that this quality results in signiÔ¨Åcantly more efÔ¨Åcient sequential
learning.","We also performed several sanity checks, some of which are included in the appendices.",2022-05-04 18:04:10+00:00,Multivariate Prediction Intervals for Random Forests,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Brendan Folie'), arxiv.Result.Author('Maxwell Hutchinson')]","Accurate uncertainty estimates can significantly improve the performance of
iterative design of experiments, as in Sequential and Reinforcement learning.
For many such problems in engineering and the physical sciences, the design
task depends on multiple correlated model outputs as objectives and/or
constraints. To better solve these problems, we propose a recalibrated
bootstrap method to generate multivariate prediction intervals for bagged
models and show that it is well-calibrated. We apply the recalibrated bootstrap
to a simulated sequential learning problem with multiple objectives and show
that it leads to a marked decrease in the number of iterations required to find
a satisfactory candidate. This indicates that the recalibrated bootstrap could
be a valuable tool for practitioners using machine learning to optimize systems
with multiple competing targets.",-0.0030497008,0.3496126,-0.06250487,B
6003,"There are over 9 diÔ¨Äerent Distance
similarity measures used in this research and further research can be done with many more similarity
measures.","Several supervised classiÔ¨Åcation models
were tested and Linear Support Vector ClassiÔ¨Åer (SVC) was the best among them with an accuracy
of 87

9 Future Works

Over this course of Turtle score research, the comparison is done using Distance Similarity measures
such as Euclidean, Manhattan, Minkowski, and many more.",Further research can also be done by bringing up many more entities.,2022-05-10 13:22:11+00:00,Turtle Score -- Similarity Based Developer Analyzer,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Sanjjushri Varshini'), arxiv.Result.Author('Ponshriharini V'), arxiv.Result.Author('Santhosh Kannan'), arxiv.Result.Author('Snekha Suresh'), arxiv.Result.Author('Harshavardhan Ramesh'), arxiv.Result.Author('Rohith Mahadevan'), arxiv.Result.Author('Raja CSP Raman')]","In day-to-day life, a highly demanding task for IT companies is to find the
right candidates who fit the companies' culture. This research aims to
comprehend, analyze and automatically produce convincing outcomes to find a
candidate who perfectly fits right in the company. Data is examined and
collected for each employee who works in the IT domain focusing on their
performance measure. This is done based on various different categories which
bring versatility and a wide view of focus. To this data, learner analysis is
done using machine learning algorithms to obtain learner similarity and
developer similarity in order to recruit people with identical working
patterns. It's been proven that the efficiency and capability of a particular
worker go higher when working with a person of a similar personality. Therefore
this will serve as a useful tool for recruiters who aim to recruit people with
high productivity. This is to say that the model designed will render the best
outcome possible with high accuracy and an immaculate recommendation score.",-0.18814367,-0.06123387,0.24482493,C
6080,fertile avenues for further research.,"This is likely just the
tip of an enormous iceberg, and that there are many          P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath.",Coherent measures of risk.,2022-05-12 02:20:34+00:00,A Survey of Risk-Aware Multi-Armed Bandits,stat.ML,"['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']","[arxiv.Result.Author('Vincent Y. F. Tan'), arxiv.Result.Author('Prashanth L. A.'), arxiv.Result.Author('Krishna Jagannathan')]","In several applications such as clinical trials and financial portfolio
optimization, the expected value (or the average reward) does not
satisfactorily capture the merits of a drug or a portfolio. In such
applications, risk plays a crucial role, and a risk-aware performance measure
is preferable, so as to capture losses in the case of adverse events. This
survey aims to consolidate and summarise the existing research on risk
measures, specifically in the context of multi-armed bandits. We review various
risk measures of interest, and comment on their properties. Next, we review
existing concentration inequalities for various risk measures. Then, we proceed
to defining risk-aware bandit problems, We consider algorithms for the regret
minimization setting, where the exploration-exploitation trade-off manifests,
as well as the best-arm identification setting, which is a pure exploration
problem -- both in the context of risk-sensitive measures. We conclude by
commenting on persisting challenges and fertile areas for future research.",-0.36274406,-0.023378072,-0.2803447,C
6132,"A theoretical investigation of how the approximation quality relates to kernel properties
is an interesting topic for further research.","However, other parameterisations may result in less accurate
estimation.","The proposed framework models prior and variational distribution with a Gaussian measure on the
space of square integrable functions.",2022-05-12 20:10:31+00:00,Generalized Variational Inference in Function Spaces: Gaussian Measures meet Bayesian Deep Learning,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Veit D. Wild'), arxiv.Result.Author('Robert Hu'), arxiv.Result.Author('Dino Sejdinovic')]","We develop a framework for generalized variational inference in
infinite-dimensional function spaces and use it to construct a method termed
Gaussian Wasserstein inference (GWI). GWI leverages the Wasserstein distance
between Gaussian measures on the Hilbert space of square-integrable functions
in order to determine a variational posterior using a tractable optimisation
criterion and avoids pathologies arising in standard variational function space
inference. An exciting application of GWI is the ability to use deep neural
networks in the variational parametrisation of GWI, combining their superior
predictive performance with the principled uncertainty quantification analogous
to that of Gaussian processes. The proposed method obtains state-of-the-art
performance on several benchmark datasets.",0.28027225,-0.056011215,-0.15864795,A
6181,"Finally, in Section 4.6, we discuss the outcomes of this
study and give some pointers to further research.","Section 4.5 presents the experimental results
that support this approach.","55
4.2 Sequential learning for recommender systems

Many new approaches tackle the sequential learning problem for RS by taking into
account the temporal aspect of interactions directly in the design of a dedicated model
and are mainly based on Markov Models (MM), Reinforcement Learning (RL) and
Recurrent Neural Networks (RNN) [Donkers et al., 2017].",2022-05-13 21:09:41+00:00,Large-Scale Sequential Learning for Recommender and Engineering Systems,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Aleksandra Burashnikova')],"In this thesis, we focus on the design of an automatic algorithms that
provide personalized ranking by adapting to the current conditions. To
demonstrate the empirical efficiency of the proposed approaches we investigate
their applications for decision making in recommender systems and energy
systems domains. For the former, we propose novel algorithm called SAROS that
take into account both kinds of feedback for learning over the sequence of
interactions. The proposed approach consists in minimizing pairwise ranking
loss over blocks constituted by a sequence of non-clicked items followed by the
clicked one for each user. We also explore the influence of long memory on the
accurateness of predictions. SAROS shows highly competitive and promising
results based on quality metrics and also it turn out faster in terms of loss
convergence than stochastic gradient descent and batch classical approaches.
Regarding power systems, we propose an algorithm for faulted lines detection
based on focusing of misclassifications in lines close to the true event
location. The proposed idea of taking into account the neighbour lines shows
statistically significant results in comparison with the initial approach based
on convolutional neural networks for faults detection in power grid.",0.04131275,0.24330026,0.03538321,B
6182,"Finally, in Section 5.5, we discuss the outcomes of this
study and give some pointers to further research.","In Section 5.3 we introduce a strategy to filter the dataset with respect
to homogeneity of the behavior in the users when interacting with the system, based
on the concept of memory.","5.2 Framework and Problem Setting

A key point in recommendation is that user preferences for items are largely determined
by the context in which they are presented to the user.",2022-05-13 21:09:41+00:00,Large-Scale Sequential Learning for Recommender and Engineering Systems,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Aleksandra Burashnikova')],"In this thesis, we focus on the design of an automatic algorithms that
provide personalized ranking by adapting to the current conditions. To
demonstrate the empirical efficiency of the proposed approaches we investigate
their applications for decision making in recommender systems and energy
systems domains. For the former, we propose novel algorithm called SAROS that
take into account both kinds of feedback for learning over the sequence of
interactions. The proposed approach consists in minimizing pairwise ranking
loss over blocks constituted by a sequence of non-clicked items followed by the
clicked one for each user. We also explore the influence of long memory on the
accurateness of predictions. SAROS shows highly competitive and promising
results based on quality metrics and also it turn out faster in terms of loss
convergence than stochastic gradient descent and batch classical approaches.
Regarding power systems, we propose an algorithm for faulted lines detection
based on focusing of misclassifications in lines close to the true event
location. The proposed idea of taking into account the neighbour lines shows
statistically significant results in comparison with the initial approach based
on convolutional neural networks for faults detection in power grid.",-0.13742845,0.0882173,0.107510336,B
6450,"Further investigation into
                                                                  this topic is the subject of further research.",estimate of the proper choice of .,Acknowledgements                                                  large number of sampling times.,2022-05-20 01:07:41+00:00,Robust Expected Information Gain for Optimal Bayesian Experimental Design Using Ambiguity Sets,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Jinwoo Go'), arxiv.Result.Author('Tobin Isaac')]","The ranking of experiments by expected information gain (EIG) in Bayesian
experimental design is sensitive to changes in the model's prior distribution,
and the approximation of EIG yielded by sampling will have errors similar to
the use of a perturbed prior. We define and analyze \emph{robust expected
information gain} (REIG), a modification of the objective in EIG maximization
by minimizing an affine relaxation of EIG over an ambiguity set of
distributions that are close to the original prior in KL-divergence. We show
that, when combined with a sampling-based approach to estimating EIG, REIG
corresponds to a `log-sum-exp' stabilization of the samples used to estimate
EIG, meaning that it can be efficiently implemented in practice. Numerical
tests combining REIG with variational nested Monte Carlo (VNMC), adaptive
contrastive estimation (ACE) and mutual information neural estimation (MINE)
suggest that in practice REIG also compensates for the variability of
under-sampled estimators.",-0.3491144,-0.12335354,-0.15436558,C
6637,"Numerical experiments are presented in Section 6 and
Section 7 concludes the main part of the paper with a discussion for further research.","Section 4
contains the statements of the theoretical results while Section 5 gathers practical considerations,
including the construction of control variates.","2
2 Preliminaries on Monte Carlo integration

The aim of this section is to present the required mathematical framework for Monte Carlo integration

and the variance reduction methods of interest, namely adaptive importance sampling and the control
variate technique.",2022-05-24 08:21:45+00:00,A Quadrature Rule combining Control Variates and Adaptive Importance Sampling,stat.ML,"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']","[arxiv.Result.Author('R√©mi Leluc'), arxiv.Result.Author('Fran√ßois Portier'), arxiv.Result.Author('Johan Segers'), arxiv.Result.Author('Aigerim Zhuman')]","Driven by several successful applications such as in stochastic gradient
descent or in Bayesian computation, control variates have become a major tool
for Monte Carlo integration. However, standard methods do not allow the
distribution of the particles to evolve during the algorithm, as is the case in
sequential simulation methods. Within the standard adaptive importance sampling
framework, a simple weighted least squares approach is proposed to improve the
procedure with control variates. The procedure takes the form of a quadrature
rule with adapted quadrature weights to reflect the information brought in by
the control variates. The quadrature points and weights do not depend on the
integrand, a computational advantage in case of multiple integrands. Moreover,
the target density needs to be known only up to a multiplicative constant. Our
main result is a non-asymptotic bound on the probabilistic error of the
procedure. The bound proves that for improving the estimate's accuracy, the
benefits from adaptive importance sampling and control variates can be
combined. The good behavior of the method is illustrated empirically on
synthetic examples and real-world data for Bayesian linear regression.",-0.023840118,-0.021453245,-0.25466228,C
6701,"To further study the phenomena, we        Distance     10‚àí1                     dist.","It Ô¨Årst
displays a performance that is close to the performance displayed by the Œ≤ 1 then transition back
into a performance similar to that of the Œ≤ 2 .",‚Ñì2  Distance    10‚àí2  dist.,2022-05-25 11:54:42+00:00,Surprises in adversarially-trained linear regression,stat.ML,"['stat.ML', 'cs.CR', 'cs.LG', 'eess.SP', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Ant√¥nio H. Ribeiro'), arxiv.Result.Author('Dave Zachariah'), arxiv.Result.Author('Thomas B. Sch√∂n')]","State-of-the-art machine learning models can be vulnerable to very small
input perturbations that are adversarially constructed. Adversarial training is
one of the most effective approaches to defend against such examples. We show
that for linear regression problems, adversarial training can be formulated as
a convex problem. This fact is then used to show that $\ell_\infty$-adversarial
training produces sparse solutions and has many similarities to the lasso
method. Similarly, $\ell_2$-adversarial training has similarities with ridge
regression. We use a robust regression framework to analyze and understand
these similarities and also point to some differences. Finally, we show how
adversarial training behaves differently from other regularization methods when
estimating overparameterized models (i.e., models with more parameters than
datapoints). It minimizes a sum of three terms which regularizes the solution,
but unlike lasso and ridge regression, it can sharply transition into an
interpolation mode. We show that for sufficiently many features or sufficiently
small regularization parameters, the learned model perfectly interpolates the
training data while still exhibiting good out-of-sample performance.",-0.018741062,-0.26791248,0.10522808,C
6702,"Finally, the empirical Ô¨Ånds of Section 6 are interesting but
still need further study.","Having tailored solvers for linear adversarial training could also make this method more
computationally attractive and useful.","We observe a sharp transition into interpolation and show there is some
connections with the 1 and 2 minimum norm solutions.",2022-05-25 11:54:42+00:00,Surprises in adversarially-trained linear regression,stat.ML,"['stat.ML', 'cs.CR', 'cs.LG', 'eess.SP', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Ant√¥nio H. Ribeiro'), arxiv.Result.Author('Dave Zachariah'), arxiv.Result.Author('Thomas B. Sch√∂n')]","State-of-the-art machine learning models can be vulnerable to very small
input perturbations that are adversarially constructed. Adversarial training is
one of the most effective approaches to defend against such examples. We show
that for linear regression problems, adversarial training can be formulated as
a convex problem. This fact is then used to show that $\ell_\infty$-adversarial
training produces sparse solutions and has many similarities to the lasso
method. Similarly, $\ell_2$-adversarial training has similarities with ridge
regression. We use a robust regression framework to analyze and understand
these similarities and also point to some differences. Finally, we show how
adversarial training behaves differently from other regularization methods when
estimating overparameterized models (i.e., models with more parameters than
datapoints). It minimizes a sum of three terms which regularizes the solution,
but unlike lasso and ridge regression, it can sharply transition into an
interpolation mode. We show that for sufficiently many features or sufficiently
small regularization parameters, the learned model perfectly interpolates the
training data while still exhibiting good out-of-sample performance.",0.2977306,0.18653679,0.07211802,A
6796,"The Ô¨Åt between the empirical distribution and Œ¶ ‚ó¶ L is much better as predicted
by Proposition 7, even for small values of d. This motivates our further study of the approximate
precision instead of the precision.","C.4 Empirical validation of Proposition 7

Figure 10 shows an empirical validation for Proposition 7 for different document size and for anchors
of different sizes.","From the results in Figure 10, we can see why the anchors need to
be small with respect to the document size: if they are two large, the approximation of the precision
is not justiÔ¨Åed.",2022-05-27 06:57:32+00:00,A Sea of Words: An In-Depth Analysis of Anchors for Text Data,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Gianluigi Lopardo'), arxiv.Result.Author('Damien Garreau'), arxiv.Result.Author('Frederic Precioso')]","Anchors [Ribeiro et al. (2018)] is a post-hoc, rule-based interpretability
method. For text data, it proposes to explain a decision by highlighting a
small set of words (an anchor) such that the model to explain has similar
outputs when they are present in a document. In this paper, we present the
first theoretical analysis of Anchors, considering that the search for the best
anchor is exhaustive. We leverage this analysis to gain insights on the
behavior of Anchors on simple models, including elementary if-then rules and
linear classifiers.",-0.06004183,-0.21106276,-0.08680291,C
6799,"Therefore, it needs further study for any possible improvement.","Under heavy-tailed noise, in

addition to the degradation of convergence rate, a new issue for the rank-r matrix recovery
                                                                              l
                                                                              r l‚àí1 d
via  rank-1    frame  is  that  the  relative  scaling  between  n, d, r  is     n     rather  than  the  expected

optimal one rnd.","24
3.8 Rank-1 Frame v.s.",2022-05-27 08:38:33+00:00,Error Bound of Empirical $\ell_2$ Risk Minimization for Noisy Standard and Generalized Phase Retrieval Problems,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Junren Chen'), arxiv.Result.Author('Michael K. Ng')]","In this paper, we study the estimation performance of empirical $\ell_2$ risk
minimization (ERM) in noisy (standard) phase retrieval (NPR) given by $y_k =
|\alpha_k^*x_0|^2+\eta_k$, or noisy generalized phase retrieval (NGPR)
formulated as $y_k = x_0^*A_kx_0 + \eta_k$, where $x_0\in\mathbb{K}^d$ is the
desired signal, $n$ is the sample size, $\eta= (\eta_1,...,\eta_n)^\top$ is the
noise vector. We establish new error bounds under different noise patterns, and
our proofs are valid for both $\mathbb{K}=\mathbb{R}$ and
$\mathbb{K}=\mathbb{C}$. In NPR under arbitrary noise vector $\eta$, we derive
a new error bound $O\big(\|\eta\|_\infty\sqrt{\frac{d}{n}} +
\frac{|\mathbf{1}^\top\eta|}{n}\big)$, which is tighter than the currently
known one $O\big(\frac{\|\eta\|}{\sqrt{n}}\big)$ in many cases. In NGPR, we
show $O\big(\|\eta\|\frac{\sqrt{d}}{n}\big)$ for arbitrary $\eta$. In both
problems, the bounds for arbitrary noise immediately give rise to
$\tilde{O}(\sqrt{\frac{d}{n}})$ for sub-Gaussian or sub-exponential random
noise, with some conventional but inessential assumptions (e.g., independent or
zero-mean condition) removed or weakened. In addition, we make a first attempt
to ERM under heavy-tailed random noise assumed to have bounded $l$-th moment.
To achieve a trade-off between bias and variance, we truncate the responses and
propose a corresponding robust ERM estimator, which is shown to possess the
guarantee $\tilde{O}\big(\big[\sqrt{\frac{d}{n}}\big]^{1-1/l}\big)$ in both
NPR, NGPR. All the error bounds straightforwardly extend to the more general
problems of rank-$r$ matrix recovery, and these results deliver a conclusion
that the full-rank frame $\{A_k\}_{k=1}^n$ in NGPR is more robust to biased
noise than the rank-1 frame $\{\alpha_k\alpha_k^*\}_{k=1}^n$ in NPR. Extensive
experimental results are presented to illustrate our theoretical findings.",0.1419455,-0.070575714,0.11165126,A
7057,"In Section 4 we further study the existence and theoretical properties

of the Ô¨Çow mapping when K = 1, for general data and graph data.","In this case, one can set the distribution of H to be a standard normal, and the desired

7
transport map is a normalizing Ô¨Çow.","Assumption 1 guarantees the existence of a shared Ô¨Çow mapping which transports the K-class condi-
tional densities up to O( ) error as long as the components of the Gaussian mixture distribution p(H|Y )
have Œ¥-separated -supports.",2022-06-02 17:28:33+00:00,Invertible Neural Networks for Graph Prediction,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Chen Xu'), arxiv.Result.Author('Xiuyuan Cheng'), arxiv.Result.Author('Yao Xie')]","Graph prediction problems prevail in data analysis and machine learning. The
inverse prediction problem, namely to infer input data from given output
labels, is of emerging interest in various applications. In this work, we
develop \textit{invertible graph neural network} (iGNN), a deep generative
model to tackle the inverse prediction problem on graphs by casting it as a
conditional generative task. The proposed model consists of an invertible
sub-network that maps one-to-one from data to an intermediate encoded feature,
which allows forward prediction by a linear classification sub-network as well
as efficient generation from output labels via a parametric mixture model. The
invertibility of the encoding sub-network is ensured by a Wasserstein-2
regularization which allows free-form layers in the residual blocks. The model
is scalable to large graphs by a factorized parametric mixture model of the
encoded feature and is computationally scalable by using GNN layers. The
existence of invertible flow mapping is backed by theories of optimal transport
and diffusion process, and we prove the expressiveness of graph convolution
layers to approximate the theoretical flows of graph data. The proposed iGNN
model is experimentally examined on synthetic data, including the example on
large graphs, and the empirical advantage is also demonstrated on
real-application datasets of solar ramping event data and traffic flow anomaly
detection.",0.19773276,-0.3463381,0.15037514,A
7058,"In Section 4 we further study the existence and theoretical properties
of the Ô¨Çow mapping when K = 1, for general data and graph data.","In this case, one can set the distribution of H to be a standard normal, and the desired
transport map is a normalizing Ô¨Çow.","Assumption 1 guarantees the existence of a shared Ô¨Çow mapping which transports the K-class condi-
tional densities up to O( ) error as long as the components of the Gaussian mixture distribution p(H|Y )
have Œ¥-separated -supports.",2022-06-02 17:28:33+00:00,Invertible Neural Networks for Graph Prediction,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Chen Xu'), arxiv.Result.Author('Xiuyuan Cheng'), arxiv.Result.Author('Yao Xie')]","Graph prediction problems prevail in data analysis and machine learning. The
inverse prediction problem, namely to infer input data from given output
labels, is of emerging interest in various applications. In this work, we
develop \textit{invertible graph neural network} (iGNN), a deep generative
model to tackle the inverse prediction problem on graphs by casting it as a
conditional generative task. The proposed model consists of an invertible
sub-network that maps one-to-one from data to an intermediate encoded feature,
which allows forward prediction by a linear classification sub-network as well
as efficient generation from output labels via a parametric mixture model. The
invertibility of the encoding sub-network is ensured by a Wasserstein-2
regularization which allows free-form layers in the residual blocks. The model
is scalable to large graphs by a factorized parametric mixture model of the
encoded feature and is computationally scalable by using GNN layers. The
existence of invertible flow mapping is backed by theories of optimal transport
and diffusion process, and we prove the expressiveness of graph convolution
layers to approximate the theoretical flows of graph data. The proposed iGNN
model is experimentally examined on synthetic data, including the example on
large graphs, and the empirical advantage is also demonstrated on
real-application datasets of solar ramping event data and traffic flow anomaly
detection.",0.20993206,-0.36270666,0.12706798,A
7286,"2.3 On Excess Risk Bounds

To understand the optimization performance of a randomized learning algorithm A with conÔ¨Ådence-
boosting, we further study here the excess risk bounds of Algorithm 1 which are of special interest
for stochastic convex optimization problems.","The result in Theorem 1 is about the generalization bound with respect to the empirical risk

over the entire training set S. We comment in passing that up to the multipliers of K, these bounds

are also valid for the generalization error R(Ak‚àó(Sk‚àó)) ‚àí RSk‚àó (Ak‚àó (Sk‚àó)) over the subset Sk‚àó.","In the following analysis, the global minimizer of the
population risk and in-expectation empirical risk sub-optimality of the randomized algorithm are
respectively denoted by w‚àó := arg minw‚ààW R(w) and

‚àÜopt := EA,S RS(A(S)) ‚àí min RS(w) .",2022-06-08 12:14:01+00:00,Boosting the Confidence of Generalization for $L_2$-Stable Randomized Learning Algorithms,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Xiao-Tong Yuan'), arxiv.Result.Author('Ping Li')]","Exponential generalization bounds with near-tight rates have recently been
established for uniformly stable learning algorithms. The notion of uniform
stability, however, is stringent in the sense that it is invariant to the
data-generating distribution. Under the weaker and distribution dependent
notions of stability such as hypothesis stability and $L_2$-stability, the
literature suggests that only polynomial generalization bounds are possible in
general cases. The present paper addresses this long standing tension between
these two regimes of results and makes progress towards relaxing it inside a
classic framework of confidence-boosting. To this end, we first establish an
in-expectation first moment generalization error bound for potentially
randomized learning algorithms with $L_2$-stability, based on which we then
show that a properly designed subbagging process leads to near-tight
exponential generalization bounds over the randomness of both data and
algorithm. We further substantialize these generic results to stochastic
gradient descent (SGD) to derive improved high-probability generalization
bounds for convex or non-convex optimization problems with natural time
decaying learning rates, which have not been possible to prove with the
existing hypothesis stability or uniform stability based results.",0.31264043,0.0077757277,-0.11477091,A
7287,"3.3 Non-convex Optimization with Smooth Loss

We further study the performance of Algorithm 1 for SGD on smooth but not necessarily convex
loss functions, such as normalized sigmoid loss (Mason et al., 1999).","(2020,

                              N     N

Theorem 3.3).","The following result is a direct
application of Theorem 1 to ASGD-w with smooth non-convex loss.",2022-06-08 12:14:01+00:00,Boosting the Confidence of Generalization for $L_2$-Stable Randomized Learning Algorithms,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Xiao-Tong Yuan'), arxiv.Result.Author('Ping Li')]","Exponential generalization bounds with near-tight rates have recently been
established for uniformly stable learning algorithms. The notion of uniform
stability, however, is stringent in the sense that it is invariant to the
data-generating distribution. Under the weaker and distribution dependent
notions of stability such as hypothesis stability and $L_2$-stability, the
literature suggests that only polynomial generalization bounds are possible in
general cases. The present paper addresses this long standing tension between
these two regimes of results and makes progress towards relaxing it inside a
classic framework of confidence-boosting. To this end, we first establish an
in-expectation first moment generalization error bound for potentially
randomized learning algorithms with $L_2$-stability, based on which we then
show that a properly designed subbagging process leads to near-tight
exponential generalization bounds over the randomness of both data and
algorithm. We further substantialize these generic results to stochastic
gradient descent (SGD) to derive improved high-probability generalization
bounds for convex or non-convex optimization problems with natural time
decaying learning rates, which have not been possible to prove with the
existing hypothesis stability or uniform stability based results.",0.42598206,-0.056718774,-0.0047446107,A
7433,"Exploring this type of model and the
inductive biases of GNNs in general is an interesting direction for further research.","As DGP has proven useful in some of our
experiments this indicates that our earlier stated hypothesis has some ground to it.","E. Details on Experiments and Datasets

In the interest of reproducability we present additional details about the experiment setups and datasets in this appendix.",2022-06-10 12:12:41+00:00,Scalable Deep Gaussian Markov Random Fields for General Graphs,stat.ML,"['stat.ML', 'cs.LG', 'cs.SI', 'stat.CO']","[arxiv.Result.Author('Joel Oskarsson'), arxiv.Result.Author('Per Sid√©n'), arxiv.Result.Author('Fredrik Lindsten')]","Machine learning methods on graphs have proven useful in many applications
due to their ability to handle generally structured data. The framework of
Gaussian Markov Random Fields (GMRFs) provides a principled way to define
Gaussian models on graphs by utilizing their sparsity structure. We propose a
flexible GMRF model for general graphs built on the multi-layer structure of
Deep GMRFs, originally proposed for lattice graphs only. By designing a new
type of layer we enable the model to scale to large graphs. The layer is
constructed to allow for efficient training using variational inference and
existing software frameworks for Graph Neural Networks. For a Gaussian
likelihood, close to exact Bayesian inference is available for the latent
field. This allows for making predictions with accompanying uncertainty
estimates. The usefulness of the proposed model is verified by experiments on a
number of synthetic and real world datasets, where it compares favorably to
other both Bayesian and deep learning methods.",-0.0017449011,0.163023,-0.04478193,B
7527,"These are interesting
issues and are worth further study in the future.","It is still a challenge to extend the strategy
to nonparametric models and general semiparametric models.","Acknowledgments

The research of Lin Lu was supported by the National Key R&D Program of
China (grant No.",2022-06-10 01:59:16+00:00,A Correlation-Ratio Transfer Learning and Variational Stein's Paradox,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Lu Lin'), arxiv.Result.Author('Weiyu Li')]","A basic condition for efficient transfer learning is the similarity between a
target model and source models. In practice, however, the similarity condition
is difficult to meet or is even violated. Instead of the similarity condition,
a brand-new strategy, linear correlation-ratio, is introduced in this paper to
build an accurate relationship between the models. Such a correlation-ratio can
be easily estimated by historical data or a part of sample. Then, a
correlation-ratio transfer learning likelihood is established based on the
correlation-ratio combination. On the practical side, the new framework is
applied to some application scenarios, especially the areas of data streams and
medical studies. Methodologically, some techniques are suggested for
transferring the information from simple source models to a relatively complex
target model. Theoretically, some favorable properties, including the global
convergence rate, are achieved, even for the case where the source models are
not similar to the target model. All in all, it can be seen from the theories
and experimental results that the inference on the target model is
significantly improved by the information from similar or dissimilar source
models. In other words, a variational Stein's paradox is illustrated in the
context of transfer learning.",-0.0361963,-0.099747464,-0.27776742,C_centroid
7589,"The detailed quantitative results
are in Table 8, Appendix E.

We further study conformal methods on two multi-target real datasets.","We observe when œÅ gets higher, only PCP shrinks the predictive set accordingly while the
predictions from other methods have little change and become loose.","Taxi Data are the taxi trip records
of New York City which include the pickup, drop-oÔ¨Ä locations of each trip and the corresponding time.",2022-06-14 03:58:03+00:00,Probabilistic Conformal Prediction Using Conditional Random Samples,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Zhendong Wang'), arxiv.Result.Author('Ruijiang Gao'), arxiv.Result.Author('Mingzhang Yin'), arxiv.Result.Author('Mingyuan Zhou'), arxiv.Result.Author('David M. Blei')]","This paper proposes probabilistic conformal prediction (PCP), a predictive
inference algorithm that estimates a target variable by a discontinuous
predictive set. Given inputs, PCP construct the predictive set based on random
samples from an estimated generative model. It is efficient and compatible with
either explicit or implicit conditional generative models. Theoretically, we
show that PCP guarantees correct marginal coverage with finite samples.
Empirically, we study PCP on a variety of simulated and real datasets. Compared
to existing methods for conformal inference, PCP provides sharper predictive
sets.",-0.11460365,0.20449148,-0.0016696695,B
7590,"The detailed quantitative results
are in Table 8, Appendix E.

We further study conformal methods on two multi-target real datasets.","We observe when œÅ gets higher, only PCP shrinks the predictive set accordingly while the
predictions from other methods have little change and become loose.","Taxi Data are the taxi trip records
of New York City which include the pickup, drop-oÔ¨Ä locations of each trip and the corresponding time.",2022-06-14 03:58:03+00:00,Probabilistic Conformal Prediction Using Conditional Random Samples,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Zhendong Wang'), arxiv.Result.Author('Ruijiang Gao'), arxiv.Result.Author('Mingzhang Yin'), arxiv.Result.Author('Mingyuan Zhou'), arxiv.Result.Author('David M. Blei')]","This paper proposes probabilistic conformal prediction (PCP), a predictive
inference algorithm that estimates a target variable by a discontinuous
predictive set. Given inputs, PCP construct the predictive set based on random
samples from an estimated generative model. It is efficient and compatible with
either explicit or implicit conditional generative models. Theoretically, we
show that PCP guarantees correct marginal coverage with finite samples.
Empirically, we study PCP on a variety of simulated and real datasets. Compared
to existing methods for conformal inference, PCP provides sharper predictive
sets.",-0.11460365,0.20449148,-0.0016696695,B
7626,"Despite the progress achieved, further research on developing non-linear regression models to handle general dependence
                                          relationships between predictors and time events is needed.","[2018], Steingrimsson and Morrison [2020]) or the conditional quantiles (Jia and Jeong [2022]).","Also, model complexity ought to be reduced through
                                          simultaneous variable selection.",2022-06-14 14:40:10+00:00,Neural interval-censored Cox regression with feature selection,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Carlos Garc√≠a Meixide'), arxiv.Result.Author('Marcos Matabuena'), arxiv.Result.Author('Michael R. Kosorok')]","The classical Cox model emerged in 1972 promoting breakthroughs in how
patient prognosis is quantified using time-to-event analysis in biomedicine.
One of the most useful characteristics of the model for practitioners is the
interpretability of the variables in the analysis. However, this comes at the
price of introducing strong assumptions concerning the functional form of the
regression model. To break this gap, this paper aims to exploit the
explainability advantages of the classical Cox model in the setting of
interval-censoring using a new Lasso neural network that simultaneously selects
the most relevant variables while quantifying non-linear relations between
predictors and survival times. The gain of the new method is illustrated
empirically in an extensive simulation study with examples that involve linear
and non-linear ground dependencies. We also demonstrate the performance of our
strategy in the analysis of physiological, clinical and accelerometer data from
the NHANES 2003-2006 waves to predict the effect of physical activity on the
survival of patients. Our method outperforms the prior results in the
literature that use the traditional Cox model.",-0.3143962,0.14455812,-0.36751944,C
7627,"Despite the progress achieved, further research on developing non-linear regression models to handle general dependence
                                          relationships between predictors and time events is needed.","[2018], Steingrimsson and Morrison [2020]) or the conditional quantiles (Jia and Jeong [2022]).","Also, model complexity ought to be reduced through
                                          simultaneous variable selection.",2022-06-14 14:40:10+00:00,Neural interval-censored Cox regression with feature selection,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Carlos Garc√≠a Meixide'), arxiv.Result.Author('Marcos Matabuena'), arxiv.Result.Author('Michael R. Kosorok')]","The classical Cox model emerged in 1972 promoting breakthroughs in how
patient prognosis is quantified using time-to-event analysis in biomedicine.
One of the most useful characteristics of the model for practitioners is the
interpretability of the variables in the analysis. However, this comes at the
price of introducing strong assumptions concerning the functional form of the
regression model. To break this gap, this paper aims to exploit the
explainability advantages of the classical Cox model in the setting of
interval-censoring using a new Lasso neural network that simultaneously selects
the most relevant variables while quantifying non-linear relations between
predictors and survival times. The gain of the new method is illustrated
empirically in an extensive simulation study with examples that involve linear
and non-linear ground dependencies. We also demonstrate the performance of our
strategy in the analysis of physiological, clinical and accelerometer data from
the NHANES 2003-2006 waves to predict the effect of physical activity on the
survival of patients. Our method outperforms the prior results in the
literature that use the traditional Cox model.",-0.3143962,0.14455812,-0.36751944,C
7656,"We encourage researchers in our community to further study those potential safety
concerns and the potential improvements in order to develop more mature supervised learning
tools.","Moreover, the
computation eÔ¨Éciency of CARD may also be further investigated if the dataset size becomes
larger.","C Computational Resources

Our models are trained and evaluated with a single Nvidia GeForce RTX 3090 GPU.",2022-06-15 03:30:38+00:00,CARD: Classification and Regression Diffusion Models,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Xizewen Han'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Mingyuan Zhou')]","Learning the distribution of a continuous or categorical response variable
$\boldsymbol y$ given its covariates $\boldsymbol x$ is a fundamental problem
in statistics and machine learning. Deep neural network-based supervised
learning algorithms have made great progress in predicting the mean of
$\boldsymbol y$ given $\boldsymbol x$, but they are often criticized for their
ability to accurately capture the uncertainty of their predictions. In this
paper, we introduce classification and regression diffusion (CARD) models,
which combine a denoising diffusion-based conditional generative model and a
pre-trained conditional mean estimator, to accurately predict the distribution
of $\boldsymbol y$ given $\boldsymbol x$. We demonstrate the outstanding
ability of CARD in conditional distribution prediction with both toy examples
and real-world datasets, the experimental results on which show that CARD in
general outperforms state-of-the-art methods, including Bayesian neural
network-based ones that are designed for uncertainty estimation, especially
when the conditional distribution of $\boldsymbol y$ given $\boldsymbol x$ is
multi-modal.",-0.17377348,0.4843849,0.3360748,B
7657,"We encourage researchers in our community to further study those potential
safety concerns and approaches for improvements in order to develop more mature supervised
learning tools.","Moreover, the computation eÔ¨Éciency of CARD may also be further investigated if the dataset
size becomes larger.","C Computational Resources

Our models are trained and evaluated with a single Nvidia GeForce RTX 3090 GPU.",2022-06-15 03:30:38+00:00,CARD: Classification and Regression Diffusion Models,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Xizewen Han'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Mingyuan Zhou')]","Learning the distribution of a continuous or categorical response variable
$\boldsymbol y$ given its covariates $\boldsymbol x$ is a fundamental problem
in statistics and machine learning. Deep neural network-based supervised
learning algorithms have made great progress in predicting the mean of
$\boldsymbol y$ given $\boldsymbol x$, but they are often criticized for their
ability to accurately capture the uncertainty of their predictions. In this
paper, we introduce classification and regression diffusion (CARD) models,
which combine a denoising diffusion-based conditional generative model and a
pre-trained conditional mean estimator, to accurately predict the distribution
of $\boldsymbol y$ given $\boldsymbol x$. We demonstrate the outstanding
ability of CARD in conditional distribution prediction with both toy examples
and real-world datasets, the experimental results on which show that CARD in
general outperforms state-of-the-art methods, including Bayesian neural
network-based ones that are designed for uncertainty estimation, especially
when the conditional distribution of $\boldsymbol y$ given $\boldsymbol x$ is
multi-modal.",-0.16885662,0.48789257,0.33575672,B
7658,"We encourage researchers
in our community to further study those potential safety concerns and approaches for improvements
in order to develop more mature supervised learning tools.","Moreover, the computation efÔ¨Åciency of
CARD may also be further investigated if the dataset size becomes larger.","C Computational Resources

Our models are trained and evaluated with a single Nvidia GeForce RTX 3090 GPU.",2022-06-15 03:30:38+00:00,CARD: Classification and Regression Diffusion Models,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Xizewen Han'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Mingyuan Zhou')]","Learning the distribution of a continuous or categorical response variable
$\boldsymbol y$ given its covariates $\boldsymbol x$ is a fundamental problem
in statistics and machine learning. Deep neural network-based supervised
learning algorithms have made great progress in predicting the mean of
$\boldsymbol y$ given $\boldsymbol x$, but they are often criticized for their
ability to accurately capture the uncertainty of their predictions. In this
paper, we introduce classification and regression diffusion (CARD) models,
which combine a denoising diffusion-based conditional generative model and a
pre-trained conditional mean estimator, to accurately predict the distribution
of $\boldsymbol y$ given $\boldsymbol x$. We demonstrate the outstanding
ability of CARD in conditional distribution prediction with both toy examples
and real-world datasets, the experimental results on which show that CARD in
general outperforms state-of-the-art methods, including Bayesian neural
network-based ones that are designed for uncertainty estimation, especially
when the conditional distribution of $\boldsymbol y$ given $\boldsymbol x$ is
multi-modal. In addition, we utilize the stochastic nature of the generative
model outputs to obtain a finer granularity in model confidence assessment at
the instance level for classification tasks.",-0.16019613,0.46703875,0.32088095,B
7659,"We encourage researchers
in our community to further study those potential safety concerns and approaches for improvements
in order to develop more mature supervised learning tools.","Moreover, the computation efÔ¨Åciency of
CARD may also be further investigated if the dataset size becomes larger.","C Computational Resources

Our models are trained and evaluated with a single Nvidia GeForce RTX 3090 GPU.",2022-06-15 03:30:38+00:00,CARD: Classification and Regression Diffusion Models,stat.ML,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('Xizewen Han'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Mingyuan Zhou')]","Learning the distribution of a continuous or categorical response variable
$\boldsymbol y$ given its covariates $\boldsymbol x$ is a fundamental problem
in statistics and machine learning. Deep neural network-based supervised
learning algorithms have made great progress in predicting the mean of
$\boldsymbol y$ given $\boldsymbol x$, but they are often criticized for their
ability to accurately capture the uncertainty of their predictions. In this
paper, we introduce classification and regression diffusion (CARD) models,
which combine a denoising diffusion-based conditional generative model and a
pre-trained conditional mean estimator, to accurately predict the distribution
of $\boldsymbol y$ given $\boldsymbol x$. We demonstrate the outstanding
ability of CARD in conditional distribution prediction with both toy examples
and real-world datasets, the experimental results on which show that CARD in
general outperforms state-of-the-art methods, including Bayesian neural
network-based ones that are designed for uncertainty estimation, especially
when the conditional distribution of $\boldsymbol y$ given $\boldsymbol x$ is
multi-modal. In addition, we utilize the stochastic nature of the generative
model outputs to obtain a finer granularity in model confidence assessment at
the instance level for classification tasks.",-0.16019613,0.46703875,0.32088095,B
7909,"2
further research into alternative ensemble methods, including ensembling over hyper-parameters (Wenzel
et al., 2020), architectures (Zaidi et al., 2021), and joint ensemble training (Webb et al., 2020).","This result, in turn, prompted

    1Or, more accurately, to symmetric losses.","In parallel,
several hypotheses have been proposed to explain the performance of deep ensembles.",2022-06-21 17:46:35+00:00,Ensembling over Classifiers: a Bias-Variance Perspective,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Neha Gupta'), arxiv.Result.Author('Jamie Smith'), arxiv.Result.Author('Ben Adlam'), arxiv.Result.Author('Zelda Mariet')]","Ensembles are a straightforward, remarkably effective method for improving
the accuracy,calibration, and robustness of models on classification tasks;
yet, the reasons that underlie their success remain an active area of research.
We build upon the extension to the bias-variance decomposition by Pfau (2013)
in order to gain crucial insights into the behavior of ensembles of
classifiers. Introducing a dual reparameterization of the bias-variance
tradeoff, we first derive generalized laws of total expectation and variance
for nonsymmetric losses typical of classification tasks. Comparing conditional
and bootstrap bias/variance estimates, we then show that conditional estimates
necessarily incur an irreducible error. Next, we show that ensembling in dual
space reduces the variance and leaves the bias unchanged, whereas standard
ensembling can arbitrarily affect the bias. Empirically, standard ensembling
reducesthe bias, leading us to hypothesize that ensembles of classifiers may
perform well in part because of this unexpected reduction.We conclude by an
empirical analysis of recent deep learning methods that ensemble over
hyperparameters, revealing that these techniques indeed favor bias reduction.
This suggests that, contrary to classical wisdom, targeting bias reduction may
be a promising direction for classifier ensembles.",0.12036288,0.2622856,0.04579856,B
7924,"To further study the linear speedup effect under various accuracy levels, we compute the total
generated samples for Ô¨Ånding an -optimal solution x¬Øt ‚àí x‚àó 2 ‚â§ and plot the 75% conÔ¨Ådence
region of log-sample against number of agents K = 5, 10, 20 for various ‚Äôs in Figure 4.",We provide the details of the baseline algorithm DSGD in Algorithm 4.,"Similar as
in Figure 2, we observe for all accuracy levels = 0.8 √ó 10‚àí6, 1.5 √ó 10‚àí6, 2 √ó 10‚àí6, the required
samples for Ô¨Ånding an -optimal solution by K agents are roughly the same, further demonstrating
the linear speedup effect.",2022-06-22 06:38:54+00:00,Decentralized Gossip-Based Stochastic Bilevel Optimization over Communication Networks,stat.ML,"['stat.ML', 'cs.LG', 'math.OC']","[arxiv.Result.Author('Shuoguang Yang'), arxiv.Result.Author('Xuezhou Zhang'), arxiv.Result.Author('Mengdi Wang')]","Bilevel optimization have gained growing interests, with numerous
applications found in meta learning, minimax games, reinforcement learning, and
nested composition optimization. This paper studies the problem of distributed
bilevel optimization over a network where agents can only communicate with
neighbors, including examples from multi-task, multi-agent learning and
federated learning. In this paper, we propose a gossip-based distributed
bilevel learning algorithm that allows networked agents to solve both the inner
and outer optimization problems in a single timescale and share information via
network propagation. We show that our algorithm enjoys the
$\mathcal{O}(\frac{1}{K \epsilon^2})$ per-agent sample complexity for general
nonconvex bilevel optimization and $\mathcal{O}(\frac{1}{K \epsilon})$ for
strongly convex objective, achieving a speedup that scales linearly with the
network size. The sample complexities are optimal in both $\epsilon$ and $K$.
We test our algorithm on the examples of hyperparameter tuning and
decentralized reinforcement learning. Simulated experiments confirmed that our
algorithm achieves the state-of-the-art training efficiency and test accuracy.",0.02463185,-0.09525211,0.14440924,A
8143,"further research into accurate and well-principled spectral
                                                                                         and Jacobian regularization techniques.","In Figure 5 (right) the                              that our scheme can yield additional beneÔ¨Åts and will spur
relative error for the power iteration scheme is visible.","From these plots we can see there is a small extra incurred
cost of working with our method compared to regularizing                                 References
with Spectral-Bound, but that our method has a signiÔ¨Åcantly
lower relative error while still being orders of magnitude                               Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
faster than calculating the analytical spectral norm.",2022-06-27 18:53:00+00:00,Exact Spectral Norm Regularization for Neural Networks,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Anton Johansson'), arxiv.Result.Author('Claes Stranneg√•rd'), arxiv.Result.Author('Niklas Engsner'), arxiv.Result.Author('Petter Mostad')]","We pursue a line of research that seeks to regularize the spectral norm of
the Jacobian of the input-output mapping for deep neural networks. While
previous work rely on upper bounding techniques, we provide a scheme that
targets the exact spectral norm. We showcase that our algorithm achieves an
improved generalization performance compared to previous spectral
regularization techniques while simultaneously maintaining a strong safeguard
against natural and adversarial noise. Moreover, we further explore some
previous reasoning concerning the strong adversarial protection that Jacobian
regularization provides and show that it can be misleading.",0.26920235,-0.09976786,0.11671209,A
8157,"We further study its robustness under
increasing correlations and distributional challenges (Sec.",5.2).,"5.3) as well as in embedding spaces of discriminatively
trained encoders (Sec.",2022-06-28 10:21:17+00:00,Disentangling Embedding Spaces with Minimal Distributional Assumptions,stat.ML,"['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tobias Leemann'), arxiv.Result.Author('Michael Kirchhof'), arxiv.Result.Author('Yao Rong'), arxiv.Result.Author('Enkelejda Kasneci'), arxiv.Result.Author('Gjergji Kasneci')]","Interest in understanding and factorizing learned embedding spaces is
growing. For instance, recent concept-based explanation techniques analyze a
machine learning model in terms of interpretable latent components. Such
components have to be discovered in the model's embedding space, e.g., through
independent component analysis (ICA) or modern disentanglement learning
techniques. While these unsupervised approaches offer a sound formal framework,
they either require access to a data generating function or impose rigid
assumptions on the data distribution, such as independence of components, that
are often violated in practice. In this work, we link conceptual explainability
for vision models with disentanglement learning and ICA. This enables us to
provide first theoretical results on how components can be identified without
requiring any distributional assumptions. From these insights, we derive the
disjoint attributions (DA) concept discovery method that is applicable to a
broader class of problems than current approaches but yet possesses a formal
identifiability guarantee. In an extensive comparison against component
analysis and over 300 state-of-the-art disentanglement models, DA stably
maintains superior performance, even under varying distributions and
correlation strengths.",0.17955193,0.05817892,0.08894324,A
8195,"Such prior knowledge typically amounts
                                          frameworks and results relating to deep learning methods in                    to an assumption that x‚àó lies in or near some restricted set X ,
                                          inverse problems, and highlight their strengths, limitations, and              which may be intrinsically low-dimensional despite Rn being
                                          directions for further research.","In this                ated algorithms/theory is the assumed prior knowledge on the
                                          paper, we provide an introductory overview of theoretical                      underlying signal x‚àó.",a high-dimensional space.,2022-06-29 02:37:50+00:00,Theoretical Perspectives on Deep Learning Methods in Inverse Problems,stat.ML,"['stat.ML', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Jonathan Scarlett'), arxiv.Result.Author('Reinhard Heckel'), arxiv.Result.Author('Miguel R. D. Rodrigues'), arxiv.Result.Author('Paul Hand'), arxiv.Result.Author('Yonina C. Eldar')]","In recent years, there have been significant advances in the use of deep
learning methods in inverse problems such as denoising, compressive sensing,
inpainting, and super-resolution. While this line of works has predominantly
been driven by practical algorithms and experiments, it has also given rise to
a variety of intriguing theoretical problems. In this paper, we survey some of
the prominent theoretical developments in this line of works, focusing in
particular on generative priors, untrained neural network priors, and unfolding
algorithms. In addition to summarizing existing results in these topics, we
highlight several ongoing challenges and open problems.",0.3662396,0.16746049,0.11530447,A
8196,"As a result, more Ô¨Åne-grained issues centered                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           constant, then it holds with probability 1 ‚àí e‚àí‚Ñ¶(m) that
around training, generalization, and representation error are
essentially abstracted away (though their further study would                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  xÀÜ ‚àí x‚àó 2 ‚â§ 6 min G(z) ‚àí x‚àó 2 + 3 Œ∑ 2.","[15] circumvented                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    z‚ààRk
these challenges by identifying simple properties of typical
generative models that sufÔ¨Åce to give meaningful recovery                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Then, if m = ‚Ñ¶(kd log w) with a sufÔ¨Åciently large implied
guarantees.","(16)
still be of signiÔ¨Åcant interest).",2022-06-29 02:37:50+00:00,Theoretical Perspectives on Deep Learning Methods in Inverse Problems,stat.ML,"['stat.ML', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Jonathan Scarlett'), arxiv.Result.Author('Reinhard Heckel'), arxiv.Result.Author('Miguel R. D. Rodrigues'), arxiv.Result.Author('Paul Hand'), arxiv.Result.Author('Yonina C. Eldar')]","In recent years, there have been significant advances in the use of deep
learning methods in inverse problems such as denoising, compressive sensing,
inpainting, and super-resolution. While this line of works has predominantly
been driven by practical algorithms and experiments, it has also given rise to
a variety of intriguing theoretical problems. In this paper, we survey some of
the prominent theoretical developments in this line of works, focusing in
particular on generative priors, untrained neural network priors, and unfolding
algorithms. In addition to summarizing existing results in these topics, we
highlight several ongoing challenges and open problems.",0.2325026,0.019986857,-0.16789606,A
8286,"Hence, there is a need to further study
the fundamentals of how loss landscapes and gradient Ô¨Åelds are affected by rigid contacts.","That
said, very few works considered contact-rich tasks, and one prior work highlighted the potential
fundamental limitations of gradients for rigid contacts [6].","For that,
we created several scenarios using Nimble [14] and Warp [27] frameworks (described below).",2022-06-28 17:08:53+00:00,Rethinking Optimization with Differentiable Simulation from a Global Perspective,stat.ML,"['stat.ML', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Rika Antonova'), arxiv.Result.Author('Jingyun Yang'), arxiv.Result.Author('Krishna Murthy Jatavallabhula'), arxiv.Result.Author('Jeannette Bohg')]","Differentiable simulation is a promising toolkit for fast gradient-based
policy optimization and system identification. However, existing approaches to
differentiable simulation have largely tackled scenarios where obtaining smooth
gradients has been relatively easy, such as systems with mostly smooth
dynamics. In this work, we study the challenges that differentiable simulation
presents when it is not feasible to expect that a single descent reaches a
global optimum, which is often a problem in contact-rich scenarios. We analyze
the optimization landscapes of diverse scenarios that contain both rigid bodies
and deformable objects. In dynamic environments with highly deformable objects
and fluids, differentiable simulators produce rugged landscapes with
nonetheless useful gradients in some parts of the space. We propose a method
that combines Bayesian optimization with semi-local 'leaps' to obtain a global
search method that can use gradients effectively, while also maintaining robust
performance in regions with noisy gradients. We show that our approach
outperforms several gradient-based and gradient-free baselines on an extensive
set of experiments in simulation, and also validate the method using
experiments with a real robot and deformables. Videos and supplementary
materials are available at https://tinyurl.com/globdiff",0.122392066,-0.023048997,0.07658345,A
8365,"A related
                                                          topic that deserves further research is the modeling
The uniform spectral gap of the graph pCN algo-           of Ô¨Çexible nonstationary graph-based GPs by appro-
rithm ensures its independent rate of convergence in      priate choice of graph-Laplacian and similarities be-
the limit N ‚Üí ‚àû with Ô¨Åxed number n of labels.","For instance, [SAY21] analyzes
                                                          the Ô¨Ånite element approach from spatial statistics us-
7.1 Theory                                                ing the techniques outlined in Section 6.",How-        tween features.,2022-07-03 18:39:03+00:00,Mathematical Foundations of Graph-Based Bayesian Semi-Supervised Learning,stat.ML,"['stat.ML', 'cs.LG', 'math.PR', 'math.ST', 'stat.ME', 'stat.TH']","[arxiv.Result.Author('Nicolas Garc√≠a Trillos'), arxiv.Result.Author('Daniel Sanz-Alonso'), arxiv.Result.Author('Ruiyi Yang')]","In recent decades, science and engineering have been revolutionized by a
momentous growth in the amount of available data. However, despite the
unprecedented ease with which data are now collected and stored, labeling data
by supplementing each feature with an informative tag remains to be
challenging. Illustrative tasks where the labeling process requires expert
knowledge or is tedious and time-consuming include labeling X-rays with a
diagnosis, protein sequences with a protein type, texts by their topic, tweets
by their sentiment, or videos by their genre. In these and numerous other
examples, only a few features may be manually labeled due to cost and time
constraints. How can we best propagate label information from a small number of
expensive labeled features to a vast number of unlabeled ones? This is the
question addressed by semi-supervised learning (SSL).
  This article overviews recent foundational developments on graph-based
Bayesian SSL, a probabilistic framework for label propagation using
similarities between features. SSL is an active research area and a thorough
review of the extant literature is beyond the scope of this article. Our focus
will be on topics drawn from our own research that illustrate the wide range of
mathematical tools and ideas that underlie the rigorous study of the
statistical accuracy and computational efficiency of graph-based Bayesian SSL.",0.1442958,-0.18742207,0.19298118,A
8366,The contrac-      text also requires further research.,"However, how best to utilize
ration of MCMC algorithms that scale in this joint        the Bayesian probabilistic framework in the SSL con-
limit is an interesting open direction.","We envision new
tion of the posterior distribution in this regime has     opportunities to develop active learning strategies for
been discussed in Section 6.",2022-07-03 18:39:03+00:00,Mathematical Foundations of Graph-Based Bayesian Semi-Supervised Learning,stat.ML,"['stat.ML', 'cs.LG', 'math.PR', 'math.ST', 'stat.ME', 'stat.TH']","[arxiv.Result.Author('Nicolas Garc√≠a Trillos'), arxiv.Result.Author('Daniel Sanz-Alonso'), arxiv.Result.Author('Ruiyi Yang')]","In recent decades, science and engineering have been revolutionized by a
momentous growth in the amount of available data. However, despite the
unprecedented ease with which data are now collected and stored, labeling data
by supplementing each feature with an informative tag remains to be
challenging. Illustrative tasks where the labeling process requires expert
knowledge or is tedious and time-consuming include labeling X-rays with a
diagnosis, protein sequences with a protein type, texts by their topic, tweets
by their sentiment, or videos by their genre. In these and numerous other
examples, only a few features may be manually labeled due to cost and time
constraints. How can we best propagate label information from a small number of
expensive labeled features to a vast number of unlabeled ones? This is the
question addressed by semi-supervised learning (SSL).
  This article overviews recent foundational developments on graph-based
Bayesian SSL, a probabilistic framework for label propagation using
similarities between features. SSL is an active research area and a thorough
review of the extant literature is beyond the scope of this article. Our focus
will be on topics drawn from our own research that illustrate the wide range of
mathematical tools and ideas that underlie the rigorous study of the
statistical accuracy and computational efficiency of graph-based Bayesian SSL.",0.0819276,0.15774831,-0.028576326,B
8422,"A further study on whether the trend during
that time period is meaningful to the U.S. economy would be an interesting topic.","However, such a trend is interrupted by the COVID-19 pan-
demic right after December 2019, making it unclear whether the trend would have continued
if there had not been the COVID-19 pandemic.","Moreover, the rolling-window p-value analysis by the FACT provides a way to unveil
the eÔ¨Äects of the COVID-19 pandemic.",2022-07-04 19:05:08+00:00,FACT: High-Dimensional Random Forests Inference,stat.ML,"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Chien-Ming Chi'), arxiv.Result.Author('Yingying Fan'), arxiv.Result.Author('Jinchi Lv')]","Random forests is one of the most widely used machine learning methods over
the past decade thanks to its outstanding empirical performance. Yet, because
of its black-box nature, the results by random forests can be hard to interpret
in many big data applications. Quantifying the usefulness of individual
features in random forests learning can greatly enhance its interpretability.
Existing studies have shown that some popularly used feature importance
measures for random forests suffer from the bias issue. In addition, there lack
comprehensive size and power analyses for most of these existing methods. In
this paper, we approach the problem via hypothesis testing, and suggest a
framework of the self-normalized feature-residual correlation test (FACT) for
evaluating the significance of a given feature in the random forests model with
bias-resistance property, where our null hypothesis concerns whether the
feature is conditionally independent of the response given all other features.
Such an endeavor on random forests inference is empowered by some recent
developments on high-dimensional random forests consistency. The vanilla
version of our FACT test can suffer from the bias issue in the presence of
feature dependency. We exploit the techniques of imbalancing and conditioning
for bias correction. We further incorporate the ensemble idea into the FACT
statistic through feature transformations for the enhanced power. Under a
fairly general high-dimensional nonparametric model setting with dependent
features, we formally establish that FACT can provide theoretically justified
random forests feature p-values and enjoy appealing power through nonasymptotic
analyses. The theoretical results and finite-sample advantages of the newly
suggested method are illustrated with several simulation examples and an
economic forecasting application in relation to COVID-19.",-0.36319232,-0.047833346,-0.20790996,C
8536,"We expect this work to
motivate physics-inspired algorithms and further research on the emerging Ô¨Åelds at the intersection of physics and
machine learning.",This discussion tells us that QAVB can be realized in a quantum system.,"Appendix A: Introduction of Appendices

   This supplemental information (SI) covers background material such as, the Gaussian Mixture Model (GMM), and
the conjugate priors used for various parameters in the GMM.",2022-07-07 06:06:36+00:00,Quantum Advantage in Variational Bayes Inference,stat.ML,"['stat.ML', 'cond-mat.stat-mech', 'cs.LG', 'quant-ph']","[arxiv.Result.Author('Hideyuki Miyahara'), arxiv.Result.Author('Vwani Roychowdhury')]","Variational Bayes (VB) inference algorithm is used widely to estimate both
the parameters and the unobserved hidden variables in generative statistical
models. The algorithm -- inspired by variational methods used in computational
physics -- is iterative and can get easily stuck in local minima, even when
classical techniques, such as deterministic annealing (DA), are used. We study
a variational Bayes (VB) inference algorithm based on a non-traditional quantum
annealing approach -- referred to as quantum annealing variational Bayes (QAVB)
inference -- and show that there is indeed a quantum advantage to QAVB over its
classical counterparts. In particular, we show that such better performance is
rooted in key concepts from quantum mechanics: (i) the ground state of the
Hamiltonian of a quantum system -- defined from the given variational Bayes
(VB) problem -- corresponds to an optimal solution for the minimization problem
of the variational free energy at very low temperatures; (ii) such a ground
state can be achieved by a technique paralleling the quantum annealing process;
and (iii) starting from this ground state, the optimal solution to the VB
problem can be achieved by increasing the heat bath temperature to unity, and
thereby avoiding local minima introduced by spontaneous symmetry breaking
observed in classical physics based VB algorithms. We also show that the update
equations of QAVB can be potentially implemented using $\lceil \log K \rceil$
qubits and $\mathcal{O} (K)$ operations per step. Thus, QAVB can match the time
complexity of existing VB algorithms, while delivering higher performance.",0.2882161,0.14776513,0.057302482,A
8909,"While other authors have reviewed available literature
and concluded that there was not suÔ¨Écient evidence to support or reject this
hypothesis (Dagorn et al., 2012), further research has been called for.","In a wider context, one of the main concerns around dFAD use has been
centered on the possibility that dFADs could constitute an ecological trap,
whereby tuna remain associated to the dFAD even as it drifts into areas that are

                                                   16
not favorable for the tuna‚Äôs growth and development (Marsac et al., 2000; Hallier
and Gaertner, 2008).","One
of the novel aspects of the current study was the application of a regression
model to the echo-sounder buoy data, which allowed for direct estimates of tuna
biomass aggregated to the dFAD (Precioso et al., 2022), and the calculation
of two derived metrics: AT and DT, which could provide further insight into
the nature of tuna‚Äôs association to dFADs.",2022-07-14 16:45:54+00:00,How do tuna schools associate to dFADs? A study using echo-sounder buoys to identify global patterns,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Manuel Navarro-Garc√≠a'), arxiv.Result.Author('Daniel Precioso'), arxiv.Result.Author(""Kathryn Gavira-O'Neill""), arxiv.Result.Author('Alberto Torres-Barr√°n'), arxiv.Result.Author('David Gordo'), arxiv.Result.Author('V√≠ctor Gallego'), arxiv.Result.Author('David G√≥mez-Ullate')]","Based on the data gathered by echo-sounder buoys attached to drifting Fish
Aggregating Devices (dFADs) across tropical oceans, the current study applies a
Machine Learning protocol to examine the temporal trends of tuna schools'
association to drifting objects. Using a binary output, metrics typically used
in the literature were adapted to account for the fact that the entire tuna
aggregation under the dFAD was considered. The median time it took tuna to
colonize the dFADs for the first time varied between 25 and 43 days, depending
on the ocean, and the longest soak and colonization times were registered in
the Pacific Ocean. The tuna schools' Continuous Residence Times were generally
shorter than Continuous Absence Times (median values between 5 and 7 days, and
9 and 11 days, respectively), in line with the results found by previous
studies. Using a regression output, two novel metrics, namely aggregation time
and disaggregation time, were estimated to obtain further insight into the
symmetry of the aggregation process. Across all oceans, the time it took for
the tuna aggregation to depart from the dFADs was not significantly longer than
the time it took for the aggregation to form. The value of these results in the
context of the ""ecological trap"" hypothesis is discussed, and further analyses
to enrich and make use of this data source are proposed.",-0.33598864,-0.101014405,-0.09725231,C
8999,"This result calls for further research, exploring systematic
ways to combine the multivariate standpoint of ManiFeSt with univariate considerations.","Now, the maximal accuracy is 88.1%, and it
is obtained by selecting only 40 features.","The enhanced generalization capabilities are demonstrated here only with respect to Ô¨Ålter methods,
since embedded and wrapper methods typically suffer from large generalization errors when applied
to small datasets [16, 13, 70].",2022-07-18 12:58:01+00:00,ManiFeSt: Manifold-based Feature Selection for Small Data Sets,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('David Cohen'), arxiv.Result.Author('Tal Shnitzer'), arxiv.Result.Author('Yuval Kluger'), arxiv.Result.Author('Ronen Talmon')]","In this paper, we present a new method for few-sample supervised feature
selection (FS). Our method first learns the manifold of the feature space of
each class using kernels capturing multi-feature associations. Then, based on
Riemannian geometry, a composite kernel is computed, extracting the differences
between the learned feature associations. Finally, a FS score based on spectral
analysis is proposed. Considering multi-feature associations makes our method
multivariate by design. This in turn allows for the extraction of the hidden
manifold underlying the features and avoids overfitting, facilitating
few-sample FS. We showcase the efficacy of our method on illustrative examples
and several benchmarks, where our method demonstrates higher accuracy in
selecting the informative features compared to competing methods. In addition,
we show that our FS leads to improved classification and better generalization
when applied to test data.",-0.1508098,0.18697298,0.19743375,B
9186,"Also, our further research will be aimed at
improving the scalability of the proposed approach using sparse GP formu-
lations.","In future work, we will attempt to overcome this issue by
proposing a new hybrid GP CC-OPF method that combines linear and non-
linear approximation methods.","References

 [1] Z. Liu, L. v. d. Sluis, W. Winter, Challenges, experiences and possible
      solutions in transmission system operation with large wind integration,

                                               25
      OSTI (2012).",2022-07-21 23:02:35+00:00,Data-Driven Stochastic AC-OPF using Gaussian Processes,stat.ML,"['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Mile Mitrovic'), arxiv.Result.Author('Aleksandr Lukashevich'), arxiv.Result.Author('Petr Vorobev'), arxiv.Result.Author('Vladimir Terzija'), arxiv.Result.Author('Semen Budenny'), arxiv.Result.Author('Yury Maximov'), arxiv.Result.Author('Deepjyoti Deka')]","In recent years, electricity generation has been responsible for more than a
quarter of the greenhouse gas emissions in the US. Integrating a significant
amount of renewables into a power grid is probably the most accessible way to
reduce carbon emissions from power grids and slow down climate change.
Unfortunately, the most accessible renewable power sources, such as wind and
solar, are highly fluctuating and thus bring a lot of uncertainty to power grid
operations and challenge existing optimization and control policies. The
chance-constrained alternating current (AC) optimal power flow (OPF) framework
finds the minimum cost generation dispatch maintaining the power grid
operations within security limits with a prescribed probability. Unfortunately,
the AC-OPF problem's chance-constrained extension is non-convex,
computationally challenging, and requires knowledge of system parameters and
additional assumptions on the behavior of renewable distribution. Known linear
and convex approximations to the above problems, though tractable, are too
conservative for operational practice and do not consider uncertainty in system
parameters. This paper presents an alternative data-driven approach based on
Gaussian process (GP) regression to close this gap. The GP approach learns a
simple yet non-convex data-driven approximation to the AC power flow equations
that can incorporate uncertainty inputs. The latter is then used to determine
the solution of CC-OPF efficiently, by accounting for both input and parameter
uncertainty. The practical efficiency of the proposed approach using different
approximations for GP-uncertainty propagation is illustrated over numerous IEEE
test cases.",0.17346674,0.05899459,0.07761824,A
9313,"There are also a number of areas for further research in the intersection of DP estimators and statistical
depth.","These assumptions are unlikely to hold in practice for many
use cases of interest, especially for use cases in which one or more variables exhibit thick tail behavior, and
we are also not aware of a way to check these assumptions using a DP mechanism without making other
strong distributional assumptions.","First, Rousseeuw and Hubert (1999) provide a quantile regression estimator based on regression
depth, along the lines of Koenker and Bassett Jr (1978), and it appears that similar techniques to the ones
used here can be used to formulate Œ≤‚àísmooth upper bounds on the local sensitivity of these estimators.",2022-07-26 01:59:07+00:00,Differentially Private Estimation via Statistical Depth,stat.ML,"['stat.ML', 'cs.CR', 'cs.LG', 'econ.EM', 'stat.ME']",[arxiv.Result.Author('Ryan Cumings-Menon')],"Constructing a differentially private (DP) estimator requires deriving the
maximum influence of an observation, which can be difficult in the absence of
exogenous bounds on the input data or the estimator, especially in high
dimensional settings. This paper shows that standard notions of statistical
depth, i.e., halfspace depth and regression depth, are particularly
advantageous in this regard, both in the sense that the maximum influence of a
single observation is easy to analyze and that this value is typically low.
This is used to motivate new approximate DP location and regression estimators
using the maximizers of these two notions of statistical depth. A more
computationally efficient variant of the approximate DP regression estimator is
also provided. Also, to avoid requiring that users specify a priori bounds on
the estimates and/or the observations, variants of these DP mechanisms are
described that satisfy random differential privacy (RDP), which is a relaxation
of differential privacy provided by Hall, Wasserman, and Rinaldo (2013). We
also provide simulations of the two DP regression methods proposed here. The
proposed estimators appear to perform favorably relative to the existing DP
regression methods we consider in these simulations when either the sample size
is at least 100-200 or the privacy-loss budget is sufficiently high.",-0.13736272,-0.18498117,-0.3596726,C
9834,"How to measure variable importance in a tailored fashion for longitudinal data

                                                21
warrents further study, because this could be beneÔ¨Åcial in both understanding
disease progression and searching for target biomarkers for drug design.","Other approaches such as the variable-delete approach may also considered, but
correlation between predictor variables may negatively aÔ¨Äect its performance.","Another direction for future methodology development is on the eÔ¨Äective
handling of high dimensional longitudinal data.",2022-08-08 13:10:47+00:00,A review on longitudinal data analysis with random forest in precision medicine,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Jianchang Hu'), arxiv.Result.Author('Silke Szymczak')]","Precision medicine provides customized treatments to patients based on their
characteristics and is a promising approach to improving treatment efficiency.
Large scale omics data are useful for patient characterization, but often their
measurements change over time, leading to longitudinal data. Random forest is
one of the state-of-the-art machine learning methods for building prediction
models, and can play a crucial role in precision medicine. In this paper, we
review extensions of the standard random forest method for the purpose of
longitudinal data analysis. Extension methods are categorized according to the
data structures for which they are designed. We consider both univariate and
multivariate responses and further categorize the repeated measurements
according to whether the time effect is relevant. Information of available
software implementations of the reviewed extensions is also given. We conclude
with discussions on the limitations of our review and some future research
directions.",-0.36697507,0.12465751,-0.012679091,B
9961,Multiple directions of further research are considered at the moment.,"In contrast, the FoS model is the easiest to Ô¨Åt among all the
models applied, howbeit it performed poorly in non-linear situations.","Our
proposed models can be extended to predict multidimensional (mainly two-
dimensional) functional response where t ‚àà Rq, q > 1.",2022-08-10 16:04:58+00:00,Neural Networks for Scalar Input and Functional Output,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Sidi Wu'), arxiv.Result.Author('C√©dric Beaulac'), arxiv.Result.Author('Jiguo Cao')]","The regression of a functional response on a set of scalar predictors can be
a challenging task, especially if there is a large number of predictors, these
predictors have interaction effects, or the relationship between those
predictors and the response is nonlinear. In this work, we propose a solution
to this problem: a feed-forward neural network (NN) designed to predict a
functional response using scalar inputs. First, we transform the functional
response to a finite-dimension representation and then we construct a NN that
outputs this representation. We proposed different objective functions to train
the NN. The proposed models are suited for both regularly and irregularly
spaced data and also provide multiple ways to apply a roughness penalty to
control the smoothness of the predicted curve. The difficulty in implementing
both those features lies in the definition of objective functions that can be
back-propagated. In our experiments, we demonstrate that our model outperforms
the conventional function-on-scalar regression model in multiple scenarios
while computationally scaling better with the dimension of the predictors.",-0.09951319,0.09207311,-0.2303852,B
10081,"Identifying
such fuels using the current model speciÔ¨Åcation would not be possible; to do so would require
changing the predictors that compose x(Ai)(s, t), which could be done in a further study.","However, as the proportion increases above
approximately 10% the value of the log-odds of p0(s, t) begins to decrease; this may be due
to a decrease in the abundance of fuels that burn more eÔ¨Éciently than grassland.","Figure S2 provides a map of the median estimated p0(s, t) for a Ô¨Åxed time t; for brevity,
we consider here only the month July of 2007, as this month was the most devastating in
terms of burnt area across the entire U.S., but maps for other selected months are provided
in Figure S4.",2022-08-16 07:42:53+00:00,A unifying partially-interpretable framework for neural network-based extreme quantile regression,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Jordan Richards'), arxiv.Result.Author('Rapha√´l Huser')]","Risk management in many environmental settings requires an understanding of
the mechanisms that drive extreme events. Useful metrics for quantifying such
risk are extreme quantiles of response variables conditioned on predictor
variables that describe e.g., climate, biosphere and environmental states.
Typically these quantiles lie outside the range of observable data and so, for
estimation, require specification of parametric extreme value models within a
regression framework. Classical approaches in this context utilise linear or
additive relationships between predictor and response variables and suffer in
either their predictive capabilities or computational efficiency; moreover,
their simplicity is unlikely to capture the truly complex structures that lead
to the creation of extreme wildfires. In this paper, we propose a new
methodological framework for performing extreme quantile regression using
artificial neutral networks, which are able to capture complex non-linear
relationships and scale well to high-dimensional data. The ""black box"" nature
of neural networks means that they lack the desirable trait of interpretability
often favoured by practitioners; thus, we combine aspects of linear, and
additive, models with deep learning to create partially interpretable neural
networks that can be used for statistical inference but retain high prediction
accuracy. To complement this methodology, we further propose a novel point
process model for extreme values which overcomes the finite lower-endpoint
problem associated with the generalised extreme value class of distributions.
Efficacy of our unified framework is illustrated on U.S. wildfire data with a
high-dimensional predictor set and we illustrate vast improvements in
predictive performance over linear and spline-based regression techniques.",-0.30474955,0.012629406,-0.05448997,B
10082,"Identifying
such fuels using the current model speciÔ¨Åcation would not be possible; to do so would require
changing the predictors that compose x(Ai)(s, t), which could be done in a further study.","However, as the proportion increases above

approximately 10% the value of the log-odds of p0(s, t) begins to decrease; this may be due
to a decrease in the abundance of fuels that burn more eÔ¨Éciently than grassland.","Figure S2 provides a map of the median estimated p0(s, t) for a Ô¨Åxed time t; for brevity,
we consider here only the month July of 2007, as this month was the most devastating in
terms of burnt area across the entire U.S., but maps for other selected months are provided
in Figure S4.",2022-08-16 07:42:53+00:00,A unifying partially-interpretable framework for neural network-based extreme quantile regression,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Jordan Richards'), arxiv.Result.Author('Rapha√´l Huser')]","Risk management in many environmental settings requires an understanding of
the mechanisms that drive extreme events. Useful metrics for quantifying such
risk are extreme quantiles of response variables conditioned on predictor
variables that describe e.g., climate, biosphere and environmental states.
Typically these quantiles lie outside the range of observable data and so, for
estimation, require specification of parametric extreme value models within a
regression framework. Classical approaches in this context utilise linear or
additive relationships between predictor and response variables and suffer in
either their predictive capabilities or computational efficiency; moreover,
their simplicity is unlikely to capture the truly complex structures that lead
to the creation of extreme wildfires. In this paper, we propose a new
methodological framework for performing extreme quantile regression using
artificial neutral networks, which are able to capture complex non-linear
relationships and scale well to high-dimensional data. The ""black box"" nature
of neural networks means that they lack the desirable trait of interpretability
often favoured by practitioners; thus, we combine aspects of linear, and
additive, models with deep learning to create partially interpretable neural
networks that can be used for statistical inference but retain high prediction
accuracy. To complement this methodology, we further propose a novel point
process model for extreme values which overcomes the finite lower-endpoint
problem associated with the generalised extreme value class of distributions.
Efficacy of our unified framework is illustrated on U.S. wildfire data with a
high-dimensional predictor set and we illustrate vast improvements in
predictive performance over linear and spline-based regression techniques.",-0.30474955,0.012629406,-0.05448997,B
10083,"Identifying
such fuels using the current model speciÔ¨Åcation is infeasible; to do so would require changing
the predictors that compose x(Ai)(s, t), which could be done in a further study.","However, as the proportion increases above
approximately 10% the value of the log-odds of p0(s, t) begins to decrease; this may be due
to a decrease in the abundance of fuels that burn more eÔ¨Éciently than grassland.","Figure 3 provides a map of the median estimated p0(s, t) for a Ô¨Åxed time t; for brevity,
we consider here only the month July of 2007, as this month was the most devastating in
terms of burnt area across the entire U.S., but maps for other selected months are provided
in Figure S1.",2022-08-16 07:42:53+00:00,Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Jordan Richards'), arxiv.Result.Author('Rapha√´l Huser')]","Risk management in many environmental settings requires an understanding of
the mechanisms that drive extreme events. Useful metrics for quantifying such
risk are extreme quantiles of response variables conditioned on predictor
variables that describe, e.g., climate, biosphere and environmental states.
Typically these quantiles lie outside the range of observable data and so, for
estimation, require specification of parametric extreme value models within a
regression framework. Classical approaches in this context utilise linear or
additive relationships between predictor and response variables and suffer in
either their predictive capabilities or computational efficiency; moreover,
their simplicity is unlikely to capture the truly complex structures that lead
to the creation of extreme wildfires. In this paper, we propose a new
methodological framework for performing extreme quantile regression using
artificial neutral networks, which are able to capture complex non-linear
relationships and scale well to high-dimensional data. The ``black box"" nature
of neural networks means that they lack the desirable trait of interpretability
often favoured by practitioners; thus, we unify linear, and additive,
regression methodology with deep learning to create partially-interpretable
neural networks that can be used for statistical inference but retain high
prediction accuracy. To complement this methodology, we further propose a novel
point process model for extreme values which overcomes the finite
lower-endpoint problem associated with the generalised extreme value class of
distributions. Efficacy of our unified framework is illustrated on U.S.
wildfire data with a high-dimensional predictor set and we illustrate vast
improvements in predictive performance over linear and spline-based regression
techniques.",-0.3067732,0.007360235,-0.07209395,B
10133,"We suspect this result may not entirely generalize
support In contrast, TSRD-œÑ employing the œÑ -Lasso estimator                     to all scenarios and this topic requires further study from the
succeeds in recovering the correct support, indicated by the                     optimization perspective.","We observe in our simulations that both                      randomly due to potential nice landscape of optimization
œÑ - and MM-estimators fail in recovering the correct support                     problem in conjunction with collaborative nature of fusion
and all irrelevant variables are selected within the estimated                   procedure.",following contingency table.,2022-08-17 11:17:47+00:00,Two-Stage Robust and Sparse Distributed Statistical Inference for Large-Scale Data,stat.ML,"['stat.ML', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Emadaldin Mozafari-Majd'), arxiv.Result.Author('Visa Koivunen')]","In this paper, we address the problem of conducting statistical inference in
settings involving large-scale data that may be high-dimensional and
contaminated by outliers. The high volume and dimensionality of the data
require distributed processing and storage solutions. We propose a two-stage
distributed and robust statistical inference procedures coping with
high-dimensional models by promoting sparsity. In the first stage, known as
model selection, relevant predictors are locally selected by applying robust
Lasso estimators to the distinct subsets of data. The variable selections from
each computation node are then fused by a voting scheme to find the sparse
basis for the complete data set. It identifies the relevant variables in a
robust manner. In the second stage, the developed statistically robust and
computationally efficient bootstrap methods are employed. The actual inference
constructs confidence intervals, finds parameter estimates and quantifies
standard deviation. Similar to stage 1, the results of local inference are
communicated to the fusion center and combined there. By using analytical
methods, we establish the favorable statistical properties of the robust and
computationally efficient bootstrap methods including consistency for a fixed
number of predictors, and robustness. The proposed two-stage robust and
distributed inference procedures demonstrate reliable performance and
robustness in variable selection, finding confidence intervals and bootstrap
approximations of standard deviations even when data is high-dimensional and
contaminated by outliers.",0.027418472,0.030646976,-0.17367774,B
10183,"While our current paper includes results that     Œ∂k, B, F := k               k     (1   ‚àí  ‚àÜtœÑ )2k‚àí(  B+  F  )  ‚àÜ  t    B  +  F  ,
suggest that a cause of the decline of mean inference ac-
curacy with network size is connected to an increase of                               B  F                                        œÑ
a network‚Äôs mean anticlustering coeÔ¨Écient (see Section
VI B), we anticipate that further research on the chal-      which depends on k and ‚àÜtœÑ , and a matrix-valued part
lenges posed by large networks to inference methods is
a particularly interesting avenue of future research.","Fi-
nally, our results demonstrated that the mean inference      which has a scalar-valued part,
accuracy for many PEMs decreases with increasing net-
work size.","One    œÜ B , F := B + F A(1) F A(1) T B ,
can compute our proposed PEMs at low computational
cost for very large networks with hundreds or thousands      which is independent of k and ‚àÜtœÑ .",2022-08-18 14:46:05+00:00,Network inference via process motifs for lagged correlation in linear stochastic processes,stat.ML,"['stat.ML', 'cs.LG', 'cs.SI', 'math.DS', 'physics.soc-ph', '92C42, 92B20, 94A16']","[arxiv.Result.Author('Alice C. Schwarze'), arxiv.Result.Author('Sara M. Ichinaga'), arxiv.Result.Author('Bingni W. Brunton')]","A major challenge for causal inference from time-series data is the trade-off
between computational feasibility and accuracy. Motivated by process motifs for
lagged covariance in an autoregressive model with slow mean-reversion, we
propose to infer networks of causal relations via pairwise edge measure (PEMs)
that one can easily compute from lagged correlation matrices. Motivated by
contributions of process motifs to covariance and lagged variance, we formulate
two PEMs that correct for confounding factors and for reverse causation. To
demonstrate the performance of our PEMs, we consider network interference from
simulations of linear stochastic processes, and we show that our proposed PEMs
can infer networks accurately and efficiently. Specifically, for slightly
autocorrelated time-series data, our approach achieves accuracies higher than
or similar to Granger causality, transfer entropy, and convergent crossmapping
-- but with much shorter computation time than possible with any of these
methods. Our fast and accurate PEMs are easy-to-implement methods for network
inference with a clear theoretical underpinning. They provide promising
alternatives to current paradigms for the inference of linear models from
time-series data, including Granger causality, vector-autoregression, and
sparse inverse covariance estimation.",0.15465115,-0.07269034,-0.14848545,A
10184,"While our current paper includes results that     Œ∂k, B, F := k               k     (1   ‚àí  ‚àÜtœÑ )2k‚àí(  B+  F  )  ‚àÜ  t    B  +  F  ,
suggest that a cause of the decline of mean inference ac-
curacy with network size is connected to an increase of                               B  F                                        œÑ
a network‚Äôs mean anticlustering coeÔ¨Écient (see Section
VI B), we anticipate that further research on the chal-      which depends on k and ‚àÜtœÑ , and a matrix-valued part
lenges posed by large networks to inference methods is
a particularly interesting avenue of future research.","Fi-
nally, our results demonstrated that the mean inference      which has a scalar-valued part,
accuracy for many PEMs decreases with increasing net-
work size.","One    œÜ B , F := B + F A(1) F A(1) T B ,
can compute our proposed PEMs at low computational
cost for very large networks with hundreds or thousands      which is independent of k and ‚àÜtœÑ .",2022-08-18 14:46:05+00:00,Network inference via process motifs for lagged correlation in linear stochastic processes,stat.ML,"['stat.ML', 'cs.LG', 'cs.SI', 'math.DS', 'physics.soc-ph', '92C42, 92B20, 94A16']","[arxiv.Result.Author('Alice C. Schwarze'), arxiv.Result.Author('Sara M. Ichinaga'), arxiv.Result.Author('Bingni W. Brunton')]","A major challenge for causal inference from time-series data is the trade-off
between computational feasibility and accuracy. Motivated by process motifs for
lagged covariance in an autoregressive model with slow mean-reversion, we
propose to infer networks of causal relations via pairwise edge measure (PEMs)
that one can easily compute from lagged correlation matrices. Motivated by
contributions of process motifs to covariance and lagged variance, we formulate
two PEMs that correct for confounding factors and for reverse causation. To
demonstrate the performance of our PEMs, we consider network interference from
simulations of linear stochastic processes, and we show that our proposed PEMs
can infer networks accurately and efficiently. Specifically, for slightly
autocorrelated time-series data, our approach achieves accuracies higher than
or similar to Granger causality, transfer entropy, and convergent crossmapping
-- but with much shorter computation time than possible with any of these
methods. Our fast and accurate PEMs are easy-to-implement methods for network
inference with a clear theoretical underpinning. They provide promising
alternatives to current paradigms for the inference of linear models from
time-series data, including Granger causality, vector-autoregression, and
sparse inverse covariance estimation.",0.15465115,-0.07269034,-0.14848545,A
10527,"In the future, further research is planned for datasets with large numbers of observations for other compression
methods and basic algorithms.","MI‚Äôs distribution parameters, on the other hand, are closely related to the number
of value combinations [2], which may also cause local networks to be combined more incorrectly than BIC, for exam-
8                          Anna V. Bubnova / Approach of variable clustering and compression for learning large Bayesian networks

ple.","BIC                                                                                                                                                      MI

   1.5Relative accuracy                                                                                                                                    1.00
   1.0                                                                                                                                  Relative accuracy  0.75
   0.5                                                                                                                                                     0.50
   0.0                     mehra       hepar2  pathfinder                                                                                                  0.25                     mehra       hepar2  pathfinder
   0.5                                                                                                                                                     0.00
                                                                                                                                                           0.25
               sangiovese                                                                                                                                  0.50

                                                                                                                                                                        sangiovese

                                  (a)                                                                                                                                                      (b)

Fig.",2022-08-29 13:55:32+00:00,Approach of variable clustering and compression for learning large Bayesian networks,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Anna V. Bubnova')],"This paper describes a new approach for learning structures of large Bayesian
networks based on blocks resulting from feature space clustering. This
clustering is obtained using normalized mutual information. And the subsequent
aggregation of blocks is done using classical learning methods except that they
are input with compressed information about combinations of feature values for
each block. Validation of this approach is done for Hill-Climbing as a graph
enumeration algorithm for two score functions: BIC and MI. In this way,
potentially parallelizable block learning can be implemented even for those
score functions that are considered unsuitable for parallelizable learning. The
advantage of the approach is evaluated in terms of speed of work as well as the
accuracy of the found structures.",0.035258796,-0.058524758,0.09328957,B
10782,"It cer-
tainly deserves further research how this worst-case complexity can be reduced if additional
constraints on the considered preference system‚Äôs metric relation R2 are imposed.","Reducing computational complexity for special cases: In its current form, the number of con-
straints of the linear program for checking Œ¥-dominance given in Proposition 3 increases
with complexity of at worst O(d4), where d denotes the number of possible quality vectors
(or the number of attained quality vectors in the observed sample, respectively).","Extension to multi-criteria decision making: The concepts presented here need by no means
be limited to the comparison of classiÔ¨Åers.",2022-09-05 09:28:15+00:00,Statistical Comparisons of Classifiers by Generalized Stochastic Dominance,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME', '62G10, 62C05']","[arxiv.Result.Author('Christoph Jansen'), arxiv.Result.Author('Malte Nalenz'), arxiv.Result.Author('Georg Schollmeyer'), arxiv.Result.Author('Thomas Augustin')]","Although being a question in the very methodological core of machine
learning, there is still no unanimous consensus on how to compare classifiers.
Every comparison framework is confronted with (at least) three fundamental
challenges: the multiplicity of quality criteria, the multiplicity of data sets
and the randomness/arbitrariness of the selection of data sets. In this paper,
we add a fresh view to the vivid debate by adopting recent developments in
decision theory. Our resulting framework, based on so-called preference
systems, ranks classifiers by a generalized concept of stochastic dominance,
which powerfully circumvents the cumbersome, and often even self-contradictory,
reliance on aggregates. Moreover, we show that generalized stochastic dominance
can be operationalized by solving easy-to-handle linear programs and
statistically tested by means of an adapted two-sample
observation-randomization test. This indeed yields a powerful framework for the
statistical comparison of classifiers with respect to multiple quality criteria
simultaneously. We illustrate and investigate our framework in a simulation
study and with standard benchmark data sets.",-0.19943908,-0.17174688,0.048222676,C
11217,"The basic intuition and exciting empirical
results of SciDNet on simulated and real datasets encourage further research in multiple directions.","However, very few
of them can be directly applicable to the setting we consider in this paper.","One would be
interested in developing a theoretical foundation of this ‚Äôscreening‚Äô and ‚Äôcleaning‚Äô strategy for provable FDR control.",2022-09-15 02:58:42+00:00,Feature Selection integrated Deep Learning for Ultrahigh Dimensional and Highly Correlated Feature Space,stat.ML,"['stat.ML', 'cs.LG']",[arxiv.Result.Author('Arkaprabha Ganguli')],"In recent years, deep learning has been a topic of interest in almost all
disciplines due to its impressive empirical success in analyzing complex data
sets, such as imaging, genetics, climate, and medical data. While most of the
developments are treated as black-box machines, there is an increasing interest
in interpretable, reliable, and robust deep learning models applicable to a
broad class of applications. Feature-selected deep learning is proven to be
promising in this regard. However, the recent developments do not address the
situations of ultra-high dimensional and highly correlated feature selection in
addition to the high noise level. In this article, we propose a novel screening
and cleaning strategy with the aid of deep learning for the cluster-level
discovery of highly correlated predictors with a controlled error rate. A
thorough empirical evaluation over a wide range of simulated scenarios
demonstrates the effectiveness of the proposed method by achieving high power
while having a minimal number of false discoveries. Furthermore, we implemented
the algorithm in the riboflavin (vitamin $B_2$) production dataset in the
context of understanding the possible genetic association with riboflavin
production. The gain of the proposed methodology is illustrated by achieving
lower prediction error compared to other state-of-the-art methods.",-0.045433067,0.03133716,-0.17283258,B
11218,"The basic intuition and exciting empirical
results of SciDNet on simulated and real datasets encourage further research in multiple directions.","However, very few
of them can be directly applicable to the setting we consider in this paper.","One would be
interested in developing a theoretical foundation of this ‚Äôscreening‚Äô and ‚Äôcleaning‚Äô strategy for provable FDR control.",2022-09-15 02:58:42+00:00,Feature Selection integrated Deep Learning for Ultrahigh Dimensional and Highly Correlated Feature Space,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Arkaprabha Ganguli'), arxiv.Result.Author('Tapabrata Maiti')]","In recent years, deep learning has been a topic of interest in almost all
disciplines due to its impressive empirical success in analyzing complex data
sets, such as imaging, genetics, climate, and medical data. While most of the
developments are treated as black-box machines, there is an increasing interest
in interpretable, reliable, and robust deep learning models applicable to a
broad class of applications. Feature-selected deep learning is proven to be
promising in this regard. However, the recent developments do not address the
situations of ultra-high dimensional and highly correlated feature selection in
addition to the high noise level. In this article, we propose a novel screening
and cleaning strategy with the aid of deep learning for the cluster-level
discovery of highly correlated predictors with a controlled error rate. A
thorough empirical evaluation over a wide range of simulated scenarios
demonstrates the effectiveness of the proposed method by achieving high power
while having a minimal number of false discoveries. Furthermore, we implemented
the algorithm in the riboflavin (vitamin $B_2$) production dataset in the
context of understanding the possible genetic association with riboflavin
production. The gain of the proposed methodology is illustrated by achieving
lower prediction error compared to other state-of-the-art methods.",-0.045433067,0.03133716,-0.17283258,B
11219,"The basic intuition and exciting empirical
results of SciDNet on simulated and real datasets open avenues for further research.","The
proposed method SciDNet efÔ¨Åciently exploits several existing tools in statistics and ML literature to circumvent some
of the complexities that current DL-based models fail to address properly.","For example, one may be interested
in developing a theoretical foundation for this ‚Äôscreening‚Äô and ‚Äôcleaning‚Äô strategy for provable FDR control.",2022-09-15 02:58:42+00:00,Error Controlled Feature Selection for Ultrahigh Dimensional and Highly Correlated Feature Space Using Deep Learning,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Arkaprabha Ganguli'), arxiv.Result.Author('David Todem'), arxiv.Result.Author('Tapabrata Maiti')]","In recent years, deep learning has been at the center of analytics due to its
impressive empirical success in analyzing complex data objects. Despite this
success, most of the existing tools behave like black-box machines, thus the
increasing interest in interpretable, reliable, and robust deep learning models
applicable to a broad class of applications. Feature-selected deep learning has
emerged as a promising tool in this realm. However, the recent developments do
not accommodate ultra-high dimensional and highly correlated features, in
addition to the high noise level. In this article, we propose a novel screening
and cleaning method with the aid of deep learning for a data-adaptive
multi-resolutional discovery of highly correlated predictors with a controlled
error rate. Extensive empirical evaluations over a wide range of simulated
scenarios and several real datasets demonstrate the effectiveness of the
proposed method in achieving high power while keeping the false discovery rate
at a minimum.",-0.101667866,0.13539632,-0.18939687,B
11251,"This example guides
us to further study the topology properties of the distributions‚Äô support required by the FD.","(3)
Therefore, the FD is not a valid divergence here since FD(p||q) = 0 ‚áí p = q.","We Ô¨Årst
extend the Fisher divergence to distributions that have support on the connected space.",2022-09-15 15:56:42+00:00,Towards Healing the Blindness of Score Matching,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Mingtian Zhang'), arxiv.Result.Author('Oscar Key'), arxiv.Result.Author('Peter Hayes'), arxiv.Result.Author('David Barber'), arxiv.Result.Author('Brooks Paige'), arxiv.Result.Author('Fran√ßois-Xavier Briol')]","Score-based divergences have been widely used in machine learning and
statistics applications. Despite their empirical success, a blindness problem
has been observed when using these for multi-modal distributions. In this work,
we discuss the blindness problem and propose a new family of divergences that
can mitigate the blindness problem. We illustrate our proposed divergence in
the context of density estimation and report improved performance compared to
traditional approaches.",0.21098211,-0.38638514,-0.0015586838,A
11252,"This example guides
us to further study the topology properties of the distributions‚Äô support required by the FD.","(3)
Therefore, the FD is not a valid divergence here since FD(p||q) = 0 ‚áí p = q.","We Ô¨Årst
extend the Fisher divergence to distributions that have support on the connected space.",2022-09-15 15:56:42+00:00,Towards Healing the Blindness of Score Matching,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Mingtian Zhang'), arxiv.Result.Author('Oscar Key'), arxiv.Result.Author('Peter Hayes'), arxiv.Result.Author('David Barber'), arxiv.Result.Author('Brooks Paige'), arxiv.Result.Author('Fran√ßois-Xavier Briol')]","Score-based divergences have been widely used in machine learning and
statistics applications. Despite their empirical success, a blindness problem
has been observed when using these for multi-modal distributions. In this work,
we discuss the blindness problem and propose a new family of divergences that
can mitigate the blindness problem. We illustrate our proposed divergence in
the context of density estimation and report improved performance compared to
traditional approaches.",0.21098211,-0.38638514,-0.0015586838,A
11292,"As for TM method, although the proposed method of
estimation of the stratum P described in Section 4.3 works eÔ¨Äectively, we believe that this crucial problem is worth
further studying and there is still room for improvement.","First, in addition to MM algorithm, other non-convex
optimization procedures can be used in the JOINT method.","For the high-dimensional X consideration of the regularised
versions of the introduced methods is of interest.",2022-09-16 08:32:53+00:00,Joint estimation of posterior probability and propensity score function for positive and unlabelled data,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Konrad Furma≈Ñczyk'), arxiv.Result.Author('Jan Mielniczuk'), arxiv.Result.Author('Wojciech Rejchel'), arxiv.Result.Author('Pawe≈Ç Teisseyre')]","Positive and unlabelled learning is an important problem which arises
naturally in many applications. The significant limitation of almost all
existing methods lies in assuming that the propensity score function is
constant (SCAR assumption), which is unrealistic in many practical situations.
Avoiding this assumption, we consider parametric approach to the problem of
joint estimation of posterior probability and propensity score functions. We
show that under mild assumptions when both functions have the same parametric
form (e.g. logistic with different parameters) the corresponding parameters are
identifiable. Motivated by this, we propose two approaches to their estimation:
joint maximum likelihood method and the second approach based on alternating
maximization of two Fisher consistent expressions. Our experimental results
show that the proposed methods are comparable or better than the existing
methods based on Expectation-Maximisation scheme.",0.104732305,-0.11371414,0.094130695,A
11362,"In order to further study how the performance is degraded of Fed-ADMM when Ã∫ in-
creases, we let Ã∫ vary from 0 to 0.9 in steps of 0.1, and compare the averaged estimation
error for Fed-ADMM and Fed-ADMM-ES.","The two rows correspond to n = 50
and 100.","Besides, we also consider edge selection based
only on local estimators (Fed-ADMM-Local-ES).",2022-09-19 03:18:10+00:00,Heterogeneous Federated Learning on a Graph,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Huiyuan Wang'), arxiv.Result.Author('Xuyang Zhao'), arxiv.Result.Author('Wei Lin')]","Federated learning, where algorithms are trained across multiple
decentralized devices without sharing local data, is increasingly popular in
distributed machine learning practice. Typically, a graph structure $G$ exists
behind local devices for communication. In this work, we consider parameter
estimation in federated learning with data distribution and communication
heterogeneity, as well as limited computational capacity of local devices. We
encode the distribution heterogeneity by parametrizing distributions on local
devices with a set of distinct $p$-dimensional vectors. We then propose to
jointly estimate parameters of all devices under the $M$-estimation framework
with the fused Lasso regularization, encouraging an equal estimate of
parameters on connected devices in $G$. We provide a general result for our
estimator depending on $G$, which can be further calibrated to obtain
convergence rates for various specific problem setups. Surprisingly, our
estimator attains the optimal rate under certain graph fidelity condition on
$G$, as if we could aggregate all samples sharing the same distribution. If the
graph fidelity condition is not met, we propose an edge selection procedure via
multiple testing to ensure the optimality. To ease the burden of local
computation, a decentralized stochastic version of ADMM is provided, with
convergence rate $O(T^{-1}\log T)$ where $T$ denotes the number of iterations.
We highlight that, our algorithm transmits only parameters along edges of $G$
at each iteration, without requiring a central machine, which preserves
privacy. We further extend it to the case where devices are randomly
inaccessible during the training process, with a similar algorithmic
convergence guarantee. The computational and statistical efficiency of our
method is evidenced by simulation experiments and the 2020 US presidential
election data set.",0.030647788,-0.1995543,0.06489138,C
12349,"For example, we
observe that while increasing Œ± does not lower the Mean Squared Error (MSE) for log-likelihood estimation,
it can be useful in lowering the MSE of the Œ∏ gradient estimates (see Figures 17 and 18 of Appendix C.2.3),
paving the way for further research into the VR-IWAE bound methodology.","Lastly, the behavior of the VR-IWAE bound as well as the SNR behavior of its gradient estimators are not the
only way to measure the success of gradient-based methods involving the VR-IWAE bound.","We now move on to our third and Ô¨Ånal numerical experiment, in which we examine a real-data scenario.",2022-10-12 14:15:39+00:00,Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Kam√©lia Daudel'), arxiv.Result.Author('Joe Benton'), arxiv.Result.Author('Yuyang Shi'), arxiv.Result.Author('Arnaud Doucet')]","Several algorithms involving the Variational R\'enyi (VR) bound have been
proposed to minimize an alpha-divergence between a target posterior
distribution and a variational distribution. Despite promising empirical
results, those algorithms resort to biased stochastic gradient descent
procedures and thus lack theoretical guarantees. In this paper, we formalize
and study the VR-IWAE bound, a generalization of the Importance Weighted
Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several
desirable properties and notably leads to the same stochastic gradient descent
procedure as the VR bound in the reparameterized case, but this time by relying
on unbiased gradient estimators. We then provide two complementary theoretical
analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those
analyses shed light on the benefits or lack thereof of these bounds. Lastly, we
illustrate our theoretical claims over toy and real-data examples.",0.08055101,-0.051350616,-0.18002519,C
12386,"With further research and wider adoption of these
principles in SBI, we believe simulators will only become more powerful tools for scientiÔ¨Åc discovery.","The substantial difference in performance highlights, as suggested by Box (1980), that we should
not have blind faith that a model is sufÔ¨Åciently accurate, and therefore must devise methods for
performing model criticism and robust inference.","10
References

Atkeson, A., Kopecky, K., and Zha, T. (2020).",2022-10-12 20:06:55+00:00,Robust Neural Posterior Estimation and Statistical Model Criticism,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Daniel Ward'), arxiv.Result.Author('Patrick Cannon'), arxiv.Result.Author('Mark Beaumont'), arxiv.Result.Author('Matteo Fasiolo'), arxiv.Result.Author('Sebastian M Schmon')]","Computer simulations have proven a valuable tool for understanding complex
phenomena across the sciences. However, the utility of simulators for modelling
and forecasting purposes is often restricted by low data quality, as well as
practical limits to model fidelity. In order to circumvent these difficulties,
we argue that modellers must treat simulators as idealistic representations of
the true data generating process, and consequently should thoughtfully consider
the risk of model misspecification. In this work we revisit neural posterior
estimation (NPE), a class of algorithms that enable black-box parameter
inference in simulation models, and consider the implication of a
simulation-to-reality gap. While recent works have demonstrated reliable
performance of these methods, the analyses have been performed using synthetic
data generated by the simulator model itself, and have therefore only addressed
the well-specified case. In this paper, we find that the presence of
misspecification, in contrast, leads to unreliable inference when NPE is used
naively. As a remedy we argue that principled scientific inquiry with
simulators should incorporate a model criticism component, to facilitate
interpretable identification of misspecification and a robust inference
component, to fit 'wrong but useful' models. We propose robust neural posterior
estimation (RNPE), an extension of NPE to simultaneously achieve both these
aims, through explicitly modelling the discrepancies between simulations and
the observed data. We assess the approach on a range of artificially
misspecified examples, and find RNPE performs well across the tasks, whereas
naively using NPE leads to misleading and erratic posteriors.",-0.11536595,0.1609964,-0.15470192,B
12606,"However, the correct way to calibrate uncertainty
estimates in CNPs is a question that requires further study, as is the potential to generalise to more diverse function
classes.","The application of CNPs to impute sparse chemical datasets could be highly impactful, even
if one had conÔ¨Ådence in only a small fraction of the imputations.","Applying similar models for probabilistic prediction, such as LNPs, in more complex imputation and molecular
optimization tasks is an exciting area for future work.",2022-10-17 16:10:12+00:00,Conditional Neural Processes for Molecules,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Miguel Garcia-Ortegon'), arxiv.Result.Author('Andreas Bender'), arxiv.Result.Author('Sergio Bacallado')]","Neural processes (NPs) are models for transfer learning with properties
reminiscent of Gaussian Processes (GPs). They are adept at modelling data
consisting of few observations of many related functions on the same input
space and are trained by minimizing a variational objective, which is
computationally much less expensive than the Bayesian updating required by GPs.
So far, most studies of NPs have focused on low-dimensional datasets which are
not representative of realistic transfer learning tasks. Drug discovery is one
application area that is characterized by datasets consisting of many chemical
properties or functions which are sparsely observed, yet depend on shared
features or representations of the molecular inputs. This paper applies the
conditional neural process (CNP) to DOCKSTRING, a dataset of docking scores for
benchmarking ML models. CNPs show competitive performance in few-shot learning
tasks relative to supervised learning baselines common in QSAR modelling, as
well as an alternative model for transfer learning based on pre-training and
refining neural network regressors. We present a Bayesian optimization
experiment which showcases the probabilistic nature of CNPs and discuss
shortcomings of the model in uncertainty quantification.",0.030404646,0.17792103,-0.06948057,B
12607,"However, the correct way to calibrate uncertainty
estimates in CNPs is a question that requires further study, as is the potential to generalise to more diverse function
classes.","The application of CNPs to impute sparse chemical datasets could be highly impactful, even
if one had conÔ¨Ådence in only a small fraction of the imputations.","Applying similar models for probabilistic prediction, such as LNPs, in more complex imputation and molecular
optimization tasks is an exciting area for future work.",2022-10-17 16:10:12+00:00,Conditional Neural Processes for Molecules,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Miguel Garcia-Ortegon'), arxiv.Result.Author('Andreas Bender'), arxiv.Result.Author('Sergio Bacallado')]","Neural processes (NPs) are models for transfer learning with properties
reminiscent of Gaussian Processes (GPs). They are adept at modelling data
consisting of few observations of many related functions on the same input
space and are trained by minimizing a variational objective, which is
computationally much less expensive than the Bayesian updating required by GPs.
So far, most studies of NPs have focused on low-dimensional datasets which are
not representative of realistic transfer learning tasks. Drug discovery is one
application area that is characterized by datasets consisting of many chemical
properties or functions which are sparsely observed, yet depend on shared
features or representations of the molecular inputs. This paper applies the
conditional neural process (CNP) to DOCKSTRING, a dataset of docking scores for
benchmarking ML models. CNPs show competitive performance in few-shot learning
tasks relative to supervised learning baselines common in chemoinformatics, as
well as an alternative model for transfer learning based on pre-training and
refining neural network regressors. We present a Bayesian optimization
experiment which showcases the probabilistic nature of CNPs and discuss
shortcomings of the model in uncertainty quantification.",0.030404646,0.17792103,-0.06948057,B
12753,‚Ä¢ Section 5 closes with a summary of our paper and directions for further research.,‚Ä¢ Section 4 contains numerical examples that illustrate and complement the theory.,‚Ä¢ An appendix includes the proofs of our main results.,2022-10-20 02:15:34+00:00,Optimization on Manifolds via Graph Gaussian Processes,stat.ML,"['stat.ML', 'cs.LG', 'math.OC']","[arxiv.Result.Author('Hwanwoo Kim'), arxiv.Result.Author('Daniel Sanz-Alonso'), arxiv.Result.Author('Ruiyi Yang')]","This paper integrates manifold learning techniques within a \emph{Gaussian
process upper confidence bound} algorithm to optimize an objective function on
a manifold. Our approach is motivated by applications where a full
representation of the manifold is not available and querying the objective is
expensive. We rely on a point cloud of manifold samples to define a graph
Gaussian process surrogate model for the objective. Query points are
sequentially chosen using the posterior distribution of the surrogate model
given all previous queries. We establish regret bounds in terms of the number
of queries and the size of the point cloud. Several numerical examples
complement the theory and illustrate the performance of our method.",0.3192094,-0.34859443,-0.010480611,A
12938,"And these
                                                                  inspections often lead to further research ideas and model
From another practical point of view, robustness examina-         improvement directions.",ness under variable deployment circumstances.,"For instance, OOD detection in
tion and stress tests also beneÔ¨Åt from a ‚Äúcomplete‚Äù set of test   computer vision might fail because of the pixel correlation
sets.",2022-10-24 17:54:46+00:00,Bridging Machine Learning and Sciences: Opportunities and Challenges,stat.ML,"['stat.ML', 'cs.LG', 'hep-ex', 'hep-ph', 'physics.data-an']",[arxiv.Result.Author('Taoli Cheng')],"The application of machine learning in sciences has seen exciting advances in
recent years. As a widely-applicable technique, anomaly detection has been long
studied in the machine learning community. Especially, deep neural nets-based
out-of-distribution detection has made great progress for high-dimensional
data. Recently, these techniques have been showing their potential in
scientific disciplines. We take a critical look at their applicative prospects
including data universality, experimental protocols, model robustness, etc. We
discuss examples that display transferable practices and domain-specific
challenges simultaneously, providing a starting point for establishing a novel
interdisciplinary research paradigm in the near future.",-0.24179593,-0.041489672,0.06566671,C
13259,"5.2 Handwritten Digits Data

We further study the performance of augmentation invariant manifold learning on the
MNIST data set (LeCun et al., 1998).","Through Table 2, we can conclude that the classiÔ¨Åcation problem gets
more diÔ¨Écult when the regression is less smooths and the new data representation resulting
from augmentation invariant manifold learning can also be helpful in the non-smooth case.","MNIST data set includes 60000 training images
and 10000 testing images.",2022-11-01 13:42:44+00:00,Augmentation Invariant Manifold Learning,stat.ML,"['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",[arxiv.Result.Author('Shulei Wang')],"Data augmentation is a widely used technique and an essential ingredient in
the recent advance in self-supervised representation learning. By preserving
the similarity between augmented data, the resulting data representation can
improve various downstream analyses and achieve state-of-art performance in
many applications. To demystify the role of data augmentation, we develop a
statistical framework on a low-dimension product manifold to theoretically
understand why the unlabeled augmented data can lead to useful data
representation. Under this framework, we propose a new representation learning
method called augmentation invariant manifold learning and develop the
corresponding loss function, which can work with a deep neural network to learn
data representations. Compared with existing methods, the new data
representation simultaneously exploits the manifold's geometric structure and
invariant property of augmented data. Our theoretical investigation precisely
characterizes how the data representation learned from augmented data can
improve the $k$-nearest neighbor classifier in the downstream analysis, showing
that a more complex data augmentation leads to more improvement in downstream
analysis. Finally, numerical experiments on simulated and real datasets are
presented to support the theoretical results in this paper.",0.20510727,0.14598411,0.22626765,A
13320,"Potential directions for future work include
(i) seeking ‚àöŒ≥T (rather than Œ≥T ) dependence in the theoretical bounds, (ii) further studying more general
scenarios with separate functions for safety and reward, and (iii) determining other helpful function properties
beyond monotonicity.","6 Conclusion

We have demonstrated that monotonicity with respect to a single safety variable can have signiÔ¨Åcant beneÔ¨Åts
for safe GP exploration and optimisation, including improved theoretical guarantees, algorithmic simplicity,
and every safe point being reachable under mild conditions.","References

 [1] S. Amani, M. Alizadeh, and C. Thrampoulidis, ‚ÄúLinear stochastic bandits under safety constraints,‚Äù
      Advances in Neural Information Processing Systems, vol.",2022-11-03 02:52:30+00:00,Benefits of Monotonicity in Safe Exploration with Gaussian Processes,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Arpan Losalka'), arxiv.Result.Author('Jonathan Scarlett')]","We consider the problem of sequentially maximising an unknown function over a
set of actions while ensuring that every sampled point has a function value
below a given safety threshold. We model the function using kernel-based and
Gaussian process methods, while differing from previous works in our assumption
that the function is monotonically increasing with respect to a safety
variable. This assumption is motivated by various practical applications such
as adaptive clinical trial design and robotics. Taking inspiration from the
GP-UCB and SafeOpt algorithms, we propose an algorithm, monotone safe UCB
(M-SafeUCB) for this task. We show that M-SafeUCB enjoys theoretical guarantees
in terms of safety, a suitably-defined regret notion, and approximately finding
the entire safe boundary. In addition, we illustrate that the monotonicity
assumption yields significant benefits in terms of both the guarantees obtained
and the algorithmic simplicity. We support our theoretical findings by
performing empirical evaluations on a variety of functions.",0.15172482,0.08323992,-0.09759861,A
13330,We will further study the metagraph G through the following notion.,"The backward implication follows from the fact that each element g ‚àà Aut(G) can be uniquely represented as a product
th where t is a translation by some z ‚àà Zd2 and h is a permutation œÉ ‚àà Sn, this is a basic property of the semidirect product
Zd2 Sd.",DeÔ¨Ånition.,2022-11-03 10:18:17+00:00,Isotropic Gaussian Processes on Finite Spaces of Graphs,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Viacheslav Borovitskiy'), arxiv.Result.Author('Mohammad Reza Karimi'), arxiv.Result.Author('Vignesh Ram Somnath'), arxiv.Result.Author('Andreas Krause')]","We propose a principled way to define Gaussian process priors on various sets
of unweighted graphs: directed or undirected, with or without loops. We endow
each of these sets with a geometric structure, inducing the notions of
closeness and symmetries, by turning them into a vertex set of an appropriate
metagraph. Building on this, we describe the class of priors that respect this
structure and are analogous to the Euclidean isotropic processes, like squared
exponential or Mat\'ern. We propose an efficient computational technique for
the ostensibly intractable problem of evaluating these priors' kernels, making
such Gaussian processes usable within the usual toolboxes and downstream
applications. We go further to consider sets of equivalence classes of
unweighted graphs and define the appropriate versions of priors thereon. We
prove a hardness result, showing that in this case, exact kernel computation
cannot be performed efficiently. However, we propose a simple Monte Carlo
approximation for handling moderately sized cases. Inspired by applications in
chemistry, we illustrate the proposed techniques on a real molecular property
prediction task in the small data regime.",-0.028560948,-0.3977894,0.11008547,A
13331,We will further study the metagraph G through the following notion.,"The backward implication follows from the fact that each element g ‚àà Aut(G) can be uniquely represented as a product
th where t is a translation by some z ‚àà Zd2 and h is a permutation œÉ ‚àà Sn, this is a basic property of the semidirect product
Zd2 Sd.",DeÔ¨Ånition.,2022-11-03 10:18:17+00:00,Isotropic Gaussian Processes on Finite Spaces of Graphs,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Viacheslav Borovitskiy'), arxiv.Result.Author('Mohammad Reza Karimi'), arxiv.Result.Author('Vignesh Ram Somnath'), arxiv.Result.Author('Andreas Krause')]","We propose a principled way to define Gaussian process priors on various sets
of unweighted graphs: directed or undirected, with or without loops. We endow
each of these sets with a geometric structure, inducing the notions of
closeness and symmetries, by turning them into a vertex set of an appropriate
metagraph. Building on this, we describe the class of priors that respect this
structure and are analogous to the Euclidean isotropic processes, like squared
exponential or Mat\'ern. We propose an efficient computational technique for
the ostensibly intractable problem of evaluating these priors' kernels, making
such Gaussian processes usable within the usual toolboxes and downstream
applications. We go further to consider sets of equivalence classes of
unweighted graphs and define the appropriate versions of priors thereon. We
prove a hardness result, showing that in this case, exact kernel computation
cannot be performed efficiently. However, we propose a simple Monte Carlo
approximation for handling moderately sized cases. Inspired by applications in
chemistry, we illustrate the proposed techniques on a real molecular property
prediction task in the small data regime.",-0.028560948,-0.3977894,0.11008547,A
13409,"Section 5 summarizes the work of the paper and gives directions worthy
of further study.","Section 4 veriÔ¨Åes the effectiveness of our scheme through presenting experimental results on both
synthetic and real datasets.","2 RELATED WORKS AND CONTRIBUTION

The main idea of outlier detection schemes based on reconstruction error is that normal data points
can be well reconstructed, while outliers will generate large reconstruction errors (Bergman &
Hoshen, 2020).",2022-11-06 07:56:33+00:00,The Importance of Suppressing Complete Reconstruction in Autoencoders for Unsupervised Outlier Detection,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Yafei Shen'), arxiv.Result.Author('Ling Yang')]","Autoencoders are widely used in outlier detection due to their superiority in
handling high-dimensional and nonlinear datasets. The reconstruction of any
dataset by the autoencoder can be considered as a complex regression process.
In regression analysis, outliers can usually be divided into high leverage
points and influential points. Although the autoencoder has shown good results
for the identification of influential points, there are still some problems
when detect high leverage points. Through theoretical derivation, we found that
most outliers are detected in the direction corresponding to the
worst-recovered principal component, but in the direction of the well-recovered
principal components, the anomalies are often ignored. We propose a new loss
function which solve the above deficiencies in outlier detection. The core idea
of our scheme is that in order to better detect high leverage points, we should
suppress the complete reconstruction of the dataset to convert high leverage
points into influential points, and it is also necessary to ensure that the
differences between the eigenvalues of the covariance matrix of the original
dataset and their corresponding reconstructed results in the direction of each
principal component are equal. Besides, we explain the rationality of our
scheme through rigorous theoretical derivation. Finally, our experiments on
multiple datasets confirm that our scheme significantly improves the accuracy
of outlier detection.",0.037278242,-0.034091692,0.14180736,A
13410,"As we know, autoencoders will focus on recovering the principal components of a dataset eventually
(Bao et al., 2020; Oftadeh et al., 2020), it is necessary to use the unit orthogonal eigenvectors as the

                                                    4
basis vectors to further study the inÔ¨Çuence of the loss function on the outlier detection results of
the autoencoder.","Since most datasets follow a multidimensional Gaussian distribution, we assume
that the input variable X follows m-dimensional Gaussian distribution.","Under Assumption 1, if the eigenvalues of the covariance matrix Œ£x are Œª1, Œª2,
.",2022-11-06 07:56:33+00:00,The Importance of Suppressing Complete Reconstruction in Autoencoders for Unsupervised Outlier Detection,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Yafei Shen'), arxiv.Result.Author('Ling Yang')]","Autoencoders are widely used in outlier detection due to their superiority in
handling high-dimensional and nonlinear datasets. The reconstruction of any
dataset by the autoencoder can be considered as a complex regression process.
In regression analysis, outliers can usually be divided into high leverage
points and influential points. Although the autoencoder has shown good results
for the identification of influential points, there are still some problems
when detect high leverage points. Through theoretical derivation, we found that
most outliers are detected in the direction corresponding to the
worst-recovered principal component, but in the direction of the well-recovered
principal components, the anomalies are often ignored. We propose a new loss
function which solve the above deficiencies in outlier detection. The core idea
of our scheme is that in order to better detect high leverage points, we should
suppress the complete reconstruction of the dataset to convert high leverage
points into influential points, and it is also necessary to ensure that the
differences between the eigenvalues of the covariance matrix of the original
dataset and their corresponding reconstructed results in the direction of each
principal component are equal. Besides, we explain the rationality of our
scheme through rigorous theoretical derivation. Finally, our experiments on
multiple datasets confirm that our scheme significantly improves the accuracy
of outlier detection.",0.20833433,0.08243993,0.12992264,A
13418,"Study Objectives
         The research team identified that the association of contributing factors leading to pedestrian

crashes under different lighting conditions is still unexplored and there is scope for further research.","Considering this viewpoint, the ARM may be a much better tool as no variables are
assigned as dependent or independent.","The
objective of this study is to offer a novel methodology, ARM, to be used to (a) identify the significant
pattern of pedestrian crashes according to three different lighting conditions, (b) provide insights on how
to choose appropriate countermeasures according to the identified crash pattern.",2022-11-06 17:44:25+00:00,Applying Association Rules Mining to Investigate Pedestrian Fatal and Injury Crash Patterns Under Different Lighting Conditions,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Ahmed Hossain'), arxiv.Result.Author('Xiaoduan Sun'), arxiv.Result.Author('Raju Thapa'), arxiv.Result.Author('Julius Codjoe')]","The pattern of pedestrian crashes varies greatly depending on lighting
circumstances, emphasizing the need of examining pedestrian crashes in various
lighting conditions. Using Louisiana pedestrian fatal and injury crash data
(2010-2019), this study applied Association Rules Mining (ARM) to identify the
hidden pattern of crash risk factors according to three different lighting
conditions (daylight, dark-with-streetlight, and dark-no-streetlight). Based on
the generated rules, the results show that daylight pedestrian crashes are
associated with children (less than 15 years), senior pedestrians (greater than
64 years), older drivers (>64 years), and other driving behaviors such as
failure to yield, inattentive/distracted, illness/fatigue/asleep. Additionally,
young drivers (15-24 years) are involved in severe pedestrian crashes in
daylight conditions. This study also found pedestrian alcohol/drug involvement
as the most frequent item in the dark-with-streetlight condition. This crash
type is particularly associated with pedestrian action (crossing
intersection/midblock), driver age (55-64 years), speed limit (30-35 mph), and
specific area type (business with mixed residential area). Fatal pedestrian
crashes are found to be associated with roadways with high-speed limits (>50
mph) during the dark without streetlight condition. Some other risk factors
linked with high-speed limit related crashes are pedestrians walking
with/against the traffic, presence of pedestrian dark clothing, pedestrian
alcohol/drug involvement. The research findings are expected to provide an
improved understanding of the underlying relationships between pedestrian crash
risk factors and specific lighting conditions. Highway safety experts can
utilize these findings to conduct a decision-making process for selecting
effective countermeasures to reduce pedestrian crashes strategically.",-0.35818923,0.00380348,0.08053355,C
13477,"While we oÔ¨Äered
heuristics for dealing with multiple change-points in our HASC case study of Section 6,
this issue merits further study.","Finally, this paper focused on scenarios with a single change-point.","Breaking up multiple change-point problems into sin-
gle change-point problems is an established device in the change-point literature, see
e.g.",2022-11-07 20:59:14+00:00,Automatic Change-Point Detection in Time Series via Deep Learning,stat.ML,"['stat.ML', 'cs.LG', 'stat.ME']","[arxiv.Result.Author('Jie Li'), arxiv.Result.Author('Paul Fearnhead'), arxiv.Result.Author('Piotr Fryzlewicz'), arxiv.Result.Author('Tengyao Wang')]","Detecting change-points in data is challenging because of the range of
possible types of change and types of behaviour of data when there is no
change. Statistically efficient methods for detecting a change will depend on
both of these features, and it can be difficult for a practitioner to develop
an appropriate detection method for their application of interest. We show how
to automatically generate new detection methods based on training a neural
network. Our approach is motivated by many existing tests for the presence of a
change-point being able to be represented by a simple neural network, and thus
a neural network trained with sufficient data should have performance at least
as good as these methods. We present theory that quantifies the error rate for
such an approach, and how it depends on the amount of training data. Empirical
results show that, even with limited training data, its performance is
competitive with the standard CUSUM test for detecting a change in mean when
the noise is independent and Gaussian, and can substantially outperform it in
the presence of auto-correlated or heavy-tailed noise. Our method also shows
strong results in detecting and localising changes in activity based on
accelerometer data.",-0.14790188,-0.16904242,-0.0028302819,C
13634,"To further study the performance of
our estimators, we assume the squared loss induced by the initial batch satisÔ¨Åes the following
RSC condition.",We set Œªt ‚àù 2 ‚àáLt(Œ≤‚àó; Œ≤t‚àí1) ‚àû in the rest of this paper.,DeÔ¨Ånition 3.1.,2022-11-11 07:31:55+00:00,Streaming Sparse Linear Regression,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Shuoguang Yang'), arxiv.Result.Author('Yuhao Yan'), arxiv.Result.Author('Xiuneng Zhu'), arxiv.Result.Author('Qiang Sun')]","Sparse regression has been a popular approach to perform variable selection
and enhance the prediction accuracy and interpretability of the resulting
statistical model. Existing approaches focus on offline regularized regression,
while the online scenario has rarely been studied. In this paper, we propose a
novel online sparse linear regression framework for analyzing streaming data
when data points arrive sequentially. Our proposed method is memory efficient
and requires less stringent restricted strong convexity assumptions.
Theoretically, we show that with a properly chosen regularization parameter,
the $\ell_2$-norm statistical error of our estimator diminishes to zero in the
optimal order of $\tilde{O}({\sqrt{s/t}})$, where $s$ is the sparsity level,
$t$ is the streaming sample size, and $\tilde{O}(\cdot)$ hides logarithmic
terms. Numerical experiments demonstrate the practical efficiency of our
algorithm.",0.25487658,-0.12405443,-0.28421006,A
13813,"plainability methods, we hope to motivate further research
However, the degree of explanability is controlled by the      on explanation methods that evaluate properties such as
complexity of the model (Molnar et al.","By
weather community to replace the full Navier-Stokes equa-      interrogating and measuring the disagreement between ex-
tion with conceptual models that are more understandable.",2019).,2022-11-16 14:45:16+00:00,Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'physics.ao-ph', 'stat.AP']","[arxiv.Result.Author('Montgomery Flora'), arxiv.Result.Author('Corey Potvin'), arxiv.Result.Author('Amy McGovern'), arxiv.Result.Author('Shawn Handler')]","With increasing interest in explaining machine learning (ML) models, the
first part of this two-part study synthesizes recent research on methods for
explaining global and local aspects of ML models. This study distinguishes
explainability from interpretability, local from global explainability, and
feature importance versus feature relevance. We demonstrate and visualize
different explanation methods, how to interpret them, and provide a complete
Python package (scikit-explain) to allow future researchers to explore these
products. We also highlight the frequent disagreement between explanation
methods for feature rankings and feature effects and provide practical advice
for dealing with these disagreements. We used ML models developed for severe
weather prediction and sub-freezing road surface temperature prediction to
generalize the behavior of the different explanation methods. For feature
rankings, there is substantially more agreement on the set of top features
(e.g., on average, two methods agree on 6 of the top 10 features) than on
specific rankings (on average, two methods only agree on the ranks of 2-3
features in the set of top 10 features). On the other hand, two feature effect
curves from different methods are in high agreement as long as the phase space
is well sampled. Finally, a lesser-known method, tree interpreter, was found
comparable to SHAP for feature effects, and with the widespread use of random
forests in geosciences and computational ease of tree interpreter, we recommend
it be explored in future research.",-0.18448636,-0.068992965,-0.11067836,C
13814,"Similarly, there is high agreement between        noring feature interactions, but further research is required
SHAP and the logistic regression coeÔ¨Écients (Fig.","2017; Hooker and      between SAGE and FSP suggests that SAGE may be ig-
Mentch 2019).","6a-d),        to test this hypothesis.",2022-11-16 14:45:16+00:00,Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'physics.ao-ph', 'stat.AP']","[arxiv.Result.Author('Montgomery Flora'), arxiv.Result.Author('Corey Potvin'), arxiv.Result.Author('Amy McGovern'), arxiv.Result.Author('Shawn Handler')]","With increasing interest in explaining machine learning (ML) models, the
first part of this two-part study synthesizes recent research on methods for
explaining global and local aspects of ML models. This study distinguishes
explainability from interpretability, local from global explainability, and
feature importance versus feature relevance. We demonstrate and visualize
different explanation methods, how to interpret them, and provide a complete
Python package (scikit-explain) to allow future researchers to explore these
products. We also highlight the frequent disagreement between explanation
methods for feature rankings and feature effects and provide practical advice
for dealing with these disagreements. We used ML models developed for severe
weather prediction and sub-freezing road surface temperature prediction to
generalize the behavior of the different explanation methods. For feature
rankings, there is substantially more agreement on the set of top features
(e.g., on average, two methods agree on 6 of the top 10 features) than on
specific rankings (on average, two methods only agree on the ranks of 2-3
features in the set of top 10 features). On the other hand, two feature effect
curves from different methods are in high agreement as long as the phase space
is well sampled. Finally, a lesser-known method, tree interpreter, was found
comparable to SHAP for feature effects, and with the widespread use of random
forests in geosciences and computational ease of tree interpreter, we recommend
it be explored in future research.",-0.3593077,-0.032199502,-0.017687991,C
13827,"¬∑
                                          Œ≤ÀÜG ‚àí Œ≤‚àó                                                                (22)
                                                2 Œ∫2 min wg2                             n     n

                                                                     g‚àà[m]

Following the framework in (Negahban et al., 2012; Wainwright, 2019), we further study the applicability of the
restricted curvature conditions in terms of random design matrix.","2. with Œªn = m8incœÉwg                     dmaxnlog 5 + lognm + Œ¥, the following bound hold for Œ≤ÀÜG in (13)
                                   g‚àà[m]

                                                           2  œÉ2 g‚ààF ‚àí1(S)  wg2  ¬∑ dmaxlog 5 + log m + Œ¥ .","Given a group structure G, Theorem 4 is developed
based on the assumption that the Ô¨Åxed design matrix X satisÔ¨Åes the restricted curvature condition.",2022-11-16 21:21:41+00:00,The non-overlapping statistical approximation to overlapping group lasso,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Mingyu Qi'), arxiv.Result.Author('Tianxi Li')]","Group lasso is a commonly used regularization method in statistical learning
in which parameters are eliminated from the model according to predefined
groups. However, when the groups overlap, optimizing the group lasso penalized
objective can be time-consuming on large-scale problems because of the
non-separability induced by the overlapping groups. This bottleneck has
seriously limited the application of overlapping group lasso regularization in
many modern problems, such as gene pathway selection and graphical model
estimation. In this paper, we propose a separable penalty as an approximation
of the overlapping group lasso penalty. Thanks to the separability, the
computation of regularization based on our penalty is substantially faster than
that of the overlapping group lasso, especially for large-scale and
high-dimensional problems. We show that the penalty is the tightest separable
relaxation of the overlapping group lasso norm within the family of
$\ell_{q_1}/\ell_{q_2}$ norms. Moreover, we show that the estimator based on
the proposed separable penalty is statistically equivalent to the one based on
the overlapping group lasso penalty with respect to their error bounds and the
rate-optimal performance under the squared loss. We demonstrate the faster
computational time and statistical equivalence of our method compared with the
overlapping group lasso in simulation examples and a classification problem of
cancer tumors based on gene expression and multiple gene pathways.",0.19986045,-0.3019178,0.019713953,A
13986,"As ah ‚Üí ‚àû, we see either increasingly   the inclusion of multiple previous points in the auto-
                                                         regression context, further study is needed to under-
large steps or increasingly fast dynamics, which means   stand the scope of this eÔ¨Äect.",Ô¨Çow.,"that the transient behaviour fully concludes in each     4 Local Linearization
step.",2022-11-20 22:19:39+00:00,Approximate Uncertainty Propagation for Continuous Gaussian Process Dynamical Systems,stat.ML,"['stat.ML', 'cs.LG', 'math.DS']","[arxiv.Result.Author('Steffen Ridderbusch'), arxiv.Result.Author('Sina Ober-Bl√∂baum'), arxiv.Result.Author('Paul Goulart')]","When learning continuous dynamical systems with Gaussian Processes, computing
trajectories requires repeatedly mapping the distributions of uncertain states
through the distribution of learned nonlinear functions, which is generally
intractable. Since sampling-based approaches are computationally expensive, we
consider approximations of the output and trajectory distributions. We show
that existing methods make an incorrect implicit independence assumption and
underestimate the model-induced uncertainty. We propose a piecewise linear
approximation of the GP model yielding a class of numerical solvers for
efficient uncertainty estimates matching sampling-based methods.",0.094116285,-0.017378077,-0.21581438,A
14033,"We hope that our comprehensive algorithmic treatment of the above-described methodologies will
contribute to a better understanding of the connections and differences between the various Bayesian
methods for Neural Networks, will support the adoption of such methods in a wide range of appli-
cations, and promote further research in this Ô¨Åeld.","Lastly, by providing an introduction to manifold optimization, we provided a discussion
on methods that can implicitly deal with the positive-deÔ¨Ånite constraint over Gaussian variational
speciÔ¨Åcations, presenting the Manifold Gaussian Variational Bayes and Exact Manifold Gaussian
Variational Bayes solutions.","34
References

R. Abraham, J. E. Marsden, and T. Ratiu.",2022-11-21 21:36:58+00:00,Bayesian Learning for Neural Networks: an algorithmic survey,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Martin Magris'), arxiv.Result.Author('Alexandros Iosifidis')]","The last decade witnessed a growing interest in Bayesian learning. Yet, the
technicality of the topic and the multitude of ingredients involved therein,
besides the complexity of turning theory into practical implementations, limit
the use of the Bayesian learning paradigm, preventing its widespread adoption
across different fields and applications. This self-contained survey engages
and introduces readers to the principles and algorithms of Bayesian Learning
for Neural Networks. It provides an introduction to the topic from an
accessible, practical-algorithmic perspective. Upon providing a general
introduction to Bayesian Neural Networks, we discuss and present both standard
and recent approaches for Bayesian inference, with an emphasis on solutions
relying on Variational Inference and the use of Natural gradients. We also
discuss the use of manifold optimization as a state-of-the-art approach to
Bayesian learning. We examine the characteristic properties of all the
discussed methods, and provide pseudo-codes for their implementation, paying
attention to practical aspects, such as the computation of the gradients",0.42289203,0.024355471,0.17481512,A
14034,"We hope that our comprehensive algorithmic treatment of the above-described methodologies will
contribute to a better understanding of the connections and differences between the various Bayesian
methods for Neural Networks, will support the adoption of such methods in a wide range of appli-
cations, and promote further research in this Ô¨Åeld.","Lastly, by providing an introduction to manifold optimization, we provided a discussion
on methods that can implicitly deal with the positive-deÔ¨Ånite constraint over Gaussian variational
speciÔ¨Åcations, presenting the Manifold Gaussian Variational Bayes and Exact Manifold Gaussian
Variational Bayes solutions.","34
Acknowledgments

The research received funding from the European Union‚Äôs Horizon 2020 research and innovation
programme under the Marie Sk≈Çodowska-Curie project BNNmetrics (grant agreement No.",2022-11-21 21:36:58+00:00,Bayesian Learning for Neural Networks: an algorithmic survey,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Martin Magris'), arxiv.Result.Author('Alexandros Iosifidis')]","The last decade witnessed a growing interest in Bayesian learning. Yet, the
technicality of the topic and the multitude of ingredients involved therein,
besides the complexity of turning theory into practical implementations, limit
the use of the Bayesian learning paradigm, preventing its widespread adoption
across different fields and applications. This self-contained survey engages
and introduces readers to the principles and algorithms of Bayesian Learning
for Neural Networks. It provides an introduction to the topic from an
accessible, practical-algorithmic perspective. Upon providing a general
introduction to Bayesian Neural Networks, we discuss and present both standard
and recent approaches for Bayesian inference, with an emphasis on solutions
relying on Variational Inference and the use of Natural gradients. We also
discuss the use of manifold optimization as a state-of-the-art approach to
Bayesian learning. We examine the characteristic properties of all the
discussed methods, and provide pseudo-codes for their implementation, paying
attention to practical aspects, such as the computation of the gradients",0.3968256,0.02683603,0.15104108,A
14035,"We hope that our comprehensive algorithmic treatment of the above-described methodologies will
contribute to a better understanding of the connections and differences between the various Bayesian
methods for Neural Networks, will support the adoption of such methods in a wide range of appli-
cations, and promote further research in this Ô¨Åeld.","Lastly, by providing an introduction to manifold optimization, we provided a discussion
on methods that can implicitly deal with the positive-deÔ¨Ånite constraint over Gaussian variational
speciÔ¨Åcations, presenting the Manifold Gaussian Variational Bayes and Exact Manifold Gaussian
Variational Bayes solutions.","34
Acknowledgments

The research received funding from the European Union‚Äôs Horizon 2020 research and innovation
programme under the Marie Sk≈Çodowska-Curie project BNNmetrics (grant agreement No.",2022-11-21 21:36:58+00:00,Bayesian Learning for Neural Networks: an algorithmic survey,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Martin Magris'), arxiv.Result.Author('Alexandros Iosifidis')]","The last decade witnessed a growing interest in Bayesian learning. Yet, the
technicality of the topic and the multitude of ingredients involved therein,
besides the complexity of turning theory into practical implementations, limit
the use of the Bayesian learning paradigm, preventing its widespread adoption
across different fields and applications. This self-contained survey engages
and introduces readers to the principles and algorithms of Bayesian Learning
for Neural Networks. It provides an introduction to the topic from an
accessible, practical-algorithmic perspective. Upon providing a general
introduction to Bayesian Neural Networks, we discuss and present both standard
and recent approaches for Bayesian inference, with an emphasis on solutions
relying on Variational Inference and the use of Natural gradients. We also
discuss the use of manifold optimization as a state-of-the-art approach to
Bayesian learning. We examine the characteristic properties of all the
discussed methods, and provide pseudo-codes for their implementation, paying
attention to practical aspects, such as the computation of the gradients",0.3968256,0.02683603,0.15104108,A
14037,A systemic way of determining an optimal Œª is subject to further study.,"Our numerical experiments indicate that Œª in the range from 0.01 to 5 generally
       works well for the classiÔ¨Åcation task, but the optimal value diÔ¨Äered from dataset to dataset.",2.,2022-11-21 22:40:43+00:00,A Bi-level Nonlinear Eigenvector Algorithm for Wasserstein Discriminant Analysis,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Dong Min Roh'), arxiv.Result.Author('Zhaojun Bai')]","Much like the classical Fisher linear discriminant analysis, Wasserstein
discriminant analysis (WDA) is a supervised linear dimensionality reduction
method that seeks a projection matrix to maximize the dispersion of different
data classes and minimize the dispersion of same data classes. However, in
contrast, WDA can account for both global and local inter-connections between
data classes using a regularized Wasserstein distance. WDA is formulated as a
bi-level nonlinear trace ratio optimization. In this paper, we present a
bi-level nonlinear eigenvector (NEPv) algorithm, called WDA-nepv. The inner
kernel of WDA-nepv for computing the optimal transport matrix of the
regularized Wasserstein distance is formulated as an NEPv, and meanwhile the
outer kernel for the trace ratio optimization is also formulated as another
NEPv. Consequently, both kernels can be computed efficiently via
self-consistent-field iterations and modern solvers for linear eigenvalue
problems. Comparing with the existing algorithms for WDA, WDA-nepv is
derivative-free and surrogate-model-free. The computational efficiency and
applications in classification accuracy of WDA-nepv are demonstrated using
synthetic and real-life datasets.",-0.10488934,0.08373773,0.005577484,B
14038,Improvement and further study of this approach will be left as our future work.,"The
       major caveat of this approach, however, is its large computational costs in computing the
       derivatives.","References

 [1] Mokhtar Z. Alaya, Maxime Berar, Gilles Gasso, and Alain Rakotomamonjy.",2022-11-21 22:40:43+00:00,A Bi-level Nonlinear Eigenvector Algorithm for Wasserstein Discriminant Analysis,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Dong Min Roh'), arxiv.Result.Author('Zhaojun Bai')]","Much like the classical Fisher linear discriminant analysis, Wasserstein
discriminant analysis (WDA) is a supervised linear dimensionality reduction
method that seeks a projection matrix to maximize the dispersion of different
data classes and minimize the dispersion of same data classes. However, in
contrast, WDA can account for both global and local inter-connections between
data classes using a regularized Wasserstein distance. WDA is formulated as a
bi-level nonlinear trace ratio optimization. In this paper, we present a
bi-level nonlinear eigenvector (NEPv) algorithm, called WDA-nepv. The inner
kernel of WDA-nepv for computing the optimal transport matrix of the
regularized Wasserstein distance is formulated as an NEPv, and meanwhile the
outer kernel for the trace ratio optimization is also formulated as another
NEPv. Consequently, both kernels can be computed efficiently via
self-consistent-field iterations and modern solvers for linear eigenvalue
problems. Comparing with the existing algorithms for WDA, WDA-nepv is
derivative-free and surrogate-model-free. The computational efficiency and
applications in classification accuracy of WDA-nepv are demonstrated using
synthetic and real-life datasets.",0.21068999,0.030634278,0.057682715,A
14039,"The algorithmic fairness literature is growing quickly,
                                          but the corresponding conceptualization and applicability need further study and structure [2].","Fairness is com-
                                          monly deÔ¨Åned as the absence of any prejudice or favoritism toward an individual or group based on
                                          their inherent or acquired characteristics [1].","For instance, the prevalent correlation-based fairness algorithms fail to detect discrimination in
                                          the presence of statistical anomalies such as Simpson‚Äôs paradox [3].",2022-11-21 22:41:24+00:00,Equality of Effort via Algorithmic Recourse,stat.ML,"['stat.ML', 'cs.LG', 'stat.AP']","[arxiv.Result.Author('Francesca E. D. Raimondi'), arxiv.Result.Author('Andrew R. Lawrence'), arxiv.Result.Author('Hana Chockler')]","This paper proposes a method for measuring fairness through equality of
effort by applying algorithmic recourse through minimal interventions. Equality
of effort is a property that can be quantified at both the individual and the
group level. It answers the counterfactual question: what is the minimal cost
for a protected individual or the average minimal cost for a protected group of
individuals to reverse the outcome computed by an automated system? Algorithmic
recourse increases the flexibility and applicability of the notion of equal
effort: it overcomes its previous limitations by reconciling multiple treatment
variables, introducing feasibility and plausibility constraints, and
integrating the actual relative costs of interventions. We extend the existing
definition of equality of effort and present an algorithm for its assessment
via algorithmic recourse. We validate our approach both on synthetic data and
on the German credit dataset.",-0.2507882,-0.080301665,0.07113116,C
14040,"The algorithmic fairness literature is growing quickly,
                                          but the corresponding conceptualization and applicability need further study and structure [2].","Fairness is com-
                                          monly deÔ¨Åned as the absence of any prejudice or favoritism toward an individual or group based on
                                          their inherent or acquired characteristics [1].","For instance, the prevalent correlation-based fairness algorithms fail to detect discrimination in
                                          the presence of statistical anomalies such as Simpson‚Äôs paradox [3].",2022-11-21 22:41:24+00:00,Equality of Effort via Algorithmic Recourse,stat.ML,"['stat.ML', 'cs.LG', 'stat.AP']","[arxiv.Result.Author('Francesca E. D. Raimondi'), arxiv.Result.Author('Andrew R. Lawrence'), arxiv.Result.Author('Hana Chockler')]","This paper proposes a method for measuring fairness through equality of
effort by applying algorithmic recourse through minimal interventions. Equality
of effort is a property that can be quantified at both the individual and the
group level. It answers the counterfactual question: what is the minimal cost
for a protected individual or the average minimal cost for a protected group of
individuals to reverse the outcome computed by an automated system? Algorithmic
recourse increases the flexibility and applicability of the notion of equal
effort: it overcomes its previous limitations by reconciling multiple treatment
variables, introducing feasibility and plausibility constraints, and
integrating the actual relative costs of interventions. We extend the existing
definition of equality of effort and present an algorithm for its assessment
via algorithmic recourse. We validate our approach both on synthetic data and
on the German credit dataset.",-0.2507882,-0.080301665,0.07113116,C
14153,"Bridging a multi-layer perceptron and an RNN, our theory and proof technique can
                                                be an initial step toward further research on deep RNNs.","In addition, we prove the universality of other recurrent networks, such as bidirectional
                                                RNNs.","Keywords: recurrent neural network, universal approximation, deep narrow network,
                                                sequential data, minimum width

                                          1.",2022-11-25 02:43:54+00:00,Minimal Width for Universal Property of Deep RNN,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Chang hoon Song'), arxiv.Result.Author('Geonho Hwang'), arxiv.Result.Author('Jun ho Lee'), arxiv.Result.Author('Myungjoo Kang')]","A recurrent neural network (RNN) is a widely used deep-learning network for
dealing with sequential data. Imitating a dynamical system, an infinite-width
RNN can approximate any open dynamical system in a compact domain. In general,
deep networks with bounded widths are more effective than wide networks in
practice; however, the universal approximation theorem for deep narrow
structures has yet to be extensively studied. In this study, we prove the
universality of deep narrow RNNs and show that the upper bound of the minimum
width for universality can be independent of the length of the data.
Specifically, we show that a deep RNN with ReLU activation can approximate any
continuous function or $L^p$ function with the widths $d_x+d_y+2$ and
$\max\{d_x+1,d_y\}$, respectively, where the target function maps a finite
sequence of vectors in $\mathbb{R}^{d_x}$ to a finite sequence of vectors in
$\mathbb{R}^{d_y}$. We also compute the additional width required if the
activation function is $\tanh$ or more. In addition, we prove the universality
of other recurrent networks, such as bidirectional RNNs. Bridging a multi-layer
perceptron and an RNN, our theory and proof technique can be an initial step
toward further research on deep RNNs.",0.19083065,0.20217773,0.07397412,A
14188,"Then, in Section 5 we prove Theorem 2 and Ô¨Ånally in Section
6 we give concluding remarks and further research directions.","In Section 4 we put
everything together and give the proof of Theorem 1.","We remark that throughout the paper, unless otherwise
speciÔ¨Åed, C will represent a constant depending only upon the parameters s, q, p and d which may change from line to
line, as is standard in analysis.",2022-11-25 23:32:26+00:00,Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev Spaces,stat.ML,"['stat.ML', 'cs.LG', 'cs.NA', 'math.NA', '41A25, 41A46, 62M45']",[arxiv.Result.Author('Jonathan W. Siegel')],"We study the problem of how efficiently, in terms of the number of
parameters, deep neural networks with the ReLU activation function can
approximate functions in the Sobolev space $W^s(L_q(\Omega))$ on a bounded
domain $\Omega$, where the error is measured in $L_p(\Omega)$. This problem is
important for studying the application of neural networks in scientific
computing and has previously been solved only in the case $p=q=\infty$. Our
contribution is to provide a solution for all $1\leq p,q\leq \infty$ and $s >
0$. Our results show that deep ReLU networks significantly outperform classical
methods of approximation, but that this comes at the cost of parameters which
are not encodable.",0.122706905,-0.34203997,-0.0977325,A
14324,"We believe that a further study of these diÔ¨Äerent directions might
                                                lead to theoretical algorithms adapted to real-world situations.","In light of the existing works, some clear directions for
                                                future research appear.","Keywords: Multiplayer bandits, Multi-armed bandits, Cognitive radio, Decentralized
                                                learning, Opportunistic Spectrum Access.",2022-11-29 15:04:09+00:00,A survey on multi-player bandits,stat.ML,"['stat.ML', 'cs.GT', 'cs.LG']","[arxiv.Result.Author('Etienne Boursier'), arxiv.Result.Author('Vianney Perchet')]","Due mostly to its application to cognitive radio networks, multiplayer
bandits gained a lot of interest in the last decade. A considerable progress
has been made on its theoretical aspect. However, the current algorithms are
far from applicable and many obstacles remain between these theoretical results
and a possible implementation of multiplayer bandits algorithms in real
cognitive radio networks. This survey contextualizes and organizes the rich
multiplayer bandits literature. In light of the existing works, some clear
directions for future research appear. We believe that a further study of these
different directions might lead to theoretical algorithms adapted to real-world
situations.",0.12650457,0.020874266,0.050065797,A
14415,"12 for the case study) obtained by F (h) to perform a
parameter f with respect to the iteration number i; this could      signiÔ¨Åcance analysis, setting an arbitrary threshold p = 25%
be the subject of further research.","(4)                          h0 ‚ààH0
                           j+1‚àíj
                                                                    We evaluate the histogram of values {F (h) | h ‚àà H} (shown
The Ô¨Årst is interesting, as it expresses the rate of change of the  in Fig.",In this implementation we       to obtain the results shown.,2022-12-01 05:27:50+00:00,Locally Adaptive Hierarchical Cluster Termination With Application To Individual Tree Delineation,stat.ML,"['stat.ML', 'cs.LG', '62H30 (Primary)', 'I.5.3; I.5.4']","[arxiv.Result.Author('Ashlin Richardson'), arxiv.Result.Author('Donald Leckie')]","A clustering termination procedure which is locally adaptive (with respect to
the hierarchical tree of sets representative of the agglomerative merging) is
proposed, for agglomerative hierarchical clustering on a set equipped with a
distance function. It represents a multi-scale alternative to conventional
scale dependent threshold based termination criteria.",-0.07370095,-0.29083443,0.2015183,C
14586,"In Chapter 6 discusses the results, the limitations of
the proposed method and gives ideas that can guide further research.","Chapter 5 shows the re-
sults of the numerical experiments.","5
2 Theoretic foundations and literature
   review

2.1 Graph terminology

Our Ô¨Årst research question requires causally modeling complex systems.",2022-12-05 17:23:59+00:00,Observational and Interventional Causal Learning for Regret-Minimizing Control,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Christian Reiser')],"We explore how observational and interventional causal discovery methods can
be combined. A state-of-the-art observational causal discovery algorithm for
time series capable of handling latent confounders and contemporaneous effects,
called LPCMCI, is extended to profit from casual constraints found through
randomized control trials. Numerical results show that, given perfect
interventional constraints, the reconstructed structural causal models (SCMs)
of the extended LPCMCI allow 84.6% of the time for the optimal prediction of
the target variable. The implementation of interventional and observational
causal discovery is modular, allowing causal constraints from other sources.
  The second part of this thesis investigates the question of regret minimizing
control by simultaneously learning a causal model and planning actions through
the causal model. The idea is that an agent to optimize a measured variable
first learns the system's mechanics through observational causal discovery. The
agent then intervenes on the most promising variable with randomized values
allowing for the exploitation and generation of new interventional data. The
agent then uses the interventional data to enhance the causal model further,
allowing improved actions the next time.
  The extended LPCMCI can be favorable compared to the original LPCMCI
algorithm. The numerical results show that detecting and using interventional
constraints leads to reconstructed SCMs that allow 60.9% of the time for the
optimal prediction of the target variable in contrast to the baseline of 53.6%
when using the original LPCMCI algorithm. Furthermore, the induced average
regret decreases from 1.2 when using the original LPCMCI algorithm to 1.0 when
using the extended LPCMCI algorithm with interventional discovery.",-0.109346926,-0.247078,-0.10572599,C
14587,I explain these limitations and point to possible further research in the following.,"47
6 Discussion

6.1 Limitations and future work

The proposed approach to extend LPCMCI to proÔ¨Åt from interventional data has limita-
tions.","6.1.1 Empirical validation

The empirical validation through numerical experiments is limited and could be extended
to include a full parameter study where all parameter combinations are simulated.",2022-12-05 17:23:59+00:00,Observational and Interventional Causal Learning for Regret-Minimizing Control,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Christian Reiser')],"We explore how observational and interventional causal discovery methods can
be combined. A state-of-the-art observational causal discovery algorithm for
time series capable of handling latent confounders and contemporaneous effects,
called LPCMCI, is extended to profit from casual constraints found through
randomized control trials. Numerical results show that, given perfect
interventional constraints, the reconstructed structural causal models (SCMs)
of the extended LPCMCI allow 84.6% of the time for the optimal prediction of
the target variable. The implementation of interventional and observational
causal discovery is modular, allowing causal constraints from other sources.
  The second part of this thesis investigates the question of regret minimizing
control by simultaneously learning a causal model and planning actions through
the causal model. The idea is that an agent to optimize a measured variable
first learns the system's mechanics through observational causal discovery. The
agent then intervenes on the most promising variable with randomized values
allowing for the exploitation and generation of new interventional data. The
agent then uses the interventional data to enhance the causal model further,
allowing improved actions the next time.
  The extended LPCMCI can be favorable compared to the original LPCMCI
algorithm. The numerical results show that detecting and using interventional
constraints leads to reconstructed SCMs that allow 60.9% of the time for the
optimal prediction of the target variable in contrast to the baseline of 53.6%
when using the original LPCMCI algorithm. Furthermore, the induced average
regret decreases from 1.2 when using the original LPCMCI algorithm to 1.0 when
using the extended LPCMCI algorithm with interventional discovery.",-0.25457114,-0.023824532,-0.16528015,C
14645,"We hope the present
work enables further research in this direction.","It seems
likely that by combining Fourier space ideas proposed in the present paper with modern techniques
from the literature, one may derive still superior system identiÔ¨Åcation methods.","10
                                       LE¬¥ VY Œ±-STABLE DRIFT IDENTIFICATION

Acknowledgments

This research was partially supported by NSF DMS-1723272, and also beneÔ¨Åted from computa-
tional resources that include the Pinnacles cluster at UC Merced (supported by NSF OAC-2019144)
and Nautilus, supported by the PaciÔ¨Åc Research Platform (NSF ACI-1541349), CHASE-CI (NSF
CNS-1730158), Towards a National Research Platform (NSF OAC-1826967), and the University of
California OfÔ¨Åce of the President.",2022-12-06 20:40:27+00:00,Drift Identification for L√©vy alpha-Stable Stochastic Systems,stat.ML,"['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",[arxiv.Result.Author('Harish S. Bhat')],"This paper focuses on a stochastic system identification problem: given time
series observations of a stochastic differential equation (SDE) driven by
L\'{e}vy $\alpha$-stable noise, estimate the SDE's drift field. For $\alpha$ in
the interval $[1,2)$, the noise is heavy-tailed, leading to computational
difficulties for methods that compute transition densities and/or likelihoods
in physical space. We propose a Fourier space approach that centers on
computing time-dependent characteristic functions, i.e., Fourier transforms of
time-dependent densities. Parameterizing the unknown drift field using Fourier
series, we formulate a loss consisting of the squared error between predicted
and empirical characteristic functions. We minimize this loss with gradients
computed via the adjoint method. For a variety of one- and two-dimensional
problems, we demonstrate that this method is capable of learning drift fields
in qualitative and/or quantitative agreement with ground truth fields.",0.02800648,-0.11631756,-0.03411355,C
15294,"Participants were ineligible if they were enrolled with
esophageal cancer or other primary cancer, no cancer upon further study, or as a negative control in the
case of spouses, friends, or other participants.","Eligibility was
deÔ¨Åned as having a positive lung cancer diagnosis.","Among those 7,585 eligible patients, we identiÔ¨Åed 7,462
(98.4%) with the temporal information necessary to deÔ¨Åne their semi-competing outcomes, namely (1) date
of primary diagnosis, (2) progression and/or death date where applicable, and (3) last follow-up date or non-
progression date.",2022-12-22 20:38:57+00:00,Deep Learning of Semi-Competing Risk Data via a New Neural Expectation-Maximization Algorithm,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Stephen Salerno'), arxiv.Result.Author('Yi Li')]","Prognostication for lung cancer, a leading cause of mortality, remains a
complex task, as it needs to quantify the associations of risk factors and
health events spanning a patient's entire life. One challenge is that an
individual's disease course involves non-terminal (e.g., disease progression)
and terminal (e.g., death) events, which form semi-competing relationships. Our
motivation comes from the Boston Lung Cancer Study, a large lung cancer
survival cohort, which investigates how risk factors influence a patient's
disease trajectory. Following developments in the prediction of time-to-event
outcomes with neural networks, deep learning has become a focal area for the
development of risk prediction methods in survival analysis. However, limited
work has been done to predict multi-state or semi-competing risk outcomes,
where a patient may experience adverse events such as disease progression prior
to death. We propose a novel neural expectation-maximization algorithm to
bridge the gap between classical statistical approaches and machine learning.
Our algorithm enables estimation of the non-parametric baseline hazards of each
state transition, risk functions of predictors, and the degree of dependence
among different transitions, via a multi-task deep neural network with
transition-specific sub-architectures. We apply our method to the Boston Lung
Cancer Study and investigate the impact of clinical and genetic predictors on
disease progression and mortality.",-0.2711285,-0.10523079,-0.04493712,C
15295,"While previous work has developed machine learning approaches for multi-state or competing-risk

                                                                     16
settings [41, 31, 30, 1], in which progression and death censor each other, we propose a new approach to
further study the correlation between time to progression and death, and the modiÔ¨Åed hazards for mortality
in the so-called sojourn time between progression and death.","7 Discussion

We propose a neural expectation-maximization approach which, through a mixture of neural network ar-
chitectures and trainable parameters, predicts time-to-event outcomes arising from a semi-competing risk
framework (i.e., when an non-terminal event such as disease progression, modiÔ¨Åes the risk of a patient‚Äôs future
survival).","In simulation, our method could accurately es-
timate the relationship between covariates and the hazards of transitioning from disease onset to progression
and death, particularly in situations where the risk relationship is complex.",2022-12-22 20:38:57+00:00,Deep Learning of Semi-Competing Risk Data via a New Neural Expectation-Maximization Algorithm,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Author('Stephen Salerno'), arxiv.Result.Author('Yi Li')]","Prognostication for lung cancer, a leading cause of mortality, remains a
complex task, as it needs to quantify the associations of risk factors and
health events spanning a patient's entire life. One challenge is that an
individual's disease course involves non-terminal (e.g., disease progression)
and terminal (e.g., death) events, which form semi-competing relationships. Our
motivation comes from the Boston Lung Cancer Study, a large lung cancer
survival cohort, which investigates how risk factors influence a patient's
disease trajectory. Following developments in the prediction of time-to-event
outcomes with neural networks, deep learning has become a focal area for the
development of risk prediction methods in survival analysis. However, limited
work has been done to predict multi-state or semi-competing risk outcomes,
where a patient may experience adverse events such as disease progression prior
to death. We propose a novel neural expectation-maximization algorithm to
bridge the gap between classical statistical approaches and machine learning.
Our algorithm enables estimation of the non-parametric baseline hazards of each
state transition, risk functions of predictors, and the degree of dependence
among different transitions, via a multi-task deep neural network with
transition-specific sub-architectures. We apply our method to the Boston Lung
Cancer Study and investigate the impact of clinical and genetic predictors on
disease progression and mortality.",-0.060157634,0.27068943,-0.100722134,B
