Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
18,"Lastly, section 7
provides a discussion of our work, with suggestions for further research.","Simulation studies are illustrated in section 5 for ﬁnite sample performance and an
application of MARS to leukemia data set is demonstrated in section 6.","2 Case study: minimal residual disease in leukemia

Responding to the emerging development of targeted and immunological treatments, the
U.S. Food and Drug Administration (FDA 2020) published a guideline clarifying for the use
of minimal residual disease (MRD) status in the development of drugs for the treatment
of hematologic malignancies.",2022-01-02 03:09:22+00:00,Evidence synthesis with reconstructed survival data,stat.ME,['stat.ME'],"[arxiv.Result.Author('Chenqi Fu'), arxiv.Result.Author('Shouhao Zhou'), arxiv.Result.Author('Xuelin Huang'), arxiv.Result.Author('Nicholas J. Short'), arxiv.Result.Author('Farhad Ravandi-Kashani'), arxiv.Result.Author('Donald A. Berry')]","We present a general approach to synthesizing evidence of time-to-event
endpoints in meta-analyses of aggregate data (AD). Our work goes beyond most
previous meta-analytic research by using reconstructed survival data as a
source of information. A Bayesian multilevel regression model, called the
""meta-analysis of reconstructed survival data"" (MARS), is introduced, by
modeling and integrating reconstructed survival information with other types of
summary data, to estimate the hazard ratio function and survival probabilities.
The method attempts to reduce selection bias, and relaxes the presumption of
proportional hazards in individual clinical studies from the conventional
approaches restricted to hazard ratio estimates. Theoretically, we establish
the asymptotic consistency of MARS, and investigate its relative efficiency
with respect to the individual participant data (IPD) meta-analysis. In
simulation studies, the MARS demonstrated comparable performance to IPD
meta-analysis with minor deviation from the true values, suggesting great
robustness and efficiency achievable in AD meta-analysis with finite sample.
Finally, we applied MARS in a meta-analysis of acute myeloid leukemia to assess
the association of minimal residual disease with survival, to help respond to
FDA's emerging concerns on translational use of surrogate biomarker in drug
development of hematologic malignancies.",0.3446868,-0.044991173,-0.04805831,C
254,"To further study the properties of our
method, we compare it with other competitive methods from the perspective of changepoint
estimation.","We also explore how the strength of spatial
correlation and change signal inﬂuence performance.","3.1 Data Generation

We randomly select N = 50 locations in a 10 × 10 spatial domain as the rejection region DR
resulting from a changepoint detection algorithm adjusted by the FDR control.",2022-01-08 02:26:56+00:00,Bayesian Changepoint Estimation for Spatially Indexed Functional Time Series,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Mengchen Wang'), arxiv.Result.Author('Trevor Harris'), arxiv.Result.Author('Bo Li')]","We propose a Bayesian hierarchical model to simultaneously estimate mean
based changepoints in spatially correlated functional time series. Unlike
previous methods that assume a shared changepoint at all spatial locations or
ignore spatial correlation, our method treats changepoints as a spatial
process. This allows our model to respect spatial heterogeneity and exploit
spatial correlations to improve estimation. Our method is derived from the
ubiquitous cumulative sum (CUSUM) statistic that dominates changepoint
detection in functional time series. However, instead of directly searching for
the maximum of the CUSUM based processes, we build spatially correlated
two-piece linear models with appropriate variance structure to locate all
changepoints at once. The proposed linear model approach increases the
robustness of our method to variability in the CUSUM process, which, combined
with our spatial correlation model, improves changepoint estimation near the
edges. We demonstrate through extensive simulation studies that our method
outperforms existing functional changepoint estimators in terms of both
estimation accuracy and uncertainty quantification, under either weak and
strong spatial correlation, and weak and strong change signals. Finally, we
demonstrate our method using a temperature data set and a coronavirus disease
2019 (COVID-19) study.",-0.122612454,-0.09219553,0.10350579,A
578,"In Section 6 conclusions from the analysis are
drawn and an outlook on further research is given.","The
set-up of the simulation study is explained in Section 4 and the obtained results are
presented and discussed in Section 5.","2 Motivating Example

Acute heart failure (AHF) is deﬁned as a rapid onset of signs and symptoms of heart
failure.",2022-01-14 14:57:00+00:00,Robust Confidence Intervals for Meta-Regression with Correlated Moderators,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO']","[arxiv.Result.Author('Eric S. Knop'), arxiv.Result.Author('Markus Pauly'), arxiv.Result.Author('Tim Friede'), arxiv.Result.Author('Thilo Welz')]","Mixed-effects meta-regression models provide a powerful tool for evidence
synthesis. In fact, modelling the study effect in terms of random effects and
moderators not only allows to examine the impact of the moderators, but often
leads to more accurate estimates of the involved parameters. Nevertheless, due
to the often small number of studies on a specific research topic, interactions
are often neglected in meta-regression. This was also the case in a recent
meta-analysis in acute heart failure where a significant decline in death rate
over calendar time was reported. However, we believe that an important
interaction has been neglected. We therefore reanalyzed the data with a
meta-regression model, including an interaction term of the median recruitment
year and the average age of the patients. The model with interaction suggests
different conclusions.
  This led to the new research questions (i) how moderator interactions
influence inference in mixed-effects meta-regression models and (ii) whether
some inference methods are more reliable than others. Focusing on confidence
intervals for main and interaction parameters, we address these questions in an
extensive simulation study. We thereby investigate coverage and length of seven
different confidence intervals under varying conditions. We conclude with some
practical recommendations.",0.2559763,0.0042586476,0.06680312,C
579,"As the model examined in this work still has a simple structure, in further research
it might be of interest to consider the performance of KH for more complex mod-
els.","Furthermore, the coverage of the KH-CIs
for β12 tended to be lower.","Interesting settings are interaction terms of higher order, other random eﬀect
distributions and more extreme coeﬃcients.",2022-01-14 14:57:00+00:00,Robust Confidence Intervals for Meta-Regression with Correlated Moderators,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO']","[arxiv.Result.Author('Eric S. Knop'), arxiv.Result.Author('Markus Pauly'), arxiv.Result.Author('Tim Friede'), arxiv.Result.Author('Thilo Welz')]","Mixed-effects meta-regression models provide a powerful tool for evidence
synthesis. In fact, modelling the study effect in terms of random effects and
moderators not only allows to examine the impact of the moderators, but often
leads to more accurate estimates of the involved parameters. Nevertheless, due
to the often small number of studies on a specific research topic, interactions
are often neglected in meta-regression. This was also the case in a recent
meta-analysis in acute heart failure where a significant decline in death rate
over calendar time was reported. However, we believe that an important
interaction has been neglected. We therefore reanalyzed the data with a
meta-regression model, including an interaction term of the median recruitment
year and the average age of the patients. The model with interaction suggests
different conclusions.
  This led to the new research questions (i) how moderator interactions
influence inference in mixed-effects meta-regression models and (ii) whether
some inference methods are more reliable than others. Focusing on confidence
intervals for main and interaction parameters, we address these questions in an
extensive simulation study. We thereby investigate coverage and length of seven
different confidence intervals under varying conditions. We conclude with some
practical recommendations.",-0.08723892,-0.35508907,0.10337155,A
580,"In further research it may also be of interest to analyze the situations
where highly inﬂated interval lengths of the HC3- and HC5-CIs occurred in detail,
because they cannot be explained by the results of this work.","For a medium number of studies
HC3 might be the most suitable, since its intervals held the nominal conﬁdence level
in every situation with k ∈ {10, 20} and were shorter compares to the HC4- and
HC5-CIs.","Concluding, meta-regression remains an important ﬁeld of statistical research.",2022-01-14 14:57:00+00:00,Robust Confidence Intervals for Meta-Regression with Correlated Moderators,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO']","[arxiv.Result.Author('Eric S. Knop'), arxiv.Result.Author('Markus Pauly'), arxiv.Result.Author('Tim Friede'), arxiv.Result.Author('Thilo Welz')]","Mixed-effects meta-regression models provide a powerful tool for evidence
synthesis. In fact, modelling the study effect in terms of random effects and
moderators not only allows to examine the impact of the moderators, but often
leads to more accurate estimates of the involved parameters. Nevertheless, due
to the often small number of studies on a specific research topic, interactions
are often neglected in meta-regression. This was also the case in a recent
meta-analysis in acute heart failure where a significant decline in death rate
over calendar time was reported. However, we believe that an important
interaction has been neglected. We therefore reanalyzed the data with a
meta-regression model, including an interaction term of the median recruitment
year and the average age of the patients. The model with interaction suggests
different conclusions.
  This led to the new research questions (i) how moderator interactions
influence inference in mixed-effects meta-regression models and (ii) whether
some inference methods are more reliable than others. Focusing on confidence
intervals for main and interaction parameters, we address these questions in an
extensive simulation study. We thereby investigate coverage and length of seven
different confidence intervals under varying conditions. We conclude with some
practical recommendations.",0.37067112,-0.006755462,0.04272659,C
699,"How to automate such heuristic for an arbitrary problem
is worth further research.","Of course, this is only a toy example of a crude yet adaptive
speciﬁcation of a knots superset.","Appendix: Spline and B-Splines

An order-d spline f (x) deﬁned on domain [a, b] comprises smoothly connected polynomial
segments of degree d − 1, with d − 2 continuous derivatives at their interior knots (or
break points) a < s1 < s2 < .",2022-01-18 08:26:41+00:00,General P-Splines for Non-Uniform B-Splines,stat.ME,['stat.ME'],"[arxiv.Result.Author('Zheyuan Li'), arxiv.Result.Author('Jiguo Cao')]","We proposed a new penalized B-splines estimator, the general P-spline, to
accommodate non-uniform B-splines on unevenly spaced knots. It is a complement
to Eilers and Marx's standard P-spline tailored for uniform B-splines on
equidistant knots. At its core, we derived a novel general difference penalty
that accounts for irregular knot spacing, while still being easy to compute and
interpret. Both P-spline variants are useful for practical smoothing, because
either one can produce a more satisfactory fit than the other, depending on the
knot sequence being used and the data being analyzed. Therefore, practitioners
should try out both before betting on either one, for which we have implemented
general P-spline in R packages gps and gps.mgcv. The new general P-spline is
closely related to O'Sullivan spline (or O-spline) through a sandwich formula
that links general difference penalty to derivative penalty. Though both
penalties seem equally powerful in wiggliness control for their mathematical
association and statistical similarity, simulation studies show that general
P-spline either outperforms O-spline in terms of mean squared error, or
performs equally well, making it a superior replacement of O-spline.",-0.09801913,-0.15021574,0.33637178,A
880,"Additional issues and
further research are discussed Section 6.","The proposed approach is applied to the inference on the
explained variation of glycohemoglobin by the environmental pollutants in a National Health
and Nutrition Examination Survey(NHANES) data set in Section 5.","2 Estimating equation approach to explained variation

2.1 Basic formulation of the problem

Let (Yi, Xi1, · · · , Xip), i = 1, · · · , n be the observed data.",2022-01-18 19:05:24+00:00,Statistical Inference on Explained Variation in High-dimensional Linear Model with Dense Effects,stat.ME,['stat.ME'],[arxiv.Result.Author('Hua Yun Chen')],"Statistical inference on the explained variation of an outcome by a set of
covariates is of particular interest in practice. When the covariates are of
moderate to high-dimension and the effects are not sparse, several approaches
have been proposed for estimation and inference. One major problem with the
existing approaches is that the inference procedures are not robust to the
normality assumption on the covariates and the residual errors. In this paper,
we propose an estimating equation approach to the estimation and inference on
the explained variation in the high-dimensional linear model. Unlike the
existing approaches, the proposed approach does not rely on the restrictive
normality assumptions for inference. It is shown that the proposed estimator is
consistent and asymptotically normally distributed under reasonable conditions.
Simulation studies demonstrate better performance of the proposed inference
procedure in comparison with the existing approaches. The proposed approach is
applied to studying the variation of glycohemoglobin explained by environmental
pollutants in a National Health and Nutrition Examination Survey data set.",0.13770682,-0.010757004,-0.067814365,C
881,"However,
this is beyond the scope of this paper and is a topic of further research.",The diﬃculty may be overcome with the help of supplementary covariate data.,"Acknowledgement
    This research is supported by a grant from the National Institute of Environmental Health
Sciences at the National Institute of Health.",2022-01-18 19:05:24+00:00,Statistical Inference on Explained Variation in High-dimensional Linear Model with Dense Effects,stat.ME,['stat.ME'],[arxiv.Result.Author('Hua Yun Chen')],"Statistical inference on the explained variation of an outcome by a set of
covariates is of particular interest in practice. When the covariates are of
moderate to high-dimension and the effects are not sparse, several approaches
have been proposed for estimation and inference. One major problem with the
existing approaches is that the inference procedures are not robust to the
normality assumption on the covariates and the residual errors. In this paper,
we propose an estimating equation approach to the estimation and inference on
the explained variation in the high-dimensional linear model. Unlike the
existing approaches, the proposed approach does not rely on the restrictive
normality assumptions for inference. It is shown that the proposed estimator is
consistent and asymptotically normally distributed under reasonable conditions.
Simulation studies demonstrate better performance of the proposed inference
procedure in comparison with the existing approaches. The proposed approach is
applied to studying the variation of glycohemoglobin explained by environmental
pollutants in a National Health and Nutrition Examination Survey data set.",0.210177,0.10774146,-0.17984723,C
1003,"• For the monitoring of continuous multivariate observations, the strategy for choosing

                    the evaluation points proposed in Section 3.1 was not completely deﬁned and requires

                    further research.","We end this section by stating a few remarks:

• An implementation of the studied multi-purpose open-end monitoring procedure will

                    be made available in the very near future in the R package npcp (Kojadinovic and

                    Verhoijsen, 2021).","One possibility would be to choose the points according to the depth

                    of the observations (a concept initially proposed by Tukey, 1975, in the bivariate case

                    with numerous extensions since then) in the learning sample.",2022-01-25 13:34:01+00:00,Multi-purpose open-end monitoring procedures for multivariate observations based on the empirical distribution function,stat.ME,"['stat.ME', '62L99, 62E20, 62G10']","[arxiv.Result.Author('Mark Holmes'), arxiv.Result.Author('Ivan Kojadinovic'), arxiv.Result.Author('Alex Verhoijsen')]","We propose nonparametric open-end sequential testing procedures that can
detect all types of changes in the contemporary distribution function of
multivariate observations. Their asymptotic properties are theoretically
investigated under stationarity and under alternatives to stationarity. Monte
Carlo experiments reveal their good finite-sample behavior in the case of
continuous univariate observations. A short data example concludes the work.",-0.06165455,0.08668479,0.10967771,A
1048,Section 6 concludes and motivates further research.,"Furthermore, the design-based simulation
contributes to a genuine demonstration of properties and advantages of MERFs in the context
of SAE.","3
2 Theory and method

In this section we propose a ﬂexible, data-driven approach using random forests for the estimation
of area-level means in the presence of unit-level survey data.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.06438369,0.10073814,0.06924498,A
1049,"However, the modiﬁcation towards a fully non-parametric
formulation of model (1) is subject to further research.","The estimation of the random eﬀects could be also done in a non-parametric way by using discrete
mixtures (Marino et al., 2018, 2019).","For ﬁtting the model (1) we use an approach reminiscent of the EM-algorithm similar to Hajjem et
al.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.33158424,-0.10772694,-0.24743442,A_centroid
1050,"The extension of recent theoretical
results, such as conditions for the consistency of unit-level predictions (Scornet et al., 2015) or
their asymptotic normality (Wager & Athey, 2018), towards area-level indicators is a conducive
topic for further research.","The theoretical background of random forests grows, but mainly aims
to quantify the uncertainty of individual predictions (Sexton & Laake, 2009; Wager et al., 2014;
Wager & Athey, 2018; Athey et al., 2019; Zhang et al., 2019).","In this paper, we propose a non-parametric random eﬀect block (REB) bootstrap for estimating
the MSE of the introduced area-level estimator given by equation (5).",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.12264438,0.11532742,-0.12022604,A
1051,"We motivate three major dimensions for further research, including theoretical work, aspects of
generalizations and advanced applications using Big Data covariates: from a theoretical perspec-
tive, further research is needed to investigate the construction of a partial-analytical MSE for
area-level means or the construction of an asymptotic MSE-estimator.","Furthermore, we obtain reasonable support for the performance in the application in Section 5.2
and the following design-based simulation in Section 5.3.","A conducive strategy is
the deduction of recent theoretical results, such as conditions for the consistency of unit-level
predictions (Scornet et al., 2015) or considerations of individual predictions intervals (Wager et
al., 2014; Zhang et al., 2019), towards area-level indicators.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.05801913,0.1951032,-0.028743558,A
1052,"Furthermore, a generalization towards binary
or count data is possible and left to further research.","Regarding additional generalizations of the proposed method, we aim to extend the
use of MERFs towards the estimation of small area quantiles and other non-linear indicators,
such as Gini-coeﬃcients or Head Count Ratios.","The semi-parametric composite formula-
tion of model (1) allows for f () to adapt any functional form regarding the estimation of the
conditional mean of yi given Xi and technically transfers to other machine learning methods,
such as gradient-boosted trees or support vector machines.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.16621742,0.12907603,-0.1553485,A
1053,"Section 6 concludes and motivates
further research.","Furthermore, the design-based simulation contributes to a genuine demonstration of
properties and advantages of MERFs in the context of SAE.","2 Theory and method

In this section we propose a ﬂexible, data-driven approach using random forests for the estimation
of area-level means in the presence of unit-level survey data.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.067357585,0.11143157,0.073621914,A
1054,"However, the modiﬁcation towards a fully non-parametric
formulation of model (1) is subject to further research.","The estimation of the random eﬀects could be also done in a non-parametric way by using discrete
mixtures (Marino et al., 2018, 2019).","For ﬁtting the model (1) we use an approach reminiscent of the EM-algorithm similar to Hajjem et
al.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.33158424,-0.10772694,-0.24743442,A
1055,"An exact theoretical determination and discussion of asymptotic
properties will be left to further research.","We discuss performance details in the model-
based simulation in Section 4.","The theoretical background of random forests grows,
but mainly aims to quantify the uncertainty of individual predictions (Sexton & Laake, 2009;
Wager et al., 2014; Wager & Athey, 2018; Athey et al., 2019; Zhang et al., 2019).",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.11098469,-0.0060662283,-0.029345509,A
1056,"We motivate three major dimensions for further research, including theoretical work, aspects of
generalizations and advanced applications using Big Data covariates: from a theoretical perspec-
tive, further research is needed to investigate the construction and theoretical discussion of a
partial-analytical MSE for area-level means.","Furthermore, we obtain reasonable support for the performance in the application in Section 5.2
and the following design-based simulation in Section 5.3.","A conducive strategy is an extension based on our
theoretical discussion in the online supplementary materials.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.043648005,0.093249865,0.07169503,A
1057,"Furthermore, a generalization towards binary or count data is possible
and left to further research.","Regarding
additional generalizations of the proposed method, we aim to extend the use of MERFs towards
the estimation of small area quantiles and other non-linear indicators, such as Gini-coeﬃcients
or Head Count Ratios.","The semi-parametric composite formulation of model (1) allows for
f () to adapt any functional form regarding the estimation of the conditional mean of yi given
Xi and technically transfers to other machine learning methods, such as gradient-boosted trees
or support vector machines.",2022-01-26 13:46:00+00:00,Flexible domain prediction using mixed effects random forests,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Timo Schmid')]","This paper promotes the use of random forests as versatile tools for
estimating spatially disaggregated indicators in the presence of small
area-specific sample sizes. Small area estimators are predominantly
conceptualized within the regression-setting and rely on linear mixed models to
account for the hierarchical structure of the survey data. In contrast, machine
learning methods offer non-linear and non-parametric alternatives, combining
excellent predictive performance and a reduced risk of model-misspecification.
Mixed effects random forests combine advantages of regression forests with the
ability to model hierarchical dependencies. This paper provides a coherent
framework based on mixed effects random forests for estimating small area
averages and proposes a non-parametric bootstrap estimator for assessing the
uncertainty of the estimates. We illustrate advantages of our proposed
methodology using Mexican income-data from the state Nuevo Le\'on. Finally, the
methodology is evaluated in model-based and design-based simulations comparing
the proposed methodology to traditional regression-based approaches for
estimating small area averages.",-0.1745452,0.13709016,-0.17504653,A
1074,"A formal comparison of Sequential Bayes Factors and such scoring rules is an
interesting ﬁeld of further research.","Scoring rules evaluated on out-of-
sample data are available as a direct output of the data tempering strategy adopted in this work, under the prequential
framework (Dawid and Musio, 2014).","28
                                                                                                   A PREPRINT - JANUARY 28, 2022

References

Andrieu, C., A. Doucet, and R. Holenstein (2010).",2022-01-26 20:01:30+00:00,Sequential Bayesian Inference for Factor Analysis,stat.ME,['stat.ME'],"[arxiv.Result.Author('Konstantinos Vamvourellis'), arxiv.Result.Author('Konstantinos Kalogeropoulos'), arxiv.Result.Author('Irini Moustaki')]","We develop an efficient Bayesian sequential inference framework for factor
analysis models observed via various data types, such as continuous, binary and
ordinal data. In the continuous data case, where it is possible to marginalise
over the latent factors, the proposed methodology tailors the Iterated Batch
Importance Sampling (IBIS) of Chopin (2002) to handle such models and we
incorporate Hamiltonian Markov Chain Monte Carlo. For binary and ordinal data,
we develop an efficient IBIS scheme to handle the parameter and latent factors,
combining with Laplace or Variational Bayes approximations. The methodology can
be used in the context of sequential hypothesis testing via Bayes factors,
which are known to have advantages over traditional null hypothesis testing.
Moreover, the developed sequential framework offers multiple benefits even in
non-sequential cases, by providing posterior distribution, model evidence and
scoring rules (under the prequential framework) in one go, and by offering a
more robust alternative computational scheme to Markov Chain Monte Carlo that
can be useful in problematic target distributions.",0.03936464,0.18999302,0.11254214,B
1156,"We discuss the limitations of our approach and set out directions
for further research in Section 5.","In Section 4, the performance
of the LDF approach is examined in a simulated example and applications to Eurozone
inﬂation forecasting using professional forecasts, and foreign exchange and US inﬂation
using time series models.","The code to reproduce our study is freely available via
the provided link2.",2022-01-28 11:17:12+00:00,A loss discounting framework for model averaging and selection in time series models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Dawid Bernaciak'), arxiv.Result.Author('Jim E. Griffin')]","We introduce a Loss Discounting Framework for forecast combination which
generalises and combines Bayesian model synthesis and generalized Bayes
methodologies. The framework allows large scale model averaging/selection and
is also suitable for handling sudden regime changes. This novel and simple
model synthesis framework is compared to both established methodologies and
state of the art methods for a number of macroeconomic forecasting examples. We
find that the proposed method offers an attractive, computationally efficient
alternative to the benchmark methodologies and often outperforms more complex
techniques.",-0.2973167,0.27958316,0.18495548,B
1157,"We discuss the limitations of our approach and set out
directions for further research in Section 5.","In Section 4, the performance
of the LDF approach is examined in a simulated example and foreign exchange, and US
inﬂation using time series models.","The code to reproduce our study is freely
available via the provided link2.",2022-01-28 11:17:12+00:00,A loss discounting framework for model averaging and selection in time series models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Dawid Bernaciak'), arxiv.Result.Author('Jim E. Griffin')]","We introduce a Loss Discounting Framework for model and forecast combination
which generalises and combines Bayesian model synthesis and generalized Bayes
methodologies. We use a loss function to score the performance of different
models and introduce a multilevel discounting scheme which allows a flexible
specification of the dynamics of the model weights. This novel and simple model
combination approach can be easily applied to large scale model
averaging/selection, can handle unusual features such as sudden regime changes,
and can be tailored to different forecasting problems. We compare our method to
both established methodologies and state of the art methods for a number of
macroeconomic forecasting examples. We find that the proposed method offers an
attractive, computationally efficient alternative to the benchmark
methodologies and often outperforms more complex techniques.",-0.24497521,0.022259621,0.112956755,B
1158,"The log-sum-exp trick is an alternative way of handling this numerical instability which
would, at least in part, eliminate the need for the constant c. We largely leave the role of
this parameter to further research.","4
where pk(yt|yt−1) is the predictive likelihood at time t, given information available at time
t − 1 for model k (i.e., the likelihood calculated using the posterior predictive distribution
yt|yt−1 for model k given the actual realisation yt) and c is a small positive number intro-
duced to avoid model probability being brought to machine zero by aberrant observations4.","The recursions in (2.1) and (2.2) amount to a closed form algorithm to calculate the
probability that model k is the best model given information from the previous time step
t − 1, for forecasting at time t. A model receives a higher weight if it performed well in
the recent past.",2022-01-28 11:17:12+00:00,A loss discounting framework for model averaging and selection in time series models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Dawid Bernaciak'), arxiv.Result.Author('Jim E. Griffin')]","We introduce a Loss Discounting Framework for model and forecast combination
which generalises and combines Bayesian model synthesis and generalized Bayes
methodologies. We use a loss function to score the performance of different
models and introduce a multilevel discounting scheme which allows a flexible
specification of the dynamics of the model weights. This novel and simple model
combination approach can be easily applied to large scale model
averaging/selection, can handle unusual features such as sudden regime changes,
and can be tailored to different forecasting problems. We compare our method to
both established methodologies and state of the art methods for a number of
macroeconomic forecasting examples. We find that the proposed method offers an
attractive, computationally efficient alternative to the benchmark
methodologies and often outperforms more complex techniques.",-0.3083999,0.035972167,0.049150903,A
1258,"Below we conclude by providing practical suggestions in using a pigeonhole design, and by
pointing out three limitations of our paper that could lead to further research directions.","We run simulation results to demonstrate the strength of the pigeonhole
design.",8.1.,2022-01-30 23:14:24+00:00,Stratifying Online Field Experiments Using the Pigeonhole Design,stat.ME,"['stat.ME', 'econ.EM']","[arxiv.Result.Author('Jinglong Zhao'), arxiv.Result.Author('Zijie Zhou')]","Practitioners and academics have long appreciated the benefits that
experimentation brings to firms. For online web-facing firms, however, it still
remains challenging in handling heterogeneity when experimental units arrive
sequentially in online field experiments. In this paper, we study a novel
online experimental design problem, which we refer to as the ""Online
Stratification Problem."" In this problem, experimental units with heterogeneous
covariate information arrive sequentially and must be immediately assigned into
either the control or the treatment group, with an objective of minimizing the
total discrepancy, which is defined as the minimum weight perfect matching
between the two groups. To solve this problem, we propose a novel experimental
design approach, which we refer to as the ""Pigeonhole Design."" The pigeonhole
design first partitions the covariate space into smaller spaces, which we refer
to as pigeonholes, and then, when the experimental units arrive at each
pigeonhole, balances the number of control and treatment units for each
pigeonhole. We analyze the theoretical performance of the pigeonhole design and
show its effectiveness by comparing against two well-known benchmark designs:
the match-pair design and the completely randomized design. We conduct
extensive simulations to study the numerical performance of the different
designs and conclude with practical suggestions.",0.12217131,-0.33647376,0.23386076,C
1259,"Below we conclude by providing practical suggestions for using a pigeonhole design, and by pointing
out three limitations of our paper that suggest further research directions.",We have run simulation results to demonstrate the strength of the pigeonhole design.,8.1.,2022-01-30 23:14:24+00:00,Stratifying Online Field Experiments Using the Pigeonhole Design,stat.ME,"['stat.ME', 'econ.EM']","[arxiv.Result.Author('Jinglong Zhao'), arxiv.Result.Author('Zijie Zhou')]","Practitioners and academics have long appreciated the benefits that
experimentation brings to firms. For online web-facing firms, however, it still
remains challenging in handling heterogeneity when experimental units arrive
sequentially in online field experiments. In this paper, we study a novel
online experimental design problem, which we refer to as the ""Online
Stratification Problem."" In this problem, experimental units with heterogeneous
covariate information arrive sequentially and must be immediately assigned into
either the control or the treatment group, with an objective of minimizing the
total discrepancy, which is defined as the minimum weight perfect matching
between the two groups. To solve this problem, we propose a novel experimental
design approach, which we refer to as the ""Pigeonhole Design."" The pigeonhole
design first partitions the covariate space into smaller spaces, which we refer
to as pigeonholes, and then, when the experimental units arrive at each
pigeonhole, balances the number of control and treatment units for each
pigeonhole. We analyze the theoretical performance of the pigeonhole design and
show its effectiveness by comparing against two well-known benchmark designs:
the match-pair design and the completely randomized design. We conduct
extensive simulations to study the numerical performance of the different
designs and conclude with practical suggestions.",0.14509442,-0.31996012,0.26018313,C
1260,"Below we conclude by providing practical suggestions for using a pigeonhole design, and by pointing
out three limitations of our paper that suggest further research directions.",We have run simulation results to demonstrate the strength of the pigeonhole design.,8.1.,2022-01-30 23:14:24+00:00,Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective,stat.ME,"['stat.ME', 'econ.EM']","[arxiv.Result.Author('Jinglong Zhao'), arxiv.Result.Author('Zijie Zhou')]","Practitioners and academics have long appreciated the benefits that
experimentation brings to firms. For online web-facing firms, however, it still
remains challenging in balancing covariate information when experimental units
arrive sequentially in online field experiments. In this paper, we study a
novel online experimental design problem, which we refer to as the ""Online
Blocking Problem."" In this problem, experimental units with heterogeneous
covariate information arrive sequentially and must be immediately assigned into
either the control or the treatment group, with an objective of minimizing the
total discrepancy, which is defined as the minimum weight perfect matching
between the two groups. To solve this problem, we propose a novel experimental
design approach, which we refer to as the ""Pigeonhole Design."" The pigeonhole
design first partitions the covariate space into smaller spaces, which we refer
to as pigeonholes, and then, when the experimental units arrive at each
pigeonhole, balances the number of control and treatment units for each
pigeonhole. We analyze the theoretical performance of the pigeonhole design and
show its effectiveness by comparing against two well-known benchmark designs:
the match-pair design and the completely randomized design. We conduct
extensive simulations to study the numerical performance of the different
designs and conclude with practical suggestions.",0.14509442,-0.31996012,0.26018313,C
1292,"Section 5 concludes the paper and gives
directions for further research.","In Section 4,

                                                  2
                                          arXiv Template                                                  A PREPRINT

the proposed algorithm is applied to several simulated and real sets of data.","Additional details and examples are provided in Appendices A-D.

2 Model selection for generalised linear models

In this section, we will ﬁrst describe the addressed model space.",2022-01-31 13:05:20+00:00,A subsampling approach for Bayesian model selection,stat.ME,"['stat.ME', 'math.ST', 'stat.CO', 'stat.TH', '62-02, 62-09, 62F07, 62F15, 62J12, 62J05, 62J99, 62M05, 05A16,\n  60J22, 92D20, 90C27, 90C59', 'G.1.2; G.1.6; G.2.1; G.3; I.2.0; I.2.6; I.2.8; I.5.1; I.6; I.6.4']","[arxiv.Result.Author('Jon Lachmann'), arxiv.Result.Author('Geir Storvik'), arxiv.Result.Author('Florian Frommlet'), arxiv.Result.Author('Aliaksadr Hubin')]","It is common practice to use Laplace approximations to compute marginal
likelihoods in Bayesian versions of generalised linear models (GLM). Marginal
likelihoods combined with model priors are then used in different search
algorithms to compute the posterior marginal probabilities of models and
individual covariates. This allows performing Bayesian model selection and
model averaging. For large sample sizes, even the Laplace approximation becomes
computationally challenging because the optimisation routine involved needs to
evaluate the likelihood on the full set of data in multiple iterations. As a
consequence, the algorithm is not scalable for large datasets. To address this
problem, we suggest using a version of a popular batch stochastic gradient
descent (BSGD) algorithm for estimating the marginal likelihood of a GLM by
subsampling from the data. We further combine the algorithm with Markov chain
Monte Carlo (MCMC) based methods for Bayesian model selection and provide some
theoretical results on the convergence of the estimates. Finally, we report
results from experiments illustrating the performance of the proposed
algorithm.",-0.24763355,-0.061670925,0.013524591,A
1293,"Also, studying if the strategy that restarts the optimisation algorithm from the previous
estimates when revisiting a model (from Theorem 1) improves the convergence properties of the combined algorithm is
of interest for further research.","We however, keep this as a potential
subject for another article.","Lastly, as an alternative to the Laplace approximation for models with latent Gaussian structures, Integrated Nested
Laplace Approximations (INLA) have emerged as an efﬁcient approximation method (Rue et al., 2009).",2022-01-31 13:05:20+00:00,A subsampling approach for Bayesian model selection,stat.ME,"['stat.ME', 'math.ST', 'stat.CO', 'stat.TH', '62-02, 62-09, 62F07, 62F15, 62J12, 62J05, 62J99, 62M05, 05A16,\n  60J22, 92D20, 90C27, 90C59', 'G.1.2; G.1.6; G.2.1; G.3; I.2.0; I.2.6; I.2.8; I.5.1; I.6; I.6.4']","[arxiv.Result.Author('Jon Lachmann'), arxiv.Result.Author('Geir Storvik'), arxiv.Result.Author('Florian Frommlet'), arxiv.Result.Author('Aliaksadr Hubin')]","It is common practice to use Laplace approximations to compute marginal
likelihoods in Bayesian versions of generalised linear models (GLM). Marginal
likelihoods combined with model priors are then used in different search
algorithms to compute the posterior marginal probabilities of models and
individual covariates. This allows performing Bayesian model selection and
model averaging. For large sample sizes, even the Laplace approximation becomes
computationally challenging because the optimisation routine involved needs to
evaluate the likelihood on the full set of data in multiple iterations. As a
consequence, the algorithm is not scalable for large datasets. To address this
problem, we suggest using a version of a popular batch stochastic gradient
descent (BSGD) algorithm for estimating the marginal likelihood of a GLM by
subsampling from the data. We further combine the algorithm with Markov chain
Monte Carlo (MCMC) based methods for Bayesian model selection and provide some
theoretical results on the convergence of the estimates. Finally, we report
results from experiments illustrating the performance of the proposed
algorithm.",-0.3207649,-0.03176158,-0.03927251,A
1356,"12
(a) Conditional probabilities of series 2 for the maximum      (b) Conditional probabilities of series 2 for the minimum
value of spreadt−1                                             value of spreadt−1

Figure 8: Transition Probabilities of Series 2: DJIA

6 Conclusions, limitations and further research

Several proposals for including of exogenous variables in MMC models have been presented.","Overall, the rest of the probabilities structure, remains the same.","The main limitations
were associated with the high complexity of the models to be developed and estimated.",2022-02-01 11:04:16+00:00,GenMarkov: Modeling Generalized Multivariate Markov Chains in R,stat.ME,['stat.ME'],"[arxiv.Result.Author('Carolina Vasconcelos'), arxiv.Result.Author('Bruno Damásio')]","This article proposes a new generalization of the Multivariate Markov Chains
(MMC) model. The future values of a Markov chain commonly depend on only the
past values of the chain in an autoregressive fashion. The generalization
proposed in this work also considers exogenous variables that can be
deterministic or stochastic. Furthermore, the effects of the MMC's past values
and the effects of pre--determined or exogenous covariates are considered in
our model by considering a non--homogeneous Markov chain. The Monte Carlo
simulation study findings showed that our model consistently detected a
non--homogeneous Markov chain. Besides, an empirical illustration demonstrated
the relevance of this new model by estimating probability transition matrices
over the space state of the exogenous variable. An additional and practical
contribution of this work is the development of a novel R package with this
generalization.",-0.19011416,0.00060863886,-0.062071823,A
1357,"The limitations are related to the
implementation in R, speciﬁcally the optimization algorithm applied is not common for MMC models, in that sense,
it would be beneﬁcial to study new approaches to optimizing the maximum likelihood function as further research.","The main contributions of this work are the development of a package with functions for multivariate Markov chains,
addressing the statistical inference in these models and the inclusion of covariates.","Additionally, extending this generalization to the MTD-probit model proposed by Nicolau (2014) would also be relevant,
which removes the constraints of the model’s parameters and allows the model to detect negative effects.",2022-02-01 11:04:16+00:00,GenMarkov: Modeling Generalized Multivariate Markov Chains in R,stat.ME,['stat.ME'],"[arxiv.Result.Author('Carolina Vasconcelos'), arxiv.Result.Author('Bruno Damásio')]","This article proposes a new generalization of the Multivariate Markov Chains
(MMC) model. The future values of a Markov chain commonly depend on only the
past values of the chain in an autoregressive fashion. The generalization
proposed in this work also considers exogenous variables that can be
deterministic or stochastic. Furthermore, the effects of the MMC's past values
and the effects of pre--determined or exogenous covariates are considered in
our model by considering a non--homogeneous Markov chain. The Monte Carlo
simulation study findings showed that our model consistently detected a
non--homogeneous Markov chain. Besides, an empirical illustration demonstrated
the relevance of this new model by estimating probability transition matrices
over the space state of the exogenous variable. An additional and practical
contribution of this work is the development of a novel R package with this
generalization.",-0.17815694,-0.030711591,0.02897428,A
1382,"Therefore,
one could choose an appropriate λ˜ = λαr/αf (equivalent to choosing some value of λ),
without knowing the value of αf or αr; nevertheless, how to calibrate λ˜ still requires
further study.","∝ (λαr/αf )K (0,i)∈T gr(yi; θ) (i,j)∈T gr(yi | yj; θ).","A Model-based Extensions to Forest Model

A.1 Extension to High-dimensional Clustering Model

For clustering high dimensional data, good performances have been demonstrated through

ﬁnding a low-dimensional sparse representation zi for each yi (Vidal, 2011; Wu et al., 2014),

and then clustering zi instead of yi.",2022-02-01 15:45:38+00:00,"Spectral Clustering, Bayesian Spanning Forest, and Forest Process",stat.ME,['stat.ME'],"[arxiv.Result.Author('Leo L. Duan'), arxiv.Result.Author('Arkaprava Roy')]","Spectral clustering views the similarity matrix as a weighted graph, and
partitions the data by minimizing a graph-cut loss. Since it minimizes the
across-cluster similarity, there is no need to model the distribution within
each cluster. As a result, one reduces the chance of model misspecification,
which is often a risk in mixture model-based clustering. Nevertheless, compared
to the latter, spectral clustering has no direct ways of quantifying the
clustering uncertainty (such as the assignment probability), or allowing easy
model extensions for complicated data applications. To fill this gap, we
propose the Bayesian forest model as a generative graphical model for spectral
clustering. This is motivated by our discovery that the posterior connecting
matrix in a forest model has almost the same leading eigenvectors, as the ones
used by normalized spectral clustering. To construct priors, we develop a
``forest process'' as a graph extension to the urn process, while we carefully
characterize the differences in the partition probability. We derive a simple
Markov chain Monte Carlo algorithm for posterior estimation, and demonstrate
superior performance compared to existing algorithms. We illustrate several
model-based extensions useful for data applications, including high-dimensional
and multi-view clustering for images.",-0.15242258,-0.18353978,0.21925595,A
1413,"Our proposed framework would beneﬁt from further research in the maximal number
of minimally invariant sets among graphs of a ﬁxed size, as this would provide larger ﬁnite
sample power for identifying ancestors.","We have validated our procedure both on simulated and real
data.","Further it is of interest to establish ﬁnite sample
guarantees or convergence rates for IAS, possibly by imposing additional assumptions on
the class of SCMs.",2022-02-02 08:28:00+00:00,Invariant Ancestry Search,stat.ME,"['stat.ME', 'cs.LG']","[arxiv.Result.Author('Phillip B. Mogensen'), arxiv.Result.Author('Nikolaj Thams'), arxiv.Result.Author('Jonas Peters')]","Recently, methods have been proposed that exploit the invariance of
prediction models with respect to changing environments to infer subsets of the
causal parents of a response variable. If the environments influence only few
of the underlying mechanisms, the subset identified by invariant causal
prediction, for example, may be small, or even empty. We introduce the concept
of minimal invariance and propose invariant ancestry search (IAS). In its
population version, IAS outputs a set which contains only ancestors of the
response and is a superset of the output of ICP. When applied to data,
corresponding guarantees hold asymptotically if the underlying test for
invariance has asymptotic level and power. We develop scalable algorithms and
perform experiments on simulated and real data.",-0.09313817,-0.44887733,0.0068581924,A
1414,"Our proposed framework would beneﬁt from further research in the maximal number

    8These results do not hold in the presence of hidden variables, because it is not guaranteed that an
invariant set exists among XO (e.g., consider a graph where all observed variables share a common, unobserved
confounder with Y ).","We have validated our procedure both on simulated and real
data.","However, if at least one minimally invariant set exists among the observed variables,
then all results stated in this paper hold.",2022-02-02 08:28:00+00:00,Invariant Ancestry Search,stat.ME,"['stat.ME', 'cs.LG']","[arxiv.Result.Author('Phillip B. Mogensen'), arxiv.Result.Author('Nikolaj Thams'), arxiv.Result.Author('Jonas Peters')]","Recently, methods have been proposed that exploit the invariance of
prediction models with respect to changing environments to infer subsets of the
causal parents of a response variable. If the environments influence only few
of the underlying mechanisms, the subset identified by invariant causal
prediction (ICP), for example, may be small, or even empty. We introduce the
concept of minimal invariance and propose invariant ancestry search (IAS). In
its population version, IAS outputs a set which contains only ancestors of the
response and is a superset of the output of ICP. When applied to data,
corresponding guarantees hold asymptotically if the underlying test for
invariance has asymptotic level and power. We develop scalable algorithms and
perform experiments on simulated and real data.",-0.06505826,-0.114222944,-0.054484013,A
1494,We conclude this paper by outlining some interesting directions for further research.,"Moreover, PLreg provides procedures for choosing the extra parameter, when needed.","The power
logit regression models may be extended to accomodate situations in which the data include obser-
vations at the boundaries.",2022-02-03 17:04:53+00:00,Power logit regression for modeling bounded data,stat.ME,['stat.ME'],"[arxiv.Result.Author('Francisco Felipe Queiroz'), arxiv.Result.Author('Silvia Lopes Paula Ferrari')]","The main purpose of this paper is to introduce a new class of regression
models for bounded continuous data, commonly encountered in applied research.
The models, named the power logit regression models, assume that the response
variable follows a distribution in a wide, flexible class of distributions with
three parameters, namely the median, a dispersion parameter and a skewness
parameter. The paper offers a comprehensive set of tools for likelihood
inference and diagnostic analysis, and introduces the new R package PLreg.
Applications with real and simulated data show the merits of the proposed
models, the statistical tools, and the computational package.",0.004902784,0.12328309,-0.12356365,A
1525,"The extension to more
general models deserves further study.","Third, we assume the overall
model belongs to an exponential family which is still restrictive.","References

 Akaike, H. (1970).",2022-02-04 02:53:36+00:00,Model Averaging for Generalized Linear Models in Fragmentary Data Prediction,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Chaoxia Yuan'), arxiv.Result.Author('Yang Wu'), arxiv.Result.Author('Fang Fang')]","Fragmentary data is becoming more and more popular in many areas which brings
big challenges to researchers and data analysts. Most existing methods dealing
with fragmentary data consider a continuous response while in many applications
the response variable is discrete. In this paper we propose a model averaging
method for generalized linear models in fragmentary data prediction. The
candidate models are fitted based on different combinations of covariate
availability and sample size. The optimal weight is selected by minimizing the
Kullback-Leibler loss in the com?pleted cases and its asymptotic optimality is
established. Empirical evidences from a simulation study and a real data
analysis about Alzheimer disease are presented.",-0.21789475,-0.13028781,-0.17362952,A
1527,"Such methods could be adapted to the population calibration context, but we leave that for
further research.","(2020a) for an analogous approach
to ABC.","Population calibration problems make up a growing proportion of the biological literature, as the
need for mechanistic approaches to accommodate variability becomes better understood (see for ex-
ample Brown et al.",2022-02-04 04:12:42+00:00,Population Calibration using Likelihood-Free Bayesian Inference,stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author('Christopher Drovandi'), arxiv.Result.Author('Brodie Lawson'), arxiv.Result.Author('Adrianne L Jenner'), arxiv.Result.Author('Alexander P Browning')]","In this paper we develop a likelihood-free approach for population
calibration, which involves finding distributions of model parameters when fed
through the model produces a set of outputs that matches available population
data. Unlike most other approaches to population calibration, our method
produces uncertainty quantification on the estimated distribution. Furthermore,
the method can be applied to any population calibration problem, regardless of
whether the model of interest is deterministic or stochastic, or whether the
population data is observed with or without measurement error. We demonstrate
the method on several examples, including one with real data. We also discuss
the computational limitations of the approach. Immediate applications for the
methodology developed here exist in many areas of medical research including
cancer, COVID-19, drug development and cardiology.",0.16505241,0.019251287,0.09530841,C
1540,"The issue of single-arm studies in the congruent dataset
should be subject of further research.","We suggest that the examined range of threshold values is reasonable for the
safety concerns of each treatment.","Models that include single arm studies in the meta-
analysis could be considered, although the risk of bias in the estimates they provide is not to
be underestimated.",2022-02-04 12:29:50+00:00,Decision curve analysis for personalized treatment choice between multiple options,stat.ME,['stat.ME'],"[arxiv.Result.Author('Konstantina Chalkou'), arxiv.Result.Author('Andrew J. Vickers'), arxiv.Result.Author('Fabio Pellegrini'), arxiv.Result.Author('Andrea Manca'), arxiv.Result.Author('Georgia Salanti')]","Decision curve analysis can be used to determine whether a personalized model
for treatment benefit would lead to better clinical decisions. Decision curve
analysis methods have been described to estimate treatment benefit using data
from a single RCT. Our main objective is to extend the decision curve analysis
methodology to the scenario where several treatment options exist and evidence
about their effects comes from a set of trials, synthesized using network
meta-analysis (NMA). We describe the steps needed to estimate the net benefit
of a prediction model using evidence from studies synthesized in an NMA. We
show how to compare personalized versus one-size-fit-all treatment
decision-making strategies, like ""treat none"" or ""treat all patients with a
specific treatment"" strategies. The net benefit per strategy can then be
plotted for a plausible range of threshold probabilities to reveal the most
clinically useful strategy. We applied our methodology to an NMA prediction
model for relapsing-remitting multiple sclerosis, which can be used to choose
between Natalizumab, Dimethyl Fumarate, Glatiramer Acetate, and placebo. We
illustrated the extended decision curve analysis methodology using several
threshold values combinations for each available treatment. For the examined
threshold values, the ""treat patients according to the prediction model""
strategy performs either better than or close to the one-size-fit-all treatment
strategies. However, even small differences may be important in clinical
decision-making. As the advantage of the personalized model was not consistent
across all thresholds, an improved model may be needed before advocating its
applicability for decision-making. This novel extension of decision curve
analysis can be applied to NMA based prediction models to evaluate their use to
aid treatment decision-making.",0.5192034,0.14917439,-0.082968436,C_centroid
1557,"Extending the results presented in this work to that setting presents an inter-
esting direction of further research, motivated by important and timely practical applications.","Although the BCT framework was originally developed for modelling and inference of discrete-
valued time series, it was recently used to develop general mixture models for real-valued time se-
ries, along with a collection of associated algorithmic tools for inference (Papageorgiou and Kon-
toyiannis, 2021).","Acknowledgments

We are grateful to Georgia Gregoriou for providing us the spike train data of Section 5.2.",2022-02-04 17:17:03+00:00,"Posterior Representations for Bayesian Context Trees: Sampling, Estimation and Convergence",stat.ME,"['stat.ME', 'cs.IT', 'math.IT', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Ioannis Papageorgiou'), arxiv.Result.Author('Ioannis Kontoyiannis')]","We revisit the Bayesian Context Trees (BCT) modelling framework for discrete
time series, which was recently found to be very effective in numerous tasks
including model selection, estimation and prediction. A novel representation of
the induced posterior distribution on model space is derived in terms of a
simple branching process, and several consequences of this are explored in
theory and in practice. First, it is shown that the branching process
representation leads to a simple variable-dimensional Monte Carlo sampler for
the joint posterior distribution on models and parameters, which can
efficiently produce independent samples. This sampler is found to be more
efficient than earlier MCMC samplers for the same tasks. Then, the branching
process representation is used to establish the asymptotic consistency of the
BCT posterior, including the derivation of an almost-sure convergence rate.
Finally, an extensive study is carried out on the performance of the induced
Bayesian entropy estimator. Its utility is illustrated through both simulation
experiments and real-world applications, where it is found to outperform
several state-of-the-art methods.",-0.35838383,0.09436692,0.026433919,A
1635,"Whether the
results remain robust or not, the comprehensive robustness test provides more statistical
information than a statistical model using the dataset of only one grid cell speciﬁcation,
to encourage further research.",Is it because key variables are prone to measurement errors?,The following section exempliﬁes the use of the test.,2022-02-07 14:46:38+00:00,Uncertainty in Grid Data: A Theory and Comprehensive Robustness Test,stat.ME,"['stat.ME', 'stat.AP']",[arxiv.Result.Author('Akisato Suzuki')],"This article makes two novel contributions to spatial political and conflict
research using grid data. First, it develops a theory of how uncertainty
specific to grid data affects inference. Second, it introduces a comprehensive
robustness test on sensitivity to this uncertainty, implemented in R. The
uncertainty stems from (1) what is the correct size of grid cells, (2) what is
the correct locations on which to draw dividing lines between these grid cells,
and (3) a greater effect of measurement errors due to finer grid cells. My test
aggregates grid cells into a larger size of choice as the multiple of the
original grid cells. It also enables different starting points of grid cell
aggregation (e.g., whether to start the aggregation from the corner of the
entire map or one grid cell of the original size away from the corner) to shift
the diving lines. I apply my test to Tollefsen, Strand, and Buhaug (2012) to
substantiate its use.",0.2023405,-0.003720142,0.08553183,C
1659,"This question has been recently identiﬁed
by an expert panel on lung–kidney interactions in critically ill patients (Joannidis et al., 2020) as an area in need of
further research.","4 Illustrative application

4.1 Background on motivating study and data

To illustrate the proposed methods, we aim to answer the following question: what is the effect of invasive mechanical
ventilation (IMV) on acute kidney injury (AKI) among COVID-19 patients?","AKI is a common condition in the ICU, complicating about 30% of ICU admissions, and causing
increased risk of in-hospital mortality and long-term morbidity and mortality (Kes & Jukic´, 2010).",2022-02-07 20:57:03+00:00,Causal survival analysis under competing risks using longitudinal modified treatment policies,stat.ME,['stat.ME'],"[arxiv.Result.Author('Iván Díaz'), arxiv.Result.Author('Katherine L Hoffman'), arxiv.Result.Author('Nima S. Hejazi')]","Longitudinal modified treatment policies (LMTP) have been recently developed
as a novel method to define and estimate causal parameters that depend on the
natural value of treatment. LMTPs represent an important advancement in causal
inference for longitudinal studies as they allow the non-parametric definition
and estimation of the joint effect of multiple categorical, numerical, or
continuous exposures measured at several time points. We extend the LMTP
methodology to problems in which the outcome is a time-to-event variable
subject to right-censoring and competing risks. We present identification
results and non-parametric locally efficient estimators that use flexible
data-adaptive regression techniques to alleviate model misspecification bias,
while retaining important asymptotic properties such as $\sqrt{n}$-consistency.
We present an application to the estimation of the effect of the
time-to-intubation on acute kidney injury amongst COVID-19 hospitalized
patients, where death by other causes is taken to be the competing event.",0.3228429,0.031827513,0.09566334,C
1831,"Large epidemiological studies are needed to further study these
signals.","Therefore, Ageusia and Anosmia might be related to the
underlying disease rather than COVID-19 vaccines.","We didn’t identify any enriched AE groups as there were only 8 safety signals using the negative control
approach.",2022-02-10 23:43:17+00:00,Bayesian learning of COVID-19 Vaccine safety while incorporating Adverse Events ontology,stat.ME,['stat.ME'],"[arxiv.Result.Author('Bangyao Zhao'), arxiv.Result.Author('Yuan Zhong'), arxiv.Result.Author('Jian Kang'), arxiv.Result.Author('Lili Zhao')]","While vaccines are crucial to end the COVID-19 pandemic, public confidence in
vaccine safety has always been vulnerable. Many statistical methods have been
applied to VAERS (Vaccine Adverse Event Reporting System) database to study the
safety of COVID-19 vaccines. However, all these methods ignored the adverse
event (AE) ontology. AEs are naturally related; for example, events of
retching, dysphagia, and reflux are all related to an abnormal digestive
system. Explicitly bringing AE relationships into the model can aid in the
detection of true AE signals amid the noise while reducing false positives. We
propose a Bayesian graphical model to estimate all AEs while incorporating the
AE ontology simultaneously. We proposed strategies to construct conjugate forms
leading to an efficient Gibbs sampler. Built upon the posterior distributions,
we proposed a negative control approach to mitigate reporting bias and an
enrichment approach to detect AE groups of concern. The proposed methods were
evaluated using simulation studies and were further illustrated on studying the
safety of COVID-19 vaccines. The proposed methods were implemented in R package
\textit{BGrass} and source code are available at
https://github.com/BangyaoZhao/BGrass.",0.33778924,0.053913306,0.036589045,C
1836,"A plausible candidate would be something like geodesic regression
Fletcher (2013), so the conjunction of these two methods opens an avenue for
further research.","So far, post-hoc analysis of MDMR tends to be done
on a situational basis, so post-hoc analysis for geometric-MDMR has yet to be
explored.","References

Aine, C. J., Bockholt, H. J., Bustillo, J. R., Can˜ive, J. M., Caprihan, A., Gas-
   parovic, C., Hanlon, F. M., Houck, J. M., Jung, R. E., Lauriello, J., Liu, J.,
   Mayer, A. R., Perrone-Bizzozero, N. I., Posse, S., Stephen, J. M., Turner,
   J.",2022-02-11 01:17:50+00:00,Multivariate distance matrix regression for a manifold-valued response variable,stat.ME,['stat.ME'],"[arxiv.Result.Author('Matt Ryan'), arxiv.Result.Author('Gary Glonek'), arxiv.Result.Author('Melissa Humphries'), arxiv.Result.Author('Jono Tuke')]","In this paper, we propose the use of geodesic distances in conjunction with
multivariate distance matrix regression, called geometric-MDMR, as a powerful
first step analysis method for manifold-valued data. Manifold-valued data is
appearing more frequently in the literature from analyses of earthquake to
analysing brain patterns. Accounting for the structure of this data increases
the complexity of your analysis, but allows for much more interpretable results
in terms of the data. To test geometric-MDMR, we develop a method to simulate
functional connectivity matrices for fMRI data to perform a simulation study,
which shows that our method outperforms the current standards in fMRI analysis.",0.006684367,-0.058389474,0.16137812,A
1839,"Developing data-driven methods
to choose their values is an interesting direction for further research.","The appropriate choice of the replacement number of the rescaled bootstrap or projection
dimension of the PRW distance is important in practice.","References

 [1] Heinz H Bauschke, Patrick L Combettes, et al.",2022-02-11 08:09:39+00:00,Inference for Projection-Based Wasserstein Distances on Finite Spaces,stat.ME,['stat.ME'],"[arxiv.Result.Author('Ryo Okano'), arxiv.Result.Author('Masaaki Imaizumi')]","The Wasserstein distance is a distance between two probability distributions
and has recently gained increasing popularity in statistics and machine
learning, owing to its attractive properties. One important approach to
extending this distance is using low-dimensional projections of distributions
to avoid a high computational cost and the curse of dimensionality in empirical
estimation, such as the sliced Wasserstein or max-sliced Wasserstein distances.
Despite their practical success in machine learning tasks, the availability of
statistical inferences for projection-based Wasserstein distances is limited
owing to the lack of distributional limit results. In this paper, we consider
distances defined by integrating or maximizing Wasserstein distances between
low-dimensional projections of two probability distributions. Then we derive
limit distributions regarding these distances when the two distributions are
supported on finite points. We also propose a bootstrap procedure to estimate
quantiles of limit distributions from data. This facilitates asymptotically
exact interval estimation and hypothesis testing for these distances. Our
theoretical results are based on the arguments of Sommerfeld and Munk (2018)
for deriving distributional limits regarding the original Wasserstein distance
on finite spaces and the theory of sensitivity analysis in nonlinear
programming. Finally, we conduct numerical experiments to illustrate the
theoretical results and demonstrate the applicability of our inferential
methods to real data analysis.",0.020314895,-0.018364653,0.17118359,A
2061,"Lohman goes on to suggest specific reasons why her theory is more applicable to the developed
countries than to the developing countries, and calls for further research on this subject.","Part of my message is that the specifics of how a central bank is
embedded in a larger political system matter a great deal, and for this reason my analysis is
unlikely to generalize to other countries in a straightforward way” (Lohmann, 1998:445).","She
arrives, in short, at a well-defined contingent generalization that identifies the scope conditions
of her theory and the population to which it best applies.",2022-02-16 13:43:45+00:00,"Where the Model Frequently Meets the Road: Combining Statistical, Formal, and Case Study Methods",stat.ME,['stat.ME'],"[arxiv.Result.Author('Andrew Bennett'), arxiv.Result.Author('Bear F. Braumoeller')]","This paper analyzes the working or default assumptions researchers in the
formal, statistical, and case study traditions typically hold regarding the
sources of unexplained variance, the meaning of outliers, parameter values,
human motivation, functional forms, time, and external validity. We argue that
these working assumptions are often not essential to each method, and that
these assumptions can be relaxed in ways that allow multimethod work to
proceed. We then analyze the comparative advantages of different combinations
of formal, statistical, and case study methods for various theory-building and
theory-testing research objectives. We illustrate these advantages and offer
methodological advice on how to combine different methods, through analysis and
critique of prominent examples of multimethod research.",0.12641287,0.013769196,0.12551725,C
2170,We conclude in Section 6 with a discussion and point to further research.,"In Section 5 we apply our methods to routinely collected health care
records to assess the causal eﬀect of SGLT2i compared to DPP4i as second-line therapy on
the risk of genital infections, using variation in general practitioner prescribing habits as an
IV.","Source code
for this research for all simulations and the application study in this paper is available at
https://github.com/GuedemannLaura/POA-IV.",2022-02-18 12:28:27+00:00,"Triangulating Instrumental Variable, confounder adjustment and Difference-in-Difference methods for comparative effectiveness research in observational data",stat.ME,['stat.ME'],"[arxiv.Result.Author('Laura Güdemann'), arxiv.Result.Author('John M. Dennis'), arxiv.Result.Author('Andrew P. McGovern'), arxiv.Result.Author('Lauren R. Rodgers'), arxiv.Result.Author('Beverley M. Shields'), arxiv.Result.Author('Jack Bowden')]","Observational studies can play a useful role in assessing the comparative
effectiveness of competing treatments. In a clinical trial the randomization of
participants to treatment and control groups generally results in well-balanced
groups with respect to possible confounders, which makes the analysis
straightforward. However, when analysing observational data, the potential for
unmeasured confounding makes comparing treatment effects much more challenging.
Causal inference methods such as the Instrumental Variable and Prior Even Rate
Ratio approaches make it possible to circumvent the need to adjust for
confounding factors that have not been measured in the data or measured with
error. Direct confounder adjustment via multivariable regression and Propensity
score matching also have considerable utility. Each method relies on a
different set of assumptions and leverages different data. In this paper, we
describe the assumptions of each method and assess the impact of violating
these assumptions in a simulation study. We propose the prior outcome augmented
Instrumental Variable method that leverages data from before and after
treatment initiation, and is robust to the violation of key assumptions.
Finally, we propose the use of a heterogeneity statistic to decide two or more
estimates are statistically similar, taking into account their correlation. We
illustrate our causal framework to assess the risk of genital infection in
patients prescribed Sodium-glucose Co-transporter-2 inhibitors versus
Dipeptidyl Peptidase-4 inhibitors as second-line treatment for type 2 diabets
using observational data from the Clinical Practice Research Datalink.",0.41271725,0.07208067,-0.0380872,C
2171,"As
further research, we plan to develop a rigorous hierarchical testing procedure for performing
a similarity analysis across an arbitrary number of estimates, whilst controlling the family
wise error rate.","We hope this statistic could be useful tool when attempting to triangulate ﬁndings from a
range of methods that make diﬀerent assumptions and utilize diﬀerent parts of the data.","We illustrated the application of these methods using routinely collected data on people
with T2D, to assess the relative beneﬁt of SGLT2i compared to DPP4i as second-line ther-
apies on the risk of genital infection.",2022-02-18 12:28:27+00:00,"Triangulating Instrumental Variable, confounder adjustment and Difference-in-Difference methods for comparative effectiveness research in observational data",stat.ME,['stat.ME'],"[arxiv.Result.Author('Laura Güdemann'), arxiv.Result.Author('John M. Dennis'), arxiv.Result.Author('Andrew P. McGovern'), arxiv.Result.Author('Lauren R. Rodgers'), arxiv.Result.Author('Beverley M. Shields'), arxiv.Result.Author('Jack Bowden')]","Observational studies can play a useful role in assessing the comparative
effectiveness of competing treatments. In a clinical trial the randomization of
participants to treatment and control groups generally results in well-balanced
groups with respect to possible confounders, which makes the analysis
straightforward. However, when analysing observational data, the potential for
unmeasured confounding makes comparing treatment effects much more challenging.
Causal inference methods such as the Instrumental Variable and Prior Even Rate
Ratio approaches make it possible to circumvent the need to adjust for
confounding factors that have not been measured in the data or measured with
error. Direct confounder adjustment via multivariable regression and Propensity
score matching also have considerable utility. Each method relies on a
different set of assumptions and leverages different data. In this paper, we
describe the assumptions of each method and assess the impact of violating
these assumptions in a simulation study. We propose the prior outcome augmented
Instrumental Variable method that leverages data from before and after
treatment initiation, and is robust to the violation of key assumptions.
Finally, we propose the use of a heterogeneity statistic to decide two or more
estimates are statistically similar, taking into account their correlation. We
illustrate our causal framework to assess the risk of genital infection in
patients prescribed Sodium-glucose Co-transporter-2 inhibitors versus
Dipeptidyl Peptidase-4 inhibitors as second-line treatment for type 2 diabets
using observational data from the Clinical Practice Research Datalink.",0.33652595,-0.09952909,0.04310475,C
2172,We conclude in Section 6 with a discussion and point to further research.,"In Section 5 we apply our methods to routinely collected health care records to
assess the causal eﬀect of SGLT2i compared to DPP4i as second-line therapy on the risk of
genital infections, exploiting variation in general practitioner prescribing habits to construct
an IV.","Source
code for this research for all simulations and the application study in this paper is available
at https://github.com/GuedemannLaura/POA-IV.",2022-02-18 12:28:27+00:00,"Triangulating Instrumental Variable, confounder adjustment and Difference-in-Difference methods for comparative effectiveness research in observational data",stat.ME,['stat.ME'],"[arxiv.Result.Author('Laura Güdemann'), arxiv.Result.Author('John M. Dennis'), arxiv.Result.Author('Andrew P. McGovern'), arxiv.Result.Author('Lauren R. Rodgers'), arxiv.Result.Author('Beverley M. Shields'), arxiv.Result.Author('William Henley'), arxiv.Result.Author('Jack Bowden')]","Observational studies can play a useful role in assessing the comparative
effectiveness of competing treatments. In a clinical trial the randomization of
participants to treatment and control groups generally results in well-balanced
groups with respect to possible confounders, which makes the analysis
straightforward. However, when analysing observational data, the potential for
unmeasured confounding makes comparing treatment effects much more challenging.
Causal inference methods such as Instrumental Variable and Prior Even Rate
Ratio approaches make it possible to circumvent the need to adjust for
confounding factors that have not been measured in the data or measured with
error. Direct confounder adjustment via multivariable regression and Propensity
score matching also have considerable utility. Each method relies on a
different set of assumptions and leverages different aspects of the data. In
this paper, we describe the assumptions of each method and assess the impact of
violating these assumptions in a simulation study. We propose the prior outcome
augmented Instrumental Variable method that leverages data from before and
after treatment initiation, and is robust to the violation of key assumptions.
Finally, we propose the use of a heterogeneity statistic to decide if two or
more estimates are statistically similar, taking into account their
correlation. We illustrate our causal framework to assess the risk of genital
infection in patients prescribed Sodium-glucose co-transporter-2 inhibitors
versus Dipeptidyl peptidase-4 inhibitors as second-line treatment for Type 2
Diabets using observational data from the Clinical Practice Research Datalink.",0.4168353,0.06673367,-0.031372055,C
2173,"As further research, we plan to develop a rigorous hierarchical testing procedure for per-
forming a similarity analysis across an arbitrary number of estimates, whilst controlling the
overall family wise error rate.","As future work we hope
to better understand when this will be the case.","Another approach for combining IV and DiD approaches has
recently been proposed by Ye et al.",2022-02-18 12:28:27+00:00,"Triangulating Instrumental Variable, confounder adjustment and Difference-in-Difference methods for comparative effectiveness research in observational data",stat.ME,['stat.ME'],"[arxiv.Result.Author('Laura Güdemann'), arxiv.Result.Author('John M. Dennis'), arxiv.Result.Author('Andrew P. McGovern'), arxiv.Result.Author('Lauren R. Rodgers'), arxiv.Result.Author('Beverley M. Shields'), arxiv.Result.Author('William Henley'), arxiv.Result.Author('Jack Bowden')]","Observational studies can play a useful role in assessing the comparative
effectiveness of competing treatments. In a clinical trial the randomization of
participants to treatment and control groups generally results in well-balanced
groups with respect to possible confounders, which makes the analysis
straightforward. However, when analysing observational data, the potential for
unmeasured confounding makes comparing treatment effects much more challenging.
Causal inference methods such as Instrumental Variable and Prior Even Rate
Ratio approaches make it possible to circumvent the need to adjust for
confounding factors that have not been measured in the data or measured with
error. Direct confounder adjustment via multivariable regression and Propensity
score matching also have considerable utility. Each method relies on a
different set of assumptions and leverages different aspects of the data. In
this paper, we describe the assumptions of each method and assess the impact of
violating these assumptions in a simulation study. We propose the prior outcome
augmented Instrumental Variable method that leverages data from before and
after treatment initiation, and is robust to the violation of key assumptions.
Finally, we propose the use of a heterogeneity statistic to decide if two or
more estimates are statistically similar, taking into account their
correlation. We illustrate our causal framework to assess the risk of genital
infection in patients prescribed Sodium-glucose co-transporter-2 inhibitors
versus Dipeptidyl peptidase-4 inhibitors as second-line treatment for Type 2
Diabets using observational data from the Clinical Practice Research Datalink.",0.11295399,-0.11831164,0.12861812,C
2238,"We further study their rates of convergence and
obtain their asymptotic normality under mild theoretical conditions.","By carefully analyzing the logistic
Lasso estimator, we develop a novel weighted debiasing method and propose computationally
eﬃcient debiased estimators for these functionals.","Moreover, conﬁdence intervals
and statistical tests for these functionals are constructed.",2022-02-21 06:08:39+00:00,Statistical Inference for Genetic Relatedness Based on High-Dimensional Logistic Regression,stat.ME,"['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']","[arxiv.Result.Author('Rong Ma'), arxiv.Result.Author('Zijian Guo'), arxiv.Result.Author('T. Tony Cai'), arxiv.Result.Author('Hongzhe Li')]","This paper studies the problem of statistical inference for genetic
relatedness between binary traits based on individual-level genome-wide
association data. Specifically, under the high-dimensional logistic regression
model, we define parameters characterizing the cross-trait genetic correlation,
the genetic covariance and the trait-specific genetic variance. A novel
weighted debiasing method is developed for the logistic Lasso estimator and
computationally efficient debiased estimators are proposed. The rates of
convergence for these estimators are studied and their asymptotic normality is
established under mild conditions. Moreover, we construct confidence intervals
and statistical tests for these parameters, and provide theoretical
justifications for the methods, including the coverage probability and expected
length of the confidence intervals, as well as the size and power of the
proposed tests. Numerical studies are conducted under both model generated data
and simulated genetic data to show the superiority of the proposed methods and
their applicability to the analysis of real genetic data. Finally, by analyzing
a real data set on autoimmune diseases, we demonstrate the ability to obtain
novel insights about the shared genetic architecture between ten pediatric
autoimmune diseases.",-0.19576213,-0.11625636,-0.212349,A
2239,"We further study their rates of convergence and
obtain their asymptotic normality under mild theoretical conditions.","By carefully analyzing the logistic
Lasso estimator, we develop a novel weighted debiasing method and propose computationally
eﬃcient debiased estimators for these functionals.","Moreover, conﬁdence intervals
and statistical tests for these functionals are constructed.",2022-02-21 06:08:39+00:00,Statistical Inference for Genetic Relatedness Based on High-Dimensional Logistic Regression,stat.ME,"['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']","[arxiv.Result.Author('Rong Ma'), arxiv.Result.Author('Zijian Guo'), arxiv.Result.Author('T. Tony Cai'), arxiv.Result.Author('Hongzhe Li')]","This paper studies the problem of statistical inference for genetic
relatedness between binary traits based on individual-level genome-wide
association data. Specifically, under the high-dimensional logistic regression
models, we define parameters characterizing the cross-trait genetic
correlation, the genetic covariance and the trait-specific genetic variance. A
novel weighted debiasing method is developed for the logistic Lasso estimator
and computationally efficient debiased estimators are proposed. The rates of
convergence for these estimators are studied and their asymptotic normality is
established under mild conditions. Moreover, we construct confidence intervals
and statistical tests for these parameters, and provide theoretical
justifications for the methods, including the coverage probability and expected
length of the confidence intervals, as well as the size and power of the
proposed tests. Numerical studies are conducted under both model generated data
and simulated genetic data to show the superiority of the proposed methods. By
analyzing a real data set on autoimmune diseases, we demonstrate its ability to
obtain novel insights about the shared genetic architecture between ten
pediatric autoimmune diseases.",-0.19576213,-0.11625636,-0.212349,A
2333,"Extensions of our uniﬁed framework to testing other problems with nuisance
parameters may deserve further study.","Finally, we apply the proposed test with the age data sets to demonstrate its applica-
tion in practice.","References

Alba, M., Barrera, D., and Jim´enez, M. (2001).",2022-02-20 08:08:51+00:00,A Unified Nonparametric Test of Transformations on Distribution Functions with Nuisance Parameters,stat.ME,"['stat.ME', 'econ.EM']","[arxiv.Result.Author('Xingyu Li'), arxiv.Result.Author('Xiaojun Song'), arxiv.Result.Author('Zhenting Sun')]","This paper proposes a simple unified approach to testing transformations on
cumulative distribution functions (CDFs) in the presence of nuisance
parameters. The proposed test is constructed based on a new characterization
that avoids the estimation of nuisance parameters. The critical values are
obtained through a numerical bootstrap method which can easily be implemented
in practice. Under suitable conditions, the proposed test is shown to be
asymptotically size controlled and consistent. The local power property of the
test is established. Finally, Monte Carlo simulations and an empirical study
show that the test performs well on finite samples.",0.18760504,-0.0799624,-0.013633672,C
2377,"Therefore, this method could be applied in other applications or causal discovery
methods for panel data (e.g., on non-time series data), which remains an exciting
avenue for further research.","Finally, we emphasize that the p-value aggregation method we employ is
a general approach and does not depend on the speciﬁc generation process.","References

C. Angermueller, T. Pärnamaa, L. Parts, and O. Stegle.",2022-02-23 16:49:13+00:00,Testing Granger Non-Causality in Panels with Cross-Sectional Dependencies,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Lenon Minorics'), arxiv.Result.Author('Caner Turkmen'), arxiv.Result.Author('David Kernert'), arxiv.Result.Author('Patrick Bloebaum'), arxiv.Result.Author('Laurent Callot'), arxiv.Result.Author('Dominik Janzing')]","This paper proposes a new approach for testing Granger non-causality on panel
data. Instead of aggregating panel member statistics, we aggregate their
corresponding p-values and show that the resulting p-value approximately bounds
the type I error by the chosen significance level even if the panel members are
dependent. We compare our approach against the most widely used Granger
causality algorithm on panel data and show that our approach yields lower FDR
at the same power for large sample sizes and panels with cross-sectional
dependencies. Finally, we examine COVID-19 data about confirmed cases and
deaths measured in countries/regions worldwide and show that our approach is
able to discover the true causal relation between confirmed cases and deaths
while state-of-the-art approaches fail.",-0.069456816,0.13645847,0.02106677,B
2428,There are several potential directions for further research.,"In this way, we have provided a more detailed analysis for both lung cancer subtypes than previous
studies.","First, it might be captivating to consider diﬀerent supervised learning
methods (for instance, a neural network with more than one layer) instead of the multinomial regression model proposed in
this paper.",2022-02-24 20:17:04+00:00,Multiple multi-sample testing under arbitrary covariance dependency,stat.ME,"['stat.ME', '62J15, 62P10']","[arxiv.Result.Author('Vladimir Vutov'), arxiv.Result.Author('Thorsten Dickhaus')]","Modern high-throughput biomedical devices routinely produce data on a large
scale, and the analysis of high-dimensional datasets has become commonplace in
biomedical studies. However, given thousands or tens of thousands of measured
variables in these datasets, extracting meaningful features poses a challenge.
In this article, we propose a procedure to evaluate the strength of the
associations between a nominal (categorical) response variable and multiple
features simultaneously. Specifically, we propose a framework of large-scale
multiple testing under arbitrary correlation dependency among test statistics.
First, marginal multinomial regressions are performed for each feature
individually. Second, we use an approach of multiple marginal models for each
baseline-category pair to establish asymptotic joint normality of the stacked
vector of the marginal multinomial regression coefficients. Third, we estimate
the (limiting) covariance matrix between the estimated coefficients from all
marginal models. Finally, our approach approximates the realized false
discovery proportion of a thresholding procedure for the marginal p-values, for
each baseline-category pair. The proposed approach offers a sensible trade-off
between the expected numbers of true and false rejections. Furthermore, we
demonstrate a practical application of the method on hyperspectral imaging
data. This dataset is obtained by a matrix-assisted laser desorption/ionization
(MALDI) instrument. MALDI demonstrates tremendous potential for clinical
diagnosis, particularly for cancer research. In our application, the nominal
response categories represent cancer subtypes.",0.08544104,0.056274988,0.13496977,C
2431,"In
the non-stratiﬁed example, the MAP CEG identiﬁed            The problem of eﬃcient model selection for CEGs
by the w-HAC algorithm was the CEG of the data           is an open one with further research needed.",the HAC algorithm in Cowell and Smith [2014].,"We hope
generating process, as was the case for HAC.",2022-02-24 20:27:17+00:00,Bayesian Model Averaging of Chain Event Graphs for Robust Explanatory Modelling,stat.ME,['stat.ME'],"[arxiv.Result.Author('Peter Strong'), arxiv.Result.Author('Jim Q Smith')]","Chain Event Graphs (CEGs) are a widely applicable class of probabilistic
graphical models that are able to represent context-specific independence
statements and asymmetric unfoldings of events in an easily interpretable way.
Existing model selection literature on CEGs has focused on obtaining the
maximum a posteriori (MAP) CEG. However, MAP selection is well-known to ignore
model uncertainty. Here, we explore the use of model averaging over this class.
We demonstrate that such methods express model uncertainty and lead to more
robust inference. Because the space of possible CEGs is huge, scoring models
exhaustively for model averaging in all but small problems is prohibitive.
However we show that a bespoke class of hybrid forward sampling and greedy
search algorithms can successfully and intelligently traverse this space of
candidate models. By applying a simple version of our search method to two
known case studies, we can illustrate the efficacy of such methods compared to
more standard MAP modelling. We also demonstrate how its outputs systematically
inform those component hypotheses that are most robustly supported the data and
high-scoring alternative models to the MAP model.",-0.17713413,-0.15424375,0.09772004,A
2476,"We conclude with a thorough discussion and further research ideas
concerning boosted copula regression models in the ’Conclusion’.","Finally,
parameter shrinkage induced by boosting in particular suits prediction setups like in delivery
management scenarios.","2 Distributional Copula Regression Models

In this section, we ﬁrst review structured additive distributional copula regression models
along the lines of Klein and Kneib (2016) and introduce speciﬁc examples of marginal distri-
butions and copula speciﬁcations that are relevant for our simulations and application in the
sections ’Simulations’ and ’Analysis of Fetal Ultrasound Data’.",2022-02-25 17:56:30+00:00,Boosting Distributional Copula Regression,stat.ME,['stat.ME'],"[arxiv.Result.Author('Nicolai Hans'), arxiv.Result.Author('Nadja Klein'), arxiv.Result.Author('Florian Faschingbauer'), arxiv.Result.Author('Michael Schneider'), arxiv.Result.Author('Andreas Mayr')]","Capturing complex dependence structures between outcome variables (e.g.,
study endpoints) is of high relevance in contemporary biomedical data problems
and medical research. Distributional copula regression provides a flexible tool
to model the joint distribution of multiple outcome variables by disentangling
the marginal response distributions and their dependence structure. In a
regression setup each parameter of the copula model, i.e. the marginal
distribution parameters and the copula dependence parameters, can be related to
covariates via structured additive predictors. We propose a framework to fit
distributional copula regression models via a model-based boosting algorithm.
Model-based boosting is a modern estimation technique that incorporates useful
features like an intrinsic variable selection mechanism, parameter shrinkage
and the capability to fit regression models in high dimensional data setting,
i.e. situations with more covariates than observations. Thus, model-based
boosting does not only complement existing Bayesian and maximum-likelihood
based estimation frameworks for this model class but rather enables unique
intrinsic mechanisms that can be helpful in many applied problems. The
performance of our boosting algorithm in the context of copula regression
models with continuous margins is evaluated in simulation studies that cover
low- and high-dimensional data settings and situations with and without
dependence between the responses. Moreover, distributional copula boosting is
used to jointly analyze and predict the length and the weight of newborns
conditional on sonographic measurements of the fetus before delivery together
with other clinical variables.",-0.08930936,0.13282092,-0.04991306,A
2592,Han P. A further study of the multiply robust estimator in missing data analysis.,28.,J Stat Plan Inference.,2022-02-28 20:05:27+00:00,Robust Causal Inference of Drug-drug Interactions,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Di Shu'), arxiv.Result.Author('Peisong Han'), arxiv.Result.Author('Sean Hennessy'), arxiv.Result.Author('Todd A Miano')]","There is growing interest in developing causal inference methods for
multi-valued treatments with a focus on pairwise average treatment effects.
Here we focus on a clinically important, yet less-studied estimand: causal
drug-drug interactions (DDIs), quantified by the change of the causal effect of
drug $A$ when switching drug $B$ status from ""off"" to ""on"". Control of
confounding by measured variables when studying the effects of DDIs can be
accomplished via inverse probability of treatment weighting (IPTW), a standard
approach to confounding adjustment, originally developed for binary treatments
and later generalized to multi-valued treatments. However, this approach
generally results in biased results when the propensity score model is
misspecified. Motivated by the need for more robust techniques, we propose two
empirical likelihood-based weighting approaches that allow for specifying a set
of propensity score models, with the second method balancing user-specified
covariates directly, by incorporating additional, nonparametric constraints.
The resulting estimators from both methods are consistent when the postulated
set of propensity score models contains a correct one; this property has been
termed multiple robustness. In this paper, we derive two multiply-robust
estimators of the causal DDI, and develop inference procedures. We then
evaluate the finite sample performance of the proposed estimators through
simulation. In doing so, we demonstrate that the proposed estimators outperform
the standard IPTW method in terms of both robustness and efficiency. Finally,
we apply the proposed method to evaluate the impact of renin-angiotensin system
inhibitors (RAS-I) on the comparative nephrotoxicity of nonsteroidal
anti-inflammatory drugs (NSAID) and opioids, using data derived from electronic
medical records from a large multi-hospital health system.",-0.009296935,0.17007387,-0.2764847,A
2593,Han P. A further study of propensity score calibration in missing data analysis.,43.,Stat Sin.,2022-02-28 20:05:27+00:00,Robust Causal Inference of Drug-drug Interactions,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Di Shu'), arxiv.Result.Author('Peisong Han'), arxiv.Result.Author('Sean Hennessy'), arxiv.Result.Author('Todd A Miano')]","There is growing interest in developing causal inference methods for
multi-valued treatments with a focus on pairwise average treatment effects.
Here we focus on a clinically important, yet less-studied estimand: causal
drug-drug interactions (DDIs), quantified by the change of the causal effect of
drug $A$ when switching drug $B$ status from ""off"" to ""on"". Control of
confounding by measured variables when studying the effects of DDIs can be
accomplished via inverse probability of treatment weighting (IPTW), a standard
approach to confounding adjustment, originally developed for binary treatments
and later generalized to multi-valued treatments. However, this approach
generally results in biased results when the propensity score model is
misspecified. Motivated by the need for more robust techniques, we propose two
empirical likelihood-based weighting approaches that allow for specifying a set
of propensity score models, with the second method balancing user-specified
covariates directly, by incorporating additional, nonparametric constraints.
The resulting estimators from both methods are consistent when the postulated
set of propensity score models contains a correct one; this property has been
termed multiple robustness. In this paper, we derive two multiply-robust
estimators of the causal DDI, and develop inference procedures. We then
evaluate the finite sample performance of the proposed estimators through
simulation. In doing so, we demonstrate that the proposed estimators outperform
the standard IPTW method in terms of both robustness and efficiency. Finally,
we apply the proposed method to evaluate the impact of renin-angiotensin system
inhibitors (RAS-I) on the comparative nephrotoxicity of nonsteroidal
anti-inflammatory drugs (NSAID) and opioids, using data derived from electronic
medical records from a large multi-hospital health system.",0.2775858,0.20917383,-0.16760609,C
2594,Han P. A further study of the multiply robust estimator in missing data analysis.,28.,J Stat Plan Inference.,2022-02-28 20:05:27+00:00,Robust Causal Inference of Drug-drug Interactions,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Di Shu'), arxiv.Result.Author('Peisong Han'), arxiv.Result.Author('Sean Hennessy'), arxiv.Result.Author('Todd A Miano')]","There is growing interest in developing causal inference methods for
multi-valued treatments with a focus on pairwise average treatment effects.
Here we focus on a clinically important, yet less-studied estimand: causal
drug-drug interactions (DDIs), which quantifies the degree to which the causal
effect of drug A is altered by the presence versus the absence of drug B.
Confounding adjustment when studying the effects of DDIs can be accomplished
via inverse probability of treatment weighting (IPTW), a standard approach
originally developed for binary treatments and later generalized to
multi-valued treatments. However, this approach generally results in biased
results when the propensity score model is misspecified. Motivated by the need
for more robust techniques, we propose two empirical likelihood-based weighting
approaches that allow for specifying a set of propensity score models, with the
second method balancing user-specified covariates directly, by incorporating
additional, nonparametric constraints. The resulting estimators from both
methods are consistent when the postulated set of propensity score models
contains a correct one; this property has been termed multiple robustness. We
then evaluate their finite sample performance through simulation. The results
demonstrate that the proposed estimators outperform the standard IPTW method in
terms of both robustness and efficiency. Finally, we apply the proposed methods
to evaluate the impact of renin-angiotensin system inhibitors (RAS-I) on the
comparative nephrotoxicity of nonsteroidal anti-inflammatory drugs (NSAID) and
opioids, using data derived from electronic medical records from a large
multi-hospital health system.",-0.009296935,0.17007387,-0.2764847,A
2595,Han P. A further study of propensity score calibration in missing data analysis.,44.,Stat Sin.,2022-02-28 20:05:27+00:00,Robust Causal Inference of Drug-drug Interactions,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Di Shu'), arxiv.Result.Author('Peisong Han'), arxiv.Result.Author('Sean Hennessy'), arxiv.Result.Author('Todd A Miano')]","There is growing interest in developing causal inference methods for
multi-valued treatments with a focus on pairwise average treatment effects.
Here we focus on a clinically important, yet less-studied estimand: causal
drug-drug interactions (DDIs), which quantifies the degree to which the causal
effect of drug A is altered by the presence versus the absence of drug B.
Confounding adjustment when studying the effects of DDIs can be accomplished
via inverse probability of treatment weighting (IPTW), a standard approach
originally developed for binary treatments and later generalized to
multi-valued treatments. However, this approach generally results in biased
results when the propensity score model is misspecified. Motivated by the need
for more robust techniques, we propose two empirical likelihood-based weighting
approaches that allow for specifying a set of propensity score models, with the
second method balancing user-specified covariates directly, by incorporating
additional, nonparametric constraints. The resulting estimators from both
methods are consistent when the postulated set of propensity score models
contains a correct one; this property has been termed multiple robustness. We
then evaluate their finite sample performance through simulation. The results
demonstrate that the proposed estimators outperform the standard IPTW method in
terms of both robustness and efficiency. Finally, we apply the proposed methods
to evaluate the impact of renin-angiotensin system inhibitors (RAS-I) on the
comparative nephrotoxicity of nonsteroidal anti-inflammatory drugs (NSAID) and
opioids, using data derived from electronic medical records from a large
multi-hospital health system.",0.2740765,0.20551819,-0.17463711,C
2799,"An open question that requires further research is what the best testing

procedure is when the number of studies k is no greater than around ﬁve.","They propose the use of

                                                18
approximately inverse variance weights, based on these working models.","Neither the adjusted Hotelling’s T 2 approach in combination with Zhang’s
estimator for the degrees of freedom, which was recommended by Tipton and
Pustejovsky (2015), nor the naive or adjusted F -tests used in our simulations
seem to be the ideal approach.",2022-03-04 10:46:50+00:00,Cluster-Robust Estimators for Bivariate Mixed-Effects Meta-Regression,stat.ME,"['stat.ME', 'stat.CO', '62H12, 62H15, 62J05']","[arxiv.Result.Author('Thilo Welz'), arxiv.Result.Author('Wolfgang Viechtbauer'), arxiv.Result.Author('Markus Pauly')]","Meta-analyses frequently include trials that report multiple effect sizes
based on a common set of study participants. These effect sizes will generally
be correlated. Cluster-robust variance-covariance estimators are a fruitful
approach for synthesizing dependent effects. However, when the number of
studies is small, state-of-the-art robust estimators can yield inflated Type 1
errors. We present two new cluster-robust estimators, in order to improve small
sample performance. For both new estimators the idea is to transform the
estimated variances of the residuals using only the diagonal entries of the hat
matrix. Our proposals are asymptotically equivalent to previously suggested
cluster-robust estimators such as the bias reduced linearization approach. We
apply the methods to real world data and compare and contrast their performance
in an extensive simulation study. We focus on bivariate meta-regression,
although the approaches can be applied more generally.",0.23560792,-0.12577918,-0.089324616,C
2976,"Accounting for such
confounding of the underlying intensity and the detection/capture probability
is an area that needs further research.","For another example, if the underlying
intensity function is a function of the distance from the transect, the underlying
point process is confounded with the detection process.","In most situations, we can avoid such
confounding during the design of the surveys.",2022-03-08 09:34:28+00:00,Data fusion of distance sampling and capture-recapture data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Narmadha M. Mohankumar'), arxiv.Result.Author('Trevor J. Hefley'), arxiv.Result.Author('Katy Silber'), arxiv.Result.Author('W. Alice Boyle')]","Species distribution models (SDMs) are increasingly used in ecology,
biogeography, and wildlife management to learn about the species-habitat
relationships and abundance across space and time. Distance sampling (DS) and
capture-recapture (CR) are two widely collected data types to learn about
species-habitat relationships and abundance; still, they are seldomly used in
SDMs due to the lack of spatial coverage. However, data fusion of the two data
sources can increase spatial coverage, which can reduce parameter uncertainty
and make predictions more accurate, and therefore, can be used for species
distribution modeling. We developed a model-based approach for data fusion of
DS and CR data. Our modeling approach accounts for two common missing data
issues: 1) missing individuals that are missing not at random (MNAR) and 2)
partially missing location information. Using a simulation experiment, we
evaluated the performance of our modeling approach and compared it to existing
approaches that use ad-hoc methods to account for missing data issues. Our
results show that our approach provides unbiased parameter estimates with
increased efficiency compared to the existing approaches. We demonstrated our
approach using data collected for Grasshopper Sparrows (Ammodramus savannarum)
in north-eastern Kansas, USA.",0.07727674,-0.11089965,-0.05591586,A
3012,"Additionally,
several special cases of the models considered herein are also of signiﬁcant interest in their
own right, and may hence merit further study.","For
example, it may be informative to consider asymptotics where the length of the censoring
intervals is allowed to change with the sample size and the number of parameters.","As noted in Section 3, more informative
high-dimensional convergence bounds can likely be obtained for special cases.",2022-03-09 08:59:51+00:00,Concave likelihood-based regression with finite-support response variables,stat.ME,['stat.ME'],"[arxiv.Result.Author('Karl Oskar Ekvall'), arxiv.Result.Author('Matteo Bottai')]","We propose likelihood-based methods for regression when the response variable
has finite support. Our work is motivated by the fact that, in practice,
observed data are discrete and bounded. The proposed methods assume a model
which includes models previously considered for interval-censored variables
with log-concave distributions as special cases. The resulting log-likelihood
is concave, which we use to establish asymptotic normality of its maximizer as
the number of observations $n$ tends to infinity with the number of parameters
$d$ fixed, and rates of convergence of $L_1$-regularized estimators when the
true parameter vector is sparse and $d$ and $n$ both tend to infinity with
$\log(d) / n \to 0$. We consider an inexact proximal Newton algorithm for
computing estimates and give theoretical guarantees for its convergence. The
range of possible applications is wide, including but not limited to survival
analysis in discrete time, the modeling of outcomes on scored surveys and
questionnaires, and, more generally, interval-censored regression. The
applicability and usefulness of the proposed methods are illustrated in
simulations and two data examples.",-0.15496917,-0.122289434,-0.29243767,A
3013,"Additionally,
several special cases of the models considered herein are also of signiﬁcant interest in their
own right, and may hence merit further study.","For

                                                          25
example, it may be informative to consider asymptotics where the length of the censoring
intervals is allowed to change with the sample size and the number of parameters.","As noted in Section 3, more informative
high-dimensional convergence bounds can likely be obtained for special cases.",2022-03-09 08:59:51+00:00,Concave likelihood-based regression with finite-support response variables,stat.ME,['stat.ME'],"[arxiv.Result.Author('Karl Oskar Ekvall'), arxiv.Result.Author('Matteo Bottai')]","We propose a unified framework for likelihood-based regression modeling when
the response variable has finite support. Our work is motivated by the fact
that, in practice, observed data are discrete and bounded. The proposed methods
assume a model which includes models previously considered for
interval-censored variables with log-concave distributions as special cases.
The resulting log-likelihood is concave, which we use to establish asymptotic
normality of its maximizer as the number of observations $n$ tends to infinity
with the number of parameters $d$ fixed, and rates of convergence of
$L_1$-regularized estimators when the true parameter vector is sparse and $d$
and $n$ both tend to infinity with $\log(d) / n \to 0$. We consider an inexact
proximal Newton algorithm for computing estimates and give theoretical
guarantees for its convergence. The range of possible applications is wide,
including but not limited to survival analysis in discrete time, the modeling
of outcomes on scored surveys and questionnaires, and, more generally,
interval-censored regression. The applicability and usefulness of the proposed
methods are illustrated in simulations and data examples.",-0.14050265,-0.12702824,-0.29724222,A
3084,"(Schoenborn et al., 2021)

         In addition to developing more realistic propensity models and model diagnostics, other
topics need further research.","Although we have kept the model covariates simple to
facilitate potential clinical use, future all-cause mortality models could be made more powerful by
accounting for the severity of comorbidities and for geriatric ‘frailty’.","The proposed two-step weighting procedure could be extended to
other epidemiologic study designs, such as case-cohort, nested case-control, and case-control
studies, which are examples of multi-phase sampling designs (Smoot and Haneuse 2015).",2022-03-10 15:05:00+00:00,Nationally Representative Individualized Risk Estimation Combining Individual Data from Epidemiologic Studies and Representative Surveys with Summary Statistics from Disease Registries,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Lingxiao Wang'), arxiv.Result.Author('Yan Li'), arxiv.Result.Author('Barry I. Graubard'), arxiv.Result.Author('Hormuzd A. Katki')]","Estimating individualized absolute risks is fundamental to clinical
decision-making but are often based on data that does not represent the target
population. Current methods improve external validity by including data from
population registries but require transportability assumptions of model
parameters (relative risks and/or population attributable risks) from
epidemiologic studies to the population. We propose a two-step weighting
procedure to estimate absolute risk of an event (in the absence of competing
events) in the target population without transportability assumptions. The
first step improves external-validity for the cohort by creating
""pseudoweights"" for the cohort using a scaled propensity-based kernel-weighting
method, which fractionally distributes sample weights from external probability
reference survey units to cohort units, according to their kernel smoothed
distance in propensity score. The second step poststratifies the pseudoweighted
events in the cohort to a population disease registry by variables available in
the registry. Our approach produces design-consistent absolute risks under
correct specification of the propensity model. Poststratification improves
efficiency and further reduces bias of risk estimates overall and by
demographic variables available in the registry when the true propensity model
is unknown. We apply our methods to develop a nationally representative
all-cause mortality risk model for potential clinical use.",0.24079944,0.22103317,-0.08267768,C
3085,"In addition to developing more realistic propensity models and model diagnostics, other
topics need further research.","Developing absolute risk estimation that is doubly robust to the post-
KW.S weights and the risk model is an area of future research.","The proposed two-step weighting procedure could be extended to
other epidemiologic study designs, such as case-cohort, nested case-control, and case-control
studies, which are examples of multi-phase sampling designs (Smoot and Haneuse 2015).",2022-03-10 15:05:00+00:00,"Representative Individualized Absolute Risk Estimation by Using Data from Epidemiologic Studies, Surveys, and Registries: Estimating Risks for Minority Subgroups",stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Lingxiao Wang'), arxiv.Result.Author('Yan Li'), arxiv.Result.Author('Barry I. Graubard'), arxiv.Result.Author('Hormuzd A. Katki')]","Absolute risks are fundamental to clinical decision-making but are often
estimated from non-representative epidemiologic studies, which usually
underrepresent minorities. ""Model-based"" methods use population registries to
improve externally validity of absolute risk estimation but assume hazard
ratios (HR) are transportable between data sources. ""Pseudoweighting"" methods
improve representativeness of studies by using and external probability-based
survey as the reference, but the resulting estimators can be biased due to
propensity model misspecification or inefficient due to variable pseudoweights
or small sample sizes of minorities in the cohort and/or survey. We propose a
two-step pseudoweighting procedure that poststratifies the event rates among
age/race/sex strata in the pseudoweighted cohort to the population rates to
produce efficient and robust absolute risk estimation. For developing an
all-cause mortality risk model representative for the US, our findings suggest
that HRs for minorities are not transportable, and that surveys can have
inadequate numbers of events for minorities. Poststratification on event rates
is crucial for obtaining reliable estimates for minority subgroups.",0.24228445,0.23629601,-0.13868475,C
3170,"We further study the asymptotic performance of the
                                                 resulting estimators and establish the oracle properties.","In addition, to improve efﬁciency for re-
                                                 gression coefﬁcients, the estimation of the working covariance matrix is involved in
                                                 the proposed iterative algorithm.","We conduct extensive nu-
                                                 merical studies to assess the performance of our proposed estimation strategy and
                                                 numerically illustrate how efﬁcient it is in selecting signiﬁcant variables.",2022-03-11 20:03:45+00:00,High-dimensional Generalized Additive Mixed Model with Longitudinal Data,stat.ME,['stat.ME'],"[arxiv.Result.Author('Mozhgan Taavoni'), arxiv.Result.Author('Mohammad Arashi')]","The problem of simultaneously estimating and selecting important variables is
challenging in the high-dimensional generalized additive mixed model (GAMM)
with longitudinal data because of the correlation structures. We give a
penalized estimation strategy for efficient variable selection when the number
of variables in the linear part and number of components in the non-linear part
can grow with the sample size. A Monte Carlo Newton-Raphson algorithm is
developed where a Metropolis step for generating random effects is used to
apply the proposed double-penalization technique in the high-dimensional GAMM.
In addition, to improve efficiency for regression coefficients, the estimation
of the working covariance matrix is involved in the proposed iterative
algorithm. We further study the asymptotic performance of the resulting
estimators and establish the oracle properties. We conduct extensive numerical
studies to assess the performance of our proposed estimation strategy and
numerically illustrate how efficient it is in selecting significant variables.",-0.24699937,-0.067820996,-0.1975576,A
3195,"In our numerical experiments, we further study the robustness of Maxwayin and compare it with
Maxwayout.","Nevertheless, the
robustness of Maxwayin may be impacted since the estimated g is not independent from the testing
data Dy, which can be viewed as an over-ﬁtting issue conceding the theoretical guarantee on the
robustness of Maxwayin provided in Theorem 2.","Interestingly, we found that Maxwayin is not necessarily less robust than Maxwayout.",2022-03-12 18:21:29+00:00,Maxway CRT: Improving the Robustness of Model-X Inference,stat.ME,['stat.ME'],"[arxiv.Result.Author('Shuangning Li'), arxiv.Result.Author('Molei Liu')]","The model-X conditional randomization test (CRT) proposed by Cand\`es et al.
(2018) is known as a flexible and powerful testing procedure for the
conditional independence hypothesis: X is independent of Y conditional on Z.
Though having many attractive properties, the model-X CRT relies on the model-X
assumption that we have access to perfect knowledge of the distribution of X
conditional on Z. If there is a specification error in modeling the
distribution of X conditional on Z, this approach may lose its validity. This
problem is even more severe when the adjustment covariates Z are of high
dimensionality, in which situation precise modeling of X against Z can be hard.
In response to this, we propose the Maxway (Model and Adjust X With the
Assistance of Y) CRT, a more robust inference approach for conditional
independence when the conditional distribution of X is unknown and needs to be
estimated from the data. Besides the distribution of X | Z, the Maxway CRT also
learns the distribution of Y | Z, using it to calibrate the resampling
distribution of X to gain robustness to the error in modeling X. We show that
the type-I error inflation of the Maxway CRT can be controlled by the learning
error for the low-dimensional adjusting model plus the product of learning
errors for the distribution of X | Z and the distribution of Y | Z. This result
can be interpreted as an ""almost doubly robust"" property of the Maxway CRT.
Through extensive simulation studies, we demonstrate that the Maxway CRT
achieves significantly better type-I error control than existing model-X
inference approaches while having similar power. Finally, we apply our
methodology to the UK biobank dataset with the goal of studying the
relationship between the functional SNP of statins and the risk for type II
diabetes mellitus.",-0.0029169172,-0.1672626,-0.071679235,A
3240,"Mechanistic derivations of ERGM-generating
continuous time graph processes would thus seem to be an important area for further research.","That said, deriving an
ERGM-generating graph process from ﬁrst principles can provide a strong motivation for applying
it in cases where the associated assumptions are met.","4.3 Considerations for Practical Use

Although our focus here is on model deﬁnition and general properties, we may glean a few con-
siderations for use of these models in practice.",2022-03-14 09:19:49+00:00,"Continuous Time Graph Processes with Known ERGM Equilibria: Contextual Review, Extensions, and Synthesis",stat.ME,"['stat.ME', 'cs.DM', 'cs.SI', 'math.ST', 'stat.TH']",[arxiv.Result.Author('Carter T. Butts')],"Graph processes that unfold in continuous time are of obvious theoretical and
practical interest. Particularly useful are those whose long-term behavior
converges to a graph distribution of known form. Here, we review some of the
conditions for such convergence, and provide examples of novel and/or known
processes that do so. These include subfamilies of the well-known stochastic
actor oriented models, as well as continuum extensions of temporal and
separable temporal exponential family random graph models. We also comment on
some related threads in the broader work on network dynamics, which provide
additional context for the continuous time case.",-0.20495325,-0.13274577,0.11007015,A
3400,"If the intervention can have eﬀects for White men,
these results suggest that Black doctors can be more eﬀective than White doctors regardless
of the race of their patient, which is an interesting hypothesis for further study.",2010).,"27
Figure 3: Estimates of the TATE for various outcome variables and target populations.",2022-03-16 15:48:11+00:00,One-step weighting to generalize and transport treatment effect estimates to a target population,stat.ME,['stat.ME'],"[arxiv.Result.Author('Ambarish Chattopadhyay'), arxiv.Result.Author('Eric R. Cohn'), arxiv.Result.Author('Jose R. Zubizarreta')]","In randomized experiments and observational studies, weighting methods are
often used to generalize and transport treatment effect estimates to a target
population. Traditional methods construct the weights by separately modeling
the treatment assignment and study selection probabilities and then multiplying
functions (e.g., inverses) of their estimates. However, these estimated
multiplicative weights may not produce adequate covariate balance and can be
highly variable, resulting in biased and unstable estimators, especially when
there is limited covariate overlap across populations or treatment groups. To
address these limitations, we propose a general weighting approach that weights
each treatment group towards the target population in a single step. We present
a framework and provide a justification for this one-step approach in terms of
generic probability distributions. We show a formal connection between our
method and inverse probability and inverse odds weighting. By construction, the
proposed approach balances covariates and produces stable estimators. We show
that our estimator for the target average treatment effect is consistent,
asymptotically Normal, multiply robust, and semiparametrically efficient. We
demonstrate the performance of this approach using a simulation study and a
randomized case study on the effects of physician racial diversity on
preventive healthcare utilization among Black men in California.",0.5133407,0.0017106235,-0.14540058,C
3401,"If the intervention can have eﬀects for white men, these results suggest that Black doctors can be
more eﬀective than white doctors regardless of the race of their patient, which is an interesting
hypothesis for further study.",2010).,"Figure 3: Estimates of the target average treatment eﬀect for various outcome variables and
target populations.",2022-03-16 15:48:11+00:00,One-Step weighting to generalize and transport treatment effect estimates to a target population,stat.ME,['stat.ME'],"[arxiv.Result.Author('Ambarish Chattopadhyay'), arxiv.Result.Author('Eric R. Cohn'), arxiv.Result.Author('Jose R. Zubizarreta')]","In randomized experiments and observational studies, weighting methods are
often used to generalize and transport treatment effect estimates to target
populations. Traditional methods construct the weights by separately modeling
the treatment assignment and study selection probabilities and then multiplying
functions (e.g., inverses) of their estimates. In this work, we provide a
justification and an implementation for weighting in a single step. We show a
formal connection between this one-step method and inverse probability and
inverse odds weighting. We demonstrate that the resulting estimator for the
target average treatment effect is consistent, asymptotically Normal, multiply
robust, and semiparametrically efficient.",0.51155263,0.028151356,-0.15075155,C
3530,"To further study when these eﬀects occur, ﬁgure 5 shows the percentage of OOB
estimates over the entire survival curve.","Estimates
are based on 2000 simulation repetitions.","OOB errors occur much more frequently at the
left and right end of the survival curve.",2022-03-18 15:03:33+00:00,A Comparison of Different Methods to Adjust Survival Curves for Confounders,stat.ME,['stat.ME'],"[arxiv.Result.Author('Robin Denz'), arxiv.Result.Author('Renate Klaaßen-Mielke'), arxiv.Result.Author('Nina Timmesfeld')]","Treatment specific survival curves are an important tool to illustrate the
treatment effect in studies with time-to-event outcomes. In non-randomized
studies, unadjusted estimates can lead to biased depictions due to confounding.
Multiple methods to adjust survival curves for confounders exist. However, it
is currently unclear which method is the most appropriate in which situation.
Our goal is to compare these methods in different scenarios with a focus on
their bias and goodness-of-fit. We provide a short review of all methods and
illustrate their usage by contrasting the survival of smokers and non-smokers,
using data from the German Epidemiological Trial on Ankle Brachial Index.
Subsequently, we compare the methods using a Monte-Carlo simulation. We
consider scenarios in which correctly or incorrectly specified covariate sets
for describing the treatment assignment and the time-to-event outcome are used
with varying sample sizes. The bias and goodness-of-fit is determined by
summary statistics which take into account the entire survival curve. When used
properly, all methods showed no systematic bias in medium to large samples.
Cox-Regression based methods, however, showed systematic bias in small samples.
The goodness-of-fit varied greatly between different methods and scenarios.
Methods utilizing an outcome model were more efficient than other techniques,
while augmented estimators using an additional treatment assignment model were
unbiased when either model was correct with a goodness-of-fit comparable to
other methods. These ""doubly-robust"" methods have important advantages in every
considered scenario. Pseudo-Value based methods, coupled with isotonic
regression to correct for non-monotonicity, are viable alternatives to
traditional methods.",0.22689891,-0.04978983,-0.104903,C
3531,"To further study when these eﬀects occur, we plotted the percentage of OOB estimates
over the entire survival curve (see appendix).","Estimates are based on
2000 simulation repetitions.","OOB errors occur much more frequently
at the left and right end of the survival curve.",2022-03-18 15:03:33+00:00,A Comparison of Different Methods to Adjust Survival Curves for Confounders,stat.ME,['stat.ME'],"[arxiv.Result.Author('Robin Denz'), arxiv.Result.Author('Renate Klaaßen-Mielke'), arxiv.Result.Author('Nina Timmesfeld')]","Treatment specific survival curves are an important tool to illustrate the
treatment effect in studies with time-to-event outcomes. In non-randomized
studies, unadjusted estimates can lead to biased depictions due to confounding.
Multiple methods to adjust survival curves for confounders exist. However, it
is currently unclear which method is the most appropriate in which situation.
Our goal is to compare forms of Inverse Probability of Treatment Weighting, the
G-Formula, Propensity Score Matching, Empirical Likelihood Estimation and
augmented estimators as well as their pseudo-values based counterparts in
different scenarios with a focus on their bias and goodness-of-fit. We provide
a short review of all methods and illustrate their usage by contrasting the
survival of smokers and non-smokers, using data from the German Epidemiological
Trial on Ankle-Brachial-Index. Subsequently, we compare the methods using a
Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly
specified models for describing the treatment assignment and the time-to-event
outcome are used with varying sample sizes. The bias and goodness-of-fit is
determined by taking the entire survival curve into account. When used
properly, all methods showed no systematic bias in medium to large samples. Cox
regression based methods, however, showed systematic bias in small samples. The
goodness-of-fit varied greatly between different methods and scenarios. Methods
utilizing an outcome model were more efficient than other techniques, while
augmented estimators using an additional treatment assignment model were
unbiased when either model was correct with a goodness-of-fit comparable to
other methods. These doubly-robust methods have important advantages in every
considered scenario.",0.18727896,-0.043508094,-0.12656584,C
3549,"If we consider under the scope of energy-efﬁcient building design domain, further research is
worth conducting in gaining causal insights by involving more factors: external conditions (e.g., weather, geography),
internal inﬂuences (e.g., user behaviors), causality analysis in dynamic time-series data, or even perspectives from
design cognition domain, etc.","In our case study, we only investigated causal relationships by involving basic building characteristics and statistics
energy performance.","From another perspective, the potential risk of wrong conclusions by applying current
analysis tools is increasing: the development of simulations and the spread of digitalization are raising the data volume
dramatically with the trend of interdisciplinary requirements.",2022-03-14 22:00:21+00:00,Introducing causal inference in the energy-efficient building design process,stat.ME,"['stat.ME', 'cs.CE']","[arxiv.Result.Author('Xia Chen'), arxiv.Result.Author('Jimmy Abualdenien'), arxiv.Result.Author('Manav Mahan Singh'), arxiv.Result.Author('André Borrmann'), arxiv.Result.Author('Philipp Geyer')]","""What-if"" questions are intuitively generated during the design process.
Engineers and architects need to inherently conduct design decisions,
progressing from one phase to another. They either use empirical domain
experience, simulations, or data-driven methods to conduct consequential
feedback. We take an example in an interdisciplinary domain, energy-efficient
building design scenario, to argue that the current methods for decision
support have four limitations: 1. Less carefully inspected parametric
independence raise risks of biased results and spurious relationships. 2. The
integration gap between data-driven methods and knowledge-based approaches. 3.
Less explicit model interpretability for informed decision-making. 4. Ambiguous
boundaries for machine assistance during the design process. In this paper, we
first clarify the nature of dynamic personal experience and constant principal
knowledge in design. Sequentially, we introduce the causal inference into the
energy-efficient design domain by proposing a two-step process to reveal and
analyze the parametric dependencies within the design space by identifying the
design causal diagram with interventions. The causal diagram provides a nexus
for integrating domain knowledge with data-driven methods and allows for
interpretability and testability against domain experience. The extraction of
causal structures from the data is close to the common design reasoning
process. As an illustration, we applied the properties of the proposed
estimators through simulations. The paper is concluded with a feasibility study
that demonstrates the realization of the proposed framework.",0.10908833,0.33385313,0.2683084,B
3550,"If we consider the scope of the energy-efﬁcient building design, further research is worth
conducting in gaining causal insights by involving more factors: external conditions (e.g., weather, geography),
internal inﬂuences (e.g., user behaviors), causality analysis in dynamic time-series data, or even perspectives from
design cognition domain, etc.","In our case study, we only investigated causal relationships by involving basic building characteristics and statistics
energy performance.","From another perspective, the potential risk of wrong conclusions by applying current
analysis tools is increasing: the development of simulations and the spread of digitalization are raising the data volume
dramatically with the trend of interdisciplinary requirements.",2022-03-14 22:00:21+00:00,Introducing causal inference in the energy-efficient building design process,stat.ME,"['stat.ME', 'cs.CE']","[arxiv.Result.Author('Xia Chen'), arxiv.Result.Author('Jimmy Abualdenien'), arxiv.Result.Author('Manav Mahan Singh'), arxiv.Result.Author('André Borrmann'), arxiv.Result.Author('Philipp Geyer')]","""What-if"" questions are intuitively generated and commonly asked during the
design process. Engineers and architects need to inherently conduct design
decisions, progressing from one phase to another. They either use empirical
domain experience, simulations, or data-driven methods to provide consequential
feedback. We take an example from an interdisciplinary domain of
energy-efficient building design to argue that the current methods for decision
support have four limitations: 1. Less carefully inspected parametric
independence raises the risks of biased results and spurious relationships. 2.
The integration gap between data-driven methods and knowledge-based approaches.
3. Less explicit model interpretability for informed decision-making. 4.
Ambiguous boundaries for machine assistance during the design process. In this
study, we first clarify the nature of dynamic experience in individuals and
constant principal knowledge in design. Sequentially, we introduce the causal
inference into the energy-efficient design domain by proposing a four-step
process to reveal and analyze the parametric dependencies within the design
space by identifying the design causal diagram with interventions. The causal
diagram provides a nexus for integrating domain knowledge with data-driven
methods and allows interpretability and testability against the domain
experience. The extraction of causal structures from the data is close to the
nature design reasoning process. As an illustration, we applied the properties
of the proposed estimators through simulations. The paper concludes with a
feasibility study that demonstrates the realization of the proposed framework.",0.10852392,0.33143878,0.27712247,B
3551,"If we consider the scope of the energy-efﬁcient building design, further research is worth
conducting to gain causal insights by involving more factors: external conditions (e.g., weather, geography), life-
cycle assessment, internal inﬂuences (e.g., user behaviors), causality analysis in dynamic time-series data, or even
perspectives from design cognition domain, etc.","In our case study, we only investigated causal relationships by involving basic building characteristics and statistics
energy performance.","In the community, we see successful symbolic-based approaches such
as energy-emergy integration for building shape optimization [Yi et al., 2015] by unifying different objectives into
the energetic ﬂow form to support design decision-making.",2022-03-14 22:00:21+00:00,Introducing causal inference in the energy-efficient building design process,stat.ME,"['stat.ME', 'cs.CE']","[arxiv.Result.Author('Xia Chen'), arxiv.Result.Author('Jimmy Abualdenien'), arxiv.Result.Author('Manav Mahan Singh'), arxiv.Result.Author('André Borrmann'), arxiv.Result.Author('Philipp Geyer')]","""What-if"" questions are intuitively generated and commonly asked during the
design process. Engineers and architects need to inherently conduct design
decisions, progressing from one phase to another. They either use empirical
domain experience, simulations, or data-driven methods to acquire consequential
feedback. We take an example from an interdisciplinary domain of
energy-efficient building design to argue that the current methods for decision
support have limitations or deficiencies in four aspects: parametric
independency identification, gaps in integrating knowledge-based and
data-driven approaches, less explicit model interpretation, and ambiguous
decision support boundaries. In this study, we first clarify the nature of
dynamic experience in individuals and constant principal knowledge in design.
Subsequently, we introduce causal inference into the domain. A four-step
process is proposed to discover and analyze parametric dependencies in a
mathematically rigorous and computationally efficient manner by identifying the
causal diagram with interventions. The causal diagram provides a nexus for
integrating domain knowledge with data-driven methods, providing
interpretability and testability against the domain experience within the
design space. Extracting causal structures from the data is close to the nature
design reasoning process. As an illustration, we applied the properties of the
proposed estimators through simulations. The paper concludes with a feasibility
study demonstrating the proposed framework's realization.",0.066612944,0.21194793,0.34909296,B
3608,"Finally, we suggest some venues for further research.","Then we use absolute moments to
improve eﬃciency for GAL.",1.4.,2022-03-21 07:36:35+00:00,Modified Method of Moments for Generalized Laplace Distributions,stat.ME,"['stat.ME', 'math.PR', '62F10, 62F12, 60E07']","[arxiv.Result.Author('Kwame Boamah-Addo'), arxiv.Result.Author('Andrey Sarantsev')]","In this short note, we demonstrate the failure of the classic method of
moments for parameters estimation symmetric variance-gamma (generalized
Laplace) distributions. This disproves the claim found in the literature that
method of moments works well for generalized asymmetric Laplace distributions
and related models. We use both theoretical analysis (multivariate delta
method) and simulations to make our case. Finally, we modify the method of
moments by taking absolute moments to improve efficiency.",0.059490707,-0.111750826,0.10712091,C
3609,"Finally, we suggest some venues for further research.","Then we use absolute moments to
improve eﬃciency for GAL.",1.4.,2022-03-21 07:36:35+00:00,Modified Method of Moments for Generalized Laplace Distributions,stat.ME,"['stat.ME', 'math.PR', '62F10, 62F12, 60E07']","[arxiv.Result.Author('Kwame Boamah-Addo'), arxiv.Result.Author('Robert Gaunt'), arxiv.Result.Author('Andrey Sarantsev')]","In this short note, we demonstrate the failure of the classic method of
moments for parameter estimation of symmetric variance-gamma (generalized
Laplace) distributions. This disproves the claim found in the literature that
method of moments works well for generalized asymmetric Laplace distributions
and related models. We use both theoretical analysis (multivariate delta
method) and simulations to make our case. Finally, we modify the method of
moments by taking absolute moments to improve efficiency.",0.059490707,-0.111750826,0.10712091,C
3757,"A further study could assess
the possible beneﬁts of such ﬁne-grained shrinkage methods.","This would be an interesting extension for future
work on Bayesian regularization of relational event models.","In addition, further research can explore the use of shrinkage priors to model tem-
poral changes of network parameters over time.",2022-03-23 15:10:18+00:00,Separating the Wheat from the Chaff: Bayesian Regularization in Dynamic Social Networks,stat.ME,['stat.ME'],"[arxiv.Result.Author('Diana Karimova'), arxiv.Result.Author('Joris Mulder'), arxiv.Result.Author('Roger Th. A. J. Leenders')]","In recent years there has been an increasing interest in the use of
relational event models for dynamic social network analysis. The basis of these
models is the concept of an ""event"", defined as a triplet of time, sender, and
receiver of some social interaction. The key question that relational event
models aim to answer is what drives social interactions among actors.
Researchers often consider a very large number of predictors in their studies
(including exogenous variables, endogenous network effects, and various
interaction effects). The problem is however that employing an excessive number
of effects may lead to model overfitting and inflated Type-I error rates.
Consequently, the fitted model can easily become overly complex and the implied
social interaction behavior becomes difficult to interpret. A potential
solution to this problem is to apply Bayesian regularization using shrinkage
priors. In this paper, we propose Bayesian regularization methods for
relational event models using four different priors: a flat prior model with no
shrinkage effect, a ridge estimator with a normal prior, a Bayesian lasso with
a Laplace prior, and a horseshoe estimator with a numerically constructed prior
that has an asymptote at zero. We develop and use these models for both an
actor-oriented relational event model and a dyad-oriented relational event
model. We show how to apply Bayesian regularization methods for these models
and provide insights about which method works best and guidelines how to apply
them in practice. Our results show that shrinkage priors can reduce Type-I
errors while keeping reasonably high predictive performance and yielding
parsimonious models to explain social network behavior.",-0.20838451,0.00029157102,0.03695568,A
3758,"In addition, further research can explore the use of shrinkage priors to model tem-
poral changes of network parameters over time.","A further study could assess
the possible beneﬁts of such ﬁne-grained shrinkage methods.","The idea would be to place a shrinkage
prior for the diﬀerence of a parameter over time, similar as in graphical models for
variables (Shaﬁee Kamalabad and Grzegorczyk, 2020).",2022-03-23 15:10:18+00:00,Separating the Wheat from the Chaff: Bayesian Regularization in Dynamic Social Networks,stat.ME,['stat.ME'],"[arxiv.Result.Author('Diana Karimova'), arxiv.Result.Author('Joris Mulder'), arxiv.Result.Author('Roger Th. A. J. Leenders')]","In recent years there has been an increasing interest in the use of
relational event models for dynamic social network analysis. The basis of these
models is the concept of an ""event"", defined as a triplet of time, sender, and
receiver of some social interaction. The key question that relational event
models aim to answer is what drives social interactions among actors.
Researchers often consider a very large number of predictors in their studies
(including exogenous variables, endogenous network effects, and various
interaction effects). The problem is however that employing an excessive number
of effects may lead to model overfitting and inflated Type-I error rates.
Consequently, the fitted model can easily become overly complex and the implied
social interaction behavior becomes difficult to interpret. A potential
solution to this problem is to apply Bayesian regularization using shrinkage
priors. In this paper, we propose Bayesian regularization methods for
relational event models using four different priors: a flat prior model with no
shrinkage effect, a ridge estimator with a normal prior, a Bayesian lasso with
a Laplace prior, and a horseshoe estimator with a numerically constructed prior
that has an asymptote at zero. We develop and use these models for both an
actor-oriented relational event model and a dyad-oriented relational event
model. We show how to apply Bayesian regularization methods for these models
and provide insights about which method works best and guidelines how to apply
them in practice. Our results show that shrinkage priors can reduce Type-I
errors while keeping reasonably high predictive performance and yielding
parsimonious models to explain social network behavior.",-0.21474238,0.008154619,0.041890096,A
3788,"In the multigroup setting, properties such as the connectedness
or convexity of such a conﬁdence region found by inverting the FAB test warrant
further study.","Hoﬀ and Yu [11] examine related conﬁdence
intervals for the elements of β in a regression model for a single group using shrinkage
prior distributions.","Another aspect of the multigroup FAB test that warrants further study are the
multiple testing properties of this test.",2022-03-23 21:30:49+00:00,Tests of Linear Hypotheses using Indirect Information,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andrew McCormack'), arxiv.Result.Author('Peter Hoff')]","In multigroup data settings with small within-group sample sizes, standard
$F$-tests of group-specific linear hypotheses can have low power, particularly
if the within-group sample sizes are not large relative to the number of
explanatory variables. To remedy this situation, in this article we derive
alternative test statistics based on information-sharing across groups. Each
group-specific test has potentially much larger power than the standard
$F$-test, while still exactly maintaining a target type I error rate if the
hypothesis for the group is true. The proposed test for a given group uses a
statistic that has optimal marginal power under a prior distribution derived
from the data of the other groups. This statistic approaches the usual
$F$-statistic as the prior distribution becomes more diffuse, but approaches a
limiting ""cone"" test statistic as the prior distribution becomes extremely
concentrated. We compare the power and $p$-values of the cone test to that of
the $F$-test in some high-dimensional asymptotic scenarios. An analysis of
educational outcome data is provided, demonstrating empirically that the
proposed test is more powerful than the $F$-test.",0.053587973,-0.14804465,-0.056633566,A
3789,"Another aspect of the multigroup FAB test that warrants further study are the
multiple testing properties of this test.","In the multigroup setting, properties such as the connectedness
or convexity of such a conﬁdence region found by inverting the FAB test warrant
further study.","The multigroup FAB test controls the type I
error rate for each group and thus controls the per-comparison error rate, but it does
not control the family-wise error rate [6].",2022-03-23 21:30:49+00:00,Tests of Linear Hypotheses using Indirect Information,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andrew McCormack'), arxiv.Result.Author('Peter Hoff')]","In multigroup data settings with small within-group sample sizes, standard
$F$-tests of group-specific linear hypotheses can have low power, particularly
if the within-group sample sizes are not large relative to the number of
explanatory variables. To remedy this situation, in this article we derive
alternative test statistics based on information-sharing across groups. Each
group-specific test has potentially much larger power than the standard
$F$-test, while still exactly maintaining a target type I error rate if the
hypothesis for the group is true. The proposed test for a given group uses a
statistic that has optimal marginal power under a prior distribution derived
from the data of the other groups. This statistic approaches the usual
$F$-statistic as the prior distribution becomes more diffuse, but approaches a
limiting ""cone"" test statistic as the prior distribution becomes extremely
concentrated. We compare the power and $p$-values of the cone test to that of
the $F$-test in some high-dimensional asymptotic scenarios. An analysis of
educational outcome data is provided, demonstrating empirically that the
proposed test is more powerful than the $F$-test.",0.2135295,-0.16036898,0.1492395,C
3792,"How to make further improvements to our estimator when
applied to data with low sampling rates is a topic for further research.","We demonstrate that the methods
work well when sampling proportion is greater than 0.4; but even in a setting with lower
sampling rates, the adjustment greatly improve performance of estimators compared to
those that ignore missing data.","Our illustrative example made use of data from the HIV prevention study in Botswana—the
BCPP.",2022-03-24 00:17:55+00:00,Estimating Viral Genetic Linkage Rates in the Presence of Missing Data,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Tyler Vu'), arxiv.Result.Author('Tuo Lin'), arxiv.Result.Author('Jingjing Zou'), arxiv.Result.Author('Vladimir Novitsky'), arxiv.Result.Author('Xin Tu'), arxiv.Result.Author('Victor De Gruttola')]","Although the interest in the the use of social and information networks has
grown, most inferences on networks assume the data collected represents the
complete. However, when ignoring missing data, even when missing completely at
random, this results in bias for estimators regarding inference network related
parameters. In this paper, we focus on constructing estimators for the
probability that a randomly selected node has node has at least one edge under
the assumption that nodes are missing completely at random along with their
corresponding edges. In addition, issues also arise in obtaining asymptotic
properties for such estimators, because linkage indicators across nodes are
correlated preventing the direct application of the Central Limit Theorem and
Law of Large Numbers. Using a subsampling approach, we present an improved
estimator for our parameter of interest that accommodates for missing data.
Utilizing the theory U-statistics, we derive consistency and asymptotic
normality of the proposed estimator. This approach decreases the bias in
estimating our parameter of interest. We illustrate our approach using the HIV
viral strains from a large cluster-randomized trial of a combination HIV
prevention intervention -- the Botswana Combination Prevention Project (BCPP).",0.2080411,0.07796044,-0.33100888,C
3811,"However, the inner product equation is not derivable from general principles or previous definition
and requires further study before it will lead us to the underlying rationale of the pair of spatial
autocorrelation equations.","The outer product equation can be derived from the
formula of Moran’s index based on a standardized variable and a normalized weight matrix.","As a matter of fact, the inner product equation is obtained by considering
one of the necessary conditions for defining Moran’s index (Chen, 2013).",2022-03-23 13:25:55+00:00,Derivation of an Inverse Spatial Autoregressive Model for Estimating Moran's Index,stat.ME,"['stat.ME', 'physics.data-an']",[arxiv.Result.Author('Yanguang Chen')],"Spatial autocorrelation measures such as Moran's index can be expressed as a
pair of equations based on a standardized size variable and a globally
normalized weight matrix. One is based on inner product, and the other is based
on outer product of the size variable. The inner product equation is actually a
spatial autocorrelation model. However, the theoretical basis of the inner
product equation for Moran's index is not clear. This paper is devoted to
revealing the antecedents and consequences of the inner product equation of
Moran's index. The method is mathematical derivation and empirical analysis.
The main results are as follows. First, the inner product equation is derived
from a simple spatial autoregressive model, and thus the relation between
Moran's index and spatial autoregressive coefficient is clarified. Second, the
least squares regression is proved to be one of effective approaches for
estimating Moran's index and spatial autoregressive coefficient. Third, the
value ranges of the Moran's index and spatial autoregressive coefficient can be
identified from three angles of view. A conclusion can be drawn that spatial
autocorrelation model is actually an inverse spatial autoregressive model, and
Moran's index, spatial autoregressive models, and canonical Moran's scatterplot
can be integrated into the same framework through inner product and outer
product equations. This work may be helpful for understanding the connections
and differences between spatial autocorrelation measurements and spatial
autoregressive modeling.",-0.19304854,-0.0448015,0.24385484,A
3812,"However, the inner product equation is not derivable from general principles or previous definition
and requires further study before it will lead us to the underlying rationale of the pair of spatial
autocorrelation equations.","The outer product equation can be derived from the
formula of Moran’s index based on a standardized variable and a globally normalized weight matrix.","As a matter of fact, the inner product equation is obtained by considering
one of the necessary conditions for defining Moran’s index (Chen, 2013).",2022-03-23 13:25:55+00:00,Derivation of an Inverse Spatial Autoregressive Model for Estimating Moran's Index,stat.ME,"['stat.ME', 'physics.data-an']",[arxiv.Result.Author('Yanguang Chen')],"Spatial autocorrelation measures such as Moran's index can be expressed as a
pair of equations based on a standardized size variable and a globally
normalized weight matrix. One is based on inner product, and the other is based
on outer product of the size variable. The inner product equation is actually a
spatial autocorrelation model. However, the theoretical basis of the inner
product equation for Moran's index is not clear. This paper is devoted to
revealing the antecedents and consequences of the inner product equation of
Moran's index. The method is mathematical derivation and empirical analysis.
The main results are as follows. First, the inner product equation is derived
from a simple spatial autoregressive model, and thus the relation between
Moran's index and spatial autoregressive coefficient is clarified. Second, the
least squares regression is proved to be one of effective approaches for
estimating spatial autoregressive coefficient. Third, the value ranges of the
spatial autoregressive coefficient can be identified from three angles of view.
A conclusion can be drawn that a spatial autocorrelation model is actually an
inverse spatial autoregressive model, and Moran's index and spatial
autoregressive models can be integrated into the same framework through inner
product and outer product equations. This work may be helpful for understanding
the connections and differences between spatial autocorrelation measurements
and spatial autoregressive modeling.",-0.19571213,-0.043176815,0.2351731,A
3847,"A lot of work needs to be carried out as a further research for the proposed
model.","This paper proposes ROC curve analysis in the direction of resilience family of distributions
for the ﬁrst time.","Development of the Bayesian estimation methodology for the resilience parameter (assuming
θ ∈ (1, ∞)) could be considered.",2022-03-24 14:38:42+00:00,Resilience family of receiver operating characteristic curves,stat.ME,"['stat.ME', 'math.ST', 'stat.TH', '92B15, 62P10']",[arxiv.Result.Author('Ruhul Ali Khan')],"A new semiparametric model of the ROC curve based on the resilience family or
proportional reversed hazard family is proposed which is an alternative to the
existing models. The resulting ROC curve and its summary indices (such as area
under the curve (AUC) and Youden index) have simple analytic forms. The partial
likelihood method is applied to estimate the ROC curve. Moreover, the
estimation methodologies of the resilience family of the ROC curve have been
developed based on AUC estimators exploiting Mann-Whitney statistics and the
Rojo approach. A simulation study has been carried out to assess the
performance of all considered estimators. Real data from the American National
Health and Nutrition Examination Survey (NHANES) has been analysed in detail
based on the proposed model and the usual binormal model prevalent in the
literature. Real data in the context of brain injury-related biomarkers is also
analysed in order to compare our model with the Lehmann family of the ROC
curves. Finally, we show that the proposed model may be applicable in the
misspecification scenario through a Ducheme muscular dystrophy data.",-0.1649334,0.064595416,-0.036334667,A
4112,"However, this study can be used to inform further research in this area.","By doing
so, we may have overlooked a range of scenarios under which the analysis strategies perform
poorly.","In conclusion, this study evaluated a range of strategies for selecting auxiliary variables
for MI models, with the aim of providing advice for researchers faced with this problem.",2022-03-30 23:51:01+00:00,A comparison of strategies for selecting auxiliary variables for multiple imputation,stat.ME,['stat.ME'],"[arxiv.Result.Author('Rheanna M. Mainzer'), arxiv.Result.Author('Cattram D. Nguyen'), arxiv.Result.Author('John B. Carlin'), arxiv.Result.Author('Margarita Moreno-Betancur'), arxiv.Result.Author('Ian R. White'), arxiv.Result.Author('Katherine J. Lee')]","Multiple imputation (MI) is a popular method for handling missing data.
Auxiliary variables can be added to the imputation model(s) to improve MI
estimates. However, the choice of which auxiliary variables to include in the
imputation model is not always straightforward. Including too few may lead to
important information being discarded, but including too many can cause
problems with convergence of the estimation procedures for imputation models.
Several data-driven auxiliary variable selection strategies have been proposed.
This paper uses a simulation study and a case study to provide a comprehensive
comparison of the performance of eight auxiliary variable selection strategies,
with the aim of providing practical advice to users of MI. A complete case
analysis and an MI analysis with all auxiliary variables included in the
imputation model (the full model) were also performed for comparison. Our
simulation study results suggest that the full model outperforms all auxiliary
variable selection strategies, providing further support for adopting an
inclusive auxiliary variable strategy where possible. Auxiliary variable
selection using the Least Absolute Selection and Shrinkage Operator (LASSO) was
the best performing auxiliary variable selection strategy overall and is a
promising alternative when the full model fails. All MI analysis strategies
that we were able to apply to the case study led to similar estimates.",0.063701406,0.18971774,-0.032775983,C
4193,"Results for the maximum likelihood estimator in the regression setting still require
further research.","), we have only given partial results when detection heterogeneity is considered to be
a mixture model or when the conditional likelihood approach is used in a regression frame-
work.","Although we showed that conditional likelihood and maximum likelihood are not asymp-
                                                      21
totically equivalent in the regression case, most of our empirical results showed similarities
in terms of efﬁciency.",2022-03-31 22:40:21+00:00,On site occupancy models with heterogeneity,stat.ME,['stat.ME'],"[arxiv.Result.Author('Wen-Han Hwang'), arxiv.Result.Author('Jakub Stoklosa'), arxiv.Result.Author('Lu-Fang Chen')]","Site occupancy models are routinely used to estimate the probability of
species presence from either abundance or presence-absence data collected
across sites with repeated sampling occasions. In the last two decades, a broad
class of occupancy models has been developed, but little attention has been
given to examining the effects of heterogeneity in parameter estimation. This
study focuses on occupancy models where heterogeneity is present in detection
intensity and the presence probability. We show that the presence probability
will be underestimated if detection heterogeneity is ignored. On the other
hand, the behavior is different if heterogeneity in the presence probability is
ignored; notably, an estimate of the average presence probability may be
unbiased or over- or under-estimated depending on the relationship between
detection and presence probabilities. In addition, when heterogeneity in the
detection intensity is related to covariates, we propose a conditional
likelihood approach to estimate the detection intensity parameters. This
alternative method shares an optimal estimating function property and it
ensures robustness against model specification on the presence probability. We
then propose a consistent estimator for the average presence probability,
provided that the detection intensity component model is correctly specified.
We illustrate the bias effects and estimator performance in simulation studies
and real data analysis.",-0.07604931,0.025199085,-0.23363936,A
4194,"Results for the maximum likelihood estimator in the regression setting still require
further research.","), we have only given partial results when detection heterogeneity is considered to be
a mixture model or when the conditional likelihood approach is used in a regression frame-
work.","Although we showed that conditional likelihood and maximum likelihood are not asymp-
                                                      21
totically equivalent in the regression case, most of our empirical results showed similarities
in terms of efﬁciency.",2022-03-31 22:40:21+00:00,On site occupancy models with heterogeneity,stat.ME,['stat.ME'],"[arxiv.Result.Author('Wen-Han Hwang'), arxiv.Result.Author('Jakub Stoklosa'), arxiv.Result.Author('Lu-Fang Chen')]","Site occupancy models are routinely used to estimate the probability of
species presence from either abundance or presence-absence data collected
across sites with repeated sampling occasions. In the last two decades, a broad
class of occupancy models has been developed, but little attention has been
given to examining the effects of heterogeneity in parameter estimation. This
study focuses on occupancy models where heterogeneity is present in detection
intensity and the presence probability. We show that the presence probability
will be underestimated if detection heterogeneity is ignored. On the other
hand, the behavior is different if heterogeneity in the presence probability is
ignored; notably, an estimate of the average presence probability may be
unbiased or over- or under-estimated depending on the relationship between
detection and presence probabilities. In addition, when heterogeneity in the
detection intensity is related to covariates, we propose a conditional
likelihood approach to estimate the detection intensity parameters. This
alternative method shares an optimal estimating function property and it
ensures robustness against model specification on the presence probability. We
then propose a consistent estimator for the average presence probability,
provided that the detection intensity component model is correctly specified.
We illustrate the bias effects and estimator performance in simulation studies
and real data analysis.",-0.07604931,0.025199085,-0.23363936,A
4354,"Theoretical analysis and discussion of

this data-driven selection procedure need further study.","Although we don’t

know the exact distribution information of the error term under null hypothesis, numerical
experiments shows that the difference of sˆ∗1 and sˆ1 under the null hypothesis can be ignored
since ηj shares the same mean and variance with η.","In practice, we also suggest setting a support order to deal with small variance models.",2022-04-04 20:33:57+00:00,An adaptive model checking test for functional linear model,stat.ME,['stat.ME'],"[arxiv.Result.Author('Enze Shi'), arxiv.Result.Author('Yi Liu'), arxiv.Result.Author('Ke Sun'), arxiv.Result.Author('Lingzhu Li'), arxiv.Result.Author('Linglong Kong')]","Numerous studies have been devoted to the estimation and inference problems
for functional linear models (FLM). However, few works focus on model checking
problem that ensures the reliability of results. Limited tests in this area do
not have tractable null distributions or asymptotic analysis under
alternatives. Also, the functional predictor is usually assumed to be fully
observed, which is impractical. To address these problems, we propose an
adaptive model checking test for FLM. It combines regular moment-based and
conditional moment-based tests, and achieves model adaptivity via the dimension
of a residual-based subspace. The advantages of our test are manifold. First,
it has a tractable chi-squared null distribution and higher powers under the
alternatives than its components. Second, asymptotic properties under different
underlying models are developed, including the unvisited local alternatives.
Third, the test statistic is constructed upon finite grid points, which
incorporates the discrete nature of collected data. We develop the desirable
relationship between sample size and number of grid points to maintain the
asymptotic properties. Besides, we provide a data-driven approach to estimate
the dimension leading to model adaptivity, which is promising in sufficient
dimension reduction. We conduct comprehensive numerical experiments to
demonstrate the advantages the test inherits from its two simple components.",0.014626242,0.03668165,-0.17690895,A
4355,"Theoretical analysis
and discussion of this data-driven selection procedure need further study.","Although we don’t know the exact distribution information of the error term
under null hypothesis, numerical experiments shows that the diﬀerence of sˆ∗1 and sˆ1 under the null
hypothesis can be ignored since ηj shares the same mean and variance with η.","In practice, we also suggest setting a support order to deal with small variance models.",2022-04-04 20:33:57+00:00,An adaptive model checking test for functional linear model,stat.ME,['stat.ME'],"[arxiv.Result.Author('Enze Shi'), arxiv.Result.Author('Yi Liu'), arxiv.Result.Author('Ke Sun'), arxiv.Result.Author('Lingzhu Li'), arxiv.Result.Author('Linglong Kong')]","Numerous studies have been devoted to the estimation and inference problems
for functional linear models (FLM). However, few works focus on model checking
problem that ensures the reliability of results. Limited tests in this area do
not have tractable null distributions or asymptotic analysis under
alternatives. Also, the functional predictor is usually assumed to be fully
observed, which is impractical. To address these problems, we propose an
adaptive model checking test for FLM. It combines regular moment-based and
conditional moment-based tests, and achieves model adaptivity via the dimension
of a residual-based subspace. The advantages of our test are manifold. First,
it has a tractable chi-squared null distribution and higher powers under the
alternatives than its components. Second, asymptotic properties under different
underlying models are developed, including the unvisited local alternatives.
Third, the test statistic is constructed upon finite grid points, which
incorporates the discrete nature of collected data. We develop the desirable
relationship between sample size and number of grid points to maintain the
asymptotic properties. Besides, we provide a data-driven approach to estimate
the dimension leading to model adaptivity, which is promising in sufficient
dimension reduction. We conduct comprehensive numerical experiments to
demonstrate the advantages the test inherits from its two simple components.",0.0018403856,0.02865535,-0.1894553,A
4394,"n     m
                                                      y=0
ξ ≡ β n−1  y2 pr(Y = y|xi; β, c) −

i=1 y=0

Because there is no generally accepted unique way of deﬁning quantiles for discrete data,
we do not further study the quantile eﬀect estimation in this case.",", m to estimate the marginal eﬀect ξ by

                                                     m 2
                                                          y pr(Y = y|xi; β, c)  .","3 Theoretical Properties

We now establish the theoretical properties of our proposed estimators for both the marginal
eﬀect and the quantile eﬀect in (4).",2022-04-05 12:57:34+00:00,Semiparametric Approach to Estimation of Marginal and Quantile Effects,stat.ME,['stat.ME'],"[arxiv.Result.Author('Seong-ho Lee'), arxiv.Result.Author('Yanyuan Ma'), arxiv.Result.Author('Elvezio Ronchetti')]","We consider a semiparametric generalized linear model and study estimation of
both marginal and quantile effects in this model. We propose an approximate
maximum likelihood estimator, and rigorously establish the consistency, the
asymptotic normality, and the semiparametric efficiency of our method in both
the marginal effect and the quantile effect estimation. Simulation studies are
conducted to illustrate the finite sample performance, and we apply the new
tool to analyze a Swiss non-labor income data and discover a new interesting
predictor.",-0.112699024,-0.11367616,-0.41535515,A
4431,"Without considering the robustness, we ﬁrst compare the performance of three candidate mod-
els – stand-alone lognormal, composite LNPaI, and composite LNGPD (mentioned in Section 3.3),
and determine the benchmark for further study of adaptive robust estimators.","Their transformations, and number of loss observations within
each range of contract are also summarized in Table 6.1.","In this stage, all
the parameters are estimated via maximum likelihood estimation and the corresponding statistics
and performance indicators (such as Negative log-likelihood and Akakie Information Criterion) are
illustrated in Table 6.2.",2022-04-05 20:26:19+00:00,Method of Winsorized Moments for Robust Fitting of Truncated and Censored Lognormal Distributions,stat.ME,['stat.ME'],"[arxiv.Result.Author('Chudamani Poudyal'), arxiv.Result.Author('Qian Zhao'), arxiv.Result.Author('Vytaras Brazauskas')]","When constructing parametric models to predict the cost of future claims,
several important details have to be taken into account: (i) models should be
designed to accommodate deductibles, policy limits, and coinsurance factors,
(ii) parameters should be estimated robustly to control the influence of
outliers on model predictions, and (iii) all point predictions should be
augmented with estimates of their uncertainty. The methodology proposed in this
paper provides a framework for addressing all these aspects simultaneously.
Using payment-per-payment and payment-per-loss variables, we construct the
adaptive version of method of winsorized moments (MWM) estimators for the
parameters of truncated and censored lognormal distribution. Further, the
asymptotic distributional properties of this approach are derived and compared
with those of the maximum likelihood estimator (MLE) and method of trimmed
moments (MTM) estimators. The latter being a primary competitor to MWM.
Moreover, the theoretical results are validated with extensive simulation
studies and risk measure sensitivity analysis. Finally, practical performance
of these methods is illustrated using the well-studied data set of 1500 U.S.
indemnity losses. With this real data set, it is also demonstrated that the
composite models do not provide much improvement in the quality of predictive
models compared to a stand-alone fitted distribution specially for truncated
and censored sample data.",-0.24833189,-0.013174454,-0.16379133,A
4660,"This work opens up a new direction of further research on the robust and efﬁcient parametric statistical inference for
more general stochastic processes.","The claimed robustness of the MDPDEs under the IIPs
are also justiﬁed theoretically through the inﬂuence function analysis in Appendix A.","An immediate future work would be to extend the MDPDEs for general stochastic
process instead of restricting to speciﬁc dependence structure such as in Independent Increment Process or Markov
Process.",2022-04-11 11:56:26+00:00,Robust and Efficient Parameter Estimation for Discretely Observed Stochastic Processes,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Rohan Hore'), arxiv.Result.Author('Abhik Ghosh')]","In various practical situations, we encounter data from stochastic processes
which can be efficiently modelled by an appropriate parametric model for
subsequent statistical analyses. Unfortunately, the most common estimation and
inference methods based on the maximum likelihood (ML) principle are
susceptible to minor deviations from assumed model or data contamination due to
their well known lack of robustness. Since the alternative non-parametric
procedures often lose significant efficiency, in this paper, we develop a
robust parameter estimation procedure for discretely observed data from a
parametric stochastic process model which exploits the nice properties of the
popular density power divergence measure in the framework of minimum distance
inference. In particular, here we define the minimum density power divergence
estimators (MDPDE) for the independent increment and the Markov processes. We
establish the asymptotic consistency and distributional results for the
proposed MDPDEs in these dependent stochastic process set-ups and illustrate
their benefits over the usual ML estimator for common examples like Poisson
process, drifted Brownian motion and auto-regressive models.",-0.3026995,0.008383462,-0.19226739,A
4777,"These may guide
                                                  clinicians as to which patient characteristics are important to consider when making treatment decisions,
                                                  and help researchers stratify patient populations for further study.","Here we
                                                  present new nonparametric treatment eﬀect variable importance measures (TE-VIMs).","TE-VIMs extend recent regression-
                                                  VIMs, viewed as nonparametric analogues to ANOVA statistics.",2022-04-12 18:34:07+00:00,Variable importance measures for heterogeneous causal effects,stat.ME,['stat.ME'],"[arxiv.Result.Author('Oliver Hines'), arxiv.Result.Author('Karla Diaz-Ordaz'), arxiv.Result.Author('Stijn Vansteelandt')]","The conditional average treatment effect (CATE) of a binary exposure on an
outcome is often studied on the basis that personalised treatment decisions
lead to better clinical outcomes. The CATE, however, may be a complicated
function of several covariates representing patient characteristics. As such,
clinicians rely on researchers to summarise insights related to treatment
effect heterogeneity. Clinical research usually focuses on the ATE, which
averages the CATE across all patient characteristics, or considers the
treatment effect in patient strata, though strata are rarely chosen
systematically. Here we present new nonparametric treatment effect variable
importance measures (TE-VIMs). These may guide clinicians as to which patient
characteristics are important to consider when making treatment decisions, and
help researchers stratify patient populations for further study. TE-VIMs extend
recent regression-VIMs, viewed as nonparametric analogues to ANOVA statistics.
Moreover, TE-VIMs are not tied to a particular model, thus are amenable to
data-adaptive (machine learning) estimation of the CATE, itself an active area
of research. Estimators for the proposed statistics are derived from their
efficient influence curves and these are illustrated through a simulation study
and an applied example.",0.34066024,0.13571283,-0.043955162,C
5074,"Due to the diversity between diﬀerent
forecasting methods and the heterogeneity across diﬀerent datasets, the performance of the
forecast reconciliation framework may not be robust, therefore further research, particularly on

                                                             21
impact of the covariance matrix estimation is needed.","It is worth to explore whether theoretical properties of
times series can be used to automate this process.","Acknowledgments

    Yanfei Kang is supported by the National Natural Science Foundation of China (No.",2022-04-20 05:23:31+00:00,Optimal reconciliation with immutable forecasts,stat.ME,"['stat.ME', 'econ.EM', 'stat.ML']","[arxiv.Result.Author('Bohan Zhang'), arxiv.Result.Author('Yanfei Kang'), arxiv.Result.Author('Anastasios Panagiotelis'), arxiv.Result.Author('Feng Li')]","The practical importance of coherent forecasts in hierarchical forecasting
has inspired many studies on forecast reconciliation. Under this approach,
so-called base forecasts are produced for every series in the hierarchy and are
subsequently adjusted to be coherent in a second reconciliation step.
Reconciliation methods have been shown to improve forecast accuracy, but will,
in general, adjust the base forecast of every series. However, in an
operational context, it is sometimes necessary or beneficial to keep forecasts
of some variables unchanged after forecast reconciliation. In this paper, we
formulate reconciliation methodology that keeps forecasts of a pre-specified
subset of variables unchanged or ""immutable"". In contrast to existing
approaches, these immutable forecasts need not all come from the same level of
a hierarchy, and our method can also be applied to grouped hierarchies. We
prove that our approach preserves unbiasedness in base forecasts. Our method
can also account for correlations between base forecasting errors and ensure
non-negativity of forecasts. We also perform empirical experiments, including
an application to sales of a large scale online retailer, to assess the impacts
of our proposed methodology.",-0.30745873,0.34576523,0.20247078,B
5115,"Also relating to algorithms, further research on algorithmic eﬃciency of θ-augmented Bayesian inference
in large samples is helpful to determine the applicability of the θ-augmented model to large datasets.",Further research is needed to tackle this problem either from the algorithmic or theoretical front.,"Finally, by requiring that the proposal model of a θ-augmented model be necessarily dominated, we are
limited in the class of nonparametric models that can be used to describe the observable random variable.",2022-04-21 03:36:40+00:00,The $θ$-augmented model for Bayesian semiparametric inference on functional parameters,stat.ME,['stat.ME'],"[arxiv.Result.Author('Vivian Y. Meng'), arxiv.Result.Author('David A. Stephens')]","Semiparametric Bayesian inference has so far relied on models for the
observable that partition into two parts, one being parametric and the other
nonparametric, with the target parameter being dependent on the parametric
component. While a partitioned structure makes specification of the marginal
prior on the target parameter simple to perform, it often arises from
conditional modelling which is subject to misspecification and ultimately a
lack of consistency. We introduce a new type of semiparametric model to allow
easy prior specification for a parameter that is defined as a functional of the
distribution for the observable. Our semiparametric model is obtained as an
extension of nonparametric models that are consistent under very general
conditions. This type of Bayesian semiparametric model can be used to obtain
Bayesian versions of Frequentist estimators that are defined as functionals of
the empirical distribution. This gives us new opportunities to conduct Bayesian
analysis in problems where Frequentist estimators exist but not well-accepted
likelihoods.",-0.22410458,0.041381072,-0.22614327,A
5116,"Also relating to algorithms, further research
on algorithmic eﬃciency of θ-augmented Bayesian inference in large samples is helpful to determine the
applicability of the θ-augmented model to large datasets.","Further research is needed to tackle
this problem either from the algorithmic or theoretical front.","Finally, choosing targets of inference that are functionals of the distribution for the observable provides
a simple way to avoid model misspeciﬁcation and counters the perception that the Bayesian paradigm is

                                                                     12
more limiting than the Frequentist simply because it always requires a likelihood.",2022-04-21 03:36:40+00:00,Targeting functional parameters with semiparametric Bayesian inference,stat.ME,['stat.ME'],"[arxiv.Result.Author('Vivian Y. Meng'), arxiv.Result.Author('David A. Stephens')]","Typical Bayesian inference requires parameter identification via likelihood
parameterization, which has invited criticism for being less flexible than the
Frequentist framework and subject to misspecification. Though misspecification
may be avoided by functional parameter inference under a nonparametric model
space, there does not exist a flexible Bayesian semiparametric model that would
allow full control over the marginal prior over any general functional
parameter. We present the technique of $\theta$-augmentation which helps us
manipulate nonparametric models into semiparametric ones that directly target
any functional parameter. The method allows Bayesian probabilistic statements
to be drawn for any estimator that is defined as a functional of the empirical
distribution without requiring a likelihood function, thus providing a path to
Bayesian analysis in problems like causal inference and censoring where there
do not exist well-accepted likelihood functions.",-0.17654048,0.08865009,-0.124778315,A
5144,"Finally, the last section contains conclusions and
guidelines to be followed for further research.","Then, Section 3 deals with the
comparison between two coeﬃcients of variation and provides, on a case-by-case basis,
the tools needed for that particular scenario, which are subsequently used in the related
examples taken from the literature.","As far as we know, some of the test analysed have not been investigated before in the
literature, see Section 3.3 concerning the Skew-Normal case and 3.4 the Negative Binomial
one.",2022-04-21 14:56:08+00:00,Testing the equality of two coefficients of variation: a new Bayesian approach,stat.ME,"['stat.ME', '62F15, 62F03, 62A, 62C10']","[arxiv.Result.Author('Francesco Bertolino'), arxiv.Result.Author('Silvia Columbu'), arxiv.Result.Author('Mara Manca'), arxiv.Result.Author('Monica Musio')]","The use of testing procedures for comparing two coefficients of variation
(CVs) of independent populations is not extensively explored in the Bayesian
context. We propose to address this issue through a test based on a measure of
evidence, the Bayesian Discrepancy Measure, recently introduced in the
literature. Computing the Bayesian Discrepancy Measure is straightforward when
the CVs depend on a single parameter of the distribution. In contrast, it
becomes more difficult when this simplification does not occur since more
parameters are involved, requiring often the use of MCMC methods. We derive the
Bayesian Discrepancy Measure and the related test by considering a variety of
distribution assumptions with multiparametric CVs and apply them to real
datasets. As far as we know, some of the examined problems have not yet been
covered in the literature.",0.16662619,-0.22359824,-0.09704969,C
5187,"In Table 3 we retain 3 clusters
among the 9 found in Table 2 for further study, i.e.","Notice that Notip is able to offer less                    Calibration
conservative guarantees on the TDP in all clusters than both                         The ﬁnal step of the procedure is to perform calibration
ARI and calibrated Simes.",chang-                      using the randomized p-values that we previously computed.,2022-04-22 08:43:29+00:00,Notip: Non-parametric True Discovery Proportion control for brain imaging,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Alexandre Blain'), arxiv.Result.Author('Bertrand Thirion'), arxiv.Result.Author('Pierre Neuvial')]","Cluster-level inference procedures are widely used for brain mapping. These
methods compare the size of clusters obtained by thresholding brain maps to an
upper bound under the global null hypothesis, computed using Random Field
Theory or permutations. However, the guarantees obtained by this type of
inference - i.e. at least one voxel is truly activated in the cluster - are not
informative with regards to the strength of the signal therein. There is thus a
need for methods to assess the amount of signal within clusters; yet such
methods have to take into account that clusters are defined based on the data,
which creates circularity in the inference scheme. This has motivated the use
of post hoc estimates that allow statistically valid estimation of the
proportion of activated voxels in clusters. In the context of fMRI data, the
All-Resolutions Inference framework introduced in [25] provides post hoc
estimates of the proportion of activated voxels. However, this method relies on
parametric threshold families, which results in conservative inference. In this
paper, we leverage randomization methods to adapt to data characteristics and
obtain tighter false discovery control. We obtain Notip, for Non-parametric
True Discovery Proportion control: a powerful, non-parametric method that
yields statistically valid guarantees on the proportion of activated voxels in
data-derived clusters. Numerical experiments demonstrate substantial gains in
number of detections compared with state-of-the-art methods on 36 fMRI
datasets. The conditions under which the proposed method brings benefits are
also discussed.",-0.007712315,-0.30874324,0.080167346,A
5207,"In Section 6, we conclude and

                                                        4
motivate further research.","After the introduction of
data sources and direct estimates in Section 5.1, we highlight modelling and robustness
properties of our proposed methods for point and uncertainty estimates compared to di-
rect and other SAE estimates under limited auxiliary data.","2 Theory and Method

This section introduces a general mixed model enabling a simultaneous discussion of
traditional LMM-based models in SAE such as the model of Battese et al.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",-0.13924544,0.035458274,-0.23574632,A
5208,"Although it is generally
possible to incorporate survey weights in the importance sampling within a forest, we
maintain that the eﬃcient use of survey weights with MERFs for the estimation of area-
level indicators requires further research which would exceed the scope of this paper.",weights are not directly used in the model-ﬁtting for MERFagg.,"Overall, all RPRs throughout Germany report comparable levels of average individual
monthly opportunity cost of care work.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",0.06392621,0.16276656,-0.030346721,C
5209,"From an empirical perspective, we face limitations that directly motivate further research.","Nevertheless, we allocate a small cluster
of lower levels of average individual opportunity cost of care work in the North-Eastern
part of Germany.","Firstly, we only calculate the opportunity cost of the working population and neglect
care work done by people who already left the labour market due to care work issues.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",0.1789917,0.057598736,-0.058925346,C
5210,"We motivate two major dimensions for further research, including theoretical work and
aspects of generalizations.","Nevertheless, given the data and our
initial aim to provide a general methodology for regional mapping of care work speciﬁc
regional diﬀerences, we consider the hourly wage as a ﬁrst reasonable approximation to

                                                       25
the unobservable “real” shadow price.","From a theoretical perspective, further research is needed
to investigate the construction of a partial-analytical MSE for area-level means or the
construction of an asymptotic MSE estimator.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",-0.048250735,0.1419804,-0.1422685,A
5211,"From a theoretical perspective, further research is needed
to investigate the construction of a partial-analytical MSE for area-level means or the
construction of an asymptotic MSE estimator.","We motivate two major dimensions for further research, including theoretical work and
aspects of generalizations.","From a statistical perspective, an in-depth
analysis regarding the eﬀects of incorporating survey weights into RFs and particularly
MERFs under aggregated covariate data is needed for point and uncertainty estimates,
as this would clearly exceed the scope of the present paper.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",-0.07953896,0.11632007,-0.21890762,A
5212,"Although, we will leave a detailed discussion of this idea to
further research, a short outline of the argument can be found in the Appendix 7.2.","Nevertheless,
we maintain that pairing our approach with a smearing argument allows for a more
general methodology and subsequently for the estimation of indicators such as quantiles
(Chambers & Dunstan, 1986).","Apart
from generalizations to quantiles, the approach of this paper is generalizable to model
(complex) spatial correlations.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",-0.16822869,0.055062074,-0.040090427,A
5213,"Additionally, a generalization towards binary or count
data is possible and left to further research.","Apart
from generalizations to quantiles, the approach of this paper is generalizable to model
(complex) spatial correlations.","The semi-parametric composite formulation
of Model (1) allows for f to adapt any functional form regarding the estimation of the
conditional mean of yij given xij and technically transfers to other machine learning
methods, such as gradient-boosted trees or support vector machines.",2022-04-22 14:49:49+00:00,Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Patrick Krennmair'), arxiv.Result.Author('Nora Würz'), arxiv.Result.Author('Timo Schmid')]","Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.",-0.28852132,0.08177229,-0.018731706,A
5343,"For the ﬁrst part of further research we chose compartment D, since
it seemed to exhibit a signiﬁcant consistency with the real-world data reports.",", n.

Hence, the main cost function for a single arbitrarily chosen compartment is deﬁned
as one of the previously introduced low-level functions with respect to the speciﬁed
compartment.","As a
result we ponder four cost-functions and deﬁned them using the following notation

   FDC = C(D) for C ∈ {MXSE, MSE, MAE, MAPE}.",2022-04-26 14:35:57+00:00,On automatic calibration of the SIRD epidemiological model for COVID-19 data in Poland,stat.ME,"['stat.ME', 'stat.ML', '92-10, 92D30, 62F10, 65L05']","[arxiv.Result.Author('Piotr Błaszczyk'), arxiv.Result.Author('Konrad Klimczak'), arxiv.Result.Author('Adam Mahdi'), arxiv.Result.Author('Piotr Oprocha'), arxiv.Result.Author('Paweł Potorski'), arxiv.Result.Author('Paweł Przybyłowicz'), arxiv.Result.Author('Michał Sobieraj')]","We propose a novel methodology for estimating the epidemiological parameters
of a modified SIRD model (acronym of Susceptible, Infected, Recovered and
Deceased individuals) and perform a short-term forecast of SARS-CoV-2 virus
spread. We mainly focus on forecasting number of deceased. The procedure was
tested on reported data for Poland. For some short-time intervals we performed
numerical test investigating stability of parameter estimates in the proposed
approach. Numerical experiments confirm the effectiveness of short-term
forecasts (up to 2 weeks) and stability of the method. To improve their
performance (i.e. computation time) GPU architecture was used in computations.",-0.0733916,-0.14331429,0.13677308,A
5369,"Proposing solutions to solve the computational burden, due to the curse of dimensionality,

                         19
is open and left for further study in a future work.","However it might not be easy
                                                          l=1

to determine an optimal maximum depth for each coordinate because we do not actually

observe data in Rk, we observe data in the hypersphere Sk, which is characterised by k − 1

directions and augment them with an extra latent resultant to produce data in the whole

space Rk, so we can not easily identify coordinates that would require ﬁner partitions.","Acknowledgements

The author acknowledges support from Asociaci´on Mexicana de Cultura, A.C.

References

Downs, T.D.",2022-04-26 23:55:14+00:00,Multivariate and regression models for directional data based on projected Pólya trees,stat.ME,['stat.ME'],[arxiv.Result.Author('Luis E. Nieto-Barajas')],"Projected distributions have proved to be useful in the study of circular and
directional data. Although any multivariate distribution can be used to produce
a projected model, these distributions are typically parametric. In this
article we consider a multivariate P\'olya tree on $R^k$ and project it to the
unit hypersphere $S^k$ to define a new Bayesian nonparametric model for
directional data. We study the properties of the proposed model and in
particular, concentrate on the implied conditional distributions of some
directions given the others to define a directional-directional regression
model. We also define a multivariate linear regression model with P\'olya tree
error and project it to define a linear-directional regression model. We obtain
the posterior characterisation of all models and show their performance with
simulated and real datasets.",-0.1226423,-0.35038167,0.33728713,A
5498,"This thresholding scheme has proved superior to
other, more classical thresholding options in the simulations, and deﬁnitely deserves further research in a follow-up
paper.","Along the way, we proposed a novel thresholding approach, based
on a jackknife estimation of the variance of each estimated wavelet coeﬃcient – this again ﬁts perfectly within the
considered framework of Hellinger-Bhattacharyya cross-validation.","Appendix

Let An the search space for λ above in the ‘weighted delta sequence’ (2.9) for a sample of size n. We introduce the
following additional assumptions on An:

Assumption 1.",2022-04-29 02:39:04+00:00,Hellinger-Bhattacharyya cross-validation for shape-preserving multivariate wavelet thresholding,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Carlos Aya-Moreno'), arxiv.Result.Author('Gery Geenens'), arxiv.Result.Author('Spiridon Penev')]","The benefits of the wavelet approach for density estimation are well
established in the literature, especially when the density to estimate is
irregular or heterogeneous in smoothness. However, wavelet density estimates
are typically not bona fide densities. In Aya-Moreno et al (2018), a
`shape-preserving' wavelet density estimator was introduced, including as main
step the estimation of the square-root of the density. A natural concept
involving square-root of densities is the Hellinger distance - or equivalently,
the Bhattacharyya affinity coefficient. In this paper, we deliver a fully
data-driven version of the above 'shape-preserving' wavelet density estimator,
where all user-defined parameters, such as resolution level or thresholding
specifications, are selected by optimising an original leave-one-out version of
the Hellinger-Bhattacharyya criterion. The theoretical optimality of the
proposed procedure is established, while simulations show the strong practical
performance of the estimator. Within that framework, we also propose a novel
but natural 'jackknife thresholding' scheme, which proves superior to other,
more classical thresholding options.",-0.16488954,-0.20427527,-0.07583093,A
5544,"Finding metrics to do so is an interesting open
problem which is left for further research.","The introduced model assessment
framework does not examine cross-dependencies between binary and continuous data when it comes to goodness of
ﬁt and predictive performance as this is not a straightforward task.","In this paper, we proceeded requiring good performance on both marginal
parts of the data in the absence of a better alternative in line with Moustaki (1996).",2022-04-29 22:24:44+00:00,Bayesian Benefit Risk Analysis,stat.ME,"['stat.ME', 'stat.AP', '62P10 (Primary) 62L12, 62H12, 62H25 (Secondary)']","[arxiv.Result.Author('Konstantinos Vamvourellis'), arxiv.Result.Author('Konstantinos Kalogeropoulos'), arxiv.Result.Author('Lawrence Phillips')]","The process of approving and assessing new drugs is often quite complicated,
mainly due to the fact that multiple criteria need to be considered. A standard
way to proceed is with benefit risk analysis, often under the Bayesian paradigm
to account for uncertainty and combine data with expert judgement, which is
operationalised via multi-criteria decision analysis (MCDA) scores. The
procedure is based on a suitable model to accommodate key features of the data,
which are typically of mixed type and potentially depended, with factor models
providing a standard choice. The contribution of this paper is threefold:
first, we extend the family of existing structured factor models. Second, we
provide a framework for choosing between them, which combines fit and
out-of-sample predictive performance. Third, we present a sequential estimation
framework that can offer multiple benefits: (i) it allows us to efficiently
re-estimate MCDA scores of different drugs each time new data become available,
thus getting an idea on potential fluctuations in differences between them,
(ii) it can provide information on potential early stopping in cases of evident
conclusions, thus reducing unnecessary further exposure to undesirable
treatments; (iii) it can potentially allow to assign treatment groups
dynamically based on research objectives. A drawback of sequential estimation
is the increased computational time, but this can be mitigated by efficient
sequential Monte Carlo schemes which we tailor in this paper to the context of
Bayesian benefit risk analysis. The developed methodology is illustrated on
real data on Type II diabetes patients who were administered Metformin (MET),
Rosiglitazone (RSG) and a combination of the two (AVM).",-0.03140971,0.08258389,0.06596135,A
5580,"We
leave it for further study.","The fused
algorithm has a potential of further enhancing generality and computational ﬂexibility.",Computational eﬃciency is usually a major concern in constructing multiple-CP tests.,2022-04-30 16:42:23+00:00,A General Framework For Constructing Locally Self-Normalized Multiple-Change-Point Tests,stat.ME,['stat.ME'],"[arxiv.Result.Author('Cheuk Hin Cheng'), arxiv.Result.Author('Kin Wai Chan')]","We propose a general framework to construct self-normalized
multiple-change-point tests with time series data. The only building block is a
user-specified one-change-point detecting statistic, which covers a wide class
of popular methods, including cumulative sum process, outlier-robust rank
statistics and order statistics. Neither robust and consistent estimation of
nuisance parameters, selection of bandwidth parameters, nor pre-specification
of the number of change points is required. The finite-sample performance shows
that our proposal is size-accurate, robust against misspecification of the
alternative hypothesis, and more powerful than existing methods. Case studies
of NASDAQ option volume and Shanghai-Hong Kong Stock Connect turnover are
provided.",-0.012580815,-0.21987614,0.30210423,A
5622,"Extension to a more
complicated scenario that allows temporal dependence certainly deserves further researches.","(2021), under which
the proof of asymptotic independence is rather highly non-trivial.","A related issue is does the asymptotic normality still holds for the sum-L2-type statistic for
temporally dependent high-dimensional time series (e.g., under Assumptions A1–A2).",2022-05-02 07:55:41+00:00,Computationally efficient and data-adaptive changepoint inference in high dimension,stat.ME,['stat.ME'],"[arxiv.Result.Author('Guanghui Wang'), arxiv.Result.Author('Long Feng')]","High-dimensional changepoint inference that adapts to various change patterns
has received much attention recently. We propose a simple, fast yet effective
approach for adaptive changepoint testing. The key observation is that two
statistics based on aggregating cumulative sum statistics over all dimensions
and possible changepoints by taking their maximum and summation, respectively,
are asymptotically independent under some mild conditions. Hence we are able to
form a new test by combining the p-values of the maximum- and summation-type
statistics according to their limit null distributions. To this end, we develop
new tools and techniques to establish asymptotic distribution of the
maximum-type statistic under a more relaxed condition on componentwise
correlations among all variables than that in existing literature. The proposed
method is simple to use and computationally efficient. It is adaptive to
different sparsity levels of change signals, and is comparable to or even
outperforms existing approaches as revealed by our numerical studies.",-0.27138793,0.069999665,-0.031819023,A
5657,"If not, then the ranking of biomarkers might still provide biological or clinical
insight, or motivate further study.","The results of the
initial stage can help assess whether the assumption of sparsity used by existing methods is tenable, and therefore
whether estimating the CATE is feasible.","If so, the CATE may be estimated more accurately, thanks to the reduced number
of features considered, using ﬂexible methods like those of Tian et al.",2022-05-03 03:19:07+00:00,A Flexible Approach for Predictive Biomarker Discovery,stat.ME,['stat.ME'],"[arxiv.Result.Author('Philippe Boileau'), arxiv.Result.Author('Nina Ting Qi'), arxiv.Result.Author('Mark J. van der Laan'), arxiv.Result.Author('Sandrine Dudoit'), arxiv.Result.Author('Ning Leng')]","An endeavor central to precision medicine is predictive biomarker discovery;
they define patient subpopulations which stand to benefit most, or least, from
a given treatment. The identification of these biomarkers is often the
byproduct of the related but fundamentally different task of treatment rule
estimation. Using treatment rule estimation methods to identify predictive
biomarkers in clinical trials where the number of covariates exceeds the number
of participants often results in high false discovery rates. The higher than
expected number of false positives translates to wasted resources when
conducting follow-up experiments for drug target identification and diagnostic
assay development. Patient outcomes are in turn negatively affected. We propose
a variable importance parameter for directly assessing the importance of
potentially predictive biomarkers, and develop a flexible semiparametric
inference procedure for this estimand. We prove that our estimator is
double-robust and asymptotically linear under loose conditions on the
data-generating process, permitting valid inference about the importance
metric. The statistical guarantees of the method are verified in a thorough
simulation study representative of randomized control trials with moderate and
high-dimensional covariate vectors. Our procedure is then used to discover
predictive biomarkers from among the tumor gene expression data of metastatic
renal cell carcinoma patients enrolled in recently completed clinical trials.
We find that our approach more readily discerns predictive from non-predictive
biomarkers than procedures whose primary purpose is treatment rule estimation.
An open-source software implementation of the methodology, the uniCATE R
package, is briefly introduced.",0.2545741,-0.011742365,0.1363709,C
5658,"If not, then the ranking of biomarkers might still provide biological or clinical
insight, or motivate further study.","The results of the
initial stage can help assess whether the assumption of sparsity used by existing methods is tenable, and therefore
whether estimating the CATE is feasible.","If so, the CATE may be estimated more accurately, thanks to the reduced number
of features considered, using ﬂexible methods like those of Tian et al.",2022-05-03 03:19:07+00:00,A Flexible Approach for Predictive Biomarker Discovery,stat.ME,['stat.ME'],"[arxiv.Result.Author('Philippe Boileau'), arxiv.Result.Author('Nina Ting Qi'), arxiv.Result.Author('Mark J. van der Laan'), arxiv.Result.Author('Sandrine Dudoit'), arxiv.Result.Author('Ning Leng')]","An endeavor central to precision medicine is predictive biomarker discovery;
they define patient subpopulations which stand to benefit most, or least, from
a given treatment. The identification of these biomarkers is often the
byproduct of the related but fundamentally different task of treatment rule
estimation. Using treatment rule estimation methods to identify predictive
biomarkers in clinical trials where the number of covariates exceeds the number
of participants often results in high false discovery rates. The higher than
expected number of false positives translates to wasted resources when
conducting follow-up experiments for drug target identification and diagnostic
assay development. Patient outcomes are in turn negatively affected. We propose
a variable importance parameter for directly assessing the importance of
potentially predictive biomarkers, and develop a flexible nonparametric
inference procedure for this estimand. We prove that our estimator is
double-robust and asymptotically linear under loose conditions on the
data-generating process, permitting valid inference about the importance
metric. The statistical guarantees of the method are verified in a thorough
simulation study representative of randomized control trials with moderate and
high-dimensional covariate vectors. Our procedure is then used to discover
predictive biomarkers from among the tumor gene expression data of metastatic
renal cell carcinoma patients enrolled in recently completed clinical trials.
We find that our approach more readily discerns predictive from non-predictive
biomarkers than procedures whose primary purpose is treatment rule estimation.
An open-source software implementation of the methodology, the uniCATE R
package, is briefly introduced.",0.2545741,-0.011742365,0.1363709,C
5944,"Clearly, combining forecasts nonlinearly requires further research.","Moreover,
Adhikari (2015) deﬁned the nonlinear term using vij = zˆi − mijzˆj zˆj − mjizˆi , where zˆi denotes the
standardized ith individual forecast using the mean y¯i and standard deviation σi, and the term mij
denotes the degree of mutual dependency between the ith and jth individual forecasts.","In particular, the forecasting
performance of the various proposed nonlinear combination schemes should be properly investigated
with a large, diverse collection of time series datasets along with appropriate statistical inference.",2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from the single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts, and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.3503186,0.41595507,0.28839198,B
5945,"Clearly, the forecast performance of nonlinear pooling approaches largely depends on diverse factors,
including the features of the target data, mixture component models, and training periods, and thereby
deserves further research.","(2015) generalized the literature by incorporating the
dependence of the mixture weights on the variable one is trying to forecast, allowing the weights
themselves to introduce the nonlinearities and thus leading to outcome-dependent density pooling.","This is in agreement with Baran and Lerch (2018) who investigated the
performance of state-of-the-art forecast combination methods through case studies and found no
substantial differences in forecast performance between the simple linear pool and the theoretically
superior but cumbersome nonlinear pooling approaches.",2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from the single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts, and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.2963726,0.45285094,0.2768745,B
5946,"In this review, our goal has been to show not only how forecast combinations have evolved
over time, but also to identify the potential and limitations of various methods, and to highlight the
areas needing further research.","In this regard, forecast combinations provide an easy path to improving forecast accuracy
by integrating the available information used in the individual forecasts.","Forecast combinations can be model-free or model-ﬁtting, linear or
nonlinear, static or time-varying, series-speciﬁc or cross-learning, and frequentist or Bayesian.",2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from the single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts, and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.34320349,0.570526,0.29030895,B_centroid
5947,"Therefore, another interesting path for further research would be to
take more account of correlations among individual forecasts in weighting schemes for probabilistic
forecast combinations.","Despite the
existence of such correlations, the literature on probabilistic forecast combinations has paid scant
attention to addressing them; they are primarily addressed from a Bayesian perspective (e.g., Winkler,
1981; McAlinn and West, 2019).",Cross-learning and feature engineering.,2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from the single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts, and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.30245876,0.5103597,0.27821204,B
5948,"In this regard, we believe that further research needs
to be done on feature engineering for time series data to unlock the potential of cross-learning.","Moreover, access to feature engineering can lead to improved forecasting
performance, providing valuable information for forecast combinations in a cross-learning fashion
(Montero-Manso et al., 2020; Kang et al., 2021).",Encouraging researchers to contribute open-source software and datasets.,2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from the single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts, and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.16951996,0.5283174,0.43217406,B
5949,"Clearly, combining forecasts nonlinearly requires further research.","Moreover,
Adhikari (2015) deﬁned the nonlinear term using vij = zˆi − mijzˆj zˆj − mjizˆi , where zˆi denotes the
standardized ith individual forecast using the mean y¯i and standard deviation σi, and the term mij
denotes the degree of mutual dependency between the ith and jth individual forecasts.","In particular, the forecasting
performance of the various proposed nonlinear combination schemes should be properly investigated
with a large, diverse collection of time series datasets along with appropriate statistical inference.",2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.3503186,0.41595507,0.28839198,B
5950,"Clearly, the forecast performance of nonlinear pooling approaches largely depends on diverse factors,
including the features of the target data, mixture component models, and training periods, and thereby
deserves further research.","(2015) generalized the literature by incorporating the
dependence of the mixture weights on the variable one is trying to forecast, allowing the weights
themselves to introduce the nonlinearities and thus leading to outcome-dependent density pooling.","This is in agreement with Baran and Lerch (2018) who investigated the
performance of state-of-the-art forecast combination methods through case studies and found no
substantial differences in forecast performance between the simple linear pool and the theoretically
superior but cumbersome nonlinear pooling approaches.",2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.2963726,0.45285094,0.2768745,B
5951,"In this review, our goal has been to show not only how forecast combinations have evolved
over time, but also to identify the potential and limitations of various methods, and to highlight the
areas needing further research.","In this regard, forecast combinations provide an easy path to improving forecast accuracy
by integrating the available information used in individual forecasts.","Forecast combinations can be model-free or model-ﬁtting, linear or
nonlinear, static or time-varying, series-speciﬁc or cross-learning, and frequentist or Bayesian.",2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.3370173,0.57204366,0.29225677,B
5952,"Therefore, another interesting path for further research would be to take
more into account of correlations among individual forecasts in weighting schemes for probabilistic
forecast combinations.","Despite the existence
of such correlations, the literature on probabilistic forecast combinations has paid scant attention
to addressing them; they are primarily addressed from a Bayesian perspective (e.g., Winkler, 1981;
McAlinn and West, 2019).",Cross-learning and feature engineering.,2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.30682153,0.5088729,0.27554828,B
5953,"In this regard, we believe that further research needs
to be done on feature engineering for time series data to unlock the potential of cross-learning.","Moreover, access to feature engineering can lead to improved forecasting
performance, providing valuable information for forecast combinations in a cross-learning fashion

                                                                 39
(Montero-Manso et al., 2020; Kang et al., 2021).",Encouraging researchers to contribute open-source software and datasets.,2022-05-09 12:14:02+00:00,Forecast combinations: an over 50-year review,stat.ME,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Xiaoqian Wang'), arxiv.Result.Author('Rob J Hyndman'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Yanfei Kang')]","Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research.",-0.17209256,0.52651876,0.42837977,B
6059,"Mod APE gives very

similar results to APE, suggesting that the α-Modiﬁed CV approach requires further study

for the Group Lasso.","On average, however,

both new methods have fewer false positives and smaller model sizes.","Although AR2 CV is not perfect in this application, in general it

reduces model size and decreases false positives at no additional computational cost.",2022-05-11 17:33:55+00:00,Tuning Parameter Selection for Penalized Estimation via R2,stat.ME,['stat.ME'],"[arxiv.Result.Author('Julia Holter'), arxiv.Result.Author('Jonathan Stallrich')]","The tuning parameter selection strategy for penalized estimation is crucial
to identify a model that is both interpretable and predictive. However, popular
strategies (e.g., minimizing average squared prediction error via
cross-validation) tend to select models with more predictors than necessary.
This paper proposes a simple, yet powerful cross-validation strategy based on
maximizing squared correlations between the observed and predicted values,
rather than minimizing squared error loss. The strategy can be applied to all
penalized least-squares estimators and we show that, under certain conditions,
the metric implicitly performs a bias adjustment. Specific attention is given
to the lasso estimator, in which our strategy is closely related to the relaxed
lasso estimator. We demonstrate our approach on a functional variable selection
problem to identify optimal placement of surface electromyogram sensors to
control a robotic hand prosthesis.",0.13502392,-0.12198806,0.11204212,C
6060,"As overspeciﬁcation is a particular concern,
the question of whether out-of-sample data can be used to ﬁnd estimates of α is another
subject of further research.","Finally, the calculation of αˆλ described here
uses in-sample predictions and observations.","References

Akaike, H. (1974) A new look at the statistical model identiﬁcation.",2022-05-11 17:33:55+00:00,Tuning Parameter Selection for Penalized Estimation via R2,stat.ME,['stat.ME'],"[arxiv.Result.Author('Julia Holter'), arxiv.Result.Author('Jonathan Stallrich')]","The tuning parameter selection strategy for penalized estimation is crucial
to identify a model that is both interpretable and predictive. However, popular
strategies (e.g., minimizing average squared prediction error via
cross-validation) tend to select models with more predictors than necessary.
This paper proposes a simple, yet powerful cross-validation strategy based on
maximizing squared correlations between the observed and predicted values,
rather than minimizing squared error loss. The strategy can be applied to all
penalized least-squares estimators and we show that, under certain conditions,
the metric implicitly performs a bias adjustment. Specific attention is given
to the lasso estimator, in which our strategy is closely related to the relaxed
lasso estimator. We demonstrate our approach on a functional variable selection
problem to identify optimal placement of surface electromyogram sensors to
control a robotic hand prosthesis.",0.029867465,0.04504956,-0.3218754,A
6061,"Mod APE gives very
similar results to APE, suggesting that the α-modiﬁed CV approach requires further study
for the Group Lasso.","On average, however,
both new methods have fewer false positives and smaller model sizes.","Although AR2 CV is not perfect in this application, in general it
reduces model size and decreases false positives at no additional computational cost.",2022-05-11 17:33:55+00:00,Tuning Parameter Selection for Penalized Estimation via $R^2$,stat.ME,['stat.ME'],"[arxiv.Result.Author('Julia Holter'), arxiv.Result.Author('Jonathan Stallrich')]","The tuning parameter selection strategy for penalized estimation is crucial
to identify a model that is both interpretable and predictive. However, popular
strategies (e.g., minimizing average squared prediction error via
cross-validation) tend to select models with more predictors than necessary.
This paper proposes a simple, yet powerful cross-validation strategy based on
maximizing squared correlations between the observed and predicted values,
rather than minimizing squared error loss for the purposes of support recovery.
The strategy can be applied to all penalized least-squares estimators and we
show that, under certain conditions, the metric implicitly performs a bias
adjustment. Specific attention is given to the Lasso estimator, in which our
strategy is closely related to the Relaxed Lasso estimator. We demonstrate our
approach on a functional variable selection problem to identify optimal
placement of surface electromyogram sensors to control a robotic hand
prosthesis.",0.13502392,-0.12198806,0.11204212,C
6062,"As overspeciﬁcation is a particular concern, the
question of whether out-of-sample data can be used to ﬁnd estimates of α is another subject
of further research.","Finally, the calculation of αˆλ described here
uses in-sample predictions and observations.","Acknowledgements

This work was partially supported by the National Science Foundation, grant IOS–2039226.",2022-05-11 17:33:55+00:00,Tuning Parameter Selection for Penalized Estimation via $R^2$,stat.ME,['stat.ME'],"[arxiv.Result.Author('Julia Holter'), arxiv.Result.Author('Jonathan Stallrich')]","The tuning parameter selection strategy for penalized estimation is crucial
to identify a model that is both interpretable and predictive. However, popular
strategies (e.g., minimizing average squared prediction error via
cross-validation) tend to select models with more predictors than necessary.
This paper proposes a simple, yet powerful cross-validation strategy based on
maximizing squared correlations between the observed and predicted values,
rather than minimizing squared error loss for the purposes of support recovery.
The strategy can be applied to all penalized least-squares estimators and we
show that, under certain conditions, the metric implicitly performs a bias
adjustment. Specific attention is given to the Lasso estimator, in which our
strategy is closely related to the Relaxed Lasso estimator. We demonstrate our
approach on a functional variable selection problem to identify optimal
placement of surface electromyogram sensors to control a robotic hand
prosthesis.",0.10686466,0.034458026,-0.25262582,C
6381,"With the aim of augmenting the inference with these
                                          estimands in practice, we further study an existing distribution-free framework for
                                          the plug-in estimation of bounds on the probability an individual beneﬁts from
                                          treatment (PIBT), a generally inestimable quantity that would concisely summarize
                                          an intervention’s efﬁcacy if it could be known.","In the context of causal inference, due to the missing-ness of one outcome, it is
                                          difﬁcult to check whether an individual’s treatment effect lies close to its prediction
                                          given by the estimated Average Treatment Effect (ATE) or Conditional Average
                                          Treatment Effect (CATE).","Given the innate uncertainty in the
                                          target population-level bounds on PIBT, we seek to better understand the margin of
                                          error for the estimation of these target parameters in order to help discern whether
                                          estimated bounds on treatment efﬁcacy are tight (or wide) due to random chance
                                          or not.",2022-05-18 17:38:32+00:00,Non-asymptotic confidence bands on the probability an individual benefits from treatment (PIBT),stat.ME,['stat.ME'],"[arxiv.Result.Author('Gabriel Ruiz'), arxiv.Result.Author('Oscar Hernan Madrid Padilla')]","The premise of this work, in a vein similar to predictive inference with
quantile regression, is that observations may lie far away from their
conditional expectation. In the context of causal inference, due to the
missing-ness of one outcome, it is difficult to check whether an individual's
treatment effect lies close to its prediction given by the estimated Average
Treatment Effect (ATE) or Conditional Average Treatment Effect (CATE). With the
aim of augmenting the inference with these estimands in practice, we further
study an existing distribution-free framework for the plug-in estimation of
bounds on the probability an individual benefits from treatment (PIBT), a
generally inestimable quantity that would concisely summarize an intervention's
efficacy if it could be known. Given the innate uncertainty in the target
population-level bounds on PIBT, we seek to better understand the margin of
error for the estimation of these target parameters in order to help discern
whether estimated bounds on treatment efficacy are tight (or wide) due to
random chance or not. In particular, we present non-asymptotic guarantees to
the estimation of bounds on marginal PIBT for a randomized controlled trial
(RCT) setting. We also derive new non-asymptotic results for the case where we
would like to understand heterogeneity in PIBT across strata of pre-treatment
covariates, with one of our main results in this setting making strategic use
of regression residuals. These results, especially those in the RCT case, can
be used to help with formal statistical power analyses and frequentist
confidence statements for settings where we are interested in inferring PIBT
through the target bounds under minimal parametric assumptions.",0.2563948,0.17891195,-0.39347142,C
6382,"With the aim of augmenting the inference with these
                                                     estimands in practice, we further study an existing distribution-free framework for
                                                     the plug-in estimation of bounds on the probability an individual beneﬁts from
                                                     treatment (PIBT), a generally inestimable quantity that would concisely summarize
                                                     an intervention’s efﬁcacy if it could be known.","In the context of causal inference, due to the missing-ness of one outcome, it is
                                                     difﬁcult to check whether an individual’s treatment effect lies close to its prediction
                                                     given by the estimated Average Treatment Effect (ATE) or Conditional Average
                                                     Treatment Effect (CATE).","Given the innate uncertainty in the
                                                     target population-level bounds on PIBT, we seek to better understand the margin of
                                                     error for the estimation of these target parameters in order to help discern whether
                                                     estimated bounds on treatment efﬁcacy are tight (or wide) due to random chance
                                                     or not.",2022-05-18 17:38:32+00:00,Non-asymptotic confidence bands on the probability an individual benefits from treatment (PIBT),stat.ME,['stat.ME'],"[arxiv.Result.Author('Gabriel Ruiz'), arxiv.Result.Author('Oscar Hernan Madrid Padilla')]","The premise of this work, in a vein similar to predictive inference with
quantile regression, is that observations may lie far away from their
conditional expectation. In the context of causal inference, due to the
missing-ness of one outcome, it is difficult to check whether an individual's
treatment effect lies close to its prediction given by the estimated Average
Treatment Effect (ATE) or Conditional Average Treatment Effect (CATE). With the
aim of augmenting the inference with these estimands in practice, we further
study an existing distribution-free framework for the plug-in estimation of
bounds on the probability an individual benefits from treatment (PIBT), a
generally inestimable quantity that would concisely summarize an intervention's
efficacy if it could be known. Given the innate uncertainty in the target
population-level bounds on PIBT, we seek to better understand the margin of
error for the estimation of these target parameters in order to help discern
whether estimated bounds on treatment efficacy are tight (or wide) due to
random chance or not. In particular, we present non-asymptotic guarantees to
the estimation of bounds on marginal PIBT for a randomized controlled trial
(RCT) setting. We also derive new non-asymptotic results for the case where we
would like to understand heterogeneity in PIBT across strata of pre-treatment
covariates, with one of our main results in this setting making strategic use
of regression residuals. These results, especially those in the RCT case, can
be used to help with formal statistical power analyses and frequentist
confidence statements for settings where we are interested in inferring PIBT
through the target bounds under minimal parametric assumptions.",0.2563948,0.17891195,-0.39347142,C
6392,"Finally, we conclude by suggesting several avenues of further research.","Section 4.2 examines an application to French
polling data.","5
2 Unbiased linear mean-square prediction

If a parametric form for Λ is known, then least-square prediction reduces to least-square
estimation of the parameters, either analytically or through a numerical scheme.",2022-05-18 22:52:20+00:00,Linear prediction of point process times and marks,stat.ME,"['stat.ME', 'math.PR']","[arxiv.Result.Author('Maximilian Aigner'), arxiv.Result.Author('Valérie Chavez-Demoulin')]","In this paper, we are interested in linear prediction of a particular kind of
stochastic process, namely a marked temporal point process. The observations
are event times recorded on the real line, with marks attached to each event.
We show that in this case, linear prediction generalises straightforwardly from
the theory of prediction for stationary stochastic processes. We propose two
recursive methods to solve the linear prediction problem and show that these
are computationally efficient. The first relies on a Wiener-Hopf integral
equation and a corresponding set of differential equations. It is particularly
well-adapted to autoregressive processes. In the second method, we develop an
innovations algorithm tailored for moving-average processes. Both methods are
assessed by an extensive simulation study and applied to a real-world dataset
of polling data ahead of the 2022 French elections. In a particular case, we
extend the ""model independent origin"" idea of Jaisson (2015) to the marked
Hawkes process through its autoregressive representation. As a corollary, we
also improve on existing non-recursive estimators such as that proposed by
Bacry and Muzy (2016).",-0.14507037,0.22740018,-0.13836128,A
6393,"Finally, we
conclude by suggesting several avenues of further research in line with our ﬁndings.","To illustrate our proposed methods,
we conduct a simulation study in Section 4.1 on two typical examples.",2.,2022-05-18 22:52:20+00:00,Linear prediction of point process times and marks,stat.ME,"['stat.ME', 'math.PR']","[arxiv.Result.Author('Maximilian Aigner'), arxiv.Result.Author('Valérie Chavez-Demoulin')]","In this paper, we are interested in linear prediction of a particular kind of
stochastic process, namely a marked temporal point process. The observations
are event times recorded on the real line, with marks attached to each event.
We show that in this case, linear prediction extends straightforwardly from the
theory of prediction for stationary stochastic processes. Following classical
lines, we derive a Wiener-Hopf-type integral equation to characterise the
linear predictor, extending the ""model independent origin"" of the Hawkes
process (Jaisson, 2015) as a corollary. We propose two recursive methods to
solve the linear prediction problem and show that these are computationally
efficient in known cases. The first solves the Wiener-Hopf equation via a set
of differential equations. It is particularly well-adapted to autoregressive
processes. In the second method, we develop an innovations algorithm tailored
for moving-average processes. A small simulation study on two typical examples
shows the application of numerical schemes for estimation of a Hawkes process
intensity.",-0.020233423,-0.23208505,-0.050408736,A
6449,Concluding remarks and further research directions are given in §6.,"Speciﬁcally, we ﬁt diﬀerent models, especially
for diﬀerent mark distributions, and compare diﬀerent models through cross-validation criteria.","2 Landslide data and predictor variables

2.1 Study area and data description

The study area covers 33.2 km2 in the municipality of Ulus (Bartin) in the Western Black
Sea, Turkey, drained by the Ulus river.",2022-05-20 00:29:20+00:00,Joint modeling of landslide counts and sizes using spatial marked point processes with sub-asymptotic mark distributions,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Rishikesh Yadav'), arxiv.Result.Author('Raphaël Huser'), arxiv.Result.Author('Thomas Opitz'), arxiv.Result.Author('Luigi Lombardo')]","To accurately quantify landslide hazard in a region of Turkey, we develop new
marked point process models within a Bayesian hierarchical framework for the
joint prediction of landslide counts and sizes. To accommodate for the dominant
role of the few largest landslides in aggregated sizes, we leverage mark
distributions with strong justification from extreme-value theory, thus
bridging the two broad areas of statistics of extremes and marked point
patterns. At the data level, we assume a Poisson distribution for landslide
counts, while we compare different ""sub-asymptotic"" distributions for landslide
sizes to flexibly model their upper and lower tails. At the latent level,
Poisson intensities and the median of the size distribution vary spatially in
terms of fixed and random effects, with shared spatial components capturing
cross-correlation between landslide counts and sizes. We robustly model spatial
dependence using intrinsic conditional autoregressive priors. Our novel models
are fitted efficiently using a customized adaptive Markov chain Monte Carlo
algorithm. We show that, for our dataset, sub-asymptotic mark distributions
provide improved predictions of large landslide sizes compared to more
traditional choices. To showcase the benefits of joint occurrence-size models
and illustrate their usefulness for risk assessment, we map landslide hazard
along major roads.",-0.047639597,0.12286083,-0.012845983,A
6548,"As these methods
can be very computation intensive and/or do require considerable expertise about training
surrogate models, there is a clear need for further research on alternative approaches that
would exploit asymptotic behavior of the approximate implicit likelihood.","Discussion and conclusions

Despite some early interest in the frequentist likelihood-free inference, more recently the
primary focus of the research has been on approximating the posterior distribution of pa-
rameters with various sampling and surrogate model based approaches.","In the present
work we considered this objective for the situation where the Jensen–Shannon divergence
between observed and simulated data is used to measure the model ﬁt and ﬁnd model
parameters that best explain the observed data.",2022-05-22 18:00:13+00:00,Nonparametric likelihood-free inference with Jensen-Shannon divergence for simulator-based models with categorical output,stat.ME,"['stat.ME', 'cs.LG', '94-xx, 62Dxx', 'G.2; G.3; I.2']","[arxiv.Result.Author('Jukka Corander'), arxiv.Result.Author('Ulpu Remes'), arxiv.Result.Author('Ida Holopainen'), arxiv.Result.Author('Timo Koski')]","Likelihood-free inference for simulator-based statistical models has recently
attracted a surge of interest, both in the machine learning and statistics
communities. The primary focus of these research fields has been to approximate
the posterior distribution of model parameters, either by various types of
Monte Carlo sampling algorithms or deep neural network -based surrogate models.
Frequentist inference for simulator-based models has been given much less
attention to date, despite that it would be particularly amenable to
applications with big data where implicit asymptotic approximation of the
likelihood is expected to be accurate and can leverage computationally
efficient strategies. Here we derive a set of theoretical results to enable
estimation, hypothesis testing and construction of confidence intervals for
model parameters using asymptotic properties of the Jensen--Shannon divergence.
Such asymptotic approximation offers a rapid alternative to more
computation-intensive approaches and can be attractive for diverse applications
of simulator-based models. 61",-0.2898069,0.018220332,-0.25410372,A
6549,"As these methods
can be very computation intensive and/or do require considerable expertise about training
surrogate models, there is a clear need for further research on alternative approaches that
would exploit asymptotic behavior of the approximate implicit likelihood.","Discussion and conclusions

Despite some early interest in the frequentist likelihood-free inference, more recently the
primary focus of the research has been on approximating the posterior distribution of pa-
rameters with various sampling and surrogate model based approaches.","In the present
work we considered this objective for the situation where the Jensen–Shannon divergence
between observed and simulated data is used to measure the model ﬁt and ﬁnd model
parameters that best explain the observed data.",2022-05-22 18:00:13+00:00,Nonparametric likelihood-free inference with Jensen-Shannon divergence for simulator-based models with categorical output,stat.ME,"['stat.ME', 'cs.LG', '94-xx, 62Dxx', 'G.2; G.3; I.2']","[arxiv.Result.Author('Jukka Corander'), arxiv.Result.Author('Ulpu Remes'), arxiv.Result.Author('Ida Holopainen'), arxiv.Result.Author('Timo Koski')]","Likelihood-free inference for simulator-based statistical models has recently
attracted a surge of interest, both in the machine learning and statistics
communities. The primary focus of these research fields has been to approximate
the posterior distribution of model parameters, either by various types of
Monte Carlo sampling algorithms or deep neural network -based surrogate models.
Frequentist inference for simulator-based models has been given much less
attention to date, despite that it would be particularly amenable to
applications with big data where implicit asymptotic approximation of the
likelihood is expected to be accurate and can leverage computationally
efficient strategies. Here we derive a set of theoretical results to enable
estimation, hypothesis testing and construction of confidence intervals for
model parameters using asymptotic properties of the Jensen--Shannon divergence.
Such asymptotic approximation offers a rapid alternative to more
computation-intensive approaches and can be attractive for diverse applications
of simulator-based models. 61",-0.2898069,0.018220332,-0.25410372,A
6636,"Addressing the non-uniqueness estimation in both CQR and CER estimators is left as an
interesting avenue for further research.","The non-unique estimates in both CQR and CER may further
cause a longstanding problem of quantile crossing in quantile estimation (Dai et al., 2022).","2.4 Order-α

To ameliorate the sensitivity of DEA to outliers, Cazals et al.",2022-05-24 08:10:47+00:00,Partial frontiers are not quantiles,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Sheng Dai'), arxiv.Result.Author('Timo Kuosmanen'), arxiv.Result.Author('Xun Zhou')]","Quantile regression and partial frontier are two distinct approaches to
nonparametric quantile frontier estimation. In this article, we demonstrate
that partial frontiers are not quantiles. Both convex and nonconvex
technologies are considered. To this end, we propose convexified order-$\alpha$
as an alternative to convex quantile regression (CQR) and convex expectile
regression (CER), and two new nonconvex estimators: isotonic CQR and isotonic
CER as alternatives to order-$\alpha$. A Monte Carlo study shows that the
partial frontier estimators perform relatively poorly and even can violate the
quantile property, particularly at low quantiles. In addition, the simulation
evidence shows that the indirect expectile approach to estimating quantiles
generally outperforms the direct quantile estimations. We further find that the
convex estimators outperform their nonconvex counterparts owing to their global
shape constraints. An illustration of those estimators is provided using a
real-world dataset of U.S. electric power plants.",0.046657708,-0.057278525,-0.21348031,A
6664,"We hope this work will stimulate further research within
this Bayesian decesion framework.","In the meanwhile, we note there are limitations; for example, the censoring assumptions
for time-to-event data are greatly simpliﬁed.","In the present work, data are supposed to be analysed after the completion of all subtrials.",2022-05-24 17:35:00+00:00,Bayesian sample size determination in basket trials borrowing information between subsets,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Haiyan Zheng'), arxiv.Result.Author('Michael J. Grayling'), arxiv.Result.Author('Pavel Mozgunov'), arxiv.Result.Author('Thomas Jaki'), arxiv.Result.Author('James M. S. Wason')]","Basket trials are increasingly used for the simultaneous evaluation of a new
treatment in various patient subgroups under one overarching protocol. We
propose a Bayesian approach to sample size determination in basket trials that
permit borrowing of information between commensurate subsets. Specifically, we
consider a randomised basket trial design where patients are randomly assigned
to the new treatment or a control within each trial subset (`subtrial' for
short). Closed-form sample size formulae are derived to ensure each subtrial
has a specified chance of correctly deciding whether the new treatment is
superior to or not better than the control by some clinically relevant
difference. Given pre-specified levels of pairwise (in)commensurability, the
subtrial sample sizes are solved simultaneously. The proposed Bayesian approach
resembles the frequentist formulation of the problem in yielding comparable
sample sizes for circumstances of no borrowing. When borrowing is enabled
between commensurate subtrials, a considerably smaller trial sample size is
required compared to the widely implemented approach of no borrowing. We
illustrate the use of our sample size formulae with two examples based on real
basket trials. A comprehensive simulation study further shows that the proposed
methodology can maintain the true positive and false positive rates at desired
levels.",-0.14434043,0.0057581365,-0.18038586,A
6692,"We think that a deep study of these extended models
is an interesting point that could be addressed as a further research.","The important
bandwidth selection problem in this context could be accomplished by a cross-validation procedure
or using a Bayesian approach (Shang, 2014).","It should be noticed that the temperature curves considered in our data illustration exhibit tem-
poral dependence.",2022-05-25 08:23:27+00:00,A flexible functional-circular regression model for analyzing temperature curves,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andrea Meilán-Vila'), arxiv.Result.Author('Rosa M. Crujeiras'), arxiv.Result.Author('Mario Francisco-Fernández')]","Changes on temperature patterns, on a local scale, are perceived by
individuals as the most direct indicators of global warming and climate change.
As a specific example, for an Atlantic climate location, spring and fall
seasons should present a mild transition between winter and summer, and summer
and winter, respectively. By observing daily temperature curves along time,
being each curve attached to a certain calendar day, a regression model for
these variables (temperature curve as covariate and calendar day as response)
would be useful for modeling their relation for a certain period. In addition,
temperature changes could be assessed by prediction and observation comparisons
in the long run. Such a model is presented and studied in this work,
considering a nonparametric Nadaraya-Watson-type estimator for functional
covariate and circular response. The asymptotic bias and variance of this
estimator, as well as its asymptotic distribution are derived. Its finite
sample performance is evaluated in a simulation study and the proposal is
applied to investigate a real-data set concerning temperature curves.",-0.18733792,-0.063597426,-0.0022525638,A
6693,"We think that a deep study of these extended models
is an interesting point that could be addressed as a further research.","The important
bandwidth selection problem in this context could be accomplished by a cross-validation procedure
or using a Bayesian approach (Shang, 2014).","It should be noticed that the temperature curves considered in our data illustration exhibit tem-
poral dependence.",2022-05-25 08:23:27+00:00,A flexible functional-circular regression model for analyzing temperature curves,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andrea Meilán-Vila'), arxiv.Result.Author('Rosa M. Crujeiras'), arxiv.Result.Author('Mario Francisco-Fernández')]","Changes on temperature patterns, on a local scale, are perceived by
individuals as the most direct indicators of global warming and climate change.
As a specific example, for an Atlantic climate location, spring and fall
seasons should present a mild transition between winter and summer, and summer
and winter, respectively. By observing daily temperature curves along time,
being each curve attached to a certain calendar day, a regression model for
these variables (temperature curve as covariate and calendar day as response)
would be useful for modeling their relation for a certain period. In addition,
temperature changes could be assessed by prediction and observation comparisons
in the long run. Such a model is presented and studied in this work,
considering a nonparametric Nadaraya-Watson-type estimator for functional
covariate and circular response. The asymptotic bias and variance of this
estimator, as well as its asymptotic distribution are derived. Its finite
sample performance is evaluated in a simulation study and the proposal is
applied to investigate a real-data set concerning temperature curves.",-0.18733792,-0.063597426,-0.0022525638,A
6728,"Email: bac7wj@virginia.edu

Classification: Statistics, Applied Biological Sciences
Keywords: practical significance, statistical evidence, hypothesis testing, null hypothesis, p-
value

                                                          1
Abstract:
         With limited resources, scientific inquiries must be prioritized for further study, funding,

and translation based on their practical significance: whether the effect size is large enough to be
meaningful in the real world."," Title: The Least Difference in Means: A Statistic for Effect Size Strength and
                                    Practical Significance

        Authors: Bruce A. Corliss1,*, Yaotian Wang2, Heman Shakeri1, Philip E. Bourne1,3
Affiliations:

    1School of Data Science, University of Virginia; Charlottesville, Virginia
    2Department of Statistics, University of Pittsburgh; Pittsburgh, Pennsylvania
    3Department of Biomedical Engineering, University of Virginia; Charlottesville, Virginia
    *Corresponding author.","Doing so must evaluate a result’s effect strength, defined as a
conservative assessment of practical significance.",2022-05-24 23:37:10+00:00,The Least Difference in Means: A Statistic for Effect Size Strength and Practical Significance,stat.ME,"['stat.ME', 'q-bio.QM']","[arxiv.Result.Author('Bruce A. Corliss'), arxiv.Result.Author('Yaotian Wang'), arxiv.Result.Author('Heman Shakeri'), arxiv.Result.Author('Philip E. Bourne')]","With limited resources, scientific inquiries must be prioritized for further
study, funding, and translation based on their practical significance: whether
the effect size is large enough to be meaningful in the real world. Doing so
must evaluate a result's effect strength, defined as a conservative assessment
of practical significance. We propose the least difference in means
($\delta_L$) as a two-sample statistic that can quantify effect strength and
perform a hypothesis test to determine if a result has a meaningful effect
size. To facilitate consensus, $\delta_L$ allows scientists to compare effect
strength between related results and choose different thresholds for hypothesis
testing without recalculation. Both $\delta_L$ and the relative $\delta_L$
outperform other candidate statistics in identifying results with higher effect
strength. We use real data to demonstrate how the relative $\delta_L$ compares
effect strength across broadly related experiments. The relative $\delta_L$ can
prioritize research based on the strength of their results.",0.5523401,0.00089264696,-0.0083471425,C
6729,"Scientists
must collectively decide which interventions demonstrate practical significance and have the
most potential for further research, funding, commercialization, and translation.","A multitude of
studies have demonstrated results where different treatments have lowered cholesterol.","While there are
many factors to consider with evaluating potential (including cost of translation, difficulty of
manufacture, and likelihood of adverse side effects), effect size serves as one of the primary
benchmarks.",2022-05-24 23:37:10+00:00,The Least Difference in Means: A Statistic for Effect Size Strength and Practical Significance,stat.ME,"['stat.ME', 'q-bio.QM']","[arxiv.Result.Author('Bruce A. Corliss'), arxiv.Result.Author('Yaotian Wang'), arxiv.Result.Author('Heman Shakeri'), arxiv.Result.Author('Philip E. Bourne')]","With limited resources, scientific inquiries must be prioritized for further
study, funding, and translation based on their practical significance: whether
the effect size is large enough to be meaningful in the real world. Doing so
must evaluate a result's effect strength, defined as a conservative assessment
of practical significance. We propose the least difference in means
($\delta_L$) as a two-sample statistic that can quantify effect strength and
perform a hypothesis test to determine if a result has a meaningful effect
size. To facilitate consensus, $\delta_L$ allows scientists to compare effect
strength between related results and choose different thresholds for hypothesis
testing without recalculation. Both $\delta_L$ and the relative $\delta_L$
outperform other candidate statistics in identifying results with higher effect
strength. We use real data to demonstrate how the relative $\delta_L$ compares
effect strength across broadly related experiments. The relative $\delta_L$ can
prioritize research based on the strength of their results.",0.5936171,0.16406259,0.05269377,C
6730,"Results that are designated as practically significant
can be used to validate a scientific hypothesis or justify further research, additional funding, or
translation for a particular intervention.",4 and 5.,"This strategy not only favors interventions that may
provide greater societal benefit, but also prioritizes a deeper scientific understanding of the most
influential control mechanisms within a studied system.",2022-05-24 23:37:10+00:00,The Least Difference in Means: A Statistic for Effect Size Strength and Practical Significance,stat.ME,"['stat.ME', 'q-bio.QM']","[arxiv.Result.Author('Bruce A. Corliss'), arxiv.Result.Author('Yaotian Wang'), arxiv.Result.Author('Heman Shakeri'), arxiv.Result.Author('Philip E. Bourne')]","With limited resources, scientific inquiries must be prioritized for further
study, funding, and translation based on their practical significance: whether
the effect size is large enough to be meaningful in the real world. Doing so
must evaluate a result's effect strength, defined as a conservative assessment
of practical significance. We propose the least difference in means
($\delta_L$) as a two-sample statistic that can quantify effect strength and
perform a hypothesis test to determine if a result has a meaningful effect
size. To facilitate consensus, $\delta_L$ allows scientists to compare effect
strength between related results and choose different thresholds for hypothesis
testing without recalculation. Both $\delta_L$ and the relative $\delta_L$
outperform other candidate statistics in identifying results with higher effect
strength. We use real data to demonstrate how the relative $\delta_L$ compares
effect strength across broadly related experiments. The relative $\delta_L$ can
prioritize research based on the strength of their results.",0.5683711,0.19938283,0.09231783,C
6741,"A temporal extension of
this algorithm is possible, which motivates further research.","This study provides an idea of alternative distance weights and
distance functions that are very effective in capturing spatial variation.","This model is explained in this study
using a real-world data set of PM 2.5 concentrations in the air, but this algorithm can be used in
other scenarios such as mining, temperature modeling, meteorological modeling, and so on.",2022-05-25 19:22:24+00:00,Spatial Cluster-based Copula Model to Interpolate Skewed Conditional Spatial Random Field,stat.ME,['stat.ME'],"[arxiv.Result.Author('Debjoy Thakur'), arxiv.Result.Author('Ishapathik Das'), arxiv.Result.Author('Shubhashree Chakravarty')]","Interpolating a skewed conditional spatial random field with missing data is
cumbersome in the absence of Gaussianity assumptions. Maintaining spatial
homogeneity and continuity around the observed random spatial point is also
challenging, especially when interpolating along a spatial surface, focusing on
the boundary points as a neighborhood. Otherwise, the point far away from one
may appear the closest to another. As a result, importing the hierarchical
clustering concept on the spatial random field is as convenient as developing
the copula with the interface of the Expectation-Maximization algorithm and
concurrently utilizing the idea of the Bayesian framework. This paper
introduces a spatial cluster-based C-vine copula and a modified Gaussian kernel
to derive a novel spatial probability distribution. Another investigation in
this paper uses an algorithm in conjunction with a different parameter
estimation technique to make spatial-based copula interpolation more compatible
and efficient. We apply the proposed spatial interpolation approach to the air
pollution of Delhi as a crucial circumstantial study to demonstrate this newly
developed novel spatial estimation technique.",-0.17290622,0.14687708,0.27353954,B
6776,"28
6.5 Disadvantages of PALs and opportunities for further research

Although we have derived PALs for a fairly broad class of models, these derivations seem tied to the
various Poisson and binomial distribution assumptions appearing in the model deﬁnitions.","The estimates for each parameter appear to converge after around 4000
iterations, the total number of iterations is 7500, corresponding to 500 coordinate ascent steps with 15 iterations
of each coordinate within each step.","The PAL
approach is therefore clearly not as general as, say, inference using SMC or ABC, which in principle
require little more than the ability to simulate from the model, but which in practice may involve various
tuning parameters and incur a substantial computational cost.",2022-05-26 20:19:28+00:00,Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods,stat.ME,"['stat.ME', 'cs.LG']","[arxiv.Result.Author('Michael Whitehouse'), arxiv.Result.Author('Nick Whiteley'), arxiv.Result.Author('Lorenzo Rimella')]","Addressing the challenge of scaling-up epidemiological inference to complex
and heterogeneous models, we introduce Poisson Approximate Likelihood (PAL)
methods. In contrast to the popular ODE approach to compartmental modelling, in
which a large population limit is used to motivate a deterministic model, PALs
are derived from approximate filtering equations for finite-population,
stochastic compartmental models, and the large population limit drives the
consistency of maximum PAL estimators. Our theoretical results appear to be the
first likelihood-based parameter estimation consistency results applicable
across a broad class of partially observed stochastic compartmental models.
Compared to simulation-based methods such as Approximate Bayesian Computation
and Sequential Monte Carlo, PALs are simple to implement, involving only
elementary arithmetic operations and no tuning parameters; and fast to
evaluate, requiring no simulation from the model and having computational cost
independent of population size. Through examples, we demonstrate how PALs can
be: embedded within Delayed Acceptance Particle Markov Chain Monte Carlo to
facilitate Bayesian inference; used to fit an age-structured model of
influenza, taking advantage of automatic differentiation in Stan; and applied
to calibrate a spatial meta-population model of measles.",-0.08712724,-0.088173434,0.09520805,A
6777,"6.4 Disadvantages of PALs and opportunities for further research

Although we have derived PALs for a fairly broad class of models, these derivations seem tied to the
various Poisson and binomial distribution assumptions appearing in the model deﬁnitions.","This demonstrates how the stochastic model,

calibrated using a PAL, can be used to forecast uncertainty.","The PAL

                                                          20
Figure 5: Measles example.",2022-05-26 20:19:28+00:00,Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods,stat.ME,"['stat.ME', 'cs.LG']","[arxiv.Result.Author('Michael Whitehouse'), arxiv.Result.Author('Nick Whiteley'), arxiv.Result.Author('Lorenzo Rimella')]","Addressing the challenge of scaling-up epidemiological inference to complex
and heterogeneous models, we introduce Poisson Approximate Likelihood (PAL)
methods. PALs are derived from approximate filtering equations for
finite-population, stochastic compartmental models, and the large population
limit drives the consistency of maximum PAL estimators. Our theoretical results
appear to be the first likelihood-based parameter estimation consistency
results applicable across a broad class of partially observed stochastic
compartmental models concerning the large population limit. Compared to
simulation-based methods such as Approximate Bayesian Computation and
Sequential Monte Carlo, PALs are simple to implement, involving only elementary
arithmetic operations and no tuning parameters; and fast to evaluate, requiring
no simulation from the model and having computational cost independent of
population size. Through examples, we demonstrate how PALs can be: embedded
within Delayed Acceptance Particle Markov Chain Monte Carlo to facilitate
Bayesian inference; used to fit an age-structured model of influenza, taking
advantage of automatic differentiation in Stan; and applied to calibrate a
spatial meta-population model of measles.",-0.08192074,0.17077509,-0.013128731,A
6813,"It states
that a causal system comprises autonomous modules that do not inform or inﬂuence each other, and
it has inspired further research on integrating machine learning and causality (Schölkopf et al., 2012;
Peters et al., 2016; von Kügelgen et al., 2020; Schölkopf et al., 2021).","3 Related Work

In this work, we use the principle of Independent Causal Mechanisms (Peters et al., 2017).","Recently, Guo et al.",2022-05-27 12:20:09+00:00,Combining observational datasets from multiple environments to detect hidden confounding,stat.ME,"['stat.ME', 'cs.LG', 'stat.ML']","[arxiv.Result.Author('Rickard K. A. Karlsson'), arxiv.Result.Author('Jesse H. Krijthe')]","A common assumption in causal inference from observational data is the
assumption of no hidden confounding. Yet it is, in general, impossible to
verify the presence of hidden confounding factors from a single dataset.
However, under the assumption of independent causal mechanisms underlying the
data generative process, we demonstrate a way to detect unobserved confounders
when having multiple observational datasets coming from different environments.
We present a theory for testable conditional independencies that are only
violated during hidden confounding and examine cases where we break its
assumptions: degenerate & dependent mechanisms, and faithfulness violations.
Additionally, we propose a procedure to test these independencies and study its
empirical finite-sample behavior using simulation studies.",0.01683523,0.22281414,0.073726,B
6814,"3
Detecting hidden confounding in observational data using multiple environments  A PREPRINT

3 Related Work

This paper contributes to the growing body of research based on the principle of Independent Causal Mechanisms (Peters
et al., 2017) which has inspired further research on integrating machine learning and causality (Schölkopf et al., 2012;
Peters et al., 2016; von Kügelgen et al., 2020; Schölkopf et al., 2021).","As an example, instrumental variable estimation is applicable when there is unobserved confounding
between T and Y , but only when the relationship between the instrumental variable and T is unconfounded (Angrist et al., 1996).","Multiple works have demonstrated how the
independent causal mechanism principle could improve causal structure learning when data comes from heterogeneous
environments that share the same causal model (Zhang et al., 2017; Ghassami et al., 2018; Guo et al., 2022).",2022-05-27 12:20:09+00:00,Detecting hidden confounding in observational data using multiple environments,stat.ME,"['stat.ME', 'cs.LG', 'stat.ML']","[arxiv.Result.Author('Rickard K. A. Karlsson'), arxiv.Result.Author('Jesse H. Krijthe')]","A common assumption in causal inference from observational data is that there
is no hidden confounding. Yet it is, in general, impossible to verify the
presence of hidden confounding factors from a single dataset. Under the
assumption of independent causal mechanisms underlying the data generating
process, we demonstrate a way to detect unobserved confounders when having
multiple observational datasets coming from different environments. We present
a theory for testable conditional independencies that are only absent during
hidden confounding and examine cases where we violate its assumptions:
degenerate & dependent mechanisms, and faithfulness violations. Additionally,
we propose a procedure to test these independencies and study its empirical
finite-sample behavior using simulation studies and semi-synthetic data based
on a real-world dataset. In most cases, our theory correctly predicts the
presence of hidden confounding, particularly when the confounding bias
is~large.",0.032598518,0.32877588,-0.010713537,B
6834,"28
6 Discussion

    To effectively analyze multi-task fMRI data, experts need interpretable models with valid inference
that can facilitate interpretation and generate new hypotheses for further study.","Post-selective inference provides ad-
ditional statistical reassurance that the shared structure we identiﬁed reﬂects real patterns in the data,
rather than arising spuriously.","Our results on both
synthetic data and the fMRI data from the ABCD study show that, ﬁrst, multiple related tasks should
be analyzed together whenever possible, and second, that post-selection inference provides a rigorous
statistical approach to attaching uncertainty estimates to the results.",2022-05-27 20:21:20+00:00,Selective Inference for Sparse Multitask Regression with Applications in Neuroimaging,stat.ME,"['stat.ME', 'stat.AP', 'stat.ML']","[arxiv.Result.Author('Snigdha Panigrahi'), arxiv.Result.Author('Natasha Stewart'), arxiv.Result.Author('Chandra Sekhar Sripada'), arxiv.Result.Author('Elizaveta Levina')]","Multi-task learning is frequently used to model a set of related response
variables from the same set of features, improving predictive performance and
modeling accuracy relative to methods that handle each response variable
separately. Despite the potential of multi-task learning to yield more powerful
inference than single-task alternatives, prior work in this area has largely
omitted uncertainty quantification. Our focus in this paper is a common
multi-task problem in neuroimaging, where the goal is to understand the
relationship between multiple cognitive task scores (or other subject-level
assessments) and brain connectome data collected from imaging. We propose a
framework for selective inference to address this problem, with the flexibility
to: (i) jointly identify the relevant covariates for each task through a
sparsity-inducing penalty, and (ii) conduct valid inference in a model based on
the estimated sparsity structure. Our framework offers a new conditional
procedure for inference, based on a refinement of the selection event that
yields a tractable selection-adjusted likelihood. This gives an approximate
system of estimating equations for maximum likelihood inference, solvable via a
single convex optimization problem, and enables us to efficiently form
confidence intervals with approximately the correct coverage. Applied to both
simulated data and data from the Adolescent Cognitive Brain Development (ABCD)
study, our selective inference methods yield tighter confidence intervals than
commonly used alternatives, such as data splitting. We also demonstrate through
simulations that multi-task learning with selective inference can more
accurately recover true signals than single-task methods.",0.1116384,0.17456368,0.058140375,C
6835,"28
6 Discussion

    To effectively analyze multi-task fMRI data, experts need interpretable models with valid inference
that can facilitate interpretation and generate new hypotheses for further study.","Post-selective inference provides ad-
ditional statistical reassurance that the shared structure we identiﬁed reﬂects real patterns in the data,
rather than arising spuriously.","Our results on both
synthetic data and the fMRI data from the ABCD study show that, ﬁrst, multiple related tasks should
be analyzed together whenever possible, and second, that post-selection inference provides a rigorous
statistical approach to attaching uncertainty estimates to the results.",2022-05-27 20:21:20+00:00,Selective Inference for Sparse Multitask Regression with Applications in Neuroimaging,stat.ME,"['stat.ME', 'stat.AP', 'stat.ML']","[arxiv.Result.Author('Snigdha Panigrahi'), arxiv.Result.Author('Natasha Stewart'), arxiv.Result.Author('Chandra Sekhar Sripada'), arxiv.Result.Author('Elizaveta Levina')]","Multi-task learning is frequently used to model a set of related response
variables from the same set of features, improving predictive performance and
modeling accuracy relative to methods that handle each response variable
separately. Despite the potential of multi-task learning to yield more powerful
inference than single-task alternatives, prior work in this area has largely
omitted uncertainty quantification. Our focus in this paper is a common
multi-task problem in neuroimaging, where the goal is to understand the
relationship between multiple cognitive task scores (or other subject-level
assessments) and brain connectome data collected from imaging. We propose a
framework for selective inference to address this problem, with the flexibility
to: (i) jointly identify the relevant covariates for each task through a
sparsity-inducing penalty, and (ii) conduct valid inference in a model based on
the estimated sparsity structure. Our framework offers a new conditional
procedure for inference, based on a refinement of the selection event that
yields a tractable selection-adjusted likelihood. This gives an approximate
system of estimating equations for maximum likelihood inference, solvable via a
single convex optimization problem, and enables us to efficiently form
confidence intervals with approximately the correct coverage. Applied to both
simulated data and data from the Adolescent Cognitive Brain Development (ABCD)
study, our selective inference methods yield tighter confidence intervals than
commonly used alternatives, such as data splitting. We also demonstrate through
simulations that multi-task learning with selective inference can more
accurately recover true signals than single-task methods.",0.1116384,0.17456368,0.058140375,C
6854,"The theory developed here can be applied in other factor copula models, and this is
                                          discussed in the ﬁnal section on further research.","The 1-factor, bi-factor and oblique factor copula models are used to illustrate the theory because with their
                                          previous numerical implementations for maximum likelihood, we can make comparisons with the faster proxy-based
                                          methods introduced within.","[14] initiate the use of proxies for latent variables to speed up numerical maximum likelihood estimation; their
                                          approach involved unweighted means in 1-factor and unweighted group means for oblique factor copula models.",2022-05-28 17:01:59+00:00,High-dimensional factor copula models with estimation of latent variables,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Xinyao Fan'), arxiv.Result.Author('Harry Joe')]","Factor models are a parsimonious way to explain the dependence of variables
using several latent variables. In Gaussian 1-factor and structural factor
models (such as bi-factor, oblique factor) and their factor copula
counterparts, factor scores or proxies are defined as conditional expectations
of latent variables given the observed variables. With mild assumptions, the
proxies are consistent for corresponding latent variables as the sample size
and the number of observed variables linked to each latent variable go to
infinity. When the bivariate copulas linking observed variables to latent
variables are not assumed in advance, sequential procedures are used for latent
variables estimation, copula family selection and parameter estimation. The use
of proxy variables for factor copulas means that approximate log-likelihoods
can be used to estimate copula parameters with less computational effort for
numerical integration.",-0.09878392,0.06924369,-0.07453916,A
6855,Section 9 has a summary and discussion for further research.,"Section 8 has suﬃcient conditions for using the proxies in
the Section 3 when observed variables have weak dependence, rather than independence, conditional on the observed
variables.",2.,2022-05-28 17:01:59+00:00,High-dimensional factor copula models with estimation of latent variables,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Xinyao Fan'), arxiv.Result.Author('Harry Joe')]","Factor models are a parsimonious way to explain the dependence of variables
using several latent variables. In Gaussian 1-factor and structural factor
models (such as bi-factor, oblique factor) and their factor copula
counterparts, factor scores or proxies are defined as conditional expectations
of latent variables given the observed variables. With mild assumptions, the
proxies are consistent for corresponding latent variables as the sample size
and the number of observed variables linked to each latent variable go to
infinity. When the bivariate copulas linking observed variables to latent
variables are not assumed in advance, sequential procedures are used for latent
variables estimation, copula family selection and parameter estimation. The use
of proxy variables for factor copulas means that approximate log-likelihoods
can be used to estimate copula parameters with less computational effort for
numerical integration.",-0.10116871,-0.013994051,-0.20667769,A
6856,"Discussion and further research

    This paper proposes the conditional expectation proxies of the latent variables in some factor copula models and
shows the consistency of proxy variables under some mild conditions.",9.,"For high-dimensional factor copula models with
a large sample size (large N, large D), simulation studies show that the sequential estimation approach can eﬃciently
estimate the latent variables and select the families of linking copulas as well as estimate the copula parameters.",2022-05-28 17:01:59+00:00,High-dimensional factor copula models with estimation of latent variables,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Xinyao Fan'), arxiv.Result.Author('Harry Joe')]","Factor models are a parsimonious way to explain the dependence of variables
using several latent variables. In Gaussian 1-factor and structural factor
models (such as bi-factor, oblique factor) and their factor copula
counterparts, factor scores or proxies are defined as conditional expectations
of latent variables given the observed variables. With mild assumptions, the
proxies are consistent for corresponding latent variables as the sample size
and the number of observed variables linked to each latent variable go to
infinity. When the bivariate copulas linking observed variables to latent
variables are not assumed in advance, sequential procedures are used for latent
variables estimation, copula family selection and parameter estimation. The use
of proxy variables for factor copulas means that approximate log-likelihoods
can be used to estimate copula parameters with less computational effort for
numerical integration.",-0.20979965,0.10940978,-0.13822705,A
6857,"Topics
of further research and applications include the following.",Applications of factor copula models making use of the theory in this paper will be developed separately.,"(a) If the 1-factor structure is not adequate and group structure of observed variables cannot be determined from
context, then a p-factor structure with varimax rotation can be ﬁt to observed variables in the normal scores scale to
check if an interpretable loading matrix with many zeros, corresponding to variables in overlapping groups, can be
found.",2022-05-28 17:01:59+00:00,High-dimensional factor copula models with estimation of latent variables,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Xinyao Fan'), arxiv.Result.Author('Harry Joe')]","Factor models are a parsimonious way to explain the dependence of variables
using several latent variables. In Gaussian 1-factor and structural factor
models (such as bi-factor, oblique factor) and their factor copula
counterparts, factor scores or proxies are defined as conditional expectations
of latent variables given the observed variables. With mild assumptions, the
proxies are consistent for corresponding latent variables as the sample size
and the number of observed variables linked to each latent variable go to
infinity. When the bivariate copulas linking observed variables to latent
variables are not assumed in advance, sequential procedures are used for latent
variables estimation, copula family selection and parameter estimation. The use
of proxy variables for factor copulas means that approximate log-likelihoods
can be used to estimate copula parameters with less computational effort for
numerical integration.",0.0463896,0.018725144,0.09107079,C
6906,"For the asymptotic properties, when the model structure is known, we focus on
the estimation performance on the linear part, while the discussion of the nonlinear part is also important,
and deserves further research.","Hence, developing methods to address practical issues will be the
focus of our future work.","Also, in practice, the missing mechanism assumption is too restrictive to be
realistic.",2022-05-30 11:08:27+00:00,Partial Replacement Imputation Estimation Method for Missing Covariates in Additive Partially Linear Model,stat.ME,['stat.ME'],"[arxiv.Result.Author('Zishu Zhan'), arxiv.Result.Author('Xiangjie Li'), arxiv.Result.Author('Jingxiao Zhang')]","Missing data is a common problem in clinical data collection, which causes
difficulty in the statistical analysis of such data. In this article, we
consider the problem under a framework of a semiparametric partially linear
model when observations are subject to missingness. If the correct model
structure of the additive partially linear model is available, we propose to
use a new imputation method called Partial Replacement IMputation Estimation
(PRIME), which can overcome problems caused by incomplete data in the partially
linear model. Also, we use PRIME in conjunction with model averaging (PRIME-MA)
to tackle the problem of unknown model structure in the partially linear model.
In simulation studies, we use various error distributions, sample sizes,
missing data rates, covariate correlations, and noise levels, and PRIME
outperforms other methods in almost all cases. With an unknown correct model
structure, PRIME-MA has satisfactory performance in terms of prediction, while
slightly worse than PRIME. Moreover, we conduct a study of influential factors
in Pima Indians Diabetes data, which shows that our method performs better than
the other models.",-0.25889218,-0.12882248,-0.25442386,A
6907,"For the asymptotic properties, when the model structure is known, we focus on
the estimation performance on the linear part, while the discussion of the nonlinear part is also important,
and deserves further research.","Hence, developing methods to address practical issues will be the
focus of our future work.","Also, in practice, the missing mechanism assumption is too restrictive to be
realistic.",2022-05-30 11:08:27+00:00,Partial Replacement Imputation Estimation Method for Complex Missing Covariates in Additive Partially Linear Models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Zishu Zhan'), arxiv.Result.Author('Xiangjie Li'), arxiv.Result.Author('Jingxiao Zhang')]","Missing data is a common problem in clinical data collection, which causes
difficulty in the statistical analysis of such data. In this article, we
consider the problem under a framework of a semiparametric partially linear
model when observations are subject to missingness with complex patterns. If
the correct model structure of the additive partially linear model is
available, we propose to use a new imputation method called Partial Replacement
IMputation Estimation (PRIME), which can overcome problems caused by incomplete
data in the partially linear model. Also, we use PRIME in conjunction with
model averaging (PRIME-MA) to tackle the problem of unknown model structure in
the partially linear model. In simulation studies, we use various error
distributions, sample sizes, missing data rates, covariate correlations, and
noise levels, and PRIME outperforms other methods in almost all cases. With an
unknown correct model structure, PRIME-MA has satisfactory performance in terms
of prediction, while slightly worse than PRIME. Moreover, we conduct a study of
influential factors in Pima Indians Diabetes data, which shows that our method
performs better than the other models.",-0.25889218,-0.12882248,-0.25442386,A
6908,This is a topic for further research.,"The estimator (5) emulates a standard one-period trial, while it is unclear what we are emulating by ignoring
the period speciﬁc treatment eﬀects [8].",Two other theoretical issues also deserve more attention.,2022-05-30 11:10:11+00:00,Unbiased and Efficient Estimation of Causal Treatment Effects in Cross-over Trials,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Jeppe Ekstrand Halkjær Madsen'), arxiv.Result.Author('Thomas Scheike'), arxiv.Result.Author('Christian Pipper')]","We introduce causal inference reasoning to cross-over trials, with a focus on
Thorough QT (TQT) studies. For such trials, we propose different sets of
assumptions and consider their impact on the modelling strategy and estimation
procedure. We show that unbiased estimates of a causal treatment effect are
obtained by a G-computation approach in combination with weighted least squares
predictions from a working regression model. Only a few natural requirements on
the working regression and weighting matrix are needed for the result to hold.
It follows that a large class of Gaussian linear mixed working models lead to
unbiased estimates of a causal treatment effect, even if they do not capture
the true data generating mechanism. We compare a range of working regression
models in a simulation study where data are simulated from a complex data
generating mechanism with input parameters estimated on a real TQT data set. In
this setting, we find that for all practical purposes working models adjusting
for baseline QTc measurements have comparable performance. Specifically, this
is observed for working models that are by default too simplistic to capture
the true data generating mechanism. Cross-over trials and particularly TQT
studies can be analysed efficiently using simple working regression models
without biasing the estimates for the causal parameters of interest.",0.23544267,0.1127538,-0.3386707,C
7000,Some limitations of our work and challenges for further research are the following.,"We have also shown that existing methods can be reframed into ﬁltering and backward
recursions: in this case, we found that more ﬂexible updates are available without increasing the computational
workload.","• Our theoretical result sheds light on important properties of sequential variational methods, but the
       assumptions involved are not fully constructive, i.e.",2022-06-01 08:35:54+00:00,Amortized backward variational inference in nonlinear state-space models,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Mathis Chagneux'), arxiv.Result.Author('Élisabeth Gassiat'), arxiv.Result.Author('Pierre Gloaguen'), arxiv.Result.Author('Sylvain Le Corff')]","We consider the problem of state estimation in general state-space models
using variational inference. For a generic variational family defined using the
same backward decomposition as the actual joint smoothing distribution, we
establish for the first time that, under mixing assumptions, the variational
approximation of expectations of additive state functionals induces an error
which grows at most linearly in the number of observations. This guarantee is
consistent with the known upper bounds for the approximation of smoothing
distributions using standard Monte Carlo methods. Moreover, we propose an
amortized inference framework where a neural network shared over all times
steps outputs the parameters of the variational kernels. We also study
empirically parametrizations which allow analytical marginalization of the
variational distributions, and therefore lead to efficient smoothing
algorithms. Significant improvements are made over state-of-the art variational
solutions, especially when the generative model depends on a strongly nonlinear
and noninjective mixing function.",-0.2682088,-0.1475969,0.106758825,A
7334,"Fifth, further research into the theoretical properties of the permutation test is warranted.","Estimated
risks above one may result from extreme weights, which is also indicative of violations of assumptions, such
as positivity.","Lastly, the estimation approach relied on parametric or semiparametric models to estimate the corresponding
nuisance parameters.",2022-06-09 12:14:09+00:00,Bridged treatment comparisons: an illustrative application in HIV treatment,stat.ME,['stat.ME'],"[arxiv.Result.Author('Paul N Zivich'), arxiv.Result.Author('Stephen R Cole'), arxiv.Result.Author('Jessie K Edwards'), arxiv.Result.Author('Bonnie E Shook-Sa'), arxiv.Result.Author('Alexander Breskin'), arxiv.Result.Author('Michael G Hudgens')]","Comparisons of treatments or exposures are of central interest in
epidemiology, but direct comparisons are not always possible due to practical
or ethical reasons. Here, we detail a fusion approach to compare treatments
across studies. The motivating example entails comparing the risk of the
composite outcome of death, AIDS, or greater than a 50% CD4 cell count decline
in people with HIV when assigned triple versus mono antiretroviral therapy,
using data from the AIDS Clinical Trial Group (ACTG) 175 (mono versus dual
therapy) and ACTG 320 (dual versus triple therapy). We review a set of
identification assumptions and estimate the risk difference using an inverse
probability weighting estimator that leverages the shared trial arms (dual
therapy). A fusion diagnostic based on comparing the shared arms is proposed
that may indicate violation of the identification assumptions. Application of
the data fusion estimator and diagnostic to the ACTG trials indicates triple
therapy results in a reduction in risk compared to monotherapy in individuals
with baseline CD4 counts between 50 and 300 cells/mm$^3$. Bridged treatment
comparisons address questions that none of the constituent data sources could
address alone, but valid fusion-based inference requires careful consideration.",0.104224056,-0.119635195,-0.057946984,C
7712,"sampling, thus motivating further research along this line.","(2022), Table 2
provides evidence on the fact that the number of iterations needed by pfm–vb to reach convergence
of the elbo goes to one as p grows to inﬁnity, while also displaying the phenomenon reminiscent of
double–descent noticed in Table 1 for i.i.d.","Interestingly, the empirical results in Table 2 also suggest that the number of iterations require by ep
does not grow with p. These analyses point toward ep as a default strategy, while suggesting pfm–vb
as a valuable alternative in high–dimensional situations where ep is computationally impractical.",2022-06-16 12:24:27+00:00,"Bayesian conjugacy in probit, tobit, multinomial probit and extensions: A review and new results",stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author(""Niccolo' Anceschi""), arxiv.Result.Author('Augusto Fasano'), arxiv.Result.Author('Daniele Durante'), arxiv.Result.Author('Giacomo Zanella')]","A broad class of models that routinely appear in several fields can be
expressed as partially or fully discretized Gaussian linear regressions.
Besides including basic Gaussian response settings, this class also encompasses
probit, multinomial probit and tobit regression, among others, thereby yielding
to one of the most widely-implemented families of models in applications. The
relevance of such representations has stimulated decades of research in the
Bayesian field, mostly motivated by the fact that, unlike for Gaussian linear
regression, the posterior distribution induced by such models does not
apparently belong to a known class, under the commonly-assumed Gaussian priors
for the coefficients. This has motivated several solutions for posterior
inference relying on sampling-based strategies or on deterministic
approximations that, however, still experience computational and accuracy
issues, especially in high dimensions. The scope of this article is to review,
unify and extend recent advances in Bayesian inference and computation for this
class of models. To address such a goal, we prove that the likelihoods induced
by these formulations share a common analytical structure that implies
conjugacy with a broad class of distributions, namely the unified skew-normals
(SUN), that generalize Gaussians to skewed contexts. This result unifies and
extends recent conjugacy properties for specific models within the class
analyzed, and opens avenues for improved posterior inference, under a broader
class of formulations and priors, via novel closed-form expressions, i.i.d.
samplers from the exact SUN posteriors, and more accurate and scalable
approximations from VB and EP. Such advantages are illustrated in simulations
and are expected to facilitate the routine-use of these core Bayesian models,
while providing a novel framework to study theoretical properties and develop
future extensions.",-0.04552535,-0.23805514,0.077109165,A
7713,"Due to the relevance of the regression models considered within the present article, such a review

                                                               30
is expected to catalyze increasing interest by applied, computational and methodological researchers,
and will hopefully motivate further research advancements along the directions opened by the results
in Sections 2–5.","sampling schemes, mean–ﬁeld variational Bayes, partially–factorized variational Bayes
and novel scalable implementations of expectation–propagation.","For instance, the closed–form expressions in Section 3.2 for inference under the ex-
act sun posterior provide additional motivations to stimulate ongoing research aimed at developing
accurate and fast methods to evaluate cumulative distribution functions of high–dimensional Gaus-
sian distributions.",2022-06-16 12:24:27+00:00,"Bayesian conjugacy in probit, tobit, multinomial probit and extensions: A review and new results",stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author(""Niccolo' Anceschi""), arxiv.Result.Author('Augusto Fasano'), arxiv.Result.Author('Daniele Durante'), arxiv.Result.Author('Giacomo Zanella')]","A broad class of models that routinely appear in several fields can be
expressed as partially or fully discretized Gaussian linear regressions.
Besides including basic Gaussian response settings, this class also encompasses
probit, multinomial probit and tobit regression, among others, thereby yielding
to one of the most widely-implemented families of models in applications. The
relevance of such representations has stimulated decades of research in the
Bayesian field, mostly motivated by the fact that, unlike for Gaussian linear
regression, the posterior distribution induced by such models does not
apparently belong to a known class, under the commonly-assumed Gaussian priors
for the coefficients. This has motivated several solutions for posterior
inference relying on sampling-based strategies or on deterministic
approximations that, however, still experience computational and accuracy
issues, especially in high dimensions. The scope of this article is to review,
unify and extend recent advances in Bayesian inference and computation for this
class of models. To address such a goal, we prove that the likelihoods induced
by these formulations share a common analytical structure that implies
conjugacy with a broad class of distributions, namely the unified skew-normals
(SUN), that generalize Gaussians to skewed contexts. This result unifies and
extends recent conjugacy properties for specific models within the class
analyzed, and opens avenues for improved posterior inference, under a broader
class of formulations and priors, via novel closed-form expressions, i.i.d.
samplers from the exact SUN posteriors, and more accurate and scalable
approximations from VB and EP. Such advantages are illustrated in simulations
and are expected to facilitate the routine-use of these core Bayesian models,
while providing a novel framework to study theoretical properties and develop
future extensions.",-0.20973417,0.030270651,-0.17235456,A
7717,There are several important avenues for further research.,"As demonstrated by both simulations
and the case study, this elevated modeling ﬂexibility gave rise to substantially better variable selection results, particularly
in terms of identifying predictors of complex functional forms, compared to existing variable selection methods suitable for
clustered survival data.","First, the riAFT-BART model could be extended to include the
random slopes and accommodate the cluster-level covariates.",2022-06-16 16:08:41+00:00,A new tool for clustered survival data and multiple treatments: Estimation of treatment effect heterogeneity and variable selection,stat.ME,"['stat.ME', 'stat.AP']",[arxiv.Result.Author('Liangyuan Hu')],"A new tool, riAFT-BART, was recently developed to draw causal inferences
about population treatment effect on patient survival from clustered and
censored survival data while accounting for the multilevel data structure. The
practical utility of this tool goes beyond the estimation of population average
treatment effect. In this work, we exposit how riAFT-BART can be used to solve
two important statistical questions with clustered survival data: estimating
the treatment effect heterogeneity and variable selection. Leveraging the
likelihood-based machine learning, we describe a way in which we can draw
posterior samples of the individual survival treatment effect from riAFT-BART
model runs, and use the drawn posterior samples to perform an exploratory
treatment effect heterogeneity analysis to identify subpopulations who may
experience differential treatment effects than population average effects. We
propose a permutation based approach using the predictor's variable inclusion
proportion supplied by the riAFT-BART model for variable selection. To address
the missing data issue frequently encountered in health databases, we propose a
strategy to combine bootstrap imputation and riAFT-BART for variable selection
among incomplete clustered survival data. We conduct an expansive simulation
study to examine the practical operating characteristics of our proposed
methods, and provide empirical evidence that our proposed methods perform
better than several existing methods across a wide range of data scenarios.
Finally, we demonstrate the methods via a case study of predictors for
in-hospital mortality among severe COVID-19 patients and estimating the
heterogeneous treatment effects of three COVID-specific medications. The
methods developed in this work are readily available in the R package
$\textsf{riAFTBART}$.",0.061533295,0.00935176,-0.0031933459,C
7842,"advantages of the proposed semi-parametric method outside of the Gaussian model is beyond the scope of our study
and is left for further research.","However, the inferential

                                                                 12
                                         GMWMX-1  GMWMX-2

                                      b  c1                d1

               1.30

               1.25

Ratio of RMSE  1.20

               1.15

               1.10

               1.05

               1.00
                         7.5 y 10.0 y 15.0 y 20.0 y 7.5 y 10.0 y 15.0 y 20.0 y 7.5 y 10.0 y 15.0 y 20.0 y

Figure 4: Ratio of the estimated RMSE of the GMWMX-1 and GMWM-2 compared to the MLE for the functional
parameters b, c1 and d1 as a function of the sample size.",4.2.,2022-06-20 09:17:35+00:00,The Generalized Method of Wavelet Moments with Exogenous Inputs: a Fast Approach for the Analysis of GNSS Position Time Series,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Davide A. Cucci'), arxiv.Result.Author('Lionel Voirol'), arxiv.Result.Author('Gaël Kermarrec'), arxiv.Result.Author('Jean-Philippe Montillet'), arxiv.Result.Author('Stéphane Guerrier')]","The Global Navigation Satellite System (GNSS) daily position time series are
often described as the sum of stochastic processes and geophysical signals
which allow studying global and local geodynamical effects such as plate
tectonics, earthquakes, or ground water variations. In this work we propose to
extend the Generalized Method of Wavelet Moments (GMWM) to estimate the
parameters of linear models with correlated residuals. This statistical
inferential framework is applied to GNSS daily position time series data to
jointly estimate functional (geophysical) as well as stochastic noise models.
Our method is called GMWMX, with X standing for eXogeneous variable: it is
semi-parametric, computationally efficient and scalable. Unlike standard
methods such as the widely used Maximum Likelihood Estimator (MLE), our
methodology offers statistical guarantees, such as consistency and asymptotic
normality, without relying on strong parametric assumptions. At the Gaussian
model, our results show that the estimated parameters are similar to the ones
obtained with the MLE. The computational performances of our approach has
important practical implications. Indeed, the estimation of the parameters of
large networks of thousands of GNSS stations quickly becomes computationally
prohibitive. Compared to standard methods, the processing time of the GMWMX is
over $1000$ times faster and allows the estimation of large scale problems
within minutes on a standard computer. We validate the performances of our
method via Monte-Carlo simulations by generating GNSS daily position time
series with missing observations and we consider composite stochastic noise
models including processes presenting long-range dependence such as power-law
or Mat\'ern processes. The advantages of our method are also illustrated using
real time series from GNSS stations located in the Eastern part of the USA.",-0.11800021,-0.0061809625,-0.028646987,A
8002,"We further study this setting with a larger sample size, n “ 2, 000.","The simulation results are summarized in Supplementary

Materials Table C.11.","This

sample size is closer to the sample size adopted in our case study I.",2022-06-23 17:25:29+00:00,Inference on the Best Policies with Many Covariates,stat.ME,['stat.ME'],"[arxiv.Result.Author('Waverly Wei'), arxiv.Result.Author('Yuqing Zhou'), arxiv.Result.Author('Zeyu Zheng'), arxiv.Result.Author('Jingshen Wang')]","Understanding the impact of the most effective policies or treatments on a
response variable of interest is desirable in many empirical works in
economics, statistics and other disciplines. Due to the widespread winner's
curse phenomenon, conventional statistical inference assuming that the top
policies are chosen independent of the random sample may lead to overly
optimistic evaluations of the best policies. In recent years, given the
increased availability of large datasets, such an issue can be further
complicated when researchers include many covariates to estimate the policy or
treatment effects in an attempt to control for potential confounders. In this
manuscript, to simultaneously address the above-mentioned issues, we propose a
resampling-based procedure that not only lifts the winner's curse in evaluating
the best policies observed in a random sample, but also is robust to the
presence of many covariates. The proposed inference procedure yields accurate
point estimates and valid frequentist confidence intervals that achieve the
exact nominal level as the sample size goes to infinity for multiple best
policy effect sizes. We illustrate the finite-sample performance of our
approach through Monte Carlo experiments and two empirical studies, evaluating
the most effective policies in charitable giving and the most beneficial group
of workers in the National Supported Work program.",0.109333485,-0.22300798,-0.020877562,C
8003,"We further study this setting with a larger sample size, n “ 2, 000.","The simulation results are summarized in Supplementary

Materials Table C.11.","This

sample size is closer to the sample size adopted in our case study I.",2022-06-23 17:25:29+00:00,Inference on the Best Policies with Many Covariates,stat.ME,['stat.ME'],"[arxiv.Result.Author('Waverly Wei'), arxiv.Result.Author('Yuqing Zhou'), arxiv.Result.Author('Zeyu Zheng'), arxiv.Result.Author('Jingshen Wang')]","Understanding the impact of the most effective policies or treatments on a
response variable of interest is desirable in many empirical works in
economics, statistics and other disciplines. Due to the widespread winner's
curse phenomenon, conventional statistical inference assuming that the top
policies are chosen independent of the random sample may lead to overly
optimistic evaluations of the best policies. In recent years, given the
increased availability of large datasets, such an issue can be further
complicated when researchers include many covariates to estimate the policy or
treatment effects in an attempt to control for potential confounders. In this
manuscript, to simultaneously address the above-mentioned issues, we propose a
resampling-based procedure that not only lifts the winner's curse in evaluating
the best policies observed in a random sample, but also is robust to the
presence of many covariates. The proposed inference procedure yields accurate
point estimates and valid frequentist confidence intervals that achieve the
exact nominal level as the sample size goes to infinity for multiple best
policy effect sizes. We illustrate the finite-sample performance of our
approach through Monte Carlo experiments and two empirical studies, evaluating
the most effective policies in charitable giving and the most beneficial group
of workers in the National Supported Work program.",0.109333485,-0.22300798,-0.020877562,C
8087,"This dimensionality

                                                     22
blessing for SFD deserves further study.","First, SFD performs better with increasing dimensions, regard-
less of whether the covariance matrix is identity or non-identity.","Second, for all the competitors, the perfor-
mances are worse when the components of i are correlated, and thus, we consider
the case with magnitudes being 1.",2022-06-27 00:57:24+00:00,Two ridge ratio criteria for multiple change point detection in tensors,stat.ME,['stat.ME'],"[arxiv.Result.Author('Jiaqi Huang'), arxiv.Result.Author('Junhui Wang'), arxiv.Result.Author('Xuehu Zhu'), arxiv.Result.Author('Lixing Zhu')]","This paper proposes two novel criteria for detecting change structures in
tensor data. To measure the difference between any two adjacent tensors and to
handle both dense and sparse model structures of the tensors, we define a
signal-screening averaged Frobenius distance for the moving sums of tensor data
and a signal-screening mode-based Frobenius distance for the moving sums of
slices of tensor data. The latter is particularly useful when some mode is not
suitable to be included in the Frobenius distance. Based on these two
sequences, we construct two signal statistics using the ratios with
adaptive-to-change ridge functions respectively, to enhance the detection
capacity of the criteria. The estimated number of changes and their estimated
locations are consistent to the corresponding true quantities in certain
senses. The results hold when the size of the tensor and the number of change
points diverge at certain rates, respectively. Numerical studies are conducted
to examine the finite sample performances of the proposed methods. We also
analyze two real data examples for illustration.",0.011529189,-0.19839148,0.23049013,A
8092,"Further extensions to the case where the expert
information reﬁnes with the sample size, and the introduction of covariates are both
of great signiﬁcance and subject of further study.",sequence of random elements.,"In that context, triangular arrays
of row-wise independent random measures will need to be considered.",2022-06-27 07:52:14+00:00,Informed censoring: the parametric combination of data and expert information,stat.ME,['stat.ME'],"[arxiv.Result.Author('Hansjörg Albrecher'), arxiv.Result.Author('Martin Bladt')]","The statistical censoring setup is extended to the situation when random
measures can be assigned to the realization of datapoints, leading to a new way
of incorporating expert information into the usual parametric estimation
procedures. The asymptotic theory is provided for the resulting estimators, and
some special cases of practical relevance are studied in more detail. Although
the proposed framework mathematically generalizes censoring and coarsening at
random, and borrows techniques from M-estimation theory, it provides a novel
and transparent methodology which enjoys significant practical applicability in
situations where expert information is present. The potential of the approach
is illustrated by a concrete actuarial application of tail parameter estimation
for a heavy-tailed MTPL dataset with limited available expert information.",-0.05633824,-0.014046885,-0.0025271163,A
8093,"Further extensions to the case where the expert
information reﬁnes with the sample size, and the introduction of covariates are both
of great signiﬁcance and subject of further study.",sequence of random elements.,"In that context, triangular arrays
of row-wise independent random measures will need to be considered.",2022-06-27 07:52:14+00:00,Informed censoring: the parametric combination of data and expert information,stat.ME,['stat.ME'],"[arxiv.Result.Author('Hansjörg Albrecher'), arxiv.Result.Author('Martin Bladt')]","The statistical censoring setup is extended to the situation when random
measures can be assigned to the realization of datapoints, leading to a new way
of incorporating expert information into the usual parametric estimation
procedures. The asymptotic theory is provided for the resulting estimators, and
some special cases of practical relevance are studied in more detail. Although
the proposed framework mathematically generalizes censoring and coarsening at
random, and borrows techniques from M-estimation theory, it provides a novel
and transparent methodology which enjoys significant practical applicability in
situations where expert information is present. The potential of the approach
is illustrated by a concrete actuarial application of tail parameter estimation
for a heavy-tailed MTPL dataset with limited available expert information.",-0.05633824,-0.014046885,-0.0025271163,A
8096,"Adapting the aforementioned results to
encompass arrays of M-type estimators with random measures remains a promising
line of further research.","θPΘ n
                                             W
                                     i“1 i

For integrands not depending on n, these types of estimators are well-understood
using the theory of informed censoring outlined in [6], which builds on the theory
of M-type estimators, confer with [21].",Example 4.2.,2022-06-27 08:52:25+00:00,Expert Kaplan--Meier estimation,stat.ME,['stat.ME'],"[arxiv.Result.Author('Martin Bladt'), arxiv.Result.Author('Christian Furrer')]","The setting of a right-censored random sample subject to contamination is
considered. In various fields, expert information is often available and used
to overcome the contamination. This paper integrates expert knowledge into the
product-limit estimator in two different ways with distinct interpretations.
Strong uniform consistency is proved for both cases under certain assumptions
on the kind of contamination and the quality of expert information, which sheds
light on the techniques and decisions that practitioners may take. The nuances
of the techniques are discussed -- also with a view towards semi-parametric
estimation -- and they are illustrated using simulated and real-world insurance
data.",-0.14228034,0.052969493,-0.3809147,A
8097,"Appropriate kernel choices (and their calibration) to capture
appropriate tail behaviors is a subject of further research.","Note that for very large durations, which are
EXPERT KAPLAN–MEIER ESTIMATION  23

maybe of lesser importance for our application, but could be of interest in other
domains, the sophisticated survival curves seem to wean oﬀ sharply as a consequence
of the choice of kernel.",Figure 5.,2022-06-27 08:52:25+00:00,Expert Kaplan--Meier estimation,stat.ME,['stat.ME'],"[arxiv.Result.Author('Martin Bladt'), arxiv.Result.Author('Christian Furrer')]","The setting of a right-censored random sample subject to contamination is
considered. In various fields, expert information is often available and used
to overcome the contamination. This paper integrates expert knowledge into the
product-limit estimator in two different ways with distinct interpretations.
Strong uniform consistency is proved for both cases under certain assumptions
on the kind of contamination and the quality of expert information, which sheds
light on the techniques and decisions that practitioners may take. The nuances
of the techniques are discussed -- also with a view towards semi-parametric
estimation -- and they are illustrated using simulated and real-world insurance
data.",-0.046873238,-0.010721562,-0.049468957,A
8160,"We reveal the outliers of four datasets; as a necessary
                                                       step for further research.","For
                                                       detecting outlier genes in a given gene expression dataset, we propose an
                                                       analytical procedure and based on the Tukey’s concept of outlier and the
                                                       notion of statistical depth, as previous methodologies lead to unassertive
                                                       and wrongful outliers.","Keywords and phrases: RNA sequencing, DNA Microarrays, Functional
                                                       Data Analysis, Normalization, Outlier Detection, Statistical Data Depth,
                                                       Statistical Functional Depth.",2022-06-28 12:02:05+00:00,Statistical Depth based Normalization and Outlier Detection of Gene Expression Data,stat.ME,['stat.ME'],"[arxiv.Result.Author('Alicia Nieto-Reyes'), arxiv.Result.Author('Javier Cabrera')]","Normalization and outlier detection belong to the preprocessing of gene
expression data. We propose a natural normalization procedure based on
statistical data depth which normalizes to the distribution of gene expressions
of the most representative gene expression of the group. This differ from the
standard method of quantile normalization, based on the coordinate-wise median
array that lacks of the well-known properties of the one-dimensional median.
The statistical data depth maintains those good properties. Gene expression
data are known for containing outliers. Although detecting outlier genes in a
given gene expression dataset has been broadly studied, these methodologies do
not apply for detecting outlier samples, given the difficulties posed by the
high dimensionality but low sample size structure of the data. The standard
procedures used for detecting outlier samples are visual and based on dimension
reduction techniques; instances are multidimensional scaling and spectral map
plots. For detecting outlier genes in a given gene expression dataset, we
propose an analytical procedure and based on the Tukey's concept of outlier and
the notion of statistical depth, as previous methodologies lead to unassertive
and wrongful outliers. We reveal the outliers of four datasets; as a necessary
step for further research.",0.06560129,-0.091604024,0.17607838,C
8197,"Exploring the impact of using the proposed parametric bootstrap SE estimators
in these more complex settings would be an interesting area of further research.","In particular, these approaches can be directly
applied for other eﬀect measures (e.g., the standardized diﬀerence of means, ratio of means) and in more
general meta-analytic settings (e.g., multivariate meta-analysis, network meta-analysis, multi-level meta-
analysis, meta-regression).",We considered some variations of the parametric bootstrap SE estimators in preliminary analyses.,2022-06-29 03:34:32+00:00,Standard error estimation in meta-analysis of studies reporting medians,stat.ME,['stat.ME'],"[arxiv.Result.Author('Sean McGrath'), arxiv.Result.Author('Stephan Katzenschlager'), arxiv.Result.Author('Alexandra J. Zimmer'), arxiv.Result.Author('Alexander Seitel'), arxiv.Result.Author('Russell Steele'), arxiv.Result.Author('Andrea Benedetti')]","We consider the setting of an aggregate data meta-analysis of a continuous
outcome of interest. When the distribution of the outcome is skewed, it is
often the case that some primary studies report the sample mean and standard
deviation of the outcome and other studies report the sample median along with
the first and third quartiles and/or minimum and maximum values. To perform
meta-analysis in this context, a number of approaches have recently been
developed to impute the sample mean and standard deviation from studies
reporting medians. Then, standard meta-analytic approaches with
inverse-variance weighting are applied based on the (imputed) study-specific
sample means and standard deviations. In this paper, we illustrate how this
common practice can severely underestimate the within-study standard errors,
which results in overestimation of between-study heterogeneity in random
effects meta-analyses. We propose a straightforward bootstrap approach to
estimate the standard errors of the imputed sample means. Our simulation study
illustrates how the proposed approach can improve estimation of the
within-study standard errors and between-study heterogeneity. Moreover, we
apply the proposed approach in a meta-analysis to identify risk factors of a
severe course of COVID-19.",0.22482857,0.08265371,-0.1717158,C
8198,"The proposed parametric bootstrap SE estimators are of course applicable to other transformation-based
approaches, which may be another interesting avenue of further research.","Since these modiﬁcations did not
clearly improve the performance of the estimator in our simulations, we did not include them for parsimony.","We focused on the methods of
McGrath et al.",2022-06-29 03:34:32+00:00,Standard error estimation in meta-analysis of studies reporting medians,stat.ME,['stat.ME'],"[arxiv.Result.Author('Sean McGrath'), arxiv.Result.Author('Stephan Katzenschlager'), arxiv.Result.Author('Alexandra J. Zimmer'), arxiv.Result.Author('Alexander Seitel'), arxiv.Result.Author('Russell Steele'), arxiv.Result.Author('Andrea Benedetti')]","We consider the setting of an aggregate data meta-analysis of a continuous
outcome of interest. When the distribution of the outcome is skewed, it is
often the case that some primary studies report the sample mean and standard
deviation of the outcome and other studies report the sample median along with
the first and third quartiles and/or minimum and maximum values. To perform
meta-analysis in this context, a number of approaches have recently been
developed to impute the sample mean and standard deviation from studies
reporting medians. Then, standard meta-analytic approaches with
inverse-variance weighting are applied based on the (imputed) study-specific
sample means and standard deviations. In this paper, we illustrate how this
common practice can severely underestimate the within-study standard errors,
which results in overestimation of between-study heterogeneity in random
effects meta-analyses. We propose a straightforward bootstrap approach to
estimate the standard errors of the imputed sample means. Our simulation study
illustrates how the proposed approach can improve estimation of the
within-study standard errors and between-study heterogeneity. Moreover, we
apply the proposed approach in a meta-analysis to identify risk factors of a
severe course of COVID-19.",-0.14435121,-0.06500526,-0.16499925,A
8266,"More types of higher-order motifs

                                                        26
and the clustering in directed networks deserve further research.","Besides, we considered triangles in undirected networks.","Finally, we take a step
forward on understanding how well the higher-order spectral clustering works, and there is
still a long way to understand it thoroughly.",2022-06-30 15:54:15+00:00,On the efficacy of higher-order spectral clustering under weighted stochastic block models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Xiao Guo'), arxiv.Result.Author('Hai Zhang'), arxiv.Result.Author('Xiangyu Chang')]","Higher-order structures of networks, namely, small subgraphs of networks
(also called network motifs), are widely known to be crucial and essential to
the organization of networks. There has been a few work studying the community
detection problem\textendash a fundamental problem in network analysis, at the
level of motifs. In particular, higher-order spectral clustering has been
developed, where the notion of \emph{motif adjacency matrix} is introduced as
the input of the algorithm. However, it remains largely unknown that how
higher-order spectral clustering works and when it performs better than its
edge-based counterpart. To elucidate these problems, we investigate
higher-order spectral clustering from a statistical perspective. In particular,
we theoretically study the clustering performance of higher-order spectral
clustering under a {\it weighted stochastic block model} and compare the
resulting bounds with the corresponding results of edge-based spectral
clustering. It turns out that when the network is dense with weak signal of
weights, higher-order spectral clustering can really lead to the performance
gain in clustering. We also use simulations and real data experiments to
support the findings.",-0.09891408,-0.35785276,0.43807897,A
8390,"The introduction of covariates into our joint distribution is a subject of further research
which would cast the model into a fully applicable pricing tool for actuaries.","A standard 0.95 conﬁdence-interval is given
by [25307.45, 25535.35], which includes the prediction from the joint model but not the
prediction from the independent model.",4.2.,2022-07-04 12:38:05+00:00,Joint discrete and continuous matrix distribution modelling,stat.ME,['stat.ME'],"[arxiv.Result.Author('Martin Bladt'), arxiv.Result.Author('Clara Brimnes Gardner')]","In this paper we introduce a bivariate distribution on $\mathbb{R}_{+} \times
\mathbb{N}$ arising from a single underlying Markov jump process. The marginal
distributions are phase-type and discrete phase-type distributed, respectively,
which allow for flexible behavior for modeling purposes. We show that the
distribution is dense in the class of distributions on $\mathbb{R}_{+} \times
\mathbb{N}$ and derive some of its main properties, all explicit in terms of
matrix calculus. Furthermore, we develop an effective EM algorithm for the
statistical estimation of the distribution parameters. In the last part of the
paper, we apply our methodology to an insurance dataset, where we model the
number of claims and the mean claim sizes of policyholders, which is seen to
perform favorably. An additional consequence of the latter analysis is that the
total loss size in the entire portfolio is captured substantially better than
with independent phase-type models.",-0.012622297,0.20521037,-0.16384575,A
8642,"As a further study, we simulate 1000 samples from a cubit model log(pj/pj+1) = αj +
β1X + β2X2 + β3X3, where (α0, α1, α2, α3) = (−1, 1.5, 2, 3), (β1, β2, β3) = (2, −1, −1.5), and
X ∼ N (0, 1).","But it may be confusing if we compare this plot with Figure 3(d)
as ”U”-shaped curves are also seen in the latter.","Assuming that the quadratic term is included in the working model while the
cubit term is not, we cast its functional residuals to the normal scale in Figure 6.",2022-07-09 17:00:18+00:00,Model diagnostics of discrete data regression: a unifying framework using functional residuals,stat.ME,"['stat.ME', 'econ.EM']","[arxiv.Result.Author('Zewei Lin'), arxiv.Result.Author('Dungang Liu')]","Model diagnostics is an indispensable component of regression analysis, yet
it is not well addressed in standard textbooks on generalized linear models.
The lack of exposition is attributed to the fact that when outcome data are
discrete, classical methods (e.g., Pearson/deviance residual analysis and
goodness-of-fit tests) have limited utility in model diagnostics and treatment.
This paper establishes a novel framework for model diagnostics of discrete data
regression. Unlike the literature defining a single-valued quantity as the
residual, we propose to use a function as a vehicle to retain the residual
information. In the presence of discreteness, we show that such a functional
residual is appropriate for summarizing the residual randomness that cannot be
captured by the structural part of the model. We establish its theoretical
properties, which leads to the innovation of new diagnostic tools including the
functional-residual-vs covariate plot and Function-to-Function (Fn-Fn) plot.
Our numerical studies demonstrate that the use of these tools can reveal a
variety of model misspecifications, such as not properly including a
higher-order term, an explanatory variable, an interaction effect, a dispersion
parameter, or a zero-inflation component. The functional residual yields, as a
byproduct, Liu-Zhang's surrogate residual mainly developed for cumulative link
models for ordinal data (Liu and Zhang, 2018, JASA). As a general notion, it
considerably broadens the diagnostic scope as it applies to virtually all
parametric models for binary, ordinal and count data, all in a unified
diagnostic scheme.",-0.055500608,-0.3335672,0.054525536,A
8643,"While beyond the scope of this paper, it warrants
further research.","It is interesting to explore whether the construction method for linear regression in this
paper, that is, using simultaneous conﬁdence bands to construct conﬁdence sets, could be
extended to nonparametric regression.","5 Appendix

In this appendix a proof of the Theorem in Section 2 is sketched.",2022-07-09 17:01:46+00:00,Confidence Sets for a level set in linear regression,stat.ME,['stat.ME'],"[arxiv.Result.Author('Fang Wan'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Frank Bretz')]","Regression modeling is the workhorse of statistics and there is a vast
literature on estimation of the regression function. It is realized in recent
years that in regression analysis the ultimate aim may be the estimation of a
level set of the regression function, instead of the estimation of the
regression function itself. The published work on estimation of the level set
has thus far focused mainly on nonparametric regression, especially on point
estimation. In this paper, the construction of confidence sets for the level
set of linear regression is considered. In particular, exact $1-\alpha$ level
upper, lower and two-sided confidence sets are constructed for the normal-error
linear regression. It is shown that these confidence sets are closely connected
with the corresponding $1-\alpha$ level simultaneous confidence bands. It is
also pointed out that the construction method is readily applicable to other
parametric regression models where the mean response depends on a linear
predictor through a monotonic link function, which include generalized linear
models, linear mixed models and generalized linear mixed models. Therefore the
method proposed in this paper is widely applicable. Two examples are given to
illustrate the method.",-0.21836971,-0.0052246926,-0.04894446,A
8644,"Construction of a two-sided conﬁdence set of exact conﬁdence
level 1 − α is clearly of interest and warrants further research.","We are unable to establish thus far whether the two-sided conﬁdence set [Gˆ2l, Gˆ2u] is of
conﬁdence level 1 − α exactly.","We are actively researching
on this and hope to report the results in the near future.",2022-07-09 17:01:46+00:00,Confidence Sets for a level set in linear regression,stat.ME,['stat.ME'],"[arxiv.Result.Author('Fang Wan'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Frank Bretz')]","Regression modeling is the workhorse of statistics and there is a vast
literature on estimation of the regression function. It is realized in recent
years that in regression analysis the ultimate aim may be the estimation of a
level set of the regression function, instead of the estimation of the
regression function itself. The published work on estimation of the level set
has thus far focused mainly on nonparametric regression, especially on point
estimation. In this paper, the construction of confidence sets for the level
set of linear regression is considered. In particular, $1-\alpha$ level upper,
lower and two-sided confidence sets are constructed for the normal-error linear
regression. It is shown that these confidence sets can be easily constructed
from the corresponding $1-\alpha$ level simultaneous confidence bands. It is
also pointed out that the construction method is readily applicable to other
parametric regression models where the mean response depends on a linear
predictor through a monotonic link function, which include generalized linear
models, linear mixed models and generalized linear mixed models. Therefore the
method proposed in this paper is widely applicable. Examples are given to
illustrate the method.",0.102796435,-0.31774566,0.11793783,A
8668,"[14] Finally, further research could investigate the impact of more variations in simulation conditions
such as the sample size and the number of k matched donors, and could determine what the optimal blending factor is
to predict outcomes.","Examples of other similarity measures would be the Fréchet distance, [13] and the locally supervised metric learning
(LSML) measure.","9
Density  0.10

                                                                                                                                                                                                             Estimates blending factor = 0
                                                                                                                                                                                                             Estimates blending factor = 1
                                                                                                                                                                                                             True data
         0.05

         0.00

                        −20  −10      0  10      20      30

Figure 6: Density of the estimates obtained with the RBD with blending factor = 0 (MD) and of those obtained with the
RBD with blending factor = 1 (PD), plotted against the density of the true data, e.g.",2022-07-11 06:22:22+00:00,"A blended distance to define ""people-like-me""",stat.ME,['stat.ME'],"[arxiv.Result.Author('Anaïs Fopma'), arxiv.Result.Author('Mingyang Cai'), arxiv.Result.Author('Stef van Buuren'), arxiv.Result.Author('Gerko Vink')]","Curve matching is a prediction technique that relies on predictive mean
matching, which matches donors that are most similar to a target based on the
predictive distance. Even though this approach leads to high prediction
accuracy, the predictive distance may make matches look unconvincing, as the
profiles of the matched donors can substantially differ from the profile of the
target. To counterbalance this, similarity between the curves of the donors and
the target can be taken into account by combining the predictive distance with
the Mahalanobis distance into a `blended distance' measure. The properties of
this measure are evaluated in two simulation studies. Simulation study I
evaluates the performance of the blended distance under different
data-generating conditions. The results show that blending towards the
Mahalanobis distance leads to worse performance in terms of bias, coverage, and
predictive power. Simulation study II evaluates the blended metric in a setting
where a single value is imputed. The results show that a property of blending
is the bias-variance trade off. Giving more weight to the Mahalanobis distance
leads to less variance in the imputations, but less accuracy as well. The main
conclusion is that the high prediction accuracy achieved with the predictive
distance necessitates the variability in the profiles of donors.",0.06752152,-0.12327768,0.12275065,C
8709,"Finally, Section 8 highlights important questions and connections raised by this work and outlines
opportunities for further research.","In
Section 7 we demonstrate implications for practice by reanalysis of two observational datasets: one measuring
genetic damage experienced by welders and one assessing the impact of right-heart catheterization on patient
mortality.","2
2 Formal framework and problem setup

2.1 Uniform randomization inference in matched designs

Consider a population of individuals each represented by a vector (Y (1), Y (0), Z, X, U ).",2022-07-11 17:13:13+00:00,Covariate-adaptive randomization inference in matched designs,stat.ME,['stat.ME'],[arxiv.Result.Author('Samuel D. Pimentel')],"It is common to conduct causal inference in matched observational studies by
proceeding as though treatment assignments within matched sets are assigned
uniformly at random and using this distribution as the basis for inference.
This approach ignores observed discrepancies in matched sets that may be
consequential for the distribution of treatment, which are succinctly captured
by within-set differences in the propensity score. We address this problem via
covariate-adaptive randomization inference, which modifies the permutation
probabilities to vary with estimated propensity score discrepancies and avoids
requirements to exclude matched pairs or model an outcome variable. We show
that the test achieves type I error control arbitrarily close to the nominal
level when large samples are available for propensity score estimation. We
characterize the large-sample behavior of the new randomization test for a
difference-in-means estimator of a constant additive effect. We also show that
existing methods of sensitivity analysis generalize effectively to
covariate-adaptive randomization inference. Finally, we evaluate the empirical
value of covariate-adaptive randomization procedures via comparisons to
traditional uniform inference in matched designs with and without propensity
score calipers and regression adjustment using simulations and analyses of
genetic damage among welders and right-heart catheterization in surgical
patients.",0.22083534,-0.003979021,-0.058862455,C
8710,"However, Z-dependence will be helpful in explaining the
empirical performance of uniform and covariate-adaptive randomization inference in certain settings, as will
be shown, and we believe it is an important topic for further study.","Z-dependence is not a central focus in what follows, and unless otherwise noted we will take
the perspective that the matched sets M are ﬁxed.","3 Adapting randomization inference to covariate discrepancies

3.1 True and estimated conditional distributions of treatment status
    To adapt randomization inference to discrepancies in observed covariates, we begin by considering the

case where no unobserved confounding is present and represent the true conditional distribution in terms of
propensity scores.",2022-07-11 17:13:13+00:00,Covariate-adaptive randomization inference in matched designs,stat.ME,['stat.ME'],[arxiv.Result.Author('Samuel D. Pimentel')],"It is common to conduct causal inference in matched observational studies by
proceeding as though treatment assignments within matched sets are assigned
uniformly at random and using this distribution as the basis for inference.
This approach ignores observed discrepancies in matched sets that may be
consequential for the distribution of treatment, which are succinctly captured
by within-set differences in the propensity score. We address this problem via
covariate-adaptive randomization inference, which modifies the permutation
probabilities to vary with estimated propensity score discrepancies and avoids
requirements to exclude matched pairs or model an outcome variable. We show
that the test achieves type I error control arbitrarily close to the nominal
level when large samples are available for propensity score estimation. We
characterize the large-sample behavior of the new randomization test for a
difference-in-means estimator of a constant additive effect. We also show that
existing methods of sensitivity analysis generalize effectively to
covariate-adaptive randomization inference. Finally, we evaluate the empirical
value of covariate-adaptive randomization procedures via comparisons to
traditional uniform inference in matched designs with and without propensity
score calipers and regression adjustment using simulations and analyses of
genetic damage among welders and right-heart catheterization in surgical
patients.",0.12136489,0.08031428,-0.21644694,C
8978,We further study the simultaneous conﬁdence band of E (Yn+1|Xn+1) given by (16).,"In each panel, the solid black line represents the true conditional
mean function E (Y |x0)(t), the red dashed line represents the estimated mean, and the blue and green dotted
lines represent the upper and the lower bounds of 95% pointwise conﬁdence intervals, respectively.","Estimation of sn in
(16) is straightforward.",2022-07-17 16:05:10+00:00,Nonlinear function-on-function regression by RKHS,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Peijun Sang'), arxiv.Result.Author('Bing Li')]","We propose a nonlinear function-on-function regression model where both the
covariate and the response are random functions. The nonlinear regression is
carried out in two steps: we first construct Hilbert spaces to accommodate the
functional covariate and the functional response, and then build a second-layer
Hilbert space for the covariate to capture nonlinearity. The second-layer space
is assumed to be a reproducing kernel Hilbert space, which is generated by a
positive definite kernel determined by the inner product of the first-layer
Hilbert space for $X$--this structure is known as the nested Hilbert spaces. We
develop estimation procedures to implement the proposed method, which allows
the functional data to be observed at different time points for different
subjects. Furthermore, we establish the convergence rate of our estimator as
well as the weak convergence of the predicted response in the Hilbert space.
Numerical studies including both simulations and a data application are
conducted to investigate the performance of our estimator in finite sample.",-0.20139265,-0.1003619,-0.14692731,A
9303,"Even though such a

                                                         2
comparison in the context of mixed eﬀects has not been attempted yet and it could be an
interesting direction for further research, it requires a careful deﬁnition of the optimality
criterion which is beyond the scope of this manuscript

    In this article, we construct statistical tools for cluster-wise and simultaneous inference
for mixed parameters under model misspeciﬁcation using simple, semiparametric random
eﬀect bootstrap as in Carpenter et al.","At this place we need
to emphasize that our goal is not to compare the performance of all existing procedures
to select an optimal scheme with respect to a predeﬁned criterion.",(2003) and Opsomer et al.,2022-07-25 18:20:27+00:00,Simple bootstrap for linear mixed effects under model misspecification,stat.ME,"['stat.ME', 'stat.CO', '62F03, 62F40, 62J05, 62J15', 'G.3']","[arxiv.Result.Author('Katarzyna Reluga'), arxiv.Result.Author('Stefan Sperlich')]","Linear mixed effects are considered excellent predictors of cluster-level
parameters in various domains. However, previous work has shown that their
performance can be seriously affected by departures from modelling assumptions.
Since the latter are common in applied studies, there is a need for inferential
methods which are to certain extent robust to misspecfications, but at the same
time simple enough to be appealing for practitioners. We construct statistical
tools for cluster-wise and simultaneous inference for mixed effects under model
misspecification using straightforward semiparametric random effect bootstrap.
In our theoretical analysis, we show that our methods are asymptotically
consistent under general regularity conditions. In simulations our intervals
were robust to severe departures from model assumptions and performed better
than their competitors in terms of empirical coverage probability.",-0.07159862,-0.09020883,-0.12601791,A
9305,A discussion of directions for further research follows in Section 5.,"We observe that classiﬁcations using core shrinkage estimators have
lower out-of-sample misclassiﬁcation rates than those using separable or unstructured
MLEs.","Proofs of
mathematical results are provided in an appendix.",2022-07-25 19:21:17+00:00,Core Shrinkage Covariance Estimation for Matrix-variate Data,stat.ME,"['stat.ME', '62H20, 15A23']","[arxiv.Result.Author('Peter Hoff'), arxiv.Result.Author('Andrew McCormack'), arxiv.Result.Author('Anru R. Zhang')]","A separable covariance model for a random matrix provides a parsimonious
description of the covariances among the rows and among the columns of the
matrix, and permits likelihood-based inference with a very small sample size.
However, in many applications the assumption of exact separability is unlikely
to be met, and data analysis with a separable model may overlook or
misrepresent important dependence patterns in the data. In this article, we
propose a compromise between separable and unstructured covariance estimation.
We show how the set of covariance matrices may be uniquely parametrized in
terms of the set of separable covariance matrices and a complementary set of
""core"" covariance matrices, where the core of a separable covariance matrix is
the identity matrix. This parametrization defines a Kronecker-core
decomposition of a covariance matrix. By shrinking the core of the sample
covariance matrix with an empirical Bayes procedure, we obtain an estimator
that can adapt to the degree of separability of the population covariance
matrix.",-0.099760406,0.011566712,-0.14737907,A
9335,"However, the implications of such an approach are not yet clear and would
require further research before this could be recommended.","This is an enticing
approach, as one does not want to reduce the number of predictor
parameters in sub-models which are not suffering from overfitting.","30
3) Reduce the acceptable level of overfitting between specific pairs of
outcomes.",2022-07-26 13:42:59+00:00,Minimum Sample Size for Developing a Multivariable Prediction Model using Multinomial Logistic Regression,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Alexander Pate'), arxiv.Result.Author('Richard D Riley'), arxiv.Result.Author('Gary S Collins'), arxiv.Result.Author('Maarten van Smeden'), arxiv.Result.Author('Ben Van Calster'), arxiv.Result.Author('Joie Ensor'), arxiv.Result.Author('Glen P Martin')]","Multinomial logistic regression models allow one to predict the risk of a
categorical outcome with more than 2 categories. When developing such a model,
researchers should ensure the number of participants (n) is appropriate
relative to the number of events (E.k) and the number of predictor parameters
(p.k) for each category k. We propose three criteria to determine the minimum n
required in light of existing criteria developed for binary outcomes. The first
criteria aims to minimise the model overfitting. The second aims to minimise
the difference between the observed and adjusted R2 Nagelkerke. The third
criterion aims to ensure the overall risk is estimated precisely. For criterion
(i), we show the sample size must be based on the anticipated Cox-snell R2 of
distinct one-to-one logistic regression models corresponding to the sub-models
of the multinomial logistic regression, rather than on the overall Cox-snell R2
of the multinomial logistic regression. We tested the performance of the
proposed criteria (i) through a simulation study, and found that it resulted in
the desired level of overfitting. Criterion (ii) and (iii) are natural
extensions from previously proposed criteria for binary outcomes. We illustrate
how to implement the sample size criteria through a worked example considering
the development of a multinomial risk prediction model for tumour type when
presented with an ovarian mass. Code is provided for the simulation and worked
example. We will embed our proposed criteria within the pmsampsize R library
and Stata modules.",0.17198786,0.043927297,0.09780244,C
9518,The strongest biomarkers can be selected for further study.,"The biomarkers X(j)’s can then be sorted in an order of in-
creasing p-values p(j) with smaller p-value indicating stronger biomarker eﬀect on the response.","To adjust for the multiple testing,
common variable selection rules such as the Benjamini–Hochberg method [32] can be applied on
these p-values.",2022-07-31 03:34:44+00:00,Statistical Methods for Selective Biomarker Testing,stat.ME,"['stat.ME', 'stat.AP', '62P10']","[arxiv.Result.Author('A. Adam Ding'), arxiv.Result.Author('Natalie DelRocco'), arxiv.Result.Author('Samuel Wu')]","Biomarker is a critically important tool in modern clinical diagnosis,
prognosis, and classification/prediction. However, there are fiscal and
analytical barriers to biomarker research. Selective Genotyping is an approach
to increasing study power and efficiency where individuals with the most
extreme phenotype (response) are chosen for genotyping (exposure) in order to
maximize the information in the sample. In this article, we describe an
analogous procedure in the biomarker testing landscape where both response and
biomarker (exposure) are continuous. We propose an intuitive reverse-regression
least squares estimator for the parameters relating biomarker value to
response. Monte Carlo simulations show that this method is unbiased and
efficient relative to estimates from random sampling when the joint normal
distribution assumption is met. We illustrate application of proposed methods
on data from a chronic pain clinical trial.",0.3031082,-0.08177292,0.24100038,C
9664,"Social scientists run large-scale
ﬁeld experiments to compare many treatments or interventions simultaneously and select the
most eﬀective for further research and implementation [Milkman et al., 2021a].","Economists rank teachers by the value they add to their students’ education [Chetty
et al., 2014a,b, Hanushek, 2011] and United States neighborhoods by the economic opportu-
nity they aﬀord their children [Chetty and Hendren, 2018a,b].","Researchers and practitioners often perform ranking and selection using point estimates
alone.",2022-08-03 13:02:34+00:00,"Bayesian ranking and selection with applications to field studies, economic mobility, and forecasting",stat.ME,"['stat.ME', 'econ.EM']",[arxiv.Result.Author('Dillon Bowen')],"Decision-making often involves ranking and selection. For example, to
assemble a team of political forecasters, we might begin by narrowing our
choice set to the candidates we are confident rank among the top 10% in
forecasting ability. Unfortunately, we do not know each candidate's true
ability but observe a noisy estimate of it. This paper develops new Bayesian
algorithms to rank and select candidates based on noisy estimates. Using
simulations based on empirical data, we show that our algorithms often
outperform frequentist ranking and selection algorithms. Our Bayesian ranking
algorithms yield shorter rank confidence intervals while maintaining
approximately correct coverage. Our Bayesian selection algorithms select more
candidates while maintaining correct error rates. We apply our ranking and
selection procedures to field experiments, economic mobility, forecasting, and
similar problems. Finally, we implement our ranking and selection techniques in
a user-friendly Python package documented here:
https://dsbowen-conditional-inference.readthedocs.io/en/latest/.",0.31860363,0.14026123,0.032497063,C
9795,"However, further research is needed to develop suitable model
veriﬁcation and selection methods.","Due to their equivalence, we hypothesize that the transformation model with FZ = logit−1
is also endowed with the same robustness properties as the MWW and can be chosen when
no a priori model is known.","Our proposed estimators assume that the diﬀerence between the diseased and nondiseased dis-
tributions is described by a shift-term, µd on the scale of FZ.",2022-08-07 09:00:28+00:00,Transformation models for ROC analysis,stat.ME,['stat.ME'],"[arxiv.Result.Author('Ainesh Sewak'), arxiv.Result.Author('Torsten Hothorn')]","Receiver operating characteristic (ROC) analysis is one of the most popular
approaches for evaluating and comparing the accuracy of medical diagnostic
tests. Although various methodologies have been developed for estimating ROC
curves and its associated summary indices, there is no consensus on a single
framework that can provide consistent statistical inference whilst handling the
complexities associated with medical data. Such complexities might include
covariates that influence the diagnostic potential of a test, ordinal test
data, censored data due to instrument detection limits or correlated
biomarkers. We propose a regression model for the transformed test results
which exploits the invariance of ROC curves to monotonic transformations and
naturally accommodates these features. Our use of maximum likelihood inference
guarantees asymptotic efficiency of the resulting estimators and associated
confidence intervals. Simulation studies show that the estimates based on
transformation models are unbiased and yield coverage at nominal levels. The
methodology is applied to a cross-sectional study of metabolic syndrome where
we investigate the covariate-specific performance of weight-to-height ratio as
a non-invasive diagnostic test. Software implementations for all the methods
described in the article are provided in the ""tram"" R package.",-0.06837981,-0.025698405,-0.29849628,A
9796,"However, further research is needed to develop suitable model
veriﬁcation and selection methods.","Due to their equivalence, we hypothesize that the transformation model with FZ = logit−1
is also endowed with the same robustness properties as the MWW and can be chosen when
no a priori model is known.","Our proposed estimators assume that the diﬀerence between the diseased and nondiseased dis-
tributions is described by a shift-term, µd on the scale of FZ.",2022-08-07 09:00:28+00:00,Transformation models for ROC analysis,stat.ME,['stat.ME'],"[arxiv.Result.Author('Ainesh Sewak'), arxiv.Result.Author('Torsten Hothorn')]","Receiver operating characteristic (ROC) analysis is one of the most popular
approaches for evaluating and comparing the accuracy of medical diagnostic
tests. Although various methodologies have been developed for estimating ROC
curves and its associated summary indices, there is no consensus on a single
framework that can provide consistent statistical inference whilst handling the
complexities associated with medical data. Such complexities might include
covariates that influence the diagnostic potential of a test, ordinal test
data, censored data due to instrument detection limits or correlated
biomarkers. We propose a regression model for the transformed test results
which exploits the invariance of ROC curves to monotonic transformations and
naturally accommodates these features. Our use of maximum likelihood inference
guarantees asymptotic efficiency of the resulting estimators and associated
confidence intervals. Simulation studies show that the estimates based on
transformation models are unbiased and yield coverage at nominal levels. The
methodology is applied to a cross-sectional study of metabolic syndrome where
we investigate the covariate-specific performance of weight-to-height ratio as
a non-invasive diagnostic test. Software implementations for all the methods
described in the article are provided in the ""tram"" R package.",-0.06837981,-0.025698405,-0.29849628,A
9799,There are several further research directions that could be pursued.,The selected genera align with existing results in microbiome literature.,"First, our estimation consis-
tency result is non-trivial leading us to develop a new technique to facilitate underlying theoretical
analyses by combining sub-Gaussian properties of sign vector with newly established bounds on
second derivatives of inverse bridge function for truncated/truncated cases.",2022-08-07 14:22:21+00:00,Sparse semiparametric discriminant analysis for high-dimensional zero-inflated data,stat.ME,['stat.ME'],"[arxiv.Result.Author('Hee Cheol Chung'), arxiv.Result.Author('Yang Ni'), arxiv.Result.Author('Irina Gaynanova')]","Sequencing-based technologies provide an abundance of high-dimensional
biological datasets with skewed and zero-inflated measurements. Classification
of such data with linear discriminant analysis leads to poor performance due to
the violation of the Gaussian distribution assumption. To address this
limitation, we propose a new semiparametric discriminant analysis framework
based on the truncated latent Gaussian copula model that accommodates both
skewness and zero inflation. By applying sparsity regularization, we
demonstrate that the proposed method leads to the consistent estimation of
classification direction in high-dimensional settings. On simulated data, the
proposed method shows superior performance compared to the existing method. We
apply the method to discriminate healthy controls from patients with Crohn's
disease based on microbiome data and to identify genera with the most influence
on the classification rule.",-0.0033438802,-0.1869888,0.014366347,A
10015,"This is a temptative
   explanation and we leave an in-depth analysis of this eﬀect for a further study.","In
   contrast, the presence of vectors with unusually large 2 norm in the NTS will make the 2 norms
   of variable stars of the test sample more ‘less surprising’, with larger p-values.","FDR, m =100      0.05

   1400                        0.04
   1200
   1000                        0.03
    800
n   600
    400
    200                        0.02

      00                       0.01

   1400   20  40  m1  60   80  100 0.00
   1200
   1000       FDR, m =100      0.05                       TDR, m =100      1.0
    800
    600                        0.04            1400                        0.8
    400                                        1200
    200                        0.03            1000                        0.6
                                                800
n     00                                    n   600
                                                400
                               0.02             200                        0.4

                               0.01               00                       0.2

          20  40  m1  60   80  100 0.00               20  40  m1  60   80  100 0.0

   Fig 11: FDR (left column) and TDR (right column) for AdaDetect KDE (top row) and Empirical
   BH (bottom row).",2022-08-13 17:14:55+00:00,Machine learning meets false discovery rate,stat.ME,"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Ariane Marandon'), arxiv.Result.Author('Lihua Lei'), arxiv.Result.Author('David Mary'), arxiv.Result.Author('Etienne Roquain')]","Classical false discovery rate (FDR) controlling procedures offer strong and
interpretable guarantees, while they often lack of flexibility. On the other
hand, recent machine learning classification algorithms, as those based on
random forests (RF) or neural networks (NN), have great practical performances
but lack of interpretation and of theoretical guarantees. In this paper, we
make these two meet by introducing a new adaptive novelty detection procedure
with FDR control, called AdaDetect. It extends the scope of recent works of
multiple testing literature to the high dimensional setting, notably the one in
Yang et al. (2021). AdaDetect is shown to both control strongly the FDR and to
have a power that mimics the one of the oracle in a specific sense. The
interest and validity of our approach is demonstrated with theoretical results,
numerical experiments on several benchmark datasets and with an application to
astrophysical data. In particular, while AdaDetect can be used in combination
with any classifier, it is particularly efficient on real-world datasets with
RF, and on images with NN.",0.07496799,-0.37660915,0.22411835,A
10016,"This is a temptative explanation and we leave an in-depth analysis of
this eﬀect for a further study.","In contrast, the presence of vectors with unusually large 2 norm
in the NTS will make the 2 norms of variable stars of the test sample more ‘less surprising’,
with larger p-values.","FDR, m =100      0.05

   1400                        0.04
   1200
   1000                        0.03
    800
n   600
    400
    200                        0.02

      00                       0.01

   1400   20  40  m1  60   80  100 0.00
   1200
   1000       FDR, m =100      0.05                    TDR, m =100      1.0
    800
    600                        0.04         1400                        0.8
    400                                     1200
    200                        0.03         1000                        0.6
                                             800
n     00                                 n   600
                                             400
                               0.02          200                        0.4

                               0.01            00                       0.2

          20  40  m1  60   80  100 0.00            20  40  m1  60   80  100 0.0

Fig 11: FDR (left column) and TDR (right column) for AdaDetect KDE (top row) and
Empirical BH (bottom row).",2022-08-13 17:14:55+00:00,Machine learning meets false discovery rate,stat.ME,"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Ariane Marandon'), arxiv.Result.Author('Lihua Lei'), arxiv.Result.Author('David Mary'), arxiv.Result.Author('Etienne Roquain')]","Classical false discovery rate (FDR) controlling procedures offer strong and
interpretable guarantees but often lack flexibility to work with complex data.
By contrast, machine learning-based classification algorithms have superior
performances on modern datasets but typically fall short of error-controlling
guarantees. In this paper, we make these two meet by introducing a new adaptive
novelty detection procedure with FDR control, called AdaDetect. It extends the
scope of recent works of multiple testing literature to the high dimensional
setting, notably the one in Yang et al. (2021). We prove that AdaDetect comes
with finite sample guarantees: it controls the FDR strongly and approximates
the oracle in terms of the power, with explicit remainder terms that are small
under mild conditions. In practice, AdaDetect can be used in combination with
any machine learning-based classifier, which allows the user to choose the most
relevant classification approach. We illustrate this with classical real-world
datasets, for which random forest and neural network classifiers are
particularly efficient. The versatility of our method is also shown with an
astrophysical application.",0.07496799,-0.37660915,0.22411835,A
10148,"In Section 7.1, we will
further study a partially linear IV model, where X = (Xa, Xb), the function class H is the direct
sum of linear functions in Xa and of L2(Xb), and θ⋆ is the linear coeﬃcient in Xa.",Here we consider a general nonparametric IV model with H = L2(X).,Example 2 (Proximal Causal Inference).,2022-08-17 13:38:31+00:00,Inference on Strongly Identified Functionals of Weakly Identified Functions,stat.ME,"['stat.ME', 'econ.EM', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Andrew Bennett'), arxiv.Result.Author('Nathan Kallus'), arxiv.Result.Author('Xiaojie Mao'), arxiv.Result.Author('Whitney Newey'), arxiv.Result.Author('Vasilis Syrgkanis'), arxiv.Result.Author('Masatoshi Uehara')]","In a variety of applications, including nonparametric instrumental variable
(NPIV) analysis, proximal causal inference under unmeasured confounding, and
missing-not-at-random data with shadow variables, we are interested in
inference on a continuous linear functional (e.g., average causal effects) of
nuisance function (e.g., NPIV regression) defined by conditional moment
restrictions. These nuisance functions are generally weakly identified, in that
the conditional moment restrictions can be severely ill-posed as well as admit
multiple solutions. This is sometimes resolved by imposing strong conditions
that imply the function can be estimated at rates that make inference on the
functional possible. In this paper, we study a novel condition for the
functional to be strongly identified even when the nuisance function is not;
that is, the functional is amenable to asymptotically-normal estimation at
$\sqrt{n}$-rates. The condition implies the existence of debiasing nuisance
functions, and we propose penalized minimax estimators for both the primary and
debiasing nuisance functions. The proposed nuisance estimators can accommodate
flexible function classes, and importantly they can converge to fixed limits
determined by the penalization regardless of the identifiability of the
nuisances. We use the penalized nuisance estimators to form a debiased
estimator for the functional of interest and prove its asymptotic normality
under generic high-level conditions, which provide for asymptotically valid
confidence intervals. We also illustrate our method in a novel partially linear
proximal causal inference problem and a partially linear instrumental variable
regression problem.",-0.052618343,-0.009076303,-0.28134248,A
10149,"In Section 7.2, we will further study a partially linear proximal causal inference model.","In Section 4, we will derive identiﬁcation conditions for
the target linear functionals.","We will
show that when the regression function E[Y (a) | U, X] is partially linear in a, there always exists
a bridge function h⋆(V, X, A) partially linear in A.",2022-08-17 13:38:31+00:00,Inference on Strongly Identified Functionals of Weakly Identified Functions,stat.ME,"['stat.ME', 'econ.EM', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Andrew Bennett'), arxiv.Result.Author('Nathan Kallus'), arxiv.Result.Author('Xiaojie Mao'), arxiv.Result.Author('Whitney Newey'), arxiv.Result.Author('Vasilis Syrgkanis'), arxiv.Result.Author('Masatoshi Uehara')]","In a variety of applications, including nonparametric instrumental variable
(NPIV) analysis, proximal causal inference under unmeasured confounding, and
missing-not-at-random data with shadow variables, we are interested in
inference on a continuous linear functional (e.g., average causal effects) of
nuisance function (e.g., NPIV regression) defined by conditional moment
restrictions. These nuisance functions are generally weakly identified, in that
the conditional moment restrictions can be severely ill-posed as well as admit
multiple solutions. This is sometimes resolved by imposing strong conditions
that imply the function can be estimated at rates that make inference on the
functional possible. In this paper, we study a novel condition for the
functional to be strongly identified even when the nuisance function is not;
that is, the functional is amenable to asymptotically-normal estimation at
$\sqrt{n}$-rates. The condition implies the existence of debiasing nuisance
functions, and we propose penalized minimax estimators for both the primary and
debiasing nuisance functions. The proposed nuisance estimators can accommodate
flexible function classes, and importantly they can converge to fixed limits
determined by the penalization regardless of the identifiability of the
nuisances. We use the penalized nuisance estimators to form a debiased
estimator for the functional of interest and prove its asymptotic normality
under generic high-level conditions, which provide for asymptotically valid
confidence intervals. We also illustrate our method in a novel partially linear
proximal causal inference problem and a partially linear instrumental variable
regression problem.",-0.02529599,0.032495506,-0.209575,A
10252,"At the very least, this topic deserves further study,
contrasting SHAP with methods such as counterfactual explanations (Wachter et al.","Based on
this review of SHAP, it is not immediately obvious that a series of functional ANOVA terms can provide
the necessary information for algorithmic recourse.",(2017)).,2022-08-21 21:46:15+00:00,Statistical Aspects of SHAP: Functional ANOVA for Model Interpretation,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Andrew Herren'), arxiv.Result.Author('P. Richard Hahn')]","SHAP is a popular method for measuring variable importance in machine
learning models. In this paper, we study the algorithm used to estimate SHAP
scores and show that it is a transformation of the functional ANOVA
decomposition. We use this connection to show that challenges in SHAP
approximations largely relate to the choice of a feature distribution and the
number of $2^p$ ANOVA terms estimated. We argue that the connection between
machine learning explainability and sensitivity analysis is illuminating in
this case, but the immediate practical consequences are not obvious since the
two fields face a different set of constraints. Machine learning explainability
concerns models which are inexpensive to evaluate but often have hundreds, if
not thousands, of features. Sensitivity analysis typically deals with models
from physics or engineering which may be very time consuming to run, but
operate on a comparatively small space of inputs.",0.26624137,-0.004650079,0.11233285,C
10253,"At the very least, this topic deserves further study,
contrasting SHAP with methods such as counterfactual explanations (Wachter et al.","Based on
this review of SHAP, it is not immediately obvious that a series of functional ANOVA terms can provide
the necessary information for algorithmic recourse.",(2017)).,2022-08-21 21:46:15+00:00,Statistical Aspects of SHAP: Functional ANOVA for Model Interpretation,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Andrew Herren'), arxiv.Result.Author('P. Richard Hahn')]","SHAP is a popular method for measuring variable importance in machine
learning models. In this paper, we study the algorithm used to estimate SHAP
scores and outline its connection to the functional ANOVA decomposition. We use
this connection to show that challenges in SHAP approximations largely relate
to the choice of a feature distribution and the number of $2^p$ ANOVA terms
estimated. We argue that the connection between machine learning explainability
and sensitivity analysis is illuminating in this case, but the immediate
practical consequences are not obvious since the two fields face a different
set of constraints. Machine learning explainability concerns models which are
inexpensive to evaluate but often have hundreds, if not thousands, of features.
Sensitivity analysis typically deals with models from physics or engineering
which may be very time consuming to run, but operate on a comparatively small
space of inputs.",0.26624137,-0.004650079,0.11233285,C
10602,"These results open the door for further research ad-
dressing the question of the control of mRNA metabolism during seed germination, notably
concerning the selectivity of translational control.","These genes could represent key regulators of the modulation of seed physio-
logical quality in response to various types of biotic and environmental stress during seed
production and/or during germination.","The germinating seed is undoubtedly
a relevant biological model for exploring the precise mechanisms of combined transcrip-
tional and translational regulation related to gene expression ending with the production
of functional protein.",2022-08-31 09:27:47+00:00,Variable selection in sparse multivariate GLARMA models: Application to germination control by environment,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('M. Gomtsyan'), arxiv.Result.Author('C. Lévy-Leduc'), arxiv.Result.Author('S. Ouadah'), arxiv.Result.Author('L. Sansonnet'), arxiv.Result.Author('C. Bailly'), arxiv.Result.Author('L. Rajjou')]","We propose a novel and efficient iterative two-stage variable selection
approach for multivariate sparse GLARMA models, which can be used for modelling
multivariate discrete-valued time series. Our approach consists in iteratively
combining two steps: the estimation of the autoregressive moving average (ARMA)
coefficients of multivariate GLARMA models and the variable selection in the
coefficients of the Generalized Linear Model (GLM) part of the model performed
by regularized methods. We explain how to implement our approach efficiently.
Then we assess the performance of our methodology using synthetic data and
compare it with alternative methods. Finally, we illustrate it on RNA-Seq data
resulting from polyribosome profiling to determine translational status for all
mRNAs in germinating seeds. Our approach, which is implemented in the
MultiGlarmaVarSel R package and available on the CRAN, is very attractive since
it benefits from a low computational load and is able to outperform the other
methods for recovering the null and non-null coefficients.",0.18327048,-0.065787315,0.20385316,C
10674,"There are several directions for further study and application of the proposed
methodology.","We hope that such a clear separation of methodologies will
allow for easier adoption of the method in operational settings.","Max-and-Smooth involves approximations and simpliﬁcations
whose impact on reliability, accuracy, and skill of postprocessed forecasts can

                                               28
be studied further.",2022-09-01 14:04:41+00:00,Spatial forecast postprocessing: The Max-and-Smooth approach,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Stefan Siegert'), arxiv.Result.Author('Ben Hooper'), arxiv.Result.Author('Joshua Lovegrove'), arxiv.Result.Author('Tyler Thomson'), arxiv.Result.Author('Birgir Hrafnkelsson')]","Numerical weather forecasts can exhibit systematic errors due to simplifying
model assumptions and computational approximations. Statistical postprocessing
is a statistical approach to correcting such biases. A statistical
postprocessing model takes input data from a numerical forecast model, and
outputs a parametric predictive distribution of a real-world observation, with
model parameters learned from past forecast-observation pairs. In this paper we
develop and discuss methods for postprocessing of gridded data. We show that
estimates of postprocessing parameters on a spatial grid can be improved by
Bayesian hierarchical modelling with spatial priors. We use the
""Max-and-Smooth"" approach [Hrafnkelsson et al., 2021] to approximate a fully
Bayesian inference in two steps. First we calculate maximum-likelihood
estimates (MLEs) of postprocessing parameters at individual grid points. Second
we smooth the MLEs using a measurement error model with a spatial prior. Our
approach provides the theoretical basis for the parameter smoothing approach by
Kharin et al. [2017], and simplifies and generalises the Bayesian hierarchical
modelling approach by Moeller et al. [2015]. A new derivation of Max-and-Smooth
is presented. The method is applicable to arbitrary postprocessing models, as
illustrated on Model Output Statistics, Logistic Regression, and Nonhomogeneous
Gaussian Regression. We report consistent improvements in forecast accuracy,
calibration, and probabilistic skill in postprocessing of temperature and
precipitation forecasts.",-0.24965522,0.24961402,0.23983777,B
10748,"To this extent, this paper highlights that further research that is beyond the scope of this initial con-
tribution is necessary.","This highlights the importance of imputation model speciﬁcation in CRTs when the interest lies
in detecting HTE with a single binary effect modiﬁer.","In particular, there are a couple paths forward that have been illuminated by these
comparison studies.",2022-09-02 23:50:32+00:00,Assessing treatment effect heterogeneity in the presence of missing effect modifier data in cluster-randomized trials,stat.ME,['stat.ME'],"[arxiv.Result.Author('Bryan S. Blette'), arxiv.Result.Author('Scott D. Halpern'), arxiv.Result.Author('Fan Li'), arxiv.Result.Author('Michael O. Harhay')]","Understanding whether and how treatment effects vary across individuals is
crucial to inform clinical practice and recommendations. Accordingly, the
assessment of heterogeneous treatment effects (HTE) based on pre-specified
potential effect modifiers has become a common goal in modern randomized
trials. However, when one or more potential effect modifiers are missing,
complete-case analysis may lead to bias, under-coverage, inflated type I error,
or low power. While statistical methods for handling missing data have been
proposed and compared for individually randomized trials with missing effect
modifier data, few guidelines exist for the cluster-randomized setting, where
intracluster correlations in the effect modifiers, outcomes, or even
missingness mechanisms may introduce further threats to accurate assessment of
HTE. In this paper, the performance of various commonly-used missing data
methods (complete case analysis, single imputation, multiple imputation,
multilevel multiple imputation [MMI], and Bayesian MMI) are neutrally compared
in a simulation study of cluster-randomized trials with missing effect modifier
data. Thereafter, we impose controlled missing data scenarios to a potential
effect modifier from the Work, Family, and Health Study to further compare the
available methods using real data. Our simulations and data application suggest
that MMI and Bayesian MMI have better performance than other available methods,
and that Bayesian MMI has improved bias and coverage over standard MMI when
there are model specification or compatibility issues. We also provide
recommendations for practitioners and outline future research areas.",0.131991,-0.050993055,-0.12905923,C
10804,"Theorem 2 also motivates further study of the large n asymptotic
behaviour of the estimator in (16), with the aim of obtaining simpler large n approximations with reliable
error bounds.","Therefore, our work paves the
way to investigate more computationally eﬃcient approximations of BNP estimators under the general PYP
prior, starting from numerical approximations of the estimator in (16) or alternative representations of the
Monte Carlo results in Proposition 3.","As this is the ﬁrst work on the estimation of coverage probabilities from sketched data, it opens several
new avenues for future research.",2022-09-05 20:48:04+00:00,Bayesian nonparametric estimation of coverage probabilities and distinct counts from sketched data,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Stefano Favaro'), arxiv.Result.Author('Matteo Sesia')]","The estimation of coverage probabilities, and in particular of the missing
mass, is a classical statistical problem with applications in numerous
scientific fields. In this paper, we study this problem in relation to
randomized data compression, or sketching. This is a novel but practically
relevant perspective, and it refers to situations in which coverage
probabilities must be estimated based on a compressed and imperfect summary, or
sketch, of the true data, because neither the full data nor the empirical
frequencies of distinct symbols can be observed directly. Our contribution is a
Bayesian nonparametric methodology to estimate coverage probabilities from data
sketched through random hashing, which also solves the challenging problems of
recovering the numbers of distinct counts in the true data and of distinct
counts with a specified empirical frequency of interest. The proposed Bayesian
estimators are shown to be easily applicable to large-scale analyses in
combination with a Dirichlet process prior, although they involve some open
computational challenges under the more general Pitman-Yor process prior. The
empirical effectiveness of our methodology is demonstrated through numerical
experiments and applications to real data sets of Covid DNA sequences, classic
English literature, and IP addresses.",-0.072202355,-0.0944504,-0.3190905,A
10832,"If the data is not informative, there is
no point in further studying recoverability of ϕ by means of ψ.","Identiﬁability and Information Gain

Oftentimes, we are interested in learning something about the (true) real-world generator G through the
P model-dependent quantity ψ, justiﬁed by its resemblance to the model-independent quantity ϕ that we
assume to play a role in G. As a ﬁrst step, we need to study whether the data generated by the unknown
process enables the P model to extract any information about ψ at all.","In a frequentist sense, we say that a quantity
ψ is identiﬁed in the given P model, if all the possible values of ψ lead to unique conditional distributions,
that is, for any ψ1 = ψ2 we have p(y | ψ1) = p(y | ψ2) [42].",2022-09-06 12:03:30+00:00,"Some models are useful, but how do we know which ones? Towards a unified Bayesian model taxonomy",stat.ME,['stat.ME'],"[arxiv.Result.Author('Paul-Christian Bürkner'), arxiv.Result.Author('Maximilian Scholz'), arxiv.Result.Author('Stefan Radev')]","Probabilistic (Bayesian) modeling has experienced a surge of applications in
almost all quantitative sciences and industrial areas. This development is
driven by a combination of several factors, including better probabilistic
estimation algorithms, flexible software, increased computing power, and a
growing awareness of the benefits of probabilistic learning. However, a
principled Bayesian model building workflow is far from complete and many
challenges remain. To aid future research and applications of a principled
Bayesian workflow, we ask and provide answers for what we perceive as two
fundamental questions of Bayesian modeling, namely (a) ``What actually
\emph{is} a Bayesian model?'' and (b) ``What makes a \emph{good} Bayesian
model?''. As an answer to the first question, we propose the PAD model taxonomy
that defines four basic kinds of Bayesian models, each representing some
combination of the assumed joint distribution of all (known or unknown)
variables (P), a posterior approximator (A), and training data (D). As an
answer to the second question, we propose ten utility dimensions according to
which we can evaluate Bayesian models holistically, namely, (1) causal
consistency, (2) parameter recoverability, (3) predictive performance, (4)
fairness, (5) structural faithfulness, (6) parsimony, (7) interpretability, (8)
convergence, (9) estimation speed, and (10) robustness. Further, we propose two
example utility decision trees that describe hierarchies and trade-offs between
utilities depending on the inferential goals that drive model building and
testing.",-0.14389834,0.016073998,-0.14118777,A
10833,"If the data is not informative, there is
no point in further studying recoverability of ϕ by means of ψ.","Identiﬁability and Information Gain

Oftentimes, we are interested in learning something about the (true) real-world generator G through the
P model-dependent quantity ψ, justiﬁed by its resemblance to the model-independent quantity ϕ that we
assume to play a role in G. As a ﬁrst step, we need to study whether the data generated by the unknown
process enables the P model to extract any information about ψ at all.","In a frequentist sense, we say that a quantity
ψ is identiﬁed in the given P model, if all the possible values of ψ lead to unique conditional distributions,
that is, for any ψ1 = ψ2 we have p(y | ψ1) = p(y | ψ2) [42].",2022-09-06 12:03:30+00:00,"Some models are useful, but how do we know which ones? Towards a unified Bayesian model taxonomy",stat.ME,['stat.ME'],"[arxiv.Result.Author('Paul-Christian Bürkner'), arxiv.Result.Author('Maximilian Scholz'), arxiv.Result.Author('Stefan T. Radev')]","Probabilistic (Bayesian) modeling has experienced a surge of applications in
almost all quantitative sciences and industrial areas. This development is
driven by a combination of several factors, including better probabilistic
estimation algorithms, flexible software, increased computing power, and a
growing awareness of the benefits of probabilistic learning. However, a
principled Bayesian model building workflow is far from complete and many
challenges remain. To aid future research and applications of a principled
Bayesian workflow, we ask and provide answers for what we perceive as two
fundamental questions of Bayesian modeling, namely (a) ""What actually is a
Bayesian model?"" and (b) ""What makes a good Bayesian model?"". As an answer to
the first question, we propose the PAD model taxonomy that defines four basic
kinds of Bayesian models, each representing some combination of the assumed
joint distribution of all (known or unknown) variables (P), a posterior
approximator (A), and training data (D). As an answer to the second question,
we propose ten utility dimensions according to which we can evaluate Bayesian
models holistically, namely, (1) causal consistency, (2) parameter
recoverability, (3) predictive performance, (4) fairness, (5) structural
faithfulness, (6) parsimony, (7) interpretability, (8) convergence, (9)
estimation speed, and (10) robustness. Further, we propose two example utility
decision trees that describe hierarchies and trade-offs between utilities
depending on the inferential goals that drive model building and testing.",-0.14389834,0.016073998,-0.14118777,A
10834,"If the data is not informative, there is
no point in further studying recoverability of ϕ by means of ψ.","Identiﬁability and Information Gain

Oftentimes, we are interested in learning something about the (true) real-world generator G through the
P model-dependent quantity ψ, justiﬁed by its resemblance to the model-independent quantity ϕ that we
assume to play a role in G. As a ﬁrst step, we need to study whether the data generated by the unknown
process enables the P model to extract any information about ψ at all.","In a frequentist sense, we say that a quantity
ψ is identiﬁed in the given P model, if all the possible values of ψ lead to unique conditional distributions,
that is, for any ψ1 = ψ2 we have p(y | ψ1) = p(y | ψ2) [47].",2022-09-06 12:03:30+00:00,"Some models are useful, but how do we know which ones? Towards a unified Bayesian model taxonomy",stat.ME,['stat.ME'],"[arxiv.Result.Author('Paul-Christian Bürkner'), arxiv.Result.Author('Maximilian Scholz'), arxiv.Result.Author('Stefan T. Radev')]","Probabilistic (Bayesian) modeling has experienced a surge of applications in
almost all quantitative sciences and industrial areas. This development is
driven by a combination of several factors, including better probabilistic
estimation algorithms, flexible software, increased computing power, and a
growing awareness of the benefits of probabilistic learning. However, a
principled Bayesian model building workflow is far from complete and many
challenges remain. To aid future research and applications of a principled
Bayesian workflow, we ask and provide answers for what we perceive as two
fundamental questions of Bayesian modeling, namely (a) ""What actually is a
Bayesian model?"" and (b) ""What makes a good Bayesian model?"". As an answer to
the first question, we propose the PAD model taxonomy that defines four basic
kinds of Bayesian models, each representing some combination of the assumed
joint distribution of all (known or unknown) variables (P), a posterior
approximator (A), and training data (D). As an answer to the second question,
we propose ten utility dimensions according to which we can evaluate Bayesian
models holistically, namely, (1) causal consistency, (2) parameter
recoverability, (3) predictive performance, (4) fairness, (5) structural
faithfulness, (6) parsimony, (7) interpretability, (8) convergence, (9)
estimation speed, and (10) robustness. Further, we propose two example utility
decision trees that describe hierarchies and trade-offs between utilities
depending on the inferential goals that drive model building and testing.",-0.1449978,0.0076742694,-0.14587635,A
10869,It will be considered in a further study.,"Nevertheless, we could investigate
if the type of complaints diﬀers by gender.",Table 2: Gender classiﬁcation of the sample with non-negative amounts.,2022-09-06 23:04:28+00:00,Building up Cyber Resilience by Better Grasping Cyber Risk Via a New Algorithm for Modelling Heavy-Tailed Data,stat.ME,"['stat.ME', 'cs.CR', '60G70, 60E05, 62P05, 90B50, 91G70']","[arxiv.Result.Author('Michel Dacorogna'), arxiv.Result.Author('Nehla Debbabi'), arxiv.Result.Author('Marie Kratz')]","Cyber security and resilience are major challenges in our modern economies;
this is why they are top priorities on the agenda of governments, security and
defense forces, management of companies and organizations. Hence, the need of a
deep understanding of cyber risks to improve resilience. We propose here an
analysis of the database of the cyber complaints filed at the {\it Gendarmerie
Nationale}. We perform this analysis with a new algorithm developed for
non-negative asymmetric heavy-tailed data, which could become a handy tool in
applied fields. This method gives a good estimation of the full distribution
including the tail. Our study confirms the finiteness of the loss expectation,
necessary condition for insurability. Finally, we draw the consequences of this
model for risk management, compare its results to other standard EVT models,
and lay the ground for a classification of attacks based on the fatness of the
tail.",0.41598627,-0.058139373,-0.09081644,C
11044,"How to robustly and eﬃciently conduct transfer learning directly on the outcome model,

i.e., on β, in the SS learning framework is worth of further research and beyond the scope

                                                      21
of this paper.","In fact,

the source unlabeled observations can only be transferred to improve the estimation rate
when the density ratio model is correctly speciﬁed, since pY(T|)X can be diﬀerent from p(YS|)X.","Moreover, the theoretical results in Theorem 1 and Corollary 1 shed light on
transfer learning of other high dimensional nonconvex problems under the restricted strong
convexity condition.",2022-09-12 01:30:22+00:00,Semi-supervised Triply Robust Inductive Transfer Learning,stat.ME,['stat.ME'],"[arxiv.Result.Author('Tianxi Cai'), arxiv.Result.Author('Mengyan Li'), arxiv.Result.Author('Molei Liu')]","In this work, we propose a semi-supervised triply robust inductive transfer
learning (STRIFLE) approach, which integrates heterogeneous data from label
rich source population and label scarce target population to improve the
learning accuracy in the target population. Specifically, we consider a high
dimensional covariate shift setting and employ two nuisance models, a density
ratio model and an imputation model, to combine transfer learning and
surrogate-assisted semi-supervised learning strategies organically and achieve
triple robustness. While the STRIFLE approach requires the target and source
populations to share the same conditional distribution of outcome Y given both
the surrogate features S and predictors X, it allows the true underlying model
of Y|X to differ between the two populations due to the potential covariate
shift in S and X. Different from double robustness, even if both nuisance
models are misspecified or the distribution of Y|S,X is not the same between
the two populations, when the transferred source population and the target
population share enough similarities, the triply robust STRIFLE estimator can
still partially utilize the source population, and it is guaranteed to be no
worse than the target-only surrogate-assisted semi-supervised estimator with
negligible errors. These desirable properties of our estimator are established
theoretically and verified in finite-sample via extensive simulation studies.
We utilize the STRIFLE estimator to train a Type II diabetes polygenic risk
prediction model for the African American target population by transferring
knowledge from electronic health records linked genomic data observed in a
larger European source population.",-0.26445693,0.022077888,-0.073617086,A
11344,"In fact, the inner product equation of Moran’s index deserves further research.","On the other hand, by means of
linear regressive analysis, the characteristic parameter becomes the regressive coefficient and gives
the value of Moran’s index (Chen, 2013).","Based on the inner
product equation, two spatial autocorrelation models can be constructed.",2022-09-18 12:59:03+00:00,Spatial Autocorrelation Equation Based on Moran's Index,stat.ME,"['stat.ME', 'physics.soc-ph']",[arxiv.Result.Author('Yanguang Chen')],"Based on standardized vector and globally normalized weight matrix, Moran's
index can be expressed as a simple formula. Further, based on inner product and
outer product of the standardized vector, a series of spatial autocorrelation
equations can be constructed for Moran's index. However, the theoretical
foundations and application direction of these equations are not yet clear.
This paper is devoted to exploring the inner and outer product equations of
Moran's index. The methods include mathematical derivation and empirical
analysis. The results are as follows. First, based on the inner product
equation, two spatial autocorrelation models can be constructed. One bears
constant terms, and the other bear no constant term. The spatial
autocorrelation models can be employed to calculate Moran's index by means of
regression analysis. Second, the inner and outer product equations can be used
to improve Moran's scatterplot. The normalized Moran's scatterplot can show
more geospatial information than the conventional Moran's scatterplot. A
conclusion can be reached that the spatial autocorrelation models are useful
spatial analysis tools, complementing the uses of spatial autocorrelation
coefficient and spatial autoregressive models. These models are helpful for
understanding the boundary values of Moran's index and spatial autoregressive
modeling process.",-0.12274463,0.042910837,0.24091846,A
11353,"General conclusions and prospects for further research

      The main advantages of the proposed method are the possibility of using such
indicators of an election that are difficult for falsifiers to predict, such as the number of
spoiled ballots, the number of votes ""against all"" and taking into account such a
statistical indicator as the variance of election data.",4.,"It is also possible to use the usual
statistical indicators – turnout, and support for certain candidates.",2022-09-18 21:15:52+00:00,A new approach to Statistical analysis of election results,stat.ME,"['stat.ME', 'math.PR', '62E17 (Primary), 62P25 (Secondary)']",[arxiv.Result.Author('Ivan H. Krykun')],"In this paper, a new method of detection of election fraud is proposed. This
method is based on the calculation of the ratio of two standard normal random
variables; estimation of parameters of obtained sample and comparison of these
estimates with known theoretical values of parameters. Also in the paper, there
is an example of the application of the method.",0.11176769,0.19557387,0.05227321,C
11575,"While further study is needed to compare the two approaches discussed above, we extend

the idea of incorporating a Lasso penalty term in the sample bound (not just in estimation

of nuisance parameters) to augmented IPW estimation.","By
                                                                                    IPW

this diﬀerence, the relaxation-based bound µˆˆ1h+(γˆ) can be seen to reﬂect the impact of using

the Lasso penalty more properly than the Lasso-substituted bound µˆ1IP+W(γˆ, βˆRWL,1+(γˆ)).","By mimicking (46), deﬁne

                         µˆˆ1R+CAL = µˆ1R+CAL + (Λ − Λ−1)λβ (βˆRWL,1+)1:p 1,                                                  (47)

where µˆ1R+CAL is deﬁned in (29) and βˆRWL,1+ = βˆRWL,1+(γˆRCAL), the same as in Section
4.2.",2022-09-23 03:16:55+00:00,Model-assisted sensitivity analysis for treatment effects under unmeasured confounding via regularized calibrated estimation,stat.ME,['stat.ME'],[arxiv.Result.Author('Zhiqiang Tan')],"Consider sensitivity analysis for estimating average treatment effects under
unmeasured confounding, assumed to satisfy a marginal sensitivity model. At the
population level, we provide new representations for the sharp population
bounds and doubly robust estimating functions, recently derived by Dorn, Guo,
and Kallus. We also derive new, relaxed population bounds, depending on
weighted linear outcome quantile regression. At the sample level, we develop
new methods and theory for obtaining not only doubly robust point estimators
for the relaxed population bounds with respect to misspecification of a
propensity score model or an outcome mean regression model, but also
model-assisted confidence intervals which are valid if the propensity score
model is correctly specified, but the outcome quantile and mean regression
models may be misspecified. The relaxed population bounds reduce to the sharp
bounds if outcome quantile regression is correctly specified. For a linear
outcome mean regression model, the confidence intervals are also doubly robust.
Our methods involve regularized calibrated estimation, with Lasso penalties but
carefully chosen loss functions, for fitting propensity score and outcome mean
and quantile regression models. We present a simulation study and an empirical
application to an observational study on the effects of right heart
catheterization.",-0.11627696,-0.1272245,-0.19093725,A
11576,"This ex-
change in turn motivated further research, including Ding and Miratrix (2015), Rohde (2019),
and Cinelli, Forney and Pearl (2020).","Rubin (2009) responded
that unblocked colliders are a stylized problem that has few practical ramiﬁcations.","Here, we observed that should colliders appear in a set
of control variables — along with the associated blocking variables — regularization can
unintentionally induce collider bias, revealing that colliders are not only a problem when
their parents are unobserved.",2022-09-23 04:20:50+00:00,"Feature selection in stratification estimators of causal effects: lessons from potential outcomes, causal diagrams, and structural equations",stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('P. Richard Hahn'), arxiv.Result.Author('Andrew Herren')]","What is the ideal regression (if any) for estimating average causal effects?
We study this question in the setting of discrete covariates, deriving
expressions for the finite-sample variance of various stratification
estimators. This approach clarifies the fundamental statistical phenomena
underlying many widely-cited results. Our exposition combines insights from
three distinct methodological traditions for studying causal effect estimation:
potential outcomes, causal diagrams, and structural models with additive
errors.",0.1471894,0.046743926,0.046463463,C
11585,"In
addition, further research has expanded the original local randomization RDD framework.","In particular, local randomization is often
recommended as a method for estimating RDDs with discrete running variables
(Cattaneo & Titiunik, 2021; Cattaneo & Vazquez-Bare, 2017; Skovron & Titiunik, 2015).","For

                                                                3
example, Keele, Titiunik, and Zubizarreta (2015) incorporate matching on observed covariates
into local randomization analysis, and Mattei and Mealli (2017) provide a formulation of the
Stable Unit Treatment Value Assumption applicable to local randomization RDD.",2022-09-23 12:31:13+00:00,Local Randomization Regression Discontinuity Designs when Test Scores are the Running Variable,stat.ME,['stat.ME'],[arxiv.Result.Author('Sophie Litschwartz')],"Explanations of the internal validity of regression discontinuity designs
(RDD) generally appeal to the idea that RDDs are ``as good as"" random near the
treatment cut point. Cattaneo, Frandsen, and Titiunik (2015) are the first to
take this justification to its full conclusion and propose estimating the RDD
local average treatment effect (LATE) the same as one would a randomized
experiment. This paper explores the implications of analyzing an RDD as a local
random experiment when the running variable is a test score. I derive a formula
for the bias in the LATE estimate estimated using the local randomization
method, $a\rho\Delta$. Where $a$ is the relationship between latent proficiency
and the potential outcome absent treatment, $\rho$ is the test reliability, and
$\Delta$ is the distance between the treatment and control running variable
value. I use this quantification of the bias to demonstrate that local
randomization and related design based methods for estimating RDDs are
problematic when the running variable is test score or other human developed
measure (e.g., medical tests).",0.05271868,-0.069919035,-0.075903326,A
11648,"Furthermore,
our simulations indicate that the CRT based e-statistics has competitive power to existing methods
with relatively small dimension of Z, but further research is necessary to investigate suitable e-
statistics and their power when Z is of higher dimension.","It would be of interest
to investigate the robustness of the test when this upper bound grows and the approximation QˆZ
of the conditional distributions of X | Z are sequentially updated with new samples.","Finally, the e-statistics in this paper are highly tailored to the MX assumption, and it is an open
question to us how to construct general sequential tests of conditional independence without the
MX assumption.",2022-09-26 12:35:51+00:00,Anytime Valid Tests of Conditional Independence Under Model-X,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Peter Grünwald'), arxiv.Result.Author('Alexander Henzi'), arxiv.Result.Author('Tyron Lardy')]","We propose a sequential, anytime valid method to test the conditional
independence of a response $Y$ and a predictor $X$ given a random vector $Z$.
The proposed test is based on $E$-statistics and test martingales, which
generalize likelihood ratios and allow valid inference at arbitrary stopping
times. In accordance with the recently introduced model-X setting, our test
depends on the availability of the conditional distribution of $X$ given $Z$,
or at least a sufficiently sharp approximation thereof. Within this setting, we
derive a full characterization of $E$-statistics for testing conditional
independence, investigate growth-rate optimal $E$-statistics and their power
properties, and show that our method yields tests with asymptotic power one in
the special case of a logistic regression model. A simulation study is done to
demonstrate that the approach is robust with respect to violations of the
model-X assumption and competitive in terms of power when compared to
established sequential and non-sequential testing methods.",-0.14458849,-0.026166,-0.20627631,A
11649,"Furthermore,

our simulations indicate that the CRT based e-statistics has competitive power to existing methods

with relatively small dimension of Z, but further research is necessary to investigate suitable e-

statistics and their power when Z is of higher dimension.","It would be of interest
to investigate the robustness of the test when this upper bound grows and the approximation QˆZ

of the conditional distributions of X | Z are sequentially updated with new samples.","Finally, the e-statistics in this paper are highly tailored to the MX assumption, and it is an open

question to us how to construct general sequential tests of conditional independence without the

MX assumption.",2022-09-26 12:35:51+00:00,Anytime Valid Tests of Conditional Independence Under Model-X,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Peter Grünwald'), arxiv.Result.Author('Alexander Henzi'), arxiv.Result.Author('Tyron Lardy')]","We propose a sequential, anytime valid method to test the conditional
independence of a response $Y$ and a predictor $X$ given a random vector $Z$.
The proposed test is based on e-statistics and test martingales, which
generalize likelihood ratios and allow valid inference at arbitrary stopping
times. In accordance with the recently introduced model-X setting, our test
depends on the availability of the conditional distribution of $X$ given $Z$,
or at least a sufficiently sharp approximation thereof. Within this setting, we
derive a full characterization of e-statistics for testing conditional
independence, investigate growth-rate optimal e-statistics and their power
properties, and show that our method yields tests with asymptotic power one in
the special case of a logistic regression model. A simulation study is done to
demonstrate that the approach is robust with respect to violations of the
model-X assumption and competitive in terms of power when compared to
established sequential and non-sequential testing methods.",-0.14458849,-0.026166,-0.20627631,A
11733,"We claim that such ""negative"" studies, if they
can explain why a method does not work in a specific situation, are needed to increase the
community's understanding, to stimulate further research and to stop other researchers from
investing time and resources in the same dead-end idea [5].","In practice, such studies may often reveal
no or only a small benefit of a new method and researchers often do not try to disseminate them or,
if they do, journals may be reluctant to publish them.","Methodological phase II may have the aim to prove that a method can be used with caution in an
applied setting which is not completely identical to the developer's target setting.",2022-09-27 13:07:29+00:00,Phases of methodological research in biostatistics - building the evidence base for new methods,stat.ME,"['stat.ME', '62A01 (Primary)']","[arxiv.Result.Author('Georg Heinze'), arxiv.Result.Author('Anne-Laure Boulesteix'), arxiv.Result.Author('Michael Kammer'), arxiv.Result.Author('Tim P. Morris'), arxiv.Result.Author('Ian R. White')]","Although the biostatistical scientific literature publishes new methods at a
very high rate, many of these developments are not trustworthy enough to be
adopted by the scientific community. We propose a framework to think about how
a piece of methodological work contributes to the evidence base for a method.
Similarly to the well-known phases of clinical research in drug development, we
define four phases of methodological research. These four phases cover (I)
providing logical reasoning and proofs, (II) providing empirical evidence,
first in a narrow target setting, then (III) in an extended range of settings
and for various outcomes, accompanied by appropriate application examples, and
(IV) investigations that establish a method as sufficiently well-understood to
know when it is preferred over others and when it is not. We provide basic
definitions of the four phases but acknowledge that more work is needed to
facilitate unambiguous classification of studies into phases. Methodological
developments that have undergone all four proposed phases are still rare, but
we give two examples with references. Our concept rebalances the emphasis to
studies in phase III and IV, i.e., carefully planned methods comparison studies
and studies that explore the empirical properties of existing methods in a
wider range of problems.",0.43386176,0.15796243,0.13744247,C
11835,"Section 6 concludes
the paper with some further research topics.","Section 5 analyzes the multinational macroeconomic indices data,
and the CT image data collected during early COVID-19 pandemic.","All the technical proofs are left in the
supplementary materials.",2022-09-29 15:02:11+00:00,Modeling High-Dimensional Matrix-Variate Observations by Tensor Factorization,stat.ME,['stat.ME'],"[arxiv.Result.Author('Xu Zhang'), arxiv.Result.Author('Zhen Pang'), arxiv.Result.Author('Jianhua Guo'), arxiv.Result.Author('Alan H. Welsh'), arxiv.Result.Author('Catherine C. Liu')]","In the era of big data, it is prevailing of high-dimensional matrix-variate
observations that may be independent or dependent. Unsupervised learning of
matrix objects through low-rank approximation has benefited discovery of the
hidden pattern and structure whilst concomitant statistical inference is known
challenging and yet in infancy by the fact that, there is limited work and all
focus on a class of bilinear form matrix factor models. In this paper, we
propose a novel class of hierarchical CP product matrix factor models which
model the rank-1 components of the low-rank CP decomposition of a matrix object
by the tool of high-dimensional vector factor models. The induced CP
tensor-decomposition based matrix factor model (TeDFaM) are apparently more
informative in that it naturally incorporates the row-wise and column-wise
interrelated information. Furthermore, the inner separable covariance structure
yields efficient moment estimators of the loading matrices and thus approximate
least squares estimators for the factor scores. The proposed TeDFaM model and
estimation procedure make the signal part achieves better peak signal to noise
ratio, evidenced in both theory and numerical analytics compared to bilinear
form matrix factor models and existing methods. We establish an inferential
theory for TeDFaM estimation including consistency, rates of convergence, and
the limiting distributions under regular conditions. In applications, the
proposed model and estimation procedure are superior in terms of matrix
reconstruction for both independent two-dimensional image data and serial
correlated matrix time series. The algorithm is fast and can be implemented
expediently through an accompanied R package TeDFaM.",0.08481525,0.08078306,0.014147813,C
11836,"Conclusion

To conclude, we identify possible topics for further research.",7.,"First of all, it is interesting
that our technical angle is inverse to that in the preprint (Chang et al., 2021), where they
raise the order of the matrix (2nd-order) series to a 3rd-order tensor and model it with the
CP decomposition (Kolda and Bader, 2009) with a diagonal core matrix approximation
replaced in the bilinear form.",2022-09-29 15:02:11+00:00,Modeling High-Dimensional Matrix-Variate Observations by Tensor Factorization,stat.ME,['stat.ME'],"[arxiv.Result.Author('Xu Zhang'), arxiv.Result.Author('Zhen Pang'), arxiv.Result.Author('Jianhua Guo'), arxiv.Result.Author('Alan H. Welsh'), arxiv.Result.Author('Catherine C. Liu')]","In the era of big data, it is prevailing of high-dimensional matrix-variate
observations that may be independent or dependent. Unsupervised learning of
matrix objects through low-rank approximation has benefited discovery of the
hidden pattern and structure whilst concomitant statistical inference is known
challenging and yet in infancy by the fact that, there is limited work and all
focus on a class of bilinear form matrix factor models. In this paper, we
propose a novel class of hierarchical CP product matrix factor models which
model the rank-1 components of the low-rank CP decomposition of a matrix object
by the tool of high-dimensional vector factor models. The induced CP
tensor-decomposition based matrix factor model (TeDFaM) are apparently more
informative in that it naturally incorporates the row-wise and column-wise
interrelated information. Furthermore, the inner separable covariance structure
yields efficient moment estimators of the loading matrices and thus approximate
least squares estimators for the factor scores. The proposed TeDFaM model and
estimation procedure make the signal part achieves better peak signal to noise
ratio, evidenced in both theory and numerical analytics compared to bilinear
form matrix factor models and existing methods. We establish an inferential
theory for TeDFaM estimation including consistency, rates of convergence, and
the limiting distributions under regular conditions. In applications, the
proposed model and estimation procedure are superior in terms of matrix
reconstruction for both independent two-dimensional image data and serial
correlated matrix time series. The algorithm is fast and can be implemented
expediently through an accompanied R package TeDFaM.",-0.2715176,-0.18174738,0.20892927,A
11837,"Next, the extracted latent factor
scores can be regarded as a low-dimensional feature extracted from the original high-
dimensional tensor observation, and it can be used for some further research aim such
as prediction or clustering.","One
may conjecture that, under TeDFaM, the estimation accuracy may be further enhanced
if the latent factor matrix may be modiﬁed properly.","Another direction is, the BiMFaM has been extended to the
tensor observations in time series ﬁeld (Chen et al., 2022), then how to extend TeDFaM
to tensor case is an interesting working direction.",2022-09-29 15:02:11+00:00,Modeling High-Dimensional Matrix-Variate Observations by Tensor Factorization,stat.ME,['stat.ME'],"[arxiv.Result.Author('Xu Zhang'), arxiv.Result.Author('Zhen Pang'), arxiv.Result.Author('Jianhua Guo'), arxiv.Result.Author('Alan H. Welsh'), arxiv.Result.Author('Catherine C. Liu')]","In the era of big data, it is prevailing of high-dimensional matrix-variate
observations that may be independent or dependent. Unsupervised learning of
matrix objects through low-rank approximation has benefited discovery of the
hidden pattern and structure whilst concomitant statistical inference is known
challenging and yet in infancy by the fact that, there is limited work and all
focus on a class of bilinear form matrix factor models. In this paper, we
propose a novel class of hierarchical CP product matrix factor models which
model the rank-1 components of the low-rank CP decomposition of a matrix object
by the tool of high-dimensional vector factor models. The induced CP
tensor-decomposition based matrix factor model (TeDFaM) are apparently more
informative in that it naturally incorporates the row-wise and column-wise
interrelated information. Furthermore, the inner separable covariance structure
yields efficient moment estimators of the loading matrices and thus approximate
least squares estimators for the factor scores. The proposed TeDFaM model and
estimation procedure make the signal part achieves better peak signal to noise
ratio, evidenced in both theory and numerical analytics compared to bilinear
form matrix factor models and existing methods. We establish an inferential
theory for TeDFaM estimation including consistency, rates of convergence, and
the limiting distributions under regular conditions. In applications, the
proposed model and estimation procedure are superior in terms of matrix
reconstruction for both independent two-dimensional image data and serial
correlated matrix time series. The algorithm is fast and can be implemented
expediently through an accompanied R package TeDFaM.",-0.1433194,0.102076545,0.2997574,B
11886,"This is an area for further research and development in
the ﬁeld of generalized ﬁducial inference.","A potential solution to this issue is the use of autodifferentiation
software to differentiate the DGA.","Meanwhile, these experiments show that the re-
turn projection scheme in Zappa et al.",2022-09-30 13:58:34+00:00,Generalized Fiducial Inference on Differentiable Manifolds,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Alexander C Murph'), arxiv.Result.Author('Jan Hannig'), arxiv.Result.Author('Jonathan P Williams')]","We introduce a novel approach to inference on parameters that take values in
a Riemannian manifold embedded in a Euclidean space. Parameter spaces of this
form are ubiquitous across many fields, including chemistry, physics, computer
graphics, and geology. This new approach uses generalized fiducial inference to
obtain a posterior-like distribution on the manifold, without needing to know a
parameterization that maps the constrained space to an unconstrained Euclidean
space. The proposed methodology, called the constrained generalized fiducial
distribution (CGFD), is obtained by using mathematical tools from Riemannian
geometry. A Bernstein-von Mises-type result for the CGFD, which provides
intuition for how the desirable asymptotic qualities of the unconstrained
generalized fiducial distribution are inherited by the CGFD, is provided. To
demonstrate the practical use of the CGFD, we provide three proof-of-concept
examples: inference for data from a multivariate normal density with the mean
parameters on a sphere, a linear logspline density estimation problem, and a
reimagined approach to the AR(1) model, all of which exhibit desirable
coverages via simulation. We discuss two Markov chain Monte Carlo algorithms
for the exploration of these constrained parameter spaces and adapt them for
the CGFD.",-0.14815345,-0.09978661,0.020042917,A
11887,"This is an area for further research and development in
the ﬁeld of generalized ﬁducial inference.","A potential solution to this issue is the use of autodifferentiation
software to differentiate the DGA.","Meanwhile, these experiments show that the re-
turn projection scheme in Zappa et al.",2022-09-30 13:58:34+00:00,Generalized Fiducial Inference on Differentiable Manifolds,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Alexander C Murph'), arxiv.Result.Author('Jan Hannig'), arxiv.Result.Author('Jonathan P Williams')]","We introduce a novel approach to inference on parameters that take values in
a Riemannian manifold embedded in a Euclidean space. Parameter spaces of this
form are ubiquitous across many fields, including chemistry, physics, computer
graphics, and geology. This new approach uses generalized fiducial inference to
obtain a posterior-like distribution on the manifold, without needing to know a
parameterization that maps the constrained space to an unconstrained Euclidean
space. The proposed methodology, called the constrained generalized fiducial
distribution (CGFD), is obtained by using mathematical tools from Riemannian
geometry. A Bernstein-von Mises-type result for the CGFD, which provides
intuition for how the desirable asymptotic qualities of the unconstrained
generalized fiducial distribution are inherited by the CGFD, is provided. To
demonstrate the practical use of the CGFD, we provide three proof-of-concept
examples: inference for data from a multivariate normal density with the mean
parameters on a sphere, a linear logspline density estimation problem, and a
reimagined approach to the AR(1) model, all of which exhibit desirable
coverages via simulation. We discuss two Markov chain Monte Carlo algorithms
for the exploration of these constrained parameter spaces and adapt them for
the CGFD.",-0.14815345,-0.09978661,0.020042917,A
11930,dMEGA has several limitations that warrant further research.,"dMEGA approximates the marginal
distributions, and it provides more robust estimation since dMEGA consid-
ers the marginal predicted ratios associated with each local site, not just
identifying samples with particularly high predicted ratios[40].","While our
federated testing approach has small network traﬃc requirements, each local
site is required to handle high computational load.",2022-10-01 23:57:09+00:00,Federated Generalized Linear Mixed Models for Collaborative Genome-wide Association Studies,stat.ME,"['stat.ME', 'q-bio.GN']","[arxiv.Result.Author('Wentao Li'), arxiv.Result.Author('Han Chen'), arxiv.Result.Author('Xiaoqian Jiang'), arxiv.Result.Author('Arif Harmanci')]","As the sequencing costs are decreasing, there is great incentive to perform
large scale association studies to increase power of detecting new variants.
Federated association testing among different institutions is a viable solution
for increasing sample sizes by sharing the intermediate testing statistics that
are aggregated by a central server. There are, however, standing challenges to
performing federated association testing. Association tests are known to be
confounded by numerous factors such as population stratification, which can be
especially important in multiancestral studies and in admixed populations among
different sites. Furthermore, disease etiology should be considered via
flexible models to avoid biases in the significance of the genetic effect. A
rising challenge for performing large scale association studies is the privacy
of participants and related ethical concerns of stigmatization and
marginalization. Here, we present dMEGA, a flexible and efficient method for
performing federated generalized linear mixed model based association testing
among multiple sites while underlying genotype and phenotype data are not
explicitly shared. dMEGA first utilizes a reference projection to estimate
population-based covariates without sharing genotype dataset among sites. Next,
dMEGA uses Laplacian approximation for the parameter likelihoods and decomposes
parameter estimation into efficient local-gradient updates among sites. We use
simulated and real datasets to demonstrate the accuracy and efficiency of
dMEGA. Overall, dMEGA's formulation is flexible to integrate fixed and random
effects in a federated setting.",0.072654076,-0.027054192,0.12095368,C
11999,"At a minimum, the
complexities of measuring structural racism that [18] and others note suggest that group
deﬁnition for algorithmic fairness work involving race deserves further study.","Recent work on a
multidimensional measure of structural racism [10] provides another potential alternative
deﬁnition of categories that could be used in metrics such as ours.","References

 [1] Zeeshan Aleem.",2022-10-03 19:16:34+00:00,An intersectional framework for counterfactual fairness in risk prediction,stat.ME,['stat.ME'],"[arxiv.Result.Author('Solvejg Wastvedt'), arxiv.Result.Author('Jared Huling'), arxiv.Result.Author('Julian Wolfson')]","Along with the increasing availability of data in many sectors has come the
rise of data-driven models to inform decision-making and policy. In the health
care sector, these models have the potential to benefit both patients and
health care providers but can also entrench or exacerbate health inequities.
Existing ""algorithmic fairness"" methods for measuring and correcting model bias
fall short of what is needed for clinical applications in two key ways. First,
methods typically focus on a single grouping along which discrimination may
occur rather than considering multiple, intersecting groups such as gender and
race. Second, in clinical applications, risk prediction is typically used to
guide treatment, and use of a treatment presents distinct statistical issues
that invalidate most existing fairness measurement techniques. We present novel
unfairness metrics that address both of these challenges. We also develop a
complete framework of estimation and inference tools for our metrics, including
the unfairness value (""u-value""), used to determine the relative extremity of
an unfairness measurement, and standard errors and confidence intervals
employing an alternative to the standard bootstrap.",0.20535272,-0.16415417,0.2101567,C
12152,"However, we refrain from further study of the original IPW and doubly robust
estimators, θqIP W ∗ and θqDR∗, because they can be computationally more complicated than
the smoothed estimators proposed in this work.","The ﬁnite sample performance of θqIP W and θqDR is studied in the simulation study
(Section 5).","In the remainder of the manuscript, we
also refer to θqIP W for the IPW estimator and θqDR for the doubly robust estimator.",2022-10-08 20:29:41+00:00,Doubly robust estimation and sensitivity analysis for marginal structural quantile models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Chao Cheng'), arxiv.Result.Author('Liangyuan Hu'), arxiv.Result.Author('Fan Li')]","The marginal structure quantile model (MSQM) is a useful tool to characterize
the causal effect of a time-varying treatment on the full distribution of
potential outcomes. However, to date, only the inverse probability weighting
(IPW) approach has been developed to identify the structural causal parameters
in MSQM, which requires correct specification of the propensity score models
for the treatment assignment mechanism. We propose a doubly robust approach for
the MSQM under the semiparametric framework. We derive the efficient influence
function associated with a MSQM and estimate causal parameters in the MSQM by
combining IPW and a new iterative conditional regression approach that models
the full potential outcome distribution. The proposed approach is consistent if
either of the models associated with treatment assignment or the potential
outcome distributions is correctly specified, and is locally efficient if both
models are correct. To implement the doubly robust MSQM estimator, we propose
to solve a smoothed estimating equation to facilitate efficient computation of
the point and variance estimates. In addition, we develop a new confounding
function and sensitivity analysis strategy to investigate the robustness of
several MSQM estimators when the no unmeasured confounding assumption is
violated. We apply the proposed methods to the Yale New Haven Health System
Electronic Health Record data to study the causal effect of antihypertensive
medications to inpatients with severe hypertension, and assess the robustness
of findings to unmeasured time-varying confounding.",-0.20196937,-0.021490477,-0.29165044,A
12245,"This streamlined process will aid
scientists in determining which interventions should be prioritized for further study, awarded funding,
published, reproduced with repeated experiments, investigated in related contexts, and translated for
societal use.","We use real data to illustrate the use of contra plots for a continuous variable
using the relative difference in means as a measure of effect size.","Problem Description

         Prioritizing interventions by their effect size is a data-informed instance of decision-making.",2022-10-10 17:37:07+00:00,Contra-Analysis: Prioritizing Meaningful Effect Size in Scientific Research,stat.ME,"['stat.ME', 'q-bio.QM']","[arxiv.Result.Author('Bruce A. Corliss'), arxiv.Result.Author('Yaotian Wang'), arxiv.Result.Author('Heman Shakeri'), arxiv.Result.Author('Philip E. Bourne')]","At every phase of scientific research, scientists must decide how to allocate
limited resources to pursue the research inquiries with the greatest potential.
This prioritization dictates which controlled interventions are studied,
awarded funding, published, reproduced with repeated experiments, investigated
in related contexts, and translated for societal use. There are many factors
that influence this decision-making, but interventions with larger effect size
are often favored because they exert the greatest influence on the system
studied. To inform these decisions, scientists must compare effect size across
studies with dissimilar experiment designs to identify the interventions with
the largest effect. These studies are often only loosely related in nature,
using experiments with a combination of different populations, conditions,
timepoints, measurement techniques, and experiment models that measure the same
phenomenon with a continuous variable. We name this assessment contra-analysis
and propose to use credible intervals of the relative difference in means to
compare effect size across studies in a meritocracy between competing
interventions. We propose a data visualization, the contra plot, that allows
scientists to score and rank effect size between studies that measure the same
phenomenon, aid in determining an appropriate threshold for meaningful effect,
and perform hypothesis tests to determine which interventions have meaningful
effect size. We illustrate the use of contra plots with real biomedical
research data. Contra-analysis promotes a practical interpretation of effect
size and facilitates the prioritization of scientific research.",0.5010819,0.29935768,-0.010336032,C
12246,"Scientists must collectively decide which
interventions demonstrate the most potential for further research, funding, commercialization, and
translation.","A multitude of studies have reported results
that demonstrate various interventions lowering TPC.","TPC levels vary from 60-3000 mg/dL across animal models used to research atherosclerosis
and are reported in units of mmol/L as well (Table S1).",2022-10-10 17:37:07+00:00,Contra-Analysis: Prioritizing Meaningful Effect Size in Scientific Research,stat.ME,"['stat.ME', 'q-bio.QM']","[arxiv.Result.Author('Bruce A. Corliss'), arxiv.Result.Author('Yaotian Wang'), arxiv.Result.Author('Heman Shakeri'), arxiv.Result.Author('Philip E. Bourne')]","At every phase of scientific research, scientists must decide how to allocate
limited resources to pursue the research inquiries with the greatest potential.
This prioritization dictates which controlled interventions are studied,
awarded funding, published, reproduced with repeated experiments, investigated
in related contexts, and translated for societal use. There are many factors
that influence this decision-making, but interventions with larger effect size
are often favored because they exert the greatest influence on the system
studied. To inform these decisions, scientists must compare effect size across
studies with dissimilar experiment designs to identify the interventions with
the largest effect. These studies are often only loosely related in nature,
using experiments with a combination of different populations, conditions,
timepoints, measurement techniques, and experiment models that measure the same
phenomenon with a continuous variable. We name this assessment contra-analysis
and propose to use credible intervals of the relative difference in means to
compare effect size across studies in a meritocracy between competing
interventions. We propose a data visualization, the contra plot, that allows
scientists to score and rank effect size between studies that measure the same
phenomenon, aid in determining an appropriate threshold for meaningful effect,
and perform hypothesis tests to determine which interventions have meaningful
effect size. We illustrate the use of contra plots with real biomedical
research data. Contra-analysis promotes a practical interpretation of effect
size and facilitates the prioritization of scientific research.",0.47133383,0.1044938,0.106103756,C
12247,"Interventions that exceed the threshold would be favorably
considered for further study and development.","Therefore, scientists with different perspectives or studying different applications can use the
same contra plot to perform this analysis.","Scoring and Ranking Effect Sizes
         While hypothesis testing will reveal which results have evidence of meaningful effect size and

highlight options with potential for further development, it would be useful to compare effect size
between studies already deemed meaningful.",2022-10-10 17:37:07+00:00,Contra-Analysis: Prioritizing Meaningful Effect Size in Scientific Research,stat.ME,"['stat.ME', 'q-bio.QM']","[arxiv.Result.Author('Bruce A. Corliss'), arxiv.Result.Author('Yaotian Wang'), arxiv.Result.Author('Heman Shakeri'), arxiv.Result.Author('Philip E. Bourne')]","At every phase of scientific research, scientists must decide how to allocate
limited resources to pursue the research inquiries with the greatest potential.
This prioritization dictates which controlled interventions are studied,
awarded funding, published, reproduced with repeated experiments, investigated
in related contexts, and translated for societal use. There are many factors
that influence this decision-making, but interventions with larger effect size
are often favored because they exert the greatest influence on the system
studied. To inform these decisions, scientists must compare effect size across
studies with dissimilar experiment designs to identify the interventions with
the largest effect. These studies are often only loosely related in nature,
using experiments with a combination of different populations, conditions,
timepoints, measurement techniques, and experiment models that measure the same
phenomenon with a continuous variable. We name this assessment contra-analysis
and propose to use credible intervals of the relative difference in means to
compare effect size across studies in a meritocracy between competing
interventions. We propose a data visualization, the contra plot, that allows
scientists to score and rank effect size between studies that measure the same
phenomenon, aid in determining an appropriate threshold for meaningful effect,
and perform hypothesis tests to determine which interventions have meaningful
effect size. We illustrate the use of contra plots with real biomedical
research data. Contra-analysis promotes a practical interpretation of effect
size and facilitates the prioritization of scientific research.",0.61916804,0.1275636,0.032624975,C
12278,"Kontsevich
and Tyler (1999) further study a problem of estimating the threshold while regarding the slope
parameter as a “nuisance” parameter.",A Bayesian version of the “best PEST” is given by Watson and Pelli (1983).,"Targeting speciﬁc parameters of the psychometric function
while accounting for the other parameters is related to so-called psi-methods, which we do not
elaborate on here (see Kingdom and Prins, 2016, Section 5.4 for an overview).",2022-10-11 07:11:35+00:00,Estimating psychometric functions from adaptive designs,stat.ME,['stat.ME'],"[arxiv.Result.Author('Simon Bang Kristensen'), arxiv.Result.Author('Katrine Bødkergaard'), arxiv.Result.Author('Bo Martin Bibby')]","An adaptive design adjusts dynamically as information is accrued and a
consequence of applying an adaptive design is the potential for inducing
small-sample bias in estimates. In psychometrics and psychophysics, a common
class of studies investigate a subject's ability to perform a task as a
function of the stimulus intensity, meaning the amount or clarity of the
information supplied for the task. The relationship between the performance and
intensity is represented by a psychometric function. Such experiments routinely
apply adaptive designs, which use both previous intensities and performance to
assign stimulus intensities, the strategy being to sample intensities where the
information about the psychometric function is maximised. Similar schemes are
often applied in drug trials to assign doses dynamically using doses and
responses from earlier observations. The present paper investigates the
influence of adaptation on statistical inference about the psychometric
function focusing on estimation, considering both parametric and non-parametric
estimation under both fixed and adaptive designs in schemes encompassing within
subject independence as well as dependence through random effects. We study the
scenarios analytically, focussing on a latent class model to derive results
under random effects, and numerically through a simulation study. We show that
while the asymptotic properties of estimators are preserved under adaptation,
the adaptive nature of the design introduces small-sample bias, in particular
in the slope parameter of the psychometric function. We argue that this poses a
dilemma for a study applying an adaptive design in the form of a trade-off
between more efficient sampling and the need to increase the number of samples
to ameliorate small-sample bias.",0.08755976,-0.016797688,-0.15897432,C
12417,"Appendix B collects further study
of interesting properties of the attined minimum as a mapping of W .","The only possible technical issues are measurability of exp(−nW Rˆn(θ)) and ﬁniteness of

5
log ZW (y), which are taken care of in proof of Lemma 1.",We close the Section with some intuition of the role of W in the Gibbs posterior.,2022-10-13 11:42:28+00:00,Probabilistic Approach to Parameteric Inverse Problems Using Gibbs Posteriors,stat.ME,['stat.ME'],"[arxiv.Result.Author('Youngsoo Baek'), arxiv.Result.Author('Wilkins Aquino'), arxiv.Result.Author('Sayan Mukherjee')]","We propose a general framework for obtaining probabilistic solutions to
PDE-based inverse problems. Bayesian methods are attractive for uncertainty
quantification, but assume knowledge of the likelihood model or data generation
process. This assumption is difficult to justify in many inverse problems, in
which the random map from the parameters to the data is complex and nonlinear.
We adopt a Gibbs posterior framework that directly posits a regularized
variational problem on the space of probability distributions of the parameter.
The minimizing solution generalizes the Bayes posterior and is robust to
misspecification or lack of knowledge of the generative model. We provide
cross-validation procedures to set the regularization hyperparameter in our
inference framework. A practical and computational advantage of our framework
is that its implementation draws on existing tools in Bayesian computational
statistics. We illustrate the utility of our framework via a simulated example,
motivated by dispersion-based wave models used to characterize artertial
vessels in ultrasound vibrometry.",-0.19020393,-0.26631683,-0.14578792,A
12741,"While

further research is needed to extend the uncertain pooling methodology to

this case the approach is clear.","(2011) and Nandram, Berg and Barboza (2014).","Let j denote a small area, e.g., a US county,

and i denote a data source where j = 1, ..., J and i = 1, ..., L. As above g de-

notes a generic partition with generic subset Sk(g) for k = 1, ..., d(g).",2022-10-19 18:39:14+00:00,Combining Data from Surveys and Related Sources,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Dexter Cahoy'), arxiv.Result.Author('Joseph Sedransk')]","To improve the precision of inferences and reduce costs there is considerable
interest in combining data from several sources such as sample surveys and
administrative data. Appropriate methodology is required to ensure satisfactory
inferences since the target populations and methods for acquiring data may be
quite different. To provide improved inferences we use methodology that has a
more general structure than the ones in current practice. We start with the
case where the analyst has only summary statistics from each of the sources. In
our primary method, uncertain pooling, it is assumed that the analyst can
regard one source, survey $r$, as the single best choice for inference. This
method starts with the data from survey $r$ and adds data from those other
sources that are shown to form clusters that include survey $r$. We also
consider Dirichlet process mixtures, one of the most popular nonparametric
Bayesian methods. We use analytical expressions and the results from numerical
studies to show properties of the methodology.",-0.06449241,-0.057473302,0.023704119,A
12803,"The ideas and methodologies presented in the paper
open up a new avenue of further research on the important question of volatility
change-point in ﬁnancial data.","The extensive simulations
presented in the paper clearly establish the superiority of the semiparamet-
ric approach over the traditional Gaussian GARCH model in the context of
change-point detection.","References

 [1] Dias A, Embrechts P. Dynamic copula models for multivariate high-
      frequency data in ﬁnance.",2022-10-20 18:45:33+00:00,A Semiparametric Approach to the Detection of Change-points in Volatility Dynamics of Financial Data,stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author('Huaiyu Hu'), arxiv.Result.Author('Ashis Gangopadhyay')]","One of the most important features of financial time series data is
volatility. There are often structural changes in volatility over time, and an
accurate estimation of the volatility of financial time series requires careful
identification of change-points. A common approach to modeling the volatility
of time series data is the well-known GARCH model. Although the problem of
change-point estimation of volatility dynamics derived from the GARCH model has
been considered in the literature, these approaches rely on parametric
assumptions of the conditional error distribution, which are often violated in
financial time series. This may lead to inaccuracies in change-point detection
resulting in unreliable GARCH volatility estimates. This paper introduces a
novel change-point detection algorithm based on a semiparametric GARCH model.
The proposed method retains the structural advantages of the GARCH process
while incorporating the flexibility of nonparametric conditional error
distribution. The approach utilizes a penalized likelihood derived from a
semiparametric GARCH model and an efficient binary segmentation algorithm. The
results show that in terms of change-point estimation and detection accuracy,
the semiparametric method outperforms the commonly used Quasi-MLE (QMLE) and
other variations of GARCH models in wide-ranging scenarios.",-0.2866554,-0.029694933,0.036131863,A
13011,"These static snapshots of the spike protein
have been useful as a starting point for the further study of its conformational dynamics;
e.g., Williams et al.","Since the initial outbreak, laboratory work (e.g., Wrapp et al., 2020) has provided PDB
structures of the SARS-CoV-2 spike protein.","(2022) used PDB structures of the RBD in its prefusion state (i.e., prior
to binding with ACE2) to study Loop 3.",2022-10-25 17:57:50+00:00,Estimating Boltzmann Averages for Protein Structural Quantities Using Sequential Monte Carlo,stat.ME,['stat.ME'],"[arxiv.Result.Author('Zhaoran Hou'), arxiv.Result.Author('Samuel W. K. Wong')]","Sequential Monte Carlo (SMC) methods are widely used to draw samples from
intractable target distributions. Particle degeneracy can hinder the use of SMC
when the target distribution is highly constrained or multimodal. As a
motivating application, we consider the problem of sampling protein structures
from the Boltzmann distribution. This paper proposes a general SMC method that
propagates multiple descendants for each particle, followed by resampling to
maintain the desired number of particles. Simulation studies demonstrate the
efficacy of the method for tackling the protein sampling problem. As a real
data example, we use our method to estimate the number of atomic contacts for a
key segment of the SARS-CoV-2 viral spike protein.",0.10005038,-0.13963467,0.3462047,C
13039,These areas of further research remain to be pursued.,"This would allow us, for example, to examine
questions of reciprocity and complementarity of help over time as well as contemporane-
ously as was done here.","Acknowledgements This research was supported by a UK Economic and Social Re-
search Council (ESRC) grant “Methods for the Analysis of Longitudinal Dyadic Data with
an Application to Inter-generational Exchanges of Family Support” (ref.",2022-10-26 14:34:55+00:00,Modelling Correlation Matrices in Multivariate Dyadic Data: Latent Variable Models for Intergenerational Exchanges of Family Support,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Siliang Zhang'), arxiv.Result.Author('Jouni Kuha'), arxiv.Result.Author('Fiona Steele')]","We define a model for the joint distribution of multiple continuous latent
variables which includes a model for how their correlations depend on
explanatory variables. This is motivated by and applied to social scientific
research questions in the analysis of intergenerational help and support within
families, where the correlations describe reciprocity of help between
generations and complementarity of different kinds of help. We propose an MCMC
procedure for estimating the model which maintains the positive definiteness of
the implied correlation matrices, and describe theoretical results which
justify this approach and facilitate efficient implementation of it. The model
is applied to data from the UK Household Longitudinal Study to analyse
exchanges of practical and financial support between adult individuals and
their non-coresident parents.",0.26306438,0.049414918,0.07820984,C
13090,"In
practical cases, the exact choice of α has to be determined case-by-case and is an open problem for
further research.","simplicity, we performed our experiments using two speciﬁc values of α (α = 0.01, α = 10).",Nemenman et al.,2022-10-27 13:17:47+00:00,Bayesian Inference of Transition Matrices from Incomplete Graph Data with a Topological Prior,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Vincenzo Perri'), arxiv.Result.Author('Luka V. Petrović'), arxiv.Result.Author('Ingo Scholtes')]","Many network analysis and graph learning techniques are based on models of
random walks which require to infer transition matrices that formalize the
underlying stochastic process in an observed graph. For weighted graphs, it is
common to estimate the entries of such transition matrices based on the
relative weights of edges. However, we are often confronted with incomplete
data, which turns the construction of the transition matrix based on a weighted
graph into an inference problem. Moreover, we often have access to additional
information, which capture topological constraints of the system, i.e. which
edges in a weighted graph are (theoretically) possible and which are not, e.g.
transportation networks, where we have access to passenger trajectories as well
as the physical topology of connections, or a set of social interactions with
the underlying social structure. Combining these two different sources of
information to infer transition matrices is an open challenge, with
implications on the downstream network analysis tasks.
  Addressing this issue, we show that including knowledge on such topological
constraints can improve the inference of transition matrices, especially for
small datasets. We derive an analytically tractable Bayesian method that uses
repeated interactions and a topological prior to infer transition matrices
data-efficiently. We compare it against commonly used frequentist and Bayesian
approaches both in synthetic and real-world datasets, and we find that it
recovers the transition probabilities with higher accuracy and that it is
robust even in cases when the knowledge of the topological constraint is
partial. Lastly, we show that this higher accuracy improves the results for
downstream network analysis tasks like cluster detection and node ranking,
which highlights the practical relevance of our method for analyses of various
networked systems.",0.15346406,-0.2700122,-0.0314897,C
13214,"While we explored several useful features of nonlinear transport maps, many further research
avenues remain:

        • Dimension reduction techniques [Jollife and Cadima, 2016, Scheidt et al., 2018, Solonen et al., 2016, Baptista
           et al., 2022], for both states and observations, lower the number of input variables to the map S and the number
           of map components Sk that need to be estimated, thereby reducing computational demand.","22
                                               Ensemble transport smoothing, Part II

6.3 Outlook

Triangular transport methods hold substantial potential for Bayesian inference, and constitute a versatile tool for ﬁltering
and smoothing alike.","• Likewise, graph structure learning methods [Drton and Maathuis, 2017, Baptista et al., 2021], for continuous
           non-Gaussian distributions, offer an approach to directly learn the conditional dependence structure of joint
           state and observation distributions relevant to smoothing, extending the heuristics proposed in Section 5.3.",2022-10-31 16:04:14+00:00,Ensemble transport smoothing -- Part 2: nonlinear updates,stat.ME,"['stat.ME', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Maximilian Ramgraber'), arxiv.Result.Author('Ricardo Baptista'), arxiv.Result.Author('Dennis McLaughlin'), arxiv.Result.Author('Youssef Marzouk')]","Smoothing is a specialized form of Bayesian inference for state-space models
that characterizes the posterior distribution of a collection of states given
an associated sequence of observations. Our companion manuscript proposes a
general framework for transport-based ensemble smoothing, which includes linear
Kalman-type smoothers as special cases. Here, we build on this foundation to
realize and demonstrate nonlinear backward ensemble transport smoothers. We
discuss parameterization and regularization of the associated transport maps,
and then examine the performance of these smoothers for nonlinear and chaotic
dynamical systems that exhibit non-Gaussian behavior. In these settings, our
nonlinear transport smoothers yield lower estimation error than conventional
linear smoothers and state-of-the-art iterative ensemble Kalman smoothers, for
comparable numbers of model evaluations.",-0.37842286,0.06723946,0.1402771,A
13230,"Although further research on diﬀerent distribution settings
   is explored, it is too diﬃcult to check the distribution in some problems.","However, RR has some restrictions that can only be exploited when the distribution of
   the repeated estimators from multiple sets is normal.","Fortunately, Bayesian framework provides
   another method to pool, that we just need to mix up posterior samples from multiply-imputed datasets [31, 32],
   and the mixed distribution represents the posterior distribution which can be inferred directly.",2022-10-31 19:47:26+00:00,Variable Selection for Multiply-imputed Data: A Bayesian Framework,stat.ME,['stat.ME'],"[arxiv.Result.Author('Jungang Zou'), arxiv.Result.Author('Sijian Wang'), arxiv.Result.Author('Qixuan Chen')]","Multiple imputation is a widely used technique to handle missing data in
large observational studies. For variable selection on multiply-imputed
datasets, however, if we conduct selection on each imputed dataset separately,
different sets of important variables may be obtained. MI-LASSO, one of the
most popular solutions to this problem, regards the same variable across all
separate imputed datasets as a group of variables and exploits Group-LASSO to
yield a consistent variable selection across all the multiply-imputed datasets.
In this paper, we extend the MI-LASSO model into Bayesian framework and utilize
five different Bayesian MI-LASSO models to perform variable selection on
multiply-imputed data. These five models consist of three shrinkage priors
based and two discrete mixture prior based approaches. We conduct a simulation
study investigating the practical characteristics of each model across various
settings. We further demonstrate these methods via a case study using the
multiply-imputed data from the University of Michigan Dioxin Exposure Study.
The Python package BMIselect is hosted on Github under an Apache-2.0 license:
https://github.com/zjg540066169/Bmiselect.",-0.0907136,0.006758609,-0.22550465,A
13231,"Therefore, further research in simultaneously imputing
missing data and selecting important variables can be a worthwhile topic.","Additionally, MI-LASSO and its Bayesian versions are all built on multiply-imputed data while
the process of MI can be integrated into Bayesian framework.","J. Zou, S. Wang, Q. Chen                                              21

TA B L E 8 Values of α0

                          Parameter           Scenario        Values
                               α0    low missing proportion     -4
                               α0    high missing proportion   -1.8

acknowledgements

Acknowledgements should include contributions from anyone who does not meet the criteria for authorship (for
example, to recognize contributions from people who provided technical help, collation of data, writing assistance,
acquisition of funding, or a department chairperson who provided general support), as well as any funding or other
support information.",2022-10-31 19:47:26+00:00,Variable Selection for Multiply-imputed Data: A Bayesian Framework,stat.ME,['stat.ME'],"[arxiv.Result.Author('Jungang Zou'), arxiv.Result.Author('Sijian Wang'), arxiv.Result.Author('Qixuan Chen')]","Multiple imputation is a widely used technique to handle missing data in
large observational studies. For variable selection on multiply-imputed
datasets, however, if we conduct selection on each imputed dataset separately,
different sets of important variables may be obtained. MI-LASSO, one of the
most popular solutions to this problem, regards the same variable across all
separate imputed datasets as a group of variables and exploits Group-LASSO to
yield a consistent variable selection across all the multiply-imputed datasets.
In this paper, we extend the MI-LASSO model into Bayesian framework and utilize
five different Bayesian MI-LASSO models to perform variable selection on
multiply-imputed data. These five models consist of three shrinkage priors
based and two discrete mixture prior based approaches. We conduct a simulation
study investigating the practical characteristics of each model across various
settings. We further demonstrate these methods via a case study using the
multiply-imputed data from the University of Michigan Dioxin Exposure Study.
The Python package BMIselect is hosted on Github under an Apache-2.0 license:
https://github.com/zjg540066169/Bmiselect.",0.14196217,0.23043223,-0.031541903,C
13236,"Results of the european group for the study
      of resistant depression (gsrd)—basis for further research and clinical practice.","[2] Bartova, L., Dold, M., Kautzky, A., Fabbri, C., Spies, M., Serretti, A., Souery, D.,
      Mendlewicz, J., Zohar, J., Montgomery, S., et al.","The World Journal of
      Biological Psychiatry 20, 6 (2019), 427–448.",2022-10-31 22:12:22+00:00,Can the potential benefit of individualizing treatment be assessed using trial summary statistics alone?,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Nina Galanter'), arxiv.Result.Author('Marco Carone'), arxiv.Result.Author('Ronald C. Kessler'), arxiv.Result.Author('Alex Luedtke')]","Individualizing treatment assignment can improve outcomes for diseases with
patient-to-patient variability in comparative treatment effects. When a
clinical trial demonstrates that some patients improve on treatment while
others do not, it is tempting to assume that treatment effect heterogeneity
exists. However, if variability in response is mainly driven by factors other
than treatment, investigating the extent to which covariate data can predict
differential treatment response is a potential waste of resources. Motivated by
recent meta-analyses assessing the potential of individualizing treatment for
major depressive disorder using only summary statistics, we provide a method
that uses summary statistics widely available in published clinical trial
results to bound the benefit of optimally assigning treatment to each patient.
We also offer alternate bounds for settings in which trial results are
stratified by another covariate. We demonstrate our approach using summary
statistics from a depression treatment trial. Our methods are implemented in
the rct2otrbounds R package, which is available at
https://github.com/ngalanter/rct2otrbounds .",0.39379552,0.0021756068,0.06706369,C
13274,"Finally, some conclusion along with discussion on possible extensions for further research has
been presented in Section 6.","In Section 5, the ability and
suitability of the introduced MFA model are evaluated using some simulated and real datasets.",2.,2022-11-01 20:19:52+00:00,A Bayesian Framework on Asymmetric Mixture of Factor Analyser,stat.ME,"['stat.ME', 'cs.LG', '68T07']","[arxiv.Result.Author('Hamid Reza Safaeyan'), arxiv.Result.Author('Karim Zare'), arxiv.Result.Author('Mohamad R. Mahmoudi'), arxiv.Result.Author('Amir Mosavi')]","Mixture of factor analyzer (MFA) model is an efficient model for the analysis
of high dimensional data through which the factor-analyzer technique based on
the covariance matrices reducing the number of free parameters. The model also
provides an important methodology to determine latent groups in data. There are
several pieces of research to extend the model based on the asymmetrical and/or
with outlier datasets with some known computational limitations that have been
examined in frequentist cases. In this paper, an MFA model with a rich and
flexible class of skew normal (unrestricted) generalized hyperbolic (called
SUNGH) distributions along with a Bayesian structure with several computational
benefits have been introduced. The SUNGH family provides considerable
flexibility to model skewness in different directions as well as allowing for
heavy tailed data. There are several desirable properties in the structure of
the SUNGH family, including, an analytically flexible density which leads to
easing up the computation applied for the estimation of parameters. Considering
factor analysis models, the SUNGH family also allows for skewness and heavy
tails for both the error component and factor scores. In the present study, the
advantages of using this family of distributions have been discussed and the
suitable efficiency of the introduced MFA model using real data examples and
simulation has been demonstrated.",0.13657033,-0.01816285,0.03051756,C
13297,"Although the proposed methodology gives
approximate solution in the presence of small noise, further research is required to ﬁnd
the best approximation of the inverse solution if it does not exist in the search space.","(5) This paper
assumes the existence of the inverse solution.","Conﬂict of interest statement: On behalf of all authors, the corresponding author
states that there is no conﬂict of interest.",2022-11-02 13:51:08+00:00,Solving an Inverse Problem for Time Series Valued Computer Simulators via Multiple Contour Estimation,stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author('Pritam Ranjan'), arxiv.Result.Author('Joseph Resch'), arxiv.Result.Author('Abhyuday Mandal')]","Computer simulators are often used as a substitute of complex real-life
phenomena which are either expensive or infeasible to experiment with. This
paper focuses on how to efficiently solve the inverse problem for an expensive
to evaluate time series valued computer simulator. The research is motivated by
a hydrological simulator which has to be tuned for generating realistic
rainfall-runoff measurements in Athens, Georgia, USA. Assuming that the
simulator returns g(x,t) over L time points for a given input x, the proposed
methodology begins with a careful construction of a discretization (time-)
point set (DPS) of size $k << L$, achieved by adopting a regression spline
approximation of the target response series at k optimal knots locations
$\{t^*_1, t^*_2, ..., t^*_k\}$. Subsequently, we solve k scalar valued inverse
problems for simulator $g(x,t^*_j)$ via the contour estimation method. The
proposed approach, named MSCE, also facilitates the uncertainty quantification
of the inverse solution. Extensive simulation study is used to demonstrate the
performance comparison of the proposed method with the popular competitors for
several test-function based computer simulators and a real-life rainfall-runoff
measurement model.",-0.20463896,-0.24818477,0.0940958,A
13382,"While the heterogeneity variance in the design prior allows
to take effect size heterogeneity into account for SSD, to some extent, further research is
needed for investigating how systematic study deviations and external knowledge can be
incorporated.","There are some limitations and possible extensions: we have developed the
methodology for “direct” replication studies (Simons, 2014) which attempt to replicate the
conditions from the original study as closely as possible; yet SSD methodology is also needed
for “conceptual” replication and for “generalization” studies which may show systematic
deviations from the original study.","Furthermore, as is standard in meta-analysis we assumed that the variances of
the effect estimates are known, which can sometimes be inadequate (Jackson & White, 2018).",2022-11-04 16:19:21+00:00,Bayesian approaches to designing replication studies,stat.ME,['stat.ME'],"[arxiv.Result.Author('Samuel Pawel'), arxiv.Result.Author('Guido Consonni'), arxiv.Result.Author('Leonhard Held')]","Replication studies are essential for assessing the credibility of claims
from original studies. A critical aspect of designing replication studies is
determining their sample size; a too small sample size may lead to inconclusive
studies whereas a too large sample size may waste resources that could be
allocated better in other studies. Here we show how Bayesian approaches can be
used for tackling this problem. The Bayesian framework allows researchers to
combine the original data and external knowledge in a design prior distribution
for the underlying parameters. Based on a design prior, predictions about the
replication data can be made, and the replication sample size can be chosen to
ensure a sufficiently high probability of replication success. Replication
success may be defined through Bayesian or non-Bayesian criteria, and different
criteria may also be combined to meet distinct stakeholders and allow
conclusive inferences based on multiple analysis approaches. We investigate
sample size determination in the normal-normal hierarchical model where
analytical results are available and traditional sample size determination is a
special case where the uncertainty on parameter values is not accounted for. An
application to data from a multisite replication project of social-behavioral
experiments illustrates how Bayesian approaches help to design informative and
cost-effective replication studies. Our methods can be used through the R
package BayesRepDesign.",0.39132845,0.17492218,-0.034419548,C
13442,"9
paper, and the subject of further research.","This is beyond the scope of this

    4Note that a top-down approach with hierarchical clustering algorithms, known as divisive hierarchical clustering, is com-
putationally very expensive with complexity typically being quartic or quintic (Roux, 2015).",Section 4 presents experiments for the Binomial case.,2022-11-07 10:33:01+00:00,Beyond Conjugacy for Chain Event Graph Model Selection,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Aditi Shenvi'), arxiv.Result.Author('Silvia Liverani')]","Chain event graphs are a family of probabilistic graphical models that
generalise Bayesian networks and have been successfully applied to a wide range
of domains. Unlike Bayesian networks, these models can encode context-specific
conditional independencies as well as asymmetric developments within the
evolution of a process. More recently, new model classes belonging to the chain
event graph family have been developed for modelling time-to-event data to
study the temporal dynamics of a process. However, existing model selection
algorithms for chain event graphs and its variants rely on all parameters
having conjugate priors. This is unrealistic for many real-world applications.
In this paper, we propose a mixture modelling approach to model selection in
chain event graphs that does not rely on conjugacy. Moreover, we also show that
this methodology is more amenable to being robustly scaled than the existing
model selection algorithms used for this family. We demonstrate our techniques
on simulated datasets.",-0.04284908,-0.27872413,0.3172832,A
13443,There are challenges that will require further study.,"Further,
the soft clustering provided naturally by the mixture modelling approach can be used with a Bayesian model
averaging setting such as in Strong and Smith (2022) for robust explanatory analyses using a set of top-scoring
models rather than just the maximum a posteriori model.","The conditional probability distribution for a situa-
tion with three or more emanating edges follows a Multinomial distribution.",2022-11-07 10:33:01+00:00,Beyond Conjugacy for Chain Event Graph Model Selection,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Aditi Shenvi'), arxiv.Result.Author('Silvia Liverani')]","Chain event graphs are a family of probabilistic graphical models that
generalise Bayesian networks and have been successfully applied to a wide range
of domains. Unlike Bayesian networks, these models can encode context-specific
conditional independencies as well as asymmetric developments within the
evolution of a process. More recently, new model classes belonging to the chain
event graph family have been developed for modelling time-to-event data to
study the temporal dynamics of a process. However, existing model selection
algorithms for chain event graphs and its variants rely on all parameters
having conjugate priors. This is unrealistic for many real-world applications.
In this paper, we propose a mixture modelling approach to model selection in
chain event graphs that does not rely on conjugacy. Moreover, we also show that
this methodology is more amenable to being robustly scaled than the existing
model selection algorithms used for this family. We demonstrate our techniques
on simulated datasets.",-0.10769356,-0.08478749,0.103646934,A
13444,There are two possible approaches that we could consider for further study.,"However, for the Multinomial case, enforcing an ordering is not suﬃcient as for a Multinomial
with k categories has degree of freedom k − 1 and it is not straightforward how to enforce an ordering on all
k − 1 categories at once.","One
being that a Multinomial distribution can be written as a series of consecutive Binomial distributions and
the other that for a speciﬁc application, non-exchangeable priors could be used.",2022-11-07 10:33:01+00:00,Beyond Conjugacy for Chain Event Graph Model Selection,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Aditi Shenvi'), arxiv.Result.Author('Silvia Liverani')]","Chain event graphs are a family of probabilistic graphical models that
generalise Bayesian networks and have been successfully applied to a wide range
of domains. Unlike Bayesian networks, these models can encode context-specific
conditional independencies as well as asymmetric developments within the
evolution of a process. More recently, new model classes belonging to the chain
event graph family have been developed for modelling time-to-event data to
study the temporal dynamics of a process. However, existing model selection
algorithms for chain event graphs and its variants rely on all parameters
having conjugate priors. This is unrealistic for many real-world applications.
In this paper, we propose a mixture modelling approach to model selection in
chain event graphs that does not rely on conjugacy. Moreover, we also show that
this methodology is more amenable to being robustly scaled than the existing
model selection algorithms used for this family. We demonstrate our techniques
on simulated datasets.",-0.048260834,-0.13633104,0.09960809,A
13445,"Despite these challenges that require further study, the approach that we propose in this paper vastly
extends the applicability of CEGs and will open up a range of opportunities.",(2021).,"Appendix

A Clustering Accuracy Metrics

The normalised mutual information (NMI) score and the Rand index are two popular metrics for comparing
the accuracy of a clustering algorithm.",2022-11-07 10:33:01+00:00,Beyond Conjugacy for Chain Event Graph Model Selection,stat.ME,"['stat.ME', 'stat.ML']","[arxiv.Result.Author('Aditi Shenvi'), arxiv.Result.Author('Silvia Liverani')]","Chain event graphs are a family of probabilistic graphical models that
generalise Bayesian networks and have been successfully applied to a wide range
of domains. Unlike Bayesian networks, these models can encode context-specific
conditional independencies as well as asymmetric developments within the
evolution of a process. More recently, new model classes belonging to the chain
event graph family have been developed for modelling time-to-event data to
study the temporal dynamics of a process. However, existing model selection
algorithms for chain event graphs and its variants rely on all parameters
having conjugate priors. This is unrealistic for many real-world applications.
In this paper, we propose a mixture modelling approach to model selection in
chain event graphs that does not rely on conjugacy. Moreover, we also show that
this methodology is more amenable to being robustly scaled than the existing
model selection algorithms used for this family. We demonstrate our techniques
on simulated datasets.",-0.054209664,-0.14860958,0.28232,A
13458,"Given the general unavailability of calibrative families of joint mixing
distributions (of p and r simultaneously; see Subsection 3.2), we believe further research should focus on
practical methods for partitioning data into separate components within which the r parameter is relatively
homogeneous, while retaining the potential beneﬁts of analyzing mixing distributions.","From both theoretical and applied perspectives, it is clear that the assumption of a ﬁxed Negative Bino-
mial r parameter is quite restrictive.","As indicated at the outset, we hope that the present (and subsequent) work encourages greater interest in
the application of heavy-tailed frequency models.",2022-11-07 15:00:12+00:00,Heavy-Tailed Loss Frequencies from Mixtures of Negative Binomial and Poisson Counts,stat.ME,"['stat.ME', 'math.PR', 'stat.AP', '60E05, 60E10']","[arxiv.Result.Author('Jiansheng Dai'), arxiv.Result.Author('Ziheng Huang'), arxiv.Result.Author('Michael R. Powers'), arxiv.Result.Author('Jiaxin Xu')]","Heavy-tailed random variables have been used in insurance research to model
both loss frequencies and loss severities, with substantially more emphasis on
the latter. In the present work, we take a step toward addressing this
imbalance by exploring the class of heavy-tailed frequency models formed by
continuous mixtures of Negative Binomial and Poisson random variables. We begin
by defining the concept of a calibrative family of mixing distributions (each
member of which is identifiable from its associated Negative Binomial mixture),
and show how to construct such families from only a single member. We then
introduce a new heavy-tailed frequency model -- the two-parameter ZY
distribution -- as a generalization of both the one-parameter Zeta and Yule
distributions, and construct calibrative families for both the new distribution
and the heavy-tailed two-parameter Waring distribution. Finally, we pursue
natural extensions of both the ZY and Waring families to a unifying,
four-parameter heavy-tailed model, providing the foundation for a novel
loss-frequency modeling approach to complement conventional GLM analyses. This
approach is illustrated by application to a classic set of Swedish commercial
motor-vehicle insurance loss data.",-0.1709134,-0.046643615,-0.058249645,A
13459,"Given the general unavailability of calibrative families of joint mixing
distributions (of p and r simultaneously; see Subsection 3.2), we believe further research should focus on
practical methods for partitioning data into separate components within which the r parameter is relatively
homogeneous, while retaining the potential beneﬁts of analyzing mixing distributions.","From both theoretical and applied perspectives, it is clear that the assumption of a ﬁxed Negative Bino-
mial r parameter is quite restrictive.","As indicated at the outset, we hope that the present (and subsequent) work encourages greater interest in
the application of heavy-tailed frequency models.",2022-11-07 15:00:12+00:00,Heavy-Tailed Loss Frequencies from Mixtures of Negative Binomial and Poisson Counts,stat.ME,"['stat.ME', 'math.PR', 'stat.AP', '60E05, 60E10']","[arxiv.Result.Author('Jiansheng Dai'), arxiv.Result.Author('Ziheng Huang'), arxiv.Result.Author('Michael R. Powers'), arxiv.Result.Author('Jiaxin Xu')]","Heavy-tailed random variables have been used in insurance research to model
both loss frequencies and loss severities, with substantially more emphasis on
the latter. In the present work, we take a step toward addressing this
imbalance by exploring the class of heavy-tailed frequency models formed by
continuous mixtures of Negative Binomial and Poisson random variables. We begin
by defining the concept of a calibrative family of mixing distributions (each
member of which is identifiable from its associated Negative Binomial mixture),
and show how to construct such families from only a single member. We then
introduce a new heavy-tailed frequency model -- the two-parameter ZY
distribution -- as a generalization of both the one-parameter Zeta and Yule
distributions, and construct calibrative families for both the new distribution
and the heavy-tailed two-parameter Waring distribution. Finally, we pursue
natural extensions of both the ZY and Waring families to a unifying,
four-parameter heavy-tailed model, providing the foundation for a novel
loss-frequency modeling approach to complement conventional GLM analyses. This
approach is illustrated by application to a classic set of Swedish commercial
motor-vehicle insurance loss data.",-0.1709134,-0.046643615,-0.058249645,A
13521,"Finally, this work opens new opportunities for further research.","That being said, this paper has taken several steps in Sections 4 and 5
to mitigate the susceptibility of the “vanilla” conformalized sketching method described in
Section 3 to possible violations of the data exchangeability assumption.","For example, in the
future one may study and compare theoretically, in some settings, the length of our confor-
mal conﬁdence intervals under diﬀerent types of coverage guarantees.",2022-11-09 00:05:29+00:00,Conformal Frequency Estimation with Sketched Data under Relaxed Exchangeability,stat.ME,"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Matteo Sesia'), arxiv.Result.Author('Stefano Favaro'), arxiv.Result.Author('Edgar Dobriban')]","A flexible method is developed to construct a confidence interval for the
frequency of a queried object in a very large data set, based on a much smaller
sketch of the data. The approach requires no knowledge of the data distribution
or of the details of the sketching algorithm; instead, it constructs provably
valid frequentist confidence intervals for random queries using a conformal
inference approach. After achieving marginal coverage for random queries under
the assumption of data exchangeability, the proposed method is extended to
provide stronger inferences accounting for possibly heterogeneous frequencies
of different random queries, redundant queries, and distribution shifts. While
the presented methods are broadly applicable, this paper focuses on use cases
involving the count-min sketch algorithm and a non-linear variation thereof, to
facilitate comparison to prior work. In particular, the developed methods are
compared empirically to frequentist and Bayesian alternatives, through
simulations and experiments with data sets of SARS-CoV-2 DNA sequences and
classic English literature.",0.04613705,-0.21698242,0.041249447,A
13603,Various issues need further research.,"Two empirical applica-
tions conﬁrm that the approach is successful at ﬁtting skewed datasets in
economics and ﬁnance.","First, it is possible to carry out a
thorough comparison between the dynamic mixture considered in this paper
and the lognormal-Pareto model mentioned in Section 2 (Scollnik, 2007), in
terms of both goodness of ﬁt and numerical diﬃculty of the estimation pro-
cedures.",2022-11-10 20:01:46+00:00,Unsupervised Mixture Estimation via Approximate Maximum Likelihood based on the Cramér - von Mises distance,stat.ME,"['stat.ME', 'stat.CO']",[arxiv.Result.Author('Marco Bee')],"Mixture distributions with dynamic weights are an efficient way of modeling
loss data characterized by heavy tails. However, maximum likelihood estimation
of this family of models is difficult, mostly because of the need to evaluate
numerically an intractable normalizing constant. In such a setup,
simulation-based estimation methods are an appealing alternative. We employ the
approximate maximum likelihood estimation (AMLE) approach, which is general and
can be applied to mixtures with any component densities, as long as simulation
is feasible. We focus on the dynamic lognormal-generalized Pareto distribution,
and use the Cram\'er - von Mises distance to measure the discrepancy between
observed and simulated samples. After deriving the theoretical properties of
the estimators, we develop a hybrid procedure, where standard maximum
likelihood is first employed to determine the bounds of the uniform priors
required as input for AMLE. Simulation experiments and two real-data
applications suggest that this approach yields a major improvement with respect
to standard maximum likelihood estimation.",-0.21807742,0.07985991,-0.1561088,A
13708,"One
avenue for further research is to explore the suitability of choices other than the
mean posterior probability for ordering functions on the hyperstage.","There is scope
for further investigation into both the MPC algorithm and binary trees.","References

Lorna M. Barclay, Jane L. Hutton, and Jim Q. Smith.",2022-11-14 09:32:36+00:00,Scalable Model Selection for Staged Trees: Mean-posterior Clustering and Binary Trees,stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author('Peter Strong'), arxiv.Result.Author('Jim Q. Smith')]","Several structure-learning algorithms for staged trees, asymmetric extensions
of Bayesian networks, have been proposed. However, these either do not scale
efficiently as the number of variables considered increases, a priori restrict
the set of models, or they do not find comparable models to existing methods.
Here, we define an alternative algorithm based on a totally ordered hyperstage.
We demonstrate how it can be used to obtain a quadratically-scaling structural
learning algorithm for staged trees that restricts the model space
a-posteriori. Through comparative analysis, we show that through the ordering
provided by the mean posterior distributions, we can outperform existing
methods in both computational time and model score. This method also enables us
to learn more complex relationships than existing model selection techniques by
expanding the model space and illustrates how this can embellish inferences in
a real study.",-0.10763338,-0.2245624,0.20691037,A
14054,"As tree space is constructed for modeling the space of phylogenetic trees, one immediate
interest of further research is the applicability to biological data and problems.","Finally, further simulation studies in both well-modeled
and misspeciﬁed cases are necessary for practical purposes.","As we only
have (approximate) algorithms for one and two dimensions, corresponding to the case
where a maximum of four taxa are present, the applicability seems to be limited for now.",2022-11-22 06:19:29+00:00,Maximum Likelihood Estimation of Log-Concave Densities on Tree Space,stat.ME,"['stat.ME', 'stat.CO']","[arxiv.Result.Author('Yuki Takazawa'), arxiv.Result.Author('Tomonari Sei')]","Phylogenetic trees are key data objects in biology, and the method of
phylogenetic reconstruction has been highly developed. The space of
phylogenetic trees is a nonpositively curved metric space. Recently,
statistical methods to analyze the set of trees on this space are being
developed utilizing this property. Meanwhile, in Euclidean space, the
log-concave maximum likelihood method has emerged as a new nonparametric method
for probability density estimation. In this paper, we derive a sufficient
condition for the existence and uniqueness of the log-concave maximum
likelihood estimator on tree space. We also propose an estimation algorithm for
one and two dimensions. Since various factors affect the inferred trees, it is
difficult to specify the distribution of sample trees. The class of log-concave
densities is nonparametric, and yet the estimation can be conducted by the
maximum likelihood method without selecting hyperparameters. We compare the
estimation performance with a previously developed kernel density estimator
numerically. In our examples where the true density is log-concave, we
demonstrate that our estimator has a smaller integrated squared error when the
sample size is large. We also conduct numerical experiments of clustering using
the Expectation-Maximization (EM) algorithm and compare the results with
k-means++ clustering using Fr\'echet mean.",-0.07547439,-0.26423776,0.23672444,A
14214,"Some suggestions for further research are the topic of our Discussion
Section(Section 5).","Some computations are given in the Numerical Study Section (Section 3) that follow the
Method Section (Section 2).","2 Method

2.1 The General Framework

The complete data consist of n pairs of observations {(Zi, Yi), i = 1, · · · , n} .",2022-11-27 07:57:56+00:00,An Empirical Bayes Approach for Constructing the Confidence Intervals of Clonality and Entropy,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Zhongren Chen'), arxiv.Result.Author('Lu Tian'), arxiv.Result.Author('Richard Olshen')]","This paper is motivated by the need to quantify human immune responses to
environmental challenges. Specifically, the genome of the selected cell
population from a blood sample is amplified by the well-known PCR process of
successive heating and cooling, producing a large number of reads. They number
roughly 30,000 to 300,000. Each read corresponds to a particular rearrangement
of so-called V(D)J sequences. In the end, the observation consists of a set of
numbers of reads corresponding to different V(D)J sequences. The underlying
relative frequencies of distinct V(D)J sequences can be summarized by a
probability vector, with the cardinality being the number of distinct V(D)J
rearrangements present in the blood. Statistical question is to make inferences
on a summary parameter of the probability vector based on a single
multinomial-type observation of a large dimension. Popular summary of the
diversity of a cell population includes clonality and entropy, or more
generally, is a suitable function of the probability vector. A point estimator
of the clonality based on multiple replicates from the same blood sample has
been proposed previously. After obtaining a point estimator of a particular
function, the remaining challenge is to construct a confidence interval of the
parameter to appropriately reflect its uncertainty. In this paper, we have
proposed to couple the empirical Bayes method with a resampling-based
calibration procedure to construct a robust confidence interval for different
population diversity parameters. The method has been illustrated via extensive
numerical study and real data examples.",-0.13291925,-0.23285592,0.045488425,A
14215,"The
extension towards this direction warrants further study.","For example, it is appealing to consider
distributions from a nonparametric exponential family: p(λ | η) ∝ p0(λ) exp{B(λ) η}, where B(λ)
is a set of basis functions given a priori such as B(λ) = (λ, λ2, λ3) [Schwartzman, 2008].","Lastly, in the proposed approach, the actual
number of distinct clones was replaced by its estimator, which may affect the performance of the
subsequent point and interval estimations.",2022-11-27 07:57:56+00:00,An Empirical Bayes Approach for Constructing the Confidence Intervals of Clonality and Entropy,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Zhongren Chen'), arxiv.Result.Author('Lu Tian'), arxiv.Result.Author('Richard Olshen')]","This paper is motivated by the need to quantify human immune responses to
environmental challenges. Specifically, the genome of the selected cell
population from a blood sample is amplified by the well-known PCR process of
successive heating and cooling, producing a large number of reads. They number
roughly 30,000 to 300,000. Each read corresponds to a particular rearrangement
of so-called V(D)J sequences. In the end, the observation consists of a set of
numbers of reads corresponding to different V(D)J sequences. The underlying
relative frequencies of distinct V(D)J sequences can be summarized by a
probability vector, with the cardinality being the number of distinct V(D)J
rearrangements present in the blood. Statistical question is to make inferences
on a summary parameter of the probability vector based on a single
multinomial-type observation of a large dimension. Popular summary of the
diversity of a cell population includes clonality and entropy, or more
generally, is a suitable function of the probability vector. A point estimator
of the clonality based on multiple replicates from the same blood sample has
been proposed previously. After obtaining a point estimator of a particular
function, the remaining challenge is to construct a confidence interval of the
parameter to appropriately reflect its uncertainty. In this paper, we have
proposed to couple the empirical Bayes method with a resampling-based
calibration procedure to construct a robust confidence interval for different
population diversity parameters. The method has been illustrated via extensive
numerical study and real data examples.",-0.21818137,-0.1448212,-0.1856223,A
14239,"However, if we further study the two individual events, we can see that
this result is mainly driven by the event A, because for the event B its drug
performs worse than the placebo.","When we consider the composite endpoint by winning either A or B, results
tell us that the drug response rate is 100% and the placebo response rate is
50%.","In particular, it can be observed that although
for Composite A or B, and also for Component A, Placebo responses 50% and
Drug responses 100%, for Component B Placebo responses 50% but the Drug
responses 0%.",2022-11-28 01:48:51+00:00,Utilizing Win Ratio Approaches and Two-Stage Enrichment Designs for Small-Sized Clinical Trials,stat.ME,"['stat.ME', 'q-bio.QM', 'stat.AP']","[arxiv.Result.Author('Jialu Wang'), arxiv.Result.Author('Yeh-Fong Chen'), arxiv.Result.Author('Thomas Gwise')]","Conventional methods for analyzing composite endpoints in clinical trials
often only focus on the time to the first occurrence of all events in the
composite. Therefore, they have inherent limitations because the individual
patients' first event can be the outcome of lesser clinical importance. To
overcome this limitation, the concept of the win ratio (WR), which accounts for
the relative priorities of the components and gives appropriate priority to the
more clinically important event, was examined. For example, because mortality
has a higher priority than hospitalization, it is reasonable to give a higher
priority when obtaining the WR. In this paper, we evaluate three innovative WR
methods (stratified matched, stratified unmatched, and unstratified unmatched)
for two and multiple components under binary and survival composite endpoints.
We compare these methods to traditional ones, including the Cox regression,
O'Brien's rank-sum-type test, and the contingency table for controlling study
Type I error rate. We also incorporate these approaches into two-stage
enrichment designs with the possibility of sample size adaptations to gain
efficiency for rare disease studies.",0.4151817,0.01075515,0.017338842,C
14242,"However, how
to handle data with serial correlation, e.g., autoregressive time series, is outside the scope of
this work and we leave this topic for further study.","[2013], which
extended the direct density-ratio estimation approach to the time series data.","2 Set-up and Background

Consider suﬃcient reference samples from domain X (typically X = Rd) with an unknown

pre-change distribution P:

                            X1, .",2022-11-28 05:08:30+00:00,Online Kernel CUSUM for Change-Point Detection,stat.ME,"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Song Wei'), arxiv.Result.Author('Yao Xie')]","We develop an online kernel Cumulative Sum (CUSUM) procedure, which consists
of a parallel set of kernel statistics with different window sizes to account
for the unknown change-point location. Compared with many existing sliding
window-based kernel change-point detection procedures, which correspond to the
Shewhart chart-type procedure, the proposed procedure is more sensitive to
small changes. We further present a recursive computation of detection
statistics, which is crucial for online procedures to achieve a constant
computational and memory complexity, such that we do not need to calculate and
remember the entire Gram matrix, which can be a computational bottleneck
otherwise. We obtain precise analytic approximations of the two fundamental
performance metrics, the Average Run Length (ARL) and Expected Detection Delay
(EDD). Furthermore, we establish the optimal window size on the order of $\log
({\rm ARL})$ such that there is nearly no power loss compared with an oracle
procedure, which is analogous to the classic result for window-limited
Generalized Likelihood Ratio (GLR) procedure. We present extensive numerical
experiments to validate our theoretical results and the competitive performance
of the proposed method.",-0.2489483,0.20998299,0.07542221,B
14243,"However, how
to handle data with serial correlation, e.g., autoregressive time series, is outside the scope of
this work and we leave this topic for further study.","[2013], which
extended the direct density-ratio estimation approach to the time series data.","2 Set-up and Background

Consider suﬃcient reference samples from domain X (typically X = Rd) with an unknown

pre-change distribution P:

                            X1, .",2022-11-28 05:08:30+00:00,Online Kernel CUSUM for Change-Point Detection,stat.ME,"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Song Wei'), arxiv.Result.Author('Yao Xie')]","We develop an online kernel Cumulative Sum (CUSUM) procedure, which consists
of a parallel set of kernel statistics with different window sizes to account
for the unknown change-point location. Compared with many existing sliding
window-based kernel change-point detection procedures, which correspond to the
Shewhart chart-type procedure, the proposed procedure is more sensitive to
small changes. We further present a recursive computation of detection
statistics, which is crucial for online procedures to achieve a constant
computational and memory complexity, such that we do not need to calculate and
remember the entire Gram matrix, which can be a computational bottleneck
otherwise. We obtain precise analytic approximations of the two fundamental
performance metrics, the Average Run Length (ARL) and Expected Detection Delay
(EDD). Furthermore, we establish the optimal window size on the order of $\log
({\rm ARL})$ such that there is nearly no power loss compared with an oracle
procedure, which is analogous to the classic result for window-limited
Generalized Likelihood Ratio (GLR) procedure. We present extensive numerical
experiments to validate our theoretical results and the competitive performance
of the proposed method.",-0.2489483,0.20998299,0.07542221,B
14244,"We further study its
asymptotic properties and draw connections with other difference-based estimators.","In Section 4, we apply the optimal-k difference sequence and
introduce a new difference-based estimator called the optimal-k estimator.","In Section

                                                         5
5, we conduct simulation studies to evaluate the ﬁnite sample performance of the new estimator
and compare it with the existing methods.",2022-11-28 06:03:37+00:00,Optimal-$k$ difference sequence in nonparametric regression,stat.ME,['stat.ME'],"[arxiv.Result.Author('Wenlin Dai'), arxiv.Result.Author('Xingwei Tong'), arxiv.Result.Author('Tiejun Tong')]","Difference-based methods have been attracting increasing attention in
nonparametric regression, in particular for estimating the residual variance.To
implement the estimation, one needs to choose an appropriate difference
sequence, mainly between {\em the optimal difference sequence} and {\em the
ordinary difference sequence}. The difference sequence selection is a
fundamental problem in nonparametric regression, and it remains a controversial
issue for over three decades. In this paper, we propose to tackle this
challenging issue from a very unique perspective, namely by introducing a new
difference sequence called {\em the optimal-$k$ difference sequence}. The new
difference sequence not only provides a better balance between the
bias-variance trade-off, but also dramatically enlarges the existing family of
difference sequences that includes the optimal and ordinary difference
sequences as two important special cases. We further demonstrate, by both
theoretical and numerical studies, that the optimal-$k$ difference sequence has
been pushing the boundaries of our knowledge in difference-based methods in
nonparametric regression, and it always performs the best in practical
situations.",-0.09799998,-0.12025231,-0.25653273,A
14278,"5
A further concern is the potential for over- or underestimation of treatment effects to affect
further research.","This could impede the uptake of results from an adaptive trial design or
discourage research teams from using these designs in practice.","In a phase II trial, for example, ineffective treatments with exaggerated
effects may be wrongly selected for further investigations in phase III trials or potentially
effective treatments may not be pursued further when their effects are underestimated47–51.",2022-11-28 18:20:34+00:00,Point estimation for adaptive trial designs II: practical considerations and guidance,stat.ME,"['stat.ME', 'stat.AP', '62F10']","[arxiv.Result.Author('David S. Robertson'), arxiv.Result.Author('Babak Choodari-Oskooei'), arxiv.Result.Author('Munya Dimairo'), arxiv.Result.Author('Laura Flight'), arxiv.Result.Author('Philip Pallmann'), arxiv.Result.Author('Thomas Jaki')]","In adaptive clinical trials, the conventional end-of-trial point estimate of
a treatment effect is prone to bias, that is, a systematic tendency to deviate
from its true value. As stated in recent FDA guidance on adaptive designs, it
is desirable to report estimates of treatment effects that reduce or remove
this bias. However, it may be unclear which of the available estimators are
preferable, and their use remains rare in practice. This paper is the second in
a two-part series that studies the issue of bias in point estimation for
adaptive trials. Part I provided a methodological review of approaches to
remove or reduce the potential bias in point estimation for adaptive designs.
In part II, we discuss how bias can affect standard estimators and assess the
negative impact this can have. We review current practice for reporting point
estimates and illustrate the computation of different estimators using a real
adaptive trial example (including code), which we use as a basis for a
simulation study. We show that while on average the values of these estimators
can be similar, for a particular trial realisation they can give noticeably
different values for the estimated treatment effect. Finally, we propose
guidelines for researchers around the choice of estimators and the reporting of
estimates following an adaptive design. The issue of bias should be considered
throughout the whole lifecycle of an adaptive design, with the estimation
strategy pre-specified in the statistical analysis plan. When available,
unbiased or bias-reduced estimates are to be preferred.",0.60394543,0.104223795,-0.060063906,C
14279,"In
terms of trial reporting, statements about the potential bias of the reported estimates can
indicate where more care is needed in interpretation of the results and the use of the point
estimates for further research including evidence synthesis and health economic analyses.","More generally, the estimation strategy should
take the design of the trial into account, which motivates the use of adjusted estimators.","Finally, to improve the uptake of unbiased and bias-adjusted estimators in practice, there is
the need for the further development of user-friendly software and code to allow
straightforward calculation of trial results and to aid in simulations.",2022-11-28 18:20:34+00:00,Point estimation for adaptive trial designs II: practical considerations and guidance,stat.ME,"['stat.ME', 'stat.AP', '62F10']","[arxiv.Result.Author('David S. Robertson'), arxiv.Result.Author('Babak Choodari-Oskooei'), arxiv.Result.Author('Munya Dimairo'), arxiv.Result.Author('Laura Flight'), arxiv.Result.Author('Philip Pallmann'), arxiv.Result.Author('Thomas Jaki')]","In adaptive clinical trials, the conventional end-of-trial point estimate of
a treatment effect is prone to bias, that is, a systematic tendency to deviate
from its true value. As stated in recent FDA guidance on adaptive designs, it
is desirable to report estimates of treatment effects that reduce or remove
this bias. However, it may be unclear which of the available estimators are
preferable, and their use remains rare in practice. This paper is the second in
a two-part series that studies the issue of bias in point estimation for
adaptive trials. Part I provided a methodological review of approaches to
remove or reduce the potential bias in point estimation for adaptive designs.
In part II, we discuss how bias can affect standard estimators and assess the
negative impact this can have. We review current practice for reporting point
estimates and illustrate the computation of different estimators using a real
adaptive trial example (including code), which we use as a basis for a
simulation study. We show that while on average the values of these estimators
can be similar, for a particular trial realisation they can give noticeably
different values for the estimated treatment effect. Finally, we propose
guidelines for researchers around the choice of estimators and the reporting of
estimates following an adaptive design. The issue of bias should be considered
throughout the whole lifecycle of an adaptive design, with the estimation
strategy pre-specified in the statistical analysis plan. When available,
unbiased or bias-reduced estimates are to be preferred.",0.45929706,0.26632226,-0.24879119,C
14375,"Hence, we believe further research should focus on trying
to remove this natural direction in simulation.","However, when considering the form of a de Bruijn
word in 2d, it becomes very diﬃcult to work in a speciﬁc direction (something that is very
important for de Bruijn graphs).","De Bruijn graphs can quickly become very complicated with many possible transition
probabilities for large word lengths.",2022-11-30 11:46:33+00:00,Modelling Correlated Bernoulli Data Part I: Theory and Run Lengths,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Louise Kimpton'), arxiv.Result.Author('Peter Challenor'), arxiv.Result.Author('Henry Wynn')]","Binary data are very common in many applications, and are typically simulated
independently via a Bernoulli distribution with a single probability of
success. However, this is not always the physical truth, and the probability of
a success can be dependent on the outcome successes of past events. Presented
here is a novel approach for simulating binary data where, for a chain of
events, successes (1) and failures (0) cluster together according to a distance
correlation. The structure is derived from de Bruijn Graphs - a directed graph,
where given a set of symbols, V, and a 'word' length, m, the nodes of the graph
consist of all possible sequences of V of length m. De Bruijn Graphs are a
generalisation of Markov chains, where the 'word' length controls the number of
states that each individual state is dependent on. This increases correlation
over a wider area. To quantify how clustered a sequence generated from a de
Bruijn process is, the run lengths of letters are observed along with run
length properties.",-0.059397798,-0.3022262,0.2201266,A
14380,"We thus believe that such an
assessment could be of broader interest and a potential topic for further research.","which compromise is sought seems to us a desirable property of any borrowing mecha-
nism, as it implies a good interpretability of the weight itself.",We have provided tools for sensitivity analyses and sample size selection.,2022-11-30 13:27:32+00:00,Robust incorporation of historical information with known type I error rate inflation,stat.ME,['stat.ME'],"[arxiv.Result.Author('Silvia Calderazzo'), arxiv.Result.Author('Annette Kopp-Schneider')]","Bayesian clinical trials can benefit of available historical information
through the elicitation of informative prior distributions. Concerns are
however often raised about the potential for prior-data conflict and the impact
of Bayes test decisions on frequentist operating characteristics, with
particular attention being assigned to inflation of type I error rates. This
motivates the development of principled borrowing mechanisms, that strike a
balance between frequentist and Bayesian decisions. Ideally, the trust assigned
to historical information defines the degree of robustness to prior-data
conflict one is willing to sacrifice. However, such relationship is often not
directly available when explicitly considering inflation of type I error rates.
We build on available literature relating frequentist and Bayesian test
decisions, and investigate a rationale for inflation of type I error rate which
explicitly and linearly relates the amount of borrowing and the amount of type
I error rate inflation in one-arm studies. A novel dynamic borrowing mechanism
tailored to hypothesis testing is additionally proposed. We show that, while
dynamic borrowing prevents the possibility to obtain a simple closed form type
I error rate computation, an explicit upper bound can still be enforced.
Connections with the robust mixture prior approach, particularly in relation to
the choice of the mixture weight and robust component, are made. Simulations
are performed to show the properties of the approach for normal and binomial
outcomes.",0.25883397,-0.0033897534,0.014346334,C
14381,"A situation requiring further study is when sepa-
rate priors are elicited for the treatment and the control arm, and will be the focus of future
research.","Extension to two-arm situations poses no difﬁcul-
ties if a single prior is elicited on the difference between the treatment and control mean:
When Normal outcomes are considered, reduction to a one-arm design is straightforward
and the method can be directly applied.","Conﬂict of Interest
The authors have declared no conﬂict of interest.",2022-11-30 13:27:32+00:00,Robust incorporation of historical information with known type I error rate inflation,stat.ME,['stat.ME'],"[arxiv.Result.Author('Silvia Calderazzo'), arxiv.Result.Author('Annette Kopp-Schneider')]","Bayesian clinical trials can benefit of available historical information
through the elicitation of informative prior distributions. Concerns are
however often raised about the potential for prior-data conflict and the impact
of Bayes test decisions on frequentist operating characteristics, with
particular attention being assigned to inflation of type I error rates. This
motivates the development of principled borrowing mechanisms, that strike a
balance between frequentist and Bayesian decisions. Ideally, the trust assigned
to historical information defines the degree of robustness to prior-data
conflict one is willing to sacrifice. However, such relationship is often not
directly available when explicitly considering inflation of type I error rates.
We build on available literature relating frequentist and Bayesian test
decisions, and investigate a rationale for inflation of type I error rate which
explicitly and linearly relates the amount of borrowing and the amount of type
I error rate inflation in one-arm studies. A novel dynamic borrowing mechanism
tailored to hypothesis testing is additionally proposed. We show that, while
dynamic borrowing prevents the possibility to obtain a simple closed form type
I error rate computation, an explicit upper bound can still be enforced.
Connections with the robust mixture prior approach, particularly in relation to
the choice of the mixture weight and robust component, are made. Simulations
are performed to show the properties of the approach for normal and binomial
outcomes.",0.27096838,0.16027315,-0.0965413,C
14623,"The results developed under the model
(1.1) can serve as a starting point for further research.",Some additional work is required.,"Second, the goodness-of-ﬁt test in Section
S2 of the supplement seems to suggest that the model (1.1) provides an adequate ﬁt to the
duration-time data from Wuhan.",2022-12-05 15:48:44+00:00,Hypothesis test on a mixture forward-incubation-time epidemic model with application to COVID-19 outbreak,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Chunlin Wang'), arxiv.Result.Author('Pengfei Li'), arxiv.Result.Author('Yukun Liu'), arxiv.Result.Author('Xiao-Hua Zhou'), arxiv.Result.Author('Jing Qin')]","The distribution of the incubation period of the novel coronavirus disease
that emerged in 2019 (COVID-19) has crucial clinical implications for
understanding this disease and devising effective disease-control measures. Qin
et al. (2020) designed a cross-sectional and forward follow-up study to collect
the duration times between a specific observation time and the onset of
COVID-19 symptoms for a number of individuals. They further proposed a mixture
forward-incubation-time epidemic model, which is a mixture of an
incubation-period distribution and a forward time distribution, to model the
collected duration times and to estimate the incubation-period distribution of
COVID-19. In this paper, we provide sufficient conditions for the
identifiability of the unknown parameters in the mixture
forward-incubation-time epidemic model when the incubation period follows a
two-parameter distribution. Under the same setup, we propose a likelihood ratio
test (LRT) for testing the null hypothesis that the mixture
forward-incubation-time epidemic model is a homogeneous exponential
distribution. The testing problem is non-regular because a nuisance parameter
is present only under the alternative. We establish the limiting distribution
of the LRT and identify an explicit representation for it. The limiting
distribution of the LRT under a sequence of local alternatives is also
obtained. Our simulation results indicate that the LRT has desirable type I
errors and powers, and we analyze a COVID-19 outbreak dataset from China to
illustrate the usefulness of the LRT.",-0.06647752,-0.10973838,-0.027064823,A
14642,"In contrast, De Veaux (2017, p. 152)
recommends using ensemble models to find variables with greatest “contribution” to model fit,
that is, importance, for further study.","R’s caret package
does this in linear models using the equivalent t-statistic.","Grömping (2009) finds that RF-CART produces variable
importances similar to the LMG and those of RF-CI are similar to the PMVD.",2022-12-06 19:45:34+00:00,The Importance of Variable Importance,stat.ME,['stat.ME'],[arxiv.Result.Author('Charles D. Coleman')],"Variable importance is defined as a measure of each regressor's contribution
to model fit. Using R^2 as the fit criterion in linear models leads to the
Shapley value (LMG) and proportionate value (PMVD) as variable importance
measures. Similar measures are defined for ensemble models, using random
forests as the example. The properties of the LMG and PMVD are compared.
Variable importance is proposed to assess regressors' practical effects or
""oomph."" The uses of variable importance in modelling, interventions and causal
analysis are discussed.",-0.020448126,0.19923745,0.14564872,B
14732,The following are areas where further research is needed.,"We see no other
advantages.","• There is a need to develop practical methods for designing statistically eﬃcient experiments to
       obtain S-N /e-N data (how many and which levels of stress, number of specimens, and how to
       allocate them to stress levels).",2022-12-08 20:32:16+00:00,Modern Statistical Models and Methods for Estimating Fatigue-Life and Fatigue-Strength Distributions from Experimental Data,stat.ME,['stat.ME'],"[arxiv.Result.Author('William Q. Meeker'), arxiv.Result.Author('Luis A. Escobar'), arxiv.Result.Author('Francis G. Pascual'), arxiv.Result.Author('Yili Hong'), arxiv.Result.Author('Peng Liu'), arxiv.Result.Author('Wayne M. Falk'), arxiv.Result.Author('Balajee Ananthasayanam')]","Engineers and scientists have been collecting and analyzing fatigue data
since the 1800s to ensure the reliability of life-critical structures.
Applications include (but are not limited to) bridges, building structures,
aircraft and spacecraft components, ships, ground-based vehicles, and medical
devices. Engineers need to estimate S-N relationships (Stress or Strain versus
Number of cycles to failure), typically with a focus on estimating small
quantiles of the fatigue-life distribution. Estimates from this kind of model
are used as input to models (e.g., cumulative damage models) that predict
failure-time distributions under varying stress patterns. Also, design
engineers need to estimate lower-tail quantiles of the closely related
fatigue-strength distribution. The history of applying incorrect statistical
methods is nearly as long and such practices continue to the present. Examples
include treating the applied stress (or strain) as the response and the number
of cycles to failure as the explanatory variable in regression analyses
(because of the need to estimate strength distributions) and ignoring or
otherwise mishandling censored observations (known as runouts in the fatigue
literature). The first part of the paper reviews the traditional modeling
approach where a fatigue-life model is specified. We then show how this
specification induces a corresponding fatigue-strength model. The second part
of the paper presents a novel alternative modeling approach where a
fatigue-strength model is specified and a corresponding fatigue-life model is
induced. We explain and illustrate the important advantages of this new
modeling approach.",0.33898664,0.015044073,0.13138549,C
14758,"The paper concludes
with a discussion of further research needed in section 7.",The estimation algorithms are demonstrated in a small scale simulation study in section 6.,2.,2022-12-09 13:51:21+00:00,Non-parametric estimation of mixed discrete choice models,stat.ME,"['stat.ME', 'math.ST', 'stat.TH']","[arxiv.Result.Author('Dietmar Bauer'), arxiv.Result.Author('Sebastian Büscher'), arxiv.Result.Author('Manuel Batram')]","In this paper, different strands of literature are combined in order to
obtain algorithms for semi-parametric estimation of discrete choice models that
include the modelling of unobserved heterogeneity by using mixing distributions
for the parameters defining the preferences. The models use the theory on
non-parametric maximum likelihood estimation (NP-MLE) that has been developed
for general mixing models. The expectation-maximization (EM) techniques used in
the NP-MLE literature are combined with strategies for choosing appropriate
approximating models using adaptive grid techniques. \\ Jointly this leads to
techniques for specification and estimation that can be used to obtain a
consistent specification of the mixing distribution. Additionally, also
algorithms for the estimation are developed that help to decrease problems due
to the curse of dimensionality. \\ The proposed algorithms are demonstrated in
a small scale simulation study to be useful for the specification and
estimation of mixture models in the discrete choice context providing some
information on the specification of the mixing distribution. The simulations
document that some aspects of the mixing distribution such as the expectation
can be estimated reliably. They also demonstrate, however, that typically
different approximations to the mixing distribution lead to similar values of
the likelihood and hence are hard to discriminate. Therefore it does not appear
to be possible to reliably infer the most appropriate parametric form for the
estimated mixing distribution.",-0.26846635,-0.09101974,-0.18545997,A
14874,"Overall, the results reveal that ICS with MCDα − COV has potential in a
clustering context, but further research is needed.","While the med
criterion is much less aﬀected by outliers than the normal criterion, it fails in
more cluster settings (results not shown).","First, since results in diﬀerent
cluster settings depend on the parameter α, it could be interesting to consider α
as a tuning parameter to be selected, e.g., together with the number of clusters.",2022-12-12 18:30:29+00:00,Tandem clustering with invariant coordinate selection,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andreas Alfons'), arxiv.Result.Author('Aurore Archimbaud'), arxiv.Result.Author('Klaus Nordhausen'), arxiv.Result.Author('Anne Ruiz-Gazen')]","For high-dimensional data or data with noise variables, tandem clustering is
a well-known technique that aims to improve cluster identification by first
reducing the dimension. However, the usual approach using principal component
analysis (PCA) has been criticized for focusing only on inertia so that the
first components do not necessarily retain the structure of interest for
clustering. To overcome this drawback, we propose a new tandem clustering
approach based on invariant coordinate selection (ICS). By jointly
diagonalizing two scatter matrices, ICS is designed to find structure in the
data while returning affine invariant components. Some theoretical results have
already been derived and guarantee that under some elliptical mixture models,
the structure of the data can be highlighted on a subset of the first and/or
last components. Nevertheless, ICS has received little attention in a
clustering context. Two challenges are the choice of the pair of scatter
matrices and the selection of the components to retain. For clustering
purposes, we demonstrate that the best scatter pairs consist of one scatter
matrix that captures the within-cluster structure and another that captures the
global structure. For the former, local shape or pairwise scatters are of great
interest, as is the minimum covariance determinant (MCD) estimator based on a
carefully selected subset size that is smaller than usual. We evaluate the
performance of ICS as a dimension reduction method in terms of preserving the
cluster structure present in data. In an extensive simulation study and in
empirical applications with benchmark data sets, we compare different
combinations of scatter matrices, component selection criteria, and the impact
of outliers. Overall, the new approach of tandem clustering with ICS shows
promising results and clearly outperforms the approach with PCA.",0.20525217,-0.27685273,0.20887986,C
14875,"This is an promising result
that needs to be veriﬁed from a theoretical point of view in further research.","For q = 5
clusters, discriminatory power is very high throughout even though there are
various cluster settings with clusters of size 20%.","Furthermore, the med criterion is close to the oracle criterion in terms of per-
formance, except in settings where the method struggles either way.",2022-12-12 18:30:29+00:00,Tandem clustering with invariant coordinate selection,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andreas Alfons'), arxiv.Result.Author('Aurore Archimbaud'), arxiv.Result.Author('Klaus Nordhausen'), arxiv.Result.Author('Anne Ruiz-Gazen')]","For high-dimensional data or data with noise variables, tandem clustering is
a well-known technique that aims to improve cluster identification by first
reducing the dimension. However, the usual approach using principal component
analysis (PCA) has been criticized for focusing only on inertia so that the
first components do not necessarily retain the structure of interest for
clustering. To overcome this drawback, we propose a new tandem clustering
approach based on invariant coordinate selection (ICS). By jointly
diagonalizing two scatter matrices, ICS is designed to find structure in the
data while returning affine invariant components. Some theoretical results have
already been derived and guarantee that under some elliptical mixture models,
the structure of the data can be highlighted on a subset of the first and/or
last components. Nevertheless, ICS has received little attention in a
clustering context. Two challenges are the choice of the pair of scatter
matrices and the selection of the components to retain. For clustering
purposes, we demonstrate that the best scatter pairs consist of one scatter
matrix that captures the within-cluster structure and another that captures the
global structure. For the former, local shape or pairwise scatters are of great
interest, as is the minimum covariance determinant (MCD) estimator based on a
carefully selected subset size that is smaller than usual. We evaluate the
performance of ICS as a dimension reduction method in terms of preserving the
cluster structure present in data. In an extensive simulation study and in
empirical applications with benchmark data sets, we compare different
combinations of scatter matrices, component selection criteria, and the impact
of outliers. Overall, the new approach of tandem clustering with ICS shows
promising results and clearly outperforms the approach with PCA.",0.1141641,-0.36310512,0.21039394,C
14876,"Discussion of the simulation study
    The main ﬁndings from our simulations are that (i) ICS with TCOV − COV

and LCOV − COV performs best for tandem clustering, (ii) the scatter pair
MCDα − COV with a value of α ≤ 0.5 could be an interesting alternative when
α is treated as a tuning parameter but further research is needed, and (iii) PCA
is not a suitable preprocessing step for clustering purposes and yields large
variability in clustering performance.",4.4.,"21
    However, there are several limitations of our simulations.",2022-12-12 18:30:29+00:00,Tandem clustering with invariant coordinate selection,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andreas Alfons'), arxiv.Result.Author('Aurore Archimbaud'), arxiv.Result.Author('Klaus Nordhausen'), arxiv.Result.Author('Anne Ruiz-Gazen')]","For high-dimensional data or data with noise variables, tandem clustering is
a well-known technique that aims to improve cluster identification by first
reducing the dimension. However, the usual approach using principal component
analysis (PCA) has been criticized for focusing only on inertia so that the
first components do not necessarily retain the structure of interest for
clustering. To overcome this drawback, we propose a new tandem clustering
approach based on invariant coordinate selection (ICS). By jointly
diagonalizing two scatter matrices, ICS is designed to find structure in the
data while returning affine invariant components. Some theoretical results have
already been derived and guarantee that under some elliptical mixture models,
the structure of the data can be highlighted on a subset of the first and/or
last components. Nevertheless, ICS has received little attention in a
clustering context. Two challenges are the choice of the pair of scatter
matrices and the selection of the components to retain. For clustering
purposes, we demonstrate that the best scatter pairs consist of one scatter
matrix that captures the within-cluster structure and another that captures the
global structure. For the former, local shape or pairwise scatters are of great
interest, as is the minimum covariance determinant (MCD) estimator based on a
carefully selected subset size that is smaller than usual. We evaluate the
performance of ICS as a dimension reduction method in terms of preserving the
cluster structure present in data. In an extensive simulation study and in
empirical applications with benchmark data sets, we compare different
combinations of scatter matrices, component selection criteria, and the impact
of outliers. Overall, the new approach of tandem clustering with ICS shows
promising results and clearly outperforms the approach with PCA.",-0.015428872,-0.26895687,0.3195262,A
14877,"More complex models
with, e.g., diﬀerent covariance structures in diﬀerent clusters or non-normal
distributions are beyond the scope of this paper and are left for further research.","An important consideration for this choice is that it allows
us to use simple clustering methods such as kmeans to validate our ﬁndings for
the discriminatory power of the selected components.",5.,2022-12-12 18:30:29+00:00,Tandem clustering with invariant coordinate selection,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andreas Alfons'), arxiv.Result.Author('Aurore Archimbaud'), arxiv.Result.Author('Klaus Nordhausen'), arxiv.Result.Author('Anne Ruiz-Gazen')]","For high-dimensional data or data with noise variables, tandem clustering is
a well-known technique that aims to improve cluster identification by first
reducing the dimension. However, the usual approach using principal component
analysis (PCA) has been criticized for focusing only on inertia so that the
first components do not necessarily retain the structure of interest for
clustering. To overcome this drawback, we propose a new tandem clustering
approach based on invariant coordinate selection (ICS). By jointly
diagonalizing two scatter matrices, ICS is designed to find structure in the
data while returning affine invariant components. Some theoretical results have
already been derived and guarantee that under some elliptical mixture models,
the structure of the data can be highlighted on a subset of the first and/or
last components. Nevertheless, ICS has received little attention in a
clustering context. Two challenges are the choice of the pair of scatter
matrices and the selection of the components to retain. For clustering
purposes, we demonstrate that the best scatter pairs consist of one scatter
matrix that captures the within-cluster structure and another that captures the
global structure. For the former, local shape or pairwise scatters are of great
interest, as is the minimum covariance determinant (MCD) estimator based on a
carefully selected subset size that is smaller than usual. We evaluate the
performance of ICS as a dimension reduction method in terms of preserving the
cluster structure present in data. In an extensive simulation study and in
empirical applications with benchmark data sets, we compare different
combinations of scatter matrices, component selection criteria, and the impact
of outliers. Overall, the new approach of tandem clustering with ICS shows
promising results and clearly outperforms the approach with PCA.",-0.09774038,-0.24104646,0.026329134,A
14878,"Empirical applications

    To further study the performance of tandem clustering with ICS, we con-
sider three publicly available benchmark datasets.",5.,"First, the crabs data set
(Campbell & Mahon, 1974) contains morphological measurements on crabs of
both sexes and from two color-diﬀerent species.",2022-12-12 18:30:29+00:00,Tandem clustering with invariant coordinate selection,stat.ME,['stat.ME'],"[arxiv.Result.Author('Andreas Alfons'), arxiv.Result.Author('Aurore Archimbaud'), arxiv.Result.Author('Klaus Nordhausen'), arxiv.Result.Author('Anne Ruiz-Gazen')]","For high-dimensional data or data with noise variables, tandem clustering is
a well-known technique that aims to improve cluster identification by first
reducing the dimension. However, the usual approach using principal component
analysis (PCA) has been criticized for focusing only on inertia so that the
first components do not necessarily retain the structure of interest for
clustering. To overcome this drawback, we propose a new tandem clustering
approach based on invariant coordinate selection (ICS). By jointly
diagonalizing two scatter matrices, ICS is designed to find structure in the
data while returning affine invariant components. Some theoretical results have
already been derived and guarantee that under some elliptical mixture models,
the structure of the data can be highlighted on a subset of the first and/or
last components. Nevertheless, ICS has received little attention in a
clustering context. Two challenges are the choice of the pair of scatter
matrices and the selection of the components to retain. For clustering
purposes, we demonstrate that the best scatter pairs consist of one scatter
matrix that captures the within-cluster structure and another that captures the
global structure. For the former, local shape or pairwise scatters are of great
interest, as is the minimum covariance determinant (MCD) estimator based on a
carefully selected subset size that is smaller than usual. We evaluate the
performance of ICS as a dimension reduction method in terms of preserving the
cluster structure present in data. In an extensive simulation study and in
empirical applications with benchmark data sets, we compare different
combinations of scatter matrices, component selection criteria, and the impact
of outliers. Overall, the new approach of tandem clustering with ICS shows
promising results and clearly outperforms the approach with PCA.",0.02658021,-0.20720905,0.25620383,A
15042,"The above observations motivate us to further study the score function-based test for linear
models, extend the results in the literature and propose new test to handle high correlation
between nuisance and testing covariates.","(2022)), some technical details
need further careful checks.","To be speciﬁc, we will do the following.",2022-12-16 12:45:27+00:00,Score function-based tests for ultrahigh-dimensional linear models,stat.ME,['stat.ME'],"[arxiv.Result.Author('Weichao Yang'), arxiv.Result.Author('Xu Guo'), arxiv.Result.Author('Lixing Zhu')]","To sufficiently exploit the model structure under the null hypothesis such
that the conditions on the whole model can be mild, this paper investigates
score function-based tests to check the significance of an
ultrahigh-dimensional sub-vector of the model coefficients when the nuisance
parameter vector is also ultrahigh-dimensional in linear models. We first
reanalyze and extend a recently proposed score function-based test to derive,
under weaker conditions, its limiting distributions under the null and local
alternative hypotheses. As it may fail to work when the correlation between
testing covariates and nuisance covariates is high, we propose an
orthogonalized score function-based test with two merits: debiasing to make the
non-degenerate error term degenerate and reducing the asymptotic variance to
enhance the power performance. Simulations evaluate the finite-sample
performances of the proposed tests, and a real data analysis illustrates its
application.",0.13555521,-0.07865613,0.017051175,C
15179,"On the other hand, when no monotonicity assump-
tion holds, both ST and SC will have to be imputed for every subject;
further research is necessary to determine the appropriate way to do so.","This scenario is broadly
similar to the strong monotonicity, one-way noncompliance situation that
this paper discussed.",Another direction of extension involved the principal score model (3).,2022-12-20 16:30:53+00:00,GEEPERs: Principal Stratification using Principal Scores and Stacked Estimating Equations,stat.ME,"['stat.ME', 'stat.AP']","[arxiv.Result.Author('Adam C. Sales'), arxiv.Result.Author('Kirk P. Vanacore'), arxiv.Result.Author('Erin R. Ottmar')]","Principal stratification is a framework for making sense of causal effects
conditioned on variables that themselves may have been affected by treatment.
Most principal stratification estimators rely on strong structural or modeling
assumptions, and many require advanced statistical training to fit and to
check. In this paper, we introduce a new M-estimation principal effect
estimator for one-way noncompliance based on a binary indicator. Estimates may
be computed using conventional regressions (though the standard errors require
a specialized sandwich formula) and do not rely on distributional assumptions.
We illustrate the new technique in an analysis of student log data from a
recent educational technology field experiment.",0.060107533,-0.06633262,-0.009948447,A
15261,"(3.2.12)

    Hence to further study the terms in equation (3.2.11), it will be necessary
to introduce some useful notions from Hamilton-Jacobi theory (see for example
Bu¨hler (2006) Chapter 1 for more information).","∂t                   ∂x

    In the case of W-F diﬀusion the Hamilton function is H(x, q) = 12 σ2(x)q2,
and the associated PDE is the dispersion equation

           ∂∂t S(x, t) + H(x, −i ∂∂x S) := St − 21 σ2(x)Sx2 = 0.",In our case σ(x) = x(1 − x).,2022-12-22 01:21:50+00:00,Small time approximation in Wright-Fisher diffusion,stat.ME,"['stat.ME', 'math.PR']","[arxiv.Result.Author('Tania Roa'), arxiv.Result.Author('María Inés Fariello'), arxiv.Result.Author('Gerardo Martínez'), arxiv.Result.Author('José León')]","Wright-Fisher model has been widely used to represent random variation in
allele frequency over time, due to its simple form even though, a closed
analytical form for the allele frequency has not been constructed. However,
variation in allele frequency allows to represent selection in the evolutionary
process. In this work, we present two alternatives of parametric approximating
functions: Asymptotic expansion (AE) and Gaussian approximation (GaussA),
obtained by means of probabilistic tools and useful for statistical inference,
to estimate the allele frequency density for a small $t/2N$ in the interval
$[0,1]$. The proposed densities satisfactorily capture the problem of fixation
at 0 or 1, unlike the commonly used methods. While Asymptotic Expansion defines
a suitable density for the distribution allele frequency (DAF), Gaussian
Approximation describes a range of validity for the Gaussian distribution.
Through a simulation study and using an adaptive method for density estimation,
the proposed densities are compared with the beta and Gaussian distribution
with, their corresponding parameters.",-0.13102679,-0.1768734,0.055730622,A
15323,"We further study the relationships among A(τ, Cn) and An(τ, Cn).",This is because f belongs to them with a high probability.,"The proof will use a

general covering number property for Gaussian RKHS from Steinwart and Scovel (2007),
which is stated as Proposition S.1 in S.3.1.4.",2022-12-23 17:49:56+00:00,Learning Optimal Dynamic Treatment Regime Subject to Stagewise Risk Controls,stat.ME,['stat.ME'],"[arxiv.Result.Author('Mochuan Liu'), arxiv.Result.Author('Yuanjia Wang'), arxiv.Result.Author('Haoda Fu'), arxiv.Result.Author('Donglin Zeng')]","Dynamic treatment regimes (DTRs) aim at tailoring individualized sequential
treatment rules that maximize cumulative beneficial outcomes by accommodating
patient's heterogeneity into decision making. For many chronic diseases
including type 2 diabetes mellitus (T2D), treatments are usually multifaceted
in the sense that the aggressive treatments with higher expected reward are
also likely to elevate the risk of acute adverse events. In this paper, we
propose a new weighted learning framework, namely benefit-risk dynamic
treatment regimes (BR-DTRs), to address the benefit-risk trade-off. The new
framework relies on a backward learning procedure by restricting the induced
risk of the treatment rule to be no larger than a pre-specified risk constraint
at each treatment stage. Computationally, the estimated treatment rule solves a
weighted support vector machine problem with a modified smooth constraint.
Theoretically, we show that the proposed DTRs are Fisher consistent and we
further obtain the convergence rates for both the value and risk functions.
Finally, the performance of the proposed method is demonstrated via extensive
simulation studies and application to a real study for T2D patients.",-0.14258972,-0.35802424,0.0015749913,A
15328,"The randomness and nonsingularity of balanced subsam-
ples make them applicable to training such estimators, although their performance for this
purpose requires further study.","The training
of the estimator involves repeatedly selecting small and nonsingular subsamples from the
full data, which, as discussed in this paper and in (Koller and Stahel, 2017), is infeasible
via simple random sampling.","Though the proposed balanced subsampling is focused on big data with categorical
covariates, it can be modiﬁed and generalized to select subsamples with numerical or mixed-
type covariates.",2022-12-23 21:35:10+00:00,Balanced Subsampling for Big Data with Categorical Covariates,stat.ME,['stat.ME'],[arxiv.Result.Author('Lin Wang')],"The use and analysis of massive data are challenging due to the high storage
and computational cost. Subsampling algorithms are popular to downsize the data
volume and reduce the computational burden. Existing subsampling approaches
focus on data with numerical covariates. Although big data with categorical
covariates are frequently encountered in many disciplines, the subsampling plan
has not been well established. In this paper, we propose a balanced subsampling
approach for reducing data with categorical covariates. The selected subsample
achieves a combinatorial balance among values of covariates and therefore
enjoys three desired merits. First, a balanced subsample is nonsingular and
thus allows the estimation of all parameters in ANOVA regression. Second, it
provides the optimal parameter estimation in the sense of minimizing the
generalized variance of the estimated parameters. Third, the model trained on a
balanced subsample provides robust predictions in the sense of minimizing the
worst-case prediction error. We demonstrate the usefulness of the balanced
subsampling over existing data reduction methods in extensive simulation
studies and a real-world application.",-0.06806643,0.055468407,-0.13151455,A
