Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
678,"During this final publication
process, supporting data may be deposited in domain-specific repositories such as the Gene
Expression Omnibus (GEO) (Clough & Barrett, 2016) or Protein Data Bank (PDB) (Berman, 2008;
Berman, Kleywegt, Nakamura, & Markley, 2013), or in lightly structured repositories provided
by journal publishers (2017) or other organizations (Blaiszik et al., 2016; Foster & Deardorff,
2017) —from where they will, ideally, be retrieved by other researchers to enable meta-
analyses or further research studies.","How the Data Lifecycle Goes off the Road

The scientific data lifecycle has long been viewed as a linear, directed path that starts with an
experiment or simulation, proceeds through analysis and review phases, and ultimately
terminates with a publication in an archival journal (Wing, 2019).","Despite the high importance assigned to the submission of data to such repositories, the data
themselves are not valued adequately; they are often rife with errors.",2022-01-17 18:31:34+00:00,Sharing Begins at Home,cs.DL,['cs.DL'],"[arxiv.Result.Author('William Dempsey'), arxiv.Result.Author('Ian Foster'), arxiv.Result.Author('Scott Fraser'), arxiv.Result.Author('Carl Kesselman')]","The broad sharing of research data is widely viewed as of critical importance
for the speed, quality, accessibility, and integrity of science. Despite
increasing efforts to encourage data sharing, both the quality of shared data,
and the frequency of data reuse, remain stubbornly low. We argue here that a
major reason for this unfortunate state of affairs is that the organization of
research results in the findable, accessible, interoperable, and reusable
(FAIR) form required for reuse is too often deferred to the end of a research
project, when preparing publications, by which time essential details are no
longer accessible. Thus, we propose an approach to research informatics that
applies FAIR principles continuously, from the very inception of a research
project, and ubiquitously, to every data asset produced by experiment or
computation. We suggest that this seemingly challenging task can be made
feasible by the adoption of simple tools, such as lightweight identifiers (to
ensure that every data asset is findable), packaging methods (to facilitate
understanding of data contents), data access methods, and metadata organization
and structuring tools (to support schema development and evolution). We use an
example from experimental neuroscience to illustrate how these methods can work
in practice.",-0.029211199,-0.39024895,-0.517622,B
679,"During this final publication
process, supporting data may be deposited in domain-specific repositories such as the Gene
Expression Omnibus (GEO) (Clough & Barrett, 2016) or Protein Data Bank (PDB) (Berman,
2008; Berman, Kleywegt, Nakamura, & Markley, 2013) or in lightly structured repositories
provided by journal publishers (Editorial, 2017) or other organizations (Blaiszik et al., 2016;
Foster & Deardorff, 2017)—from where they will, ideally, be retrieved by other researchers for
meta-analyses or further research studies.","How the Data Lifecycle Goes off the Road

The scientific data lifecycle has long been viewed as a linear, directed path that starts with an
experiment or simulation, proceeds through analysis and review phases, and ultimately
terminates with a publication in an archival journal (Wing, 2019).","Despite the high importance assigned to the submission of data to such repositories, the data
themselves are not valued adequately; they are often rife with errors.",2022-01-17 18:31:34+00:00,Sharing Begins at Home,cs.DL,['cs.DL'],"[arxiv.Result.Author('William Dempsey'), arxiv.Result.Author('Ian Foster'), arxiv.Result.Author('Scott Fraser'), arxiv.Result.Author('Carl Kesselman')]","The broad sharing of research data is widely viewed as of critical importance
for the speed, quality, accessibility, and integrity of science. Despite
increasing efforts to encourage data sharing, both the quality of shared data,
and the frequency of data reuse, remain stubbornly low. We argue here that a
major reason for this unfortunate state of affairs is that the organization of
research results in the findable, accessible, interoperable, and reusable
(FAIR) form required for reuse is too often deferred to the end of a research
project, when preparing publications, by which time essential details are no
longer accessible. Thus, we propose an approach to research informatics that
applies FAIR principles continuously, from the very inception of a research
project, and ubiquitously, to every data asset produced by experiment or
computation. We suggest that this seemingly challenging task can be made
feasible by the adoption of simple tools, such as lightweight identifiers (to
ensure that every data asset is findable), packaging methods (to facilitate
understanding of data contents), data access methods, and metadata organization
and structuring tools (to support schema development and evolution). We use an
example from experimental neuroscience to illustrate how these methods can work
in practice.",-0.026760029,-0.3967682,-0.51500946,B
680,"During this final publication
process, supporting data may be deposited in domain-specific repositories such as the Gene
Expression Omnibus (GEO) (Clough & Barrett, 2016) or Protein Data Bank (PDB) (Berman,
2008; Berman, Kleywegt, Nakamura, & Markley, 2013) or in lightly structured repositories
provided by journal publishers (Editorial, 2017) or other organizations (Blaiszik et al., 2016;
Foster & Deardorff, 2017) —from where they will, ideally, be retrieved by other researchers for
meta-analyses or further research studies.","How the Data Lifecycle Goes off the Road

The scientific data lifecycle has long been viewed as a linear, directed path that starts with an
experiment or simulation, proceeds through analysis and review phases, and ultimately
terminates with a publication in an archival journal (Wing, 2019).","Despite the high importance assigned to the submission of data to such repositories, the data
themselves are not valued adequately; they are often rife with errors.",2022-01-17 18:31:34+00:00,Sharing Begins at Home,cs.DL,['cs.DL'],"[arxiv.Result.Author('William Dempsey'), arxiv.Result.Author('Ian Foster'), arxiv.Result.Author('Scott Fraser'), arxiv.Result.Author('Carl Kesselman')]","The broad sharing of research data is widely viewed as of critical importance
for the speed, quality, accessibility, and integrity of science. Despite
increasing efforts to encourage data sharing, both the quality of shared data,
and the frequency of data reuse, remain stubbornly low. We argue here that a
major reason for this unfortunate state of affairs is that the organization of
research results in the findable, accessible, interoperable, and reusable
(FAIR) form required for reuse is too often deferred to the end of a research
project, when preparing publications, by which time essential details are no
longer accessible. Thus, we propose an approach to research informatics that
applies FAIR principles continuously, from the very inception of a research
project, and ubiquitously, to every data asset produced by experiment or
computation. We suggest that this seemingly challenging task can be made
feasible by the adoption of simple tools, such as lightweight identifiers (to
ensure that every data asset is findable), packaging methods (to facilitate
understanding of data contents), data access methods, and metadata organization
and structuring tools (to support schema development and evolution). We use an
example from experimental neuroscience to illustrate how these methods can work
in practice.",-0.026760029,-0.3967682,-0.51500946,B
681,"Yet the documentation for one of the sanctioned repositories states that
instructions on how to publish a dataset are a “work in progress.”
process, supporting data may be deposited in domain-specific repositories such as the Gene
Expression Omnibus (GEO) (Clough & Barrett, 2016) or Protein Data Bank (PDB) (Berman,
2008; Berman et al., 2013) or in lightly structured repositories provided by journal publishers
(Editorial, 2017) or other organizations (Blaiszik et al., 2016; Foster & Deardorff, 2017) —from
where they will, ideally, be retrieved by other researchers for meta-analyses or further research
studies.","During this final publication

1 As a case in point, a resource sharing form provided by the National Institutes of Health for
research funded under the BRAIN initiative provides a checklist of repositories to which data
must be deposited.","Despite the high importance assigned to the submission of data to such repositories, the data
themselves are not valued adequately; they are often rife with errors.",2022-01-17 18:31:34+00:00,Sharing Begins at Home,cs.DL,['cs.DL'],"[arxiv.Result.Author('William Dempsey'), arxiv.Result.Author('Ian Foster'), arxiv.Result.Author('Scott Fraser'), arxiv.Result.Author('Carl Kesselman')]","The broad sharing of research data is widely viewed as of critical importance
for the speed, quality, accessibility, and integrity of science. Despite
increasing efforts to encourage data sharing, both the quality of shared data,
and the frequency of data reuse, remain stubbornly low. We argue here that a
major reason for this unfortunate state of affairs is that the organization of
research results in the findable, accessible, interoperable, and reusable
(FAIR) form required for reuse is too often deferred to the end of a research
project, when preparing publications, by which time essential details are no
longer accessible. Thus, we propose an approach to research informatics that
applies FAIR principles continuously, from the very inception of a research
project, and ubiquitously, to every data asset produced by experiment or
computation. We suggest that this seemingly challenging task can be made
feasible by the adoption of simple tools, such as lightweight identifiers (to
ensure that every data asset is findable), packaging methods (to facilitate
understanding of data contents), data access methods, and metadata organization
and structuring tools (to support schema development and evolution). We use an
example from experimental neuroscience to illustrate how these methods can work
in practice.",-0.028027995,-0.37998182,-0.42609286,B
940,"Therefore, we will further study
the possibility of linking semantic macros with numeric regions of interest.","However, for identifying differences in the DLMF and
CAS, especially for analyzing the positioning of branch cuts, an automatic evaluation
of these particular values would be very beneficial and can be used to collect a
comprehensive, inter-system library of branch cuts.","24 https://lacast.wmflabs.org [accessed 01/01/2022]
Comparative Verification of the DLMF and CAS  17

Acknowledgements

We thank Jürgen Gerhard from Maplesoft for providing access and support for Maple.",2022-01-24 06:51:07+00:00,Comparative Verification of the Digital Library of Mathematical Functions and Computer Algebra Systems,cs.DL,['cs.DL'],"[arxiv.Result.Author('André Greiner-Petter'), arxiv.Result.Author('Howard S. Cohl'), arxiv.Result.Author('Abdou Youssef'), arxiv.Result.Author('Moritz Schubotz'), arxiv.Result.Author('Avi Trost'), arxiv.Result.Author('Rajen Dey'), arxiv.Result.Author('Akiko Aizawa'), arxiv.Result.Author('Bela Gipp')]","Digital mathematical libraries assemble the knowledge of years of
mathematical research. Numerous disciplines (e.g., physics, engineering, pure
and applied mathematics) rely heavily on compendia gathered findings. Likewise,
modern research applications rely more and more on computational solutions,
which are often calculated and verified by computer algebra systems. Hence, the
correctness, accuracy, and reliability of both digital mathematical libraries
and computer algebra systems is a crucial attribute for modern research.
  In this paper, we present a novel approach to verify a digital mathematical
library and two computer algebra systems with one another by converting
mathematical expressions from one system to the other. We use our previously
eveloped conversion tool (referred to as LaCASt) to translate formulae from the
NIST Digital Library of Mathematical Functions to the computer algebra systems
Maple and Mathematica. The contributions of our presented work are as follows:
(1) we present the most comprehensive verification of computer algebra systems
and digital mathematical libraries with one another; (2) we significantly
enhance the performance of the underlying translator in terms of coverage and
accuracy; and (3) we provide open access to translations for Maple and
Mathematica of the formulae in the NIST Digital Library of Mathematical
Functions.",0.57106376,-0.35887426,0.2268618,C
941,"Therefore, we will further study
the possibility of linking semantic macros with numeric regions of interest.","However, for identifying differences in the DLMF and
CAS, especially for analyzing the positioning of branch cuts, an automatic evaluation
of these particular values would be very beneficial and can be used to collect a
comprehensive, inter-system library of branch cuts.","24 https://lacast.wmflabs.org [accessed 01/01/2022]
25 https://github.com/ag-gipp/LaCASt [accessed 04/01/2022]
Comparative Verification of the DLMF and CAS  17

Acknowledgements

We thank Jürgen Gerhard from Maplesoft for providing access and support for Maple.",2022-01-24 06:51:07+00:00,Comparative Verification of the Digital Library of Mathematical Functions and Computer Algebra Systems,cs.DL,['cs.DL'],"[arxiv.Result.Author('André Greiner-Petter'), arxiv.Result.Author('Howard S. Cohl'), arxiv.Result.Author('Abdou Youssef'), arxiv.Result.Author('Moritz Schubotz'), arxiv.Result.Author('Avi Trost'), arxiv.Result.Author('Rajen Dey'), arxiv.Result.Author('Akiko Aizawa'), arxiv.Result.Author('Bela Gipp')]","Digital mathematical libraries assemble the knowledge of years of
mathematical research. Numerous disciplines (e.g., physics, engineering, pure
and applied mathematics) rely heavily on compendia gathered findings. Likewise,
modern research applications rely more and more on computational solutions,
which are often calculated and verified by computer algebra systems. Hence, the
correctness, accuracy, and reliability of both digital mathematical libraries
and computer algebra systems is a crucial attribute for modern research.
  In this paper, we present a novel approach to verify a digital mathematical
library and two computer algebra systems with one another by converting
mathematical expressions from one system to the other. We use our previously
eveloped conversion tool (referred to as LaCASt) to translate formulae from the
NIST Digital Library of Mathematical Functions to the computer algebra systems
Maple and Mathematica. The contributions of our presented work are as follows:
(1) we present the most comprehensive verification of computer algebra systems
and digital mathematical libraries with one another; (2) we significantly
enhance the performance of the underlying translator in terms of coverage and
accuracy; and (3) we provide open access to translations for Maple and
Mathematica of the formulae in the NIST Digital Library of Mathematical
Functions.",0.5839472,-0.38076553,0.22109792,C
1633,"A discussion
section details responses to the hypotheses as well as observations about the research
project and avenues for further research.","A results section describes outcomes of the analyses,
followed by limitations of the data and approaches presented here.","An appendix provides additional technical
details.",2022-02-01 21:13:34+00:00,One-Year In: COVID-19 Research at the International Level in CORD-19 Data,cs.DL,"['cs.DL', 'cs.SI', 'physics.soc-ph', 'stat.AP']","[arxiv.Result.Author('Caroline S. Wagner'), arxiv.Result.Author('Xiaojing Cai'), arxiv.Result.Author('Yi Zhang'), arxiv.Result.Author('Caroline V. Fry')]","The appearance of a novel coronavirus in late 2019 radically changed the
community of researchers working on coronaviruses since the 2002 SARS epidemic.
In 2020, coronavirus-related publications grew by 20 times over the previous
two years, with 130,000 more researchers publishing on related topics. The
United States, the United Kingdom and China led dozens of nations working on
coronavirus prior to the pandemic, but leadership consolidated among these
three nations in 2020, which collectively accounted for 50% of all papers,
garnering well more than 60% of citations. China took an early lead on COVID-19
research, but dropped rapidly in production and international participation
through the year. Europe showed an opposite pattern, beginning slowly in
publications but growing in contributions during the year. The share of
internationally collaborative publications dropped from pre-pandemic rates;
single-authored publications grew. For all nations, including China, the number
of publications about COVID track closely with the outbreak of COVID-19 cases.
Lower-income nations participate very little in COVID-19 research in 2020.
Topic maps of internationally collaborative work show the rise of patient care
and public health clusters, two topics that were largely absent from
coronavirus research in the two years prior to 2020. Findings are consistent
with global science as a self-organizing system operating on a reputation-based
dynamic.",-0.24863824,-0.08369826,0.07582844,B
1634,"It is clear that many of the
connections made around COVID-19 may not have existed prior to the pandemic, so
further research is needed to understand how people connected with each other
under crisis conditions.","When that cannot take place, it is unclear
whether people look for physically proximate partners, choose to work alone, become
connected to new people through friend-of-a-friend, through social media, or perhaps
just a ‘cold-call’ outreach to someone they do not know.","Acknowledgements
Thanks go to Clayton E. Tillman and Thomas Collins for help with data collection and
formatting.",2022-02-01 21:13:34+00:00,One-Year In: COVID-19 Research at the International Level in CORD-19 Data,cs.DL,"['cs.DL', 'cs.SI', 'physics.soc-ph', 'stat.AP']","[arxiv.Result.Author('Caroline S. Wagner'), arxiv.Result.Author('Xiaojing Cai'), arxiv.Result.Author('Yi Zhang'), arxiv.Result.Author('Caroline V. Fry')]","The appearance of a novel coronavirus in late 2019 radically changed the
community of researchers working on coronaviruses since the 2002 SARS epidemic.
In 2020, coronavirus-related publications grew by 20 times over the previous
two years, with 130,000 more researchers publishing on related topics. The
United States, the United Kingdom and China led dozens of nations working on
coronavirus prior to the pandemic, but leadership consolidated among these
three nations in 2020, which collectively accounted for 50% of all papers,
garnering well more than 60% of citations. China took an early lead on COVID-19
research, but dropped rapidly in production and international participation
through the year. Europe showed an opposite pattern, beginning slowly in
publications but growing in contributions during the year. The share of
internationally collaborative publications dropped from pre-pandemic rates;
single-authored publications grew. For all nations, including China, the number
of publications about COVID track closely with the outbreak of COVID-19 cases.
Lower-income nations participate very little in COVID-19 research in 2020.
Topic maps of internationally collaborative work show the rise of patient care
and public health clusters, two topics that were largely absent from
coronavirus research in the two years prior to 2020. Findings are consistent
with global science as a self-organizing system operating on a reputation-based
dynamic.",-0.13579826,0.14541477,0.16481957,A
1766,"The diversity of curatorial actions shown in just these two papers highlights the need
for further research into the specific curatorial workflows and communication regimes in different scholarly settings.","Comparison of our two frameworks reveals that ICPSR has much more detailed quality
check protocols, and ICPSR’s curators spend considerable time on tasks like “adding question text” that simply are not
needed in the earth science fields.","It is well understood from research on data practices that there are significant domain differences in curation needs
[Akers and Doty 2013; Cragin et al.",2022-02-09 16:53:15+00:00,"The craft and coordination of data curation: complicating ""workflow"" views of data science",cs.DL,"['cs.DL', 'cs.DB', 'cs.HC']","[arxiv.Result.Author('Andrea K. Thomer'), arxiv.Result.Author('Dharma Akmon'), arxiv.Result.Author('Jeremy York'), arxiv.Result.Author('Allison R. B. Tyler'), arxiv.Result.Author('Faye Polasek'), arxiv.Result.Author('Sara Lafia'), arxiv.Result.Author('Libby Hemphill'), arxiv.Result.Author('Elizabeth Yakel')]","Data curation is the process of making a dataset fit-for-use and archiveable.
It is critical to data-intensive science because it makes complex data
pipelines possible, makes studies reproducible, and makes data (re)usable. Yet
the complexities of the hands-on, technical and intellectual work of data
curation is frequently overlooked or downplayed. Obscuring the work of data
curation not only renders the labor and contributions of the data curators
invisible; it also makes it harder to tease out the impact curators' work has
on the later usability, reliability, and reproducibility of data. To better
understand the specific work of data curation -- and thereby, explore ways of
showing curators' impact -- we conducted a close examination of data curation
at a large social science data repository, the Inter-university Consortium of
Political and Social Research (ICPSR). We asked, What does curatorial work
entail at ICPSR, and what work is more or less visible to different
stakeholders and in different contexts? And, how is that curatorial work
coordinated across the organization? We triangulate accounts of data curation
from interviews and records of curation in Jira tickets to develop a rich and
detailed account of curatorial work. We find that curators describe a number of
craft practices needed to perform their work, which defies the rote sequence of
events implied by many lifecycle or workflow models. Further, we show how best
practices and craft practices are deeply intertwined.",-0.07349634,0.092307955,-0.17377558,A
2255,"The conclusion points to issues for further research
as more complete data will become available on the Covid-19 period.","The explosion of publications on
Covid-19 in 2020 has a quite visible impact on the topic map in infectious diseases and changes the
position of some countries in this field of research.","Keywords
Scientific publications, scientometric analysis, specialisation index, impact index, France, topic
mapping, infectious diseases, Covid-19

                                                                                                                           2
Introduction

L’infectiologie constitue un domaine particulièrement intéressant à étudier avec les outils de la
scientométrie au moment où la recherche y connaît une croissance extraordinaire du fait de la pandémie
de Covid-19.",2022-02-15 09:07:47+00:00,Analyse scientométrique du domaine de l'infectiologie de 2000 à 2020,cs.DL,"['cs.DL', 'cs.IR']","[arxiv.Result.Author('Lesya Baudoin'), arxiv.Result.Author('Anne Glanard'), arxiv.Result.Author('Abdelghani Maddi'), arxiv.Result.Author('Wilfriedo Mescheba'), arxiv.Result.Author('Frédérique Sachwald')]","Research on infectious diseases constitutes a transversal scientific field. A
specific corpus is designed by combining a controlled language (Medline MeSH
thesaurus) and the categorization of journals (Web of Science). From this
global corpus, the article characterizes the publications from the top 20
countries publishing in the field and evolutions between 2000 and 2020. Topic
maps show the research themes within the field of infectious diseases both in
the world and in France. The explosion of publications on Covid-19 in 2020 has
a quite visible impact on the topic map in infectious diseases and changes the
position of some countries in this field of research. The conclusion points to
issues for further research as more complete data will become available on the
Covid-19 period.",-0.14482075,-0.0621631,0.13414846,B
3763,There are a number of avenues for potential further research in this area.,"As such, in Section 4.2 we have outlined methods for
drilling down to conduct closer reading of research corpora, at greater detail, using
dynamic FoS networks.","For
example, in a corpus where full paper texts or abstracts are available, it may be
informative to explore semantic relationships between the ﬁelds of study represented
in the network.",2022-03-23 16:02:58+00:00,Author Multidisciplinarity and Disciplinary Roles in Field of Study Networks,cs.DL,"['cs.DL', 'cs.SI']","[arxiv.Result.Author('Eoghan Cunningham'), arxiv.Result.Author('Barry Smyth'), arxiv.Result.Author('Derek Greene')]","When studying large research corpora, ""distant reading"" methods are vital to
understand the topics and trends in the corresponding research space. In
particular, given the recognised benefits of multidisciplinary research, it may
be important to map schools or communities of diverse research topics, and to
understand the multidisciplinary role that topics play within and between these
communities. This work proposes Field of Study (FoS) networks as a novel
network representation for use in scientometric analysis. We describe the
formation of FoS networks, which relate research topics according to the
authors who publish in them, from corpora of articles in which fields of study
can be identified. FoS networks are particularly useful for the distant reading
of large datasets of research papers when analysed through the lens of
exploring multidisciplinary science. In an evolving scientific landscape,
modular communities in FoS networks offer an alternative categorisation
strategy for research topics and sub-disciplines, when compared to traditional
prescribed discipline classification schemes. Furthermore, structural role
analysis of FoS networks can highlight important characteristics of topics in
such communities. To support this, we present two case studies which explore
multidisciplinary research in corpora of varying size and scope; namely, 6,323
articles relating to network science research and 4,184,011 articles relating
to research on the COVID-19-pandemic.",0.086188875,0.114000514,-0.039230097,A
3950,"Results from bioas-
says inﬂuence decisions about whether the chemical warrants further study in
human clinical trials, potentially leading to its release onto the market.","By revealing whether a compound or biologic has the desired eﬀect
on a biological target, bioassays can drive decision-making throughout the drug
discovery process, to ultimately bring new drugs to patients.","Thus,
as an important precursor step, bioassays should be carefully planned to ensure
they are optimized for their speciﬁc purpose.",2022-03-28 08:35:01+00:00,The Digitalization of Bioassays in the Open Research Knowledge Graph,cs.DL,"['cs.DL', 'cs.AI', 'cs.IR']","[arxiv.Result.Author(""Jennifer D'Souza""), arxiv.Result.Author('Anita Monteverdi'), arxiv.Result.Author('Muhammad Haris'), arxiv.Result.Author('Marco Anteghini'), arxiv.Result.Author('Kheir Eddine Farfar'), arxiv.Result.Author('Markus Stocker'), arxiv.Result.Author('Vitor A. P. Martins dos Santos'), arxiv.Result.Author('Sören Auer')]","Background: Recent years are seeing a growing impetus in the semantification
of scholarly knowledge at the fine-grained level of scientific entities in
knowledge graphs. The Open Research Knowledge Graph (ORKG)
https://www.orkg.org/ represents an important step in this direction, with
thousands of scholarly contributions as structured, fine-grained,
machine-readable data. There is a need, however, to engender change in
traditional community practices of recording contributions as unstructured,
non-machine-readable text. For this in turn, there is a strong need for AI
tools designed for scientists that permit easy and accurate semantification of
their scholarly contributions. We present one such tool, ORKG-assays.
Implementation: ORKG-assays is a freely available AI micro-service in ORKG
written in Python designed to assist scientists obtain semantified bioassays as
a set of triples. It uses an AI-based clustering algorithm which on
gold-standard evaluations over 900 bioassays with 5,514 unique property-value
pairs for 103 predicates shows competitive performance. Results and Discussion:
As a result, semantified assay collections can be surveyed on the ORKG platform
via tabulation or chart-based visualizations of key property values of the
chemicals and compounds offering smart knowledge access to biochemists and
pharmaceutical researchers in the advancement of drug development.",-0.19236144,-0.44404748,0.015972601,B
4219,"Some recommendations are made to policymakers at
institutional and national level along with directions for further research that can be useful for the
improvement of the overall academic performance of the country.","Thus, the article presents a comprehensive and updated analysis
and characterization of international collaboration patterns in Indian scientific research, which
can be useful for various purposes.","References

Abramo, G., D’Angelo, C. A., & Di Costa, F. (2009).",2022-04-01 14:05:23+00:00,Measuring and Characterizing International Collaboration Patterns in Indian Scientific Research: Which proximity dimensions matter?,cs.DL,['cs.DL'],"[arxiv.Result.Author('Jyoti Dua'), arxiv.Result.Author('Vivek Kumar Singh'), arxiv.Result.Author('Hiran H. L')]","Scientific collaboration at international level has increased manifolds
during last two decades. Collaboration is not only associated with higher
research productivity but has also been found to be positively correlated with
impact. Considering the benefits and advantages of international collaboration
for the national science and technology systems of a country, policymakers in
different countries have designed programs to promote international
collaboration in science. Several proximity dimensions: mainly geographical,
cognitive, institutional, social, and economic, are found to shape and drive
the international collaborations. This paper attempts to measure and
characterize the international collaboration patterns in Indian scientific
research. Research publication data for the last 20 years (2001-2020), obtained
from Web of Science, is analysed for the purpose. Analytical results show that
Indian international collaboration has grown at a rate of 12.27% during this
period, rising from only 20.73% internationally collaborated papers in 2001 to
32.35% internationally collaborated papers in 2020. USA, Germany, England,
South Korea and China are found to be the top 5 major collaborating partners
for India during his period, however, some subject area-wise variations are
also seen. Among the internationally collaborated papers, about 50% papers have
an Indian researcher as lead (first) author, and more than 50% authors of such
internationally collaborated papers are from India. The internationally
collaborated papers from India are also found to have a slight advantage in
terms of citation impact and social media visibility. The cognitive proximity
is found to be the most effective proximity dimension in shaping the
international collaboration patterns of India.",-0.33078617,0.18558286,0.016975384,B
4220,"Some recommendations are made to policymakers at institutional and
national level along with directions for further research that can be useful for the improvement of
the overall academic performance of the country.","The lessons learned from the data and the past experiences can be
used to modify the existing programs and to institute new international cooperation programs in
science and technology.","Declarations

Funding and/or Conflicts of interests/Competing interests: The authors declare that no funding
was received for this work.",2022-04-01 14:05:23+00:00,Measuring and Characterizing International Collaboration Patterns in Indian Scientific Research,cs.DL,['cs.DL'],"[arxiv.Result.Author('Jyoti Dua'), arxiv.Result.Author('Vivek Kumar Singh'), arxiv.Result.Author('Hiran H. L')]","Scientific collaboration at international level has increased manifolds
during last two decades. Collaboration is not only associated with higher
research productivity but has also been found to be positively correlated with
impact. Considering the benefits and advantages of international collaboration
for the national science and technology systems of a country, policymakers in
different countries have designed programs to promote international
collaboration in science. This paper attempts to measure and characterize the
international collaboration patterns in Indian scientific research. Research
publication data for the last 20 years (2001-2020), obtained from Web of
Science, is analysed for the purpose. Analytical results show that India's
international collaboration has grown at a rate of 12.27% during this period,
rising from only 20.73% internationally collaborated papers in 2001 to 32.35%
internationally collaborated papers in 2020. USA, Germany, England, South Korea
and China are found to be the top collaborating partners for India during his
period, however, some subject area-wise variations are also seen. Among the
internationally collaborated papers, about 50% papers have an Indian researcher
as lead (first) author, and more than 50% authors of such internationally
collaborated papers are from India. The internationally collaborated papers
from India are also found to have a slight advantage in terms of citation
impact and social media visibility. The probable factors shaping the Indian
international collaboration and the major policy implications are discussed.",-0.2979327,0.06630516,0.00089115836,B
4255,"In future work, we plan to create a hierarchical publication-level
multi-labeled dataset to better understand the benefits of SciNoBo
and to encourage further research.","Even though
our dataset and baseline support only multiclass evaluation, ex-
perimental results and qualitative analysis demonstrated that our
method is effective and outperforms a deep-learning method which
rely solely on abstract information.","In addition, we plan to extend
our FoS taxonomy to broader levels, to provide a better sense of
granularity, which will also help us to identify emerging FoS labels
SciNoBo : A Hierarchical Multi-Label Classifier of Scientific Publications                             of the Web Conference 2021.",2022-04-02 15:09:33+00:00,SciNoBo : A Hierarchical Multi-Label Classifier of Scientific Publications,cs.DL,"['cs.DL', 'cs.AI']","[arxiv.Result.Author('Nikolaos Gialitsis'), arxiv.Result.Author('Sotiris Kotitsas'), arxiv.Result.Author('Haris Papageorgiou')]","Classifying scientific publications according to Field-of-Science (FoS)
taxonomies is of crucial importance, allowing funders, publishers, scholars,
companies and other stakeholders to organize scientific literature more
effectively. Most existing works address classification either at venue level
or solely based on the textual content of a research publication. We present
SciNoBo, a novel classification system of publications to predefined FoS
taxonomies, leveraging the structural properties of a publication and its
citations and references organised in a multilayer network. In contrast to
other works, our system supports assignments of publications to multiple fields
by considering their multidisciplinarity potential. By unifying publications
and venues under a common multilayer network structure made up of citing and
publishing relationships, classifications at the venue-level can be augmented
with publication-level classifications. We evaluate SciNoBo on a publications'
dataset extracted from Microsoft Academic Graph and we perform a comparative
analysis against a state-of-the-art neural-network baseline. The results reveal
that our proposed system is capable of producing high-quality classifications
of publications.",0.21698055,0.12484525,-0.07295869,A
4461,"This observation, made for the first
time on a local social media platform, calls again for further research on the fundamental aspects
of social media metrics that are independent of their global or local orientation.","The results corroborate
that WeChat indicators have a strong social media focus, rather than a scholarly focus (Wouters
et al., 2019), supporting the idea that local social media metrics may share fundamental traits
with global social media metrics (e.g., lack of correlation with bibliometrics, highly skewed
distributions, strong correlations within social media metrics) (Díaz-Faes et al., 2019; Costas et
al., 2016; Haunschild & Bornmann, 2017; Thelwall, 2021).","As such, the
potential existence of such fundamental aspects could pave the way towards generalized social
media metrics theories and frameworks (e.g., like the framework of “heterogeneous couplings”
recently proposed by Costas et al., 2020), which could be relevant to explain and work both
with global and local social media metrics.",2022-04-06 13:45:47+00:00,WeChat uptake of Chinese scholarly journals: an analysis of CSSCI-indexed journals,cs.DL,['cs.DL'],"[arxiv.Result.Author('Ting Cong'), arxiv.Result.Author('Zhichao Fang'), arxiv.Result.Author('Rodrigo Costas')]","The study of how science is discussed and how scholarly actors interact on
social media has increasingly become popular in the field of scientometrics in
recent years. While most prior studies focused on research outputs discussed on
global platforms, such as Twitter or Facebook, the presence of scholarly
journals on local platforms was seldom studied, especially in the Chinese
social media context. To fill this gap, this study investigates the uptake of
WeChat (a Chinese social network app) by the Chinese scholarly journals indexed
by the Chinese Social Sciences Citation Index (CSSCI). The results show that
65.3% of CSSCI-indexed journals have created WeChat public accounts and posted
over 193 thousand WeChat posts in total. At the journal level, bibliometric
indicators (e.g., citations, downloads, and journal impact factors) and WeChat
indicators (e.g., clicks, likes, replies, and recommendations) are weakly
correlated with each other, reinforcing the idea of fundamentally
differentiated dimensions of indicators between bibliometrics and social media
metrics. Results also show that journals with WeChat public accounts slightly
outperform those without WeChat public accounts in terms of citation impact,
suggesting that the WeChat presence of scientific journals is mostly positively
associated with their citation impact.",0.10047568,0.30420887,0.17937617,A
4551,"Our task is different, and so our model bares little resemblance, but
the overall strategy we argue is worth further study.","(2007) produced one of the ﬁrst applications of Bayesian modeling to the study of
citation behavior and inﬂuences.","Several latent factors exist in bibliometric study
to which modern machine learning may yield beneﬁts, and the scale of bibliometric data provides
fertile ground to new and technical challenges to advance the ﬁeld.",2022-04-08 04:03:17+00:00,Does the Market of Citations Reward Reproducible Work?,cs.DL,"['cs.DL', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Edward Raff')],"The field of bibliometrics, studying citations and behavior, is critical to
the discussion of reproducibility. Citations are one of the primary incentive
and reward systems for academic work, and so we desire to know if this
incentive rewards reproducible work. Yet to the best of our knowledge, only one
work has attempted to look at this combined space, concluding that
non-reproducible work is more highly cited. We show that answering this
question is more challenging than first proposed, and subtle issues can inhibit
a robust conclusion. To make inferences with more robust behavior, we propose a
hierarchical Bayesian model that incorporates the citation rate over time,
rather than the total number of citations after a fixed amount of time. In
doing so we show that, under current evidence the answer is more likely that
certain fields of study such as Medicine and Machine Learning (ML) do correlate
reproducible works with more citations, but other fields appear to have no
relationship. Further, we find that making code available and thoroughly
referencing prior works appear to also positively correlate with increased
citations. Our code and data can be found at
https://github.com/EdwardRaff/ReproducibleCitations .",0.16938691,0.27401805,0.02261672,A
4552,"This type of scientiﬁc communication appears to have a particularly
complex relationship with reproduction and the incentives around reproduction that thus warrants
further study.","Raff (2019) noted no relationship between this variable and replication,
while later work found that papers which use conceptualization ﬁgures take less time/human effort
to reproduce (Raff, 2021).","7
ML Evaluation Standards Workshop at ICLR 2022

The last points of note are that publishing in Journals, and more tables appear to increase citation
rate while publishing in a workshop reduces it.",2022-04-08 04:03:17+00:00,Does the Market of Citations Reward Reproducible Work?,cs.DL,"['cs.DL', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Edward Raff')],"The field of bibliometrics, studying citations and behavior, is critical to
the discussion of reproducibility. Citations are one of the primary incentive
and reward systems for academic work, and so we desire to know if this
incentive rewards reproducible work. Yet to the best of our knowledge, only one
work has attempted to look at this combined space, concluding that
non-reproducible work is more highly cited. We show that answering this
question is more challenging than first proposed, and subtle issues can inhibit
a robust conclusion. To make inferences with more robust behavior, we propose a
hierarchical Bayesian model that incorporates the citation rate over time,
rather than the total number of citations after a fixed amount of time. In
doing so we show that, under current evidence the answer is more likely that
certain fields of study such as Medicine and Machine Learning (ML) do correlate
reproducible works with more citations, but other fields appear to have no
relationship. Further, we find that making code available and thoroughly
referencing prior works appear to also positively correlate with increased
citations. Our code and data can be found at
https://github.com/EdwardRaff/ReproducibleCitations .",-0.14330736,0.073078774,-0.04786169,B
5509,"Apart from many practical aspects of interest, UDC LD development represents a good
testbed for further research especially through its interaction with other KOSs.","These connections made
through linked data can help to:

     •enrich bibliographic data to support information discovery by increasing subject access points using UDC
     terminology, by enabling semantic expansion (broadening); and by improving precision through contextual-
     ization;
     •improve systematic presentation, grouping, and visualization of resources and collections (linear or multi-
     dimensional) to facilitate browsing and serendipitous discovery of information;
     •link the classification to other KOSs to enable cross-collection information discovery; and,
     •validate and update local classification data and local authority files or bypass local and obsolete classifica-
     tion data in information exchange.","As an ex-
ample of an analytico-synthetic and faceted scheme, it provides a case study for managing
the alignment between the simple codes that appear in the scheme and the complex class-
marks generated through document indexing that contain unlimited numbers of combina-
tions of UDC classmarks.",2022-04-29 13:03:35+00:00,Classifications as Linked Open Data. Challenges and Opportunities,cs.DL,['cs.DL'],"[arxiv.Result.Author('Rick Szostak'), arxiv.Result.Author('Richard P. Smiraglia'), arxiv.Result.Author('Andrea Scharnhorst'), arxiv.Result.Author('Aida Slavic'), arxiv.Result.Author('Daniel Martínez-Ávila'), arxiv.Result.Author('Tobias Renwick')]","Linked Data (LD) as a web--based technology enables in principle the
seamless, machine--supported integration, interplay and augmentation of all
kinds of knowledge, into what has been labeled a huge knowledge graph. Despite
decades of web technology and, more recently, the LD approach, the task to
fully exploit these new technologies in the public domain is only commencing.
One specific challenge is to transfer techniques developed preweb to order our
knowledge into the realm of Linked Open Data (LOD) This paper illustrates two
different models in which a general analytico--synthetic classification can be
published and made available as LD. In both cases, an LD solution deals with
the intricacies of a pre--coordinated indexing language.",0.35280186,-0.15009266,-0.17613512,C
6173,"Finally, we found these additional data types:

    • (ix) documentation, consisting in deliverables for funders (2) and project
         notes (1)

    • (x) standards – languages or other conventions officially recognised as the
         norm in a certain research field (e.g., for interoperability)

    • (xi) corpora, a category specific to linguistics research
    • (xii) personal data28, which are always protected and never shared
    • (xiii) “born-digital artefacts” – by-products of users’ online interaction with

         digital infrastructures (e.g., tags, associations, texts) that can be used as input
         for further research.","All 3 require (viii) software to be written and executed, which stands in a category of
its own, both in Edmond’s classification (2019, p. 2) and in ours.","25 We defined events as one-off gatherings of people organised as a result of a research project, to
share ideas, offer training, or present something to the public.",2022-05-13 16:47:43+00:00,"What do we mean by ""data""? A proposed classification of data types in the arts and humanities",cs.DL,['cs.DL'],"[arxiv.Result.Author('Bianca Gualandi'), arxiv.Result.Author('Luca Pareschi'), arxiv.Result.Author('Silvio Peroni')]","Objectives: We describe here the interviews we conducted in late 2021 with 19
researchers at the Department of Classical Philology and Italian Studies at the
University of Bologna. The purpose has been to shed light on the definition of
the word ""data"" in the humanities domain, as far as FAIR data management
practices are concerned, and on what researchers think of the term. Methods: We
invited one researcher for each of the disciplinary areas represented within
the department and all 19 accepted to participate in the study. We divided
participants into 5 main research areas: philology and literary criticism,
language and linguistics, history of art, computer science, archival studies.
The interviews were transcribed and analysed using a grounded theory approach.
Results: A list of 13 research data types in the humanities has been compiled
thanks to the information collected from participants; publications emerge as
the most important one. The term ""data"" does not seem to be especially
problematic, contrary to what has been reported elsewhere. Regarding current
research and data management practices, methodologies and teamwork appear more
central than previously reported. Conclusions: ""Data"" in the FAIR framework
need to include all types of input and outputs humanities research work with,
including publications. Humanities researchers appear ready for a discussion
around making their data FAIR: they do not find the terminology particularly
problematic, while they rely on precise and recognised methodologies, as well
as on sharing and collaboration. Future studies should include more disciplines
to paint a more precise picture.",0.10319613,0.100799896,-0.25802416,B
6174,"Researchers keep everything (private)

All researchers but 1 stated that they save most of the data they use and produce
during a research project because they are potentially useful both in teaching and for
further research60.","As far as privacy is concerned, projects dealing with personal data in connection with
linguistic corpora or with born-digital artifacts have opted to either restrict access to
them and/or to have an advisor for privacy and ethical issues.","However, only 2 participants mention using an (open) repository,
in both cases Zenodo, while they all seem to be storing digital data on their own
devices, using external hard drives, or on commercial cloud storage services61.",2022-05-13 16:47:43+00:00,"What do we mean by ""data""? A proposed classification of data types in the arts and humanities",cs.DL,['cs.DL'],"[arxiv.Result.Author('Bianca Gualandi'), arxiv.Result.Author('Luca Pareschi'), arxiv.Result.Author('Silvio Peroni')]","Objectives: We describe here the interviews we conducted in late 2021 with 19
researchers at the Department of Classical Philology and Italian Studies at the
University of Bologna. The purpose has been to shed light on the definition of
the word ""data"" in the humanities domain, as far as FAIR data management
practices are concerned, and on what researchers think of the term. Methods: We
invited one researcher for each of the disciplinary areas represented within
the department and all 19 accepted to participate in the study. We divided
participants into 5 main research areas: philology and literary criticism,
language and linguistics, history of art, computer science, archival studies.
The interviews were transcribed and analysed using a grounded theory approach.
Results: A list of 13 research data types in the humanities has been compiled
thanks to the information collected from participants; publications emerge as
the most important one. The term ""data"" does not seem to be especially
problematic, contrary to what has been reported elsewhere. Regarding current
research and data management practices, methodologies and teamwork appear more
central than previously reported. Conclusions: ""Data"" in the FAIR framework
need to include all types of input and outputs humanities research work with,
including publications. Humanities researchers appear ready for a discussion
around making their data FAIR: they do not find the terminology particularly
problematic, while they rely on precise and recognised methodologies, as well
as on sharing and collaboration. Future studies should include more disciplines
to paint a more precise picture.",-0.22293451,0.11302134,-0.20486808,B
6175,"Finally, we found these additional data types:

    • (viii) documentation, consisting in deliverables for funders (2 projects) and
         project notes (1)

    • (x) personal data [10]
    • (xi) corpora, a category specific to linguistics research
    • (xii) standards – languages or other conventions officially recognised as the

         norm in a certain research field (e.g., for interoperability)
    • (xiii) “born-digital artefacts”, or by-products of users’ online interaction with

         digital infrastructures (e.g., tags, associations, texts) that can be used as input
         for further research.","All 3 require (vii) software to be written and executed, which stands in a category of
its own, both in Edmond’s classification (2019, p. 2) and in ours.","These 13 data types represent the “building blocks” of research in the arts and
humanities, as emerged from the interviews.",2022-05-13 16:47:43+00:00,"What do we mean by ""data""? A proposed classification of data types in the arts and humanities",cs.DL,['cs.DL'],"[arxiv.Result.Author('Bianca Gualandi'), arxiv.Result.Author('Luca Pareschi'), arxiv.Result.Author('Silvio Peroni')]","Purpose: This article describes the interviews we conducted in late 2021 with
19 researchers at the Department of Classical Philology and Italian Studies at
the University of Bologna. The main purpose was to shed light on the definition
of the word ""data"" in the humanities domain, as far as FAIR data management
practices are concerned, and on what researchers think of the term.
Methodology: We invited one researcher for each of the official disciplinary
areas represented within the department and all 19 accepted to participate in
the study. Participants were then divided into 5 main research areas: philology
and literary criticism, language and linguistics, history of art, computer
science, archival studies. The interviews were transcribed and analysed using a
grounded theory approach. Findings: A list of 13 research data types has been
compiled thanks to the information collected from participants. The term ""data""
does not emerge as especially problematic, contrary to what has been reported
elsewhere. Looking at current research management practices, methodologies and
teamwork appear more central than previously reported. Originality: Our
findings confirm that ""data"" within the FAIR framework should include all types
of input and outputs humanities research work with, including publications.
Also, the participants to this study appear ready for a discussion around
making their research data FAIR: they do not find the terminology particularly
problematic, while they rely on precise and recognised methodologies, as well
as on sharing and collaboration with colleagues.",0.0040788017,0.11225276,-0.303184,B
6176,"13

    • (viii) documentation, consisting in deliverables for funders (2 projects) and
         project notes (1)

    • (x) personal data [10]
    • (xi) corpora, a category specific to linguistics research
    • (xii) standards – languages or other conventions officially recognised as the

         norm in a certain research field (e.g., for interoperability)
    • (xiii) “born-digital artefacts”, or by-products of users’ online interaction with

         digital infrastructures (e.g., tags, associations, texts) that can be used as input
         for further research.","Finally, we found these additional data types:
WHAT DO WE MEAN BY “DATA” IN THE ARTS AND HUMANITIES?","These 13 data types represent the “building blocks” of research in the arts and
humanities, as emerged from the interviews.",2022-05-13 16:47:43+00:00,"What do we mean by ""data""? A proposed classification of data types in the arts and humanities",cs.DL,['cs.DL'],"[arxiv.Result.Author('Bianca Gualandi'), arxiv.Result.Author('Luca Pareschi'), arxiv.Result.Author('Silvio Peroni')]","Purpose: This article describes the interviews we conducted in late 2021 with
19 researchers at the Department of Classical Philology and Italian Studies at
the University of Bologna. The main purpose was to shed light on the definition
of the word ""data"" in the humanities domain, as far as FAIR data management
practices are concerned, and on what researchers think of the term.
Methodology: We invited one researcher for each of the official disciplinary
areas represented within the department and all 19 accepted to participate in
the study. Participants were then divided into 5 main research areas: philology
and literary criticism, language and linguistics, history of art, computer
science, archival studies. The interviews were transcribed and analysed using a
grounded theory approach. Findings: A list of 13 research data types has been
compiled thanks to the information collected from participants. The term ""data""
does not emerge as especially problematic, although a good deal of confusion
remains. Looking at current research management practices, methodologies and
teamwork appear more central than previously reported. Originality: Our
findings confirm that ""data"" within the FAIR framework should include all types
of input and outputs humanities research work with, including publications.
Also, the participants to this study appear ready for a discussion around
making their research data FAIR: they do not find the terminology particularly
problematic, while they rely on precise and recognised methodologies, as well
as on sharing and collaboration with colleagues.",-0.06301487,0.1945365,-0.33891255,B
6177,"13

    • (viii) documentation, consisting in deliverables for funders (2 projects) and
         project notes (1)

    • (x) personal data [10]
    • (xi) corpora, a category specific to linguistics research
    • (xii) standards – languages or other conventions officially recognised as the

         norm in a certain research field (e.g., for interoperability)
    • (xiii) “born-digital artefacts”, or by-products of users’ online interaction with

         digital infrastructures (e.g., tags, associations, texts) that can be used as input
         for further research.","Finally, we found these additional data types:
WHAT DO WE MEAN BY “DATA” IN THE ARTS AND HUMANITIES?","These 13 data types represent the “building blocks” of research in the arts and
humanities, as emerged from the interviews.",2022-05-13 16:47:43+00:00,"What do we mean by ""data""? A proposed classification of data types in the arts and humanities",cs.DL,['cs.DL'],"[arxiv.Result.Author('Bianca Gualandi'), arxiv.Result.Author('Luca Pareschi'), arxiv.Result.Author('Silvio Peroni')]","Purpose: This article describes the interviews we conducted in late 2021 with
19 researchers at the Department of Classical Philology and Italian Studies at
the University of Bologna. The main purpose was to shed light on the definition
of the word ""data"" in the humanities domain, as far as FAIR data management
practices are concerned, and on what researchers think of the term.
Methodology: We invited one researcher for each of the official disciplinary
areas represented within the department and all 19 accepted to participate in
the study. Participants were then divided into 5 main research areas: philology
and literary criticism, language and linguistics, history of art, computer
science, archival studies. The interviews were transcribed and analysed using a
grounded theory approach. Findings: A list of 13 research data types has been
compiled thanks to the information collected from participants. The term ""data""
does not emerge as especially problematic, although a good deal of confusion
remains. Looking at current research management practices, methodologies and
teamwork appear more central than previously reported. Originality: Our
findings confirm that ""data"" within the FAIR framework should include all types
of input and outputs humanities research work with, including publications.
Also, the participants to this study appear ready for a discussion around
making their research data FAIR: they do not find the terminology particularly
problematic, while they rely on precise and recognised methodologies, as well
as on sharing and collaboration with colleagues.",-0.06301487,0.1945365,-0.33891255,B
8638,"To further study the dynamical patterns
underlying individual paper citation dynamics, we fit each paper’s citation dynamics to
a generative model [35] (detailed model descriptions are shown in Supplementary
Material S3.1).","First, we investigate the
association between paper interdisciplinarity and 𝑇% for each year by conducting
separate regressions (Figure S15), and use alternative interdisciplinary indicators
(Figure S14A), finding similar results.","We then replace 𝑇% with relevant parameters in the model, and
conduct the regression analysis, finding consistent results (Figure S14B).",2022-07-09 10:51:47+00:00,Delayed Impact of Interdisciplinary Research,cs.DL,"['cs.DL', 'physics.soc-ph']","[arxiv.Result.Author('Yang Zhang'), arxiv.Result.Author('Yang Wang'), arxiv.Result.Author('Haifeng Du'), arxiv.Result.Author('Shlomo Havlin')]","Interdisciplinary research increasingly fuels innovation, and is considered
to be a key to tomorrow breakthrough. Yet little is known about whether
interdisciplinary research manifests delayed impact. Here, we use the time to
reach the citation peak to quantify the highest impact time and citation
dynamics, and examine its relationship with interdisciplinarity. Using large
scale publication datasets, our results suggest that interdisciplinary papers
show significant delayed impact both microscopically per paper and
macroscopically collectively, as it takes longer time for interdisciplinary
papers to reach their citation peak. Furthermore, we study the underlying
forces of such delayed impact, finding that the effect goes beyond the Matthew
effect (i.e., the rich-get-richer effect). Finally, we find that team size and
content conventionality only partly account for this effect. Overall, our
results suggest that governments, research administrators, funding agencies
should be aware of this general feature of interdisciplinary science, which may
have broad policy implications.",0.018454522,0.20342222,0.023087364,A
9628,"To further study the trends in the research output and percentage contribution of the different
institution systems, the proportionate share of research contribution of each institution system
to India’s total output is shown in Figure 5.",Research output of each group of institutions during the period of 2001-2020.,"The pie charts are shown for four different time
period blocks, 2001-05, 2006-10, 2011-15, and 2016-20.",2022-08-02 16:52:56+00:00,Research Contribution of major Centrally Funded Institution Systems of India,cs.DL,['cs.DL'],"[arxiv.Result.Author('Anurag Kanaujia'), arxiv.Result.Author('Prashasti Singh'), arxiv.Result.Author('Abhirup Nandy'), arxiv.Result.Author('Vivek Kumar Singh')]","India is now among the major knowledge producers of the world, ranking among
the top 5 countries in total research output, as per some recent reports. The
institutional setup for Research & Development (R&D) in India comprises a
diverse set of Institutions, including Universities, government departments,
research laboratories, and private sector institutions etc. It may be noted
that more than 45% share of India's Gross Expenditure on Research and
Development (GERD) comes from the central government. In this context, this
article attempts to explore the quantum of research contribution of centrally
funded institutions and institution systems of India. The volume, proportionate
share and growth patterns of research publications from the major centrally
funded institutions, organised in 16 groups, is analysed. These institutions
taken together account for 67.54% of Indian research output during 2001 to
2020. The research output of the centrally funded institutions in India has
increased steadily since 2001 with a good value for CAGR. The paper presents
noteworthy insights about scientific research production of India that may be
useful to policymakers, researchers and science practitioners in India. It
presents a case for increased activity by the state governments and private
sector to further the cause of sustainable and inclusive research and
development in the country.",-0.24271624,0.07074904,0.13585511,B
10013,"We further study how the single-topic collaboration phenomenon evolved in the past
decades.","The results show that both correlations decrease with the
career years of the focal scientists, suggesting that senior scientists are less sensitive to the
past collaboration performance when involving existing collaborators to a new topic.","To this end, we consider focal scientists who started their career in diﬀerent years
and calculate the fraction of their single-topic collaborators.",2022-08-13 16:54:19+00:00,Impactful scientists have higher tendency to involve collaborators in new topics,cs.DL,"['cs.DL', 'physics.soc-ph']","[arxiv.Result.Author('An Zeng'), arxiv.Result.Author('Ying Fan'), arxiv.Result.Author('Zengru Di'), arxiv.Result.Author('Yougui Wang'), arxiv.Result.Author('Shlomo Havlin')]","In scientific research, collaboration is one of the most effective ways to
take advantage of new ideas, skills, resources, and for performing
interdisciplinary research. Although collaboration networks have been
intensively studied, the question of how individual scientists choose
collaborators to study a new research topic remains almost unexplored. Here, we
investigate the statistics and mechanisms of collaborations of individual
scientists along their careers, revealing that, in general, collaborators are
involved in significantly fewer topics than expected from controlled surrogate.
In particular, we find that highly productive scientists tend to have higher
fraction of single-topic collaborators, while highly cited, i.e., impactful,
scientists have higher fraction of multi-topic collaborators. We also suggest a
plausible mechanism for this distinction. Moreover, we investigate the cases
where scientists involve existing collaborators into a new topic. We find that
compared to productive scientists, impactful scientists show strong preference
of collaboration with high impact scientists on a new topic. Finally, we
validate our findings by investigating active scientists in different years and
across different disciplines.",-0.24514624,0.27917612,0.14074533,A
10014,"We further study for a focal scientist what are the features of his/her
existing collaborators when starting a new topic.","The impactful scientists tend to have collaborators
sharing similar research interests, while productive scientists tend to have collaborators
specialized in a topic.","We ﬁnd a stronger tendency of highly cited
scientists to involve collaborators with many publications and high citations per paper, yet,
in contrast, highly productive scientists have much weaker such tendency.",2022-08-13 16:54:19+00:00,Impactful scientists have higher tendency to involve collaborators in new topics,cs.DL,"['cs.DL', 'physics.soc-ph']","[arxiv.Result.Author('An Zeng'), arxiv.Result.Author('Ying Fan'), arxiv.Result.Author('Zengru Di'), arxiv.Result.Author('Yougui Wang'), arxiv.Result.Author('Shlomo Havlin')]","In scientific research, collaboration is one of the most effective ways to
take advantage of new ideas, skills, resources, and for performing
interdisciplinary research. Although collaboration networks have been
intensively studied, the question of how individual scientists choose
collaborators to study a new research topic remains almost unexplored. Here, we
investigate the statistics and mechanisms of collaborations of individual
scientists along their careers, revealing that, in general, collaborators are
involved in significantly fewer topics than expected from controlled surrogate.
In particular, we find that highly productive scientists tend to have higher
fraction of single-topic collaborators, while highly cited, i.e., impactful,
scientists have higher fraction of multi-topic collaborators. We also suggest a
plausible mechanism for this distinction. Moreover, we investigate the cases
where scientists involve existing collaborators into a new topic. We find that
compared to productive scientists, impactful scientists show strong preference
of collaboration with high impact scientists on a new topic. Finally, we
validate our findings by investigating active scientists in different years and
across different disciplines.",-0.253384,0.3088062,0.08486907,A
10345,"We hope this dataset will support further research on the interactions of scientific
authors on social media and serve as a base for developing alternative and/or complementary
approaches to match Twitter users and authors.","This short
paper aims to introduce a dataset of scholars’ Twitter accounts identified with a naïve algorithm
based entirely on available open data, presenting the process in detail with accompanying R and
Python scripts so that the process can be easily replicated and/or improved upon by the research
community.",The paper is structured as follows.,2022-08-23 16:16:41+00:00,An open dataset of scholars on Twitter,cs.DL,['cs.DL'],"[arxiv.Result.Author('Philippe Mongeon'), arxiv.Result.Author('Timothy D. Bowman'), arxiv.Result.Author('Rodrigo Costas')]","The role played by research scholars in the dissemination of scientific
knowledge on social media has always been a central topic in social media
metrics (altmetrics) research. Different approaches have been implemented to
identify and characterize active scholars on social media platforms like
Twitter. Some limitations of past approaches were their complexity and, most
importantly, their reliance on licensed scientometric and altmetric data. The
emergence of new open data sources like OpenAlex or Crossref Event Data
provides opportunities to identify scholars on social media using only open
data. This paper presents a novel and simple approach to match authors from
OpenAlex with Twitter users identified in Crossref Event Data. The matching
procedure is described and validated with ORCID data. The new approach matches
nearly 500,000 matched scholars with their Twitter accounts with a level of
high precision and moderate recall. The dataset of matched scholars is
described and made openly available to the scientific community to empower more
advanced studies of the interactions of research scholars on Twitter.",0.03683785,0.46266952,-0.04725579,A_centroid
10346,"We hope this dataset will support further research on the interactions of scientific
authors on social media and serve as a base for developing alternative and/or complementary
approaches to match Twitter users and authors.","This short
paper aims to introduce a dataset of scholars’ Twitter accounts identified with a naïve algorithm
based entirely on available open data, presenting the process in detail with accompanying R and
Python scripts so that the process can be easily replicated and/or improved upon by the research
community.",The paper is structured as follows.,2022-08-23 16:16:41+00:00,An open dataset of scholars on Twitter,cs.DL,['cs.DL'],"[arxiv.Result.Author('Philippe Mongeon'), arxiv.Result.Author('Timothy D. Bowman'), arxiv.Result.Author('Rodrigo Costas')]","The role played by research scholars in the dissemination of scientific
knowledge on social media has always been a central topic in social media
metrics (altmetrics) research. Different approaches have been implemented to
identify and characterize active scholars on social media platforms like
Twitter. Some limitations of past approaches were their complexity and, most
importantly, their reliance on licensed scientometric and altmetric data. The
emergence of new open data sources like OpenAlex or Crossref Event Data
provides opportunities to identify scholars on social media using only open
data. This paper presents a novel and simple approach to match authors from
OpenAlex with Twitter users identified in Crossref Event Data. The matching
procedure is described and validated with ORCID data. The new approach matches
nearly 500,000 matched scholars with their Twitter accounts with a level of
high precision and moderate recall. The dataset of matched scholars is
described and made openly available to the scientific community to empower more
advanced studies of the interactions of research scholars on Twitter.",0.03683785,0.46266952,-0.04725579,A
10693,"Other
                                                                     potential areas of further research include looking into
be one of the key technologies in our eﬀort to develop sci-          open-access policies of most used or impactful software
ence and technologies to measure human biology in action             or exploring diﬀerences in how software usage varies over
[47, 48].","Trends in imaging                                             the notion of Eigenfactor, which has been proposed mea-
    The Chan Zuckerberg Initiative considers imaging to              sure impact for journals and authors, to software.",To fund this area more eﬃciently and support               time.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe a new dataset of software mentions in biomedical papers.
Plain-text software mentions are extracted with a trained SciBERT model from
several sources: the NIH PubMed Central collection and from papers provided by
various publishers to the Chan Zuckerberg Initiative. The dataset provides
sources, context and metadata, and, for a number of mentions, the disambiguated
software entities and links. We extract 1.12 million unique string software
mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k
unique mentions from the NIH PMC-OA Non-Commercial subset (both gathered in
October 2021) and 934k unique mentions from 3 million papers in the Publishers'
collection. There is variation in how software is mentioned in papers and
extracted by the NER algorithm. We propose a clustering-based disambiguation
algorithm to map plain-text software mentions into distinct software entities
and apply it on the NIH PubMed Central Commercial collection. Through this
methodology, we disambiguate 1.12 million unique strings extracted by the NER
model into 97600 unique software entities, covering 78% of all software-paper
links. We link 185000 of the mentions to a repository, covering about 55% of
all software-paper links. We describe in detail the process of building the
datasets, disambiguating and linking the software mentions, as well as
opportunities and challenges that come with a dataset of this size. We make all
data and code publicly available as a new resource to help assess the impact of
software (in particular scientific open source projects) on science.",-0.35918987,-0.2485908,0.18188284,B
10694,"ware mentions to corresponding URLs in repositories or
                                                                     databases is an interesting area for further research.","Improving the quality of linking soft-
our results with Wikidata in the future releases.",6.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe a new dataset of software mentions in biomedical papers.
Plain-text software mentions are extracted with a trained SciBERT model from
several sources: the NIH PubMed Central collection and from papers provided by
various publishers to the Chan Zuckerberg Initiative. The dataset provides
sources, context and metadata, and, for a number of mentions, the disambiguated
software entities and links. We extract 1.12 million unique string software
mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k
unique mentions from the NIH PMC-OA Non-Commercial subset (both gathered in
October 2021) and 934k unique mentions from 3 million papers in the Publishers'
collection. There is variation in how software is mentioned in papers and
extracted by the NER algorithm. We propose a clustering-based disambiguation
algorithm to map plain-text software mentions into distinct software entities
and apply it on the NIH PubMed Central Commercial collection. Through this
methodology, we disambiguate 1.12 million unique strings extracted by the NER
model into 97600 unique software entities, covering 78% of all software-paper
links. We link 185000 of the mentions to a repository, covering about 55% of
all software-paper links. We describe in detail the process of building the
datasets, disambiguating and linking the software mentions, as well as
opportunities and challenges that come with a dataset of this size. We make all
data and code publicly available as a new resource to help assess the impact of
software (in particular scientific open source projects) on science.",0.47123715,0.067196354,-0.19651443,C
10695,"Other
                                                                     potential areas of further research include looking into
be one of the key technologies in our eﬀort to develop sci-          open-access policies of most used or impactful software
ence and technologies to measure human biology in action             or exploring diﬀerences in how software usage varies over
[49, 50].","Trends in imaging                                             the notion of Eigenfactor, which has been proposed mea-
    The Chan Zuckerberg Initiative considers imaging to              sure impact for journals and authors, to software.",To fund this area more eﬃciently and support               time.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe a new dataset of software mentions in biomedical papers.
Plain-text software mentions are extracted with a trained SciBERT model from
several sources: the NIH PubMed Central collection and from papers provided by
various publishers to the Chan Zuckerberg Initiative. The dataset provides
sources, context and metadata, and, for a number of mentions, the disambiguated
software entities and links. We extract 1.12 million unique string software
mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k
unique mentions from the NIH PMC-OA Non-Commercial subset (both gathered in
October 2021) and 934k unique mentions from 3 million papers in the Publishers'
collection. There is variation in how software is mentioned in papers and
extracted by the NER algorithm. We propose a clustering-based disambiguation
algorithm to map plain-text software mentions into distinct software entities
and apply it on the NIH PubMed Central Commercial collection. Through this
methodology, we disambiguate 1.12 million unique strings extracted by the NER
model into 97600 unique software entities, covering 78% of all software-paper
links. We link 185000 of the mentions to a repository, covering about 55% of
all software-paper links. We describe in detail the process of building the
datasets, disambiguating and linking the software mentions, as well as
opportunities and challenges that come with a dataset of this size. We make all
data and code publicly available as a new resource to help assess the impact of
software (in particular scientific open source projects) on science.",-0.36179477,-0.24818707,0.18002358,B_centroid
10696,"Improving the quality of linking soft-                 the ACM 2011 Conference on Computer Supported Cooperative
ware mentions to corresponding URLs in repositories or                      Work, CSCW ’11, page 513–522, New York, NY, USA, 2011.
databases is an interesting area for further research.","In Proceedings of
are worth exploring.",Association for Computing Machinery.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe a new dataset of software mentions in biomedical papers.
Plain-text software mentions are extracted with a trained SciBERT model from
several sources: the NIH PubMed Central collection and from papers provided by
various publishers to the Chan Zuckerberg Initiative. The dataset provides
sources, context and metadata, and, for a number of mentions, the disambiguated
software entities and links. We extract 1.12 million unique string software
mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k
unique mentions from the NIH PMC-OA Non-Commercial subset (both gathered in
October 2021) and 934k unique mentions from 3 million papers in the Publishers'
collection. There is variation in how software is mentioned in papers and
extracted by the NER algorithm. We propose a clustering-based disambiguation
algorithm to map plain-text software mentions into distinct software entities
and apply it on the NIH PubMed Central Commercial collection. Through this
methodology, we disambiguate 1.12 million unique strings extracted by the NER
model into 97600 unique software entities, covering 78% of all software-paper
links. We link 185000 of the mentions to a repository, covering about 55% of
all software-paper links. We describe in detail the process of building the
datasets, disambiguating and linking the software mentions, as well as
opportunities and challenges that come with a dataset of this size. We make all
data and code publicly available as a new resource to help assess the impact of
software (in particular scientific open source projects) on science.",0.53355855,0.10603521,-0.02356524,C_centroid
10697,"Other
                                                                     potential areas of further research include looking into
be one of the key technologies in our eﬀort to develop sci-          open-access policies of most used or impactful software
ence and technologies to measure human biology in action             or exploring diﬀerences in how software usage varies over
[49, 50].","Trends in imaging                                             the notion of Eigenfactor, which has been proposed mea-
    The Chan Zuckerberg Initiative considers imaging to              sure impact for journals and authors, to software.",To fund this area more eﬃciently and support               time.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe a new dataset of software mentions in biomedical papers.
Plain-text software mentions are extracted with a trained SciBERT model from
several sources: the NIH PubMed Central collection and from papers provided by
various publishers to the Chan Zuckerberg Initiative. The dataset provides
sources, context and metadata, and, for a number of mentions, the disambiguated
software entities and links. We extract 1.12 million unique string software
mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k
unique mentions from the NIH PMC-OA Non-Commercial subset (both gathered in
October 2021) and 934k unique mentions from 3 million papers in the Publishers'
collection. There is variation in how software is mentioned in papers and
extracted by the NER algorithm. We propose a clustering-based disambiguation
algorithm to map plain-text software mentions into distinct software entities
and apply it on the NIH PubMed Central Commercial collection. Through this
methodology, we disambiguate 1.12 million unique strings extracted by the NER
model into 97600 unique software entities, covering 78% of all software-paper
links. We link 185000 of the mentions to a repository, covering about 55% of
all software-paper links. We describe in detail the process of building the
datasets, disambiguating and linking the software mentions, as well as
opportunities and challenges that come with a dataset of this size. We make all
data and code publicly available as a new resource to help assess the impact of
software (in particular scientific open source projects) on science.",-0.36179477,-0.24818707,0.18002358,B
10698,"Improving the quality of linking soft-                 the ACM 2011 Conference on Computer Supported Cooperative
ware mentions to corresponding URLs in repositories or                      Work, CSCW ’11, page 513–522, New York, NY, USA, 2011.
databases is an interesting area for further research.","In Proceedings of
are worth exploring.",Association for Computing Machinery.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe a new dataset of software mentions in biomedical papers.
Plain-text software mentions are extracted with a trained SciBERT model from
several sources: the NIH PubMed Central collection and from papers provided by
various publishers to the Chan Zuckerberg Initiative. The dataset provides
sources, context and metadata, and, for a number of mentions, the disambiguated
software entities and links. We extract 1.12 million unique string software
mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k
unique mentions from the NIH PMC-OA Non-Commercial subset (both gathered in
October 2021) and 934k unique mentions from 3 million papers in the Publishers'
collection. There is variation in how software is mentioned in papers and
extracted by the NER algorithm. We propose a clustering-based disambiguation
algorithm to map plain-text software mentions into distinct software entities
and apply it on the NIH PubMed Central Commercial collection. Through this
methodology, we disambiguate 1.12 million unique strings extracted by the NER
model into 97600 unique software entities, covering 78% of all software-paper
links. We link 185000 of the mentions to a repository, covering about 55% of
all software-paper links. We describe in detail the process of building the
datasets, disambiguating and linking the software mentions, as well as
opportunities and challenges that come with a dataset of this size. We make all
data and code publicly available as a new resource to help assess the impact of
software (in particular scientific open source projects) on science.",0.53355855,0.10603521,-0.02356524,C
10699,"Other
                                                                     potential areas of further research include looking into
be one of the key technologies in our eﬀort to develop sci-          open-access policies of most used or impactful software
ence and technologies to measure human biology in action             or exploring diﬀerences in how software usage varies over
[49, 50].","Trends in imaging                                             the notion of Eigenfactor, which has been proposed mea-
    The Chan Zuckerberg Initiative considers imaging to              sure impact for journals and authors, to software.",To fund this area more eﬃciently and support               time.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe the CZ Software Mentions dataset, a new dataset of software
mentions in biomedical papers. Plain-text software mentions are extracted with
a trained SciBERT model from several sources: the NIH PubMed Central collection
and from papers provided by various publishers to the Chan Zuckerberg
Initiative. The dataset provides sources, context and metadata, and, for a
number of mentions, the disambiguated software entities and links. We extract
1.12 million unique string software mentions from 2.4 million papers in the NIH
PMC-OA Commercial subset, 481k unique mentions from the NIH PMC-OA
Non-Commercial subset (both gathered in October 2021) and 934k unique mentions
from 3 million papers in the Publishers' collection. There is variation in how
software is mentioned in papers and extracted by the NER algorithm. We propose
a clustering-based disambiguation algorithm to map plain-text software mentions
into distinct software entities and apply it on the NIH PubMed Central
Commercial collection. Through this methodology, we disambiguate 1.12 million
unique strings extracted by the NER model into 97600 unique software entities,
covering 78% of all software-paper links. We link 185000 of the mentions to a
repository, covering about 55% of all software-paper links. We describe in
detail the process of building the datasets, disambiguating and linking the
software mentions, as well as opportunities and challenges that come with a
dataset of this size. We make all data and code publicly available as a new
resource to help assess the impact of software (in particular scientific open
source projects) on science.",-0.36179477,-0.24818707,0.18002358,B
10700,"Improving the quality of linking soft-                 the ACM 2011 Conference on Computer Supported Cooperative
ware mentions to corresponding URLs in repositories or                      Work, CSCW ’11, page 513–522, New York, NY, USA, 2011.
databases is an interesting area for further research.","In Proceedings of
are worth exploring.",Association for Computing Machinery.,2022-09-01 19:04:47+00:00,A large dataset of software mentions in the biomedical literature,cs.DL,"['cs.DL', 'q-bio.OT']","[arxiv.Result.Author('Ana-Maria Istrate'), arxiv.Result.Author('Donghui Li'), arxiv.Result.Author('Dario Taraborelli'), arxiv.Result.Author('Michaela Torkar'), arxiv.Result.Author('Boris Veytsman'), arxiv.Result.Author('Ivana Williams')]","We describe the CZ Software Mentions dataset, a new dataset of software
mentions in biomedical papers. Plain-text software mentions are extracted with
a trained SciBERT model from several sources: the NIH PubMed Central collection
and from papers provided by various publishers to the Chan Zuckerberg
Initiative. The dataset provides sources, context and metadata, and, for a
number of mentions, the disambiguated software entities and links. We extract
1.12 million unique string software mentions from 2.4 million papers in the NIH
PMC-OA Commercial subset, 481k unique mentions from the NIH PMC-OA
Non-Commercial subset (both gathered in October 2021) and 934k unique mentions
from 3 million papers in the Publishers' collection. There is variation in how
software is mentioned in papers and extracted by the NER algorithm. We propose
a clustering-based disambiguation algorithm to map plain-text software mentions
into distinct software entities and apply it on the NIH PubMed Central
Commercial collection. Through this methodology, we disambiguate 1.12 million
unique strings extracted by the NER model into 97600 unique software entities,
covering 78% of all software-paper links. We link 185000 of the mentions to a
repository, covering about 55% of all software-paper links. We describe in
detail the process of building the datasets, disambiguating and linking the
software mentions, as well as opportunities and challenges that come with a
dataset of this size. We make all data and code publicly available as a new
resource to help assess the impact of software (in particular scientific open
source projects) on science.",0.53355855,0.10603521,-0.02356524,C
10889,"The coappearing authors for these journal titles
are Kulkarni A. J. and Sarmah D. K. Kulkarni A. J. is leading author has he had developed CI
algorithms and further research continued under his supervision.","20 are studies in computational intelligence, neural computing and applications,
advances in intelligent systems and computing, expert systems with application, intelligent
systems reference library, evolutionary intelligence, International Journal of Parallel, Emergent
and Distributed Systems, Information Sciences etc.","Kulkarni A. J. has his research
lab also called OAT Research Lab and he did his PhD from NTU Singapore.",2022-09-07 09:09:33+00:00,Biblio-Analysis of Cohort Intelligence (CI) Algorithm and its allied applications from Scopus and Web of Science Perspective,cs.DL,"['cs.DL', 'cs.AI', 'stat.OT']","[arxiv.Result.Author('Ishaan Kale'), arxiv.Result.Author('Rahul Joshi'), arxiv.Result.Author('Kalyani Kadam')]","Cohort Intelligence or CI is one of its kind of novel optimization algorithm.
Since its inception, in a very short span it is applied successfully in various
domains and its results are observed to be effectual in contrast to algorithm
of its kind. Till date, there is no such type of bibliometric analysis carried
out on CI and its related applications. So, this research paper in a way will
be an ice breaker for those who want to take up CI to a new level. In this
research papers, CI publications available in Scopus are analyzed through
graphs, networked diagrams about authors, source titles, keywords over the
years, journals over the time. In a way this bibliometric paper showcase CI,
its applications and detail outs systematic review in terms its bibliometric
details.",0.00088386587,0.06083855,0.07657164,B
10935,"At the
same time, we also believe that there are many issues about the method entity worthy of further study.","We have seen valuable work on the extraction, evaluation, and application of method entities.","Therefore, in this section, we discuss the future directions of method entity-related research

(1) Carrying out large-scale method entity extraction with the limited corpus

Compared with the data set for general NER, the public data set of method entities has a small number
and a single type.",2022-09-08 10:12:21+00:00,"A Review on Method Entities in the Academic Literature: Extraction, Evaluation, and Application",cs.DL,"['cs.DL', 'cs.CL']","[arxiv.Result.Author('Yuzhuo Wang'), arxiv.Result.Author('Chengzhi Zhang'), arxiv.Result.Author('Kai Li')]","In scientific research, the method is an indispensable means to solve
scientific problems and a critical research object. With the advancement of
sciences, many scientific methods are being proposed, modified, and used in
academic literature. The authors describe details of the method in the abstract
and body text, and key entities in academic literature reflecting names of the
method are called method entities. Exploring diverse method entities in a
tremendous amount of academic literature helps scholars understand existing
methods, select the appropriate method for research tasks, and propose new
methods. Furthermore, the evolution of method entities can reveal the
development of a discipline and facilitate knowledge discovery. Therefore, this
article offers a systematic review of methodological and empirical works
focusing on extracting method entities from full-text academic literature and
efforts to build knowledge services using these extracted method entities.
Definitions of key concepts involved in this review were first proposed. Based
on these definitions, we systematically reviewed the approaches and indicators
to extract and evaluate method entities, with a strong focus on the pros and
cons of each approach. We also surveyed how extracted method entities are used
to build new applications. Finally, limitations in existing works as well as
potential next steps were discussed.",0.08483519,0.06490012,-0.040222477,A
11010,"A brief
analysis of the questionnaire followed by recommendations and considerations for further research are presented.","The study shows that only 71.4% of a sample of authors of papers tied to NSF grants through
acknowledgements reported in the survey that NSF funded the research presented in the respective papers.","The discrepancy in reporting appears to indicate that funding streams can be fluid and not always apparent to
authors, overall raising the question of what sorts of research should be addressed with funding statements, where
conceptually tying a paper directly to a grant is not straightforward.",2022-09-09 22:35:40+00:00,On the Reliability of Funding Acknowledgements as Research Data: Evidence from Astronomy,cs.DL,['cs.DL'],[arxiv.Result.Author('Gretchen Stahlman')],"Online bibliographic databases have enabled new research through which
bibliographic records are analyzed as data about science. Within these records,
the acknowledgements sections of papers are often used to draw conclusions
about funding support for published research. While acknowledgements and
funding statements can be informative for research and policy development, this
poster adds to a body of literature that highlights limitations of funding data
for scientometric and policy research, using evidence gathered from a
questionnaire of authors of astronomy journal articles. The study shows that
only 71.4% of a sample of authors of papers tied to NSF grants through
acknowledgements reported in the survey that NSF funded the research presented
in the respective papers. A brief analysis of the questionnaire followed by
recommendations and considerations for further research are presented. The
discrepancy in reporting appears to indicate that funding streams can be fluid
and not always apparent to authors, overall raising the question of what sorts
of research should be addressed with funding statements, where conceptually
tying a paper directly to a grant is not straightforward.",-0.23813301,0.028466873,0.012598546,B
11568,"This will be
beneficial for researchers and practitioners who require a reliable knowledge base from
which to launch further research into learning and teaching via social media.","and
revealed research trends over the last four years by citation analysis.","Future
work can be carried out to conduct content analysis to evaluate and identify trends and
themes in data sets and identify self-citations of authors that may some impact on
research quality.",2022-09-22 19:32:31+00:00,The Impact of Social Media in Learning and Teaching: A Bibliometric-based Citation Analysis,cs.DL,"['cs.DL', 'cs.CY', 'cs.SI']","[arxiv.Result.Author('Abdul Shaikh'), arxiv.Result.Author('Saqib Ali'), arxiv.Result.Author('Ramla Al-Maamari')]","This paper presents the results of a systematic review of the literature on
the impact of social media in learning and teaching through bibliometric based
Citation analysis. The objective of the review was to map the evolution of the
current literature and identify the leading sources of knowledge in terms of
the most influential journals, authors, and articles. From a total of 50 top
most relevant articles selected from the Scopus database, a detailed citation
analysis was conducted. The study explored the overall theoretical foundation
of social media research involving in learning and studying and identified the
leading sources of knowledge in terms of and papers and revealed research
trends over the last four years by citation analysis. The analysis of citation
data showed that International Journal of Management Education is the leading
journal in social media in learning and teaching research. Author Abdullah Z
was found to be the leading author in this field in terms of a total number of
publications, total citations, and h index, while the most cited article was
authored by Baaran S. and by Bapitha L. The contribution of this study is to
clearly outline the current state of knowledge regarding social media in
learning and teaching services in the literature.",-0.030129407,0.36299524,-0.037589483,A
12043,"This result is partially different from the conclusions drawn by the CLEF-HIPE-2020
challenge [Ehrmann et al., 2020a], and warrants further study.","The CRF baselines appears to be more precise, while the neural network architecture
usually achieved better recall.","Finally, in Figure 16 we provide confusion matrices showing how predictions compare against the ground truth at
the entity type level (Person, Place, Organization).",2022-10-03 20:43:07+00:00,Unsilencing Colonial Archives via Automated Entity Recognition,cs.DL,['cs.DL'],"[arxiv.Result.Author('Mrinalini Luthra'), arxiv.Result.Author('Konstantin Todorov'), arxiv.Result.Author('Charles Jeurgens'), arxiv.Result.Author('Giovanni Colavizza')]","Colonial archives are at the center of increased interest from a variety of
perspectives, as they contain traces of historically marginalized people.
Unfortunately, like most archives, they remain difficult to access due to
significant persisting barriers. We focus here on one of them: the biases to be
found in historical findings aids, such as indexes of person names, which
remain in use to this day. In colonial archives, indexes can perpetuate
silences by omitting to include mentions of historically marginalized persons.
In order to overcome such limitations and pluralize the scope of existing
finding aids, we propose using automated entity recognition. To this end, we
contribute a fit-for-purpose annotation typology and apply it on the colonial
archive of the Dutch East India Company (VOC). We release a corpus of nearly
70,000 annotations as a shared task, for which we provide baselines using
state-of-the-art neural network models. Our work intends to stimulate further
contributions in the direction of broadening access to (colonial) archives,
integrating automation as a possible means to this end.",0.32173282,0.054739185,0.0946745,C
12640,"These notices can be searched for and manually resolved
later, and the record of the multiple matches can be used to further research and develop
automated means of finding notices with multiple title matches.","Second, if we find title matches to more than one
article, we simply record the multiple matches elsewhere for internal research purposes, but
make no further attempts to locate the article to which the notice refers (this is what happens in
the generic notices described above).","This combination of safeguards
has yielded encouraging results: out of a random sample of 100 notices identified through title
detection, 98 were correctly tied to the article they reference or the notice itself.",2022-10-18 03:03:30+00:00,Title detection: a novel approach to automatically finding retractions and other editorial notices in the scholarly literature,cs.DL,['cs.DL'],"[arxiv.Result.Author('Ashish Uppala'), arxiv.Result.Author('Domenic Rosati'), arxiv.Result.Author('Josh M. Nicholson'), arxiv.Result.Author('Milo Mordaunt'), arxiv.Result.Author('Peter Grabitz'), arxiv.Result.Author('Sean C. Rife')]","Despite being a key element in the process of disseminating scientific
knowledge, editorial notices are often obscured and not clearly linked to the
papers to which they refer. In the present paper, we describe established
methods of aggregating notice data, and introduce a novel method of finding
editorial notices in the scientific literature. Specifically, we describe how
article titles denote notices to existing publications, and how this
information can be used to tie notices to papers in an automated fashion.
Finally, as part of a broader movement to make science more transparent, we
make notices detected through this and other methods publicly available and
describe this dataset and how it can be accessed.",0.22418183,0.009637181,0.14273432,A
12651,"The extracted funding information poses
various possibilities and challenges for further research.","6 Conclusion

The analysis of the automatically extracted entities revealed diﬀerences and
distinct patterns in the distribution of acknowledged entities of diﬀerent types
between diﬀerent scientiﬁc domains.","Thus, grant numbers,
funding organizations and corporations might give an insight on the impact of
funding sponsorship on scientiﬁc research.",2022-10-18 09:50:47+00:00,A Comprehensive Analysis of Acknowledgement Texts in Web of Science: a case study on four scientific domains,cs.DL,"['cs.DL', 'cs.CL']","[arxiv.Result.Author('Nina Smirnova'), arxiv.Result.Author('Philipp Mayr')]","Analysis of acknowledgments is particularly interesting as acknowledgments
may give information not only about funding, but they are also able to reveal
hidden contributions to authorship and the researcher's collaboration patterns,
context in which research was conducted, and specific aspects of the academic
work. The focus of the present research is the analysis of a large sample of
acknowledgement texts indexed in the Web of Science (WoS) Core Collection.
Record types 'article' and 'review' from four different scientific domains,
namely social sciences, economics, oceanography and computer science, published
from 2014 to 2019 in a scientific journal in English were considered. Six types
of acknowledged entities, i.e., funding agency, grant number, individuals,
university, corporation and miscellaneous, were extracted from the
acknowledgement texts using a Named Entity Recognition (NER) tagger and
subsequently examined. A general analysis of the acknowledgement texts showed
that indexing of funding information in WoS is incomplete. The analysis of the
automatically extracted entities revealed differences and distinct patterns in
the distribution of acknowledged entities of different types between different
scientific domains. A strong association was found between acknowledged entity
and scientific domain and acknowledged entity and entity type. Only negligible
correlation was found between the number of citations and the number of
acknowledged entities. Generally, the number of words in the acknowledgement
texts positively correlates with the number of acknowledged funding
organizations, universities, individuals and miscellaneous entities. At the
same time, acknowledgement texts with the larger number of sentences have more
acknowledged individuals and miscellaneous categories.",-0.058001816,0.08577274,-0.097241536,A
12689,"Healthcare professionals are the first line of defense in a
pandemic, so they should be proficient enough to deal with raw data for further research in the
healthcare sector.","(Shwetha K 2017)
         Iqbal Singh tried to illustrate digital literacy skills among healthcare professionals at GGS
Medical College, Faridkot, Punjab.","The majority of respondents in this study, i.e., 84%, were aware of internet
applications like MS office.",2022-10-14 06:05:26+00:00,Digital Literacy and Reading Habits of the Central University of Tamil Nadu Students: A Survey Study,cs.DL,"['cs.DL', 'cs.CY', 'cs.IR']","[arxiv.Result.Author('Subaveerapandiyan A'), arxiv.Result.Author('Priyanka Sinha')]","The study attempted to understand the University students' digital reading
habits and their related skills. It also has a view of students' preferred
sources of reading, whether physical or digital resources. For this study, we
conducted a survey study with students and research scholars of the Central
University of Tamil Nadu, India. The instrument was a structured questionnaire
distributed with various modes. The result found that the majority of the
students are well known about digital tools and usage, most of the students are
excellent in digital literacy skills and other findings is however they are
good in digital literacy even though they like to read print books is their
most favorable preference. The results conclude that whatever technological
devices are developed and students have also grown their technical knowledge.
The result finds out, in education especially reading-wise, students or
readers' first wish is printed resources; digital books are secondary to them.",-0.23244753,0.05164287,0.086661555,B
13083,"Currently, further research is necessary to identify what and how such changes could effectively be implemented to
address questionable publishing practices.","Consequently, much broader systematic changes are required to address the
research and publishing culture that motivates researchers to use these venues to reduce the publication rate of poor
quality research and to shift research of acceptable quality to reputable venues that will not cast doubt on its validity.","Limitations

This study has several strengths.",2022-10-27 11:54:09+00:00,Medical articles in questionable journals are less impactful than those in non-questionable journals but still extensively cited,cs.DL,['cs.DL'],[arxiv.Result.Author('Dimity Stephen')],"A key feature of questionable journals is a lack of adequate peer review of
their articles. Content of thus unknown quality may be utilised by unsuspecting
practitioners or incorporated into peer-reviewed research, becoming
legitimised. It is therefore necessary to examine the citation patterns of
articles in questionable journals to understand the impact and reach of
research in questionable journals. Similar research has tended to focus on
authors from low- and middle-income countries. As such, this study investigates
the profile and impact of research in questionable journals by authors in
Germany. Questionable journals were identified by matching journals with
articles by authors at German institutions from Dimensions to Cabell's
Predatory Reports. Metadata for these articles and a comparative sample of
articles in non-questionable journals were extracted from Dimensions and the 3
year citations, self-citations, uncited rate, profile of co-authoring and
citing countries, and institution type of authors were compared between groups.
Nearly 600 articles in 88 questionable journals were published by German
authors in 2010-2020. Three-quarters were in the medical and health sciences.
Medical articles in questionable journals received significantly fewer
citations than similar articles in non-questionable journals. However, articles
in questionable journals were still extensively cited in 1,736 primarily
non-questionable journals. Self-citations accounted for only 12% of these
citations. Authors from non-university medical facilities were over-represented
in articles in questionable journals. System-level changes are necessary to
eliminate questionable journals and shift high-quality research into reputable
venues.",-0.23212579,0.0254473,-0.07003692,B
13718,"That fewer than half of participants
indicated they had reused a PID for people was therefore a concerning finding, and one that should be the
subject of further research.","Though several PID approaches exists in this context, the research instrument was clearly
alluding to ORCID, with which most participants would be to be familiar.","It is possible that the description of 'PIDs for people' within the research instrument
was too opaque for some participants to make the connection with ORCID, resulting in under reporting.",2022-11-14 14:02:52+00:00,Exploring the concept of PID literacy: user perceptions and understanding of persistent identifiers in support of open scholarly infrastructure,cs.DL,"['cs.DL', 'E.1; H.3']","[arxiv.Result.Author('George Macgregor'), arxiv.Result.Author('Barbara S. Lancho-Barrantes'), arxiv.Result.Author('Diane Rasmussen Pennington')]","The increasing centrality of persistent identifiers (PIDs) to scholarly
ecosystems and the contribution they can make to the burgeoning 'PID graph' has
the potential to transform scholarship. Despite their importance as originators
of PID data, little is known about researchers' awareness and understanding of
PIDs, or their efficacy in using them. In this article we report on the results
of an online interactive test designed to elicit exploratory data about
researcher awareness and understanding of PIDs. This instrument was designed to
explore recognition of PIDs and the extent to which researchers correctly apply
PIDs within digital scholarly ecosystems, as well as measure researchers'
perceptions of PIDs. Our results reveal irregular patterns of PID understanding
and certainty across all participants, though statistically significant
disciplinary and academic job role differences were observed in some instances.
Uncertainty and confusion were found to exist in relation to dominant schemes
such as ORCID and DOIs, even when contextualized within real-world examples. We
also show researchers' perceptions of PIDs to be generally positive but that
disciplinary differences can be noted, as well as higher levels of aversion to
PIDs in specific use cases and negative perceptions where PIDs are measured on
an 'activity' semantic dimension. This work therefore contributes to our
understanding of academics' 'PID literacy' and should inform those designing
PID-centric scholarly infrastructures, that a significant need for training and
outreach to active researchers remains necessary.",-0.21875907,-0.110081814,0.042991217,B
13719,"Future research and limitations

Our research suffers from several limitations, which also stimulate further research.","However, with no
widely accepted understanding of why PIDs exist and why they are necessary, it seems apparent that
unsatisfactory and erratic (re)use of PIDs will always occur among academic users.","Firstly, not all possible
variables were tested or discussed in this work, due to the limits of space.",2022-11-14 14:02:52+00:00,Exploring the concept of PID literacy: user perceptions and understanding of persistent identifiers in support of open scholarly infrastructure,cs.DL,"['cs.DL', 'E.1; H.3']","[arxiv.Result.Author('George Macgregor'), arxiv.Result.Author('Barbara S. Lancho-Barrantes'), arxiv.Result.Author('Diane Rasmussen Pennington')]","The increasing centrality of persistent identifiers (PIDs) to scholarly
ecosystems and the contribution they can make to the burgeoning 'PID graph' has
the potential to transform scholarship. Despite their importance as originators
of PID data, little is known about researchers' awareness and understanding of
PIDs, or their efficacy in using them. In this article we report on the results
of an online interactive test designed to elicit exploratory data about
researcher awareness and understanding of PIDs. This instrument was designed to
explore recognition of PIDs and the extent to which researchers correctly apply
PIDs within digital scholarly ecosystems, as well as measure researchers'
perceptions of PIDs. Our results reveal irregular patterns of PID understanding
and certainty across all participants, though statistically significant
disciplinary and academic job role differences were observed in some instances.
Uncertainty and confusion were found to exist in relation to dominant schemes
such as ORCID and DOIs, even when contextualized within real-world examples. We
also show researchers' perceptions of PIDs to be generally positive but that
disciplinary differences can be noted, as well as higher levels of aversion to
PIDs in specific use cases and negative perceptions where PIDs are measured on
an 'activity' semantic dimension. This work therefore contributes to our
understanding of academics' 'PID literacy' and should inform those designing
PID-centric scholarly infrastructures, that a significant need for training and
outreach to active researchers remains necessary.",-0.11538552,-0.15819147,0.12395097,B
13720,"There is
consequently a need for further research to address this weakness, ideally incorporating mixed methods.","The instrument was therefore satisfactory at surfacing perception data, as well as identifying differences
between groups, but less satisfactory at understanding why these perceptions or differences existed.","Such
further work might use a smaller cohort of participants by studying users within a controlled task-based setting
perhaps using, for example, protocol analysis (or 'think aloud') (Ericsson & Simon, 1993) or stimulated recall
user study approaches (Lazar et al., 2014), thereby generating rich qualitative data which could be mined for
insight.",2022-11-14 14:02:52+00:00,Exploring the concept of PID literacy: user perceptions and understanding of persistent identifiers in support of open scholarly infrastructure,cs.DL,"['cs.DL', 'E.1; H.3']","[arxiv.Result.Author('George Macgregor'), arxiv.Result.Author('Barbara S. Lancho-Barrantes'), arxiv.Result.Author('Diane Rasmussen Pennington')]","The increasing centrality of persistent identifiers (PIDs) to scholarly
ecosystems and the contribution they can make to the burgeoning 'PID graph' has
the potential to transform scholarship. Despite their importance as originators
of PID data, little is known about researchers' awareness and understanding of
PIDs, or their efficacy in using them. In this article we report on the results
of an online interactive test designed to elicit exploratory data about
researcher awareness and understanding of PIDs. This instrument was designed to
explore recognition of PIDs and the extent to which researchers correctly apply
PIDs within digital scholarly ecosystems, as well as measure researchers'
perceptions of PIDs. Our results reveal irregular patterns of PID understanding
and certainty across all participants, though statistically significant
disciplinary and academic job role differences were observed in some instances.
Uncertainty and confusion were found to exist in relation to dominant schemes
such as ORCID and DOIs, even when contextualized within real-world examples. We
also show researchers' perceptions of PIDs to be generally positive but that
disciplinary differences can be noted, as well as higher levels of aversion to
PIDs in specific use cases and negative perceptions where PIDs are measured on
an 'activity' semantic dimension. This work therefore contributes to our
understanding of academics' 'PID literacy' and should inform those designing
PID-centric scholarly infrastructures, that a significant need for training and
outreach to active researchers remains necessary.",-0.1763686,0.029677734,0.18695828,B
14134,"further research, we found that APA guidelines do allow ANOVA
This is a reduction of about 33.8%.","However, upon
unique rules out of the total 1, 426 rules were included by others.",A distribution of the amount        to be reported without an 𝑟 -value.,2022-11-24 14:29:20+00:00,Reducing a Set of Regular Expressions and Analyzing Differences of Domain-specific Statistic Reporting,cs.DL,['cs.DL'],"[arxiv.Result.Author('Tobias Kalmbach'), arxiv.Result.Author('Marcel Hoffmann'), arxiv.Result.Author('Nicolas Lell'), arxiv.Result.Author('Ansgar Scherp')]","Due to the large amount of daily scientific publications, it is impossible to
manually review each one. Therefore, an automatic extraction of key information
is desirable. In this paper, we examine STEREO, a tool for extracting
statistics from scientific papers using regular expressions. By adapting an
existing regular expression inclusion algorithm for our use case, we decrease
the number of regular expressions used in STEREO by about $33.8\%$. We reveal
common patterns from the condensed rule set that can be used for the creation
of new rules. We also apply STEREO, which was previously trained in the
life-sciences and medical domain, to a new scientific domain, namely
Human-Computer-Interaction (HCI), and re-evaluate it. According to our
research, statistics in the HCI domain are similar to those in the medical
domain, although a higher percentage of APA-conform statistics were found in
the HCI domain. Additionally, we compare extraction on PDF and LaTeX source
files, finding LaTeX to be more reliable for extraction.",-0.15426335,-0.2613992,0.26086766,B
14135,This section presents some further research conducted.,"Here, “significantly higher”                  B DEEPER ANALYSIS
can be followed by some other letters and then the 𝑝-value in
parentheses afterwards.","A.3 Use Optional Sequences                                                  B.1 Inclusion Comparison by Rule ID

We already mentioned some of the following patterns which use               In Figure 7, we show the amount of rules removed, grouped by
optional sequences:                                                         rule ID.",2022-11-24 14:29:20+00:00,Reducing a Set of Regular Expressions and Analyzing Differences of Domain-specific Statistic Reporting,cs.DL,['cs.DL'],"[arxiv.Result.Author('Tobias Kalmbach'), arxiv.Result.Author('Marcel Hoffmann'), arxiv.Result.Author('Nicolas Lell'), arxiv.Result.Author('Ansgar Scherp')]","Due to the large amount of daily scientific publications, it is impossible to
manually review each one. Therefore, an automatic extraction of key information
is desirable. In this paper, we examine STEREO, a tool for extracting
statistics from scientific papers using regular expressions. By adapting an
existing regular expression inclusion algorithm for our use case, we decrease
the number of regular expressions used in STEREO by about $33.8\%$. We reveal
common patterns from the condensed rule set that can be used for the creation
of new rules. We also apply STEREO, which was previously trained in the
life-sciences and medical domain, to a new scientific domain, namely
Human-Computer-Interaction (HCI), and re-evaluate it. According to our
research, statistics in the HCI domain are similar to those in the medical
domain, although a higher percentage of APA-conform statistics were found in
the HCI domain. Additionally, we compare extraction on PDF and LaTeX source
files, finding LaTeX to be more reliable for extraction.",0.38930312,-0.2326268,0.23283929,C
15003,"□
                                                                                                                         32

Conclusions and suggestions for further research

In this article, we defined natural properties for a measure m to be
considered an impact measure.","In any case Y ≤ Z
while AY > AZ, contradicting (II).","Essentially these requirements are:

(a) m must have distinguishing power on the left-hand side of the graph
of the functions Z in Z (the sources with the most impact) for which it is
assumed to measure impact.",2022-12-15 14:27:45+00:00,Impact in informetrics and beyond,cs.DL,['cs.DL'],"[arxiv.Result.Author('Leo Egghe'), arxiv.Result.Author('Ronald Rousseau')]","The concept of impact is one of the most important concepts in informetrics.
It is here studied mathematically. We first fix a topic for which we want to
find influential objects such as authors or journals, and their production,
such as publications generating citations. These objects are then said to have
a certain degree of impact. We work on three levels. On the first level, we
need a measure for these objects, represented by their rank-frequency function,
describing the number of items per source (ranked in decreasing order of the
number of items): an impact measure. These measures focus on the production of
the most productive sources. The h-index is one example. In paper II we study a
formal definition of impact measures based on these left-hand sides of the
source-item rank-frequency functions representing these objects. The second
level of impact investigation is using impact bundles (or sheaves) as in paper
III. As an illustration, we mention that the h-index of a function Z is defined
as x for which Z(x) = x, i.e., the abscissa of the intersection of the graph of
Z with the line y = x. The h-bundle is defined in the same way but now the line
y = x is replaced by an increasing line through the origin: y = $\theta$.x,
$\theta$ > 0. So, we have a bundle of impact measures which is more powerful to
measure the impact of an object Z. Impact bundles are characterized in paper
III. A third level of impact investigations involves the non-normalized form of
the Lorenz curve. In papers IV and V we study global impact measures as
measures that respect the non-normalized Lorenz order between the
rank-frequency functions representing objects Z. We say that object Z has more
impact than object Y if Y is smaller than Z in the sense of the non-normalized
Lorenz order. This is the highest level of impact treatment: a mathematical
definition of the concept itself.",0.08209951,-0.08370506,0.5356616,B
15004,"We suggest as a topic for
further research an investigation of the equivalence (or not) of well-
known measures such as the h, g, and the R index.","In
this case, m and n cannot be considered to be “equivalent” (in the sense
of not acting in the same way on all functions).",The second classification uses (IV).,2022-12-15 14:27:45+00:00,Impact in informetrics and beyond,cs.DL,['cs.DL'],"[arxiv.Result.Author('Leo Egghe'), arxiv.Result.Author('Ronald Rousseau')]","The concept of impact is one of the most important concepts in informetrics.
It is here studied mathematically. We first fix a topic for which we want to
find influential objects such as authors or journals, and their production,
such as publications generating citations. These objects are then said to have
a certain degree of impact. We work on three levels. On the first level, we
need a measure for these objects, represented by their rank-frequency function,
describing the number of items per source (ranked in decreasing order of the
number of items): an impact measure. These measures focus on the production of
the most productive sources. The h-index is one example. In paper II we study a
formal definition of impact measures based on these left-hand sides of the
source-item rank-frequency functions representing these objects. The second
level of impact investigation is using impact bundles (or sheaves) as in paper
III. As an illustration, we mention that the h-index of a function Z is defined
as x for which Z(x) = x, i.e., the abscissa of the intersection of the graph of
Z with the line y = x. The h-bundle is defined in the same way but now the line
y = x is replaced by an increasing line through the origin: y = $\theta$.x,
$\theta$ > 0. So, we have a bundle of impact measures which is more powerful to
measure the impact of an object Z. Impact bundles are characterized in paper
III. A third level of impact investigations involves the non-normalized form of
the Lorenz curve. In papers IV and V we study global impact measures as
measures that respect the non-normalized Lorenz order between the
rank-frequency functions representing objects Z. We say that object Z has more
impact than object Y if Y is smaller than Z in the sense of the non-normalized
Lorenz order. This is the highest level of impact treatment: a mathematical
definition of the concept itself.",0.21310008,-0.15925705,0.57279956,C
