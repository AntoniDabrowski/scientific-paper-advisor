Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
4,"299–310,
from a different distribution to the training data, using domain                    2012.
adversarial training [27], [28] would help to uncover domain
invariant features in the generated samples, and is therefore an              [10] N. Chen, A. Klushyn, R. Kurle, X. Jiang, J. Bayer, and P. Smagt,
interesting avenue for further research.","4, pp.","We have also assumed                       “Metrics for deep generative models,” in International Conference on
that generated samples strictly inherit the label from an origi-                    Artiﬁcial Intelligence and Statistics.",2021-12-31 20:35:46+00:00,Evaluating Deep Music Generation Methods Using Data Augmentation,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Toby Godwin'), arxiv.Result.Author('Georgios Rizos'), arxiv.Result.Author('Alice Baird'), arxiv.Result.Author('Najla D. Al Futaisi'), arxiv.Result.Author('Vincent Brisse'), arxiv.Result.Author('Bjoern W. Schuller')]","Despite advances in deep algorithmic music generation, evaluation of
generated samples often relies on human evaluation, which is subjective and
costly. We focus on designing a homogeneous, objective framework for evaluating
samples of algorithmically generated music. Any engineered measures to evaluate
generated music typically attempt to define the samples' musicality, but do not
capture qualities of music such as theme or mood. We do not seek to assess the
musical merit of generated music, but instead explore whether generated samples
contain meaningful information pertaining to emotion or mood/theme. We achieve
this by measuring the change in predictive performance of a music mood/theme
classifier after augmenting its training data with generated samples. We
analyse music samples generated by three models -- SampleRNN, Jukebox, and DDSP
-- and employ a homogeneous framework across all methods to allow for objective
comparison. This is the first attempt at augmenting a music genre
classification dataset with conditionally generated music. We investigate the
classification performance improvement using deep music generation and the
ability of the generators to make emotional music by using an additional,
emotion annotation of the dataset. Finally, we use a classifier trained on real
data to evaluate the label validity of class-conditionally generated samples.",-0.012943648,0.076899454,-0.14067216,A
282,"Finally,
further study into employing capsule networks in speaker and speech recognition with proven results is required
because capsule networks are currently in development.","In addition, we plan to study speaker
identification performance in emotional environments using different input features, e.g., spectrograms.","Compliance with Ethical Standards

   The authors thank the University of Sharjah for supporting this work through the Machine Learning and Arabic
Language Processing research group.",2022-01-09 12:37:51+00:00,Emotional Speaker Identification using a Novel Capsule Nets Model,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Ali Bou Nassif'), arxiv.Result.Author('Ismail Shahin'), arxiv.Result.Author('Ashraf Elnagar'), arxiv.Result.Author('Divya Velayudhan'), arxiv.Result.Author('Adi Alhudhaif'), arxiv.Result.Author('Kemal Polat')]","Speaker recognition systems are widely used in various applications to
identify a person by their voice; however, the high degree of variability in
speech signals makes this a challenging task. Dealing with emotional variations
is very difficult because emotions alter the voice characteristics of a person;
thus, the acoustic features differ from those used to train models in a neutral
environment. Therefore, speaker recognition models trained on neutral speech
fail to correctly identify speakers under emotional stress. Although
considerable advancements in speaker identification have been made using
convolutional neural networks (CNN), CNNs cannot exploit the spatial
association between low-level features. Inspired by the recent introduction of
capsule networks (CapsNets), which are based on deep learning to overcome the
inadequacy of CNNs in preserving the pose relationship between low-level
features with their pooling technique, this study investigates the performance
of using CapsNets in identifying speakers from emotional speech recordings. A
CapsNet-based speaker identification model is proposed and evaluated using
three distinct speech databases, i.e., the Emirati Speech Database, SUSAS
Dataset, and RAVDESS (open-access). The proposed model is also compared to
baseline systems. Experimental results demonstrate that the novel proposed
CapsNet model trains faster and provides better results over current
state-of-the-art schemes. The effect of the routing algorithm on speaker
identification performance was also studied by varying the number of
iterations, both with and without a decoder network.",0.2498576,0.078210875,-0.16860077,B
389,"We further study the use of perceptual
                                                                  losses in EVC training to improve the intelligibility of the
                                                                  converted emotion.","Inspired by the successful  into the emotional speech synthesis systems, which moti-
attempts in prosody style control, several studies control        vates our study.","4

2.5 Research Gap (Summary)                                         Source            Linguistic Transplant
                                                                   Speech                                     Linguistic
Below, we summarise the gaps in the literature of emotional        (Neutral)
voice conversion that we aim to address in this paper.",2022-01-10 02:11:25+00:00,Emotion Intensity and its Control for Emotional Voice Conversion,cs.SD,"['cs.SD', 'cs.CL', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Kun Zhou'), arxiv.Result.Author('Berrak Sisman'), arxiv.Result.Author('Rajib Rana'), arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Haizhou Li')]","Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.",0.51994497,-0.12687336,-0.21964656,B_centroid
390,"We further study the use of perceptual
                                                                  losses in EVC training to improve the intelligibility of the
                                                                  converted emotion.","Inspired by the successful  into the emotional speech synthesis systems, which moti-
attempts in prosody style control, several studies control        vates our study.","4

2.5 Research Gap (Summary)                                         Source            Linguistic Transplant
                                                                   Speech                                     Linguistic
Below, we summarise the gaps in the literature of emotional        (Neutral)
voice conversion that we aim to address in this paper.",2022-01-10 02:11:25+00:00,Emotion Intensity and its Control for Emotional Voice Conversion,cs.SD,"['cs.SD', 'cs.CL', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Kun Zhou'), arxiv.Result.Author('Berrak Sisman'), arxiv.Result.Author('Rajib Rana'), arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Haizhou Li')]","Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.",0.51994497,-0.12687336,-0.21964656,B
763,"Results                                                                                                              can open up a number of possibilities for further research in the
                                                                                                                          areas of SVS.","Opencpop
3.3.","Following [37], evaluation metrics, i.e., F0 Root Mean Square
Error (F0-RMSE), F0 Pearson Correlation Coefﬁcient (F0-PCC),                                                                            5.",2022-01-19 06:12:47+00:00,Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis,cs.SD,"['cs.SD', 'cs.DB', 'eess.AS']","[arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Xinsheng Wang'), arxiv.Result.Author('Pengcheng Zhu'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Hanzhao Li'), arxiv.Result.Author('Heyang Xue'), arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Mengxiao Bi')]","This paper introduces Opencpop, a publicly available high-quality Mandarin
singing corpus designed for singing voice synthesis (SVS). The corpus consists
of 100 popular Mandarin songs performed by a female professional singer. Audio
files are recorded with studio quality at a sampling rate of 44,100 Hz and the
corresponding lyrics and musical scores are provided. All singing recordings
have been phonetically annotated with phoneme boundaries and syllable (note)
boundaries. To demonstrate the reliability of the released data and to provide
a baseline for future research, we built baseline deep neural network-based SVS
models and evaluated them with both objective metrics and subjective mean
opinion score (MOS) measure. Experimental results show that the best SVS model
trained on our database achieves 3.70 MOS, indicating the reliability of the
provided corpus. Opencpop is released to the open-source community WeNet, and
the corpus, as well as synthesized demos, can be found on the project homepage.",-0.29336974,-0.38298398,0.15420377,A
764,"Results                                                                                                              can open up a number of possibilities for further research in the
                                                                                                                          areas of SVS.","Opencpop
3.3.","Following [37], evaluation metrics, i.e., F0 Root Mean Square
Error (F0-RMSE), F0 Pearson Correlation Coefﬁcient (F0-PCC),                                                                            5.",2022-01-19 06:12:47+00:00,Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis,cs.SD,"['cs.SD', 'cs.DB', 'eess.AS']","[arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Xinsheng Wang'), arxiv.Result.Author('Pengcheng Zhu'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Hanzhao Li'), arxiv.Result.Author('Heyang Xue'), arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Mengxiao Bi')]","This paper introduces Opencpop, a publicly available high-quality Mandarin
singing corpus designed for singing voice synthesis (SVS). The corpus consists
of 100 popular Mandarin songs performed by a female professional singer. Audio
files are recorded with studio quality at a sampling rate of 44,100 Hz and the
corresponding lyrics and musical scores are provided. All singing recordings
have been phonetically annotated with phoneme boundaries and syllable (note)
boundaries. To demonstrate the reliability of the released data and to provide
a baseline for future research, we built baseline deep neural network-based SVS
models and evaluated them with both objective metrics and subjective mean
opinion score (MOS) measure. Experimental results show that the best SVS model
trained on our database achieves 3.70 MOS, indicating the reliability of the
provided corpus. Opencpop is released to the open-source community WeNet, and
the corpus, as well as synthesized demos, can be found on the project homepage.",-0.29336974,-0.38298398,0.15420377,A
850,This work paved the way for further research in the area of genre classiﬁcation.,"The authors deduced three essential features
for musical content, namely timbral texture, rhythm, and pitch content for Western music in various styles, including
classical, jazz, pop and rock.","Either
whole recordings or homogeneous sections within them were used, and a classiﬁcation accuracy of 61% was achieved
for ten genres, using statistical pattern recognition classiﬁers.",2022-01-20 20:48:07+00:00,"Kinit Classification in Ethiopian Chants, Azmaris and Modern Music: A New Dataset and CNN Benchmark",cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Ephrem A. Retta'), arxiv.Result.Author('Richard Sutcliffe'), arxiv.Result.Author('Eiad Almekhlafi'), arxiv.Result.Author('Yosef K. Enku'), arxiv.Result.Author('Eyob Alemu'), arxiv.Result.Author('Tigist D. Gemechu'), arxiv.Result.Author('Michael A. Berwo'), arxiv.Result.Author('Mustafa Mhamed'), arxiv.Result.Author('Jun Feng')]","In this paper, we create EMIR, the first-ever Music Information Retrieval
dataset for Ethiopian music. EMIR is freely available for research purposes and
contains 600 sample recordings of Orthodox Tewahedo chants, traditional Azmari
songs and contemporary Ethiopian secular music. Each sample is classified by
five expert judges into one of four well-known Ethiopian Kinits, Tizita, Bati,
Ambassel and Anchihoye. Each Kinit uses its own pentatonic scale and also has
its own stylistic characteristics. Thus, Kinit classification needs to combine
scale identification with genre recognition. After describing the dataset, we
present the Ethio Kinits Model (EKM), based on VGG, for classifying the EMIR
clips. In Experiment 1, we investigated whether Filterbank, Mel-spectrogram,
Chroma, or Mel-frequency Cepstral coefficient (MFCC) features work best for
Kinit classification using EKM. MFCC was found to be superior and was therefore
adopted for Experiment 2, where the performance of EKM models using MFCC was
compared using three different audio sample lengths. 3s length gave the best
results. In Experiment 3, EKM and four existing models were compared on the
EMIR dataset: AlexNet, ResNet50, VGG16 and LSTM. EKM was found to have the best
accuracy (95.00%) as well as the fastest training time. We hope this work will
encourage others to explore Ethiopian music and to experiment with other models
for Kinit classification.",0.07942037,0.16299415,0.5194367,C
1660,"ever, further study is necessary to determine which of these
                                       techniques or technique combinations provide the optimal so-         Xception models have also been used for AED, such as
                                       lution.",How-      the ESC-50 leaderboard at the time of its publication.,"This paper analyzes the effects of knowledge transfer,   in [7, 16], as the model provides high performance with rela-
                                       data augmentation, and pretraining on a popular small dataset,   tively few parameters compared to other top-performing mod-
                                       ESC-50 [3].",2022-02-07 20:57:40+00:00,"Maximizing Audio Event Detection Model Performance on Small Datasets Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation Study",cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Daniel Tompkins'), arxiv.Result.Author('Kshitiz Kumar'), arxiv.Result.Author('Jian Wu')]","An Xception model reaches state-of-the-art (SOTA) accuracy on the ESC-50
dataset for audio event detection through knowledge transfer from ImageNet
weights, pretraining on AudioSet, and an on-the-fly data augmentation pipeline.
This paper presents an ablation study that analyzes which components contribute
to the boost in performance and training time. A smaller Xception model is also
presented which nears SOTA performance with almost a third of the parameters.",-0.1845313,-0.19703594,-0.13596556,A
2020,"This ﬁrst study sets the basis
                                           For most applications using time-frequency representations,                 and intuition for further research on this idea.","Introduction                                   of-concept examples in a direct comparison to (magnitude)
                                                                                                                       scattering based on the STFT.","only the magnitude of the obtained complex coeﬃcients
                                        is considered as, e.g., in the cases of spectrogram and                           The content of this paper was mainly developed in the Mas-
                                        scalogram [1].",2022-02-15 14:51:58+00:00,Phase-Based Signal Representations for Scattering,cs.SD,"['cs.SD', 'eess.AS', 'eess.SP']","[arxiv.Result.Author('Daniel Haider'), arxiv.Result.Author('Peter Balazs'), arxiv.Result.Author('Nicki Holighaus')]","The scattering transform is a non-linear signal representation method based
on cascaded wavelet transform magnitudes. In this paper we introduce phase
scattering, a novel approach where we use phase derivatives in a scattering
procedure. We first revisit phase-related concepts for representing
time-frequency information of audio signals, in particular, the partial
derivatives of the phase in the time-frequency domain. By putting analytical
and numerical results in a new light, we set the basis to extend the
phase-based representations to higher orders by means of a scattering
transform, which leads to well localized signal representations of large-scale
structures. All the ideas are introduced in a general way and then applied
using the STFT.",-0.23179871,0.08983968,0.21473414,C
2043,"We setup baselines for monolingual and zero-shot
                                        isting work has focused on detecting abusive behaviour in        cross-lingual setting for encouraging further research in this
                                        textual data [1–6].",Majority of the ex-       detection.,Abusive content detection on images and      direction.,2022-02-16 11:09:50+00:00,ADIMA: Abuse Detection In Multilingual Audio,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Vikram Gupta'), arxiv.Result.Author('Rini Sharon'), arxiv.Result.Author('Ramit Sawhney'), arxiv.Result.Author('Debdoot Mukherjee')]","Abusive content detection in spoken text can be addressed by performing
Automatic Speech Recognition (ASR) and leveraging advancements in natural
language processing. However, ASR models introduce latency and often perform
sub-optimally for profane words as they are underrepresented in training
corpora and not spoken clearly or completely. Exploration of this problem
entirely in the audio domain has largely been limited by the lack of audio
datasets. Building on these challenges, we propose ADIMA, a novel,
linguistically diverse, ethically sourced, expert annotated and well-balanced
multilingual profanity detection audio dataset comprising of 11,775 audio
samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique
users. Through quantitative experiments across monolingual and cross-lingual
zero-shot settings, we take the first step in democratizing audio based content
moderation in Indic languages and set forth our dataset to pave future work.",0.20253739,-0.20770325,-0.04993992,B
3156,"This is of particular importance for a large-scale deployment required for environmental
monitoring, where a model will have to perform equally well across several different locations, and, correspondingly,
needs to be addressed via further research.","It constitutes a major challenge for
real world applications as it makes model selection and, more crucially, model testing harder, leading to catastrophic
failures during deployment.","For audio analysis in particular, the notion of generalisation is closely linked to that of robustness to different
perturbations, a topic that has received considerable attention over the years.",2022-03-10 13:32:31+00:00,Climate Change & Computer Audition: A Call to Action and Overview on Audio Intelligence to Help Save the Planet,cs.SD,"['cs.SD', 'cs.LG']","[arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Alican Akman'), arxiv.Result.Author('Yi Chang'), arxiv.Result.Author('Harry Coppock'), arxiv.Result.Author('Alexander Gebhard'), arxiv.Result.Author('Alexander Kathan'), arxiv.Result.Author('Esther Rituerto-González'), arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Florian B. Pokorny')]","Among the seventeen Sustainable Development Goals (SDGs) proposed within the
2030 Agenda and adopted by all the United Nations member states, the 13$^{th}$
SDG is a call for action to combat climate change for a better world. In this
work, we provide an overview of areas in which audio intelligence -- a powerful
but in this context so far hardly considered technology -- can contribute to
overcome climate-related challenges. We categorise potential computer audition
applications according to the five elements of earth, water, air, fire, and
aether, proposed by the ancient Greeks in their five element theory; this
categorisation serves as a framework to discuss computer audition in relation
to different ecological aspects. Earth and water are concerned with the early
detection of environmental changes and, thus, with the protection of humans and
animals, as well as the monitoring of land and aquatic organisms. Aerial audio
is used to monitor and obtain information about bird and insect populations.
Furthermore, acoustic measures can deliver relevant information for the
monitoring and forecasting of weather and other meteorological phenomena. The
fourth considered element is fire. Due to the burning of fossil fuels, the
resulting increase in CO$_2$ emissions and the associated rise in temperature,
fire is used as a symbol for man-made climate change and in this context
includes the monitoring of noise pollution, machines, as well as the early
detection of wildfires. In all these areas, computer audition can help
counteract climate change. Aether then corresponds to the technology itself
that makes this possible. This work explores these areas and discusses
potential applications, while positioning computer audition in relation to
methodological alternatives.",-0.08872,0.19282655,0.18471625,C
3199,"6:513. doi: 10.3389/fpsyg.2015.00513
challenges, which the researchers hope the future researchers
would be able to resolve with further study.",Psychol.,"Accuracy of 0.77               [15] Mukherjee, Samarpita & Mukherjee, Roan.",2022-03-13 06:12:27+00:00,Bi-Sampling Approach to Classify Music Mood leveraging Raga-Rasa Association in Indian Classical Music,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']","[arxiv.Result.Author('Mohan Rao B C'), arxiv.Result.Author('Vinayak Arkachaari'), arxiv.Result.Author('Harsha M N'), arxiv.Result.Author('Sushmitha M N'), arxiv.Result.Author('Gayathri Ramesh K K'), arxiv.Result.Author('Ullas M S'), arxiv.Result.Author('Pathi Mohan Rao'), arxiv.Result.Author('Sudha G'), arxiv.Result.Author('Narayana Darapaneni')]","The impact of Music on the mood or emotion of the listener is a
well-researched area in human psychology and behavioral science. In Indian
classical music, ragas are the melodic structure that defines the various
styles and forms of the music. Each raga has been found to evoke a specific
emotion in the listener. With the advent of advanced capabilities of audio
signal processing and the application of machine learning, the demand for
intelligent music classifiers and recommenders has received increased
attention, especially in the 'Music as a service' cloud applications. This
paper explores a novel framework to leverage the raga-rasa association in
Indian classical Music to build an intelligent classifier and its application
in music recommendation system based on user's current mood and the mood they
aspire to be in.",0.022054117,-0.39417914,0.2229209,A
3377,"Although the performance improved by approximately           teacher model generated soft labels for the student model with
2.5% by applying ResNorm, further research is required to          a longer audio duration (i.e., 20 s or 30 s consisting of multiple
maintain the scene classiﬁcation performance in unseen de-         audio segments).","Furthermore, the system
vice generalization [43], the accuracy outperformed other sys-     was trained with a knowledge distillation strategy, in which the
tems.","The longer the recording time, the more var-
vices.",2022-03-16 07:26:12+00:00,Instance-level loss based multiple-instance learning framework for acoustic scene classification,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Won-Gook Choi'), arxiv.Result.Author('Joon-Hyuk Chang'), arxiv.Result.Author('Jae-Mo Yang'), arxiv.Result.Author('Han-Gil Moon')]","In the acoustic scene classification (ASC) task, an acoustic scene consists
of diverse sounds and is inferred by identifying combinations of distinct
attributes among them. This study aims to extract and cluster these attributes
effectively using an improved multiple-instance learning (MIL) framework for
ASC. MIL, known as a weakly supervised learning method, is a strategy for
extracting an instance from a bundle of frames composing an input audio clip
and inferring a scene corresponding to the input data using these unlabeled
instances. However, many studies pointed out an underestimation problem of MIL.
In this study, we develop a MIL framework more suitable for ASC systems by
defining instance-level labels and loss to extract and cluster instances
effectively. Furthermore, we design a fully separated convolutional module,
which is a lightweight neural network comprising pointwise, frequency-sided
depthwise, and temporal-sided depthwise convolutional filters. As a result,
compared to vanilla MIL, the confidence and proportion of positive instances
increase significantly, overcoming the underestimation problem and improving
the classification accuracy up to 11%. The proposed system achieved a
performance of 81.1% and 72.3% on the TAU urban acoustic scenes 2019 and 2020
mobile datasets with 139 K parameters, respectively. Especially, it achieves
the highest performance among the systems having under the 1 M parameters on
the TAU urban acoustic scenes 2019 dataset.",0.015824877,0.05800463,-0.022288937,C
3380,"This provides evidence that audio representations learned by isotropic, all-MLP
models require further research.","At the very least, we believe it is a very interest-
ing outcome that the embeddings learned by an extremely small model, on a rather small
dataset, can outperform much larger models pretrained on larger audio corpuses on a se-
cret task.","In our ablation experiments (Section 6.3) we obtain improved results for various tasks,
including better than ﬁrst place results on Gunshot Triangulation (Cooper and Shaw, 2020)
and Mridingham Stroke and Tonic (Anantapadmanabhan et al., 2020).",2022-03-16 09:33:36+00:00,Learning Audio Representations with MLPs,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Mashrur M. Morshed'), arxiv.Result.Author('Ahmad Omar Ahsan'), arxiv.Result.Author('Hasan Mahmud'), arxiv.Result.Author('Md. Kamrul Hasan')]","In this paper, we propose an efficient MLP-based approach for learning audio
representations, namely timestamp and scene-level audio embeddings. We use an
encoder consisting of sequentially stacked gated MLP blocks, which accept 2D
MFCCs as inputs. In addition, we also provide a simple temporal
interpolation-based algorithm for computing scene-level embeddings from
timestamp embeddings. The audio representations generated by our method are
evaluated across a diverse set of benchmarks at the Holistic Evaluation of
Audio Representations (HEAR) challenge, hosted at the NeurIPS 2021 competition
track. We achieved first place on the Speech Commands (full), Speech Commands
(5 hours), and the Mridingham Tonic benchmarks. Furthermore, our approach is
also the most resource-efficient among all the submitted methods, in terms of
both the number of model parameters and the time required to compute
embeddings.",0.18295592,0.18037508,0.03970089,C
3466,"Going a step further, emotional alterations (either
temporary or persistent) leave also correlates in the speech and phonation signature, and may be subjects
of further study by acoustic analysis [9, 10]

    The relationship between acoustic correlates and voice pathology has been clinically established in the
last decades, subjectively and quantitatively [11, 12].","Some references on the inﬂu-
ence of AD in speech and voice can be found in [5–8].","Acoustic Voice Quality Analysis (AVQA) is a wide
term for a set of diﬀerent methodologies designed to quantify acoustic correlates giving a deﬁnition of the
quality of phonation or speech production.",2022-03-17 15:54:44+00:00,Robust and Complex Approach of Pathological Speech Signal Analysis,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Jiri Mekyska'), arxiv.Result.Author('Eva Janousova'), arxiv.Result.Author('Pedro Gomez-Vilda'), arxiv.Result.Author('Zdenek Smekal'), arxiv.Result.Author('Irena Rektorova'), arxiv.Result.Author('Ilona Eliasova'), arxiv.Result.Author('Milena Kostalova'), arxiv.Result.Author('Martina Mrackova'), arxiv.Result.Author('Jesus B. Alonso-Hernandez'), arxiv.Result.Author('Marcos Faundez-Zanuy'), arxiv.Result.Author('Karmele López-de-Ipiña')]","This paper presents a study of the approaches in the state-of-the-art in the
field of pathological speech signal analysis with a special focus on
parametrization techniques. It provides a description of 92 speech features
where some of them are already widely used in this field of science and some of
them have not been tried yet (they come from different areas of speech signal
processing like speech recognition or coding). As an original contribution,
this work introduces 36 completely new pathological voice measures based on
modulation spectra, inferior colliculus coefficients, bicepstrum, sample and
approximate entropy and empirical mode decomposition. The significance of these
features was tested on 3 (English, Spanish and Czech) pathological voice
databases with respect to classification accuracy, sensitivity and specificity.",0.2422921,-0.013860708,0.0691032,B
3521,"We hope to encourage further research
reason for the difference in performance seems to be due to the onset        into low-resource, multi-purpose AMT systems and believe that the
detection accuracy which is higher in OF, since Acc is more similar          proposed solution can be a valuable baseline.",The main            computational requirements.,for both methods (42.8% for OF vs. 37.5% for NMP).,2022-03-18 12:07:36+00:00,A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Rachel M. Bittner'), arxiv.Result.Author('Juan José Bosch'), arxiv.Result.Author('David Rubinstein'), arxiv.Result.Author('Gabriel Meseguer-Brocal'), arxiv.Result.Author('Sebastian Ewert')]","Automatic Music Transcription (AMT) has been recognized as a key enabling
technology with a wide range of applications. Given the task's complexity, best
results have typically been reported for systems focusing on specific settings,
e.g. instrument-specific systems tend to yield improved results over
instrument-agnostic methods. Similarly, higher accuracy can be obtained when
only estimating frame-wise $f_0$ values and neglecting the harder note event
detection. Despite their high accuracy, such specialized systems often cannot
be deployed in the real-world. Storage and network constraints prohibit the use
of multiple specialized models, while memory and run-time constraints limit
their complexity. In this paper, we propose a lightweight neural network for
musical instrument transcription, which supports polyphonic outputs and
generalizes to a wide variety of instruments (including vocals). Our model is
trained to jointly predict frame-wise onsets, multipitch and note activations,
and we experimentally show that this multi-output structure improves the
resulting frame-level note accuracy. Despite its simplicity, benchmark results
show our system's note estimation to be substantially better than a comparable
baseline, and its frame-level accuracy to be only marginally below those of
specialized state-of-the-art AMT systems. With this work we hope to encourage
the community to further investigate low-resource, instrument-agnostic AMT
systems.",-0.49659964,-0.251281,-0.11115003,A
3522,"We hope to encourage further research
reason for the difference in performance seems to be due to the onset        into low-resource, multi-purpose AMT systems and believe that the
detection accuracy which is higher in OF, since Acc is more similar          proposed solution can be a valuable baseline.",The main            computational requirements.,for both methods (42.8% for OF vs. 37.5% for NMP).,2022-03-18 12:07:36+00:00,A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Rachel M. Bittner'), arxiv.Result.Author('Juan José Bosch'), arxiv.Result.Author('David Rubinstein'), arxiv.Result.Author('Gabriel Meseguer-Brocal'), arxiv.Result.Author('Sebastian Ewert')]","Automatic Music Transcription (AMT) has been recognized as a key enabling
technology with a wide range of applications. Given the task's complexity, best
results have typically been reported for systems focusing on specific settings,
e.g. instrument-specific systems tend to yield improved results over
instrument-agnostic methods. Similarly, higher accuracy can be obtained when
only estimating frame-wise $f_0$ values and neglecting the harder note event
detection. Despite their high accuracy, such specialized systems often cannot
be deployed in the real-world. Storage and network constraints prohibit the use
of multiple specialized models, while memory and run-time constraints limit
their complexity. In this paper, we propose a lightweight neural network for
musical instrument transcription, which supports polyphonic outputs and
generalizes to a wide variety of instruments (including vocals). Our model is
trained to jointly predict frame-wise onsets, multipitch and note activations,
and we experimentally show that this multi-output structure improves the
resulting frame-level note accuracy. Despite its simplicity, benchmark results
show our system's note estimation to be substantially better than a comparable
baseline, and its frame-level accuracy to be only marginally below those of
specialized state-of-the-art AMT systems. With this work we hope to encourage
the community to further investigate low-resource, instrument-agnostic AMT
systems.",-0.49659964,-0.251281,-0.11115003,A
3526,"Therefore, further research
is needed to investigate which speakers have a worse                 1.0 Clusters by FP position
prediction performance and construct prediction mod-
els that perform well for them.","The F-scores for each cluster                                                                                                  cluster3
of positions and words are widely distributed, indicat-                   Position           Word                                                       cluster4
ing that grouping speakers improves the performance
on average, but the tendency of improvement differs                       F score for position or word
from speaker to speaker.","0.8

4.5.5.",2022-03-18 13:55:32+00:00,Personalized filled-pause generation with group-wise prediction models,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Yuta Matsunaga'), arxiv.Result.Author('Takaaki Saeki'), arxiv.Result.Author('Shinnosuke Takamichi'), arxiv.Result.Author('Hiroshi Saruwatari')]","In this paper, we propose a method to generate personalized filled pauses
(FPs) with group-wise prediction models. Compared with fluent text generation,
disfluent text generation has not been widely explored. To generate more
human-like texts, we addressed disfluent text generation. The usage of
disfluency, such as FPs, rephrases, and word fragments, differs from speaker to
speaker, and thus, the generation of personalized FPs is required. However, it
is difficult to predict them because of the sparsity of position and the
frequency difference between more and less frequently used FPs. Moreover, it is
sometimes difficult to adapt FP prediction models to each speaker because of
the large variation of the tendency within each speaker. To address these
issues, we propose a method to build group-dependent prediction models by
grouping speakers on the basis of their tendency to use FPs. This method does
not require a large amount of data and time to train each speaker model. We
further introduce a loss function and a word embedding model suitable for FP
prediction. Our experimental results demonstrate that group-dependent models
can predict FPs with higher scores than a non-personalized one and the
introduced loss function and word embedding model improve the prediction
performance.",0.052666288,-0.10227638,0.08991848,B
3527,"Therefore, further research
prediction scores of each FP.","Evaluation of prediction for each FP               ing that grouping speakers improves the performance
To investigate whether the group-dependent models         on average, but the tendency of improvement differs
can reproduce the diversity of FP words, we show the      from speaker to speaker.","is needed to investigate which speakers have a worse
Figure 5 shows the F-scores of the prediction on each     prediction performance and construct prediction mod-
FP word in the group-dependent models by FP words         els that perform well for them.",2022-03-18 13:55:32+00:00,Personalized Filled-pause Generation with Group-wise Prediction Models,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Yuta Matsunaga'), arxiv.Result.Author('Takaaki Saeki'), arxiv.Result.Author('Shinnosuke Takamichi'), arxiv.Result.Author('Hiroshi Saruwatari')]","In this paper, we propose a method to generate personalized filled pauses
(FPs) with group-wise prediction models. Compared with fluent text generation,
disfluent text generation has not been widely explored. To generate more
human-like texts, we addressed disfluent text generation. The usage of
disfluency, such as FPs, rephrases, and word fragments, differs from speaker to
speaker, and thus, the generation of personalized FPs is required. However, it
is difficult to predict them because of the sparsity of position and the
frequency difference between more and less frequently used FPs. Moreover, it is
sometimes difficult to adapt FP prediction models to each speaker because of
the large variation of the tendency within each speaker. To address these
issues, we propose a method to build group-dependent prediction models by
grouping speakers on the basis of their tendency to use FPs. This method does
not require a large amount of data and time to train each speaker model. We
further introduce a loss function and a word embedding model suitable for FP
prediction. Our experimental results demonstrate that group-dependent models
can predict FPs with higher scores than a non-personalized one and the
introduced loss function and word embedding model improve the prediction
performance.",0.16034639,-0.12139115,0.115495406,B
3677,"In fu-
line1 [24] is adapted from the NI-SQA model and comprises            ture work, we will conduct further research to demonstrate our
a deep feedforward network followed by LSTM and average              idea.","The base-             speeches, causing the ﬁnal result may not to be the best.","Additionally, the loss function used in SQA-Discriminator
pooling.",2022-03-22 07:19:58+00:00,Residual-Guided Non-Intrusive Speech Quality Assessment,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Zhe Ye'), arxiv.Result.Author('Jiahao Chen'), arxiv.Result.Author('Diqun Yan')]","This paper proposes an approach to improve Non-Intrusive speech quality
assessment(NI-SQA) based on the residuals between impaired speech and enhanced
speech. The difficulty in our task is particularly lack of information, for
which the corresponding reference speech is absent. We generate an enhanced
speech on the impaired speech to compensate for the absence of the reference
audio, then pair the information of residuals with the impaired speech.
Compared to feeding the impaired speech directly into the model, residuals
could bring some extra helpful information from the contrast in enhancement.
The human ear is sensitive to certain noises but different to deep learning
model. Causing the Mean Opinion Score(MOS) the model predicted is not enough to
fit our subjective sensitive well and causes deviation. These residuals have a
close relationship to reference speech and then improve the ability of the deep
learning models to predict MOS. During the training phase, experimental results
demonstrate that paired with residuals can quickly obtain better evaluation
indicators under the same conditions. Furthermore, our final results improved
31.3 percent and 14.1 percent, respectively, in PLCC and RMSE.",-0.092700094,0.065408476,-0.24391052,B
3843,"state-of-the-art performance for audio-text retrieval, hereby provid-
                                                                        ing additional directions for further research and contributing to the
4.4.","Overall, our approach of
for the failure example in the right column of Table 3, the top three   incorporating PANNs features combined with NetRVLAD delivers
retrievals are all semantically aligned with the given audio.",Comparison with state-of-the-art                                   promotion of content-based retrieval solutions.,2022-03-25 13:41:17+00:00,Audio-text Retrieval in Context,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Siyu Lou'), arxiv.Result.Author('Xuenan Xu'), arxiv.Result.Author('Mengyue Wu'), arxiv.Result.Author('Kai Yu')]","Audio-text retrieval based on natural language descriptions is a challenging
task. It involves learning cross-modality alignments between long sequences
under inadequate data conditions. In this work, we investigate several audio
features as well as sequence aggregation methods for better audio-text
alignment. Moreover, through a qualitative analysis we observe that semantic
mapping is more important than temporal relations in contextual retrieval.
Using pre-trained audio features and a descriptor-based aggregation method, we
build our contextual audio-text retrieval system. Specifically, we utilize
PANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,
which directly works with averaged descriptors. Experiments are conducted on
the AudioCaps and CLOTHO datasets, and results are compared with the previous
state-of-the-art system. With our proposed system, a significant improvement
has been achieved on bidirectional audio-text retrieval, on all metrics
including recall, median and mean rank.",-0.012348266,0.14833257,0.194413,C
3844,"state-of-the-art performance for audio-text retrieval, hereby provid-
                                                                        ing additional directions for further research and contributing to the
4.4.","Overall, our approach of
for the failure example in the right column of Table 3, the top three   incorporating PANNs features combined with NetRVLAD delivers
retrievals are all semantically aligned with the given audio.",Comparison with state-of-the-art                                   promotion of content-based retrieval solutions.,2022-03-25 13:41:17+00:00,Audio-text Retrieval in Context,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Siyu Lou'), arxiv.Result.Author('Xuenan Xu'), arxiv.Result.Author('Mengyue Wu'), arxiv.Result.Author('Kai Yu')]","Audio-text retrieval based on natural language descriptions is a challenging
task. It involves learning cross-modality alignments between long sequences
under inadequate data conditions. In this work, we investigate several audio
features as well as sequence aggregation methods for better audio-text
alignment. Moreover, through a qualitative analysis we observe that semantic
mapping is more important than temporal relations in contextual retrieval.
Using pre-trained audio features and a descriptor-based aggregation method, we
build our contextual audio-text retrieval system. Specifically, we utilize
PANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,
which directly works with averaged descriptors. Experiments are conducted on
the AudioCaps and CLOTHO datasets, and results are compared with the previous
state-of-the-art system. With our proposed system, a significant improvement
has been achieved on bidirectional audio-text retrieval, on all metrics
including recall, median and mean rank.",-0.012348266,0.14833257,0.194413,C
4002,"However,
                                                                                                                 since the principles of neural net TTS are different from those
baseline 2 (phonemes, accents, initial lowering, and dependency length)                                          of human speech, this proposal needs further research toward
    4N: k i / n o o #6 y a / m a \ n a sh i n o #1 m o / r i \ g u ch i n o #1 a / n i \ y o m e n o             improving the psychological reality of computational models.",psychological reality of the phonological structures.,#1 w a / r u \ g u ch i o #2 k o / o e N d e #1 ts U / t a e t a .,2022-03-29 06:45:28+00:00,Applying Syntax$\unicode{x2013}$Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Kei Furukawa'), arxiv.Result.Author('Takeshi Kishiyama'), arxiv.Result.Author('Satoshi Nakamura')]","End-to-end text-to-speech synthesis (TTS), which generates speech sounds
directly from strings of texts or phonemes, has improved the quality of speech
synthesis over the conventional TTS. However, most previous studies have been
evaluated based on subjective naturalness and have not objectively examined
whether they can reproduce pitch patterns of phonological phenomena such as
downstep, rhythmic boost, and initial lowering that reflect syntactic
structures in Japanese. These phenomena can be linguistically explained by
phonological constraints and the syntax$\unicode{x2013}$prosody mapping
hypothesis (SPMH), which assumes projections from syntactic structures to
phonological hierarchy. Although some experiments in psycholinguistics have
verified the validity of the SPMH, it is crucial to investigate whether it can
be implemented in TTS. To synthesize linguistic phenomena involving syntactic
or phonological constraints, we propose a model using phonological symbols
based on the SPMH and prosodic well-formedness constraints. Experimental
results showed that the proposed method synthesized similar pitch patterns to
those reported in linguistics experiments for the phenomena of initial lowering
and rhythmic boost. The proposed model efficiently synthesizes phonological
phenomena in the test data that were not explicitly included in the training
data.",0.22684515,-0.03620733,-0.07674699,B
4055,"In an attempt to stimulate further research in this promising new          Frame-to-frame spectral mapping is the mainstream in pre-
                                        direction, recent sequence-to-sequence EVC papers were system-        vious studies [10, 11, 12], however, emotion is inherently supra-
                                        atically investigated and reviewed from six perspectives: their       segmental and complex with multiple signal attributes concern-
                                        motivation, training strategies, model architectures, datasets,       ing both the spectrum and prosody.",petitive paradigm for models that can overcome those challenges.,"Thus, frame-based mapping
                                        model inputs, and evaluation methods.",2022-03-29 19:41:34+00:00,An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion,cs.SD,"['cs.SD', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zijiang Yang'), arxiv.Result.Author('Xin Jing'), arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Meishu Song'), arxiv.Result.Author('Ilhan Aslan'), arxiv.Result.Author('Björn W. Schuller')]","Emotional voice conversion (EVC) focuses on converting a speech utterance
from a source to a target emotion; it can thus be a key enabling technology for
human-computer interaction applications and beyond. However, EVC remains an
unsolved research problem with several challenges. In particular, as speech
rate and rhythm are two key factors of emotional conversion, models have to
generate output sequences of differing length. Sequence-to-sequence modelling
is recently emerging as a competitive paradigm for models that can overcome
those challenges. In an attempt to stimulate further research in this promising
new direction, recent sequence-to-sequence EVC papers were systematically
investigated and reviewed from six perspectives: their motivation, training
strategies, model architectures, datasets, model inputs, and evaluation
methods. This information is organised to provide the research community with
an easily digestible overview of the current state-of-the-art. Finally, we
discuss existing challenges of sequence-to-sequence EVC.",0.2727039,0.0034859115,-0.028767847,B
4056,"The collection and release of suit-
happiness, and sadness) pronounced by 5 Korean actors and 5           able datasets to the public would foster further research in this
Korean actresses, for a total of 4 000 utterances.","Similarly, the dataset in Choi and Hahn [20] includes       of samples; however, there are non-speech utterances included,
100 sentences in 4 different emotional categories (neutral, anger,    such as laughter and yawn.",promising ﬁeld and help improve the performance.,2022-03-29 19:41:34+00:00,An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion,cs.SD,"['cs.SD', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zijiang Yang'), arxiv.Result.Author('Xin Jing'), arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Meishu Song'), arxiv.Result.Author('Ilhan Aslan'), arxiv.Result.Author('Björn W. Schuller')]","Emotional voice conversion (EVC) focuses on converting a speech utterance
from a source to a target emotion; it can thus be a key enabling technology for
human-computer interaction applications and beyond. However, EVC remains an
unsolved research problem with several challenges. In particular, as speech
rate and rhythm are two key factors of emotional conversion, models have to
generate output sequences of differing length. Sequence-to-sequence modelling
is recently emerging as a competitive paradigm for models that can overcome
those challenges. In an attempt to stimulate further research in this promising
new direction, recent sequence-to-sequence EVC papers were systematically
investigated and reviewed from six perspectives: their motivation, training
strategies, model architectures, datasets, model inputs, and evaluation
methods. This information is organised to provide the research community with
an easily digestible overview of the current state-of-the-art. Finally, we
discuss existing challenges of sequence-to-sequence EVC.",0.42257684,-0.12601376,-0.010696679,B
4169,"[24] T. Hori, S. Watanabe, and J. Hershey, “Joint CTC/attention decod-
                                                                                ing for end-to-end speech recognition,” in Proceedings of the 55th
 [8] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                 Annual Meeting of the Association for Computational Linguistics
      K. Han, and X. Li, “A further study of unsupervised pretraining           (Volume 1: Long Papers), Jul.",10 937–10 947.,"2017, pp.",2022-03-31 15:33:56+00:00,Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Junyi Ao'), arxiv.Result.Author('Ziqiang Zhang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Haizhou Li'), arxiv.Result.Author('Tom Ko'), arxiv.Result.Author('Lirong Dai'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Yao Qian'), arxiv.Result.Author('Furu Wei')]","This paper studies a novel pre-training technique with unpaired speech data,
Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within
a multi-task learning framework, we introduce two pre-training tasks for the
encoder-decoder network using acoustic units, i.e., pseudo codes, derived from
an offline clustering model. One is to predict the pseudo codes via masked
language modeling in encoder output, like HuBERT model, while the other lets
the decoder learn to reconstruct pseudo codes autoregressively instead of
generating textual scripts. In this way, the decoder learns to reconstruct
original speech information with codes before learning to generate correct
text. Comprehensive experiments on the LibriSpeech corpus show that the
proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over
the method without decoder pre-training, and also outperforms significantly the
state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h.",0.24483365,0.068265386,-0.29954946,B
4170,"[24] T. Hori, S. Watanabe, and J. Hershey, “Joint CTC/attention decod-
                                                                                ing for end-to-end speech recognition,” in Proceedings of the 55th
 [8] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                 Annual Meeting of the Association for Computational Linguistics
      K. Han, and X. Li, “A further study of unsupervised pretraining           (Volume 1: Long Papers), Jul.",10 937–10 947.,"2017, pp.",2022-03-31 15:33:56+00:00,Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Junyi Ao'), arxiv.Result.Author('Ziqiang Zhang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Haizhou Li'), arxiv.Result.Author('Tom Ko'), arxiv.Result.Author('Lirong Dai'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Yao Qian'), arxiv.Result.Author('Furu Wei')]","This paper studies a novel pre-training technique with unpaired speech data,
Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within
a multi-task learning framework, we introduce two pre-training tasks for the
encoder-decoder network using acoustic units, i.e., pseudo codes, derived from
an offline clustering model. One is to predict the pseudo codes via masked
language modeling in encoder output, like HuBERT model, while the other lets
the decoder learn to reconstruct pseudo codes autoregressively instead of
generating textual scripts. In this way, the decoder learns to reconstruct
original speech information with codes before learning to generate correct
text. Comprehensive experiments on the LibriSpeech corpus show that the
proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over
the method without decoder pre-training, and also outperforms significantly the
state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h.
We release our code and model at
https://github.com/microsoft/SpeechT5/tree/main/Speech2C.",0.24483365,0.068265386,-0.29954946,B
4188,"that speech from the proposed model is able to retain the level
of intonation variation present in teacher forced data rather than       We see several opportunities for further research.",We conclude      ing the approach feasible for low-resource languages.,"First, we
the lower intonation variation from the augmented speech.",2022-03-31 20:01:32+00:00,Data-augmented cross-lingual synthesis in a teacher-student framework,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS', 'I.2.7']","[arxiv.Result.Author('Marcel de Korte'), arxiv.Result.Author('Jaebok Kim'), arxiv.Result.Author('Aki Kunikoshi'), arxiv.Result.Author('Adaeze Adigwe'), arxiv.Result.Author('Esther Klabbers')]","Cross-lingual synthesis can be defined as the task of letting a speaker
generate fluent synthetic speech in another language. This is a challenging
task, and resulting speech can suffer from reduced naturalness, accented
speech, and/or loss of essential voice characteristics. Previous research shows
that many models appear to have insufficient generalization capabilities to
perform well on every of these cross-lingual aspects. To overcome these
generalization problems, we propose to apply the teacher-student paradigm to
cross-lingual synthesis. While a teacher model is commonly used to produce
teacher forced data, we propose to also use it to produce augmented data of
unseen speaker-language pairs, where the aim is to retain essential speaker
characteristics. Both sets of data are then used for student model training,
which is trained to retain the naturalness and prosodic variation present in
the teacher forced data, while learning the speaker identity from the augmented
data. Some modifications to the student model are proposed to make the
separation of teacher forced and augmented data more straightforward. Results
show that the proposed approach improves the retention of speaker
characteristics in the speech, while managing to retain high levels of
naturalness and prosodic variation.",0.35705194,-0.030989973,-0.13440338,B
4246,We also further study the masking          der the E2E ASR framework.,"In addition, deep investigations are carried out into dif-
                                       ferent units in masking, which shows the effectiveness of our          We try to solve PR from a mask training perspective un-
                                       proposed masking unit.","Recently, a lot of excellent works
                                       method and optimize ﬁlling strategy of phone mask.",2022-04-02 09:04:24+00:00,Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Guodong Ma'), arxiv.Result.Author('Pengfei Hu'), arxiv.Result.Author('Jian Kang'), arxiv.Result.Author('Shen Huang'), arxiv.Result.Author('Hao Huang')]","In Uyghur speech, consonant and vowel reduction are often encountered,
especially in spontaneous speech with high speech rate, which will cause a
degradation of speech recognition performance. To solve this problem, we
propose an effective phone mask training method for Conformer-based Uyghur
end-to-end (E2E) speech recognition. The idea is to randomly mask off a certain
percentage features of phones during model training, which simulates the above
verbal phenomena and facilitates E2E model to learn more contextual
information. According to experiments, the above issues can be greatly
alleviated. In addition, deep investigations are carried out into different
units in masking, which shows the effectiveness of our proposed masking unit.
We also further study the masking method and optimize filling strategy of phone
mask. Finally, compared with Conformer-based E2E baseline without mask
training, our model demonstrates about 5.51% relative Word Error Rate (WER)
reduction on reading speech and 12.92% on spontaneous speech, respectively. The
above approach has also been verified on test-set of open-source data THUYG-20,
which shows 20% relative improvements.",-0.16383533,-0.064232424,-0.29851174,A
4390,"However, the detection                  1https://github.com/yangdongchao/RaDur
results for transient and long events still have a large gap,
which deserves further study in our future work.","Because of the multi-scale feature extractor     Fundamental Research Programs JSGG20191129105421211
and duration-aware focal loss, RaDur provides signiﬁcant
improvement on transient events.",and GXWD20201231165807007-20200814115301001.,2022-04-05 12:08:13+00:00,RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Dongchao Yang'), arxiv.Result.Author('Helin Wang'), arxiv.Result.Author('Zhongjie Ye'), arxiv.Result.Author('Yuexian Zou'), arxiv.Result.Author('Wenwu Wang')]","Target sound detection (TSD) aims to detect the target sound from a mixture
audio given the reference information. Previous methods use a conditional
network to extract a sound-discriminative embedding from the reference audio,
and then use it to detect the target sound from the mixture audio. However, the
network performs much differently when using different reference audios (e.g.
performs poorly for noisy and short-duration reference audios), and tends to
make wrong decisions for transient events (i.e. shorter than $1$ second). To
overcome these problems, in this paper, we present a reference-aware and
duration-robust network (RaDur) for TSD. More specifically, in order to make
the network more aware of the reference information, we propose an embedding
enhancement module to take into account the mixture audio while generating the
embedding, and apply the attention pooling to enhance the features of target
sound-related frames and weaken the features of noisy frames. In addition, a
duration-robust focal loss is proposed to help model different-duration events.
To evaluate our method, we build two TSD datasets based on UrbanSound and
Audioset. Extensive experiments show the effectiveness of our methods.",-0.36421746,-0.14039269,-0.19792795,A
4490,"[3] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,
      K. Han, and X. Li, “A further study of unsupervised pretraining
      for transformer based speech recognition,” in ICASSP 2021-2021
      IEEE International Conference on Acoustics, Speech and Signal
      Processing (ICASSP).",7694–7698.,"IEEE, 2021, pp.",2022-04-07 06:12:05+00:00,Speech Pre-training with Acoustic Piece,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Shuo Ren'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Furu Wei')]","Previous speech pre-training methods, such as wav2vec2.0 and HuBERT,
pre-train a Transformer encoder to learn deep representations from audio data,
with objectives predicting either elements from latent vector quantized space
or pre-generated labels (known as target codes) with offline clustering.
However, those training signals (quantized elements or codes) are independent
across different tokens without considering their relations. According to our
observation and analysis, the target codes share obvious patterns aligned with
phonemized text data. Based on that, we propose to leverage those patterns to
better pre-train the model considering the relations among the codes. The
patterns we extracted, called ""acoustic piece""s, are from the sentence piece
result of HuBERT codes. With the acoustic piece as the training signal, we can
implicitly bridge the input audio and natural language, which benefits
audio-to-text tasks, such as automatic speech recognition (ASR). Simple but
effective, our method ""HuBERT-AP"" significantly outperforms strong baselines on
the LibriSpeech ASR task.",0.09519586,0.2912397,-0.22842929,B
4621,"At last, setting ""w/ Fully Paired Data"" is the             At last, we further study the effectiveness of our method un-
model pre-trained with fully parallel data (431 hours in total).","metrics (by comparing setting ""w/ Less Unpaired Data"" and
""Our Method"").",der semi-supervised setting.,2022-04-10 10:25:37+00:00,Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Yu Kang'), arxiv.Result.Author('Tianqiao Liu'), arxiv.Result.Author('Hang Li'), arxiv.Result.Author('Yang Hao'), arxiv.Result.Author('Wenbiao Ding')]","Multimodal pre-training for audio-and-text has recently been proved to be
effective and has significantly improved the performance of many downstream
speech understanding tasks. However, these state-of-the-art pre-training
audio-text models work well only when provided with large amount of parallel
audio-and-text data, which brings challenges on many languages that are rich in
unimodal corpora but scarce of parallel cross-modal corpus. In this paper, we
investigate whether it is possible to pre-train an audio-text multimodal model
with extremely low-resource parallel data and extra non-parallel unimodal data.
Our pre-training framework consists of the following components: (1)
Intra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input
text (audio) representations from a noisy version of itself. (2) Cross-modal
Denoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input
text (audio), given both a noisy version of the input text (audio) and the
corresponding translated noisy audio features (text embeddings). (3) Iterative
Denoising Process (IDP), which iteratively translates raw audio (text) and the
corresponding text embeddings (audio features) translated from previous
iteration into the new less-noisy text embeddings (audio features). We adapt a
dual cross-modal Transformer as our backbone model which consists of two
unimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our
method achieves comparable performance on multiple downstream speech
understanding tasks compared with the model pre-trained on fully parallel data,
demonstrating the great potential of the proposed method. Our code is available
at: \url{https://github.com/KarlYuKang/Low-Resource-Multimodal-Pre-training}.",-0.15718262,0.05424174,-0.07766342,C
4970,"We expect this to enable        ever, a clear distinction between them can be observed in terms
further research interfacing JTFS and convnets for audio analysis       of the frequency band alignment in time.","How-
networks for supervised classification.","Unlike time scattering,
and synthesis.",2022-04-18 12:02:08+00:00,Differentiable Time-Frequency Scattering in Kymatio,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('John Muradeli'), arxiv.Result.Author('Cyrus Vahidi'), arxiv.Result.Author('Changhong Wang'), arxiv.Result.Author('Han Han'), arxiv.Result.Author('Vincent Lostanlen'), arxiv.Result.Author('Mathieu Lagrange'), arxiv.Result.Author('George Fazekas')]","Joint time-frequency scattering (JTFS) is a convolutional operator in the
time-frequency domain which extracts spectrotemporal modulations at various
rates and scales. It offers an idealized model of spectrotemporal receptive
fields (STRF) in the primary auditory cortex, and thus may serve as a
biological plausible surrogate for human perceptual judgments at the scale of
isolated audio events. Yet, prior implementations of JTFS and STRF have
remained outside of the standard toolkit of perceptual similarity measures and
evaluation methods for audio generation. We trace this issue down to three
limitations: differentiability, speed, and flexibility. In this paper, we
present an implementation of time-frequency scattering in Kymatio, an
open-source Python package for scattering transforms. Unlike prior
implementations, Kymatio accommodates NumPy and PyTorch as backends and is thus
portable on both CPU and GPU. We demonstrate the usefulness of JTFS in Kymatio
via three applications: unsupervised manifold learning of spectrotemporal
modulations, supervised classification of musical instruments, and texture
resynthesis of bioacoustic sounds.",-0.18582177,0.5756575,0.22086334,C_centroid
4971,"We expect this to enable        ever, a clear distinction between them can be observed in terms
further research interfacing JTFS and convnets for audio analysis       of the frequency band alignment in time.","How-
networks for supervised classification.","Unlike time scattering,
and synthesis.",2022-04-18 12:02:08+00:00,Differentiable Time-Frequency Scattering in Kymatio,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('John Muradeli'), arxiv.Result.Author('Cyrus Vahidi'), arxiv.Result.Author('Changhong Wang'), arxiv.Result.Author('Han Han'), arxiv.Result.Author('Vincent Lostanlen'), arxiv.Result.Author('Mathieu Lagrange'), arxiv.Result.Author('George Fazekas')]","Joint time-frequency scattering (JTFS) is a convolutional operator in the
time-frequency domain which extracts spectrotemporal modulations at various
rates and scales. It offers an idealized model of spectrotemporal receptive
fields (STRF) in the primary auditory cortex, and thus may serve as a
biological plausible surrogate for human perceptual judgments at the scale of
isolated audio events. Yet, prior implementations of JTFS and STRF have
remained outside of the standard toolkit of perceptual similarity measures and
evaluation methods for audio generation. We trace this issue down to three
limitations: differentiability, speed, and flexibility. In this paper, we
present an implementation of time-frequency scattering in Kymatio, an
open-source Python package for scattering transforms. Unlike prior
implementations, Kymatio accommodates NumPy and PyTorch as backends and is thus
portable on both CPU and GPU. We demonstrate the usefulness of JTFS in Kymatio
via three applications: unsupervised manifold learning of spectrotemporal
modulations, supervised classification of musical instruments, and texture
resynthesis of bioacoustic sounds.",-0.18582177,0.5756575,0.22086334,C
4972,"We expect this to enable        ever, a clear distinction between them can be observed in terms
further research interfacing JTFS and convnets for audio analysis       of the frequency band alignment in time.","How-
networks for supervised classification.","Unlike time scattering,
and synthesis.",2022-04-18 12:02:08+00:00,Differentiable Time-Frequency Scattering in Kymatio,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('John Muradeli'), arxiv.Result.Author('Cyrus Vahidi'), arxiv.Result.Author('Changhong Wang'), arxiv.Result.Author('Han Han'), arxiv.Result.Author('Vincent Lostanlen'), arxiv.Result.Author('Mathieu Lagrange'), arxiv.Result.Author('George Fazekas')]","Joint time-frequency scattering (JTFS) is a convolutional operator in the
time-frequency domain which extracts spectrotemporal modulations at various
rates and scales. It offers an idealized model of spectrotemporal receptive
fields (STRF) in the primary auditory cortex, and thus may serve as a
biological plausible surrogate for human perceptual judgments at the scale of
isolated audio events. Yet, prior implementations of JTFS and STRF have
remained outside of the standard toolkit of perceptual similarity measures and
evaluation methods for audio generation. We trace this issue down to three
limitations: differentiability, speed, and flexibility. In this paper, we
present an implementation of time-frequency scattering in Kymatio, an
open-source Python package for scattering transforms. Unlike prior
implementations, Kymatio accommodates NumPy and PyTorch as backends and is thus
portable on both CPU and GPU. We demonstrate the usefulness of JTFS in Kymatio
via three applications: unsupervised manifold learning of spectrotemporal
modulations, supervised classification of musical instruments, and texture
resynthesis of bioacoustic sounds.",-0.18582177,0.5756575,0.22086334,C
4973,We expect this to enable further research interfacing    of the frequency band alignment in time.,"learned 2-dimensional deep convolutional networks for supervised         However, a clear distinction between them can be observed in terms
classification.","Unlike time scattering,
JTFS and convnets for audio analysis and synthesis.",2022-04-18 12:02:08+00:00,Differentiable Time-Frequency Scattering on GPU,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('John Muradeli'), arxiv.Result.Author('Cyrus Vahidi'), arxiv.Result.Author('Changhong Wang'), arxiv.Result.Author('Han Han'), arxiv.Result.Author('Vincent Lostanlen'), arxiv.Result.Author('Mathieu Lagrange'), arxiv.Result.Author('George Fazekas')]","Joint time-frequency scattering (JTFS) is a convolutional operator in the
time-frequency domain which extracts spectrotemporal modulations at various
rates and scales. It offers an idealized model of spectrotemporal receptive
fields (STRF) in the primary auditory cortex, and thus may serve as a
biological plausible surrogate for human perceptual judgments at the scale of
isolated audio events. Yet, prior implementations of JTFS and STRF have
remained outside of the standard toolkit of perceptual similarity measures and
evaluation methods for audio generation. We trace this issue down to three
limitations: differentiability, speed, and flexibility. In this paper, we
present an implementation of time-frequency scattering in Python. Unlike prior
implementations, ours accommodates NumPy, PyTorch, and TensorFlow as backends
and is thus portable on both CPU and GPU. We demonstrate the usefulness of JTFS
via three applications: unsupervised manifold learning of spectrotemporal
modulations, supervised classification of musical instruments, and texture
resynthesis of bioacoustic sounds.",-0.1870679,0.5885248,0.1319678,C
5126,"proaches and predict that further research will reduce er-
                                                           ror rates considerably.","However, we remain convinced
implying that, while results are encouraging, impacts of   in the merit of DNN-based embedding-level fusion ap-
spooﬁng remain.","B2 has better potential to harness
DNN fusion, B2 – The last row of Table 2 shows re-         the synergy between CM and ASV sub-systems; this is
sults for DNN-based back-end embedding-level fusion.",2022-04-21 09:04:12+00:00,Baseline Systems for the First Spoofing-Aware Speaker Verification Challenge: Score and Embedding Fusion,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Hye-jin Shim'), arxiv.Result.Author('Hemlata Tak'), arxiv.Result.Author('Xuechen Liu'), arxiv.Result.Author('Hee-Soo Heo'), arxiv.Result.Author('Jee-weon Jung'), arxiv.Result.Author('Joon Son Chung'), arxiv.Result.Author('Soo-Whan Chung'), arxiv.Result.Author('Ha-Jin Yu'), arxiv.Result.Author('Bong-Jin Lee'), arxiv.Result.Author('Massimiliano Todisco'), arxiv.Result.Author('Héctor Delgado'), arxiv.Result.Author('Kong Aik Lee'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Tomi Kinnunen'), arxiv.Result.Author('Nicholas Evans')]","Deep learning has brought impressive progress in the study of both automatic
speaker verification (ASV) and spoofing countermeasures (CM). Although
solutions are mutually dependent, they have typically evolved as standalone
sub-systems whereby CM solutions are usually designed for a fixed ASV system.
The work reported in this paper aims to gauge the improvements in reliability
that can be gained from their closer integration. Results derived using the
popular ASVspoof2019 dataset indicate that the equal error rate (EER) of a
state-of-the-art ASV system degrades from 1.63% to 23.83% when the evaluation
protocol is extended with spoofed trials.%subjected to spoofing attacks.
However, even the straightforward integration of ASV and CM systems in the form
of score-sum and deep neural network-based fusion strategies reduce the EER to
1.71% and 6.37%, respectively. The new Spoofing-Aware Speaker Verification
(SASV) challenge has been formed to encourage greater attention to the
integration of ASV and CM systems as well as to provide a means to benchmark
different solutions.",-0.16497156,-0.03985837,-0.30234745,A
5243,"We end the paper with conclusions and further research perspectives in
Section 6.","One about
stylistic diﬀerences between slow movements from three diﬀerent authors and the
other about stylistic diﬀerences between various types of composition of a given
author.","We remark that all codes developed for this paper are available at
https://github.com/MartinMij/TDA-SQ.",2022-04-23 20:33:49+00:00,Musical Stylistic Analysis: A Study of Intervallic Transition Graphs via Persistent Homology,cs.SD,"['cs.SD', 'eess.AS', 'math.AT', '62R40, 55N31 (Primary) 05C90, 00A65 (Secondary)']","[arxiv.Result.Author('Martín Mijangos'), arxiv.Result.Author('Alessandro Bravetti'), arxiv.Result.Author('Pablo Padilla')]","Topological data analysis has been recently applied to investigate stylistic
signatures and trends in musical compositions. A useful tool in this area is
Persistent Homology. In this paper, we develop a novel method to represent a
weighted directed graph as a finite metric space and then use persistent
homology to extract useful features. We apply this method to weighted directed
graphs obtained from pitch transitions information of a given musical fragment
and use these techniques to the study of stylistic trends. In particular, we
are interested in using these tools to make quantitative stylistic comparisons.
As a first illustration, we analyze a selection of string quartets by Haydn,
Mozart and Beethoven and discuss possible implications of our results in terms
of different approaches by these composers to stylistic exploration and
variety. We observe that Haydn is stylistically the most conservative, followed
by Mozart, while Beethoven is the most innovative, expanding and modifying the
string quartet as a musical form. Finally we also compare the variability of
different genres, namely minuets, allegros, prestos and adagios, by a given
composer and conclude that the minuet is the most stable form of the string
quartet movements.",0.079558074,-0.29411364,0.17035288,B
5253,"Overall, we believe
has several limitations that warrant further research.","That being said, our study still                                                                                                                     sonation capabilities of the given sample).","While we                                                                                                                     our work will ultimately lead to a better understanding of the
tested multiple speaker encoders based on two different acous-                                                                                                                      speech modality and more secure human-computer interaction.",2022-04-24 15:31:41+00:00,Dictionary Attacks on Speaker Verification,cs.SD,"['cs.SD', 'cs.CV', 'eess.AS']","[arxiv.Result.Author('Mirko Marras'), arxiv.Result.Author('Pawel Korus'), arxiv.Result.Author('Anubhav Jain'), arxiv.Result.Author('Nasir Memon')]","In this paper, we propose dictionary attacks against speaker verification - a
novel attack vector that aims to match a large fraction of speaker population
by chance. We introduce a generic formulation of the attack that can be used
with various speech representations and threat models. The attacker uses
adversarial optimization to maximize raw similarity of speaker embeddings
between a seed speech sample and a proxy population. The resulting master voice
successfully matches a non-trivial fraction of people in an unknown population.
Adversarial waveforms obtained with our approach can match on average 69% of
females and 38% of males enrolled in the target system at a strict decision
threshold calibrated to yield false alarm rate of 1%. By using the attack with
a black-box voice cloning system, we obtain master voices that are effective in
the most challenging conditions and transferable between speaker encoders. We
also show that, combined with multiple attempts, this attack opens even more to
serious issues on the security of these systems.",0.24518533,-0.003293505,-0.050637107,B
5307,"2, 2013.
learning combined with adaptive ﬁlters and hope our complete        [21] L. Lu, K.-L. Yin, R. C. de Lamare, Z. Zheng, Y. Yu, X. Yang, and
code release will stimulate further research and rapid progress.","93, no.","B. Chen, “A survey on active noise control in the past decade—part i:
                                                                          Linear systems,” Elsevier Signal Processing, vol.",2022-04-25 19:44:24+00:00,Meta-AF: Meta-Learning for Adaptive Filters,cs.SD,"['cs.SD', 'eess.AS', 'eess.SP']","[arxiv.Result.Author('Jonah Casebeer'), arxiv.Result.Author('Nicholas J. Bryan'), arxiv.Result.Author('Paris Smaragdis')]","Adaptive filtering algorithms are pervasive throughout modern society and
have had a significant impact on a wide variety of domains including audio
processing, telecommunications, biomedical sensing, astropyhysics and
cosmology, seismology, and many more. Adaptive filters typically operate via
specialized online, iterative optimization methods such as least-mean squares
or recursive least squares and aim to process signals in unknown or
nonstationary environments. Such algorithms, however, can be slow and laborious
to develop, require domain expertise to create, and necessitate mathematical
insight for improvement. In this work, we seek to go beyond the limits of
human-derived adaptive filter algorithms and present a comprehensive framework
for learning online, adaptive signal processing algorithms or update rules
directly from data. To do so, we frame the development of adaptive filters as a
meta-learning problem in the context of deep learning and use a form of
self-supervision to learn online iterative update rules for adaptive filters.
To demonstrate our approach, we focus on audio applications and systematically
develop meta-learned adaptive filters for five canonical audio problems
including system identification, acoustic echo cancellation, blind
equalization, multi-channel dereverberation, and beamforming. For each
application, we compare against common baselines and/or current
state-of-the-art methods and show we can learn high-performing adaptive filters
that operate in real-time and, in most cases, significantly out perform all
past specially developed methods for each task using a single general-purpose
configuration of our method.",-0.22475424,-0.00730828,-0.15054844,A
5308,"DISCUSSION, FUTURE WORK, AND CONCLUSION                          further research and rapid progress.","We are excited
parameters and single CPU core RTF of ≈ 0.25.                           about the future of deep learning combined with adaptive
                                                                        ﬁlters and hope our complete code release will stimulate
   IX.",A.,2022-04-25 19:44:24+00:00,Meta-AF: Meta-Learning for Adaptive Filters,cs.SD,"['cs.SD', 'eess.AS', 'eess.SP']","[arxiv.Result.Author('Jonah Casebeer'), arxiv.Result.Author('Nicholas J. Bryan'), arxiv.Result.Author('Paris Smaragdis')]","Adaptive filtering algorithms are pervasive throughout signal processing and
have had a material impact on a wide variety of domains including audio
processing, telecommunications, biomedical sensing, astrophysics and cosmology,
seismology, and many more. Adaptive filters typically operate via specialized
online, iterative optimization methods such as least-mean squares or recursive
least squares and aim to process signals in unknown or nonstationary
environments. Such algorithms, however, can be slow and laborious to develop,
require domain expertise to create, and necessitate mathematical insight for
improvement. In this work, we seek to improve upon hand-derived adaptive filter
algorithms and present a comprehensive framework for learning online, adaptive
signal processing algorithms or update rules directly from data. To do so, we
frame the development of adaptive filters as a meta-learning problem in the
context of deep learning and use a form of self-supervision to learn online
iterative update rules for adaptive filters. To demonstrate our approach, we
focus on audio applications and systematically develop meta-learned adaptive
filters for five canonical audio problems including system identification,
acoustic echo cancellation, blind equalization, multi-channel dereverberation,
and beamforming. We compare our approach against common baselines and/or recent
state-of-the-art methods. We show we can learn high-performing adaptive filters
that operate in real-time and, in most cases, significantly outperform each
method we compare against -- all using a single general-purpose configuration
of our approach.",-0.27146304,0.10959913,-0.34149137,A
5471,"In this article,  two steps known as feature extraction and feature
to further study this important factor in Farsi, we examine           classification.","Recognition of emotions depends on the type of                speech emotion recognition (SER) approaches consist of
expression that varies between different languages.","At the first step of speech processing,
various deep learning techniques on the SheEMO dataset.",2022-04-28 16:02:05+00:00,Emotion Recognition In Persian Speech Using Deep Neural Networks,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS', '68Txx', 'I.2']","[arxiv.Result.Author('Ali Yazdani'), arxiv.Result.Author('Hossein Simchi'), arxiv.Result.Author('Yaser Shekofteh')]","Speech Emotion Recognition (SER) is of great importance in Human-Computer
Interaction (HCI), as it provides a deeper understanding of the situation and
results in better interaction. In recent years, various machine learning and
deep learning algorithms have been developed to improve SER techniques.
Recognition of emotions depends on the type of expression that varies between
different languages. In this article, to further study this important factor in
Farsi, we examine various deep learning techniques on the SheEMO dataset. Using
signal features in low- and high-level descriptions and different deep networks
and machine learning techniques, Unweighted Average Recall (UAR) of 65.20 is
achieved with an accuracy of 78.29.",0.26662025,0.02049905,-0.14991677,B
5472,"In  introduce our contribution which includes reviewing the
this paper, to further study important factors in the Farsi         ShEMO data set and two common methods of extracting
language, we examine various DL techniques on a Farsi/Persian       features as LLDs (Low-Level Descriptors) and functionals
dataset, Sharif Emotional Speech Database (ShEMO), which was        from voice signals, as well as testing different DNN models on
released in 2018.","In section 3, we
the type of expression that varies between different languages.",Using signal features in low- and high-level      these features.,2022-04-28 16:02:05+00:00,Emotion Recognition In Persian Speech Using Deep Neural Networks,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS', '68T10 (primary) 68T07 (secondary)', 'I.2']","[arxiv.Result.Author('Ali Yazdani'), arxiv.Result.Author('Hossein Simchi'), arxiv.Result.Author('Yasser Shekofteh')]","Speech Emotion Recognition (SER) is of great importance in Human-Computer
Interaction (HCI), as it provides a deeper understanding of the situation and
results in better interaction. In recent years, various machine learning and
Deep Learning (DL) algorithms have been developed to improve SER techniques.
Recognition of the spoken emotions depends on the type of expression that
varies between different languages. In this paper, to further study important
factors in the Farsi language, we examine various DL techniques on a
Farsi/Persian dataset, Sharif Emotional Speech Database (ShEMO), which was
released in 2018. Using signal features in low- and high-level descriptions and
different deep neural networks and machine learning techniques, Unweighted
Accuracy (UA) of 65.20% and Weighted Accuracy (WA) of 78.29% are achieved.",0.27629697,-0.014498167,-0.14798644,B
6006,"We nonetheless hope our work
                                                                   to inspire further research or applications.","This necessarily limits the
                                                                   statistical power of ﬁndings.","A game perceived
                                                                   sufﬁciently ‘cool’ may have the potential to go viral.",2022-05-10 14:25:06+00:00,Gamified Speaker Comparison by Listening,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Sandip Ghimire'), arxiv.Result.Author('Tomi Kinnunen'), arxiv.Result.Author('Rosa Gonzalez Hautamäki')]","We address speaker comparison by listening in a game-like environment,
hypothesized to make the task more motivating for naive listeners. We present
the same 30 trials selected with the help of an x-vector speaker recognition
system from VoxCeleb to a total of 150 crowdworkers recruited through Amazon's
Mechanical Turk. They are divided into cohorts of 50, each using one of three
alternative interface designs: (i) a traditional (nongamified) design; (ii) a
gamified design with feedback on decisions, along with points, game level
indications, and possibility for interface customization; (iii) another
gamified design with an additional constraint of maximum of 5 'lives' consumed
by wrong answers. We analyze the impact of these interface designs to listener
error rates (both misses and false alarms), probability calibration, time of
quitting, along with survey questionnaire. The results indicate improved
performance from (i) to (ii) and (iii), particularly in terms of balancing the
two types of detection errors.",0.101792805,-0.36733547,0.31751275,A
6056,"CONCLUSION
                                                               [1] O. Nieto, “Unsupervised clustering of extreme vocal
We introduced a new annotated dataset to aid and encourage          effects,” in Proceedings of the 10th International Con-
further research in vocal detection in heavy metal music.","REFERENCES
                       7.","ference Advances in Quantitative Laryngology, 2013, p.
Both the dataset and code have been made publicly avail-           115.
able.",2022-05-11 15:48:56+00:00,Scream Detection in Heavy Metal Music,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Vedant Kalbag'), arxiv.Result.Author('Alexander Lerch')]","Harsh vocal effects such as screams or growls are far more common in heavy
metal vocals than the traditionally sung vocal. This paper explores the problem
of detection and classification of extreme vocal techniques in heavy metal
music, specifically the identification of different scream techniques. We
investigate the suitability of various feature representations, including
cepstral, spectral, and temporal features as input representations for
classification. The main contributions of this work are (i) a manually
annotated dataset comprised of over 280 minutes of heavy metal songs of various
genres with a statistical analysis of occurrences of different extreme vocal
techniques in heavy metal music, and (ii) a systematic study of different input
feature representations for the classification of heavy metal vocals",0.12714297,0.22110572,0.17134355,C
6929,"The
                                        presents the Respiratory and Drug Actuation (RDA) Suite1 for              efﬁcient and effective management of asthma is strongly
                                        benchmarking and further research.","Aerosol devices deliver a ﬁxed medication dose, rapidly
                                        paper revisits audio pattern recognition and machine learning             and directly into the airways, from a pressurized canister,
                                        techniques for asthma medication adherence assessment and                 containing a medication/propellant mixture [14], [15].","The RDA Suite includes a               connected with the patient adherence to the prescribed action
                                        set of tools for audio processing, feature extraction and classiﬁca-      plan, while reduced adherence has been strongly linked with
                                        tion and is provided along with a dataset consisting of respiratory       signiﬁcant indicators of health degradation.",2022-05-30 18:08:28+00:00,Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite,cs.SD,"['cs.SD', 'cs.CV', 'cs.CY', 'cs.GL', 'eess.AS']","[arxiv.Result.Author('Nikos D. Fakotakis'), arxiv.Result.Author('Stavros Nousias'), arxiv.Result.Author('Gerasimos Arvanitis'), arxiv.Result.Author('Evangelia I. Zacharaki'), arxiv.Result.Author('Konstantinos Moustakas')]","Asthma is a common, usually long-term respiratory disease with negative
impact on society and the economy worldwide. Treatment involves using medical
devices (inhalers) that distribute medication to the airways, and its
efficiency depends on the precision of the inhalation technique. Health
monitoring systems equipped with sensors and embedded with sound signal
detection enable the recognition of drug actuation and could be powerful tools
for reliable audio content analysis. This paper revisits audio pattern
recognition and machine learning techniques for asthma medication adherence
assessment and presents the Respiratory and Drug Actuation (RDA)
Suite(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for
benchmarking and further research. The RDA Suite includes a set of tools for
audio processing, feature extraction and classification and is provided along
with a dataset consisting of respiratory and drug actuation sounds. The
classification models in RDA are implemented based on conventional and advanced
machine learning and deep network architectures. This study provides a
comparative evaluation of the implemented approaches, examines potential
improvements and discusses challenges and future tendencies.",-0.0477001,0.11058984,-0.016573764,C
6930,"The
                                       paper revisits audio pattern recognition and machine learning             efﬁcient and effective management of asthma is strongly
                                       techniques for asthma medication adherence assessment and                 connected with the patient adherence to the prescribed action
                                       presents the Respiratory and Drug Actuation (RDA) Suite1 for              plan, while reduced adherence has been strongly linked with
                                       benchmarking and further research.","This               containing a medication/propellant mixture [14], [15].",The RDA Suite includes a               signiﬁcant indicators of health degradation.,2022-05-30 18:08:28+00:00,Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite,cs.SD,"['cs.SD', 'cs.CV', 'cs.CY', 'cs.GL', 'eess.AS']","[arxiv.Result.Author('Nikos D. Fakotakis'), arxiv.Result.Author('Stavros Nousias'), arxiv.Result.Author('Gerasimos Arvanitis'), arxiv.Result.Author('Evangelia I. Zacharaki'), arxiv.Result.Author('Konstantinos Moustakas')]","Asthma is a common, usually long-term respiratory disease with negative
impact on society and the economy worldwide. Treatment involves using medical
devices (inhalers) that distribute medication to the airways, and its
efficiency depends on the precision of the inhalation technique. Health
monitoring systems equipped with sensors and embedded with sound signal
detection enable the recognition of drug actuation and could be powerful tools
for reliable audio content analysis. This paper revisits audio pattern
recognition and machine learning techniques for asthma medication adherence
assessment and presents the Respiratory and Drug Actuation (RDA)
Suite(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for
benchmarking and further research. The RDA Suite includes a set of tools for
audio processing, feature extraction and classification and is provided along
with a dataset consisting of respiratory and drug actuation sounds. The
classification models in RDA are implemented based on conventional and advanced
machine learning and deep network architectures. This study provides a
comparative evaluation of the implemented approaches, examines potential
improvements and discusses challenges and future tendencies.",-0.025854297,0.17255843,0.0057697296,C
7010,"The chapter concludes with a discussion on the advantages as well as the
limitations of the method, which motivates further research presented in the upcoming
chapters.","It further describes
experiments on metric learning for alignment and demonstrates the applicability of
this method in various scenarios encompassing diﬀerent instrumentation and acoustic
settings.",3.1.,2022-05-31 16:45:25+00:00,Towards Context-Aware Neural Performance-Score Synchronisation,cs.SD,"['cs.SD', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Ruchit Agrawal')],"Music can be represented in multiple forms, such as in the audio form as a
recording of a performance, in the symbolic form as a computer readable score,
or in the image form as a scan of the sheet music. Music synchronisation
provides a way to navigate among multiple representations of music in a unified
manner by generating an accurate mapping between them, lending itself
applicable to a myriad of domains like music education, performance analysis,
automatic accompaniment and music editing. Traditional synchronisation methods
compute alignment using knowledge-driven and stochastic approaches, typically
employing handcrafted features. These methods are often unable to generalise
well to different instruments, acoustic environments and recording conditions,
and normally assume complete structural agreement between the performances and
the scores. This PhD furthers the development of performance-score
synchronisation research by proposing data-driven, context-aware alignment
approaches, on three fronts: Firstly, I replace the handcrafted features by
employing a metric learning based approach that is adaptable to different
acoustic settings and performs well in data-scarce conditions. Secondly, I
address the handling of structural differences between the performances and
scores, which is a common limitation of standard alignment methods. Finally, I
eschew the reliance on both feature engineering and dynamic programming, and
propose a completely data-driven synchronisation method that computes
alignments using a neural framework, whilst also being robust to structural
differences between the performances and scores.",-0.017514212,0.1913798,0.11796286,C
7011,"Future work  160

step in my opinion to foster further research in this direction.","The creation of such datasets
is therefore an important bottleneck for future developments and is the next logical
6.2.","Hierarchical neural
frameworks drawing inspiration from Müller et al.",2022-05-31 16:45:25+00:00,Towards Context-Aware Neural Performance-Score Synchronisation,cs.SD,"['cs.SD', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Ruchit Agrawal')],"Music can be represented in multiple forms, such as in the audio form as a
recording of a performance, in the symbolic form as a computer readable score,
or in the image form as a scan of the sheet music. Music synchronisation
provides a way to navigate among multiple representations of music in a unified
manner by generating an accurate mapping between them, lending itself
applicable to a myriad of domains like music education, performance analysis,
automatic accompaniment and music editing. Traditional synchronisation methods
compute alignment using knowledge-driven and stochastic approaches, typically
employing handcrafted features. These methods are often unable to generalise
well to different instruments, acoustic environments and recording conditions,
and normally assume complete structural agreement between the performances and
the scores. This PhD furthers the development of performance-score
synchronisation research by proposing data-driven, context-aware alignment
approaches, on three fronts: Firstly, I replace the handcrafted features by
employing a metric learning based approach that is adaptable to different
acoustic settings and performs well in data-scarce conditions. Secondly, I
address the handling of structural differences between the performances and
scores, which is a common limitation of standard alignment methods. Finally, I
eschew the reliance on both feature engineering and dynamic programming, and
propose a completely data-driven synchronisation method that computes
alignments using a neural framework, whilst also being robust to structural
differences between the performances and scores.",-0.10407499,0.025127746,-0.08927563,A
7254,"We also release
                                          our evaluation platform SPEAKERGUARD to foster further research.",This work sheds further light on the research directions in this ﬁeld.,"Index Terms—Speaker recognition, adversarial defenses, adversarial examples, input transformation, adversarial training

                                                                                                          !",2022-06-07 15:38:27+00:00,Towards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition,cs.SD,"['cs.SD', 'cs.AI', 'cs.CR', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Guangke Chen'), arxiv.Result.Author('Zhe Zhao'), arxiv.Result.Author('Fu Song'), arxiv.Result.Author('Sen Chen'), arxiv.Result.Author('Lingling Fan'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Jiashui Wang')]","Speaker recognition systems (SRSs) have recently been shown to be vulnerable
to adversarial attacks, raising significant security concerns. In this work, we
systematically investigate transformation and adversarial training based
defenses for securing SRSs. According to the characteristic of SRSs, we present
22 diverse transformations and thoroughly evaluate them using 7 recent
promising adversarial attacks (4 white-box and 3 black-box) on speaker
recognition. With careful regard for best practices in defense evaluations, we
analyze the strength of transformations to withstand adaptive attacks. We also
evaluate and understand their effectiveness against adaptive attacks when
combined with adversarial training. Our study provides lots of useful insights
and findings, many of them are new or inconsistent with the conclusions in the
image and speech recognition domains, e.g., variable and constant bit rate
speech compressions have different performance, and some non-differentiable
transformations remain effective against current promising evasion techniques
which often work well in the image domain. We demonstrate that the proposed
novel feature-level transformation combined with adversarial training is rather
effective compared to the sole adversarial training in a complete white-box
setting, e.g., increasing the accuracy by 13.62% and attack cost by two orders
of magnitude, while other transformations do not necessarily improve the
overall defense capability. This work sheds further light on the research
directions in this field. We also release our evaluation platform SPEAKERGUARD
to foster further research.",0.20586589,0.11219,-0.11382122,B
7255,"We release      tasks: CSI with enrollment (CSI-E) and CSI without enroll-
  our platform to foster further research in this direction    ment (CSI-NE).","Moreover, CSI could be classiﬁed into two sub-
  tion metrics, and diverse defense solutions.",CSI-E exactly follows the above description.,2022-06-07 15:38:27+00:00,Towards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition,cs.SD,"['cs.SD', 'cs.AI', 'cs.CR', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Guangke Chen'), arxiv.Result.Author('Zhe Zhao'), arxiv.Result.Author('Fu Song'), arxiv.Result.Author('Sen Chen'), arxiv.Result.Author('Lingling Fan'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Jiashui Wang')]","Speaker recognition systems (SRSs) have recently been shown to be vulnerable
to adversarial attacks, raising significant security concerns. In this work, we
systematically investigate transformation and adversarial training based
defenses for securing SRSs. According to the characteristic of SRSs, we present
22 diverse transformations and thoroughly evaluate them using 7 recent
promising adversarial attacks (4 white-box and 3 black-box) on speaker
recognition. With careful regard for best practices in defense evaluations, we
analyze the strength of transformations to withstand adaptive attacks. We also
evaluate and understand their effectiveness against adaptive attacks when
combined with adversarial training. Our study provides lots of useful insights
and findings, many of them are new or inconsistent with the conclusions in the
image and speech recognition domains, e.g., variable and constant bit rate
speech compressions have different performance, and some non-differentiable
transformations remain effective against current promising evasion techniques
which often work well in the image domain. We demonstrate that the proposed
novel feature-level transformation combined with adversarial training is rather
effective compared to the sole adversarial training in a complete white-box
setting, e.g., increasing the accuracy by 13.62% and attack cost by two orders
of magnitude, while other transformations do not necessarily improve the
overall defense capability. This work sheds further light on the research
directions in this field. We also release our evaluation platform SPEAKERGUARD
to foster further research.",-0.1730042,-0.3474641,0.12945805,A
7256,"We
pointed out many possible future works in both adversarial                [19] R. Olivier, B. Raj, and M. Shah, “High-frequency adversarial
attacks and defenses in the speaker recognition domain, and                       defense for speech and audio,” in ICASSP, 2021.
released our evaluation platform SPEAKERGUARD to foster
further research.","This opens up a new research direction                      INTERSPEECH, 2021.
on transformations for mitigating adversarial examples.","[20] F. Trame`r, N. Carlini, W. Brendel, and A. Madry, “On adaptive
                                                                                  attacks to adversarial example defenses,” in NeurIPS, 2020.",2022-06-07 15:38:27+00:00,Towards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition,cs.SD,"['cs.SD', 'cs.AI', 'cs.CR', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Guangke Chen'), arxiv.Result.Author('Zhe Zhao'), arxiv.Result.Author('Fu Song'), arxiv.Result.Author('Sen Chen'), arxiv.Result.Author('Lingling Fan'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Jiashui Wang')]","Speaker recognition systems (SRSs) have recently been shown to be vulnerable
to adversarial attacks, raising significant security concerns. In this work, we
systematically investigate transformation and adversarial training based
defenses for securing SRSs. According to the characteristic of SRSs, we present
22 diverse transformations and thoroughly evaluate them using 7 recent
promising adversarial attacks (4 white-box and 3 black-box) on speaker
recognition. With careful regard for best practices in defense evaluations, we
analyze the strength of transformations to withstand adaptive attacks. We also
evaluate and understand their effectiveness against adaptive attacks when
combined with adversarial training. Our study provides lots of useful insights
and findings, many of them are new or inconsistent with the conclusions in the
image and speech recognition domains, e.g., variable and constant bit rate
speech compressions have different performance, and some non-differentiable
transformations remain effective against current promising evasion techniques
which often work well in the image domain. We demonstrate that the proposed
novel feature-level transformation combined with adversarial training is rather
effective compared to the sole adversarial training in a complete white-box
setting, e.g., increasing the accuracy by 13.62% and attack cost by two orders
of magnitude, while other transformations do not necessarily improve the
overall defense capability. This work sheds further light on the research
directions in this field. We also release our evaluation platform SPEAKERGUARD
to foster further research.",0.12646492,0.14040908,-0.20258826,B
7536,"First, further research on the δ-modiﬁcation or related            The Table VIII column ""Background"", shows the experi-
modiﬁcations should be conducted.","Choice of the NN architectures: airport background
                                                                removal
   This work opens several doors for future directions.","For example, learning         ment from Appendix A-A applied to the ﬁrst fold of the
the L-WPT on diﬀerent noise levels would show if the            Background denoising task.",2022-06-13 13:05:58+00:00,Robust Time Series Denoising with Learnable Wavelet Packet Transform,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Gaetan Frusque'), arxiv.Result.Author('Olga Fink')]","In many applications, signal denoising is often the first pre-processing step
before any subsequent analysis or learning task. In this paper, we propose to
apply a deep learning denoising model inspired by a signal processing, a
learnable version of wavelet packet transform. The proposed algorithm has
signficant learning capabilities with few interpretable parameters and has an
intuitive initialisation. We propose a post-learning modification of the
parameters to adapt the denoising to different noise levels. We evaluate the
performance of the proposed methodology on two case studies and compare it to
other state of the art approaches, including wavelet schrinkage denoising,
convolutional neural network, autoencoder and U-net deep models. The first case
study is based on designed functions that have typically been used to study
denoising properties of the algorithms. The second case study is an audio
background removal task. We demonstrate how the proposed algorithm relates to
the universality of signal processing methods and the learning capabilities of
deep learning approaches. In particular, we evaluate the obtained denoising
performances on structured noisy signals inside and outside the classes used
for training. In addition to having good performance in denoising signals
inside and outside to the training class, our method shows to be particularly
robust when different noise levels, noise types and artifacts are added.",-0.40542182,-0.087062255,-0.14694294,A
7537,"First, further research on the δ-modiﬁcation or related mod-
iﬁcations should be conducted.",This work opens several doors for future directions.,"For example, learning the L-WPT on diﬀerent noise levels would show if the kernels
remain similar.This would tell us how optimal the delta-modiﬁcation is.",2022-06-13 13:05:58+00:00,Robust Time Series Denoising with Learnable Wavelet Packet Transform,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Gaetan Frusque'), arxiv.Result.Author('Olga Fink')]","In many applications, signal denoising is often the first pre-processing step
before any subsequent analysis or learning task. In this paper, we propose to
apply a deep learning denoising model inspired by a signal processing, a
learnable version of wavelet packet transform. The proposed algorithm has
signficant learning capabilities with few interpretable parameters and has an
intuitive initialisation. We propose a post-learning modification of the
parameters to adapt the denoising to different noise levels. We evaluate the
performance of the proposed methodology on two case studies and compare it to
other state of the art approaches, including wavelet schrinkage denoising,
convolutional neural network, autoencoder and U-net deep models. The first case
study is based on designed functions that have typically been used to study
denoising properties of the algorithms. The second case study is an audio
background removal task. We demonstrate how the proposed algorithm relates to
the universality of signal processing methods and the learning capabilities of
deep learning approaches. In particular, we evaluate the obtained denoising
performances on structured noisy signals inside and outside the classes used
for training. In addition to having good performance in denoising signals
inside and outside to the training class, our method shows to be particularly
robust when different noise levels, noise types and artifacts are added.",-0.36664534,-0.085120656,-0.13210174,A
7538,"First, further research on the δ-modiﬁcation or related mod-
iﬁcations should be conducted.","14
    This work opens several doors for future directions.","For example, learning the L-WPT on diﬀerent noise levels would show if the kernels
remain similar.This would tell us how optimal the delta-modiﬁcation is.",2022-06-13 13:05:58+00:00,Robust Time Series Denoising with Learnable Wavelet Packet Transform,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Gaetan Frusque'), arxiv.Result.Author('Olga Fink')]","In many applications, signal denoising is often the first pre-processing step
before any subsequent analysis or learning task. In this paper, we propose to
apply a deep learning denoising model inspired by a signal processing, a
learnable version of wavelet packet transform. The proposed algorithm has
signficant learning capabilities with few interpretable parameters and has an
intuitive initialisation. We propose a post-learning modification of the
parameters to adapt the denoising to different noise levels. We evaluate the
performance of the proposed methodology on two case studies and compare it to
other state of the art approaches, including wavelet schrinkage denoising,
convolutional neural network, autoencoder and U-net deep models. The first case
study is based on designed functions that have typically been used to study
denoising properties of the algorithms. The second case study is an audio
background removal task. We demonstrate how the proposed algorithm relates to
the universality of signal processing methods and the learning capabilities of
deep learning approaches. In particular, we evaluate the obtained denoising
performances on structured noisy signals inside and outside the classes used
for training. In addition to having good performance in denoising signals
inside and outside to the training class, our method shows to be particularly
robust when different noise levels, noise types and artifacts are added.",-0.35693032,-0.101647645,-0.13325009,A_centroid
7539,"First, further research on the δ-modiﬁcation or related mod-
iﬁcations should be conducted.","14
    This work opens several doors for future directions.","For example, learning the L-WPT on diﬀerent noise levels would show if the kernels
remain similar.This would tell us how optimal the delta-modiﬁcation is.",2022-06-13 13:05:58+00:00,Robust Time Series Denoising with Learnable Wavelet Packet Transform,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Gaetan Frusque'), arxiv.Result.Author('Olga Fink')]","Signal denoising is a key preprocessing step for many applications, as the
performance of a learning task is closely related to the quality of the input
data. In this paper, we apply a signal processing based deep neural network
architecture, a learnable extension of the wavelet packet transform. As main
advantages, this model has few parameters, an intuitive initialization and
strong learning capabilities. Moreover, we show that it is possible to easily
modify the parameters of the model after the training step to tailor to
different noise intensities. Two case studies are conducted to compare this
model with the state of the art and commonly used denoising procedures. The
first experiment uses standard signals to study denoising properties of the
algorithms. The second experiment is a real application with the objective to
remove audio background noises. We show that the learnable wavelet packet
transform has the learning capabilities of deep learning methods while
maintaining the robustness of standard signal processing approaches. More
specifically, we demonstrate that our approach maintains excellent denoising
performances on signal classes separate from those used during the training
step. Moreover, the learnable wavelet packet transform was found to be robust
when different noise intensities, noise varieties and artifacts are considered.",-0.35693032,-0.101647645,-0.13325009,A
8047,"Caveats
Shor et al., 2022; Baevski et al., 2020), we were surprised
to discover that, on the emotional intensity task, our best   Machine perception models of apparent emotional expres-
pre-trained Conformer-based models only marginally out-       sion, including in vocal bursts, remain an open area of in-
performed (0.648 vs 0.645) relatively smaller models like     vestigation where further research is needed.","However, given the success of large self-supervised repre-
sentations for downstream acoustic tasks (Hsu et al., 2021;   4.","The models
ResNet34 that were trained directly on HUME-VB without        in this work do not aim to infer the internal emotional state
any pre-training.",2022-06-24 21:42:16+00:00,Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Josh Belanich'), arxiv.Result.Author('Krishna Somandepalli'), arxiv.Result.Author('Brian Eoff'), arxiv.Result.Author('Brendan Jou')]","This technical report presents the modeling approaches used in our submission
to the ICML Expressive Vocalizations Workshop & Competition multitask track
(ExVo-MultiTask). We first applied image classification models of various sizes
on mel-spectrogram representations of the vocal bursts, as is standard in sound
event detection literature. Results from these models show an increase of
21.24% over the baseline system with respect to the harmonic mean of the task
metrics, and comprise our team's main submission to the MultiTask track. We
then sought to characterize the headroom in the MultiTask track by applying a
large pre-trained Conformer model that previously achieved state-of-the-art
results on paralinguistic tasks like speech emotion recognition and mask
detection. We additionally investigated the relationship between the sub-tasks
of emotional expression, country of origin, and age prediction, and discovered
that the best performing models are trained as single-task models, questioning
whether the problem truly benefits from a multitask setting.",0.27618456,0.012923263,-0.12321071,B
8261,"However,
further research is required to accurately evaluate the eﬀect of training for the
soniﬁcation method and for developing a full picture of its learning curve.","Because the visual navigation was more famil-
iar for both groups, they could improve their speed on this modality.",Table 4.,2022-06-30 13:49:10+00:00,Sonification as a Reliable Alternative to Conventional Visual Surgical Navigation,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Sasan Matinfar'), arxiv.Result.Author('Mehrdad Salehi'), arxiv.Result.Author('Daniel Suter'), arxiv.Result.Author('Matthias Seibold'), arxiv.Result.Author('Navid Navab'), arxiv.Result.Author('Shervin Dehghani'), arxiv.Result.Author('Florian Wanivenhaus'), arxiv.Result.Author('Philipp Fürnstahl'), arxiv.Result.Author('Mazda Farshad'), arxiv.Result.Author('Nassir Navab')]","Despite the undeniable advantages of image-guided surgical assistance systems
in terms of accuracy, such systems have not yet fully met surgeons' needs or
expectations regarding usability, time efficiency, and their integration into
the surgical workflow. On the other hand, perceptual studies have shown that
presenting independent but causally correlated information via multimodal
feedback involving different sensory modalities can improve task performance.
This article investigates an alternative method for computer-assisted surgical
navigation, introduces a novel sonification methodology for navigated pedicle
screw placement, and discusses advanced solutions based on multisensory
feedback. The proposed method comprises a novel sonification solution for
alignment tasks in four degrees of freedom based on frequency modulation (FM)
synthesis. We compared the resulting accuracy and execution time of the
proposed sonification method with visual navigation, which is currently
considered the state of the art. We conducted a phantom study in which 17
surgeons executed the pedicle screw placement task in the lumbar spine, guided
by either the proposed sonification-based or the traditional visual navigation
method. The results demonstrated that the proposed method is as accurate as the
state of the art while decreasing the surgeon's need to focus on visual
navigation displays instead of the natural focus on surgical tools and targeted
anatomy during task execution.",-0.17493041,-0.22781095,0.0016284697,A
8262,"Soniﬁcation as a Reliable Alternative ...  19

    Further, previous soniﬁcation research [46,47,48,49,50,51,52,73] has investi-
gated diﬀerent soniﬁcation strategies for navigation, providing the preliminary
basis for further research.","Future research in
computer-assisted surgery can focus on investigating the possible answers to
such fundamental questions in the application ﬁeld of surgical navigation, as the
foundation is well established in cognitive science [86,87].","Future studies should take the cons and pros of soni-
ﬁcation paradigms into account.",2022-06-30 13:49:10+00:00,Sonification as a Reliable Alternative to Conventional Visual Surgical Navigation,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Sasan Matinfar'), arxiv.Result.Author('Mehrdad Salehi'), arxiv.Result.Author('Daniel Suter'), arxiv.Result.Author('Matthias Seibold'), arxiv.Result.Author('Navid Navab'), arxiv.Result.Author('Shervin Dehghani'), arxiv.Result.Author('Florian Wanivenhaus'), arxiv.Result.Author('Philipp Fürnstahl'), arxiv.Result.Author('Mazda Farshad'), arxiv.Result.Author('Nassir Navab')]","Despite the undeniable advantages of image-guided surgical assistance systems
in terms of accuracy, such systems have not yet fully met surgeons' needs or
expectations regarding usability, time efficiency, and their integration into
the surgical workflow. On the other hand, perceptual studies have shown that
presenting independent but causally correlated information via multimodal
feedback involving different sensory modalities can improve task performance.
This article investigates an alternative method for computer-assisted surgical
navigation, introduces a novel sonification methodology for navigated pedicle
screw placement, and discusses advanced solutions based on multisensory
feedback. The proposed method comprises a novel sonification solution for
alignment tasks in four degrees of freedom based on frequency modulation (FM)
synthesis. We compared the resulting accuracy and execution time of the
proposed sonification method with visual navigation, which is currently
considered the state of the art. We conducted a phantom study in which 17
surgeons executed the pedicle screw placement task in the lumbar spine, guided
by either the proposed sonification-based or the traditional visual navigation
method. The results demonstrated that the proposed method is as accurate as the
state of the art while decreasing the surgeon's need to focus on visual
navigation displays instead of the natural focus on surgical tools and targeted
anatomy during task execution.",-0.12532221,-0.2819711,0.079955906,A
8348,"We further study our approach on
the DATATANG-dialog dataset to see if there is still space for                                   (b) conditional decoder
performance improvement when i-vector is adopted.","I-vector can introduce additional
speaker context and reduce the speaker mismatch between the                    (a) baseline                                                                                                           
training set and the test set.","Figure 3: Attention scores of 3-th decoder layers, a darker color
     The i-vector estimator is trained with all the training data   indicates a higher score for the character.",2022-07-02 17:17:47+00:00,Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Kun Wei'), arxiv.Result.Author('Pengcheng Guo'), arxiv.Result.Author('Ning Jiang')]","Transformer-based models have demonstrated their effectiveness in automatic
speech recognition (ASR) tasks and even shown superior performance over the
conventional hybrid framework. The main idea of Transformers is to capture the
long-range global context within an utterance by self-attention layers.
However, for scenarios like conversational speech, such utterance-level
modeling will neglect contextual dependencies that span across utterances. In
this paper, we propose to explicitly model the inter-sentential information in
a Transformer based end-to-end architecture for conversational speech
recognition. Specifically, for the encoder network, we capture the contexts of
previous speech and incorporate such historic information into current input by
a context-aware residual attention mechanism. For the decoder, the prediction
of current utterance is also conditioned on the historic linguistic information
through a conditional decoder framework. We show the effectiveness of our
proposed method on several open-source dialogue corpora and the proposed method
consistently improved the performance from the utterance-level
Transformer-based ASR models.",0.19760019,-0.023828898,-0.30357757,B
8482,"Re-         the noncausality of our codec, and further research on causal
                                       cent advances in speech coding follow progress in speech syn-        Transformer models is required to precisely quantify the gains
                                       thesis [8–10] by replacing the decoder [11–13] as well as the        resulting from speech features that exploit long-distance depen-
                                       quantizer [14] with a machine-learning (ML) based model that         dencies.","In classical coding methods [1–7], all       quality differences in synthesized speech can be attributed to
                                       processing was based on knowledge of human experts only.",signiﬁcantly improves the coding quality.,2022-07-05 18:52:11+00:00,Ultra-Low-Bitrate Speech Coding with Pretrained Transformers,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Ali Siahkoohi'), arxiv.Result.Author('Michael Chinen'), arxiv.Result.Author('Tom Denton'), arxiv.Result.Author('W. Bastiaan Kleijn'), arxiv.Result.Author('Jan Skoglund')]","Speech coding facilitates the transmission of speech over low-bandwidth
networks with minimal distortion. Neural-network based speech codecs have
recently demonstrated significant improvements in quality over traditional
approaches. While this new generation of codecs is capable of synthesizing
high-fidelity speech, their use of recurrent or convolutional layers often
restricts their effective receptive fields, which prevents them from
compressing speech efficiently. We propose to further reduce the bitrate of
neural speech codecs through the use of pretrained Transformers, capable of
exploiting long-range dependencies in the input signal due to their inductive
bias. As such, we use a pretrained Transformer in tandem with a convolutional
encoder, which is trained end-to-end with a quantizer and a generative
adversarial net decoder. Our numerical experiments show that supplementing the
convolutional encoder of a neural speech codec with Transformer speech
embeddings yields a speech codec with a bitrate of $600\,\mathrm{bps}$ that
outperforms the original neural speech codec in synthesized speech quality when
trained at the same bitrate. Subjective human evaluations suggest that the
quality of the resulting codec is comparable or better than that of
conventional codecs operating at three to four times the rate.",0.13702895,0.113420576,-0.17144832,B
8860,"Besides the pre-training setup introduced
in the original paper, we further study a widely compared setup where the models are additionally
supervisedly pre-trained with AudioSet data and labels before ﬁne-tuning on ESC-50.","C.1 ESC-50 with AudioSet-2M Supervised Pre-training

ESC-50 is designed for environmental sound classiﬁcation.","Table 4
summarizes the results under this setup where our Audio-MAE achieves state of the art accuracy with
the additional AudioSet-2M supervised pre-training.",2022-07-13 17:59:55+00:00,Masked Autoencoders that Listen,cs.SD,"['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Po-Yao'), arxiv.Result.Author('Huang'), arxiv.Result.Author('Hu Xu'), arxiv.Result.Author('Juncheng Li'), arxiv.Result.Author('Alexei Baevski'), arxiv.Result.Author('Michael Auli'), arxiv.Result.Author('Wojciech Galuba'), arxiv.Result.Author('Florian Metze'), arxiv.Result.Author('Christoph Feichtenhofer')]","This paper studies a simple extension of image-based Masked Autoencoders
(MAE) to self-supervised representation learning from audio spectrograms.
Following the Transformer encoder-decoder design in MAE, our Audio-MAE first
encodes audio spectrogram patches with a high masking ratio, feeding only the
non-masked tokens through encoder layers. The decoder then re-orders and
decodes the encoded context padded with mask tokens, in order to reconstruct
the input spectrogram. We find it beneficial to incorporate local window
attention in the decoder, as audio spectrograms are highly correlated in local
time and frequency bands. We then fine-tune the encoder with a lower masking
ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art
performance on six audio and speech classification tasks, outperforming other
recent models that use external supervised pre-training. The code and models
will be at https://github.com/facebookresearch/AudioMAE.",-0.00077004544,0.39498717,0.0017840397,C
8861,"Besides the pre-training setup introduced
in the original paper, we further study a widely compared setup where the models are additionally
supervisedly pre-trained with AudioSet data and labels before ﬁne-tuning on ESC-50.","C.1 ESC-50 with AudioSet-2M Supervised Pre-training

ESC-50 is designed for environmental sound classiﬁcation.","Table 4
summarizes the results under this setup where our Audio-MAE achieves state of the art accuracy with
the additional AudioSet-2M supervised pre-training.",2022-07-13 17:59:55+00:00,Masked Autoencoders that Listen,cs.SD,"['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Po-Yao Huang'), arxiv.Result.Author('Hu Xu'), arxiv.Result.Author('Juncheng Li'), arxiv.Result.Author('Alexei Baevski'), arxiv.Result.Author('Michael Auli'), arxiv.Result.Author('Wojciech Galuba'), arxiv.Result.Author('Florian Metze'), arxiv.Result.Author('Christoph Feichtenhofer')]","This paper studies a simple extension of image-based Masked Autoencoders
(MAE) to self-supervised representation learning from audio spectrograms.
Following the Transformer encoder-decoder design in MAE, our Audio-MAE first
encodes audio spectrogram patches with a high masking ratio, feeding only the
non-masked tokens through encoder layers. The decoder then re-orders and
decodes the encoded context padded with mask tokens, in order to reconstruct
the input spectrogram. We find it beneficial to incorporate local window
attention in the decoder, as audio spectrograms are highly correlated in local
time and frequency bands. We then fine-tune the encoder with a lower masking
ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art
performance on six audio and speech classification tasks, outperforming other
recent models that use external supervised pre-training. The code and models
will be at https://github.com/facebookresearch/AudioMAE.",-0.00077004544,0.39498717,0.0017840397,C
9288,"We further study the reason for the performance                    3 2016, pp.","IEEE,
addition.",2608–2612.,2022-07-07 02:53:39+00:00,Domain Adapting Speech Emotion Recognition modals to real-world scenario with Deep Reinforcement Learning,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Thejan Rajapakshe'), arxiv.Result.Author('Rajib Rana'), arxiv.Result.Author('Sara Khalifa')]","Deep reinforcement learning has been a popular training paradigm as deep
learning has gained popularity in the field of machine learning. Domain
adaptation allows us to transfer knowledge learnt by a model across domains
after a phase of training. The inability to adapt an existing model to a
real-world domain is one of the shortcomings of current domain adaptation
algorithms. We present a deep reinforcement learning-based strategy for
adapting a pre-trained model to a newer domain while interacting with the
environment and collecting continual feedback. This method was used on the
Speech Emotion Recognition task, which included both cross-corpus and
cross-language domain adaption schema. Furthermore, it demonstrates that in a
real-world environment, our approach outperforms the supervised learning
strategy by 42% and 20% in cross-corpus and cross-language schema,
respectively.",-0.33273697,-0.37105983,0.13441518,A
9327,"Thus, soundscape emotion recognition (SER)
                                        competitive performance, with R2 > 0.86 and R2 > 0.63 (values      requires further research to support perception and context
                                        obtained after a cross-validation procedure), respectively.",The suggested linear models provide very good and    and unnoticed.,"descriptors [12], [25].",2022-07-26 08:49:26+00:00,An exhaustive variable selection study for linear models of soundscape emotions: rankings and Gibbs analysis,cs.SD,"['cs.SD', 'eess.AS', 'eess.SP', 'stat.CO', 'stat.ME']","[arxiv.Result.Author('R. San Millán-Castillo'), arxiv.Result.Author('L. Martino'), arxiv.Result.Author('E. Morgado'), arxiv.Result.Author('F. Llorente')]","In the last decade, soundscapes have become one of the most active topics in
Acoustics, providing a holistic approach to the acoustic environment, which
involves human perception and context. Soundscapes-elicited emotions are
central and substantially subtle and unnoticed (compared to speech or music).
Currently, soundscape emotion recognition is a very active topic in the
literature. We provide an exhaustive variable selection study (i.e., a
selection of the soundscapes indicators) to a well-known dataset
(emo-soundscapes). We consider linear soundscape emotion models for two
soundscapes descriptors: arousal and valence.
  Several ranking schemes and procedures for selecting the number of variables
are applied. We have also performed an alternating optimization scheme for
obtaining the best sequences keeping fixed a certain number of features.
Furthermore, we have designed a novel technique based on Gibbs sampling, which
provides a more complete and clear view of the relevance of each variable.
Finally, we have also compared our results with the analysis obtained by the
classical methods based on p-values. As a result of our study, we suggest two
simple and parsimonious linear models of only 7 and 16 variables (within the
122 possible features) for the two outputs (arousal and valence), respectively.
The suggested linear models provide very good and competitive performance, with
$R^2>0.86$ and $R^2>0.63$ (values obtained after a cross-validation procedure),
respectively.",0.26570803,-0.042643435,0.19197261,B
9673,"𝑃 𝐶𝐸𝑅(𝐶(𝑥), 𝐶(𝑥 + 𝑣) > 𝑡) ≥ 𝛿             (4.5)

systems are not a streaming ASR system and mainly attacked                ∼

under WTA circumstances, so further research is needed.","∥ 𝑣 ∥ < 𝜖

works have practical significance, but the attacked ASR             2.","They initialized a perturbation V and updated V by iterating

Unlike controlling the size of perturbations on images to           through the speech signals Xi in the speech examples

make them imperceptible, adversarial examples against ASR           distribution μ.",2022-08-03 06:46:42+00:00,Adversarial Attacks on ASR Systems: An Overview,cs.SD,"['cs.SD', 'cs.AI', 'cs.CL', 'cs.CR', 'eess.AS']","[arxiv.Result.Author('Xiao Zhang'), arxiv.Result.Author('Hao Tan'), arxiv.Result.Author('Xuan Huang'), arxiv.Result.Author('Denghui Zhang'), arxiv.Result.Author('Keke Tang'), arxiv.Result.Author('Zhaoquan Gu')]","With the development of hardware and algorithms, ASR(Automatic Speech
Recognition) systems evolve a lot. As The models get simpler, the difficulty of
development and deployment become easier, ASR systems are getting closer to our
life. On the one hand, we often use APPs or APIs of ASR to generate subtitles
and record meetings. On the other hand, smart speaker and self-driving car rely
on ASR systems to control AIoT devices. In past few years, there are a lot of
works on adversarial examples attacks against ASR systems. By adding a small
perturbation to the waveforms, the recognition results make a big difference.
In this paper, we describe the development of ASR system, different assumptions
of attacks, and how to evaluate these attacks. Next, we introduce the current
works on adversarial examples attacks from two attack assumptions: white-box
attack and black-box attack. Different from other surveys, we pay more
attention to which layer they perturb waveforms in ASR system, the relationship
between these attacks, and their implementation methods. We focus on the effect
of their works.",-0.026708465,-0.11072801,-0.20602198,B
10231,"The limitations
the potential to make LFCC a relatively more suitable feature for      suggest some areas for further research.","Therefore, this reason has      In this section, we discuss some of its limitations.",detecting vocoder fingerprints.,2022-08-20 09:23:21+00:00,An Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']","[arxiv.Result.Author('Xinrui Yan'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Chenglong Wang'), arxiv.Result.Author('Haoxin Ma'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Shiming Wang'), arxiv.Result.Author('Ruibo Fu')]","Many effective attempts have been made for fake audio detection. However,
they can only provide detection results but no countermeasures to curb this
harm. For many related practical applications, what model or algorithm
generated the fake audio also is needed. Therefore, We propose a new problem
for detecting vocoder fingerprints of fake audio. Experiments are conducted on
the datasets synthesized by eight state-of-the-art vocoders. We have
preliminarily explored the features and model architectures. The t-SNE
visualization shows that different vocoders generate distinct vocoder
fingerprints.",-0.33045077,0.07748476,-0.089179665,A
10232,"In the
      detecting vocoder fingerprints of eight                                  future, we will conduct further research on the forensics of
      vocoders                                                                 unknown types of vocoders to better achieve the generaliz-
                                                                               ability of vocoder forensics.","• Truly generalized countermeasures: The proposed coun-
5.5 Comparison of the performance for                                          termeasure does not apply well to unknown vocoders.","As shown in Table 3, we have some findings for the detection of
eight types of vocoder fingerprints under three feature input en-      7 CONCLUSIONS
vironments.",2022-08-20 09:23:21+00:00,An Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']","[arxiv.Result.Author('Xinrui Yan'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Chenglong Wang'), arxiv.Result.Author('Haoxin Ma'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Shiming Wang'), arxiv.Result.Author('Ruibo Fu')]","Many effective attempts have been made for fake audio detection. However,
they can only provide detection results but no countermeasures to curb this
harm. For many related practical applications, what model or algorithm
generated the fake audio also is needed. Therefore, We propose a new problem
for detecting vocoder fingerprints of fake audio. Experiments are conducted on
the datasets synthesized by eight state-of-the-art vocoders. We have
preliminarily explored the features and model architectures. The t-SNE
visualization shows that different vocoders generate distinct vocoder
fingerprints.",-0.25908267,0.020445252,-0.041972157,A
10310,"The later emergence of convolutional neural networks           These motivate our further research in the field of deepfake au-
(CNNs) [24] has achieved further performance improvements.","The gaussian mixture model (GMM) is the traditional classifica-
tion model.",As           dio detection.,2022-08-21 05:15:40+00:00,System Fingerprints Detection for DeepFake Audio: An Initial Dataset and Investigation,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']","[arxiv.Result.Author('Xinrui Yan'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Chenglong Wang'), arxiv.Result.Author('Haoxin Ma'), arxiv.Result.Author('Zhengkun Tian'), arxiv.Result.Author('Ruibo Fu')]","Many effective attempts have been made for deepfake audio detection. However,
they can only distinguish between real and fake. For many practical application
scenarios, what tool or algorithm generated the deepfake audio also is needed.
This raises a question: Can we detect the system fingerprints of deepfake
audio? Therefore, this paper conducts a preliminary investigation to detect
system fingerprints of deepfake audio. Experiments are conducted on deepfake
audio datasets from five latest deep-learning speech synthesis systems. The
results show that LFCC features are relatively more suitable for system
fingerprints detection. Moreover, the ResNet achieves the best detection
results among LCNN and x-vector based models. The t-SNE visualization shows
that different speech synthesis systems generate distinct system fingerprints.",-0.24113423,0.16582292,-0.25237828,A
10311,"Fi-
        aspects are worth further research in the future.","Then for each short-time analysis win-
     • We have a deep discussion on the existing limitations of the                     dow, the corresponding spectrum is obtained by FFT; the spectrum
        current system fingerprints detection methods, and what                         is passed through a linear filter set to obtain a linear spectrum.","nally, cepstrum analysis is performed on top of the linear spectrum
                                                                                        to obtain LFCC.",2022-08-21 05:15:40+00:00,System Fingerprints Detection for DeepFake Audio: An Initial Dataset and Investigation,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']","[arxiv.Result.Author('Xinrui Yan'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Chenglong Wang'), arxiv.Result.Author('Haoxin Ma'), arxiv.Result.Author('Zhengkun Tian'), arxiv.Result.Author('Ruibo Fu')]","Many effective attempts have been made for deepfake audio detection. However,
they can only distinguish between real and fake. For many practical application
scenarios, what tool or algorithm generated the deepfake audio also is needed.
This raises a question: Can we detect the system fingerprints of deepfake
audio? Therefore, this paper conducts a preliminary investigation to detect
system fingerprints of deepfake audio. Experiments are conducted on deepfake
audio datasets from five latest deep-learning speech synthesis systems. The
results show that LFCC features are relatively more suitable for system
fingerprints detection. Moreover, the ResNet achieves the best detection
results among LCNN and x-vector based models. The t-SNE visualization shows
that different speech synthesis systems generate distinct system fingerprints.",-0.43043673,-0.065361835,0.17009546,A
10601,"In AES Convention 114. AES, Amsterdam, Netherlands, 8.
the system feels in its prediction, but it was not used in the
evaluation of the system since further research is needed to         [7] Andrew Hawryshkewich, Philippe Pasquier, and Arne Eigenfeldt.","We briefly described clarity (see        meter of musical audio signals: Seeking recurrences in beat segment
Algorithm 1), that is a first metric to describe how confident            descriptors.","2010.
correctly interpret the meaning of this metric.",2022-08-31 09:25:39+00:00,A Real-Time Tempo and Meter Tracking System for Rhythmic Improvis,cs.SD,"['cs.SD', 'cs.MM', 'eess.AS']","[arxiv.Result.Author('Filippo Carnovalini'), arxiv.Result.Author('Antonio Rodà')]","Music is a form of expression that often requires interaction between
players. If one wishes to interact in such a musical way with a computer, it is
necessary for the machine to be able to interpret the input given by the human
to find its musical meaning. In this work, we propose a system capable of
detecting basic rhythmic features that can allow an application to synchronize
its output with the rhythm given by the user, without having any prior
agreement or requirement on the possible input. The system is described in
detail and an evaluation is given through simulation using quantitative
metrics. The evaluation shows that the system can detect tempo and meter
consistently under certain settings, and could be a solid base for further
developments leading to a system robust to rhythmically changing inputs.",0.024342693,0.043684714,0.3297919,C
10636,"reﬂect a structural hierarchy, but at least this ﬁnding offers
insight into how music generation might be improved, and                 Returning to our consideration of structure over time:
it raises questions for further study.",This does not mean that “good music” must                ration consists of new material and the rest is repetition.,How does surprise vary with structure?,2022-09-01 02:22:11+00:00,What is missing in deep music generation? A study of repetition and structure in popular music,cs.SD,"['cs.SD', 'cs.IR', 'eess.AS']","[arxiv.Result.Author('Shuqi Dai'), arxiv.Result.Author('Huiran Yu'), arxiv.Result.Author('Roger B. Dannenberg')]","Structure is one of the most essential aspects of music, and music structure
is commonly indicated through repetition. However, the nature of repetition and
structure in music is still not well understood, especially in the context of
music generation, and much remains to be explored with Music Information
Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and
American) illustrate important music construction principles: (1) structure
exists at multiple hierarchical levels, (2) songs use repetition and limited
vocabulary so that individual songs do not follow general statistics of song
collections, (3) structure interacts with rhythm, melody, harmony, and
predictability, and (4) over the course of a song, repetition is not random,
but follows a general trend as revealed by cross-entropy. These and other
findings offer challenges as well as opportunities for deep-learning music
generation and suggest new formal music criteria and evaluation methods. Music
from recent music generation systems is analyzed and compared to human-composed
music in our datasets, often revealing striking differences from a structural
perspective.",0.047186725,-0.13350649,0.5766045,C
10637,"Also, the probabilities of differ-
matters to listeners or simply reﬂects composers’ inten-        ent pitches at different phrase-level positions only change
tions requires further research.",Whether this organization        the real POP909 phrases.,slightly in the analyzed outputs.,2022-09-01 02:22:11+00:00,What is missing in deep music generation? A study of repetition and structure in popular music,cs.SD,"['cs.SD', 'cs.IR', 'eess.AS']","[arxiv.Result.Author('Shuqi Dai'), arxiv.Result.Author('Huiran Yu'), arxiv.Result.Author('Roger B. Dannenberg')]","Structure is one of the most essential aspects of music, and music structure
is commonly indicated through repetition. However, the nature of repetition and
structure in music is still not well understood, especially in the context of
music generation, and much remains to be explored with Music Information
Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and
American) illustrate important music construction principles: (1) structure
exists at multiple hierarchical levels, (2) songs use repetition and limited
vocabulary so that individual songs do not follow general statistics of song
collections, (3) structure interacts with rhythm, melody, harmony, and
predictability, and (4) over the course of a song, repetition is not random,
but follows a general trend as revealed by cross-entropy. These and other
findings offer challenges as well as opportunities for deep-learning music
generation and suggest new formal music criteria and evaluation methods. Music
from recent music generation systems is analyzed and compared to human-composed
music in our datasets, often revealing striking differences from a structural
perspective.",0.24376,-0.07167269,0.29044294,B
10870,"Introduction                                         However, several key aspects of paralinguistic control de-
                                                                                                            mand further study in speech synthesis research.",1.,"The aforemen-
                                       The human ability to control and manipulate the voice both           tioned vocal attributes, and, more importantly, their descrip-
                                       linguistically and paralinguistically facilitates vocal communi-     tions and categorizations, are often unavailable to the user of
                                       cation of ideas, emotions, personality traits and identity with      a synthesized voice once the voice is built.",2022-09-06 23:34:30+00:00,The Role of Voice Persona in Expressive Communication:An Argument for Relevance in Speech Synthesis Design,cs.SD,"['cs.SD', 'cs.HC', 'cs.SY', 'eess.AS', 'eess.SY']","[arxiv.Result.Author('Camille Noufi'), arxiv.Result.Author('Lloyd May'), arxiv.Result.Author('Jonathan Berger')]","We present an approach to imbuing expressivity in a synthesized voice by
acquiring a thematic analysis of 10 interviews with vocal studies and
performance experts to inform the design framework for a real-time, interactive
vocal persona that would generate compelling and appropriate
contextually-dependent expression. The resultant tone of voice is defined as a
point existing within a continuous, contextually-dependent probability space.
The inclusion of voice persona in synthesized voice can be significant in a
broad range of applications. Of particular interest is the potential impact in
augmentative and assistive communication (AAC) community. Finally, we conclude
with an introduction to our ongoing research investigating the themes of vocal
persona and how they may continue to inform proposed expressive speech
synthesis design frameworks.",0.43782347,0.0026335549,-0.03177028,B
10871,"We use thematic analysis as an
                                        determined parameter management, demand further study               analysis framework of expert interview data, as it speciﬁcally
                                        in speech synthesis research.","However, several key aspects of par-         Zoom video conferencing software, offering ﬂexibility of
                                        alinguistic control, such as contextual awareness and user-         topic-focus to participants.","Furthermore, a more nuanced           allows for the inclusion of an initial hypothesis through a
                                        approach is needed to enable speech synthesis to meaning-           deﬁnition of a priori themes that are then iteratively modiﬁed
                                        fully explore the relationship between speaker identity and
                                        adjustable prosodic control.",2022-09-06 23:34:30+00:00,The Role of Vocal Persona in Natural and Synthesized Speech,cs.SD,"['cs.SD', 'cs.HC', 'cs.SY', 'eess.AS', 'eess.SY']","[arxiv.Result.Author('Camille Noufi'), arxiv.Result.Author('Lloyd May'), arxiv.Result.Author('Jonathan Berger')]","The inclusion of voice persona in synthesized voice can be significant in a
broad range of human-computer-interaction (HCI) applications, including
augmentative and assistive communication (AAC), artistic performance, and
design of virtual agents. We propose a framework to imbue compelling and
contextually-dependent expression within a synthesized voice by introducing the
role of the vocal persona within a synthesis system. In this framework, the
resultant 'tone of voice' is defined as a point existing within a continuous,
contextually-dependent probability space that is traversable by the user of the
voice. We also present initial findings of a thematic analysis of 10 interviews
with vocal studies and performance experts to further understand the role of
the vocal persona within a natural communication ecology. The themes identified
are then used to inform the design of the aforementioned framework.",0.34959057,-0.026235048,0.015223943,B
11004,"Open-Sourced SSL Dataset

    In order to contribute to further research in the ﬁeld of COVID-19 cough
sound diagnosis, we have added the training labels obtained through our SSL
majority agreement scheme to the latest version of the COUGHVID dataset
public Zenodo repository.",4.3.,"This version has been expanded to include all of the
crowdsourced recordings obtained through October 2021, whereas the original
dataset only contained recordings uploaded through December 2020.",2022-09-09 15:44:26+00:00,A Semi-Supervised Algorithm for Improving the Consistency of Crowdsourced Datasets: The COVID-19 Case Study on Respiratory Disorder Classification,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']","[arxiv.Result.Author('Lara Orlandic'), arxiv.Result.Author('Tomas Teijeiro'), arxiv.Result.Author('David Atienza')]","Cough audio signal classification is a potentially useful tool in screening
for respiratory disorders, such as COVID-19. Since it is dangerous to collect
data from patients with such contagious diseases, many research teams have
turned to crowdsourcing to quickly gather cough sound data, as it was done to
generate the COUGHVID dataset. The COUGHVID dataset enlisted expert physicians
to diagnose the underlying diseases present in a limited number of uploaded
recordings. However, this approach suffers from potential mislabeling of the
coughs, as well as notable disagreement between experts. In this work, we use a
semi-supervised learning (SSL) approach to improve the labeling consistency of
the COUGHVID dataset and the robustness of COVID-19 versus healthy cough sound
classification. First, we leverage existing SSL expert knowledge aggregation
techniques to overcome the labeling inconsistencies and sparsity in the
dataset. Next, our SSL approach is used to identify a subsample of re-labeled
COUGHVID audio samples that can be used to train or augment future cough
classification models. The consistency of the re-labeled data is demonstrated
in that it exhibits a high degree of class separability, 3x higher than that of
the user-labeled data, despite the expert label inconsistency present in the
original dataset. Furthermore, the spectral differences in the user-labeled
audio segments are amplified in the re-labeled data, resulting in significantly
different power spectral densities between healthy and COVID-19 coughs, which
demonstrates both the increased consistency of the new dataset and its
explainability from an acoustic perspective. Finally, we demonstrate how the
re-labeled dataset can be used to train a cough classifier. This SSL approach
can be used to combine the medical knowledge of several experts to improve the
database consistency for any diagnostic classification task.",-0.042508952,0.12835552,0.07260751,C
11317,"ing holds, however, warrants further study.","Whether this reason-
sheets and full performances.","Despite worse longer-
                                                                          range SI scores, we discover that the two w/o pretraining variants
               Lead sheet      Performance             ∆                  are, contrarily, more prone to over-repetition (which we arbitrarily
            SImid SIlong      SImid SIlong     SImid SIlong               deﬁne as: one bar of melody being exactly and consecutively re-
                                                                          peated over 6 times, or two neighboring bars of melody repeated
C&E         42.9 36.6 35.1 25.8 0−7.8 −10.8                               over 4 times)—5.5% of these two variants’ generations suffer from
                                                                          it, while only 2.5% of pieces by pretrained variants, and 0.5% of real
w/o struct  44.4 36.5 34.2 23.8 −10.2 −12.7                               data do.",2022-09-17 01:20:59+00:00,Compose & Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach,cs.SD,"['cs.SD', 'cs.AI', 'cs.MM', 'eess.AS']","[arxiv.Result.Author('Shih-Lun Wu'), arxiv.Result.Author('Yi-Hsuan Yang')]","Even with strong sequence models like Transformers, generating expressive
piano performances with long-range musical structures remains challenging.
Meanwhile, methods to compose well-structured melodies or lead sheets (melody +
chords), i.e., simpler forms of music, gained more success. Observing the
above, we devise a two-stage Transformer-based framework that Composes a lead
sheet first, and then Embellishes it with accompaniment and expressive touches.
Such a factorization also enables pretraining on non-piano data. Our objective
and subjective experiments show that Compose & Embellish shrinks the gap in
structureness between a current state of the art and real performances by half,
and improves other musical aspects such as richness and coherence as well.",0.064836904,-0.2570186,0.41339862,C
11525,"However, L2 speech is far more complex in terms       information has been added in the variance adaptor, such as
of speaking style and variation than L1 speech, that calls for     emotion [43]–[45] and style [24], [46], [47] to control the
further study of two research problems.","Furthermore, more variance
frame-level.",ﬁne-grained expressiveness accordingly.,2022-09-22 06:13:07+00:00,Controllable Accented Text-to-Speech Synthesis,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Rui Liu'), arxiv.Result.Author('Berrak Sisman'), arxiv.Result.Author('Guanglai Gao'), arxiv.Result.Author('Haizhou Li')]","Accented text-to-speech (TTS) synthesis seeks to generate speech with an
accent (L2) as a variant of the standard version (L1). Accented TTS synthesis
is challenging as L2 is different from L1 in both in terms of phonetic
rendering and prosody pattern. Furthermore, there is no easy solution to the
control of the accent intensity in an utterance. In this work, we propose a
neural TTS architecture, that allows us to control the accent and its intensity
during inference. This is achieved through three novel mechanisms, 1) an accent
variance adaptor to model the complex accent variance with three prosody
controlling factors, namely pitch, energy and duration; 2) an accent intensity
modeling strategy to quantify the accent intensity; 3) a consistency constraint
module to encourage the TTS system to render the expected accent intensity at a
fine level. Experiments show that the proposed system attains superior
performance to the baseline models in terms of accent rendering and intensity
control. To our best knowledge, this is the first study of accented TTS
synthesis with explicit intensity control.",0.4095055,-0.20910594,0.022434074,B
11753,There are several immediate suggestions for further research.,"Finally, we perform an
experimental study with ﬂamenco melodies demonstrating how the resulting STP can be employed
in a comparative performance analysis to quantify the amount of variation among a set of melodies.",• The ﬁrst issue is related to the complexity.,2022-09-27 08:40:55+00:00,Computing Melodic Templates in Oral Music Traditions,cs.SD,"['cs.SD', 'cs.IR', 'eess.AS']","[arxiv.Result.Author('Sergey Bereg'), arxiv.Result.Author('José-Miguel Díaz-Báñez'), arxiv.Result.Author('Nadine Kroher'), arxiv.Result.Author('Inmaculada Ventura')]","The term melodic template or skeleton refers to a basic melody which is
subject to variation during a music performance. In many oral music tradition,
these templates are implicitly passed throughout generations without ever being
formalized in a score. In this work, we introduce a new geometric optimization
problem, the spanning tube problem, to approximate a melodic template for a set
of labeled performance transcriptions corresponding to an specific style in
oral music traditions. Given a set of $n$ piecewise linear functions, we solve
the problem of finding a continuous function, $f^*$, and a minimum value,
$\varepsilon^*$, such that, the vertical segment of length $2\varepsilon^*$
centered at $(x,f^*(x))$ intersects at least $p$ functions ($p\leq n$). The
method explored here also provide a novel tool for quantitatively assess the
amount of melodic variation which occurs across performances.",0.0420073,-0.16139713,0.5513184,C
13012,"The training targets of the
To further study the potential of generalization for regression-  regression model were adjusted by max pause within queries.","Comparison of end-pointer outputs based on classiﬁ-
                                                                  cation (black) and regression (red).","based end-pointing, we visualized the model outputs based on
a simpler setting.",2022-10-25 18:09:42+00:00,Dynamic Speech Endpoint Detection with Regression Targets,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Dawei Liang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Tarun Singh'), arxiv.Result.Author('Jay Mahadeokar'), arxiv.Result.Author('Shanil Puri'), arxiv.Result.Author('Jiedan Zhu'), arxiv.Result.Author('Edison Thomaz'), arxiv.Result.Author('Mike Seltzer')]","Interactive voice assistants have been widely used as input interfaces in
various scenarios, e.g. on smart homes devices, wearables and on AR devices.
Detecting the end of a speech query, i.e. speech end-pointing, is an important
task for voice assistants to interact with users. Traditionally, speech
end-pointing is based on pure classification methods along with arbitrary
binary targets. In this paper, we propose a novel regression-based speech
end-pointing model, which enables an end-pointer to adjust its detection
behavior based on context of user queries. Specifically, we present a pause
modeling method and show its effectiveness for dynamic end-pointing. Based on
our experiments with vendor-collected smartphone and wearables speech queries,
our strategy shows a better trade-off between endpointing latency and accuracy,
compared to the traditional classification-based method. We further discuss the
benefits of this model and generalization of the framework in the paper.",0.0463098,-0.20425023,0.049551565,B
13085,Because the training data did           requires further study.,"Each student            nentially moving average teacher were worse than the student
model with the exponentially moving average teacher as TUP         models trained with the static teacher and the baseline, which
was trained for 35 epochs.","not contain clean speech, and there was no validation set for
selecting the best model, we always tested the model after the         In Table 2, we compare our best model with previous su-
last epoch in the experiments.",2022-10-27 12:26:24+00:00,A Teacher-student Framework for Unsupervised Speech Enhancement Using Noise Remixing Training and Two-stage Inference,cs.SD,"['cs.SD', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM', 'eess.AS']","[arxiv.Result.Author('Li-Wei Chen'), arxiv.Result.Author('Yao-Fei Cheng'), arxiv.Result.Author('Hung-Shin Lee'), arxiv.Result.Author('Yu Tsao'), arxiv.Result.Author('Hsin-Min Wang')]","The lack of clean speech is a practical challenge to the development of
speech enhancement systems, which means that the training of neural network
models must be done in an unsupervised manner, and there is an inevitable
mismatch between their training criterion and evaluation metric. In response to
this unfavorable situation, we propose a teacher-student training strategy that
does not require any subjective/objective speech quality metrics as learning
reference by improving the previously proposed noisy-target training (NyTT).
Because homogeneity between in-domain noise and extraneous noise is the key to
the effectiveness of NyTT, we train various student models by remixing the
teacher model's estimated speech and noise for clean-target training or raw
noisy speech and the teacher model's estimated noise for noisy-target training.
We use the NyTT model as the initial teacher model. Experimental results show
that our proposed method outperforms several baselines, especially with
two-stage inference, where clean speech is derived successively through the
bootstrap model and the final student model.",0.04442033,-0.15130372,-0.1821307,C
13086,Because the training data did           requires further study.,"Each student            nentially moving average teacher were worse than the student
model with the exponentially moving average teacher as TUP         models trained with the static teacher and the baseline, which
was trained for 35 epochs.","not contain clean speech, and there was no validation set for
selecting the best model, we always tested the model after the         In Table 2, we compare our best model with previous su-
last epoch in the experiments.",2022-10-27 12:26:24+00:00,A Teacher-student Framework for Unsupervised Speech Enhancement Using Noise Remixing Training and Two-stage Inference,cs.SD,"['cs.SD', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM', 'eess.AS']","[arxiv.Result.Author('Li-Wei Chen'), arxiv.Result.Author('Yao-Fei Cheng'), arxiv.Result.Author('Hung-Shin Lee'), arxiv.Result.Author('Yu Tsao'), arxiv.Result.Author('Hsin-Min Wang')]","The lack of clean speech is a practical challenge to the development of
speech enhancement systems, which means that the training of neural network
models must be done in an unsupervised manner, and there is an inevitable
mismatch between their training criterion and evaluation metric. In response to
this unfavorable situation, we propose a teacher-student training strategy that
does not require any subjective/objective speech quality metrics as learning
reference by improving the previously proposed noisy-target training (NyTT).
Because homogeneity between in-domain noise and extraneous noise is the key to
the effectiveness of NyTT, we train various student models by remixing the
teacher model's estimated speech and noise for clean-target training or raw
noisy speech and the teacher model's estimated noise for noisy-target training.
We use the NyTT model as the initial teacher model. Experimental results show
that our proposed method outperforms several baselines, especially with
two-stage inference, where clean speech is derived successively through the
bootstrap model and the final student model.",0.04442033,-0.15130372,-0.1821307,C
13165,publicly released to encourage further research.,"This is perhaps because knowledge of
All our models are built using the ESPNet[14] toolkit, and will be      the discrete emotion aids in the prediction of dominance and arousal.","We use HUBERT-         We also observe an absolute 0.8% improvement on accuracy in Hier-
large [15] embeddings as the input features and a 4 layer con-          archical D-C.",2022-10-29 16:12:31+00:00,Unifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition,cs.SD,"['cs.SD', 'cs.AI', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Roshan Sharma'), arxiv.Result.Author('Hira Dhamyal'), arxiv.Result.Author('Bhiksha Raj'), arxiv.Result.Author('Rita Singh')]","Traditionally, in paralinguistic analysis for emotion detection from speech,
emotions have been identified with discrete or dimensional (continuous-valued)
labels. Accordingly, models that have been proposed for emotion detection use
one or the other of these label types. However, psychologists like Russell and
Plutchik have proposed theories and models that unite these views, maintaining
that these representations have shared and complementary information. This
paper is an attempt to validate these viewpoints computationally. To this end,
we propose a model to jointly predict continuous and discrete emotional
attributes and show how the relationship between these can be utilized to
improve the robustness and performance of emotion recognition tasks. Our
approach comprises multi-task and hierarchical multi-task learning frameworks
that jointly model the relationships between continuous-valued and discrete
emotion labels. Experimental results on two widely used datasets (IEMOCAP and
MSPPodcast) for speech-based emotion recognition show that our model results in
statistically significant improvements in performance over strong baselines
with non-unified approaches. We also demonstrate that using one type of label
(discrete or continuous-valued) for training improves recognition performance
in tasks that use the other type of label. Experimental results and reasoning
for this approach (called the mismatched training approach) are also presented.",0.20030919,-0.09951456,-0.02624109,B
13196,"All
                                                                                                              audio ﬁles and annotations are publicly accessible, making it easier
Since the model solely relies on self-supervision, the model’s output                                         for reproduction and further research.",Post-Training Calibration                                                                                tains a variety of other genres including western pop and rock.,"The reader may refer to the pa-
might mismatch the true metrical boundaries by a global offset.",2022-10-31 10:05:19+00:00,Self-Supervised Hierarchical Metrical Structure Modeling,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Junyan Jiang'), arxiv.Result.Author('Gus Xia')]","We propose a novel method to model hierarchical metrical structures for both
symbolic music and audio signals in a self-supervised manner with minimal
domain knowledge. The model trains and inferences on beat-aligned music signals
and predicts an 8-layer hierarchical metrical tree from beat, measure to the
section level. The training procedural does not require any hierarchical
metrical labeling except for beats, purely relying on the nature of metrical
regularity and inter-voice consistency as inductive biases. We show in
experiments that the method achieves comparable performance with supervised
baselines on multiple metrical structure analysis tasks on both symbolic music
and audio signals. All demos, source code and pre-trained models are publicly
available on GitHub.",0.13105677,0.24805668,0.3190316,C
13278,"We recommend further research in the direction of
these systems across WERs, we plot the median latency for various        shared acoustic understanding between ASR and EP tasks.",To better visualize the performance of      separate models.,WERs for the test set evaluated in Table 1.,2022-11-01 23:43:15+00:00,Unified End-to-End Speech Recognition and Endpointing for Fast and Efficient Speech Systems,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Author('Shaan Bijwadia'), arxiv.Result.Author('Shuo-yiin Chang'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Tara Sainath'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Yanzhang He')]","Automatic speech recognition (ASR) systems typically rely on an external
endpointer (EP) model to identify speech boundaries. In this work, we propose a
method to jointly train the ASR and EP tasks in a single end-to-end (E2E)
multitask model, improving EP quality by optionally leveraging information from
the ASR audio encoder. We introduce a ""switch"" connection, which trains the EP
to consume either the audio frames directly or low-level latent representations
from the ASR model. This results in a single E2E model that can be used during
inference to perform frame filtering at low cost, and also make high quality
end-of-query (EOQ) predictions based on ongoing ASR computation. We present
results on a voice search test set showing that, compared to separate
single-task models, this approach reduces median endpoint latency by 120 ms
(30.8% reduction), and 90th percentile latency by 170 ms (23.0% reduction),
without regressing word error rate. For continuous recognition, WER improves by
10.6% (relative).",-0.12175166,0.034605097,0.08300123,C
13435,"These classiﬁcation models could      utterances with three emotions such as annoyed, friendly, or
                                       be used as benchmarks in further research.","The dataset distinguishes the
                                       a pre-trained neural network.","neutral; however, only 218 out of 1247 are labeled.",2022-11-07 08:57:16+00:00,"Hi,KIA: A Speech Emotion Recognition Dataset for Wake-Up Words",cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Taesu Kim'), arxiv.Result.Author('SeungHeon Doh'), arxiv.Result.Author('Gyunpyo Lee'), arxiv.Result.Author('Hyungseok Jeon'), arxiv.Result.Author('Juhan Nam'), arxiv.Result.Author('Hyeon-Jeong Suk')]","Wake-up words (WUW) is a short sentence used to activate a speech recognition
system to receive the user's speech input. WUW utterances include not only the
lexical information for waking up the system but also non-lexical information
such as speaker identity or emotion. In particular, recognizing the user's
emotional state may elaborate the voice communication. However, there is few
dataset where the emotional state of the WUW utterances is labeled. In this
paper, we introduce Hi, KIA, a new WUW dataset which consists of 488 Korean
accent emotional utterances collected from four male and four female speakers
and each of utterances is labeled with four emotional states including anger,
happy, sad, or neutral. We present the step-by-step procedure to build the
dataset, covering scenario selection, post-processing, and human validation for
label agreement. Also, we provide two classification models for WUW speech
emotion recognition using the dataset. One is based on traditional hand-craft
features and the other is a transfer-learning approach using a pre-trained
neural network. These classification models could be used as benchmarks in
further research.",0.34823066,-0.03596667,-0.07237924,B
13785,"This motivates further research
                                        standing.","In addition to the practical appli-
                                        strengths and weaknesses of our methods helps us better un-     cations, urban scenes provide a challenging scenario for vi-
                                        derstand the problem of visual sound source localization and    sual sound source localization where state-of-the-art methods
                                        sheds light on open challenges for audio-visual scene under-    prove to be inadequate [11].","into the limitations of state-of-the-art methods as well as the
                                                                                                        datasets used for their development.",2022-11-15 18:12:10+00:00,FlowGrad: Using Motion for Visual Sound Source Localization,cs.SD,"['cs.SD', 'cs.CV', 'cs.MM', 'eess.AS']","[arxiv.Result.Author('Rajsuryan Singh'), arxiv.Result.Author('Pablo Zinemanas'), arxiv.Result.Author('Xavier Serra'), arxiv.Result.Author('Juan Pablo Bello'), arxiv.Result.Author('Magdalena Fuentes')]","Most recent work in visual sound source localization relies on semantic
audio-visual representations learned in a self-supervised manner, and by design
excludes temporal information present in videos. While it proves to be
effective for widely used benchmark datasets, the method falls short for
challenging scenarios like urban traffic. This work introduces temporal context
into the state-of-the-art methods for sound source localization in urban scenes
using optical flow as a means to encode motion information. An analysis of the
strengths and weaknesses of our methods helps us better understand the problem
of visual sound source localization and sheds light on open challenges for
audio-visual scene understanding.",-0.031144064,0.28904733,0.1852133,C
14129,"We further study the reason for better perfor-    the shared representation of the two modalities through the
mance on the alignment between speech and text represen-         alignment information from paired data, TESSP learns the
tation and the representation space between speech and text.","While TESSP tries to learn
the character.","respective knowledge of the two modalities in the high lay-
                                                                 ers.",2022-11-24 07:08:51+00:00,TESSP: Text-Enhanced Self-Supervised Speech Pre-training,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Zhuoyuan Yao'), arxiv.Result.Author('Shuo Ren'), arxiv.Result.Author('Sanyuan Chen'), arxiv.Result.Author('Ziyang Ma'), arxiv.Result.Author('Pengcheng Guo'), arxiv.Result.Author('Lei Xie')]","Self-supervised speech pre-training empowers the model with the contextual
structure inherent in the speech signal while self-supervised text pre-training
empowers the model with linguistic information. Both of them are beneficial for
downstream speech tasks such as ASR. However, the distinct pre-training
objectives make it challenging to jointly optimize the speech and text
representation in the same model. To solve this problem, we propose
Text-Enhanced Self-Supervised Speech Pre-training (TESSP), aiming to
incorporate the linguistic information into speech pre-training. Our model
consists of three parts, i.e., a speech encoder, a text encoder and a shared
encoder. The model takes unsupervised speech and text data as the input and
leverages the common HuBERT and MLM losses respectively. We also propose
phoneme up-sampling and representation swapping to enable joint modeling of the
speech and text information. Specifically, to fix the length mismatching
problem between speech and text data, we phonemize the text sequence and
up-sample the phonemes with the alignment information extracted from a small
set of supervised data. Moreover, to close the gap between the learned speech
and text representations, we swap the text representation with the speech
representation extracted by the respective private encoders according to the
alignment information. Experiments on the Librispeech dataset shows the
proposed TESSP model achieves more than 10% improvement compared with WavLM on
the test-clean and test-other sets. We also evaluate our model on the SUPERB
benchmark, showing our model has better performance on Phoneme Recognition,
Acoustic Speech Recognition and Speech Translation compared with WavLM.",0.16802157,-0.004637815,-0.1716015,B
14266,"Although further research is needed
to validate these methods in diﬀerent marine environments and with diﬀerent
animal species, we believe that deep learning will ﬁnally enable the creation and
deployment of cost-eﬀective monitoring platforms.","In this paper we demonstrated that
modern deep learning approaches can detect dolphin whistles with an impressive
accuracy, and are thus well-suited to become the new standard for the automatic
processing of underwater acoustic signals.","Conﬂict of Interest Statement

The authors declare that the research was conducted in the absence of any
commercial or ﬁnancial relationships that could be construed as a potential
conﬂict of interest.",2022-11-28 15:06:46+00:00,Automated Detection of Dolphin Whistles with Convolutional Networks and Transfer Learning,cs.SD,"['cs.SD', 'cs.CV', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Burla Nur Korkmaz'), arxiv.Result.Author('Roee Diamant'), arxiv.Result.Author('Gil Danino'), arxiv.Result.Author('Alberto Testolin')]","Effective conservation of maritime environments and wildlife management of
endangered species require the implementation of efficient, accurate and
scalable solutions for environmental monitoring. Ecoacoustics offers the
advantages of non-invasive, long-duration sampling of environmental sounds and
has the potential to become the reference tool for biodiversity surveying.
However, the analysis and interpretation of acoustic data is a time-consuming
process that often requires a great amount of human supervision. This issue
might be tackled by exploiting modern techniques for automatic audio signal
analysis, which have recently achieved impressive performance thanks to the
advances in deep learning research. In this paper we show that convolutional
neural networks can indeed significantly outperform traditional automatic
methods in a challenging detection task: identification of dolphin whistles
from underwater audio recordings. The proposed system can detect signals even
in the presence of ambient noise, at the same time consistently reducing the
likelihood of producing false positives and false negatives. Our results
further support the adoption of artificial intelligence technology to improve
the automatic monitoring of marine ecosystems.",-0.14529404,0.28186733,-0.15140669,C
14570,"With further research on device source identiﬁcation, the feature dimension
Zeng et al.","used a multi-feature parallel convolution network, combined with
             the attention mechanism for device source identiﬁcation to achieve an improved
             eﬀect.","Page 6 of 20

             of device source identiﬁcation and the number of devices for device source identi-
             ﬁcation tasks will further increase.",2022-12-05 07:56:04+00:00,End-to-end Recording Device Identification Based on Deep Representation Learning,cs.SD,"['cs.SD', 'eess.AS']","[arxiv.Result.Author('Chunyan Zeng'), arxiv.Result.Author('Dongliang Zhu'), arxiv.Result.Author('Zhifeng Wang'), arxiv.Result.Author('Minghu Wu'), arxiv.Result.Author('Wei Xiong'), arxiv.Result.Author('Nan Zhao')]","Deep learning techniques have achieved specific results in recording device
source identification. The recording device source features include spatial
information and certain temporal information. However, most recording device
source identification methods based on deep learning only use spatial
representation learning from recording device source features, which cannot
make full use of recording device source information. Therefore, in this paper,
to fully explore the spatial information and temporal information of recording
device source, we propose a new method for recording device source
identification based on the fusion of spatial feature information and temporal
feature information by using an end-to-end framework. From a feature
perspective, we designed two kinds of networks to extract recording device
source spatial and temporal information. Afterward, we use the attention
mechanism to adaptively assign the weight of spatial information and temporal
information to obtain fusion features. From a model perspective, our model uses
an end-to-end framework to learn the deep representation from spatial feature
and temporal feature and train using deep and shallow loss to joint optimize
our network. This method is compared with our previous work and baseline
system. The results show that the proposed method is better than our previous
work and baseline system under general conditions.",-0.2819965,0.073589906,-0.21226054,A
14995,"For
rounds 13-18 (REACT survey dates from 2021-06-24 to 2022-03-01), participants were
asked if they agreed to be contacted about further research led by the UKHSA.",The swab was either posted or collected by courier for PCR testing at laboratories.,"After sending

                                                                                                                          6
their swab to a laboratory and completing the REACT-1 survey, those who agreed to be
contacted were sent an email invitation to the online survey for this study which included
audio recordings (survey questions and responses listed in Table 2).",2022-12-15 11:40:40+00:00,A large-scale and PCR-referenced vocal audio dataset for COVID-19,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","[arxiv.Result.Author('Jobie Budd'), arxiv.Result.Author('Kieran Baker'), arxiv.Result.Author('Emma Karoune'), arxiv.Result.Author('Harry Coppock'), arxiv.Result.Author('Selina Patel'), arxiv.Result.Author('Ana Tendero Cañadas'), arxiv.Result.Author('Alexander Titcomb'), arxiv.Result.Author('Richard Payne'), arxiv.Result.Author('David Hurley'), arxiv.Result.Author('Sabrina Egglestone'), arxiv.Result.Author('Lorraine Butler'), arxiv.Result.Author('Jonathon Mellor'), arxiv.Result.Author('George Nicholson'), arxiv.Result.Author('Ivan Kiskin'), arxiv.Result.Author('Vasiliki Koutra'), arxiv.Result.Author('Radka Jersakova'), arxiv.Result.Author('Rachel A. McKendry'), arxiv.Result.Author('Peter Diggle'), arxiv.Result.Author('Sylvia Richardson'), arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Steven Gilmour'), arxiv.Result.Author('Davide Pigoli'), arxiv.Result.Author('Stephen Roberts'), arxiv.Result.Author('Josef Packham'), arxiv.Result.Author('Tracey Thornley'), arxiv.Result.Author('Chris Holmes')]","The UK COVID-19 Vocal Audio Dataset is designed for the training and
evaluation of machine learning models that classify SARS-CoV-2 infection status
or associated respiratory symptoms using vocal audio. The UK Health Security
Agency recruited voluntary participants through the national Test and Trace
programme and the REACT-1 survey in England from March 2021 to March 2022,
during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and
some Omicron variant sublineages. Audio recordings of volitional coughs,
exhalations, and speech were collected in the 'Speak up to help beat
coronavirus' digital survey alongside demographic, self-reported symptom and
respiratory condition data, and linked to SARS-CoV-2 test results. The UK
COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2
PCR-referenced audio recordings to date. PCR results were linked to 70,794 of
72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms
were reported by 45.62% of participants. This dataset has additional potential
uses for bioacoustics research, with 11.30% participants reporting asthma, and
27.20% with linked influenza PCR test results.",-0.036934566,-0.18566325,0.15288271,C
15245,"We could work on making the computations faster and further study
how to optimally use the FTL during training.","This loss is currently

                                                             54
not perfectly optimised in terms of engineering, which limits its use during training to the ﬁrst
two RADAR steps only.","55
Chapter 7

Conclusion

In this project, we introduce a new hybrid strategy to help a peak-based AFP system improve its
robustness to background noise recorded in real environments.",2022-12-21 09:46:12+00:00,Audio Denoising for Robust Audio Fingerprinting,cs.SD,"['cs.SD', 'cs.IR', 'cs.LG']",[arxiv.Result.Author('Kamil Akesbi')],"Music discovery services let users identify songs from short mobile
recordings. These solutions are often based on Audio Fingerprinting, and rely
more specifically on the extraction of spectral peaks in order to be robust to
a number of distortions. Few works have been done to study the robustness of
these algorithms to background noise captured in real environments. In
particular, AFP systems still struggle when the signal to noise ratio is low,
i.e when the background noise is strong. In this project, we tackle this
problematic with Deep Learning. We test a new hybrid strategy which consists of
inserting a denoising DL model in front of a peak-based AFP algorithm. We
simulate noisy music recordings using a realistic data augmentation pipeline,
and train a DL model to denoise them. The denoising model limits the impact of
background noise on the AFP system's extracted peaks, improving its robustness
to noise. We further propose a novel loss function to adapt the DL model to the
considered AFP system, increasing its precision in terms of retrieved spectral
peaks. To the best of our knowledge, this hybrid strategy has not been tested
before.",-0.40351853,0.0060562454,-0.14662354,A
15380,"Secondly, the use of data conversations from time-sequence to average mean can cause
information loss, a time series forecasting model can consider using in further research, e.g.,
seasonal support vector regression (SSVR).","Firstly, our
SFS is not generic enough, the use of wrapper-based FSA is heavily relying on a learning model
to select features, this may cause bias on the certain machine learning model, in future, a more
generic FSA approach may need to be consider, such as filter-based FSAs, hybrid FSAs.","Lastly splitting valence and arousal into two different
regressions may not fully cover the relation from the features to an emotion status, since the
nature of emotion may have a compound relationship to each other, therefore the hybrid-based
FSA approach may consider for the future research.",2022-12-27 05:55:34+00:00,Feature Selection Approaches for Optimising Music Emotion Recognition Methods,cs.SD,"['cs.SD', 'cs.LG', 'cs.MM', 'eess.AS']","[arxiv.Result.Author('Le Cai'), arxiv.Result.Author('Sam Ferguson'), arxiv.Result.Author('Haiyan Lu'), arxiv.Result.Author('Gengfa Fang')]","The high feature dimensionality is a challenge in music emotion recognition.
There is no common consensus on a relation between audio features and emotion.
The MER system uses all available features to recognize emotion; however, this
is not an optimal solution since it contains irrelevant data acting as noise.
In this paper, we introduce a feature selection approach to eliminate redundant
features for MER. We created a Selected Feature Set (SFS) based on the feature
selection algorithm (FSA) and benchmarked it by training with two models,
Support Vector Regression (SVR) and Random Forest (RF) and comparing them
against with using the Complete Feature Set (CFS). The result indicates that
the performance of MER has improved for both Random Forest (RF) and Support
Vector Regression (SVR) models by using SFS. We found using FSA can improve
performance in all scenarios, and it has potential benefits for model
efficiency and stability for MER task.",0.12669611,-0.17275704,0.04692646,B
