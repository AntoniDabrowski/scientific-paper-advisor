Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
137,"With this capability, we imagine
the work to enable further research in cloud robotics, by                   [20] S. A. Miratabzadeh, N. Gallardo, N. Gamez, K. Haradi, A. R.
enabling transparent proxying for ROS, using its native                           Puthussery, P. Rad, and M. Jamshidi, “Cloud robotics: A software
communication protocols and technologies.","949–955, 2017.
work segments in general.","architecture: For heterogeneous large-scale autonomous robots,” in
                                                                                  2016 World Automation Congress (WAC).",2022-01-05 13:50:13+00:00,Proxying ROS communications -- enabling containerized ROS deployments in distributed multi-host environments,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Arne Wendt'), arxiv.Result.Author('Prof. Dr. -Ing. Thorsten Schüppstuhl')]","With the ability to use containers at the edge, they pose a unified solution
to combat the complexity of distributed multi-host ROS deployments, as well as
individual ROS-node and dependency deployment. The bidirectional communication
in ROS poses a challenge to using containerized ROS deployments alongside
non-containerized ones spread over multiple machines though. We will analyze
the communication protocol employed by ROS, and the suitability of different
container networking modes and their implications on ROS deployments. Finally,
we will present a layer 7 transparent proxy server architecture for ROS, as a
solution to the identified problems. Enabling the use of ROS not only in
containerized environments, but proxying ROS between network segments in
general.",0.040602718,0.24519823,0.2881629,B
138,"With this capability, we imagine
the work to enable further research in cloud robotics, by                   [20] S. A. Miratabzadeh, N. Gallardo, N. Gamez, K. Haradi, A. R.
enabling transparent proxying for ROS, using its native                           Puthussery, P. Rad, and M. Jamshidi, “Cloud robotics: A software
communication protocols and technologies.","949–955, 2017.
work segments in general.","architecture: For heterogeneous large-scale autonomous robots,” in
                                                                                  2016 World Automation Congress (WAC).",2022-01-05 13:50:13+00:00,Proxying ROS communications -- enabling containerized ROS deployments in distributed multi-host environments,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Arne Wendt'), arxiv.Result.Author('Prof. Dr. -Ing. Thorsten Schüppstuhl')]","With the ability to use containers at the edge, they pose a unified solution
to combat the complexity of distributed multi-host ROS deployments, as well as
individual ROS-node and dependency deployment. The bidirectional communication
in ROS poses a challenge to using containerized ROS deployments alongside
non-containerized ones spread over multiple machines though. We will analyze
the communication protocol employed by ROS, and the suitability of different
container networking modes and their implications on ROS deployments. Finally,
we will present a layer 7 transparent proxy server architecture for ROS, as a
solution to the identified problems. Enabling the use of ROS not only in
containerized environments, but proxying ROS between network segments in
general.",0.040602718,0.24519823,0.2881629,B
216,"To decrease the latency and
increase safety, we can move to edge computing servers; however, that subject needs further study.","For communication with people that physically do not have
access to the robot, we designed the IoT node; the node connected to a cloud server.","Currently, we used

                                                              21
This is the preprint version.",2022-01-06 18:57:35+00:00,"A wearable sensor vest for social humanoid robots with GPGPU, IoT, and modular software architecture",cs.RO,"['cs.RO', 'cs.AI', 'cs.HC', 'cs.SY', 'eess.SY', '68T40', 'I.2.9']","[arxiv.Result.Author('Mohsen Jafarzadeh'), arxiv.Result.Author('Stephen Brooks'), arxiv.Result.Author('Shimeng Yu'), arxiv.Result.Author('Balakrishnan Prabhakaran'), arxiv.Result.Author('Yonas Tadesse')]","Currently, most social robots interact with their surroundings and humans
through sensors that are integral parts of the robots, which limits the
usability of the sensors, human-robot interaction, and interchangeability. A
wearable sensor garment that fits many robots is needed in many applications.
This article presents an affordable wearable sensor vest, and an open-source
software architecture with the Internet of Things (IoT) for social humanoid
robots. The vest consists of touch, temperature, gesture, distance, vision
sensors, and a wireless communication module. The IoT feature allows the robot
to interact with humans locally and over the Internet. The designed
architecture works for any social robot that has a general-purpose graphics
processing unit (GPGPU), I2C/SPI buses, Internet connection, and the Robotics
Operating System (ROS). The modular design of this architecture enables
developers to easily add/remove/update complex behaviors. The proposed software
architecture provides IoT technology, GPGPU nodes, I2C and SPI bus mangers,
audio-visual interaction nodes (speech to text, text to speech, and image
understanding), and isolation between behavior nodes and other nodes. The
proposed IoT solution consists of related nodes in the robot, a RESTful web
service, and user interfaces. We used the HTTP protocol as a means of two-way
communication with the social robot over the Internet. Developers can easily
edit or add nodes in C, C++, and Python programming languages. Our architecture
can be used for designing more sophisticated behaviors for social humanoid
robots.",0.13567904,0.13313101,0.19073077,B
223,"1 m distance should be kept from people, but further research
Even though there is some evidence in literature that physical        is needed to determine an appropriate value.","Based
   We predicted, as stated in H4, that UR does not deteriorate        on the results in this study we recommend that a minimum of
the spatial awareness of the users, and the results conﬁrmed it.","Whereas there is
rotations increase spatial awareness [28], we did not explicitly      research on human-robot proxemics in VR [41], it lacks the
tell the participants to always turn to face the direction of         telepresence perspective.",2022-01-07 10:52:25+00:00,Unwinding Rotations Improves User Comfort with Immersive Telepresence Robots,cs.RO,"['cs.RO', 'cs.HC', 'cs.MM']","[arxiv.Result.Author('Markku Suomalainen'), arxiv.Result.Author('Basak Sakcak'), arxiv.Result.Author('Adhi Widagdo'), arxiv.Result.Author('Juho Kalliokoski'), arxiv.Result.Author('Katherine J. Mimnaugh'), arxiv.Result.Author('Alexis P. Chambers'), arxiv.Result.Author('Timo Ojala'), arxiv.Result.Author('Steven M. LaValle')]","We propose unwinding the rotations experienced by the user of an immersive
telepresence robot to improve comfort and reduce VR sickness of the user. By
immersive telepresence we refer to a situation where a 360\textdegree~camera on
top of a mobile robot is streaming video and audio into a head-mounted display
worn by a remote user possibly far away. Thus, it enables the user to be
present at the robot's location, look around by turning the head and
communicate with people near the robot. By unwinding the rotations of the
camera frame, the user's viewpoint is not changed when the robot rotates. The
user can change her viewpoint only by physically rotating in her local setting;
as visual rotation without the corresponding vestibular stimulation is a major
source of VR sickness, physical rotation by the user is expected to reduce VR
sickness. We implemented unwinding the rotations for a simulated robot
traversing a virtual environment and ran a user study (N=34) comparing
unwinding rotations to user's viewpoint turning when the robot turns. Our
results show that the users found unwound rotations more preferable and
comfortable and that it reduced their level of VR sickness. We also present
further results about the users' path integration capabilities, viewing
directions, and subjective observations of the robot's speed and distances to
simulated people and objects.",-0.21525824,0.00011418294,-0.080354005,A
224,"Evidently, using the same environment
and the same robot path to avoid confounding factors lead to          The general concept of immersive telepresence requires
directing participants’ attention to aspects that were tangential  further research on many fronts to become competitive against
to the study.",corrupt the results.,"Even though counterbalancing ensured that this       current commercial ﬂat-screen alternatives, especially for tasks
occurred approximately evenly for both conditions, injecting       that require bidirectional communication.",2022-01-07 10:52:25+00:00,Unwinding Rotations Improves User Comfort with Immersive Telepresence Robots,cs.RO,"['cs.RO', 'cs.HC', 'cs.MM']","[arxiv.Result.Author('Markku Suomalainen'), arxiv.Result.Author('Basak Sakcak'), arxiv.Result.Author('Adhi Widagdo'), arxiv.Result.Author('Juho Kalliokoski'), arxiv.Result.Author('Katherine J. Mimnaugh'), arxiv.Result.Author('Alexis P. Chambers'), arxiv.Result.Author('Timo Ojala'), arxiv.Result.Author('Steven M. LaValle')]","We propose unwinding the rotations experienced by the user of an immersive
telepresence robot to improve comfort and reduce VR sickness of the user. By
immersive telepresence we refer to a situation where a 360\textdegree~camera on
top of a mobile robot is streaming video and audio into a head-mounted display
worn by a remote user possibly far away. Thus, it enables the user to be
present at the robot's location, look around by turning the head and
communicate with people near the robot. By unwinding the rotations of the
camera frame, the user's viewpoint is not changed when the robot rotates. The
user can change her viewpoint only by physically rotating in her local setting;
as visual rotation without the corresponding vestibular stimulation is a major
source of VR sickness, physical rotation by the user is expected to reduce VR
sickness. We implemented unwinding the rotations for a simulated robot
traversing a virtual environment and ran a user study (N=34) comparing
unwinding rotations to user's viewpoint turning when the robot turns. Our
results show that the users found unwound rotations more preferable and
comfortable and that it reduced their level of VR sickness. We also present
further results about the users' path integration capabilities, viewing
directions, and subjective observations of the robot's speed and distances to
simulated people and objects.",-0.1740951,-0.124445945,-0.07424495,A
379,"Finally, while we made technical contributions to various aspects in each sub-challenge, it is clear that all of
them warrant further research to increase ﬂexibility and applicability to other domains.","While this is highly interesting and provides
research challenges, it would be good to have similar tasks from one competition edition to the next to give
the community time to learn from their promising approaches.","Acknowledgments

We thank all members of our team NimbRo for their support before and during the competition.",2022-01-11 09:08:54+00:00,"Target Chase, Wall Building, and Fire Fighting: Autonomous UAVs of Team NimbRo at MBZIRC 2020",cs.RO,['cs.RO'],"[arxiv.Result.Author('Marius Beul'), arxiv.Result.Author('Max Schwarz'), arxiv.Result.Author('Jan Quenzel'), arxiv.Result.Author('Malte Splietker'), arxiv.Result.Author('Simon Bultmann'), arxiv.Result.Author('Daniel Schleich'), arxiv.Result.Author('Andre Rochow'), arxiv.Result.Author('Dmytro Pavlichenko'), arxiv.Result.Author('Radu Alexandru Rosu'), arxiv.Result.Author('Patrick Lowin'), arxiv.Result.Author('Bruno Scheider'), arxiv.Result.Author('Michael Schreiber'), arxiv.Result.Author('Finn Süberkrüb'), arxiv.Result.Author('Sven Behnke')]","The Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2020 posed
diverse challenges for unmanned aerial vehicles (UAVs). We present our four
tailored UAVs, specifically developed for individual aerial-robot tasks of
MBZIRC, including custom hardware- and software components.
  In Challenge 1, a target UAV is pursued using a high-efficiency, onboard
object detection pipeline to capture a ball from the target UAV. A second UAV
uses a similar detection method to find and pop balloons scattered throughout
the arena.
  For Challenge 2, we demonstrate a larger UAV capable of autonomous aerial
manipulation: Bricks are found and tracked from camera images. Subsequently,
they are approached, picked, transported, and placed on a wall.
  Finally, in Challenge 3, our UAV autonomously finds fires using LiDAR and
thermal cameras. It extinguishes the fires with an onboard fire extinguisher.
  While every robot features task-specific subsystems, all UAVs rely on a
standard software stack developed for this particular and future competitions.
We present our mostly open-source software solutions, including tools for
system configuration, monitoring, robust wireless communication, high-level
control, and agile trajectory generation. For solving the MBZIRC 2020 tasks, we
advanced the state of the art in multiple research areas like machine vision
and trajectory generation.
  We present our scientific contributions that constitute the foundation for
our algorithms and systems and analyze the results from the MBZIRC competition
2020 in Abu Dhabi, where our systems reached second place in the Grand
Challenge. Furthermore, we discuss lessons learned from our participation in
this complex robotic challenge.",0.21036687,-0.06932675,-0.12401365,C
464,"As the vehicle comes to a stop the nose is
   dipped down 1 inch the tail raises 1 inch
   before leveling out

                                                   Figure 5: Autonomous vehicle stops one car’s
                                                   length farther away from the pedestrian’s
                                                   crossing point

Figure 2: Traditional vehicle with a human in      3 How nuReality Can Sup-
the driver’s seat
                                                        port AV Research
Figure 3: Autonomous vehicle with visible LI-
DAR sensors and no human driver                    The nuReality ﬁles are designed as a base to
                                                   support experimentation and further research
                                                   into expressive robotics, autonomous vehicles,
                                                   pedestrian interactions, and related areas.","Appears to
   slow as it approaches the intersection but
   continues through without stopping

• Scene 10: AV with Expressive Nose Dive.","Fur-
                                                   ther, they are designed to overcome some of
                                                   the challenges of existing simulation tools.",2022-01-12 23:54:09+00:00,nuReality: A VR environment for research of pedestrian and autonomous vehicle interactions,cs.RO,['cs.RO'],"[arxiv.Result.Author('Paul Schmitt'), arxiv.Result.Author('Nicholas Britten'), arxiv.Result.Author('JiHyun Jeong'), arxiv.Result.Author('Amelia Coffey'), arxiv.Result.Author('Kevin Clark'), arxiv.Result.Author('Shweta Sunil Kothawade'), arxiv.Result.Author('Elena Corina Grigore'), arxiv.Result.Author('Adam Khaw'), arxiv.Result.Author('Christopher Konopka'), arxiv.Result.Author('Linh Pham'), arxiv.Result.Author('Kim Ryan'), arxiv.Result.Author('Christopher Schmitt'), arxiv.Result.Author('Aryaman Pandya'), arxiv.Result.Author('Emilio Frazzoli')]","We present nuReality, a virtual reality 'VR' environment designed to test the
efficacy of vehicular behaviors to communicate intent during interactions
between autonomous vehicles 'AVs' and pedestrians at urban intersections. In
this project we focus on expressive behaviors as a means for pedestrians to
readily recognize the underlying intent of the AV's movements. VR is an ideal
tool to use to test these situations as it can be immersive and place subjects
into these potentially dangerous scenarios without risk. nuReality provides a
novel and immersive virtual reality environment that includes numerous visual
details (road and building texturing, parked cars, swaying tree limbs) as well
as auditory details (birds chirping, cars honking in the distance, people
talking). In these files we present the nuReality environment, its 10 unique
vehicle behavior scenarios, and the Unreal Engine and Autodesk Maya source
files for each scenario. The files are publicly released as open source at
www.nuReality.org, to support the academic community studying the critical
AV-pedestrian interaction.",0.0062773935,-0.03282401,-0.0063147354,A
518,"An interesting direction for further research would be
task and a variant of the Change of Places with Obstacle ex-                           to explore the online-learning of human intentions further and
periment in which the central obstacle was replaced with a wall                        incorporate scene semantics.","Additional Capabilities                                                           velocity model while retaining the ability to improve over time
                                                                                       as a prior distribution is learnt over likely human goal inten-
   We additionally tested our approach in a Narrow Passage                             tions.",across half of the room.,2022-01-13 16:34:28+00:00,Motion Planning in Dynamic Environments Using Context-Aware Human Trajectory Prediction,cs.RO,['cs.RO'],"[arxiv.Result.Author('Mark Nicholas Finean'), arxiv.Result.Author('Luka Petrović'), arxiv.Result.Author('Wolfgang Merkt'), arxiv.Result.Author('Ivan Marković'), arxiv.Result.Author('Ioannis Havoutis')]","Over the years, the separate fields of motion planning, mapping, and human
trajectory prediction have advanced considerably. However, the literature is
still sparse in providing practical frameworks that enable mobile manipulators
to perform whole-body movements and account for the predicted motion of moving
obstacles. Previous optimisation-based motion planning approaches that use
distance fields have suffered from the high computational cost required to
update the environment representation. We demonstrate that GPU-accelerated
predicted composite distance fields significantly reduce the computation time
compared to calculating distance fields from scratch. We integrate this
technique with a complete motion planning and perception framework that
accounts for the predicted motion of humans in dynamic environments, enabling
reactive and pre-emptive motion planning that incorporates predicted motions.
To achieve this, we propose and implement a novel human trajectory prediction
method that combines intention recognition with trajectory optimisation-based
motion planning. We validate our resultant framework on a real-world Toyota
Human Support Robot (HSR) using live RGB-D sensor data from the onboard camera.
In addition to providing analysis on a publicly available dataset, we release
the Oxford Indoor Human Motion (Oxford-IHM) dataset and demonstrate
state-of-the-art performance in human trajectory prediction. The Oxford-IHM
dataset is a human trajectory prediction dataset in which people walk between
regions of interest in an indoor environment. Both static and robot-mounted
RGB-D cameras observe the people while tracked with a motion-capture system.",-0.003341172,0.024194919,-0.36771852,C
852,"Also,
     fostering further research in the area.",This dataset is made available to the public for         expensive but accrue signiﬁcant computational costs.,"intelligent malware might have the ability to recognize such
                                                                   testbeds and could potentially avoid discovery [26].",2022-01-20 22:11:38+00:00,RoboMal: Malware Detection for Robot Network Systems,cs.RO,"['cs.RO', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Upinder Kaur'), arxiv.Result.Author('Haozhe Zhou'), arxiv.Result.Author('Xiaxin Shen'), arxiv.Result.Author('Byung-Cheol Min'), arxiv.Result.Author('Richard M. Voyles')]","Robot systems are increasingly integrating into numerous avenues of modern
life. From cleaning houses to providing guidance and emotional support, robots
now work directly with humans. Due to their far-reaching applications and
progressively complex architecture, they are being targeted by adversarial
attacks such as sensor-actuator attacks, data spoofing, malware, and network
intrusion. Therefore, security for robotic systems has become crucial. In this
paper, we address the underserved area of malware detection in robotic
software. Since robots work in close proximity to humans, often with direct
interactions, malware could have life-threatening impacts. Hence, we propose
the RoboMal framework of static malware detection on binary executables to
detect malware before it gets a chance to execute. Additionally, we address the
great paucity of data in this space by providing the RoboMal dataset comprising
controller executables of a small-scale autonomous car. The performance of the
framework is compared against widely used supervised learning models: GRU, CNN,
and ANN. Notably, the LSTM-based RoboMal model outperforms the other models
with an accuracy of 85% and precision of 87% in 10-fold cross-validation, hence
proving the effectiveness of the proposed framework.",0.29902658,0.014618786,-0.16697197,C
939,"The simplified model of the
lower-limb system proposed in the paper is proven to be appropriate and can be taken for further research in the
future.","Originality/value - The MRAC is applied to control the humanoid’s lower limb, and the back-stepping process is
utilized to combine with an external SEA system but still maintain stabilization.","Keywords Series elastic actuators (SEAs), humanoid robot, back-stepping algorithm, model reference adaptive
control (MRAC).",2022-01-24 04:57:46+00:00,Hybrid Adaptive Control for Series Elastic Actuator of Humanoid Robot,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Anh Khoa Lanh Luu'), arxiv.Result.Author('Van Tu Duong'), arxiv.Result.Author('Huy Hung Nguyen'), arxiv.Result.Author('Sang Bong Kim'), arxiv.Result.Author('Tan Tien Nguyen')]","Generally, humanoid robots usually suffer significant impact force when
walking or running in a non-predefined environment that could easily damage the
actuators due to high stiffness. In recent years, the usages of passive,
compliant series elastic actuators (SEA) for driving humanoid's joints have
proved the capability in many aspects so far. However, despite being widely
applied in the biped robot research field, the stable control problem for a
humanoid powered by the SEAs, especially in the walking process, is still a
challenge. This paper proposes a model reference adaptive control (MRAC)
combined with the backstepping algorithm to deal with the parameter
uncertainties in a humanoid's lower limb driven by the SEA system. This is also
an extension of our previous research (Lanh et al.,2021). Firstly, a dynamic
model of SEA is obtained. Secondly, since there are unknown and uncertain
parameters in the SEA model, a model reference adaptive controller (MRAC) is
employed to guarantee the robust performance of the humanoid's lower limb.
Finally, an experiment is carried out to evaluate the effectiveness of the
proposed controller and the SEA mechanism.",-0.23203239,-0.1846906,0.27058792,A
1081,"V) and
interested to further study and improve the RL performance         the trajectory planning success rate (Plan R.) of all 25 exper-
for excavations with valid plans.",We are          methods using the average excavation volume (Avg.,imented excavations.,2022-01-27 02:59:56+00:00,Excavation Reinforcement Learning Using Geometric Representation,cs.RO,['cs.RO'],"[arxiv.Result.Author('Qingkai Lu'), arxiv.Result.Author('Yifan Zhu'), arxiv.Result.Author('Liangjun Zhang')]","Excavation of irregular rigid objects in clutter, such as fragmented rocks
and wood blocks, is very challenging due to their complex interaction dynamics
and highly variable geometries. In this paper, we adopt reinforcement learning
(RL) to tackle this challenge and learn policies to plan for a sequence of
excavation trajectories for irregular rigid objects, given point clouds of
excavation scenes. Moreover, we separately learn a compact representation of
the point cloud on geometric tasks that do not require human labeling. We show
that using the representation reduces training time for RL, while achieving
similar asymptotic performance compare to an end-to-end RL algorithm. When
using a policy trained in simulation directly on a real scene, we show that the
policy trained with the representation outperforms end-to-end RL. To our best
knowledge, this paper presents the first application of RL to plan a sequence
of excavation trajectories of irregular rigid objects in clutter.",0.122434564,0.012241341,0.051134236,C
1145,"reversals) can invalidate infer-
                                        encouraging and serve as the foundation for further research.","The validity, stability and explainability of the approach are                          Ignoring confounding bias (a.k.a.",ences in any domain [32].,2022-01-27 22:15:56+00:00,Empirical Estimates on Hand Manipulation are Recoverable: A Step Towards Individualized and Explainable Robotic Support in Everyday Activities,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Alexander Wich'), arxiv.Result.Author('Holger Schultheis'), arxiv.Result.Author('Michael Beetz')]","A key challenge for robotic systems is to figure out the behavior of another
agent. The capability to draw correct inferences is crucial to derive human
behavior from examples.
  Processing correct inferences is especially challenging when (confounding)
factors are not controlled experimentally (observational evidence). For this
reason, robots that rely on inferences that are correlational risk a biased
interpretation of the evidence.
  We propose equipping robots with the necessary tools to conduct observational
studies on people. Specifically, we propose and explore the feasibility of
structural causal models with non-parametric estimators to derive empirical
estimates on hand behavior in the context of object manipulation in a virtual
kitchen scenario. In particular, we focus on inferences under (the weaker)
conditions of partial confounding (the model covering only some factors) and
confront estimators with hundreds of samples instead of the typical order of
thousands. Studying these conditions explores the boundaries of the approach
and its viability.
  Despite the challenging conditions, the estimates inferred from the
validation data are correct. Moreover, these estimates are stable against three
refutation strategies where four estimators are in agreement. Furthermore, the
causal quantity for two individuals reveals the sensibility of the approach to
detect positive and negative effects.
  The validity, stability and explainability of the approach are encouraging
and serve as the foundation for further research.",0.18510804,-0.39549494,-0.23586841,C
1636,"Wood, and C. J.
                                                                                                           Walsh, “Soft robotic glove for combined assistance and at-home
valves and pump, encouraging further research on soft                                                      rehabilitation,” in Robotics and Autonomous Systems, Nov. 2015,
                                                                                                           vol.","robots are mostly bulky and heavy because of their                                                         P. Polygerinos, Z. Wang, K. C. Galloway, R. J.","73, pp.",2022-02-07 14:47:31+00:00,Air-Releasable Soft Robots for Explosive Ordnance Disposal,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Tyler C. Looney'), arxiv.Result.Author('Nathan M. Savard'), arxiv.Result.Author('Gus T. Teran'), arxiv.Result.Author('Archie G. Milligan'), arxiv.Result.Author('Ryley I. Wheelock'), arxiv.Result.Author('Michael Scalise'), arxiv.Result.Author('Daniel P. Perno'), arxiv.Result.Author('Gregory C. Lewin'), arxiv.Result.Author('Carlo Pinciroli'), arxiv.Result.Author('Cagdas D. Onal'), arxiv.Result.Author('Markus P. Nemitz')]","The demining of landmines using drones is challenging; air-releasable
payloads are typically non-intelligent (e.g., water balloons or explosives) and
deploying them at even low altitudes (~6 meter) is inherently inaccurate due to
complex deployment trajectories and constrained visual awareness by the drone
pilot. Soft robotics offers a unique approach for aerial demining, namely due
to the robust, low-cost, and lightweight designs of soft robots. Instead of
non-intelligent payloads, here, we propose the use of air-releasable soft
robots for demining. We developed a full system consisting of an unmanned
aerial vehicle retrofitted to a soft robot carrier including a custom-made
deployment mechanism, and an air-releasable, lightweight (296 g), untethered
soft hybrid robot with integrated electronics that incorporates a new type of a
vacuum-based flasher roller actuator system. We demonstrate a deployment cycle
in which the drone drops the soft robotic hybrid from an altitude of 4.5 m
meters and after which the robot approaches a dummy landmine. By deploying soft
robots at points of interest, we can transition soft robotic technologies from
the laboratory to real-world environments.",-0.40281743,0.03204195,0.16972524,A
1645,"More advanced SLAM, where the whole map is too large
for any one robot to carry, is clearly more difﬁcult to dis-       As the performance and scale of many-robot systems
tribute without a global server, and this will require much     may greatly improve due to work such as ours, it is impor-
further research.","inter-operable alternative to a single uniﬁed cloud maps so-
                                                                lution.",tant to consider potential ethical concerns.,2022-02-07 16:00:25+00:00,A Robot Web for Distributed Many-Device Localisation,cs.RO,"['cs.RO', 'cs.AI', 'cs.MA']","[arxiv.Result.Author('Riku Murai'), arxiv.Result.Author('Joseph Ortiz'), arxiv.Result.Author('Sajad Saeedi'), arxiv.Result.Author('Paul H. J. Kelly'), arxiv.Result.Author('Andrew J. Davison')]","We show that a distributed network of robots or other devices which make
measurements of each other can collaborate to globally localise via efficient
ad-hoc peer to peer communication. Our Robot Web solution is based on Gaussian
Belief Propagation on the fundamental non-linear factor graph describing the
probabilistic structure of all of the observations robots make internally or of
each other, and is flexible for any type of robot, motion or sensor. We define
a simple and efficient communication protocol which can be implemented by the
publishing and reading of web pages or other asynchronous communication
technologies. We show in simulations with up to 1000 robots interacting in
arbitrary patterns that our solution convergently achieves global accuracy as
accurate as a centralised non-linear factor graph solver while operating with
high distributed efficiency of computation and communication. Via the use of
robust factors in GBP, our method is tolerant to a high percentage of faults in
sensor measurements or dropped communication packets.",0.047676906,0.36286187,0.14100027,B
2045,"We hope that our work could                       arXiv:1707.06347, 2017.
facilitate the further study of real-time control on complex               [25] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,

                                                                                 and Martin Riedmiller.","arXiv preprint

nonlinearity on control terms.",Deterministic policy gradient algorithms.,2022-02-16 11:40:36+00:00,Deep Koopman Operator with Control for Nonlinear Systems,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Haojie Shi'), arxiv.Result.Author('Max Q. H. Meng')]","Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. We then design an auxiliary control network to encode
the nonlinear state-dependent control term to model the nonlinearity in control
input. For linear control, this encoded term is considered the new control
variable instead, ensuring the linearity of the embedding space. Then we deploy
Linear Quadratic Regulator (LQR) on the linear embedding space to derive the
optimal control policy and decode the actual control input from the control
net. Experimental results demonstrate that our approach outperforms other
existing methods, reducing the prediction error by order-of-magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and 7 Dof robotic manipulator.",-0.012701519,-0.15211403,0.08281276,C
2046,"Citeseer, 2004.
facilitate the further study of real-time control on complex
nonlinear systems.",We hope that our work could                          222–229.,"[19] Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba.",2022-02-16 11:40:36+00:00,Deep Koopman Operator with Control for Nonlinear Systems,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Haojie Shi'), arxiv.Result.Author('Max Q. -H. Meng')]","Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. Then, an auxiliary control network is augmented to
encode the nonlinear state-dependent control term to model the nonlinearity in
the control input. This encoded term is considered the new control variable
instead to ensure linearity of the modeled system in the embedding system.We
next deploy Linear Quadratic Regulator (LQR) on the linear embedding space to
derive the optimal control policy and decode the actual control input from the
control net. Experimental results demonstrate that our approach outperforms
other existing methods, reducing the prediction error by order of magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and the seven DOF robotic manipulator.",-0.08337835,-0.16234833,0.25106066,A
2136,"So it is
an open question for further research.","But
intuitively it is quite possible that several quasi-buckling points may exist for some particular initial configurations.","4 Manipulator stiffness in the neighborhood of the straight configuration

      Now let us consider in detail the manipulator stiffness behavior under the loading assuming that the initial
configuration is straight.",2022-02-17 14:51:02+00:00,Non-linear stiffness behavior of planar serial robotic manipulators,cs.RO,['cs.RO'],"[arxiv.Result.Author('Wanda Zhao'), arxiv.Result.Author('Alexandr Klimchik'), arxiv.Result.Author('Anatol Pashkevich'), arxiv.Result.Author('Damien Chablat')]","The paper focuses on the stiffness analysis of multi-link serial planar
manipulators, which may demonstrate nonlinear stiffness behavior under the
compressive loading. Two important cases are considered, where the manipulator
has either a straight or non-straight initial configuration. It was proved that
in the first case the loading may cause the buckling if it exceeds some
critical value, and the manipulator suddenly changes its straight shape and
stiffness properties. For computing this critical force, a general
eigenvalue-based technique was proposed that can be applied to any multi-link
serial manipulator. For the second case dealing with non-straight initial
configurations, a universal energy-based technique was applied that allowed to
detect quasi-buckling phenomenon when it is observed very fast but not instant
change of the manipulator shape and its stiffness coefficient under the
loading. These results are illustrated by numerous examples of non-linear
stiffness behavior of three-and four-link manipulators that are subjected to
compressive force.",-0.1215603,-0.27050078,0.2655033,A
2207,"CARLA Experiments
    Though the SUMO experiments validate the proposed method preliminar-

ily, we wish to further study the eﬀectiveness of safety exploration in a high-
ﬁdelity simulator.",4.2.,"Compared to SUMO simulator, CARLA simulator[24]

                                               20
             Table 3: ABLATION RESULTS

Framework              Method       Success rate(%) ↑  Average time(s) ↓
single-task              TD3                92.4               12.37
                                            94.5               13.08
multi-task        TD3+Attention             93.9               14.56
                    TD3+Safety              98.3              10.35
                                            94.9               13.13
             TD3+Safety+Attention           90.9               16.48
                pre-trained+Safety          91.2               7.52
                         TD3                93.2               17.04
                  TD3+Attention             97.1               9.40
                    TD3+Safety              91.7               7.73

             TD3+Safety+Attention
                pre-trained+Safety

provides abundant adjustable settings for building a high-ﬁdelity vehicles
model.",2022-02-19 17:09:46+00:00,Multi-task Safe Reinforcement Learning for Navigating Intersections in Dense Traffic,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Yuqi Liu'), arxiv.Result.Author('Qichao Zhang'), arxiv.Result.Author('Dongbin Zhao')]","Multi-task intersection navigation including the unprotected turning left,
turning right, and going straight in dense traffic is still a challenging task
for autonomous driving. For the human driver, the negotiation skill with other
interactive vehicles is the key to guarantee safety and efficiency. However, it
is hard to balance the safety and efficiency of the autonomous vehicle for
multi-task intersection navigation. In this paper, we formulate a multi-task
safe reinforcement learning with social attention to improve the safety and
efficiency when interacting with other traffic participants. Specifically, the
social attention module is used to focus on the states of negotiation vehicles.
In addition, a safety layer is added to the multi-task reinforcement learning
framework to guarantee safe negotiation. We compare the experiments in the
simulator SUMO with abundant traffic flows and CARLA with high-fidelity vehicle
models, which both show that the proposed algorithm can improve safety with
consistent traffic efficiency for multi-task intersection navigation.",0.06359663,-0.22833046,-0.12373851,C
2251,"On an ending note, we propose further research on down-                           Ieee, 2009.
stream usages of OG-SGG such as knowledge-graph driven                         [12] Ivan Donadello and Luciano Seraﬁni.","In 2009 IEEE
                                                                                     conference on computer vision and pattern recognition, pages 248–255.","Integration of numeric and sym-
image captioning or robotic visual question answering, futher                        bolic information for semantic image interpretation.",2022-02-21 13:23:15+00:00,OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer Learning for Telepresence Robotics,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Fernando Amodeo'), arxiv.Result.Author('Fernando Caballero'), arxiv.Result.Author('Natalia Díaz-Rodríguez'), arxiv.Result.Author('Luis Merino')]","Scene graph generation from images is a task of great interest to
applications such as robotics, because graphs are the main way to represent
knowledge about the world and regulate human-robot interactions in tasks such
as Visual Question Answering (VQA). Unfortunately, its corresponding area of
machine learning is still relatively in its infancy, and the solutions
currently offered do not specialize well in concrete usage scenarios.
Specifically, they do not take existing ""expert"" knowledge about the domain
world into account; and that might indeed be necessary in order to provide the
level of reliability demanded by the use case scenarios. In this paper, we
propose an initial approximation to a framework called Ontology-Guided Scene
Graph Generation (OG-SGG), that can improve the performance of an existing
machine learning based scene graph generator using prior knowledge supplied in
the form of an ontology; and we present results evaluated on a specific
scenario founded in telepresence robotics.",0.17310415,0.16418898,-0.3207863,B
2252,"It still takes a high K cutoﬀ to capture a sizable majority     On an ending note, we propose further research on down-
of the triplets present in ground truth annotations, indicating a    stream usages of OG-SGG such as knowledge-graph driven
need for better ﬁltering.","Nevertheless, there is margin for further reﬁnements and ﬁl-
tering.","The quality of the ﬁltering also depends   image captioning or robotic visual question answering, further
on how detailed the ontology is – naturally, the more axioms and     leveraging structured approaches to incorporating prior relevant
predicates that exist the more precise the predictions will be.",2022-02-21 13:23:15+00:00,OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer Learning for Telepresence Robotics,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Fernando Amodeo'), arxiv.Result.Author('Fernando Caballero'), arxiv.Result.Author('Natalia Díaz-Rodríguez'), arxiv.Result.Author('Luis Merino')]","Scene graph generation from images is a task of great interest to
applications such as robotics, because graphs are the main way to represent
knowledge about the world and regulate human-robot interactions in tasks such
as Visual Question Answering (VQA). Unfortunately, its corresponding area of
machine learning is still relatively in its infancy, and the solutions
currently offered do not specialize well in concrete usage scenarios.
Specifically, they do not take existing ""expert"" knowledge about the domain
world into account; and that might indeed be necessary in order to provide the
level of reliability demanded by the use case scenarios. In this paper, we
propose an initial approximation to a framework called Ontology-Guided Scene
Graph Generation (OG-SGG), that can improve the performance of an existing
machine learning based scene graph generator using prior knowledge supplied in
the form of an ontology; and we present results evaluated on a specific
scenario founded in telepresence robotics.",0.19134773,0.06848963,-0.38342428,C
2253,"intuition or by pre-existing knowledge; and as such take a con-
siderably higher amount of eﬀort to reﬁne, mostly through pure         On an ending note, we propose further research on down-
trial and error.","All of these methods,      of relationships between objects in a similar way to the scene
as with other non-XAI techniques, cannot be driven by human         graph generation problem.","stream usages of OG-SGG such as knowledge-graph driven
                                                                    image captioning or robotic visual question answering, further
   Nevertheless, there is margin for further reﬁnements and ﬁl-

                                                              17
Table 7: AI2THOR evaluation results on Simple Semantic model.",2022-02-21 13:23:15+00:00,OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer Learning for Telepresence Robotics,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Fernando Amodeo'), arxiv.Result.Author('Fernando Caballero'), arxiv.Result.Author('Natalia Díaz-Rodríguez'), arxiv.Result.Author('Luis Merino')]","Scene graph generation from images is a task of great interest to
applications such as robotics, because graphs are the main way to represent
knowledge about the world and regulate human-robot interactions in tasks such
as Visual Question Answering (VQA). Unfortunately, its corresponding area of
machine learning is still relatively in its infancy, and the solutions
currently offered do not specialize well in concrete usage scenarios.
Specifically, they do not take existing ""expert"" knowledge about the domain
world into account; and that might indeed be necessary in order to provide the
level of reliability demanded by the use case scenarios. In this paper, we
propose an initial approximation to a framework called Ontology-Guided Scene
Graph Generation (OG-SGG), that can improve the performance of an existing
machine learning based scene graph generator using prior knowledge supplied in
the form of an ontology (specifically, using the axioms defined within); and we
present results evaluated on a specific scenario founded in telepresence
robotics. These results show quantitative and qualitative improvements in the
generated scene graphs.",0.091304734,0.15471475,-0.40267885,B
2340,These VDEs are still relatively unexplored and may be fruitful avenues for further research.,"In contrast,
object comprehension and virtual robot alteration VDEs are the newest variants to be explored, first introduced in 2004
and 2006 respectively.","Overall, in this paper we have explored a wide range of research that demonstrates that VAM-HRI is an active and
rapidly growing research area.",2022-02-23 00:39:44+00:00,"Virtual, Augmented, and Mixed Reality for Human-Robot Interaction: A Survey and Virtual Design Element Taxonomy",cs.RO,"['cs.RO', 'cs.AI', 'cs.GR', 'cs.HC']","[arxiv.Result.Author('Michael Walker'), arxiv.Result.Author('Thao Phung'), arxiv.Result.Author('Tathagata Chakraborti'), arxiv.Result.Author('Tom Williams'), arxiv.Result.Author('Daniel Szafir')]","Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI)
has been gaining considerable attention in research in recent years. However,
the HRI community lacks a set of shared terminology and framework for
characterizing aspects of mixed reality interfaces, presenting serious problems
for future research. Therefore, it is important to have a common set of terms
and concepts that can be used to precisely describe and organize the diverse
array of work being done within the field. In this paper, we present a novel
taxonomic framework for different types of VAM-HRI interfaces, composed of four
main categories of virtual design elements (VDEs). We present and justify our
taxonomy and explain how its elements have been developed over the last 30
years as well as the current directions VAM-HRI is headed in the coming decade.",-0.16424663,0.058203418,-0.16423826,A
2417,"for this, further research is still required to effectively deploy
                                        them on-board mobile robots.",Although 3D scene graphs have already been utilized                  factors proposed on this work.,"do neither consider nor model the relation between these
                                                                                                                       elements to further constraint their geometry, which would
                                           To this end, we present in this paper a real-time online                    improve their performance and the understanding of the
                                        built Situational Graphs (S-Graphs), composed of a single graph                situation around the robot.",2022-02-24 16:59:06+00:00,Situational Graphs for Robot Navigation in Structured Indoor Environments,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Hriday Bavle'), arxiv.Result.Author('Jose Luis Sanchez-Lopez'), arxiv.Result.Author('Muhammad Shaheer'), arxiv.Result.Author('Javier Civera'), arxiv.Result.Author('Holger Voos')]","Autonomous mobile robots should be aware of their situation, understood as a
comprehensive understanding of the environment along with the estimation of its
own state, to successfully make decisions and execute tasks in natural
environments. 3D scene graphs are an emerging field of research with great
potential to represent these situations in a joint model comprising geometric,
semantic and relational/topological dimensions. Although 3D scene graphs have
already been utilized for this, further research is still required to
effectively deploy them on-board mobile robots.
  To this end, we present in this paper a real-time online built Situational
Graphs (S-Graphs), composed of a single graph representing the environment,
while simultaneously improving the robot pose estimation. Our method utilizes
odometry readings and planar surfaces extracted from 3D LiDAR scans, to
construct and optimize in real-time a three layered S-Graph that includes a
robot tracking layer where the robot poses are registered, a metric-semantic
layer with features such as planar walls and our novel topological layer
constraining higher-level features such as corridors and rooms. Our proposal
does not only demonstrate state-of-the-art results for pose estimation of the
robot, but also contributes with a metric-semantic-topological model of the
environment",-0.01760963,0.31556907,0.055161506,B
2418,"bined with SLAM techniques to provide robots with situational
                                       understanding, further research is still required to effectively               among others high-level task deﬁnitions, the localization and
                                       deploy them on-board mobile robots.",Although 3D scene graphs have already been com-                    factors proposed on this work.,"mapping performance in cluttered, repetitive or dynamic envi-
                                                                                                                      ronments.",2022-02-24 16:59:06+00:00,Situational Graphs for Robot Navigation in Structured Indoor Environments,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Hriday Bavle'), arxiv.Result.Author('Jose Luis Sanchez-Lopez'), arxiv.Result.Author('Muhammad Shaheer'), arxiv.Result.Author('Javier Civera'), arxiv.Result.Author('Holger Voos')]","Mobile robots should be aware of their situation, comprising the deep
understanding of their surrounding environment along with the estimation of its
own state, to successfully make intelligent decisions and execute tasks
autonomously in real environments. 3D scene graphs are an emerging field of
research that propose to represent the environment in a joint model comprising
geometric, semantic and relational/topological dimensions. Although 3D scene
graphs have already been combined with SLAM techniques to provide robots with
situational understanding, further research is still required to effectively
deploy them on-board mobile robots.
  To this end, we present in this paper a novel, real-time, online built
Situational Graph (S-Graph), which combines in a single optimizable graph, the
representation of the environment with the aforementioned three dimensions,
together with the robot pose. Our method utilizes odometry readings and planar
surfaces extracted from 3D LiDAR scans, to construct and optimize in real-time
a three layered S-Graph that includes (1) a robot tracking layer where the
robot poses are registered, (2) a metric-semantic layer with features such as
planar walls and (3) our novel topological layer constraining the planar walls
using higher-level features such as corridors and rooms. Our proposal does not
only demonstrate state-of-the-art results for pose estimation of the robot, but
also contributes with a metric-semantic-topological model of the environment",-0.101085916,0.4726746,-0.04708401,B
2433,"Mathematically, the
                                                  scales can be treated as inducing a ”Finsler metric” on the conﬁguration space, and this paper lays
                                                  the groundwork for further research into application of such Finsler metrics to robotic locomotion.","We present a geometric model for a single-joint undulating
                                                  system with scales and identify the features needed to understand its motion.","Keywords Nonholonomic Mechanisms and Systems · Nonholonomic Motion Planning · Kinematics

                                        1 Introduction

                                        To locomote, an animal or robot can deform its body in a pattern that interacts with the environment to create net
                                        motion.",2022-02-24 22:21:17+00:00,Scales and Locomotion: Non-Reversible Longitudinal Drag,cs.RO,"['cs.RO', 'physics.bio-ph']","[arxiv.Result.Author('Quinten Konyn'), arxiv.Result.Author('Ross L. Hatton')]","Locomotion requires that an animal or robot be able to move itself forward
farther than it moves backward in each gait cycle (formally, that it be able to
break the symmetry of its interactions with the world). Previous work has
established that a difference between lateral and longitudinal drag provides
sufficient conditions for locomotion to be possible. The geometric mechanics
community has used this principle to build a geometric framework for describing
the effectiveness and efficiency of undulatory locomotion. Researchers in
biology and robotics have observed that structures such as snake scales
additionally provide a difference between forward and backward longitudinal
drag. As yet, however, the impact of scales on the geometric features relevant
to locomotion effectiveness and efficiency have not yet been explored. We
present a geometric model for a single-joint undulating system with scales and
identify the features needed to understand its motion. Mathematically, the
scales can be treated as inducing a ""Finsler metric"" on the configuration
space, and this paper lays the groundwork for further research into application
of such Finsler metrics to robotic locomotion.",-0.2500842,0.034502164,0.17203225,A
2440,"Since our preliminary experiments utilizing me-
The motion model assumes that each tile is subjected to          chanically actuated bristle-bots indicated a lack of reliability
the same magnitude of force and torque but acts in the           and repeatability, our further research will probably move
opposite direction.",and tested.,"To this end, we implemented a physics-       towards the actuation via a magnetic ﬁeld [21].",2022-02-25 07:49:07+00:00,Self-Stabilizing Self-Assembly,cs.RO,"['cs.RO', 'cond-mat.soft', 'cs.ET']","[arxiv.Result.Author('M. Jílek'), arxiv.Result.Author('K. Stránská'), arxiv.Result.Author('M. Somr'), arxiv.Result.Author('M. Kulich'), arxiv.Result.Author('J. Zeman'), arxiv.Result.Author('L. Přeučil')]","The emerging field of passive macro-scale tile-based self-assembly (TBSA)
holds promise for enabling effective manufacturing processes by harnessing
TBSA's intrinsic parallelism. However, the current TBSA methodology still does
not fulfill its potential, largely because such assemblies are typically
error-prone and the size of an individual assembly is limited by a lack of
mechanical stability. Moreover, the instability issue becomes worse as
assemblies become larger. Here, using a novel type of tile that is carried by a
bristle-bot drive, we propose a framework that reverts this tendency; i.e., as
an assembly grows, it becomes more stable. Using physics-based computational
experiments, we show that a system of such tiles indeed possesses
self-stabilizing characteristics and enables building assemblies containing
hundreds of tiles. These results indicate that one of the main current
limitations of mechanical, agitation-based TBSA approaches might be overcome by
employing a swarm of free-running sensorless mobile robots, herein represented
by passive tiles at the macroscopic scale.",-0.37073624,-0.21206236,0.13553229,A
2441,"ments utilizing mechanically actuated bristle-bots indicated a
This introduces a tile motion model as a necessary part of       lack of reliability and repeatability, our further research will
the tilesystem description.","Since our preliminary experi-
controlled by a single parameter (amplitude or frequency).",move in the direction of actuation via a magnetic ﬁeld [25].,2022-02-25 07:49:07+00:00,Self-Stabilizing Self-Assembly,cs.RO,"['cs.RO', 'cond-mat.soft', 'cs.ET']","[arxiv.Result.Author('M. Jílek'), arxiv.Result.Author('K. Stránská'), arxiv.Result.Author('M. Somr'), arxiv.Result.Author('M. Kulich'), arxiv.Result.Author('J. Zeman'), arxiv.Result.Author('L. Přeučil')]","The emerging field of passive macro-scale tile-based self-assembly (TBSA)
shows promise in enabling effective manufacturing processes by harnessing
TBSA's intrinsic parallelism. However, current TBSA methodologies still do not
fulfill their potentials, largely because such assemblies are often prone to
errors, and the size of an individual assembly is limited due to insufficient
mechanical stability. Moreover, the instability issue worsens as assemblies
grow in size. Using a novel type of magnetically-bonded tiles carried by
bristle-bot drives, we propose here a framework that reverses this tendency;
i.e., as an assembly grows, it becomes more stable. Stability is achieved by
introducing two sets of tiles that move in opposite directions, thus zeroing
the assembly net force. Using physics-based computational experiments, we
compare the performance of the proposed approach with the common orbital
shaking method, proving that the proposed system of tiles indeed possesses
self-stabilizing characteristics. Our approach enables assemblies containing
hundreds of tiles to be built, while the shaking approach is inherently limited
to a few tens of tiles. Our results indicate that one of the primary
limitations of mechanical, agitation-based TBSA approaches, instability, might
be overcome by employing a swarm of free-running, sensorless mobile robots,
herein represented by passive tiles at the macroscopic scale.",-0.393734,-0.18937767,0.16922206,A
2632,"3D non-linear dynamical model of our cable-driven robotic
                                                                    eye and compared linear and nonlinear approximations for
   7) Current limitations and further study                         its open-loop optimal control.","We developed a full
a follow-up study.","Both approximations led to
   Although the simulated model (Fig.",2022-03-01 14:36:31+00:00,Emergence of human oculomotor behavior from optimal control of a cable-driven biomimetic robotic eye,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Reza Javanmard Alitappeh'), arxiv.Result.Author('Akhil John'), arxiv.Result.Author('Bernardo Dias'), arxiv.Result.Author('A. John van Opstal'), arxiv.Result.Author('Alexandre Bernardino')]","In human-robot interactions, eye movements play an important role in
non-verbal communication. However, controlling the motions of a robotic eye
that display similar performance as the human oculomotor system is still a
major challenge. In this paper, we study how to control a realistic model of
the human eye with a cable-driven actuation system that mimics the six degrees
of freedom of the extra-ocular muscles. The biomimetic design introduces novel
challenges to address, most notably the need to control the pretension on each
individual muscle to prevent the loss of tension during motion, that would lead
to cable slack and lack of control. We built a robotic prototype and developed
a nonlinear simulator and two controllers. In the first approach, we linearized
the nonlinear model, using a local derivative technique, and designed
linear-quadratic optimal controllers to optimize a cost function that accounts
for accuracy, energy expenditure, and movement duration. The second method uses
a recurrent neural network that learns the nonlinear system dynamics from
sample trajectories of the system, and a non-linear trajectory optimization
solver that minimizes a similar cost function. We focused on the generation of
rapid saccadic eye movements with fully unconstrained kinematics, and the
generation of control signals for the six cables that simultaneously satisfied
several dynamic optimization criteria. The model faithfully mimics the
three-dimensional rotational kinematics and dynamics observed for human
saccades. Our experimental results indicate that while both methods yielded
similar results, the nonlinear method is more flexible for future improvements
to the model, for which the calculations of the linearized model's
position-dependent pretensions and local derivatives become particularly
tedious.",-0.25683278,-0.04831763,0.14390494,A
2635,"Moreover, further research is needed in
                                        the industrial domain for parametric robots’ capabilities while                  *                 *           *
                                        ensuring safety.","+ primitives: Primitive[]
                                        Therefore, this work emphasizes that a taxonomy based on task,
                                        skill and primitives should be used by future works to align                + execute()
                                        with existing literature.","Parameter    DerivedParameter *         Primitive

                                           Index Terms—PPR, HMLV, task, skill, primitive, robot,                                                      + execute()
                                        review, survey
                                                                                                                    *
                                                                 I.",2022-03-01 15:17:41+00:00,Capability-based Frameworks for Industrial Robot Skills: a Survey,cs.RO,['cs.RO'],"[arxiv.Result.Author('Matteo Pantano'), arxiv.Result.Author('Thomas Eiband'), arxiv.Result.Author('Dongheui Lee')]","The research community is puzzled with words like skill, action, atomic unit
and others when describing robots' capabilities. However, for giving the
possibility to integrate capabilities in industrial scenarios, a
standardization of these descriptions is necessary. This work uses a structured
review approach to identify commonalities and differences in the research
community of robots' skill frameworks. Through this method, 210 papers were
analyzed and three main results were obtained. First, the vast majority of
authors agree on a taxonomy based on task, skill and primitive. Second, the
most investigated robots' capabilities are pick and place. Third, industrial
oriented applications focus more on simple robots' capabilities with fixed
parameters while ensuring safety aspects. Therefore, this work emphasizes that
a taxonomy based on task, skill and primitives should be used by future works
to align with existing literature. Moreover, further research is needed in the
industrial domain for parametric robots' capabilities while ensuring safety.",-0.20794894,-0.05732751,0.05033493,A
2849,"Traditional cloud computing provides
further research.","The cloud service provider of serverless
allows real-time map estimation through parallel computing           applications automatically provisions, scales, and manages the
and implements a highly sophisticated system using low-cost          infrastructure required to run the code, enabling developers to
components, thus lowering the technological threshold for            build applications faster.","users with the computer resources whether they use them or
                                                                     not, while serverless computing allows users to dynamically
   Concerning complex tasks that a single robot is not able          pull only the resources they need.",2022-03-04 23:18:46+00:00,Sustainable Verbal and Non-verbal Human-Robot Interaction Through Cloud Services,cs.RO,['cs.RO'],"[arxiv.Result.Author('Lucrezia Grassi'), arxiv.Result.Author('Carmine Tommaso Recchiuto'), arxiv.Result.Author('Antonio Sgorbissa')]","This article presents the design and the implementation of CAIR: a cloud
system for knowledge-based autonomous interaction devised for Social Robots and
other conversational agents. The system is particularly convenient for low-cost
robots and devices. Developers are provided with a sustainable solution to
manage verbal and non-verbal interaction through a network connection, with
about 3,000 topics of conversation ready for ""chit-chatting"" and a library of
pre-cooked plans that only needs to be grounded into the robot's physical
capabilities. The system is structured as a set of REST API endpoints so that
it can be easily expanded by adding new APIs to improve the capabilities of the
clients connected to the cloud. Another key feature of the system is that it
has been designed to make the development of its clients straightforward: in
this way, multiple devices can be easily endowed with the capability of
autonomously interacting with the user, understanding when to perform specific
actions, and exploiting all the information provided by cloud services. The
article outlines and discusses the results of the experiments performed to
assess the system's performance in terms of response time, paving the way for
its use both for research and market solutions. Links to repositories with
clients for ROS and popular robots such as Pepper and NAO are given.",0.12901154,0.38034603,0.23063661,B
2850,"Traditional cloud computing provides
further research.","The cloud service provider of serverless
allows real-time map estimation through parallel computing           applications automatically provisions, scales, and manages the
and implements a highly sophisticated system using low-cost          infrastructure required to run the code, enabling developers to
components, thus lowering the technological threshold for            build applications faster.","users with the computer resources whether they use them or
                                                                     not, while serverless computing allows users to dynamically
   Concerning complex tasks that a single robot is not able          pull only the resources they need.",2022-03-04 23:18:46+00:00,Sustainable Verbal and Non-verbal Human-Robot Interaction Through Cloud Services,cs.RO,['cs.RO'],"[arxiv.Result.Author('Lucrezia Grassi'), arxiv.Result.Author('Carmine Tommaso Recchiuto'), arxiv.Result.Author('Antonio Sgorbissa')]","This article presents the design and the implementation of CAIR: a cloud
system for knowledge-based autonomous interaction devised for Social Robots and
other conversational agents. The system is particularly convenient for low-cost
robots and devices. Developers are provided with a sustainable solution to
manage verbal and non-verbal interaction through a network connection, with
about 3,000 topics of conversation ready for ""chit-chatting"" and a library of
pre-cooked plans that only needs to be grounded into the robot's physical
capabilities. The system is structured as a set of REST API endpoints so that
it can be easily expanded by adding new APIs to improve the capabilities of the
clients connected to the cloud. Another key feature of the system is that it
has been designed to make the development of its clients straightforward: in
this way, multiple devices can be easily endowed with the capability of
autonomously interacting with the user, understanding when to perform specific
actions, and exploiting all the information provided by cloud services. The
article outlines and discusses the results of the experiments performed to
assess the system's performance in terms of response time, paving the way for
its use both for research and market solutions. Links to repositories with
clients for ROS and popular robots such as Pepper and NAO are given.",0.12901154,0.38034603,0.23063661,B
2851,"Regarding the interaction capabilities of
ping (SLAM) system consisting of low-cost robots           such conversational agents, they have many draw-
and remote Amazon servers, which allows real-time          backs: it is not easy to customize them to show
map estimation through parallel computing and im-          good performance in addressing many diﬀerent top-
plements a highly sophisticated system using low-          ics; they cannot properly manage the dialogue; they
cost components, thus lowering the technological           are not easy to integrate with the actions that a
threshold for further research.","by such cognitive services, building a full conver-
                                                           sational agent that meets the requirements for so-
   To explore the beneﬁts of cloud-based technolo-         cial interaction is still challenging both concerning
gies for sensing, [14] presented a cloud-based collab-     interaction capabilities and from the technical per-
orative visual Simultaneous Localization and Map-          spective.","robot can perform; they are not “grounded” with
                                                           the data acquired by their sensors; they do not have
   Concerning complex tasks that a single robot is         a system for knowledge representation [34]; they are
not able to perform, cloud robotics is widely ac-          available only in English.",2022-03-04 23:18:46+00:00,Sustainable Verbal Human-Robot Interaction Through Cloud Services,cs.RO,['cs.RO'],"[arxiv.Result.Author('Lucrezia Grassi'), arxiv.Result.Author('Carmine Tommaso Recchiuto'), arxiv.Result.Author('Antonio Sgorbissa')]","This article presents the design and the implementation of CAIR: a cloud
system for knowledge-based autonomous interaction devised for Social Robots and
other conversational agents. The system is particularly convenient for low-cost
robots and devices: it can be used as a stand-alone dialogue system or as an
integration to provide ""background"" dialogue capabilities to any preexisting
Natural Language Processing ability that the robot may already have as part of
its basic skills. By connecting to CAIR, developers are provided with a
sustainable solution to manage verbal interaction through a network connection,
with about 3,000 topics of conversation ready for ""chit-chatting"" and a library
of pre-cooked plans that only needs to be grounded into the robot's physical
capabilities. The system is structured as a set of REST API endpoints so that
it can be easily expanded by adding new APIs to improve the capabilities of the
clients connected to the cloud. Another key feature of the system is that it
has been designed to make the development of its clients straightforward: in
this way, multiple robots and devices can be easily endowed with the capability
of autonomously interacting with the user, understanding when to perform
specific actions, and exploiting all the information provided by cloud
services. The article outlines and discusses the results of the experiments
performed to assess the system's performance in terms of response time, paving
the way for its use both for research and market solutions. Links to
repositories with clients for ROS and popular robots such as Pepper and NAO are
given.",-0.03179232,0.41186804,-0.023659196,B
2853,"[5] M. Rubenstein, C. Ahler, and R. Nagpal, “Kilobot: A low cost scalable
However, further research should be made to ensure that the           robot system for collective behaviors,” in 2012 IEEE International
current performance of the SwarmUS system is sufﬁcient.","The SwarmUS platforms ensures those standards what-
ever the robotics platform because of its ﬁxed hardware.","Conference on Robotics and Automation, May 2012, pp.",2022-03-05 02:20:18+00:00,SwarmUS: An open hardware and software on-board platform for swarm robotics development,cs.RO,"['cs.RO', 'cs.MA', 'cs.SY', 'eess.SY', 'C.3; J.7; I.2.9; C.2.4; B.1.5']","[arxiv.Result.Author('Étienne Villemure'), arxiv.Result.Author('Philippe Arsenault'), arxiv.Result.Author('Gabriel Lessard'), arxiv.Result.Author('Thierry Constantin'), arxiv.Result.Author('Hubert Dubé'), arxiv.Result.Author('Louis-Daniel Gaulin'), arxiv.Result.Author('Xavier Groleau'), arxiv.Result.Author('Samuel Laperrière'), arxiv.Result.Author('Charles Quesnel'), arxiv.Result.Author('François Ferland')]","Real life implementations of distributed swarm robotics are rare. The
standardization of a general purpose swarm robotics platform could greatly
accelerate swarm robotics towards real life implementations. The SwarmUS
platform is an open-source hardware and software on-board embedded system
designed to be added onto existing robots while providing them with swarm
features, thus proposing a new take on the platform standardization problem.
These features include a distributed relative localization system based on
Ultra-Wideband, a local communication system based on Wi-Fi and a distributed
coordination system based on the Buzz programming language between robots
connected within a SwarmUS platform. Additionally, a human-swarm interaction
mobile application and an emulation of the platform in the Robot Operating
System (ROS) is presented. Finally, an implementation of the system was
realized and tested on two types of robots : a TurtleBot3 Burger and two
Pioneer 2DX.",-0.1481252,0.11418849,0.2832003,A
2889,"While there are many frontiers in dynamic aerial grasping                    [13] “Application of ﬁnray effect approach for production process automa-
that require further research, we see the more precise mod-                           tion,” 2011.
eling of aerodynamic ground effects for reference tracking
as the main challenge to consistently pick up small objects,                    [14] M. H. Ali, A. Zhanabayev, S. Khamzhin, and K. Mussin, “Biologically
especially when the drone is operating close to surfaces.",2015.,"inspired gripper based on the ﬁn ray effect,” 2019 5th International
Moreover, future work will require to address the challenges                          Conference on Control, Automation and Robotics (ICCAR), pp.",2022-03-06 18:05:35+00:00,RAPTOR: Rapid Aerial Pickup and Transport of Objects by Robots,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Aurel Appius'), arxiv.Result.Author('Erik Bauer'), arxiv.Result.Author('Marc Blöchlinger'), arxiv.Result.Author('Aashi Kalra'), arxiv.Result.Author('Robin Oberson'), arxiv.Result.Author('Arman Raayatsanati'), arxiv.Result.Author('Pascal Strauch'), arxiv.Result.Author('Sarath Suresh'), arxiv.Result.Author('Marco von Salis'), arxiv.Result.Author('Robert K. Katzschmann')]","Rapid aerial grasping promises vast applications that utilize the dynamic
picking up and placing of objects by robots. Rigid grippers traditionally used
in aerial manipulators require very high precision and specific object
geometries for successful grasping. We propose RAPTOR, a quadcopter platform
combined with a custom Fin Ray gripper to enable a more flexible grasping of
objects with different geometries, leveraging the properties of soft materials
to increase the contact surface between the gripper and the objects. To reduce
the communication latency, we present a novel FastDDS-based middleware solution
as an alternative to ROS (Robot Operating System). We show that RAPTOR achieves
an average of 83% grasping efficacy in a real-world setting for four different
object geometries while moving at an average velocity of 1 m/s during grasping,
which is approximately five times faster than the state-of-the-art while
supporting up to four times the payload. Our results further solidify the
potential of quadcopters in warehouses and other automated pick-and-place
applications over longer distances where speed and robustness become essential.",-0.30008218,0.15413918,0.13024354,A
2890,"While there are many frontiers in dynamic aerial grasping
                                                                               that require further research, we see the more precise model-
                                                                               ing of aerodynamic ground effects for reference tracking as
                                                                               the main challenge to consistently picking up small objects,
                                                                               especially when the drone is operating close to surfaces.","In particular, we were able to show robust grasping at high
                                                                               speeds—with average velocities of 1 m s−1—while our quad-
                                                                               copter could also grasp smaller objects that have additional
                                                                               requirements for reference tracking, showing potential for a
                                                                               wider range of applications.","Moreover, future work will require to address the challenges
                                                                               associated with making the platform independent of motion
                                                                               capture systems while exploring the picking up of objects
                                                                               without a stand due to the additional force caused by the
                                                                               propeller thrust.",2022-03-06 18:05:35+00:00,RAPTOR: Rapid Aerial Pickup and Transport of Objects by Robots,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Aurel Appius'), arxiv.Result.Author('Erik Bauer'), arxiv.Result.Author('Marc Blöchlinger'), arxiv.Result.Author('Aashi Kalra'), arxiv.Result.Author('Robin Oberson'), arxiv.Result.Author('Arman Raayatsanati'), arxiv.Result.Author('Pascal Strauch'), arxiv.Result.Author('Sarath Suresh'), arxiv.Result.Author('Marco von Salis'), arxiv.Result.Author('Robert K. Katzschmann')]","Rapid aerial grasping through robots can lead to many applications that
utilize fast and dynamic picking and placing of objects. Rigid grippers
traditionally used in aerial manipulators require high precision and specific
object geometries for successful grasping. We propose RAPTOR, a quadcopter
platform combined with a custom Fin Ray gripper to enable more flexible
grasping of objects with different geometries, leveraging the properties of
soft materials to increase the contact surface between the gripper and the
objects. To reduce the communication latency, we present a new lightweight
middleware solution based on Fast DDS (Data Distribution Service) as an
alternative to ROS (Robot Operating System). We show that RAPTOR achieves an
average of 83% grasping efficacy in a real-world setting for four different
object geometries while moving at an average velocity of 1 m/s during grasping.
In a high-velocity setting, RAPTOR supports up to four times the payload
compared to previous works. Our results highlight the potential of aerial
drones in automated warehouses and other manipulation applications where speed,
swiftness, and robustness are essential while operating in hard-to-reach
places.",-0.33716097,0.19370432,0.17459694,A
2905,"The boundaries and scope
Third, we also discuss open research questions and opportuni-              of our corpus may not be clear cut, and as with any selection of
ties that facilitate further research in this field.",plete or exhaustive list in this domain.,"We believe that       papers, there were many papers right at the boundaries of our
our taxonomy — with the design classifications and their insights,         inclusion/exclusion criteria.",2022-03-07 10:22:59+00:00,Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces,cs.RO,"['cs.RO', 'cs.CV', 'cs.HC']","[arxiv.Result.Author('Ryo Suzuki'), arxiv.Result.Author('Adnan Karim'), arxiv.Result.Author('Tian Xia'), arxiv.Result.Author('Hooman Hedayati'), arxiv.Result.Author('Nicolai Marquardt')]","This paper contributes to a taxonomy of augmented reality and robotics based
on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have
emerged as a new way to enhance human-robot interaction (HRI) and robotic
interfaces (e.g., actuated and shape-changing interfaces). Recently, an
increasing number of studies in HCI, HRI, and robotics have demonstrated how AR
enables better interactions between people and robots. However, often research
remains focused on individual explorations and key design strategies, and
research questions are rarely analyzed systematically. In this paper, we
synthesize and categorize this research field in the following dimensions: 1)
approaches to augmenting reality; 2) characteristics of robots; 3) purposes and
benefits; 4) classification of presented information; 5) design components and
strategies for visual augmentation; 6) interaction techniques and modalities;
7) application domains; and 8) evaluation strategies. We formulate key
challenges and opportunities to guide and inform future research in AR and
robotics.",0.19606593,-0.31514835,-0.1660047,C
2975,"As further research,                 2020.3022506.
we will tackle shortcomings in the challenging data association
by introducing a probabilistic approach where multiple hypothe-                Henry P, Krainin M, Herbst E, Ren X, Fox D. Rgb-d mapping: Us-
ses will be considered before taking an association decision.",designed our system to operate in real-time.,"ing kinect-style depth cameras for dense 3d modeling of indoor environ-
                                                                                  ments.",2022-03-08 09:14:37+00:00,An Online Semantic Mapping System for Extending and Enhancing Visual SLAM,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Thorsten Hempel'), arxiv.Result.Author('Ayoub Al-Hamadi')]","We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods.",0.10590644,0.29622307,-0.215858,B
3061,"Hence, we remove the use of reference or nominal
further research of learning torque control.","This representation is consid-     might vary with different environments and initial conﬁgu-
ered to serve as a building block and baseline for potential       rations.","Thus, we design       torques and instead use a scaling vector to give ﬁnal torques
a compact observation space that can alleviate overﬁtting to       to the motor controller.",2022-03-10 07:09:05+00:00,Learning Torque Control for Quadrupedal Locomotion,cs.RO,"['cs.RO', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Shuxiao Chen'), arxiv.Result.Author('Bike Zhang'), arxiv.Result.Author('Mark W. Mueller'), arxiv.Result.Author('Akshara Rai'), arxiv.Result.Author('Koushil Sreenath')]","Reinforcement learning (RL) is a promising tool for developing controllers
for quadrupedal locomotion. The design of most learning-based locomotion
controllers adopts the joint position-based paradigm, wherein a low-frequency
RL policy outputs target joint positions that are then tracked by a
high-frequency proportional-derivative (PD) controller that outputs joint
torques. However, the low frequency of such a policy hinders the advancement of
highly dynamic locomotion behaviors. Moreover, determining the PD gains for
optimal tracking performance is laborious and dependent on the task at hand. In
this paper, we introduce a learning torque control framework for quadrupedal
locomotion, which trains an RL policy that directly predicts joint torques at a
high frequency, thus circumventing the use of PD controllers. We validate the
proposed framework with extensive experiments where the robot is able to both
traverse various terrains and resist external pushes, given user-specified
commands. To our knowledge, this is the first attempt of learning torque
control for quadrupedal locomotion with an end-to-end single neural network
that has led to successful real-world experiments among recent research on
learning-based quadrupedal locomotion which is mostly position-based.",-0.2202436,-0.22190642,0.015959173,A
3117,"Block2D: Evaluation results for PLATO for both scripted play            To further study the effects of encoding just the interaction
data and human play data are shown in Table I, along with            period in the latent space, we implement PLATO-PRE, a ver-
results on PLATO-PRE and PLATO-R. On scripted play data,             sion of PLATO in which the posterior encodes sampled trajec-
PLATO is able to substantially outperform the baselines on           tories from both the pre-interaction period τ (−) and interaction
every task.","best performing architectures for each method on human play
                                                                     data involved larger policy hidden sizes as compared to the
E. Results                                                           best performing methods on scripted play data.","Play-LMP and Play-GCBC get roughly similar               period τ (i), instead of just the interaction period.",2022-03-10 20:41:00+00:00,PLATO: Predicting Latent Affordances Through Object-Centric Play,cs.RO,['cs.RO'],"[arxiv.Result.Author('Suneel Belkhale'), arxiv.Result.Author('Dorsa Sadigh')]","Constructing a diverse repertoire of manipulation skills in a scalable
fashion remains an unsolved challenge in robotics. One way to address this
challenge is with unstructured human play, where humans operate freely in an
environment to reach unspecified goals. Play is a simple and cheap method for
collecting diverse user demonstrations with broad state and goal coverage over
an environment. Due to this diverse coverage, existing approaches for learning
from play are more robust to online policy deviations from the offline data
distribution. However, these methods often struggle to learn under scene
variation and on challenging manipulation primitives, due in part to improperly
associating complex behaviors to the scene changes they induce. Our insight is
that an object-centric view of play data can help link human behaviors and the
resulting changes in the environment, and thus improve multi-task policy
learning. In this work, we construct a latent space to model object affordances
-- properties of an object that define its uses -- in the environment, and then
learn a policy to achieve the desired affordances. By modeling and predicting
the desired affordance across variable horizon tasks, our method, Predicting
Latent Affordances Through Object-Centric Play (PLATO), outperforms existing
methods on complex manipulation tasks in both 2D and 3D object manipulation
simulation environments for diverse types of interactions. Videos can be found
on our website: https://tinyurl.com/2m8wn449",0.14403248,-0.12740892,-0.20005059,C
3118,"To further study the effects of encoding
just the interaction period in the latent space, we implement PLATO-PRE, a version of PLATO
in which the posterior encodes sampled trajectories from both the pre-interaction period τ (−) and
interaction period τ (i), instead of just the interaction period.","Play-LMP and Play-GCBC get roughly similar performance on most tasks, and struggle the most on
tasks involving the tether action (Pulling and Side-Rotate).","We see that PLATO and PLATO-
PRE do roughly equivalently on average, with PLATO doing substantially better on Side-Rotate,
but PLATO-PRE doing better on the lift primitive.",2022-03-10 20:41:00+00:00,PLATO: Predicting Latent Affordances Through Object-Centric Play,cs.RO,['cs.RO'],"[arxiv.Result.Author('Suneel Belkhale'), arxiv.Result.Author('Dorsa Sadigh')]","Constructing a diverse repertoire of manipulation skills in a scalable
fashion remains an unsolved challenge in robotics. One way to address this
challenge is with unstructured human play, where humans operate freely in an
environment to reach unspecified goals. Play is a simple and cheap method for
collecting diverse user demonstrations with broad state and goal coverage over
an environment. Due to this diverse coverage, existing approaches for learning
from play are more robust to online policy deviations from the offline data
distribution. However, these methods often struggle to learn under scene
variation and on challenging manipulation primitives, due in part to improperly
associating complex behaviors to the scene changes they induce. Our insight is
that an object-centric view of play data can help link human behaviors and the
resulting changes in the environment, and thus improve multi-task policy
learning. In this work, we construct a latent space to model object
\textit{affordances} -- properties of an object that define its uses -- in the
environment, and then learn a policy to achieve the desired affordances. By
modeling and predicting the desired affordance across variable horizon tasks,
our method, Predicting Latent Affordances Through Object-Centric Play (PLATO),
outperforms existing methods on complex manipulation tasks in both 2D and 3D
object manipulation simulation and real world environments for diverse types of
interactions. Videos can be found on our website: https://tinyurl.com/4u23hwfv",-0.0116043575,-0.20727077,-0.16955668,C
3119,"To further study the effects of encoding
just the interaction period in the latent space, we implement PLATO-PRE, a version of PLATO
in which the posterior encodes sampled trajectories from both the pre-interaction period τ (−) and
interaction period τ (i), instead of just the interaction period.","Play-LMP and Play-GCBC get roughly similar performance on most tasks, and struggle the most on
tasks involving the tether action (Pulling and Side-Rotate).","We see that PLATO and PLATO-
PRE do roughly equivalently on average, with PLATO doing substantially better on Side-Rotate,
but PLATO-PRE doing better on the lift primitive.",2022-03-10 20:41:00+00:00,PLATO: Predicting Latent Affordances Through Object-Centric Play,cs.RO,['cs.RO'],"[arxiv.Result.Author('Suneel Belkhale'), arxiv.Result.Author('Dorsa Sadigh')]","Constructing a diverse repertoire of manipulation skills in a scalable
fashion remains an unsolved challenge in robotics. One way to address this
challenge is with unstructured human play, where humans operate freely in an
environment to reach unspecified goals. Play is a simple and cheap method for
collecting diverse user demonstrations with broad state and goal coverage over
an environment. Due to this diverse coverage, existing approaches for learning
from play are more robust to online policy deviations from the offline data
distribution. However, these methods often struggle to learn under scene
variation and on challenging manipulation primitives, due in part to improperly
associating complex behaviors to the scene changes they induce. Our insight is
that an object-centric view of play data can help link human behaviors and the
resulting changes in the environment, and thus improve multi-task policy
learning. In this work, we construct a latent space to model object affordances
-- properties of an object that define its uses -- in the environment, and then
learn a policy to achieve the desired affordances. By modeling and predicting
the desired affordance across variable horizon tasks, our method, Predicting
Latent Affordances Through Object-Centric Play (PLATO), outperforms existing
methods on complex manipulation tasks in both 2D and 3D object manipulation
simulation and real world environments for diverse types of interactions.
Videos can be found on our website: https://tinyurl.com/4u23hwfv",-0.0116043575,-0.20727077,-0.16955668,C
3125,"We include     C. Capturing Real Images
initial results using both DOPE [4] and CosyPose [3], and
we release pre-trained network weights both as an off-the-          To acquire images for the dataset, we placed the real
shelf system for use in robotics labs and as a baseline method   objects in ten different environments, with ﬁve object ar-
for further research in this area.","BOP benchmark evaluation server2 provides the ability to
compute metrics against the test set annotations.",We also provide 3D object     rangements / camera poses per environment.,2022-03-11 01:19:04+00:00,6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An Accessible Dataset and Benchmark,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Stephen Tyree'), arxiv.Result.Author('Jonathan Tremblay'), arxiv.Result.Author('Thang To'), arxiv.Result.Author('Jia Cheng'), arxiv.Result.Author('Terry Mosier'), arxiv.Result.Author('Jeffrey Smith'), arxiv.Result.Author('Stan Birchfield')]","We present a new dataset for 6-DoF pose estimation of known objects, with a
focus on robotic manipulation research. We propose a set of toy grocery
objects, whose physical instantiations are readily available for purchase and
are appropriately sized for robotic grasping and manipulation. We provide 3D
scanned textured models of these objects, suitable for generating synthetic
training data, as well as RGBD images of the objects in challenging, cluttered
scenes exhibiting partial occlusion, extreme lighting variations, multiple
instances per image, and a large variety of poses. Using semi-automated
RGBD-to-model texture correspondences, the images are annotated with ground
truth poses that were verified empirically to be accurate to within a few
millimeters. We also propose a new pose evaluation metric called {ADD-H} based
upon the Hungarian assignment algorithm that is robust to symmetries in object
geometry without requiring their explicit enumeration. We share pre-trained
pose estimators for all the toy grocery objects, along with their baseline
performance on both validation and test sets. We offer this dataset to the
community to help connect the efforts of computer vision researchers with the
needs of roboticists.",0.06662468,0.33295357,-0.20702809,B
3126,"The environ-
robotics labs as well as a baseline method for further research   ments are shown in Fig.","We include initial results using        To acquire images for the dataset, we placed the real
both DOPE [4] and CosyPose [3], whose pre-trained network         objects in ten different environments, with ﬁve object ar-
weights are released as an off-the-shelf system for use in        rangements / camera poses per environment.","3, and the object arrangements for
in this area.",2022-03-11 01:19:04+00:00,6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An Accessible Dataset and Benchmark,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Stephen Tyree'), arxiv.Result.Author('Jonathan Tremblay'), arxiv.Result.Author('Thang To'), arxiv.Result.Author('Jia Cheng'), arxiv.Result.Author('Terry Mosier'), arxiv.Result.Author('Jeffrey Smith'), arxiv.Result.Author('Stan Birchfield')]","We present a new dataset for 6-DoF pose estimation of known objects, with a
focus on robotic manipulation research. We propose a set of toy grocery
objects, whose physical instantiations are readily available for purchase and
are appropriately sized for robotic grasping and manipulation. We provide 3D
scanned textured models of these objects, suitable for generating synthetic
training data, as well as RGBD images of the objects in challenging, cluttered
scenes exhibiting partial occlusion, extreme lighting variations, multiple
instances per image, and a large variety of poses. Using semi-automated
RGBD-to-model texture correspondences, the images are annotated with ground
truth poses accurate within a few millimeters. We also propose a new pose
evaluation metric called ADD-H based on the Hungarian assignment algorithm that
is robust to symmetries in object geometry without requiring their explicit
enumeration. We share pre-trained pose estimators for all the toy grocery
objects, along with their baseline performance on both validation and test
sets. We offer this dataset to the community to help connect the efforts of
computer vision researchers with the needs of roboticists.",-0.08520973,0.30591884,-0.18298385,B
3234,"For Gaussian
takes into account spatio-temporal correlations and is suitable
for mobile robotic applications, requires further research.","In the case of heteroscedastic GPR, parameters of the

   In summary, long-term prediction of human activity, which        likelihood function can vary with the input.",likelihoods this changes the original GP model (eq.,2022-03-14 08:02:51+00:00,Non-Parametric Modeling of Spatio-Temporal Human Activity Based on Mobile Robot Observations,cs.RO,['cs.RO'],"[arxiv.Result.Author('Marvin Stuede'), arxiv.Result.Author('Moritz Schappler')]","This work presents a non-parametric spatio-temporal model for mapping human
activity by mobile autonomous robots in a long-term context. Based on
Variational Gaussian Process Regression, the model incorporates prior
information of spatial and temporal-periodic dependencies to create a
continuous representation of human occurrences. The inhomogeneous data
distribution resulting from movements of the robot is included in the model via
a heteroscedastic likelihood function and can be accounted for as predictive
uncertainty. Using a sparse formulation, data sets over multiple weeks and
several hundred square meters can be used for model creation. The experimental
evaluation, based on multi-week data sets, demonstrates that the proposed
approach outperforms the state of the art both in terms of predictive quality
and subsequent path planning.",-0.04600828,-0.002036024,-0.026322607,A
3235,"In summary, the long-term prediction of human activity,          C. Variational Inference for Multiple Latent Functions
which is suitable for mobile robotic applications, requires
further research.",likelihood.,"The contributions of this paper are therefore:       In the case of heteroscedastic GPR, parameters of the
1) A model for long-term predictions of human presence that
compensates for inhomogeneous data distribution resulting           likelihood function can vary with the input.",2022-03-14 08:02:51+00:00,Non-Parametric Modeling of Spatio-Temporal Human Activity Based on Mobile Robot Observations,cs.RO,['cs.RO'],"[arxiv.Result.Author('Marvin Stuede'), arxiv.Result.Author('Moritz Schappler')]","This work presents a non-parametric spatio-temporal model for mapping human
activity by mobile autonomous robots in a long-term context. Based on
Variational Gaussian Process Regression, the model incorporates prior
information of spatial and temporal-periodic dependencies to create a
continuous representation of human occurrences. The inhomogeneous data
distribution resulting from movements of the robot is included in the model via
a heteroscedastic likelihood function and can be accounted for as predictive
uncertainty. Using a sparse formulation, data sets over multiple weeks and
several hundred square meters can be used for model creation. The experimental
evaluation, based on multi-week data sets, demonstrates that the proposed
approach outperforms the state of the art both in terms of predictive quality
and subsequent path planning.",-0.07311869,0.030435856,-0.13913631,A
3258,"Also the hyperpa-
                                                             rameters of the RL and DMPs could be further researched to
                                                             make the training faster and be er.","Besides, the re-
                                                             ward function of RL could be improved to reach a be er per-
                                                             formance according to di erent scenarios.","References

(b) Fext − Fd in y                                           [1] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Bil-
                                                                  lard, “Recent advances in robot learning from demon-
                                                                  stration,” Annual Review of Control, Robotics, and Au-
                                                                  tonomous Systems, vol.",2022-03-14 15:30:20+00:00,Impedance Adaptation by Reinforcement Learning with Contact Dynamic Movement Primitives,cs.RO,['cs.RO'],"[arxiv.Result.Author('Chunyang Chang'), arxiv.Result.Author('Kevin Haninger'), arxiv.Result.Author('Yunlei Shi'), arxiv.Result.Author('Chengjie Yuan'), arxiv.Result.Author('Zhaopeng Chen'), arxiv.Result.Author('Jianwei Zhang')]","Dynamic movement primitives (DMPs) allow complex position trajectories to be
efficiently demonstrated to a robot. In contact-rich tasks, where position
trajectories alone may not be safe or robust over variation in contact
geometry, DMPs have been extended to include force trajectories. However,
different task phases or degrees of freedom may require the tracking of either
position or force -- e.g., once contact is made, it may be more important to
track the force demonstration trajectory in the contact direction. The robot
impedance balances between following a position or force reference trajectory,
where a high stiffness tracks position and a low stiffness tracks force. This
paper proposes using DMPs to learn position and force trajectories from
demonstrations, then adapting the impedance parameters online with a
higher-level control policy trained by reinforcement learning. This allows
one-shot demonstration of the task with DMPs, and improved robustness and
performance from the impedance adaptation. The approach is validated on
peg-in-hole and adhesive strip application tasks.",-0.16376299,-0.14186868,-0.04384151,A
3259,"Also the hyperpa-
                                                             rameters of the RL and DMPs could be further researched to
                                                             make the training faster and be er.","Besides, the re-
                                                             ward function of RL could be improved to reach a be er per-
                                                             formance according to di erent scenarios.","References

(b) Fext − Fd in y                                           [1] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Bil-
                                                                  lard, “Recent advances in robot learning from demon-
                                                                  stration,” Annual Review of Control, Robotics, and Au-
                                                                  tonomous Systems, vol.",2022-03-14 15:30:20+00:00,Impedance Adaptation by Reinforcement Learning with Contact Dynamic Movement Primitives,cs.RO,['cs.RO'],"[arxiv.Result.Author('Chunyang Chang'), arxiv.Result.Author('Kevin Haninger'), arxiv.Result.Author('Yunlei Shi'), arxiv.Result.Author('Chengjie Yuan'), arxiv.Result.Author('Zhaopeng Chen'), arxiv.Result.Author('Jianwei Zhang')]","Dynamic movement primitives (DMPs) allow complex position trajectories to be
efficiently demonstrated to a robot. In contact-rich tasks, where position
trajectories alone may not be safe or robust over variation in contact
geometry, DMPs have been extended to include force trajectories. However,
different task phases or degrees of freedom may require the tracking of either
position or force -- e.g., once contact is made, it may be more important to
track the force demonstration trajectory in the contact direction. The robot
impedance balances between following a position or force reference trajectory,
where a high stiffness tracks position and a low stiffness tracks force. This
paper proposes using DMPs to learn position and force trajectories from
demonstrations, then adapting the impedance parameters online with a
higher-level control policy trained by reinforcement learning. This allows
one-shot demonstration of the task with DMPs, and improved robustness and
performance from the impedance adaptation. The approach is validated on
peg-in-hole and adhesive strip application tasks.",-0.16376299,-0.14186868,-0.04384151,A
3274,"Instead of accurately simulating
                                        we hope will provide a resource to compare and quantify                the deformations and forces on the sensor, as in [3], [2], we
                                        various novel approaches, and motivate further research.","We also release an         directly from the raw ﬂuid-based tactile data such as that
                                        accompanying task-agnostic dataset for the BioTac SP, which            of the BioTac SP sensor.","compute robust features from the spatio-temporal changes
                                                                                                               in the tactile data, which carry essential information about
                                                                 I.",2022-03-14 17:00:11+00:00,GradTac: Spatio-Temporal Gradient Based Tactile Sensing,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kanishka Ganguly'), arxiv.Result.Author('Pavan Mantripragada'), arxiv.Result.Author('Chethan M. Parameshwara'), arxiv.Result.Author('Cornelia Fermüller'), arxiv.Result.Author('Nitin J. Sanket'), arxiv.Result.Author('Yiannis Aloimonos')]","Tactile sensing for robotics is achieved through a variety of mechanisms,
including magnetic, optical-tactile, and conductive fluid. Currently, the
fluid-based sensors have struck the right balance of anthropomorphic sizes and
shapes and accuracy of tactile response measurement. However, this design is
plagued by a low Signal to Noise Ratio (SNR) due to the fluid based sensing
mechanism ""damping"" the measurement values that are hard to model. To this end,
we present a spatio-temporal gradient representation on the data obtained from
fluid-based tactile sensors, which is inspired from neuromorphic principles of
event based sensing. We present a novel algorithm (GradTac) that converts
discrete data points from spatial tactile sensors into spatio-temporal surfaces
and tracks tactile contours across these surfaces. Processing the tactile data
using the proposed spatio-temporal domain is robust, makes it less susceptible
to the inherent noise from the fluid based sensors, and allows accurate
tracking of regions of touch as compared to using the raw data. We successfully
evaluate and demonstrate the efficacy of GradTac on many real-world experiments
performed using the Shadow Dexterous Hand, equipped with the BioTac SP sensors.
Specifically, we use it for tracking tactile input across the sensor's surface,
measuring relative forces, detecting linear and rotational slip, and for edge
tracking. We also release an accompanying task-agnostic dataset for the BioTac
SP, which we hope will provide a resource to compare and quantify various novel
approaches, and motivate further research.",-0.19482946,-0.0131228,-0.04149912,A
3333,"In future work, we will further study the
dynamic control of the soft arm in the task space to make full
use of the dynamic interaction between the soft robot and the
environment.","This paper shows that by considering the
soft robot’s stiffness and damping, the soft robot’s fastest
dynamic control is achieved (the velocity of the end effector
can reach 7m/s).",VI.,2022-03-15 14:13:28+00:00,Dynamical Modeling and Control of Soft Robots with Non-constant Curvature Deformation,cs.RO,['cs.RO'],"[arxiv.Result.Author('Zhanchi Wang'), arxiv.Result.Author('Gaotian Wang'), arxiv.Result.Author('Xiaoping Chen'), arxiv.Result.Author('Nikolaos M. Freris')]","The Piecewise Constant Curvature (PCC) model is the most widely used soft
robotic modeling and control. However, the PCC fails to accurately describe the
deformation of the soft robots when executing dynamic tasks or interacting with
the environment. This paper presents a simple threedimensional (3D) modeling
method for a multi-segment soft robotic manipulator with non-constant curvature
deformation. We devise kinematic and dynamical models for soft manipulators by
modeling each segment of the manipulator as two stretchable links connected by
a universal joint. Based on that, we present two controllers for dynamic
trajectory tracking in confguration space and pose control in task space,
respectively. Model accuracy is demonstrated with simulations and experimental
data. The controllers are implemented on a four-segment soft robotic
manipulator and validated in dynamic motions and pose control with unknown
loads. The experimental results show that the dynamic controller enables a
stable reference trajectory tracking at speeds up to 7m/s.",-0.3504016,-0.20402709,0.3153119,A
3553,"The primary          Maddern W, Pascoe G, Linegar C and Newman P (2017) 1 Year,
purpose of this dataset is to enable further research into              1000 km: The Oxford RobotCar dataset.",of the Boreas data obtained in sunny weather.,"The International
long-term localization across seasons and adverse weather               Journal of Robotics Research 36(1): 3–15.",2022-03-18 21:40:50+00:00,Boreas: A Multi-Season Autonomous Driving Dataset,cs.RO,['cs.RO'],"[arxiv.Result.Author('Keenan Burnett'), arxiv.Result.Author('David J. Yoon'), arxiv.Result.Author('Yuchen Wu'), arxiv.Result.Author('Andrew Zou Li'), arxiv.Result.Author('Haowei Zhang'), arxiv.Result.Author('Shichen Lu'), arxiv.Result.Author('Jingxing Qian'), arxiv.Result.Author('Wei-Kang Tseng'), arxiv.Result.Author('Andrew Lambert'), arxiv.Result.Author('Keith Y. K. Leung'), arxiv.Result.Author('Angela P. Schoellig'), arxiv.Result.Author('Timothy D. Barfoot')]","The Boreas dataset was collected by driving a repeated route over the course
of one year, resulting in stark seasonal variations and adverse weather
conditions such as rain and falling snow. In total, the Boreas dataset contains
over 350km of driving data featuring a 128-channel Velodyne Alpha-Prime lidar,
a 360 degree Navtech CIR304-H scanning radar, a 5MP FLIR Blackfly S camera, and
centimetre-accurate post-processed ground truth poses. At launch, our dataset
will support live leaderboards for odometry, metric localization, and 3D object
detection. The dataset and development kit are available at:
https://www.boreas.utias.utoronto.ca",-0.062881716,0.32751194,0.009409188,B
3648,"on Intelligent Robots and
1.1M measurements for further study, along with software                     Systems, 2012.
that executes our experiments on arbitrary objects and material
parameters.",Conf.,"Finally, our simulated results are also shown to            [5] E. Coumans and Y. Bai, “PyBullet: A Python module for physics
have good correspondence with real-world grasp outcomes.",2022-03-21 18:53:03+00:00,DefGraspSim: Physics-based simulation of grasp outcomes for 3D deformable objects,cs.RO,['cs.RO'],"[arxiv.Result.Author('Isabella Huang'), arxiv.Result.Author('Yashraj Narang'), arxiv.Result.Author('Clemens Eppner'), arxiv.Result.Author('Balakumar Sundaralingam'), arxiv.Result.Author('Miles Macklin'), arxiv.Result.Author('Ruzena Bajcsy'), arxiv.Result.Author('Tucker Hermans'), arxiv.Result.Author('Dieter Fox')]","Robotic grasping of 3D deformable objects (e.g., fruits/vegetables, internal
organs, bottles/boxes) is critical for real-world applications such as food
processing, robotic surgery, and household automation. However, developing
grasp strategies for such objects is uniquely challenging. Unlike rigid
objects, deformable objects have infinite degrees of freedom and require field
quantities (e.g., deformation, stress) to fully define their state. As these
quantities are not easily accessible in the real world, we propose studying
interaction with deformable objects through physics-based simulation. As such,
we simulate grasps on a wide range of 3D deformable objects using a GPU-based
implementation of the corotational finite element method (FEM). To facilitate
future research, we open-source our simulated dataset (34 objects, 1e5 Pa
elasticity range, 6800 grasp evaluations, 1.1M grasp measurements), as well as
a code repository that allows researchers to run our full FEM-based grasp
evaluation pipeline on arbitrary 3D object models of their choice. Finally, we
demonstrate good correspondence between grasp outcomes on simulated objects and
their real counterparts.",-0.3675383,0.02950862,-0.0031510703,A
3679,"However, results for the other two prin-
        Liu et al (2017b); Liu and Sun (2018) imple-        ciples were not conclusive, and further research is
    mented a midst-mapping fusion approach using            needed.","indicate that hierarchical processing was indeed
                                                            beneﬁcial.",Toprak et al.,2022-03-22 08:55:36+00:00,Visuo-Haptic Object Perception for Robots: An Overview,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Nicolás Navarro-Guerrero'), arxiv.Result.Author('Sibel Toprak'), arxiv.Result.Author('Josip Josifovski'), arxiv.Result.Author('Lorenzo Jamone')]","This article summarizes the current state of multimodal object perception for
robotic applications. It covers aspects of biological inspiration, sensor
technologies, data sets, and sensory data processing for object recognition and
grasping. Firstly, the biological basis of multimodal object perception is
outlined. Then the sensing technologies and data collection strategies are
discussed. Next, an introduction to the main computational aspects is
presented, highlighting a few representative articles for each main application
area, including object recognition, object manipulation and grasping, texture
recognition, and transfer learning. Finally, informed by the current
advancements in each area, this article outlines promising new research
directions.",0.27021325,0.002997999,-0.22191173,C
3680,"However,       imizes their product or (3) the object label that
    results for the other two principles were not conclu-  maximizes the sum of these posterior probabili-
    sive, and further research is needed.","The results indicate that hierar-      based on (2) either on the object label that max-
    chical processing was indeed beneﬁcial.",Toprak et al.,2022-03-22 08:55:36+00:00,Visuo-Haptic Object Perception for Robots: An Overview,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Nicolás Navarro-Guerrero'), arxiv.Result.Author('Sibel Toprak'), arxiv.Result.Author('Josip Josifovski'), arxiv.Result.Author('Lorenzo Jamone')]","Object perception capabilities of humans are impressive, something that
becomes even more evident when trying to develop solutions with similar
proficiency for robots. Taking inspiration from how humans use vision and
haptics for object perception and related tasks, this article summarises the
current state of multimodal object perception for robotic applications. It
covers aspects of biological inspiration, sensor technologies, data sets, and
sensory data processing for object recognition and grasping. Firstly, the
biological basis of multimodal object perception is outlined. Then the sensing
technologies and data collection strategies are discussed. Next, an
introduction to the main computational aspects is presented, highlighting a few
representative articles for each main application area, including object
recognition, transfer learning, and object manipulation and grasping. Finally,
informed by the current advancements in each area, this article outlines
promising new research directions.",0.046421714,-0.19872299,-0.21229051,C
3726,"We hope our work leads to further research in this area
thereby enabling ﬁeld robots to navigate without predeﬁned                      [19] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, “Se-
waypoints in unstructured outdoor environments.",1722–1729.,"manticfusion: Dense 3d semantic mapping with convolutional neural
                                                                                      networks,” in 2017 IEEE International Conference on Robotics and
                            REFERENCES                                                automation (ICRA).",2022-03-22 22:02:03+00:00,WayFAST: Traversability Predictive Navigation for Field Robots,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Mateus Valverde Gasparino'), arxiv.Result.Author('Arun Narenthiran Sivakumar'), arxiv.Result.Author('Yixiao Liu'), arxiv.Result.Author('Andres Eduardo Baquero Velasquez'), arxiv.Result.Author('Vitor Akihiro Hisano Higuti'), arxiv.Result.Author('John Rogers'), arxiv.Result.Author('Huy Tran'), arxiv.Result.Author('Girish Chowdhary')]","We present a self-supervised approach for learning to predict traversable
paths for wheeled mobile robots that require good traction to navigate. Our
algorithm, termed WayFAST (Waypoint Free Autonomous Systems for
Traversability), uses RGB and depth data, along with navigation experience, to
autonomously generate traversable paths in outdoor unstructured environments.
Our key inspiration is that traction can be estimated for rolling robots using
kinodynamic models. Using traction estimates provided by an online receding
horizon estimator, we are able to train a traversability prediction neural
network in a self-supervised manner, without requiring heuristics utilized by
previous methods. We demonstrate the effectiveness of WayFAST through extensive
field testing in varying environments, ranging from sandy dry beaches to forest
canopies and snow covered grass fields. Our results clearly demonstrate that
WayFAST can learn to avoid geometric obstacles as well as untraversable
terrain, such as snow, which would be difficult to avoid with sensors that
provide only geometric data, such as LiDAR. Furthermore, we show that our
training pipeline based on online traction estimates is more data-efficient
than other heuristic-based methods.",-0.083493754,0.4853112,-0.19414485,B_centroid
3727,"[21] S. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin, “Combining
We hope our work leads to further research in this area                               optimal control and learning for visual navigation in novel environ-
                                                                                      ments,” in Conference on Robot Learning.","of semantic understanding, such as in the avoiding snow.","PMLR, 2020, pp.",2022-03-22 22:02:03+00:00,WayFAST: Navigation with Predictive Traversability in the Field,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.SY', 'eess.SY', 'I.2.9; I.2.6; I.2.10']","[arxiv.Result.Author('Mateus Valverde Gasparino'), arxiv.Result.Author('Arun Narenthiran Sivakumar'), arxiv.Result.Author('Yixiao Liu'), arxiv.Result.Author('Andres Eduardo Baquero Velasquez'), arxiv.Result.Author('Vitor Akihiro Hisano Higuti'), arxiv.Result.Author('John Rogers'), arxiv.Result.Author('Huy Tran'), arxiv.Result.Author('Girish Chowdhary')]","We present a self-supervised approach for learning to predict traversable
paths for wheeled mobile robots that require good traction to navigate. Our
algorithm, termed WayFAST (Waypoint Free Autonomous Systems for
Traversability), uses RGB and depth data, along with navigation experience, to
autonomously generate traversable paths in outdoor unstructured environments.
Our key inspiration is that traction can be estimated for rolling robots using
kinodynamic models. Using traction estimates provided by an online receding
horizon estimator, we are able to train a traversability prediction neural
network in a self-supervised manner, without requiring heuristics utilized by
previous methods. We demonstrate the effectiveness of WayFAST through extensive
field testing in varying environments, ranging from sandy dry beaches to forest
canopies and snow covered grass fields. Our results clearly demonstrate that
WayFAST can learn to avoid geometric obstacles as well as untraversable
terrain, such as snow, which would be difficult to avoid with sensors that
provide only geometric data, such as LiDAR. Furthermore, we show that our
training pipeline based on online traction estimates is more data-efficient
than other heuristic-based methods.",-0.19828142,0.18825772,-0.27986106,A
3853,"While re-arrangement is general, we      agent interaction present further research opportunities for
caution that many instances of rearrangement can be solved     developing agents of increasing complexity.","(2021) propose scene re-arrangement as a universal      this work, more open-ended assembly goals as well as multi-
task for embodied AI.","as a sequence of largely independent sub-tasks that do not
inﬂuence each other, and can be performed in any order.",2022-03-15 18:21:02+00:00,Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Seyed Kamyar Seyed Ghasemipour'), arxiv.Result.Author('Daniel Freeman'), arxiv.Result.Author('Byron David'), arxiv.Result.Author('Shixiang'), arxiv.Result.Author('Gu'), arxiv.Result.Author('Satoshi Kataoka'), arxiv.Result.Author('Igor Mordatch')]","Assembly of multi-part physical structures is both a valuable end product for
autonomous robotics, as well as a valuable diagnostic task for open-ended
training of embodied intelligent agents. We introduce a naturalistic
physics-based environment with a set of connectable magnet blocks inspired by
children's toy kits. The objective is to assemble blocks into a succession of
target blueprints. Despite the simplicity of this objective, the compositional
nature of building diverse blueprints from a set of blocks leads to an
explosion of complexity in structures that agents encounter. Furthermore,
assembly stresses agents' multi-step planning, physical reasoning, and bimanual
coordination. We find that the combination of large-scale reinforcement
learning and graph-based policies -- surprisingly without any additional
complexity -- is an effective recipe for training agents that not only
generalize to complex unseen blueprints in a zero-shot manner, but even operate
in a reset-free setting without being trained to do so. Through extensive
experiments, we highlight the importance of large-scale training, structured
representations, contributions of multi-task vs. single-task learning, as well
as the effects of curriculums, and discuss qualitative behaviors of trained
agents.",-0.18188201,-0.025826614,-0.15590811,A
3854,"Structured
agent interaction present further research opportunities for        agents for physical construction.","And lastly, while we focused on blueprint assembly in     Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld,
this work, more open-ended assembly goals as well as multi-         K., Kohli, P., Battaglia, P., and Hamrick, J.","In International Confer-
developing agents of increasing complexity.",2022-03-15 18:21:02+00:00,Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Seyed Kamyar Seyed Ghasemipour'), arxiv.Result.Author('Daniel Freeman'), arxiv.Result.Author('Byron David'), arxiv.Result.Author('Shixiang Shane Gu'), arxiv.Result.Author('Satoshi Kataoka'), arxiv.Result.Author('Igor Mordatch')]","Assembly of multi-part physical structures is both a valuable end product for
autonomous robotics, as well as a valuable diagnostic task for open-ended
training of embodied intelligent agents. We introduce a naturalistic
physics-based environment with a set of connectable magnet blocks inspired by
children's toy kits. The objective is to assemble blocks into a succession of
target blueprints. Despite the simplicity of this objective, the compositional
nature of building diverse blueprints from a set of blocks leads to an
explosion of complexity in structures that agents encounter. Furthermore,
assembly stresses agents' multi-step planning, physical reasoning, and bimanual
coordination. We find that the combination of large-scale reinforcement
learning and graph-based policies -- surprisingly without any additional
complexity -- is an effective recipe for training agents that not only
generalize to complex unseen blueprints in a zero-shot manner, but even operate
in a reset-free setting without being trained to do so. Through extensive
experiments, we highlight the importance of large-scale training, structured
representations, contributions of multi-task vs. single-task learning, as well
as the effects of curriculums, and discuss qualitative behaviors of trained
agents.",-0.10181867,-0.0773531,0.03750065,A
3861,"By making available such a swarm,
                                                                    together with an accessible development pipeline, we hope to
                                                                                  15

reduce the barriers to entry and stimulate further research.","essary to handle the ﬁve concurrent video streams, time-of-
ﬂight pointcloud, and other senses, and entirely autonomously          Building swarm robotic applications for the real world is
search for, ﬁnd, manoeuvre under with precision, lift, carry        hard, and one of the biggest obstacles is the lack of actually
                                                                    existing robot swarms with the types of capabilities necessary
                                                                    to experiment with ideas.","[16] S. Su, F. Heide, R. Swanson, J. Klein, C. Callenberg, M. Hullin, and
                                                                                        W. Heidrich, “Material classiﬁcation using raw time-of-ﬂight measure-
                        ACKNOWLEDGMENT                                                  ments,” in Proceedings of the IEEE Conference on Computer Vision and
                                                                                        Pattern Recognition, 2016, pp.",2022-03-25 17:52:49+00:00,DOTS: An Open Testbed for Industrial Swarm Robotic Solutions,cs.RO,['cs.RO'],"[arxiv.Result.Author('Simon Jones'), arxiv.Result.Author('Emma Milner'), arxiv.Result.Author('Mahesh Sooriyabandara'), arxiv.Result.Author('Sabine Hauert')]","We present DOTS, a new open access testbed for industrial swarm robotics
experimentation. It consists of 20 fast agile robots with high sensing and
computational performance, and real-world payload capability. They are housed
in an arena equipped with private 5G, motion capture, multiple cameras, and
openly accessible via an online portal. We reduce barriers to entry by
providing a complete platform-agnostic pipeline to develop, simulate, and
deploy experimental applications to the swarm. We showcase the testbed
capabilities with a swarm logistics application, autonomously and reliably
searching for and retrieving multiple cargo carriers.",-0.14919029,0.22798376,0.01453229,B
3878,"How to further improve the generalization of our
method is one of our further research directions.","1             goal1          goal2       goal3          goal4
Based on the experimental results, we ﬁnd that when marked
changes appear in environment, our method may fail to fulﬁll     roll(rad)  0.5
the task.","0

                                                                            -0.5

                                                                                  0  0.5  1         1.5  2      2.5       3    3.5         4  4.5

                                                                                                         Px(m)

                                                                                               (b) Rotation trajectories.",2022-03-26 09:29:23+00:00,Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Qiyu Sun'), arxiv.Result.Author('Jinbao Fang'), arxiv.Result.Author('Wei Xing Zheng'), arxiv.Result.Author('Yang Tang')]","The ability to perform aggressive movements, which are called aggressive
flights, is important for quadrotors during navigation. However, aggressive
quadrotor flights are still a great challenge to practical applications. The
existing solutions to aggressive flights heavily rely on a predefined
trajectory, which is a time-consuming preprocessing step. To avoid such path
planning, we propose a curiosity-driven reinforcement learning method for
aggressive flight missions and a similarity-based curiosity module is
introduced to speed up the training procedure. A branch structure exploration
(BSE) strategy is also applied to guarantee the robustness of the policy and to
ensure the policy trained in simulations can be performed in real-world
experiments directly. The experimental results in simulations demonstrate that
our reinforcement learning algorithm performs well in aggressive flight tasks,
speeds up the convergence process and improves the robustness of the policy.
Besides, our algorithm shows a satisfactory simulated to real transferability
and performs well in real-world experiments.",0.035422467,-0.15770896,0.0011151265,C
4033,"In this paper, we analyze the
                                        performance of 3D SOM to further study the tradeoff between                   To reduce the resource scheduling complexity of spectrum
                                        accuracy and efﬁciency in 3D SOM.",ing a great momentum of 6G.,"We discover that the error               sharing, the Radio Environment Map (REM) based spectrum
                                        of 3D SOM is related with the area of the boundary surfaces                sharing was proposed and widely studied [4].",2022-03-29 14:29:11+00:00,Three-Dimensional Spectrum Occupancy Measurement using UAV: Performance Analysis and Algorithm Design,cs.RO,['cs.RO'],"[arxiv.Result.Author('Zhiqing Wei'), arxiv.Result.Author('Rubing Yao'), arxiv.Result.Author('Jie Kang'), arxiv.Result.Author('Xu Chen'), arxiv.Result.Author('Huici Wu')]","Spectrum sharing, as an approach to significantly improve spectrum efficiency
in the era of 6th generation mobile networks (6G), has attracted extensive
attention. Radio Environment Map (REM) based low-complexity spectrum sharing is
widely studied where the spectrum occupancy measurement (SOM) is vital to
construct REM. The SOM in three-dimensional (3D) space is becoming increasingly
essential to support the spectrum sharing with space-air-ground integrated
network being a great momentum of 6G. In this paper, we analyze the performance
of 3D SOM to further study the tradeoff between accuracy and efficiency in 3D
SOM. We discover that the error of 3D SOM is related with the area of the
boundary surfaces of licensed networks, the number of discretized cubes, and
the length of the edge of 3D space. Moreover, we design a fast and accurate 3D
SOM algorithm that utilizes unmanned aerial vehicle (UAV) to measure the
spectrum occupancy considering the path planning of UAV, which improves the
measurement efficiency by requiring less measurement time and flight time of
the UAV for satisfactory performance. The theoretical results obtained in this
paper reveal the essential dependencies that describe the 3D SOM methodology,
and the proposed algorithm is beneficial to improve the efficiency of 3D SOM.
It is noted that the theoretical results and algorithm in this paper may
provide a guideline for more areas such as spectrum monitoring, spectrum
measurement, network measurement, planning, etc.",0.3172629,0.16224378,0.15634555,B
4053,"We share our code,
                                                                    models and data to promote further research in the space of
   Figure 4 provides qualitative visualizations of one              inertial localization.",a backbone instead of Transformer).,trajectory from building A.,2022-03-29 18:45:27+00:00,Neural Inertial Localization,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Sachini Herath'), arxiv.Result.Author('David Caruso'), arxiv.Result.Author('Chen Liu'), arxiv.Result.Author('Yufan Chen'), arxiv.Result.Author('Yasutaka Furukawa')]","This paper proposes the inertial localization problem, the task of estimating
the absolute location from a sequence of inertial sensor measurements. This is
an exciting and unexplored area of indoor localization research, where we
present a rich dataset with 53 hours of inertial sensor data and the associated
ground truth locations. We developed a solution, dubbed neural inertial
localization (NILoc) which 1) uses a neural inertial navigation technique to
turn inertial sensor history to a sequence of velocity vectors; then 2) employs
a transformer-based neural architecture to find the device location from the
sequence of velocities. We only use an IMU sensor, which is energy efficient
and privacy preserving compared to WiFi, cameras, and other data sources. Our
approach is significantly faster and achieves competitive results even compared
with state-of-the-art methods that require a floorplan and run 20 to 30 times
slower. We share our code, model and data at https://sachini.github.io/niloc.",-0.07340474,0.2397346,-0.049266316,B
4061,"Continuous Option Space We further study if our framework works with
continuous option space.","Therefore, by decreasing the level of conditional option selection via allowing
more independent agents, the “lower-degree” Partially-Dec performs better.","Continuous option is rarely investigated in prior litera-
ture and it is a type of option that can be sampled from a continuous distribution
(e.g.",2022-03-29 22:02:28+00:00,Multi-Agent Asynchronous Cooperation with Hierarchical Reinforcement Learning,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.MA']","[arxiv.Result.Author('Xubo Lyu'), arxiv.Result.Author('Amin Banitalebi-Dehkordi'), arxiv.Result.Author('Mo Chen'), arxiv.Result.Author('Yong Zhang')]","Hierarchical multi-agent reinforcement learning (MARL) has shown a
significant learning efficiency by searching policy over higher-level,
temporally extended actions (options). However, standard policy gradient-based
MARL methods have a difficulty generalizing to option-based scenarios due to
the asynchronous executions of multi-agent options. In this work, we propose a
mathematical framework to enable policy gradient optimization over asynchronous
multi-agent options by adjusting option-based policy distribution as well as
trajectory probability. We study our method under a set of multi-agent
cooperative setups with varying inter-dependency levels, and evaluate the
effectiveness of our method on typical option-based multi-agent cooperation
tasks.",0.07137464,-0.25175327,-0.17542888,C
4192,"nothing has gone wrong or is being performed differently, but                 Nevertheless, we strongly afﬁrm that this platform can still be
                                                                              used for further research and already has an spiking controller
                                                                              implementation, which is an essential upside.","Of course, extensive testing will ensure                   may be cheaper, more efﬁcient and require less infrastructure.","Therefore, we
                                                                              encourage fellow researchers to use the ED-Scorbot platform,
                                                                              as there is only room for improvement and breakthrough.",2022-03-31 22:11:46+00:00,An MPSoC-based on-line Edge Infrastructure for Embedded Neuromorphic Robotic Controllers,cs.RO,['cs.RO'],"[arxiv.Result.Author('Enrique Pinero-Fuentes'), arxiv.Result.Author('Salvador Canas-Moreno'), arxiv.Result.Author('Antonio Rios-Navarro'), arxiv.Result.Author('Daniel Cascado-Caballero'), arxiv.Result.Author('Angel Jimenez-Fernandez'), arxiv.Result.Author('Alejandro Linares-Barranco')]","In this work, an all-in-one neuromorphic controller system with reduced
latency and power consumption for a robotic arm is presented. Biological muscle
movement consists of stretching and shrinking fibres via spike-commanded
signals that come from motor neurons, which in turn are connected to a central
pattern generator neural structure. In addition, biological systems are able to
respond to diverse stimuli rather fast and efficiently, and this is based on
the way information is coded within neural processes. As opposed to
human-created encoding systems, neural ones use neurons and spikes to process
the information and make weighted decisions based on a continuous learning
process. The Event-Driven Scorbot platform (ED-Scorbot) consists of a 6 Degrees
of Freedom (DoF) robotic arm whose controller implements a Spiking
Proportional-Integrative- Derivative algorithm, mimicking in this way the
previously commented biological systems. In this paper, we present an
infrastructure upgrade to the ED-Scorbot platform, replacing the controller
hardware, which was comprised of two Spartan Field Programmable Gate Arrays
(FPGAs) and a barebone computer, with an edge device, the Xilinx Zynq-7000 SoC
(System on Chip) which reduces the response time, power consumption and overall
complexity.",-0.055448323,-0.082373075,0.071851,A
4363,There are several further research.,"This may be caused by the fact that Gait 2 is a locally
            most robust gait.","Firstly, applying the gait into the feedback linear-
            ization in a tilt-rotor control is desired, e.g., a tracking experiment with different gaits.",2022-04-04 21:36:01+00:00,Four-dimensional Gait Surfaces for A Tilt-rotor -- Two Color Map Theorem,cs.RO,['cs.RO'],"[arxiv.Result.Author('Zhe Shen'), arxiv.Result.Author('Yudong Ma'), arxiv.Result.Author('Takeshi Tsuchiya')]","This article presents the four-dimensional surfaces which instruct the gait
plan for a tilt-rotor. The previous gaits analyzed in the tilt-rotor research
are inspired by animals; no theoretical base backs the robustness of these
gaits. This research deduces the gaits by diminishing the effect of the
attitude of the tilt-rotor for the first time. Four-dimensional gait surfaces
are subsequently found, on which the gaits are expected to be robust to the
attitude. These surfaces provide the region where the gait is suggested to be
planned. However, a discontinuous region may hinder the gait plan process while
utilizing the proposal gait surfaces. A Two Color Map Theorem is then
established to guarantee the continuity of each gait designed. The robustness
of the typical gaits obeying the Two Color Map Theorem and on the gait surface
is demonstrated by comparing the singular curve in attitude with the gaits not
on the gait surface. The result shows that the acceptable attitudes enlarge for
the gaits on the gait surface.",-0.13100965,-0.1699883,0.24525377,A
4410,"To further study the inﬂuence of the joint learning interval T , we conduct
35 experiments on the bottle category with more values of T .","33 3 Ablation study

34 Joint learning interval.",As suggested in Tab.,2022-04-05 16:26:22+00:00,Learning Generalizable Dexterous Manipulation from Human Grasp Affordance,cs.RO,"['cs.RO', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yueh-Hua Wu'), arxiv.Result.Author('Jiashun Wang'), arxiv.Result.Author('Xiaolong Wang')]","Dexterous manipulation with a multi-finger hand is one of the most
challenging problems in robotics. While recent progress in imitation learning
has largely improved the sample efficiency compared to Reinforcement Learning,
the learned policy can hardly generalize to manipulate novel objects, given
limited expert demonstrations. In this paper, we propose to learn dexterous
manipulation using large-scale demonstrations with diverse 3D objects in a
category, which are generated from a human grasp affordance model. This
generalizes the policy to novel object instances within the same category. To
train the policy, we propose a novel imitation learning objective jointly with
a geometric representation learning objective using our demonstrations. By
experimenting with relocating diverse objects in simulation, we show that our
approach outperforms baselines with a large margin when manipulating novel
objects. We also ablate the importance on 3D object representation learning for
manipulation. We include videos, code, and additional information on the
project website - https://kristery.github.io/ILAD/ .",0.045630366,-0.45786119,-0.33971268,C
4411,"To further study the inﬂuence of the joint learning interval T , we conduct
35 experiments on the bottle category with more values of T .","33 3 Ablation study

34 Joint learning interval.",As suggested in Tab.,2022-04-05 16:26:22+00:00,Learning Generalizable Dexterous Manipulation from Human Grasp Affordance,cs.RO,"['cs.RO', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yueh-Hua Wu'), arxiv.Result.Author('Jiashun Wang'), arxiv.Result.Author('Xiaolong Wang')]","Dexterous manipulation with a multi-finger hand is one of the most
challenging problems in robotics. While recent progress in imitation learning
has largely improved the sample efficiency compared to Reinforcement Learning,
the learned policy can hardly generalize to manipulate novel objects, given
limited expert demonstrations. In this paper, we propose to learn dexterous
manipulation using large-scale demonstrations with diverse 3D objects in a
category, which are generated from a human grasp affordance model. This
generalizes the policy to novel object instances within the same category. To
train the policy, we propose a novel imitation learning objective jointly with
a geometric representation learning objective using our demonstrations. By
experimenting with relocating diverse objects in simulation, we show that our
approach outperforms baselines with a large margin when manipulating novel
objects. We also ablate the importance on 3D object representation learning for
manipulation. We include videos, code, and additional information on the
project website - https://kristery.github.io/ILAD/ .",0.045630366,-0.45786119,-0.33971268,C
4412,"To further study the inﬂuence of the joint learning interval T , we conduct
35 experiments on the bottle category with more values of T .","33 3 Ablation study

34 Joint learning interval.",As suggested in Tab.,2022-04-05 16:26:22+00:00,Learning Generalizable Dexterous Manipulation from Human Grasp Affordance,cs.RO,"['cs.RO', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yueh-Hua Wu'), arxiv.Result.Author('Jiashun Wang'), arxiv.Result.Author('Xiaolong Wang')]","Dexterous manipulation with a multi-finger hand is one of the most
challenging problems in robotics. While recent progress in imitation learning
has largely improved the sample efficiency compared to Reinforcement Learning,
the learned policy can hardly generalize to manipulate novel objects, given
limited expert demonstrations. In this paper, we propose to learn dexterous
manipulation using large-scale demonstrations with diverse 3D objects in a
category, which are generated from a human grasp affordance model. This
generalizes the policy to novel object instances within the same category. To
train the policy, we propose a novel imitation learning objective jointly with
a geometric representation learning objective using our demonstrations. By
experimenting with relocating diverse objects in simulation, we show that our
approach outperforms baselines with a large margin when manipulating novel
objects. We also ablate the importance on 3D object representation learning for
manipulation. We include videos, code, and additional information on the
project website - https://kristery.github.io/ILAD/ .",0.045630366,-0.45786119,-0.33971268,C
4614,This work opens various avenues for further research.,"Speciﬁcally, our results show that we can capture
                                                             the dynamics of the robots, adapt our learned model to
                                                             new operating conditions, and perform dynamic maneuvers
                                                             using trajectory optimization.","For
                                                             instance, the exploitation of model inaccuracies by policy
                                                             optimizers has been investigated in the literature [25], [50],
                                                             [51].",2022-04-09 22:07:34+00:00,Gradient-Based Trajectory Optimization With Learned Dynamics,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Bhavya Sukhija'), arxiv.Result.Author('Nathanael Köhler'), arxiv.Result.Author('Miguel Zamora'), arxiv.Result.Author('Simon Zimmermann'), arxiv.Result.Author('Sebastian Curi'), arxiv.Result.Author('Andreas Krause'), arxiv.Result.Author('Stelian Coros')]","Trajectory optimization methods have achieved an exceptional level of
performance on real-world robots in recent years. These methods heavily rely on
accurate analytical models of the dynamics, yet some aspects of the physical
world can only be captured to a limited extent. An alternative approach is to
leverage machine learning techniques to learn a differentiable dynamics model
of the system from data. In this work, we use trajectory optimization and model
learning for performing highly dynamic and complex tasks with robotic systems
in absence of accurate analytical models of the dynamics. We show that a neural
network can model highly nonlinear behaviors accurately for large time
horizons, from data collected in only 25 minutes of interactions on two
distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore,
we use the gradients of the neural network to perform gradient-based trajectory
optimization. In our hardware experiments, we demonstrate that our learned
model can represent complex dynamics for both the Spot and Radio-controlled
(RC) car, and gives good performance in combination with trajectory
optimization methods.",-0.30769315,-0.055493742,0.06988724,A
4788,"We hope it will be useful as a starting
point for further research and will bring us closer towards                    [17] M. Shridhar and D. Hsu, “Interactive Visual Grounding of Referring
general-purpose robots that can relate human language to their                       Expressions for Human-Robot Interaction,” in RSS, 2018.
perception and actions.",CALVIN benchmark.,"[18] J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno,
                        ACKNOWLEDGMENTS                                              W. Ko, and J. Tan, “Interactively picking real-world objects with uncon-
                                                                                     strained spoken language instructions,” in ICRA, 2018.",2022-04-13 08:45:32+00:00,What Matters in Language Conditioned Robotic Imitation Learning over Unstructured Data,cs.RO,"['cs.RO', 'cs.AI', 'cs.CL', 'cs.CV']","[arxiv.Result.Author('Oier Mees'), arxiv.Result.Author('Lukas Hermann'), arxiv.Result.Author('Wolfram Burgard')]","A long-standing goal in robotics is to build robots that can perform a wide
range of daily tasks from perceptions obtained with their onboard sensors and
specified only via natural language. While recently substantial advances have
been achieved in language-driven robotics by leveraging end-to-end learning
from pixels, there is no clear and well-understood process for making various
design choices due to the underlying variation in setups. In this paper, we
conduct an extensive study of the most critical challenges in learning language
conditioned policies from offline free-form imitation datasets. We further
identify architectural and algorithmic techniques that improve performance,
such as a hierarchical decomposition of the robot control learning, a
multimodal transformer encoder, discrete latent plans and a self-supervised
contrastive loss that aligns video and language representations. By combining
the results of our investigation with our improved model components, we are
able to present a novel approach that significantly outperforms the state of
the art on the challenging language conditioned long-horizon robot manipulation
CALVIN benchmark. We have open-sourced our implementation to facilitate future
research in learning to perform many complex manipulation skills in a row
specified with natural language. Codebase and trained models available at
http://hulc.cs.uni-freiburg.de",-0.2122258,0.123732954,-0.25306547,A
4843,"Our proposed             the performance of the ALP approach in such environments
algorithm, Glocalized Curriculum-Aided Learning or Glo-           warrants further study.","Email: ANIL004@e.ntu.edu.sg
grasping tasks with various difﬁculty levels and compare our      However, for discrete task environments considered in this
results with other approaches including a learning progress-      paper, a continuous ALP signal may not be available, so
based algorithm and a random curriculum.","CAL for short, generates a curriculum based on formation
of task clusters according to the capabilities of an agent           The value disagreement approach is a recent goal sampling
at a certain time instant.",2022-04-14 09:06:23+00:00,GloCAL: Glocalized Curriculum-Aided Learning of Multiple Tasks with Application to Robotic Grasping,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Anil Kurkcu'), arxiv.Result.Author('Cihan Acar'), arxiv.Result.Author('Domenico Campolo'), arxiv.Result.Author('Keng Peng Tee')]","The domain of robotics is challenging to apply deep reinforcement learning
due to the need for large amounts of data and for ensuring safety during
learning. Curriculum learning has shown good performance in terms of sample-
efficient deep learning. In this paper, we propose an algorithm (named GloCAL)
that creates a curriculum for an agent to learn multiple discrete tasks, based
on clustering tasks according to their evaluation scores. From the
highest-performing cluster, a global task representative of the cluster is
identified for learning a global policy that transfers to subsequently formed
new clusters, while the remaining tasks in the cluster are learned as local
policies. The efficacy and efficiency of our GloCAL algorithm are compared with
other approaches in the domain of grasp learning for 49 objects with varied
object complexity and grasp difficulty from the EGAD! dataset. The results show
that GloCAL is able to learn to grasp 100% of the objects, whereas other
approaches achieve at most 86% despite being given 1.5 times longer training
time.",-0.18907118,0.04389394,-0.19351017,A
4851,"Training datasets              Federated learning
                                               Centralized learning with aggregated data

                                          S0t  S1t  S2t    S0t,1 S0t,2 S1t,2 S1t,2,3                        S0t,1  S0t,2  S1t,2        S1t,2,3

               Validation datasets  Sim   S0v 0.28 0.56 0.56 0.33 0.50 0.71 0.52                            0.85   0.85   0.85          0.85
                                          S1v 0.33 0.50 0.75 0.33 0.50 0.75 0.60                            0.94   0.93   0.93          0.95
                                          S2v 0.43 0.94 0.46 0.42 0.50 0.23 0.62                            0.96   0.95   0.95          0.95

                                          Rt0 Rt1 Rt2 Rt0,1 Rt0,2 Rt1,2 Rt1,2,3                             Rt0,1  Rt0,2  Rt1,2        Rt1,2,3

                                    Real  Rv0 0.63 0.58 0.42 0.75 0.63 0.08 0.58                            0.88   0.88   0.88          0.88
                                          Rv1 0.31 0.66 0.60 0.35 0.25 0.50 0.70                            0.83   0.85   0.85          0.85
                                          Rv2 0.69 0.57 0.87 0.46 0.51 0.48 0.56                            0.90   0.92   0.90          0.92

behaviour requires further study and will be the object of          improves performance across the simulated and real worlds.",TABLE IV: Area under ROC curve (AUC) values for the aggregated centralized learning and federated learning approaches.,future research.,2022-04-14 13:20:52+00:00,Federated Learning for Vision-based Obstacle Avoidance in the Internet of Robotic Things,cs.RO,['cs.RO'],"[arxiv.Result.Author('Xianjia Yu'), arxiv.Result.Author('Jorge Peña Queralta'), arxiv.Result.Author('Tomi Westerlund')]","Deep learning methods have revolutionized mobile robotics, from advanced
perception models for an enhanced situational awareness to novel control
approaches through reinforcement learning. This paper explores the potential of
federated learning for distributed systems of mobile robots enabling
collaboration on the Internet of Robotic Things. To demonstrate the
effectiveness of such an approach, we deploy wheeled robots in different indoor
environments. We analyze the performance of a federated learning approach and
compare it to a traditional centralized training process with a priori
aggregated data. We show the benefits of collaborative learning across
heterogeneous environments and the potential for sim-to-real knowledge
transfer. Our results demonstrate significant performance benefits of FL and
sim-to-real transfer for vision-based navigation, in addition to the inherent
privacy-preserving nature of FL by keeping computation at the edge. This is, to
the best of our knowledge, the first work to leverage FL for vision-based
navigation that also tests results in real-world settings.",0.2820157,-0.12383905,-0.14334053,C
4881,"Clearly,
further research is required to ascertain if such methods can be further developed to obtain proven
performance guarantees, a signiﬁcant step required to make a control systems applicable in practice.","However, existing methods lack formal stability guarantees and fail to capture physically-
based intuition that is often crucial in the design and tuning of advanced control systems.","Disclosure statement
The authors conﬁrm that there are no known conﬂicts of interest associated with this publication.",2022-04-15 03:54:32+00:00,"A review of path following control strategies for autonomous robotic vehicles: theory, simulations, and experiments",cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Nguyen Hung'), arxiv.Result.Author('Francisco Rego'), arxiv.Result.Author('Joao Quintas'), arxiv.Result.Author('Joao Cruz'), arxiv.Result.Author('Marcelo Jacinto'), arxiv.Result.Author('David Souto'), arxiv.Result.Author('Andre Potes'), arxiv.Result.Author('Luis Sebastiao'), arxiv.Result.Author('Antonio Pascoal')]","This article presents an in-depth review of the topic of path following for
autonomous robotic vehicles, with a specific focus on vehicle motion in two
dimensional space (2D). From a control system standpoint, path following can be
formulated as the problem of stabilizing a path following error system that
describes the dynamics of position and possibly orientation errors of a vehicle
with respect to a path, with the errors defined in an appropriate reference
frame. In spite of the large variety of path following methods described in the
literature we show that, in principle, most of them can be categorized in two
groups: stabilization of the path following error system expressed either in
the vehicle's body frame or in a frame attached to a ""reference point"" moving
along the path, such as a Frenet-Serret (F-S) frame or a Parallel Transport
(P-T) frame. With this observation, we provide a unified formulation that is
simple but general enough to cover many methods available in the literature. We
then discuss the advantages and disadvantages of each method, comparing them
from the design and implementation standpoint. We further show experimental
results of the path following methods obtained from field trials testing with
under-actuated and fully-actuated autonomous marine vehicles. In addition, we
introduce open-source Matlab and Gazebo/ROS simulation toolboxes that are
helpful in testing path following methods prior to their integration in the
combined guidance, navigation, and control systems of autonomous vehicles.",-0.019851554,-0.28276336,0.4361198,A
4885,"In particular, our results indicate
that further research on large-scale pre-trained and attention-      via stochastic gradient descent.","Nonethe-                                    n L(fθ(xi), yi) (1)
less, we observed that type of learned representation has a
great impact in determining whether a good accuracy and                                                      i=1
robustness can be achieved.","The differentiable loss func-
based representations are promising paths for learning robust        tion L : Y × Y → R characterizes how well the network’s
and accurate models in practice.",2022-04-15 08:12:15+00:00,Revisiting the Adversarial Robustness-Accuracy Tradeoff in Robot Learning,cs.RO,"['cs.RO', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Mathias Lechner'), arxiv.Result.Author('Alexander Amini'), arxiv.Result.Author('Daniela Rus'), arxiv.Result.Author('Thomas A. Henzinger')]","Adversarial training (i.e., training on adversarially perturbed input data)
is a well-studied method for making neural networks robust to potential
adversarial attacks during inference. However, the improved robustness does not
come for free but rather is accompanied by a decrease in overall model accuracy
and performance. Recent work has shown that, in practical robot learning
applications, the effects of adversarial training do not pose a fair trade-off
but inflict a net loss when measured in holistic robot performance. This work
revisits the robustness-accuracy trade-off in robot learning by systematically
analyzing if recent advances in robust training methods and theory in
conjunction with adversarial robot learning can make adversarial training
suitable for real-world robot applications. We evaluate a wide variety of robot
learning tasks ranging from autonomous driving in a high-fidelity environment
amenable to sim-to-real deployment, to mobile robot gesture recognition. Our
results demonstrate that, while these techniques make incremental improvements
on the trade-off on a relative scale, the negative side-effects caused by
adversarial training still outweigh the improvements by an order of magnitude.
We conclude that more substantial advances in robust learning methods are
necessary before they can benefit robot learning tasks in practice.",0.10381217,0.011619192,-0.34661084,C
5152,"Likewise, in this paper,
                                        scheduling information to further study and improve the ROS 2          we present a message ﬂow analysis method for ROS 2
                                        executor as well as optimize any ROS 2 system.","This can provide valuable timing and           optimization (i.e., the bottleneck).",The source code         distributed computation graphs.,2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on publish-subscribe frameworks,
such as ROS, to efficiently implement modular computation graphs. The ROS 2
executor, a high-level task scheduler which handles messages internally, is a
performance bottleneck. In previous work, we presented ros2_tracing, a
framework with instrumentation and tools for real-time tracing of ROS 2. We now
extend on that instrumentation and leverage the tracing tools to propose an
analysis and visualization of the flow of messages across distributed ROS 2
systems. Our proposed method detects one-to-many and many-to-many causal links
between input and output messages, including indirect causal links through
simple user-level annotations. We validate our method on both synthetic and
real robotic systems, and demonstrate its low runtime overhead. Moreover, the
underlying intermediate execution representation database can be further
leveraged to extract additional metrics and high-level results. This can
provide valuable timing and scheduling information to further study and improve
the ROS 2 executor as well as optimize any ROS 2 system. The source code is
available at: https://github.com/christophebedard/ros2-message-flow-analysis.",0.49228242,-0.016795816,0.37681133,C
5153,"The        useful information to further study or work on resolving
proposed instrumentation allows extracting simple metrics,       those open problems.","Building a model and graph of
we presented ros2 tracing [10], a framework with low-            the path of messages across a ROS 2 system would provide
overhead instrumentation and tracing tools for ROS 2.","Thus, we propose a low-overhead
such as publishing rate and subscription or timer callback       technique that can transparently & natively track messages
duration.",2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on publish-subscribe frameworks,
such as ROS, to efficiently implement modular computation graphs. The ROS 2
executor, a high-level task scheduler which handles messages internally, is a
performance bottleneck. In previous work, we presented ros2_tracing, a
framework with instrumentation and tools for real-time tracing of ROS 2. We now
extend on that instrumentation and leverage the tracing tools to propose an
analysis and visualization of the flow of messages across distributed ROS 2
systems. Our proposed method detects one-to-many and many-to-many causal links
between input and output messages, including indirect causal links through
simple user-level annotations. We validate our method on both synthetic and
real robotic systems, and demonstrate its low runtime overhead. Moreover, the
underlying intermediate execution representation database can be further
leveraged to extract additional metrics and high-level results. This can
provide valuable timing and scheduling information to further study and improve
the ROS 2 executor as well as optimize any ROS 2 system. The source code is
available at: https://github.com/christophebedard/ros2-message-flow-analysis.",0.5818981,-0.04681438,0.31937593,C
5154,"Likewise, in this paper,
                                        scheduling information to further study and improve the ROS 2          we present a message ﬂow analysis method for ROS 2
                                        executor as well as optimize any ROS 2 system.","This can provide valuable timing and           optimization (i.e., the bottleneck).",The source code         distributed computation graphs.,2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on publish-subscribe frameworks,
such as ROS, to efficiently implement modular computation graphs. The ROS 2
executor, a high-level task scheduler which handles messages internally, is a
performance bottleneck. In previous work, we presented ros2_tracing, a
framework with instrumentation and tools for real-time tracing of ROS 2. We now
extend on that instrumentation and leverage the tracing tools to propose an
analysis and visualization of the flow of messages across distributed ROS 2
systems. Our proposed method detects one-to-many and many-to-many causal links
between input and output messages, including indirect causal links through
simple user-level annotations. We validate our method on both synthetic and
real robotic systems, and demonstrate its low runtime overhead. Moreover, the
underlying intermediate execution representation database can be further
leveraged to extract additional metrics and high-level results. This can
provide valuable timing and scheduling information to further study and improve
the ROS 2 executor as well as optimize any ROS 2 system. The source code is
available at: https://github.com/christophebedard/ros2-message-flow-analysis.",0.49228242,-0.016795816,0.37681133,C
5155,"The        useful information to further study or work on resolving
proposed instrumentation allows extracting simple metrics,       those open problems.","Building a model and graph of
we presented ros2 tracing [10], a framework with low-            the path of messages across a ROS 2 system would provide
overhead instrumentation and tracing tools for ROS 2.","Thus, we propose a low-overhead
such as publishing rate and subscription or timer callback       technique that can transparently & natively track messages
duration.",2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on publish-subscribe frameworks,
such as ROS, to efficiently implement modular computation graphs. The ROS 2
executor, a high-level task scheduler which handles messages internally, is a
performance bottleneck. In previous work, we presented ros2_tracing, a
framework with instrumentation and tools for real-time tracing of ROS 2. We now
extend on that instrumentation and leverage the tracing tools to propose an
analysis and visualization of the flow of messages across distributed ROS 2
systems. Our proposed method detects one-to-many and many-to-many causal links
between input and output messages, including indirect causal links through
simple user-level annotations. We validate our method on both synthetic and
real robotic systems, and demonstrate its low runtime overhead. Moreover, the
underlying intermediate execution representation database can be further
leveraged to extract additional metrics and high-level results. This can
provide valuable timing and scheduling information to further study and improve
the ROS 2 executor as well as optimize any ROS 2 system. The source code is
available at: https://github.com/christophebedard/ros2-message-flow-analysis.",0.5818981,-0.04681438,0.31937593,C
5156,"Likewise, in this paper,
                                       information to further study and improve the ROS 2 executor as         we present a message ﬂow analysis method for ROS 2
                                       well as optimize any ROS 2 system.","This can provide valuable timing and scheduling         optimization (i.e., the bottleneck).",The source code is available        distributed computation graphs.,2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on publish-subscribe frameworks such
as the Robot Operating System (ROS) to efficiently implement modular
computation graphs. The ROS 2 executor, a high-level task scheduler which
handles ROS 2 messages, is a performance bottleneck. We extend ros2_tracing, a
framework with instrumentation and tools for real-time tracing of ROS 2, with
the analysis and visualization of the flow of messages across distributed ROS 2
systems. Our method detects one-to-many and many-to-many causal links between
input and output messages, including indirect causal links through simple
user-level annotations. We validate our method on both synthetic and real
robotic systems, and demonstrate its low runtime overhead. Moreover, the
underlying intermediate execution representation database can be further
leveraged to extract additional metrics and high-level results. This can
provide valuable timing and scheduling information to further study and improve
the ROS 2 executor as well as optimize any ROS 2 system. The source code is
available at: https://github.com/christophebedard/ros2-message-flow-analysis.",0.48705596,-0.014129642,0.39775717,C
5157,"The        useful information to further study or work on resolving
proposed instrumentation allows extracting simple metrics,       those open problems.","Building a model and graph of
we presented ros2 tracing [10], a framework with low-            the path of messages across a ROS 2 system would provide
overhead instrumentation and tracing tools for ROS 2.","Thus, we propose a low-overhead
such as publishing rate and subscription or timer callback       technique that can transparently & natively track messages
duration.",2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on publish-subscribe frameworks such
as the Robot Operating System (ROS) to efficiently implement modular
computation graphs. The ROS 2 executor, a high-level task scheduler which
handles ROS 2 messages, is a performance bottleneck. We extend ros2_tracing, a
framework with instrumentation and tools for real-time tracing of ROS 2, with
the analysis and visualization of the flow of messages across distributed ROS 2
systems. Our method detects one-to-many and many-to-many causal links between
input and output messages, including indirect causal links through simple
user-level annotations. We validate our method on both synthetic and real
robotic systems, and demonstrate its low runtime overhead. Moreover, the
underlying intermediate execution representation database can be further
leveraged to extract additional metrics and high-level results. This can
provide valuable timing and scheduling information to further study and improve
the ROS 2 executor as well as optimize any ROS 2 system. The source code is
available at: https://github.com/christophebedard/ros2-message-flow-analysis.",0.5818981,-0.04681438,0.31937593,C
5158,"This can provide valuable timing and scheduling         such as providing information about the overall performance
                                        information to further study and improve the ROS 2 executor as         of the message-passing system.","Moreover,           tracing framework allows extracting ROS 2 execution infor-
                                        the underlying intermediate execution representation database          mation, and can be extended with additional instrumentation
                                        can be further leveraged to extract additional metrics and high-       for more advanced use-cases or performance analysis goals,
                                        level results.",well as optimize any ROS 2 system.,2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on the publish-subscribe
communication paradigm and middleware frameworks that support it, such as the
Robot Operating System (ROS), to efficiently implement modular computation
graphs. The ROS 2 executor, a high-level task scheduler which handles ROS 2
messages, is a performance bottleneck. We extend ros2_tracing, a framework with
instrumentation and tools for real-time tracing of ROS 2, with the analysis and
visualization of the flow of messages across distributed ROS 2 systems. Our
method detects one-to-many and many-to-many causal links between input and
output messages, including indirect causal links through simple user-level
annotations. We validate our method on both synthetic and real robotic systems,
and demonstrate its low runtime overhead. Moreover, the underlying intermediate
execution representation database can be further leveraged to extract
additional metrics and high-level results. This can provide valuable timing and
scheduling information to further study and improve the ROS 2 executor as well
as optimize any ROS 2 system. The source code is available at:
https://github.com/christophebedard/ros2-message-flow-analysis.",0.5528852,-0.07637255,0.3267435,C
5159,"One     useful information to further study or work on resolving
interesting technique is the critical path method, where the     those open problems.","Building a model and graph of
   To extract useful information from trace data, advanced       the path of messages across a ROS 2 system would provide
trace analysis methods build models from the trace data.","Thus, we propose a low-overhead
critical path is deﬁned as the longest path in a DAG.",2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on the publish-subscribe
communication paradigm and middleware frameworks that support it, such as the
Robot Operating System (ROS), to efficiently implement modular computation
graphs. The ROS 2 executor, a high-level task scheduler which handles ROS 2
messages, is a performance bottleneck. We extend ros2_tracing, a framework with
instrumentation and tools for real-time tracing of ROS 2, with the analysis and
visualization of the flow of messages across distributed ROS 2 systems. Our
method detects one-to-many and many-to-many causal links between input and
output messages, including indirect causal links through simple user-level
annotations. We validate our method on both synthetic and real robotic systems,
and demonstrate its low runtime overhead. Moreover, the underlying intermediate
execution representation database can be further leveraged to extract
additional metrics and high-level results. This can provide valuable timing and
scheduling information to further study and improve the ROS 2 executor as well
as optimize any ROS 2 system. The source code is available at:
https://github.com/christophebedard/ros2-message-flow-analysis.",0.5859016,0.0017703683,0.24635774,C
5160,"This can provide valuable timing and scheduling         such as providing information about the overall performance
                                        information to further study and improve the ROS 2 executor as         of the message-passing system.","Moreover,           tracing framework allows extracting ROS 2 execution infor-
                                        the underlying intermediate execution representation database          mation, and can be extended with additional instrumentation
                                        can be further leveraged to extract additional metrics and high-       for more advanced use-cases or performance analysis goals,
                                        level results.",well as optimize any ROS 2 system.,2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on the publish-subscribe
communication paradigm and middleware frameworks that support it, such as the
Robot Operating System (ROS), to efficiently implement modular computation
graphs. The ROS 2 executor, a high-level task scheduler which handles ROS 2
messages, is a performance bottleneck. We extend ros2_tracing, a framework with
instrumentation and tools for real-time tracing of ROS 2, with the analysis and
visualization of the flow of messages across distributed ROS 2 systems. Our
method detects one-to-many and many-to-many causal links between input and
output messages, including indirect causal links through simple user-level
annotations. We validate our method on both synthetic and real robotic systems,
and demonstrate its low runtime overhead. Moreover, the underlying intermediate
execution representation database can be further leveraged to extract
additional metrics and high-level results. This can provide valuable timing and
scheduling information to further study and improve the ROS 2 executor as well
as optimize any ROS 2 system. The source code is available at:
https://github.com/christophebedard/ros2-message-flow-analysis.",0.5528852,-0.07637255,0.3267435,C
5161,"One     useful information to further study or work on resolving
interesting technique is the critical path method, where the     those open problems.","Building a model and graph of
   To extract useful information from trace data, advanced       the path of messages across a ROS 2 system would provide
trace analysis methods build models from the trace data.","Thus, we propose a low-overhead
critical path is deﬁned as the longest path in a DAG.",2022-04-21 15:45:01+00:00,Message Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christophe Bédard'), arxiv.Result.Author('Pierre-Yves Lajoie'), arxiv.Result.Author('Giovanni Beltrame'), arxiv.Result.Author('Michel Dagenais')]","Distributed robotic systems rely heavily on the publish-subscribe
communication paradigm and middleware frameworks that support it, such as the
Robot Operating System (ROS), to efficiently implement modular computation
graphs. The ROS 2 executor, a high-level task scheduler which handles ROS 2
messages, is a performance bottleneck. We extend ros2_tracing, a framework with
instrumentation and tools for real-time tracing of ROS 2, with the analysis and
visualization of the flow of messages across distributed ROS 2 systems. Our
method detects one-to-many and many-to-many causal links between input and
output messages, including indirect causal links through simple user-level
annotations. We validate our method on both synthetic and real robotic systems,
and demonstrate its low runtime overhead. Moreover, the underlying intermediate
execution representation database can be further leveraged to extract
additional metrics and high-level results. This can provide valuable timing and
scheduling information to further study and improve the ROS 2 executor as well
as optimize any ROS 2 system. The source code is available at:
https://github.com/christophebedard/ros2-message-flow-analysis.",0.5859016,0.0017703683,0.24635774,C
5189,"Despite the great
identiﬁcation of possible actions, evaluation of their useful-    impact of this work, however, no further research has been
ness and execution of those considered as optimal.","(2016) as an open
standpoint, and divided in three phases for tractability:         challenge more than ﬁve years ago.",Solving        done in this ﬁeld.,2022-04-22 10:53:42+00:00,Enough is Enough: Towards Autonomous Uncertainty-driven Stopping Criteria,cs.RO,['cs.RO'],"[arxiv.Result.Author('Julio A. Placed'), arxiv.Result.Author('José A. Castellanos')]","Autonomous robotic exploration has long attracted the attention of the
robotics community and is a topic of high relevance. Deploying such systems in
the real world, however, is still far from being a reality. In part, it can be
attributed to the fact that most research is directed towards improving
existing algorithms and testing novel formulations in simulation environments
rather than addressing practical issues of real-world scenarios. This is the
case of the fundamental problem of autonomously deciding when exploration has
to be terminated or changed (stopping criteria), which has not received any
attention recently. In this paper, we discuss the importance of using
appropriate stopping criteria and analyse the behaviour of a novel criterion
based on the evolution of optimality criteria in active graph-SLAM.",0.0833962,-0.29915485,-0.090088174,C_centroid
5190,"Despite the great
identiﬁcation of possible actions, evaluation of their useful-    impact of this work, however, no further research has been
ness and execution of those considered as optimal.","(2016) as an open
standpoint, and divided in three phases for tractability:         challenge more than ﬁve years ago.",Solving        done in this ﬁeld.,2022-04-22 10:53:42+00:00,Enough is Enough: Towards Autonomous Uncertainty-driven Stopping Criteria,cs.RO,['cs.RO'],"[arxiv.Result.Author('Julio A. Placed'), arxiv.Result.Author('José A. Castellanos')]","Autonomous robotic exploration has long attracted the attention of the
robotics community and is a topic of high relevance. Deploying such systems in
the real world, however, is still far from being a reality. In part, it can be
attributed to the fact that most research is directed towards improving
existing algorithms and testing novel formulations in simulation environments
rather than addressing practical issues of real-world scenarios. This is the
case of the fundamental problem of autonomously deciding when exploration has
to be terminated or changed (stopping criteria), which has not received any
attention recently. In this paper, we discuss the importance of using
appropriate stopping criteria and analyse the behaviour of a novel criterion
based on the evolution of optimality criteria in active graph-SLAM.",0.0833962,-0.29915485,-0.090088174,C
5228,"We assume that at the current time step,                                   bCi =  ...  (25)
the sheep is outside the P, this ensures β h2 ≥ 0.
                                                                                                       γ2 bim + (xSi − xDm )T f i

                                                                           4
To ensure all collision avoidance with all n sheep1, we                         B. Monte Carlo Simulations

compose constraints (24) for all the herd as follows:                              We further study the performance of the proposed control
                                                                                strategy by using Monte Carlo simulations with varying
AC1           bC1                                                           initial conﬁgurations and varying number of sheep n and dog
                                                                                robots m. The values of the constants in sheep dynamics
 ...  uaDll ≤  ...  =⇒ AC uaDll ≤ bC (26)                               are kG = 1, kS = 0.3 and kD = 0.08.","Here                                                γ2 bi1 + (xSi − xD1 )T f i 
f T f ≥ 0 always.","We vary n and
                                                                                m from 1 to 10 and for a given pair of (n, m) we run the
           ACn           bCn                                                    simulation for a hundred times with a random initialization of
                                                                                x0 = (xS1 (0), · · · , xSn (0), xD1 (0), · · · , xDm (0)) in every
Given the defending (16) and collision avoidance (26) con-                      run.",2022-04-22 22:14:03+00:00,Noncooperative Herding With Control Barrier Functions: Theory and Experiments,cs.RO,"['cs.RO', 'math.OC']","[arxiv.Result.Author('Jaskaran Grover'), arxiv.Result.Author('Nishant Mohanty'), arxiv.Result.Author('Wenhao Luo'), arxiv.Result.Author('Changliu Liu'), arxiv.Result.Author('Katia Sycara')]","In this paper, we consider the problem of protecting a high-value unit from
inadvertent attack by a group of agents using defending robots. Specifically,
we develop a control strategy for the defending agents that we call ""dog
robots"" to prevent a flock of ""sheep agents"" from breaching a protected zone.
We take recourse to control barrier functions to pose this problem and exploit
the interaction dynamics between the sheep and dogs to find dogs' velocities
that result in the sheep getting repelled from the zone. We solve a QP
reactively that incorporates the defending constraints to compute the desired
velocities for all dogs. Owing to this, our proposed framework is composable
\textit{i.e.} it allows for simultaneous inclusion of multiple protected zones
in the constraints on dog robots' velocities. We provide a theoretical proof of
feasibility of our strategy for the one dog/one sheep case. Additionally, we
provide empirical results of two dogs defending the protected zone from upto
ten sheep averaged over a hundred simulations and report high success rates. We
also demonstrate this algorithm experimentally on non-holonomic robots. Videos
of these results are available at https://tinyurl.com/4dj2kjwx.",-0.1449961,-0.14557268,0.22686234,A
5476,"vehicle, the service returns the correspondent value to
In [15] a further study described a vehicular cloud net-                the client.","When a property is requested from a nearby
in VANETs can be improved by incorporating edge nodes.","A property, therefore, is data computed based
working (VCN) system combining information-centric net-                 on a speciﬁc road user’s state.",2022-04-28 17:05:30+00:00,Interaction of Autonomous and Manually-Controlled Vehicles:Implementation of a Road User Communication Service,cs.RO,['cs.RO'],"[arxiv.Result.Author('Nikita Smirnov'), arxiv.Result.Author('Sebastian Tschernuth'), arxiv.Result.Author('Walter Morales-Alvarez'), arxiv.Result.Author('Cristina Olaverri-Monreal')]","Communication between vehicles with varying degrees of automation is
increasingly challenging as highly automated vehicles are unable to interpret
the non-verbal signs of other road users. The lack of understanding on roads
leads to lower trust in automated vehicles and impairs traffic safety. To
address these problems, we propose the Road User Communication Service, a
software as a service platform, which provides information exchange and cloud
computing services for vehicles with varying degrees of automation. To inspect
the operability of the proposed solution, field tests were carried out on a
test track, where the autonomous JKU-ITS research vehicle requested the state
of a driver in a manually-controlled vehicle through the implemented service.
The test results validated the approach showing its feasibility to be used as a
communication platform. A link to the source code is available.",0.39730397,0.13012241,0.049029645,B
5556,"motivating further study into the impact of different sensors (e.g.,
[31]) as well as different model-ﬁtting techniques to prevent overﬁt-
ting (which we avoided by implementing a relatively simple linear
model structure relative to the complexity of the dataset).","We expect that this will further improve phase          expects high conﬁdence in the measurement, resulting in persistent
estimation due to the new information available from these sensors,        negative biases in the incline estimate.","To further improve our estimates in future work, we would ﬁrst
target the inter-person kinematic variability that increases the variance
of not only the estimated phase, but also the remaining state vector
elements.",2022-04-30 04:07:53+00:00,Real-Time Gait Phase and Task Estimation for Controlling a Powered Ankle Exoskeleton on Extremely Uneven Terrain,cs.RO,['cs.RO'],"[arxiv.Result.Author('Roberto Leo Medrano'), arxiv.Result.Author('Gray Cortright Thomas'), arxiv.Result.Author('Connor G. Keais'), arxiv.Result.Author('Elliott J. Rouse'), arxiv.Result.Author('Robert D. Gregg')]","Positive biomechanical outcomes have been reported with lower-limb
exoskeletons in laboratory settings, but these devices have difficulty
delivering appropriate assistance in synchrony with human gait as the task or
rate of phase progression change in real-world environments. This paper
presents a controller for an ankle exoskeleton that uses a data-driven
kinematic model to continuously estimate the phase, phase rate, stride length,
and ground incline states during locomotion, which enables the real-time
adaptation of torque assistance to match human torques observed in a
multi-activity database of 10 able-bodied subjects. We demonstrate in live
experiments with a new cohort of 10 able-bodied participants that the
controller yields phase estimates comparable to the state of the art, while
also estimating task variables with similar accuracy to recent machine learning
approaches. The implemented controller successfully adapts its assistance in
response to changing phase and task variables, both during controlled treadmill
trials (N=10, phase RMSE: 4.8 +- 2.4\%) and a real-world stress test with
extremely uneven terrain (N=1, phase RMSE: 4.8 +- 2.7\%).",-0.014619166,-0.050468676,-0.024922084,A
5881,"This study paves the way for
further research into the robotic approach on a larger group of participants
with varying degrees of scoliosis severity, as well as eventual commercialization.","The beneﬁts
of employing a robotic approach were explored.","Acknowledgments

The authors thank Stella Chui Yi Chan and Kelly Lai for their contribution
in managing the recruitment of the subjects.",2022-05-07 06:14:16+00:00,Reliability of Robotic Ultrasound Scanning for Scoliosis Assessment in Comparison with Manual Scanning,cs.RO,['cs.RO'],"[arxiv.Result.Author('Maria Victorova'), arxiv.Result.Author('Heidi Hin Ting Lau'), arxiv.Result.Author('Timothy Tin-Yan Lee'), arxiv.Result.Author('David Navarro-Alarcon'), arxiv.Result.Author('Yongping Zheng')]","Background: Ultrasound (US) imaging for scoliosis assessment is challenging
for a non-experienced operator. The robotic scanning was developed to follow a
spinal curvature with deep learning and apply consistent forces to the patient'
back. Methods: 23 scoliosis patients were scanned with US devices both,
robotically and manually. Two human raters measured each subject's spinous
process angles (SPA) on robotic and manual coronal images. Results: The robotic
method showed high intra- (ICC > 0.85) and inter-rater (ICC > 0.77)
reliabilities. Compared with the manual method, the robotic approach showed no
significant difference (p < 0.05) when measuring coronal deformity angles. The
MAD for intra-rater analysis lies within an acceptable range from 0 deg to 5
deg for a minimum of 86% and a maximum 97% of a total number of the measured
angles. Conclusions: This study demonstrated that scoliosis deformity angles
measured on ultrasound images obtained with robotic scanning are comparable to
those obtained by manual scanning.",-0.21551755,-0.111399144,-0.043558996,A
6008,"Companies like                    addition, further research has made an attempt to produce
Google2, Amazon3, and Microsoft4 all have their own vari-                   the Lombard effect, with research relying on incremental
ations of these vocoders.","In
and Tacotron, uses spectrogram synthesis.","These TTS technologies produce                    adaptation of loudness to the context of distance and user
realistic speech and are normally freely available, or available            targeting [39] or the adjustment of volume based on envi-
with for a small fee, to the general public.",2022-05-10 15:10:23+00:00,Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Emma Hughson'), arxiv.Result.Author('Paige Tuttosi'), arxiv.Result.Author('Akihiro Matsufuji'), arxiv.Result.Author('Angelica Lim')]","Adapting one's voice to different ambient environments and social
interactions is required for human social interaction. In robotics, the ability
to recognize speech in noisy and quiet environments has received significant
attention, but considering ambient cues in the production of social speech
features has been little explored. Our research aims to modify a robot's speech
to maximize acceptability in various social and acoustic contexts, starting
with a use case for service robots in varying restaurants. We created an
original dataset collected over Zoom with participants conversing in scripted
and unscripted tasks given 7 different ambient sounds and background images.
Voice conversion methods, in addition to altered Text-to-Speech that matched
ambient specific data, were used for speech synthesis tasks. We conducted a
subjective perception study that showed humans prefer synthetic speech that
matches ambience and social context, ultimately preferring more human-like
voices. This work provides three solutions to ambient and socially appropriate
synthetic voices: (1) a novel protocol to collect real contextual audio voice
data, (2) tools and directions to manipulate robot speech for appropriate
social and ambient specific interactions, and (3) insight into voice
conversion's role in flexibly altering robot speech to match different ambient
environments.",0.21530843,-0.17176142,-0.0603602,C
6089,"3.As a pure
visual-based method in such a big work range, we also haven’t reach 100% in
insertion, just like a lot of other methods, we will further research in this area
and improve the performance to 100%.","In the
future, we want the robot to auto-detect the target alignment line.","4.We will also explore more binocular
applications and verify them on more precise tasks.",2022-05-12 09:01:29+00:00,Economical Precise Manipulation and Auto Eye-Hand Coordination with Binocular Visual Reinforcement Learning,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Yiwen Chen'), arxiv.Result.Author('Sheng Guo'), arxiv.Result.Author('Lei Zhou'), arxiv.Result.Author('Xian Yao Ng'), arxiv.Result.Author('Marcelo H. Ang Jr')]","Precision robotic manipulation tasks (insertion, screwing, precisely pick,
precisely place) are required in many scenarios. Previous methods achieved good
performance on such manipulation tasks. However, such methods typically require
tedious calibration or expensive sensors. 3D/RGB-D cameras and torque/force
sensors add to the cost of the robotic application and may not always be
economical. In this work, we aim to solve these but using only weak-calibrated
and low-cost webcams. We propose Binocular Alignment Learning (BAL), which
could automatically learn the eye-hand coordination and points alignment
capabilities to solve the four tasks. Our work focuses on working with unknown
eye-hand coordination and proposes different ways of performing eye-in-hand
camera calibration automatically. The algorithm was trained in simulation and
used a practical pipeline to achieve sim2real and test it on the real robot.
Our method achieves a competitively good result with minimal cost on the four
tasks.",-0.13016854,0.1777887,-0.11183271,B
6090,"3.As a pure
visual-based method in such a big work range, we also haven’t reach 100% in
insertion, just like a lot of other methods, we will further research in this area
and improve the performance to 100%.","In the
future, we want the robot to auto-detect the target alignment line.","4.We will also explore more binocular
applications and verify them on more precise tasks.",2022-05-12 09:01:29+00:00,Economical Precise Manipulation and Auto Eye-Hand Coordination with Binocular Visual Reinforcement Learning,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Yiwen Chen'), arxiv.Result.Author('Sheng Guo'), arxiv.Result.Author('Zedong Zhang'), arxiv.Result.Author('Lei Zhou'), arxiv.Result.Author('Xian Yao Ng'), arxiv.Result.Author('Marcelo H. Ang Jr')]","Precision robotic manipulation tasks (insertion, screwing, precisely pick,
precisely place) are required in many scenarios. Previous methods achieved good
performance on such manipulation tasks. However, such methods typically require
tedious calibration or expensive sensors. 3D/RGB-D cameras and torque/force
sensors add to the cost of the robotic application and may not always be
economical. In this work, we aim to solve these but using only weak-calibrated
and low-cost webcams. We propose Binocular Alignment Learning (BAL), which
could automatically learn the eye-hand coordination and points alignment
capabilities to solve the four tasks. Our work focuses on working with unknown
eye-hand coordination and proposes different ways of performing eye-in-hand
camera calibration automatically. The algorithm was trained in simulation and
used a practical pipeline to achieve sim2real and test it on the real robot.
Our method achieves a competitively good result with minimal cost on the four
tasks.",-0.13016854,0.1777887,-0.11183271,B
6171,"The ﬁndings of their study suggest that there may be a
                                                                    correlation between the robot-target distance and the robot’s
                                                                    social presence, which calls for further study.","[3] reported that
                                                                    there was no signiﬁcant evidence that the distance between
                                                                    a user and target inﬂuenced the social presence of a robot.","This paper
                                                                    focuses on mobility-constrained robots; distance issues are
                                                                    more complex for highly mobile robots (e.g., drones) that can
                                                                    move quickly and efﬁcently toward a target.",2022-05-13 16:30:53+00:00,Augmented Reality Appendages for Robots: Design Considerations and Recommendations for Maximizing Social and Functional Perception,cs.RO,['cs.RO'],"[arxiv.Result.Author('Ipek Goktan'), arxiv.Result.Author('Karen Ly'), arxiv.Result.Author('Thomas R. Groechel'), arxiv.Result.Author('Maja J. Mataric')]","In order to address the limitations of gestural capabilities in physical
robots, researchers in Virtual, Augmented, Mixed Reality Human-Robot
Interaction (VAM-HRI) have been using augmented-reality visualizations that
increase robot expressivity and improve user perception (e.g., social
presence). While a multitude of virtual robot deictic gestures (e.g., pointing
to an object) have been implemented to improve interactions within VAM-HRI,
such systems are often reported to have tradeoffs between functional and social
user perceptions of robots, creating a need for a unified approach that
considers both attributes. We performed a literature analysis that selected
factors that were noted to significantly influence either user perception or
task efficiency and propose a set of design considerations and recommendations
that address those factors by combining anthropomorphic and non-anthropomorphic
virtual gestures based on the motivation of the interaction, visibility of the
target and robot, salience of the target, and distance between the target and
robot. The proposed recommendations provide the VAM-HRI community with starting
points for selecting appropriate gesture types for a multitude of interaction
contexts.",-0.22182238,0.036957055,0.0957344,A
6183,"However, further research is required to understand the coupling between planning and learning-based con-
trol near actuation limits.","In particular, for those
planners which require a precise and agile downstream controller (e.g., for close-proximity ﬂight or drone
racing [1, 10]), our method immediately provides a solution and further pushes the boundary of these plan-
ners, because our state-of-the-art tracking capabilities enable tighter conﬁgurations and smaller clearances.","Future work will consider using Neural-Fly in a combined planning and control
structure such as MPC, which will be able to handle actuation limits.",2022-05-13 21:55:28+00:00,Neural-Fly Enables Rapid Learning for Agile Flight in Strong Winds,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author(""Michael O'Connell""), arxiv.Result.Author('Guanya Shi'), arxiv.Result.Author('Xichen Shi'), arxiv.Result.Author('Kamyar Azizzadenesheli'), arxiv.Result.Author('Anima Anandkumar'), arxiv.Result.Author('Yisong Yue'), arxiv.Result.Author('Soon-Jo Chung')]","Executing safe and precise flight maneuvers in dynamic high-speed winds is
important for the ongoing commoditization of uninhabited aerial vehicles
(UAVs). However, because the relationship between various wind conditions and
its effect on aircraft maneuverability is not well understood, it is
challenging to design effective robot controllers using traditional control
design methods. We present Neural-Fly, a learning-based approach that allows
rapid online adaptation by incorporating pretrained representations through
deep learning. Neural-Fly builds on two key observations that aerodynamics in
different wind conditions share a common representation and that the
wind-specific part lies in a low-dimensional space. To that end, Neural-Fly
uses a proposed learning algorithm, domain adversarially invariant
meta-learning (DAIML), to learn the shared representation, only using 12
minutes of flight data. With the learned representation as a basis, Neural-Fly
then uses a composite adaptation law to update a set of linear coefficients for
mixing the basis elements. When evaluated under challenging wind conditions
generated with the Caltech Real Weather Wind Tunnel, with wind speeds up to
43.6 kilometers/hour (12.1 meters/second), Neural-Fly achieves precise flight
control with substantially smaller tracking error than state-of-the-art
nonlinear and adaptive controllers. In addition to strong empirical
performance, the exponential stability of Neural-Fly results in robustness
guarantees. Last, our control design extrapolates to unseen wind conditions, is
shown to be effective for outdoor flights with only onboard sensors, and can
transfer across drones with minimal performance degradation.",-0.20467138,-0.05796504,0.08827341,A
6629,"These datasets have been made
    (Tunnel)                                                                           open-source to promote further research on lidar odometry
                                                                                       and SLAM in underground environments: github.com/
    power plant                                       feature-poor corridors,          NeBula-Autonomy/nebula-odometry-dataset.","The
                                                                                       ground truth trajectory of the robot is determined based on
    Bruceton Mine                                     self-similar                     LOCUS 1.0 and its multi-stage registration technique: scan-
                                                                                       to-scan and scan-to-map (with high computational param-
F   Pittsburgh, PA        Husky 1569.73 49:13 self-repetitive geometries       3*      eters and slower pace of data processing) and some man-
                                                                                       ual post-processing work.","G   Elma, WA              Husky 877.21         93:10  large & narrow spaces    3

    (Urban)

    Subway Station                                    3-level,

H   Los Angeles, CA       Spot 1777.45 46:57          multiple stairs,         3

    (Urban)                                           feature-poor corridors,

                                                      large & narrow spaces

    Kentucky Underground                              large area,

I Limestone Mine, KY Spot 768.82               19:28 non-uniform environment,  1

    (Cave)                                            degraded lightning

    Kentucky Underground                              large area,

J Limestone Mine, KY Husky 2339.81 57:55 non-uniform environment, 3

    (Cave)                                            degraded lightning

* For our experiments we use only two lidars.",2022-05-24 04:51:08+00:00,LOCUS 2.0: Robust and Computationally Efficient Lidar Odometry for Real-Time Underground 3D Mapping,cs.RO,['cs.RO'],"[arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Luca Carlone'), arxiv.Result.Author('Ali-akbar Agha-mohammadi')]","Lidar odometry has attracted considerable attention as a robust localization
method for autonomous robots operating in complex GNSS-denied environments.
However, achieving reliable and efficient performance on heterogeneous
platforms in large-scale environments remains an open challenge due to the
limitations of onboard computation and memory resources needed for autonomous
operation. In this work, we present LOCUS 2.0, a robust and
computationally-efficient \lidar odometry system for real-time underground 3D
mapping. LOCUS 2.0 includes a novel normals-based \morrell{Generalized
Iterative Closest Point (GICP)} formulation that reduces the computation time
of point cloud alignment, an adaptive voxel grid filter that maintains the
desired computation load regardless of the environment's geometry, and a
sliding-window map approach that bounds the memory consumption. The proposed
approach is shown to be suitable to be deployed on heterogeneous robotic
platforms involved in large-scale explorations under severe computation and
memory constraints. We demonstrate LOCUS 2.0, a key element of the CoSTAR
team's entry in the DARPA Subterranean Challenge, across various underground
scenarios.
  We release LOCUS 2.0 as an open-source library and also release a
\lidar-based odometry dataset in challenging and large-scale underground
environments. The dataset features legged and wheeled platforms in multiple
environments including fog, dust, darkness, and geometrically degenerate
surroundings with a total of $11~h$ of operations and $16~km$ of distance
traveled.",0.00087745115,0.3827842,0.041717023,B
6630,"These datasets have been made open-source
                                                                  large area,                to promote further research on lidar odometry and SLAM
    (Tunnel)                                             non-uniform environment,            in underground environments: github.com/NeBula-Autonomy/
                                                                                             nebula-odometry-dataset.","(Cave)                                                        large area,                The ground truth trajectory of the robot is determined based on
                                                         non-uniform environment,            LOCUS 1.0 and its multi-stage registration technique: scan-
    Bruceton Mine                                                                            to-scan and scan-to-map (with high computational parameters
                                                             degraded lightning              and slower pace of data processing) and some manual post-
F   Pittsburgh, PA        Husky 1569.73                                                      processing work.","power plant                                              degraded lightning

G   Elma, WA              Husky 877.21

    (Urban)

    Subway Station

H   Los Angeles, CA       Spot 1777.45

    (Urban)

      Kentucky Underground       768.82
I Limestone Mine, KY Spot

                (Cave)

       Kentucky Underground
J Limestone Mine, KY Husky 2339.81

                 (Cave)

* For our experiments we use only two lidars.",2022-05-24 04:51:08+00:00,LOCUS 2.0: Robust and Computationally Efficient Lidar Odometry for Real-Time Underground 3D Mapping,cs.RO,['cs.RO'],"[arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Luca Carlone'), arxiv.Result.Author('Ali-akbar Agha-mohammadi')]","Lidar odometry has attracted considerable attention as a robust localization
method for autonomous robots operating in complex GNSS-denied environments.
However, achieving reliable and efficient performance on heterogeneous
platforms in large-scale environments remains an open challenge due to the
limitations of onboard computation and memory resources needed for autonomous
operation. In this work, we present LOCUS 2.0, a robust and
computationally-efficient \lidar odometry system for real-time underground 3D
mapping. LOCUS 2.0 includes a novel normals-based \morrell{Generalized
Iterative Closest Point (GICP)} formulation that reduces the computation time
of point cloud alignment, an adaptive voxel grid filter that maintains the
desired computation load regardless of the environment's geometry, and a
sliding-window map approach that bounds the memory consumption. The proposed
approach is shown to be suitable to be deployed on heterogeneous robotic
platforms involved in large-scale explorations under severe computation and
memory constraints. We demonstrate LOCUS 2.0, a key element of the CoSTAR
team's entry in the DARPA Subterranean Challenge, across various underground
scenarios.
  We release LOCUS 2.0 as an open-source library and also release a
\lidar-based odometry dataset in challenging and large-scale underground
environments. The dataset features legged and wheeled platforms in multiple
environments including fog, dust, darkness, and geometrically degenerate
surroundings with a total of $11~h$ of operations and $16~km$ of distance
traveled.",0.053474557,0.34925407,0.056228716,B
6748,"EXPERIMENTS                            We have prepared these datasets, in the format of pose
                                                                 graphs and keyed scans, and released them open-source
   This section showcases the performance of LAMP 2.0 and        to promote further research on multi-robot loop closure
provides experimental results to highlight the new features      detection and robust pose graph optimization.",III.,and improvements from the previous version [8].,2022-05-26 03:52:56+00:00,LAMP 2.0: A Robust Multi-Robot SLAM System for Operation in Challenging Large-Scale Underground Environments,cs.RO,"['cs.RO', 'cs.MA']","[arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Christopher E. Denniston'), arxiv.Result.Author('Muhammad Fadhil Ginting'), arxiv.Result.Author('Antoni Rosinol'), arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Jingnan Shi'), arxiv.Result.Author('Arghya Chatterjee'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Ali-akbar Agha-mohammadi'), arxiv.Result.Author('Luca Carlone')]","Search and rescue with a team of heterogeneous mobile robots in unknown and
large-scale underground environments requires high-precision localization and
mapping. This crucial requirement is faced with many challenges in complex and
perceptually-degraded subterranean environments, as the onboard perception
system is required to operate in off-nominal conditions (poor visibility due to
darkness and dust, rugged and muddy terrain, and the presence of self-similar
and ambiguous scenes). In a disaster response scenario and in the absence of
prior information about the environment, robots must rely on noisy sensor data
and perform Simultaneous Localization and Mapping (SLAM) to build a 3D map of
the environment and localize themselves and potential survivors. To that end,
this paper reports on a multi-robot SLAM system developed by team CoSTAR in the
context of the DARPA Subterranean Challenge. We extend our previous work, LAMP,
by incorporating a single-robot front-end interface that is adaptable to
different odometry sources and lidar configurations, a scalable multi-robot
front-end to support inter- and intra-robot loop closure detection for large
scale environments and multi-robot teams, and a robust back-end equipped with
an outlier-resilient pose graph optimization based on Graduated Non-Convexity.
We provide a detailed ablation study on the multi-robot front-end and back-end,
and assess the overall system performance in challenging real-world datasets
collected across mines, power plants, and caves in the United States. We also
release our multi-robot back-end datasets (and the corresponding ground truth),
which can serve as challenging benchmarks for large-scale underground SLAM.",-0.12475232,0.31267136,0.06375474,B
6749,"EXPERIMENTS                            We have prepared these datasets, in the format of pose
                                                                 graphs and keyed scans, and released them open-source
   This section showcases the performance of LAMP 2.0 and        to promote further research on multi-robot loop closure
provides experimental results to highlight the new features      detection and robust pose graph optimization.",III.,and improvements from the previous version [8].,2022-05-26 03:52:56+00:00,LAMP 2.0: A Robust Multi-Robot SLAM System for Operation in Challenging Large-Scale Underground Environments,cs.RO,"['cs.RO', 'cs.MA']","[arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Christopher E. Denniston'), arxiv.Result.Author('Muhammad Fadhil Ginting'), arxiv.Result.Author('Antoni Rosinol'), arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Jingnan Shi'), arxiv.Result.Author('Arghya Chatterjee'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Ali-akbar Agha-mohammadi'), arxiv.Result.Author('Luca Carlone')]","Search and rescue with a team of heterogeneous mobile robots in unknown and
large-scale underground environments requires high-precision localization and
mapping. This crucial requirement is faced with many challenges in complex and
perceptually-degraded subterranean environments, as the onboard perception
system is required to operate in off-nominal conditions (poor visibility due to
darkness and dust, rugged and muddy terrain, and the presence of self-similar
and ambiguous scenes). In a disaster response scenario and in the absence of
prior information about the environment, robots must rely on noisy sensor data
and perform Simultaneous Localization and Mapping (SLAM) to build a 3D map of
the environment and localize themselves and potential survivors. To that end,
this paper reports on a multi-robot SLAM system developed by team CoSTAR in the
context of the DARPA Subterranean Challenge. We extend our previous work, LAMP,
by incorporating a single-robot front-end interface that is adaptable to
different odometry sources and lidar configurations, a scalable multi-robot
front-end to support inter- and intra-robot loop closure detection for large
scale environments and multi-robot teams, and a robust back-end equipped with
an outlier-resilient pose graph optimization based on Graduated Non-Convexity.
We provide a detailed ablation study on the multi-robot front-end and back-end,
and assess the overall system performance in challenging real-world datasets
collected across mines, power plants, and caves in the United States. We also
release our multi-robot back-end datasets (and the corresponding ground truth),
which can serve as challenging benchmarks for large-scale underground SLAM.",-0.12475232,0.31267136,0.06375474,B
6750,"EXPERIMENTS                         promote further research on multi-robot loop closure detection
                                                                  and robust pose graph optimization.","Adaptable to different lidar conﬁgs
         Adaptable to different odometry input                       We have prepared these datasets, in the format of pose
                                                                  graphs and keyed scans, and released them open-source to
                         III.","This section showcases the performance of LAMP 2.0 and
provides experimental results to highlight the new features and   C. Component Evaluation
improvements from the previous version [8].",2022-05-26 03:52:56+00:00,LAMP 2.0: A Robust Multi-Robot SLAM System for Operation in Challenging Large-Scale Underground Environments,cs.RO,"['cs.RO', 'cs.MA']","[arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Christopher E. Denniston'), arxiv.Result.Author('Muhammad Fadhil Ginting'), arxiv.Result.Author('Antoni Rosinol'), arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Jingnan Shi'), arxiv.Result.Author('Arghya Chatterjee'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Ali-akbar Agha-mohammadi'), arxiv.Result.Author('Luca Carlone')]","Search and rescue with a team of heterogeneous mobile robots in unknown and
large-scale underground environments requires high-precision localization and
mapping. This crucial requirement is faced with many challenges in complex and
perceptually-degraded subterranean environments, as the onboard perception
system is required to operate in off-nominal conditions (poor visibility due to
darkness and dust, rugged and muddy terrain, and the presence of self-similar
and ambiguous scenes). In a disaster response scenario and in the absence of
prior information about the environment, robots must rely on noisy sensor data
and perform Simultaneous Localization and Mapping (SLAM) to build a 3D map of
the environment and localize themselves and potential survivors. To that end,
this paper reports on a multi-robot SLAM system developed by team CoSTAR in the
context of the DARPA Subterranean Challenge. We extend our previous work, LAMP,
by incorporating a single-robot front-end interface that is adaptable to
different odometry sources and lidar configurations, a scalable multi-robot
front-end to support inter- and intra-robot loop closure detection for large
scale environments and multi-robot teams, and a robust back-end equipped with
an outlier-resilient pose graph optimization based on Graduated Non-Convexity.
We provide a detailed ablation study on the multi-robot front-end and back-end,
and assess the overall system performance in challenging real-world datasets
collected across mines, power plants, and caves in the United States. We also
release our multi-robot back-end datasets (and the corresponding ground truth),
which can serve as challenging benchmarks for large-scale underground SLAM.",-0.07421363,0.43655652,0.11007514,B
6797,"strategy, described in Section III-C.
      Our ﬁndings in Section V indicate that there is not a
      clear winning solution, which motivates further research        A local planner is used during the navigation to generate
      on autonomous exploration strategies for indoor SAR          forward simulated kinematic trajectories locally on the global
      application scenarios.","We con-         while a sampling-based Rapidly-exploring Random Tree
      sider diverse indoor environments with varying initial       (RRT) [20] was used as our global planner for the exploration
      conditions (e.g., different entry points for the UAV).","map to avoid static and dynamic obstacles based on the cost

   The rest of the paper is structured as follows.",2022-05-27 07:33:30+00:00,Quantitative and Qualitative Assessment of Indoor Exploration Algorithms for Autonomous UAVs,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Adil Farooq'), arxiv.Result.Author('Christos Laoudias'), arxiv.Result.Author('Panayiotis S. Kolios'), arxiv.Result.Author('Theocharis Theocharides')]","Indoor exploration is an important task in disaster relief, emergency
response scenarios, and Search And Rescue (SAR) missions. Unmanned Aerial
Vehicle (UAV) systems can aid first responders by maneuvering autonomously in
areas inside buildings dangerous for humans to traverse, exploring the
interior, and providing an accurate and reliable indoor map before the
emergency response team takes action. Due to the challenging conditions in such
scenarios and the inherent battery limitations and time constraints, we
investigate 2D autonomous exploration strategies (e.g., based on 2D LiDAR) for
mapping 3D indoor environments. First, we introduce a battery consumption model
to consider the battery life aspect for the first time as a critical factor for
evaluating the flight endurance of exploration strategies. Second, we perform
extensive simulation experiments in diverse indoor environments using various
state-of-the-art 2D LiDAR-based exploration strategies. We evaluate our
findings in terms of various quantitative and qualitative performance
indicators, suggesting that these strategies behave differently depending on
the complexity of the environment and initial conditions, e.g., the entry
point.",-0.0013026472,0.24135402,0.10118379,B
6798,"Clearly,
                                                                 these observations motivate further research on autonomous
   The qualitative Mc and Ms results were obtained from          exploration strategies for indoor SAR application scenarios.",thus offering additional time for further exploration.,the spider plots in Fig.,2022-05-27 07:33:30+00:00,Quantitative and Qualitative Assessment of Indoor Exploration Algorithms for Autonomous UAVs,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Adil Farooq'), arxiv.Result.Author('Christos Laoudias'), arxiv.Result.Author('Panayiotis S. Kolios'), arxiv.Result.Author('Theocharis Theocharides')]","Indoor exploration is an important task in disaster relief, emergency
response scenarios, and Search And Rescue (SAR) missions. Unmanned Aerial
Vehicle (UAV) systems can aid first responders by maneuvering autonomously in
areas inside buildings dangerous for humans to traverse, exploring the
interior, and providing an accurate and reliable indoor map before the
emergency response team takes action. Due to the challenging conditions in such
scenarios and the inherent battery limitations and time constraints, we
investigate 2D autonomous exploration strategies (e.g., based on 2D LiDAR) for
mapping 3D indoor environments. First, we introduce a battery consumption model
to consider the battery life aspect for the first time as a critical factor for
evaluating the flight endurance of exploration strategies. Second, we perform
extensive simulation experiments in diverse indoor environments using various
state-of-the-art 2D LiDAR-based exploration strategies. We evaluate our
findings in terms of various quantitative and qualitative performance
indicators, suggesting that these strategies behave differently depending on
the complexity of the environment and initial conditions, e.g., the entry
point.",0.07360489,0.07680619,-0.02065443,B
6897,"E. Ablating System Components in Simulation

                                                                         In simulation we further study the relative importance of
                                                                      the different components of our system.","The ofﬂine policy is trained for 100,000 steps,
                                                                      and is ﬁnetuned online for 1,000 steps after each episode.","In Figure 7, we
                                                                      observe that removing each component of the system hurts
                                                                      performance on both simulated tasks.",2022-05-30 04:52:58+00:00,Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual Imitation Learning,cs.RO,"['cs.RO', 'cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Maximilian Du'), arxiv.Result.Author('Olivia Y. Lee'), arxiv.Result.Author('Suraj Nair'), arxiv.Result.Author('Chelsea Finn')]","Humans are capable of completing a range of challenging manipulation tasks
that require reasoning jointly over modalities such as vision, touch, and
sound. Moreover, many such tasks are partially-observed; for example, taking a
notebook out of a backpack will lead to visual occlusion and require reasoning
over the history of audio or tactile information. While robust tactile sensing
can be costly to capture on robots, microphones near or on a robot's gripper
are a cheap and easy way to acquire audio feedback of contact events, which can
be a surprisingly valuable data source for perception in the absence of vision.
Motivated by the potential for sound to mitigate visual occlusion, we aim to
learn a set of challenging partially-observed manipulation tasks from visual
and audio inputs. Our proposed system learns these tasks by combining offline
imitation learning from a modest number of tele-operated demonstrations and
online finetuning using human provided interventions. In a set of simulated
tasks, we find that our system benefits from using audio, and that by using
online interventions we are able to improve the success rate of offline
imitation learning by ~20%. Finally, we find that our system can complete a set
of challenging, partially-observed tasks on a Franka Emika Panda robot, like
extracting keys from a bag, with a 70% success rate, 50% higher than a policy
that does not use audio.",0.10527547,-0.22923958,-0.042707585,C
6970,"The experiences and learnings during the application of this software stack at the IAC
allowed us to identify crucial further research directions to enable safe autonomy within the future:

Firstly and foremost, the transfer of algorithms and knowledge among diﬀerent domains of autonomy has
to be improved.","Furthermore, by developing a dedicated testing and development
pipeline, we created a robust and advantageous software that is tested in various simulations and real-world
racing competitions.","While autonomous racing with one or two vehicles is a reasonable proving ground, we see
a strong need to increase the complexity of these challenges to align with the problems faced in urban and
highway scenarios.",2022-05-31 17:35:52+00:00,TUM Autonomous Motorsport: An Autonomous Racing Software for the Indy Autonomous Challenge,cs.RO,['cs.RO'],"[arxiv.Result.Author('Johannes Betz'), arxiv.Result.Author('Tobias Betz'), arxiv.Result.Author('Felix Fent'), arxiv.Result.Author('Maximilian Geisslinger'), arxiv.Result.Author('Alexander Heilmeier'), arxiv.Result.Author('Leonhard Hermansdorfer'), arxiv.Result.Author('Thomas Herrmann'), arxiv.Result.Author('Sebastian Huch'), arxiv.Result.Author('Phillip Karle'), arxiv.Result.Author('Markus Lienkamp'), arxiv.Result.Author('Boris Lohmann'), arxiv.Result.Author('Felix Nobis'), arxiv.Result.Author('Levent Ögretmen'), arxiv.Result.Author('Matthias Rowold'), arxiv.Result.Author('Florian Sauerbeck'), arxiv.Result.Author('Tim Stahl'), arxiv.Result.Author('Rainer Trauth'), arxiv.Result.Author('Frederik Werner'), arxiv.Result.Author('Alexander Wischnewski')]","For decades, motorsport has been an incubator for innovations in the
automotive sector and brought forth systems like disk brakes or rearview
mirrors. Autonomous racing series such as Roborace, F1Tenth, or the Indy
Autonomous Challenge (IAC) are envisioned as playing a similar role within the
autonomous vehicle sector, serving as a proving ground for new technology at
the limits of the autonomous systems capabilities. This paper outlines the
software stack and approach of the TUM Autonomous Motorsport team for their
participation in the Indy Autonomous Challenge, which holds two competitions: A
single-vehicle competition on the Indianapolis Motor Speedway and a passing
competition at the Las Vegas Motor Speedway. Nine university teams used an
identical vehicle platform: A modified Indy Lights chassis equipped with
sensors, a computing platform, and actuators. All the teams developed different
algorithms for object detection, localization, planning, prediction, and
control of the race cars. The team from TUM placed first in Indianapolis and
secured second place in Las Vegas. During the final of the passing competition,
the TUM team reached speeds and accelerations close to the limit of the
vehicle, peaking at around 270 km/h and 28 ms2. This paper will present details
of the vehicle hardware platform, the developed algorithms, and the workflow to
test and enhance the software applied during the two-year project. We derive
deep insights into the autonomous vehicle's behavior at high speed and high
acceleration by providing a detailed competition analysis. Based on this, we
deduce a list of lessons learned and provide insights on promising areas of
future work based on the real-world evaluation of the displayed concepts.",0.069636375,0.16712146,-0.015982099,B
7032,"Univer-
tributed to these ideas; however, they can constitute       sal Design principles advocate for designing objects to
good starting points for further research.","This is in line with the idea of using Universal
for them), so only a small subset of participants con-      Design [63] for developing robotic technologies.",serve people across a spectrum of ability.,2022-06-01 21:40:27+00:00,Robots in healthcare as envisioned by care professionals,cs.RO,"['cs.RO', 'cs.CY']","[arxiv.Result.Author('Fran Soljacic'), arxiv.Result.Author('Meia Chita-Tegmark'), arxiv.Result.Author('Theresa Law'), arxiv.Result.Author('Matthias Scheutz')]","As AI-enabled robots enter the realm of healthcare and caregiving, it is
important to consider how they will address the dimensions of care and how they
will interact not just with the direct receivers of assistance, but also with
those who provide it (e.g., caregivers, healthcare providers etc.). Caregiving
in its best form addresses challenges in a multitude of dimensions of a
person's life: from physical, to social-emotional and sometimes even
existential dimensions (such as issues surrounding life and death). In this
study we use semi-structured qualitative interviews administered to healthcare
professions with multidisciplinary backgrounds (physicians, public health
professionals, social workers, and chaplains) to understand their expectations
regarding the possible roles robots may play in the healthcare ecosystem in the
future. We found that participants drew inspiration in their mental models of
robots from both works of science fiction but also from existing commercial
robots. Participants envisioned roles for robots in the full spectrum of care,
from physical to social-emotional and even existential-spiritual dimensions,
but also pointed out numerous limitations that robots have in being able to
provide comprehensive humanistic care. While no dimension of care was deemed as
exclusively the realm of humans, participants stressed the importance of
caregiving humans as the primary providers of comprehensive care, with robots
assisting with more narrowly focused tasks. Throughout the paper we point out
the encouraging confluence of ideas between the expectations of healthcare
providers and research trends in the human-robot interaction (HRI) literature.",-0.18530568,-0.10062726,-0.063217476,A
7215,Our dataset will be made public to help further research in this area.,"We validated DriveIRL on the Las Vegas Strip
                                       and demonstrated fully autonomous driving in heavy trafﬁc, including scenarios
                                       involving cut-ins, abrupt braking by the lead vehicle, and hotel pickup/dropoff
                                       zones.","1 Introduction

                                       Self-driving cars have been the focus of signiﬁcant research and development over the past decade.",2022-06-07 04:36:46+00:00,Driving in Real Life with Inverse Reinforcement Learning,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'I.2.6; I.2.9']","[arxiv.Result.Author('Tung Phan-Minh'), arxiv.Result.Author('Forbes Howington'), arxiv.Result.Author('Ting-Sheng Chu'), arxiv.Result.Author('Sang Uk Lee'), arxiv.Result.Author('Momchil S. Tomov'), arxiv.Result.Author('Nanxiang Li'), arxiv.Result.Author('Caglayan Dicle'), arxiv.Result.Author('Samuel Findler'), arxiv.Result.Author('Francisco Suarez-Ruiz'), arxiv.Result.Author('Robert Beaudoin'), arxiv.Result.Author('Bo Yang'), arxiv.Result.Author('Sammy Omari'), arxiv.Result.Author('Eric M. Wolff')]","In this paper, we introduce the first learning-based planner to drive a car
in dense, urban traffic using Inverse Reinforcement Learning (IRL). Our
planner, DriveIRL, generates a diverse set of trajectory proposals, filters
these trajectories with a lightweight and interpretable safety filter, and then
uses a learned model to score each remaining trajectory. The best trajectory is
then tracked by the low-level controller of our self-driving vehicle. We train
our trajectory scoring model on a 500+ hour real-world dataset of expert
driving demonstrations in Las Vegas within the maximum entropy IRL framework.
DriveIRL's benefits include: a simple design due to only learning the
trajectory scoring function, relatively interpretable features, and strong
real-world performance. We validated DriveIRL on the Las Vegas Strip and
demonstrated fully autonomous driving in heavy traffic, including scenarios
involving cut-ins, abrupt braking by the lead vehicle, and hotel pickup/dropoff
zones. Our dataset will be made public to help further research in this area.",0.09021689,0.14870149,-0.045789417,B
7282,"However, there are many
problems that deserve further research in the future.","Illustrative examples are given to
demonstrate the effectiveness of the presented procedure accordingly.",(1) Data set.,2022-06-08 11:14:13+00:00,A Control with Patterns Approach for the Stability Issue of Dynamical Systems Operating in Data-Rich Environments,cs.RO,['cs.RO'],"[arxiv.Result.Author('Quan Quan'), arxiv.Result.Author('Kai-Yuan Cai')]","Nowadays, data are richly accessible to accumulate, and the increasingly
powerful capability with computing offers reasonable ease of handling big data.
This remarkable scenario leads to a new way for solving some control problems
which was previously hard to analyze and solve. In this paper, a new type of
control methods, namely control with patterns (CWP), is proposed to handle data
sets corresponding to nonlinear dynamical systems subject to a discrete control
constraint set. For data sets of this kind, a new definition, namely
exponential attraction on data sets, is proposed to describe nonlinear
dynamical systems under consideration. Based on the data sets and parameterized
Lyapunov functions, the problem for exponential attraction on data sets is
converted to a pattern classification one. Furthermore, the controller design
is proposed accordingly, where the pattern classification function is used to
decide which control element in the control set should be employed.
Illustrative examples are given to show the effectiveness of the proposed CWP.",0.28387895,-0.3433534,-0.21800631,C
7468,"Thus, further research on this method can         polygon-shaped buildings [63].","the best of the author’s knowledge, the RoadTracer is the first  This method has been used to predict road network graphs,
work that uses the iterative graph growing method to generate    including line segments [61], line-shaped objects [62], and
road networks.",Table IV.,2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving: A Review,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.27024192,0.065159895,0.044118308,B
7469,"The architecture adopts the SSD [69] method
top-down method requires further research.","Thus, road marking extraction with the              head stage input.","16

to compute prediction in the last stage.",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving: A Review,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.31753457,0.15899733,-0.08552147,B
7470,"Thus, further research on improving the pole-like
object extraction performance on imperfect data is required.",point clouds.,"A Lanelet2 map contains three layers: the physical, the
                                                                                     relational, and the topological layer, as shown in Fig.",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving: A Review,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.21456166,0.38926086,-0.072829634,B
7471,"Available:
and individual researchers, which requires further research and             www.here.com.",[Online].,"conclusion                                                         [9] J. Houston et al., “One Thousand and One
                                                                            Hours: Self-driving Motion Prediction
   In this review, recent HD map generation technologies for                Dataset,” Jun.",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving: A Review,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.13732737,0.35927016,-0.11650547,B
7472,"“Vision meets robotics: The KITTI dataset,”
                                                                            International Journal of Robotics Research,
   Some challenging issues for further research and                         vol.","Some useful tools for converting map formats among three           [12] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun,
frameworks were also provided.","32, no.",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving: A Review,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",-0.03683164,0.43595284,-0.14097942,B
7473,"Thus, further research on this method can
process, aiming to solve the poor performance caused by            increase the road network generation efficiency for large-scale
occlusion.",The RoadTracer utilizes an iterative graph construction      road networks.,The RoadTracer has a search algorithm guided by a       road maps.,2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.36607894,0.29247552,0.0063393004,B
7474,"Thus, road marking extraction with the
             AerialLanes18                     0.6003    0.7502     top-down method requires further research.","To the author’s
             AerialLanes18                     0.5459    0.7062     best knowledge, there are not a lot of approaches for road
SegCaps                      0.7158    0.7347                       marking extraction utilizing the top-down method compared to
  [104]      UAVMark20       0.7416    0.7591  0.5688    0.7251     the bottom-up method.","SA-
CapsFPN                                                             C. Pole-like Objects Extraction

  [105]

b) Top-down Method

The top-down method uses the existing object detection                                          TABLE IX.",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.29022694,0.2001104,0.072533965,B
7475,"Thus, further research on improving the pole-like
                                                                   object extraction performance on imperfect data is required.","Pole-like
                                                                   object extractions are mainly done on 3D point clouds, thus the
                                                                   performance of the extraction also depends on the quality of the
                                                                   point clouds.","V. FRAMEWORK FOR HD MAPS

                                                                      With the increasing complexity of the HD maps and the
                                                                   number of environmental features that require extraction, it is
                                                                   necessary to have good software in the form of a framework to
                                                                   sufficiently store the relevant information in a map and ensure
                                                                   a consistent view of the map [131].",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.18188027,0.40976414,0.03255557,B
7476,"Lanelet2 map can             individual researchers, which requires further research and
also be converted to OpenDRIVE map format.","This is still an open problem for academic and
the OpenDRIVE format to Apollo format.","Carla, an open-          conclusion
source simulator for autonomous driving, provides a
PythonAPI for converting the OSM map to the OpenDRIVE                                          VII.",2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.2605837,0.25037998,-0.00042788265,B
7477,"However, it still lacks depth information, which is extremely          Some challenging issues for further research and
important when the ego vehicle drives around obstacles.",collected GPS data and adding the corresponding altitude.,2D HD       development are 1.,2022-06-11 02:32:11+00:00,High-Definition Map Generation Technologies For Autonomous Driving,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Zhibin Bao'), arxiv.Result.Author('Sabir Hossain'), arxiv.Result.Author('Haoxiang Lang'), arxiv.Result.Author('Xianke Lin')]","Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.",0.12197687,0.27404845,0.017997963,B
7676,"Each of these ﬁelds, was evaluated
   In order to further study how do our deﬁned gestures            by every participant using a questionnaire to ﬁll out after the
interfere with each other, we create a confusion matrix (see       experiment, see Table I, based on [kirby2010social].","perceptions of the sociability, naturalness, security and
                                                                   comfort characteristics.",Fig.,2022-06-15 13:49:22+00:00,Body Gesture Recognition to Control a Social Robot,cs.RO,"['cs.RO', 'cs.CV', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Javier Laplaza'), arxiv.Result.Author('Joan Jaume Oliver'), arxiv.Result.Author('Ramón Romero'), arxiv.Result.Author('Alberto Sanfeliu'), arxiv.Result.Author('Anaís Garrell')]","In this work, we propose a gesture based language to allow humans to interact
with robots using their body in a natural way. We have created a new gesture
detection model using neural networks and a custom dataset of humans performing
a set of body gestures to train our network. Furthermore, we compare body
gesture communication with other communication channels to acknowledge the
importance of adding this knowledge to robots. The presented approach is
extensively validated in diverse simulations and real-life experiments with
non-trained volunteers. This attains remarkable results and shows that it is a
valuable framework for social robotics applications, such as human robot
collaboration or human-robot interaction.",-0.12612917,-0.3346126,-0.17385736,A
7716,"Finally, relevant datasets are
referenced in order to prompt further research effort in the S&R area.",A summary of systems deployed in previous circuits of DARPA SubT follows.,"2.1 Degraded sensing

Perception in subterranean environments faces constant degradation of the sensor outputs due to the harsh conditions
of such places.",2022-06-16 13:54:33+00:00,UAVs Beneath the Surface: Cooperative Autonomy for Subterranean Search and Rescue in DARPA SubT,cs.RO,"['cs.RO', 'cs.AI', 'cs.MA', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Matej Petrlik'), arxiv.Result.Author('Pavel Petracek'), arxiv.Result.Author('Vit Kratky'), arxiv.Result.Author('Tomas Musil'), arxiv.Result.Author('Yurii Stasinchuk'), arxiv.Result.Author('Matous Vrba'), arxiv.Result.Author('Tomas Baca'), arxiv.Result.Author('Daniel Hert'), arxiv.Result.Author('Martin Pecka'), arxiv.Result.Author('Tomas Svoboda'), arxiv.Result.Author('Martin Saska')]","This paper presents a novel approach for autonomous cooperating UAVs in
search and rescue operations in subterranean domains with complex topology. The
proposed system was ranked second in the Virtual Track of the DARPA SubT Finals
as part of the team CTU-CRAS-NORLAB. In contrast to the winning solution that
was developed specifically for the Virtual Track, the proposed solution also
proved to be a robust system for deployment onboard physical UAVs flying in the
extremely harsh and confined environment of the real-world competition. The
proposed approach enables fully autonomous and decentralized deployment of a
UAV team with seamless simulation-to-world transfer, and proves its advantage
over less mobile UGV teams in the flyable space of diverse environments. The
main contributions of the paper are present in the mapping and navigation
pipelines. The mapping approach employs novel map representations -- SphereMap
for efficient risk-aware long-distance planning, FacetMap for surface coverage,
and the compressed topological-volumetric LTVMap for allowing multi-robot
cooperation under low-bandwidth communication. These representations are used
in navigation together with novel methods for visibility-constrained informed
search in a general 3D environment with no assumptions about the environment
structure, while balancing deep exploration with sensor-coverage exploitation.
The proposed solution also includes a visual-perception pipeline for on-board
detection and localization of objects of interest in four RGB stream at 5 Hz
each without a dedicated GPU. Apart from participation in the DARPA SubT, the
performance of the UAV system is supported by extensive experimental
verification in diverse environments with both qualitative and quantitative
evaluation.",0.21854913,0.020469718,-0.06480607,B
7733,"For further research in this area, we propose VLMbench, which includes various VLM tasks, and
AMSolver, used for automatic VLM task generation.","7 Discussion and Future Work

Vision-and-Language Manipulation (VLM) tasks are essential since they are inevitable for embodied
AI.","In addition, we test the 6D-CLIPort agent, a
keypoint-based 6 DoF agent, on the benchmark.",2022-06-17 03:07:18+00:00,VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation,cs.RO,"['cs.RO', 'cs.CL', 'cs.CV']","[arxiv.Result.Author('Kaizhi Zheng'), arxiv.Result.Author('Xiaotong Chen'), arxiv.Result.Author('Odest Chadwicke Jenkins'), arxiv.Result.Author('Xin Eric Wang')]","Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., ""move the red mug next to the box while keeping it upright."" To
this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator
and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.",-0.0663231,0.118436895,-0.22183779,A
7734,"We hope this
information will be helpful for further research in this area.","“Move to [relative position] [manipulated object]; Grasp [manipulated
object]"" correspond to the pre-grasp action and grasp action in the Control unit task.","B.3 6D-CLIPort

In this section, we provide hyperparameter values in our 6D-CLIPort training.",2022-06-17 03:07:18+00:00,VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation,cs.RO,"['cs.RO', 'cs.CL', 'cs.CV']","[arxiv.Result.Author('Kaizhi Zheng'), arxiv.Result.Author('Xiaotong Chen'), arxiv.Result.Author('Odest Chadwicke Jenkins'), arxiv.Result.Author('Xin Eric Wang')]","Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., ""move the red mug next to the box while keeping it upright."" To
this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator
and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.",-0.2905729,-0.083294146,-0.14430033,A
7735,"For further research in this area, we propose VLMbench, which includes various VLM tasks, and
AMSolver, used for automatic VLM task generation.","More results and analysis can be found in Appendix D.

7 Conclusion and Future Work

Vision-and-Language Manipulation (VLM) tasks are essential since they are inevitable for embodied
AI.","In addition, we test the 6D-CLIPort agent, a
keypoint-based 6 DoF agent, on the benchmark.",2022-06-17 03:07:18+00:00,VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation,cs.RO,"['cs.RO', 'cs.CL', 'cs.CV']","[arxiv.Result.Author('Kaizhi Zheng'), arxiv.Result.Author('Xiaotong Chen'), arxiv.Result.Author('Odest Chadwicke Jenkins'), arxiv.Result.Author('Xin Eric Wang')]","Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., ""move the red mug next to the box while keeping it upright."" To
this end, we introduce an Automatic Manipulation Solver (AMSolver) system and
build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.",-0.06850813,0.10453587,-0.20407325,A
7736,"We hope this
information will be helpful for further research in this area.","“Move to [relative position] [manipulated object]; Grasp [manipulated
object]"" correspond to the pre-grasp action and grasp action in the Control unit task.","B.3 6D-CLIPort

In this section, we provide hyperparameter values in our 6D-CLIPort training.",2022-06-17 03:07:18+00:00,VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation,cs.RO,"['cs.RO', 'cs.CL', 'cs.CV']","[arxiv.Result.Author('Kaizhi Zheng'), arxiv.Result.Author('Xiaotong Chen'), arxiv.Result.Author('Odest Chadwicke Jenkins'), arxiv.Result.Author('Xin Eric Wang')]","Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., ""move the red mug next to the box while keeping it upright."" To
this end, we introduce an Automatic Manipulation Solver (AMSolver) system and
build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.",-0.2905729,-0.083294146,-0.14430033,A
8026,"Hence, further research
per image.","We believe this to be due to network
                                                                              backbone used, that uses a simple bilinear upsampling to
TABLE IV: AUC for varying number of correspondences sampled                   produce the dense descriptor image.","NC denotes the number of non correspondences per                   should investigate and focus on improving the stability of
correspondence for pixelwise loss.",2022-06-24 08:24:42+00:00,Efficient and Robust Training of Dense Object Nets for Multi-Object Robot Manipulation,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('David B. Adrian'), arxiv.Result.Author('Andras Gabor Kupcsik'), arxiv.Result.Author('Markus Spies'), arxiv.Result.Author('Heiko Neumann')]","We propose a framework for robust and efficient training of Dense Object Nets
(DON) with a focus on multi-object robot manipulation scenarios. DON is a
popular approach to obtain dense, view-invariant object descriptors, which can
be used for a multitude of downstream tasks in robot manipulation, such as,
pose estimation, state representation for control, etc.. However, the original
work focused training on singulated objects, with limited results on
instance-specific, multi-object applications. Additionally, a complex data
collection pipeline, including 3D reconstruction and mask annotation of each
object, is required for training. In this paper, we further improve the
efficacy of DON with a simplified data collection and training regime, that
consistently yields higher precision and enables robust tracking of keypoints
with less data requirements. In particular, we focus on training with
multi-object data instead of singulated objects, combined with a well-chosen
augmentation scheme. We additionally propose an alternative loss formulation to
the original pixelwise formulation that offers better results and is less
sensitive to hyperparameters. Finally, we demonstrate the robustness and
accuracy of our proposed framework on a real-world robotic grasping task.",0.19800809,0.24595043,-0.23574758,B
8145,"A GitHub repository containing our code and dataset
rors in the navigation process and Liu, Wang, Li, Hou, Zhu             as a benchmark and to encourage further research in
and Wang (2022) demonstrated an INS/DVL integration                    the ﬁeld.","tion model that, unlike standard navigation algorithms, does
not require a motion model in order to avoid modeling er-          3.","method based on a radial basis function neural network for
current compensation.",2022-06-27 19:38:38+00:00,BeamsNet: A data-driven Approach Enhancing Doppler Velocity Log Measurements for Autonomous Underwater Vehicle Navigation,cs.RO,"['cs.RO', 'cs.LG', 'cs.SY', 'eess.SP', 'eess.SY']","[arxiv.Result.Author('Nadav Cohen'), arxiv.Result.Author('Itzik Klein')]","Autonomous underwater vehicles (AUV) perform various applications such as
seafloor mapping and underwater structure health monitoring. Commonly, an
inertial navigation system aided by a Doppler velocity log (DVL) is used to
provide the vehicle's navigation solution. In such fusion, the DVL provides the
velocity vector of the AUV, which determines the navigation solution's accuracy
and helps estimate the navigation states. This paper proposes BeamsNet, an
end-to-end deep learning framework to regress the estimated DVL velocity vector
that improves the accuracy of the velocity vector estimate, and could replace
the model-based approach. Two versions of BeamsNet, differing in their input to
the network, are suggested. The first uses the current DVL beam measurements
and inertial sensors data, while the other utilizes only DVL data, taking the
current and past DVL measurements for the regression process. Both simulation
and sea experiments were made to validate the proposed learning approach
relative to the model-based approach. Sea experiments were made with the Snapir
AUV in the Mediterranean Sea, collecting approximately four hours of DVL and
inertial sensor data. Our results show that the proposed approach achieved an
improvement of more than 60% in estimating the DVL velocity vector.",0.061807167,0.10708829,0.0028121693,B
8524,"We further study the viability of aerial manipulation of such objects from
the end-effector precision perspective and use the proposed real-time dynamic
manipulability polytope computation methods to analyze the feasibility of the
wire manipulation task.","We provide a deformable one-dimensional objects detection
method suitable for manipulation tasks and introduce a new perspective on
planning for manipulating these objects in speciﬁc settings such as utility cable
boxes.","This goal of this work is to improve the state-of-the-art in physical interaction
      and manipulation using aerial robots by providing faster integration for fully-
      actuated robots, better estimation of available wrenches and new solutions to
      allow real-world physical interaction with rigid and deformable objects.",2022-07-06 13:15:10+00:00,Physical Interaction and Manipulation of the Environment using Aerial Robots,cs.RO,"['cs.RO', 'cs.CV', 'cs.SY', 'eess.SY']",[arxiv.Result.Author('Azarakhsh Keipour')],"The physical interaction of aerial robots with their environment has
countless potential applications and is an emerging area with many open
challenges. Fully-actuated multirotors have been introduced to tackle some of
these challenges. They provide complete control over position and orientation
and eliminate the need for attaching a multi-DoF manipulation arm to the robot.
However, there are many open problems before they can be used in real-world
applications. Researchers have introduced some methods for physical interaction
in limited settings. Their experiments primarily use prototype-level software
without an efficient path to integration with real-world applications. We
describe a new cost-effective solution for integrating these robots with the
existing software and hardware flight systems for real-world applications and
expand it to physical interaction applications. On the other hand, the existing
control approaches for fully-actuated robots assume conservative limits for the
thrusts and moments available to the robot. Using conservative assumptions for
these already-inefficient robots makes their interactions even less optimal and
may even result in many feasible physical interaction applications becoming
infeasible. This work proposes a real-time method for estimating the complete
set of instantaneously available forces and moments that robots can use to
optimize their physical interaction performance. Finally, many real-world
applications where aerial robots can improve the existing manual solutions deal
with deformable objects. However, the perception and planning for their
manipulation is still challenging. This research explores how aerial physical
interaction can be extended to deformable objects. It provides a detection
method suitable for manipulating deformable one-dimensional objects and
introduces a new perspective on planning the manipulation of these objects.",-0.33286828,0.10863106,0.17687798,A
8525,"Finally, Chapter 8 provides a summary of the work and outlines the possible
further research.","It further explores the feasibility of such manipulation tasks from
different perspectives for aerial robots.","1.5 Bibliographical Remarks

This thesis only contains the work and research where this author is the primary
contributor:

– Chapter 3 is the work done with Sebastian Scherer, Mohammadreza Mousaei,
   and Junyi Geng, with contributions from John Keller, Andrew Saba, Andrew
   Ashley, Greg Armstrong, Dongwei (Saeed) Bai, and Near Earth Autonomy.",2022-07-06 13:15:10+00:00,Physical Interaction and Manipulation of the Environment using Aerial Robots,cs.RO,"['cs.RO', 'cs.CV', 'cs.SY', 'eess.SY']",[arxiv.Result.Author('Azarakhsh Keipour')],"The physical interaction of aerial robots with their environment has
countless potential applications and is an emerging area with many open
challenges. Fully-actuated multirotors have been introduced to tackle some of
these challenges. They provide complete control over position and orientation
and eliminate the need for attaching a multi-DoF manipulation arm to the robot.
However, there are many open problems before they can be used in real-world
applications. Researchers have introduced some methods for physical interaction
in limited settings. Their experiments primarily use prototype-level software
without an efficient path to integration with real-world applications. We
describe a new cost-effective solution for integrating these robots with the
existing software and hardware flight systems for real-world applications and
expand it to physical interaction applications. On the other hand, the existing
control approaches for fully-actuated robots assume conservative limits for the
thrusts and moments available to the robot. Using conservative assumptions for
these already-inefficient robots makes their interactions even less optimal and
may even result in many feasible physical interaction applications becoming
infeasible. This work proposes a real-time method for estimating the complete
set of instantaneously available forces and moments that robots can use to
optimize their physical interaction performance. Finally, many real-world
applications where aerial robots can improve the existing manual solutions deal
with deformable objects. However, the perception and planning for their
manipulation is still challenging. This research explores how aerial physical
interaction can be extended to deformable objects. It provides a detection
method suitable for manipulating deformable one-dimensional objects and
introduces a new perspective on planning the manipulation of these objects.",-0.31058687,0.12890622,0.18915962,A
8592,"These results lay the foundation for
further research on the modeling and control of TSA-driven
soft ﬁngers.","Secondly, the results re-
vealed the nonlinear mapping between the strings’ twists and
the bending of the ﬁngers.","1https://github.com/EmDash00/ArduinoLogger
6       THIS PAPER IS CURRENTLY UNDER REVISION IN IEEE TRANSACTIONS ON ROBOTICS

   (a)  (b)  (c)

   (d)  (e)  (f)

   (g)  (h)  (i)

   (j)  (k)  (l)

Fig.",2022-07-07 22:15:04+00:00,Anthropomorphic Twisted String-Actuated Soft Robotic Gripper with Tendon-Based Stiffening,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('David Bombara'), arxiv.Result.Author('Revanth Konda'), arxiv.Result.Author('Steven Swanbeck'), arxiv.Result.Author('Jun Zhang')]","Realizing high-performance soft robotic grippers is challenging because of
the inherent limitations of the soft actuators and artificial muscles that
drive them, including low force output, small actuation range, and poor
compactness. Despite advances in this area, realizing compact soft grippers
with high dexterity and force output is still challenging. This paper explores
twisted string actuators (TSAs) to drive a soft robotic gripper. TSAs have been
used in numerous robotic applications, but their inclusion in soft robots has
been limited. The proposed design of the gripper was inspired by the human
hand. Tunable stiffness was implemented in the fingers with antagonistic TSAs.
The fingers' bending angles, actuation speed, blocked force output, and
stiffness tuning were experimentally characterized. The gripper achieved a
score of 6 on the Kapandji test and recreated 31 of the 33 grasps of the Feix
GRASP taxonomy. It exhibited a maximum grasping force of 72 N, which was almost
13 times its own weight. A comparison study revealed that the proposed gripper
exhibited equivalent or superior performance compared to other similar soft
grippers.",-0.24129108,-0.091212906,0.36865017,A
8593,"These results lay the foundation for
further research on the modeling and control of TSA-driven
soft ﬁngers.","Secondly, the results re-
vealed the nonlinear mapping between the strings’ twists and
the bending of the ﬁngers.","1https://github.com/EmDash00/ArduinoLogger
6       THIS PAPER IS CURRENTLY UNDER REVISION IN IEEE TRANSACTIONS ON ROBOTICS

   (a)  (b)  (c)

   (d)  (e)  (f)

   (g)  (h)  (i)

   (j)  (k)  (l)

Fig.",2022-07-07 22:15:04+00:00,Anthropomorphic Twisted String-Actuated Soft Robotic Gripper with Tendon-Based Stiffening,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('David Bombara'), arxiv.Result.Author('Revanth Konda'), arxiv.Result.Author('Steven Swanbeck'), arxiv.Result.Author('Jun Zhang')]","Realizing high-performance soft robotic grippers is challenging because of
the inherent limitations of the soft actuators and artificial muscles that
drive them, including low force output, small actuation range, and poor
compactness. Despite advances in this area, realizing compact soft grippers
with high dexterity and force output is still challenging. This paper explores
twisted string actuators (TSAs) to drive a soft robotic gripper. TSAs have been
used in numerous robotic applications, but their inclusion in soft robots has
been limited. The proposed design of the gripper was inspired by the human
hand. Tunable stiffness was implemented in the fingers with antagonistic TSAs.
The fingers' bending angles, actuation speed, blocked force output, and
stiffness tuning were experimentally characterized. The gripper achieved a
score of 6 on the Kapandji test and recreated 31 of the 33 grasps of the Feix
GRASP taxonomy. It exhibited a maximum grasping force of 72 N, which was almost
13 times its own weight. A comparison study revealed that the proposed gripper
exhibited equivalent or superior performance compared to other similar soft
grippers.",-0.24129108,-0.091212906,0.36865017,A
8596,"iteration in the alternating optimization algorithm represents
different computation costs compared to the other two meth-         To further study the effect of the behavior parameter, in the
ods.","Consequently, one         decided to yield the ego agent even with a higher velocity.","For a fair comparison, we only draw the reward of the       third experiment, both agents were set as egoistic drivers, but
ﬁnal trajectory generated by the alternating optimization al-    the ego agent was more egoistic than the opponent.",2022-07-08 03:25:50+00:00,Efficient Game-Theoretic Planning with Prediction Heuristic for Socially-Compliant Autonomous Driving,cs.RO,['cs.RO'],"[arxiv.Result.Author('Chenran Li'), arxiv.Result.Author('Tu Trinh'), arxiv.Result.Author('Letian Wang'), arxiv.Result.Author('Changliu Liu'), arxiv.Result.Author('Masayoshi Tomizuka'), arxiv.Result.Author('Wei Zhan')]","Planning under social interactions with other agents is an essential problem
for autonomous driving. As the actions of the autonomous vehicle in the
interactions affect and are also affected by other agents, autonomous vehicles
need to efficiently infer the reaction of the other agents. Most existing
approaches formulate the problem as a generalized Nash equilibrium problem
solved by optimization-based methods. However, they demand too much
computational resource and easily fall into the local minimum due to the
non-convexity. Monte Carlo Tree Search (MCTS) successfully tackles such issues
in game-theoretic problems. However, as the interaction game tree grows
exponentially, the general MCTS still requires a huge amount of iterations to
reach the optima. In this paper, we introduce an efficient game-theoretic
trajectory planning algorithm based on general MCTS by incorporating a
prediction algorithm as a heuristic. On top of it, a social-compliant reward
and a Bayesian inference algorithm are designed to generate diverse driving
behaviors and identify the other driver's driving preference. Results
demonstrate the effectiveness of the proposed framework with datasets
containing naturalistic driving behavior in highly interactive scenarios.",0.03917993,-0.26922727,0.064393565,C
8901,"The cart-pole pendulum’s low-dimensional dynamics                   motor summons rate-dependent control constraints which
                                        eases its analysis while its simple design reduces cost and                 may motivate further research on optimization-based control
                                        maintenance.","In turn, actuating a reaction wheel with an electric
                                        sition.","Yet, for systems with more DOF, numerous                       [6].",2022-07-14 15:16:46+00:00,The Wheelbot: A Jumping Reaction Wheel Unicycle,cs.RO,['cs.RO'],"[arxiv.Result.Author('A. René Geist'), arxiv.Result.Author('Jonathan Fiene'), arxiv.Result.Author('Naomi Tashiro'), arxiv.Result.Author('Zheng Jia'), arxiv.Result.Author('Sebastian Trimpe')]","Combining off-the-shelf components with 3Dprinting, the Wheelbot is a
symmetric reaction wheel unicycle that can jump onto its wheels from any
initial position. With non-holonomic and under-actuated dynamics, as well as
two coupled unstable degrees of freedom, the Wheelbot provides a challenging
platform for nonlinear and data-driven control research. This paper presents
the Wheelbot's mechanical and electrical design, its estimation and control
algorithms, as well as experiments demonstrating both self-erection and
disturbance rejection while balancing.",-0.21964422,-0.29098308,0.35629436,A
8979,"They further study the inconsistency of EKF-based
      obvious improvement in estimation accuracy.","[23] [24] proved that visual-inertial esti-
                                                                  mator has four unobservable directions which correspond
   • A FEJ-VIRO framework is proposed which extends               to the global translation and the global rotation about the
      the FEJ technique to maintain consistency, leading to       gravity.","VIO and propose a ﬁrst-estimated Jacobian technique to
                                                                  address the inconsistency issue.",2022-07-17 16:18:22+00:00,FEJ-VIRO: A Consistent First-Estimate Jacobian Visual-Inertial-Ranging Odometry,cs.RO,['cs.RO'],"[arxiv.Result.Author('Shenhan Jia'), arxiv.Result.Author('Yanmei Jiao'), arxiv.Result.Author('Zhuqing Zhang'), arxiv.Result.Author('Rong Xiong'), arxiv.Result.Author('Yue Wang')]","In recent years, Visual-Inertial Odometry (VIO) has achieved many significant
progresses. However, VIO methods suffer from localization drift over long
trajectories. In this paper, we propose a First-Estimates Jacobian
Visual-Inertial-Ranging Odometry (FEJ-VIRO) to reduce the localization drifts
of VIO by incorporating ultra-wideband (UWB) ranging measurements into the VIO
framework \textit{consistently}. Considering that the initial positions of UWB
anchors are usually unavailable, we propose a long-short window structure to
initialize the UWB anchors' positions as well as the covariance for state
augmentation. After initialization, the FEJ-VIRO estimates the UWB anchors'
positions simultaneously along with the robot poses. We further analyze the
observability of the visual-inertial-ranging estimators and proved that there
are \textit{four} unobservable directions in the ideal case, while one of them
vanishes in the actual case due to the gain of spurious information. Based on
these analyses, we leverage the FEJ technique to enforce the unobservable
directions, hence reducing inconsistency of the estimator. Finally, we validate
our analysis and evaluate the proposed FEJ-VIRO with both simulation and
real-world experiments.",0.0016774107,0.115304396,0.05982101,B
8988,"A two-fold increase in the swimming speed of
the pneumatic fish compared to the reference is observed and the further study of the untether fish
demonstrates a record-breaking velocity of 2.03 BL/s (43.6 cm/s) for the untethered compliant
swimmer, outperforming the previously report fastest one with a significant margin of 194%.","Designing rules are
proposed following the theories and verification.","This
work probably heralds a structural revolution for next-generation compliant robotics.",2022-07-18 02:05:28+00:00,In-plane prestressed hair clip mechanism for the fastest untethered compliant fish robot,cs.RO,['cs.RO'],"[arxiv.Result.Author('Zechen Xiong'), arxiv.Result.Author('Liqi Chen'), arxiv.Result.Author('Wenxiong Hao'), arxiv.Result.Author('Pengfei Yang'), arxiv.Result.Author('Shicheng Wang'), arxiv.Result.Author('Sarah Li Wilkinson'), arxiv.Result.Author('Yufeng Su'), arxiv.Result.Author('Xiangyi Ren'), arxiv.Result.Author('Nipun Poddar'), arxiv.Result.Author('Xi Chen'), arxiv.Result.Author('Hod Lipson')]","A trend has emerged over the past decades pointing to the harnessing of
structural instability in movable, programmable, and transformable mechanisms.
Inspired by a steel hair clip, we combine the in-plane assembly with a bistable
structure and build a compliant flapping mechanism using semi-rigid plastic
sheets and installed it on both a tethered pneumatic soft robotic fish and an
untethered motor-driven one to demonstrate its unprecedented advantages.
Designing rules are proposed following the theories and verification. A
two-fold increase in the swimming speed of the pneumatic fish compared to the
reference is observed and the further study of the untether fish demonstrates a
record-breaking velocity of 2.03 BL/s (43.6 cm/s) for the untethered compliant
swimmer, outperforming the previously report fastest one with a significant
margin of 194%. This work probably heralds a structural revolution for
next-generation compliant robotics.",-0.2951407,-0.132651,0.23421243,A
9096,"We will highlight the need to further study the integration of several deliberation functions besides acting,
that are necessary for intelligent behavior.","We put forward the case for a generative model of cognitive architectures, that is, a formal
description captured in one or more domain speciﬁc modeling languages (DSMLs) enabling automatic
synthesis of concrete architectures for LTRA as a satisfying solution to a set of traceable requirements.","In addition, we consider that cognitive penetrability and
meta-reasoning models should be explicit and formal, amenable to theoretical analysis, veriﬁcation,
and automatic transformations.",2022-07-20 07:27:23+00:00,The Need for a Meta-Architecture for Robot Autonomy,cs.RO,"['cs.RO', 'cs.AI', 'Robotics']","[arxiv.Result.Author('Stalin Muñoz Gutiérrez'), arxiv.Result.Author('Gerald Steinbauer-Wagner')]","Long-term autonomy of robotic systems implicitly requires dependable
platforms that are able to naturally handle hardware and software faults,
problems in behaviors, or lack of knowledge. Model-based dependable platforms
additionally require the application of rigorous methodologies during the
system development, including the use of correct-by-construction techniques to
implement robot behaviors. As the level of autonomy in robots increases, so do
the cost of offering guarantees about the dependability of the system.
Certifiable dependability of autonomous robots, we argue, can benefit from
formal models of the integration of several cognitive functions, knowledge
processing, reasoning, and meta-reasoning. Here we put forward the case for a
generative model of cognitive architectures for autonomous robotic agents that
subscribes to the principles of model-based engineering and certifiable
dependability, autonomic computing, and knowledge-enabled robotics.",0.18712819,-0.15970768,-0.17383489,C
9104,"With a multitude of new methods proposed            Import script for          Dataset import     annotation framerate,
                                        by different communities, the lack of standardized benchmarks
                                        and objective comparisons is increasingly becoming a major                 new datasets 1            Preprocessing      obstacle and semantic
                                        limitation to assess progress and guide further research.","                                                 The Atlas Benchmark: an Automated Evaluation Framework
                                                                      for Human Motion Prediction

                                                 Andrey Rudenko1,3, Luigi Palmieri1, Wanting Huang1,2, Achim J. Lilienthal3 and Kai O. Arras1

                                           Abstract— Human motion trajectory prediction, an essential             Datasets library:                             Dataset features:
                                        task for autonomous systems in many domains, has been on the              ETH, ATC, THÖR
                                        rise in recent years.","Ex-
                                        isting benchmarks are limited in their scope and ﬂexibility to                Automated                 Prediction   2  map, goals
                                        conduct relevant experiments and to account for contextual cues               paramater
arXiv:2207.09830v1 [cs.RO] 20 Jul 2022  of agents and environments.",2022-07-20 11:33:12+00:00,The Atlas Benchmark: an Automated Evaluation Framework for Human Motion Prediction,cs.RO,['cs.RO'],"[arxiv.Result.Author('Andrey Rudenko'), arxiv.Result.Author('Luigi Palmieri'), arxiv.Result.Author('Wanting Huang'), arxiv.Result.Author('Achim J. Lilienthal'), arxiv.Result.Author('Kai O. Arras')]","Human motion trajectory prediction, an essential task for autonomous systems
in many domains, has been on the rise in recent years. With a multitude of new
methods proposed by different communities, the lack of standardized benchmarks
and objective comparisons is increasingly becoming a major limitation to assess
progress and guide further research. Existing benchmarks are limited in their
scope and flexibility to conduct relevant experiments and to account for
contextual cues of agents and environments. In this paper we present Atlas, a
benchmark to systematically evaluate human motion trajectory prediction
algorithms in a unified framework. Atlas offers data preprocessing functions,
hyperparameter optimization, comes with popular datasets and has the
flexibility to setup and conduct underexplored yet relevant experiments to
analyze a method's accuracy and robustness. In an example application of Atlas,
we compare five popular model- and learning-based predictors and find that,
when properly applied, early physics-based approaches are still remarkably
competitive. Such results confirm the necessity of benchmarks like Atlas.",0.068748996,0.20280257,-0.26493615,B
9105,"8, justiﬁes                     preprocessing functions, hyperparameter optimization, three
the need for further research into interaction models, both                    popular datasets and the fexibility to setup and conduct
engineered and learned ones.","That, and the considerable runtime differences                 Unlike related benchmarks and challenges, Atlas offers data
in favor of the model-based approaches in Fig.","Another conclusion is that, in                    underexplored yet relevant experiments to stress a method’s
our experiments, the predictive social force model does not                    accuracy and robustness.",2022-07-20 11:33:12+00:00,The Atlas Benchmark: an Automated Evaluation Framework for Human Motion Prediction,cs.RO,['cs.RO'],"[arxiv.Result.Author('Andrey Rudenko'), arxiv.Result.Author('Luigi Palmieri'), arxiv.Result.Author('Wanting Huang'), arxiv.Result.Author('Achim J. Lilienthal'), arxiv.Result.Author('Kai O. Arras')]","Human motion trajectory prediction, an essential task for autonomous systems
in many domains, has been on the rise in recent years. With a multitude of new
methods proposed by different communities, the lack of standardized benchmarks
and objective comparisons is increasingly becoming a major limitation to assess
progress and guide further research. Existing benchmarks are limited in their
scope and flexibility to conduct relevant experiments and to account for
contextual cues of agents and environments. In this paper we present Atlas, a
benchmark to systematically evaluate human motion trajectory prediction
algorithms in a unified framework. Atlas offers data preprocessing functions,
hyperparameter optimization, comes with popular datasets and has the
flexibility to setup and conduct underexplored yet relevant experiments to
analyze a method's accuracy and robustness. In an example application of Atlas,
we compare five popular model- and learning-based predictors and find that,
when properly applied, early physics-based approaches are still remarkably
competitive. Such results confirm the necessity of benchmarks like Atlas.",-0.10036066,-0.26550657,-0.16027984,A
9106,"While these
                                                                               ﬁndings motivate further research particularly in agent in-
                                                                               teraction modeling, they also show the necessity for such
  Fig.","Finally, the results                  we compared ﬁve popular prediction methods, three early
                                                                               physics-based approaches and two learning-based state-of-
                                                                               the-art approaches and found that the model-based methods,
                                                                               properly applied, are surprisingly competitive.",9.,2022-07-20 11:33:12+00:00,The Atlas Benchmark: an Automated Evaluation Framework for Human Motion Prediction,cs.RO,['cs.RO'],"[arxiv.Result.Author('Andrey Rudenko'), arxiv.Result.Author('Luigi Palmieri'), arxiv.Result.Author('Wanting Huang'), arxiv.Result.Author('Achim J. Lilienthal'), arxiv.Result.Author('Kai O. Arras')]","Human motion trajectory prediction, an essential task for autonomous systems
in many domains, has been on the rise in recent years. With a multitude of new
methods proposed by different communities, the lack of standardized benchmarks
and objective comparisons is increasingly becoming a major limitation to assess
progress and guide further research. Existing benchmarks are limited in their
scope and flexibility to conduct relevant experiments and to account for
contextual cues of agents and environments. In this paper we present Atlas, a
benchmark to systematically evaluate human motion trajectory prediction
algorithms in a unified framework. Atlas offers data preprocessing functions,
hyperparameter optimization, comes with popular datasets and has the
flexibility to setup and conduct underexplored yet relevant experiments to
analyze a method's accuracy and robustness. In an example application of Atlas,
we compare five popular model- and learning-based predictors and find that,
when properly applied, early physics-based approaches are still remarkably
competitive. Such results confirm the necessity of benchmarks like Atlas.",-0.048454896,-0.26317984,-0.2126564,C
9157,"It is intended             used as a visualisation tool for human operators, later work
                                        to be used by a remote human operator to easily visualise               has focused on the aspect of surfacic mesh mapping for
                                        the mapped environment during or after the mission or as                navigation purposes, arguing that surface-based maps contain
                                        a development base for further researches in the ﬁeld of                more dense information compared to sparse point clouds.","challenging unknown environments and provide an associated              While 3D mesh reconstruction was mainly intended to be
                                        colored 3D mesh representation in real time.",exploration robotics.,2022-07-21 14:09:43+00:00,Online Localisation and Colored Mesh Reconstruction Architecture for 3D Visual Feedback in Robotic Exploration Missions,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Quentin Serdel'), arxiv.Result.Author('Christophe Grand'), arxiv.Result.Author('Julien Marzat'), arxiv.Result.Author('Julien Moras')]","This paper introduces an Online Localisation and Colored Mesh Reconstruction
(OLCMR) ROS perception architecture for ground exploration robots aiming to
perform robust Simultaneous Localisation And Mapping (SLAM) in challenging
unknown environments and provide an associated colored 3D mesh representation
in real time. It is intended to be used by a remote human operator to easily
visualise the mapped environment during or after the mission or as a
development base for further researches in the field of exploration robotics.
The architecture is mainly composed of carefully-selected open-source ROS
implementations of a LiDAR-based SLAM algorithm alongside a colored surface
reconstruction procedure using a point cloud and RGB camera images projected
into the 3D space. The overall performances are evaluated on the Newer College
handheld LiDAR-Vision reference dataset and on two experimental trajectories
gathered on board of representative wheeled robots in respectively urban and
countryside outdoor environments. Index Terms: Field Robots, Mapping, SLAM,
Colored Surface Reconstruction",-0.054610465,0.4194608,0.0068046353,B
9226,"With further research,
                                        portation, Autonomous Fly-driving, Urban Air Mobility, Intelli-              people pay attention to amphibious vehicles (roadway driving
                                        gent Transportation System                                                   aircraft or ﬂyable cars) that combine two motion modes and
                                                                                                                     present the following advantages over vehicles with only one
                                                                  I.","It operates along predeﬁned spatial corridors, so it
                                           Index Terms—Amphibious Vehicles, Three-dimensional Trans-                 cannot take full advantage of space.","INTRODUCTION                                    locomotion mode [1]–[8]:

                                        M ODERN transportation is a three-dimensional, compre-                          • Amphibious vehicles possess higher ﬂexibility owing to
                                                hensive, and complementary network system consisting                       multi-modal locomotion abilities.",2022-07-23 00:57:34+00:00,Intelligent Amphibious Ground-Aerial Vehicles: State of the Art Technology for Future Transportation,cs.RO,['cs.RO'],"[arxiv.Result.Author('Xinyu Zhang'), arxiv.Result.Author('Jiangeng Huang'), arxiv.Result.Author('Yuanhao Huang'), arxiv.Result.Author('Kangyao Huang'), arxiv.Result.Author('Lei Yang'), arxiv.Result.Author('Yan Han'), arxiv.Result.Author('Li Wang'), arxiv.Result.Author('Huaping Liu'), arxiv.Result.Author('Jianxi Luo'), arxiv.Result.Author('Jun Li')]","Amphibious ground-aerial vehicles fuse flying and driving modes to enable
more flexible air-land mobility and have received growing attention recently.
By analyzing the existing amphibious vehicles, we highlight the autonomous
fly-driving functionality for the effective uses of amphibious vehicles in
complex three-dimensional urban transportation systems. We review and summarize
the key enabling technologies for intelligent flying-driving in existing
amphibious vehicle designs, identify major technological barriers and propose
potential solutions for future research and innovation. This paper aims to
serve as a guide for research and development of intelligent amphibious
vehicles for urban transportation toward the future.",-0.07107023,-0.038641363,0.21221878,A
9397,"Such a relation yet remains unavailable for nonlinear
a better portrayal of the uncertainty’s possible effect on the           systems and requires further research.",Using more scenarios in the optimization provides                 [29].,"It should also be noted
system, and therefore affords the controller more information            that, even though the constraints will be violated less when
to counteract the disturbance.",2022-07-28 03:52:06+00:00,Model Predictive Control of Nonlinear Latent Force Models: A Scenario-Based Approach,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Thomas Woodruff'), arxiv.Result.Author('Iman Askari'), arxiv.Result.Author('Guanghui Wang'), arxiv.Result.Author('Huazhen Fang')]","Control of nonlinear uncertain systems is a common challenge in the robotics
field. Nonlinear latent force models, which incorporate latent uncertainty
characterized as Gaussian processes, carry the promise of representing such
systems effectively, and we focus on the control design for them in this work.
To enable the design, we adopt the state-space representation of a Gaussian
process to recast the nonlinear latent force model and thus build the ability
to predict the future state and uncertainty concurrently. Using this feature, a
stochastic model predictive control problem is formulated. To derive a
computational algorithm for the problem, we use the scenario-based approach to
formulate a deterministic approximation of the stochastic optimization. We
evaluate the resultant scenario-based model predictive control approach through
a simulation study based on motion planning of an autonomous vehicle, which
shows much effectiveness. The proposed approach can find prospective use in
various other robotics applications.",-0.023794305,-0.2979955,0.22753775,C
9401,A further study should             agreement No 2014-2020.4.01.20-0289.,"Based on the results, the proposed method improved              Estonian Ministry of Education and Research, under grant
safety and prevented unsafe overtaking.","IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL.",2022-07-28 08:46:28+00:00,Learning Based High-Level Decision Making for Abortable Overtaking in Autonomous Vehicles,cs.RO,['cs.RO'],"[arxiv.Result.Author('Ehsan Malayjerdi'), arxiv.Result.Author('Gokhan Alcan'), arxiv.Result.Author('Eshagh Kargar'), arxiv.Result.Author('Hatem Darweesh'), arxiv.Result.Author('Raivo Sell'), arxiv.Result.Author('Ville Kyrki Senior Member')]","Autonomous vehicles are a growing technology that aims to enhance safety,
accessibility, efficiency, and convenience through autonomous maneuvers ranging
from lane change to overtaking. Overtaking is one of the most challenging
maneuvers for autonomous vehicles, and current techniques for autonomous
overtaking are limited to simple situations. This paper studies how to increase
safety in autonomous overtaking by allowing the maneuver to be aborted. We
propose a decision-making process based on a deep Q-Network to determine if and
when the overtaking maneuver needs to be aborted. The proposed algorithm is
empirically evaluated in simulation with varying traffic situations, indicating
that the proposed method improves safety during overtaking maneuvers.
Furthermore, the approach is demonstrated in real-world experiments using the
autonomous shuttle iseAuto.",0.23256254,-0.14557138,0.013217261,C
9402,A further study should             agreement No 2014-2020.4.01.20-0289.,"Based on the results, the proposed method improved              Estonian Ministry of Education and Research, under grant
safety and prevented unsafe overtaking.","IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL.",2022-07-28 08:46:28+00:00,Learning Based High-Level Decision Making for Abortable Overtaking in Autonomous Vehicles,cs.RO,['cs.RO'],"[arxiv.Result.Author('Ehsan Malayjerdi'), arxiv.Result.Author('Gokhan Alcan'), arxiv.Result.Author('Eshagh Kargar'), arxiv.Result.Author('Hatem Darweesh'), arxiv.Result.Author('Raivo Sell'), arxiv.Result.Author('Ville Kyrki')]","Autonomous vehicles are a growing technology that aims to enhance safety,
accessibility, efficiency, and convenience through autonomous maneuvers ranging
from lane change to overtaking. Overtaking is one of the most challenging
maneuvers for autonomous vehicles, and current techniques for autonomous
overtaking are limited to simple situations. This paper studies how to increase
safety in autonomous overtaking by allowing the maneuver to be aborted. We
propose a decision-making process based on a deep Q-Network to determine if and
when the overtaking maneuver needs to be aborted. The proposed algorithm is
empirically evaluated in simulation with varying traffic situations, indicating
that the proposed method improves safety during overtaking maneuvers.
Furthermore, the approach is demonstrated in real-world experiments using the
autonomous shuttle iseAuto.",0.23256254,-0.14557138,0.013217261,C
9494,"All the code in
                                                                                                                                          both Pytorch and Tensorﬂow framework as well as the Android                  2.7
                                                                                                                                          application code have been shared to improve further research.","A               2.9                                ResNet18
                                                                                                                                          comprehensive evaluation using four different datasets as well as
                                                                                                                                          the proposed dataset and real device implementation has been                 2.8
                                                                                                                                          done to prove the performance of the architecture.","2.6
                                                                                                                                             Index Terms—Deep learning, Data-driven methods, Inertial
                                                                                                                                          navigation, IMU measurements.",2022-07-29 20:42:01+00:00,IMUNet: Efficient Regression Architecture for IMU Navigation and Positioning,cs.RO,['cs.RO'],"[arxiv.Result.Author('Behnam Zeinali'), arxiv.Result.Author('Hadi Zandizari'), arxiv.Result.Author('J. Morris Chang')]","Data-driven based method for navigation and positioning has absorbed
attention in recent years and it outperforms all its competitor methods in
terms of accuracy and efficiency. This paper introduces a new architecture
called IMUNet which is accurate and efficient for position estimation on edge
device implementation receiving a sequence of raw IMU measurements. The
architecture has been compared with one dimension version of the
state-of-the-art CNN networks that have been introduced recently for edge
device implementation in terms of accuracy and efficiency. Moreover, a new
method for collecting a dataset using IMU sensors on cell phones and Google
ARCore API has been proposed and a publicly available dataset has been
recorded. A comprehensive evaluation using four different datasets as well as
the proposed dataset and real device implementation has been done to prove the
performance of the architecture. All the code in both Pytorch and Tensorflow
framework as well as the Android application code have been shared to improve
further research.",0.12014796,0.14077464,-0.13659307,B
9495,"for collecting a dataset was introduced and the code was
                                                                             shared that allows anyone with a cellphone to collect a
                                                                             dataset for further research.","A new method
ciency.","An empirical study was done
                                                                             by implementing and harnessing a one-dimensional version
                                                                             of edge device-friendly state-of-the-art convolutional neural
                                                                             networks for inertial navigation purposes.",2022-07-29 20:42:01+00:00,IMUNet: Efficient Regression Architecture for IMU Navigation and Positioning,cs.RO,['cs.RO'],"[arxiv.Result.Author('Behnam Zeinali'), arxiv.Result.Author('Hadi Zandizari'), arxiv.Result.Author('J. Morris Chang')]","Data-driven based method for navigation and positioning has absorbed
attention in recent years and it outperforms all its competitor methods in
terms of accuracy and efficiency. This paper introduces a new architecture
called IMUNet which is accurate and efficient for position estimation on edge
device implementation receiving a sequence of raw IMU measurements. The
architecture has been compared with one dimension version of the
state-of-the-art CNN networks that have been introduced recently for edge
device implementation in terms of accuracy and efficiency. Moreover, a new
method for collecting a dataset using IMU sensors on cell phones and Google
ARCore API has been proposed and a publicly available dataset has been
recorded. A comprehensive evaluation using four different datasets as well as
the proposed dataset and real device implementation has been done to prove the
performance of the architecture. All the code in both Pytorch and Tensorflow
framework as well as the Android application code have been shared to improve
further research.",0.053221546,0.2578697,-0.2148298,B
9496,"All the code was
                                                                             shared for modiﬁcation to enhance further research.","An empirical study was done
                                                                             by implementing and harnessing a one-dimensional version
                                                                             of edge device-friendly state-of-the-art convolutional neural
                                                                             networks for inertial navigation purposes.","SUBMITTED TO IEEE TRANSACTIONS ON MOBILE COMPUTING                                                                                               10

                             REFERENCES                                         [23] M. Tan and Q.",2022-07-29 20:42:01+00:00,IMUNet: Efficient Regression Architecture for IMU Navigation and Positioning,cs.RO,['cs.RO'],"[arxiv.Result.Author('Behnam Zeinali'), arxiv.Result.Author('Hadi Zandizari'), arxiv.Result.Author('J. Morris Chang')]","Data-driven based method for navigation and positioning has absorbed
attention in recent years and it outperforms all its competitor methods in
terms of accuracy and efficiency. This paper introduces a new architecture
called IMUNet which is accurate and efficient for position estimation on edge
device implementation receiving a sequence of raw IMU measurements. The
architecture has been compared with one dimension version of the
state-of-the-art CNN networks that have been introduced recently for edge
device implementation in terms of accuracy and efficiency. Moreover, a new
method for collecting a dataset using IMU sensors on cell phones and Google
ARCore API has been proposed and a publicly available dataset has been
recorded. A comprehensive evaluation using four different datasets as well as
the proposed dataset and real device implementation has been done to prove the
performance of the architecture. All the code in both Pytorch and Tensorflow
framework as well as the Android application code have been shared to improve
further research.",-0.0025084498,0.21540835,-0.08975695,B
9507,"To reinforce                      takes advantage of kinematics, differential kinematics, and
                                        further research endeavors, our implementation is offered as                  dynamics to estimate the contact state.","Furthermore, its efﬁcacy is demonstrated in                   probabilistic framework with a Hidden Markov Model that
                                        base estimation with a real TALOS humanoid.","This approach does
                                        an open-source ROS/Python package, coined Legged Contact                      not rely on F/T sensors, but effectively exploits joint po-
                                        Detection (LCD).",2022-07-30 17:19:47+00:00,Robust Contact State Estimation in Humanoid Walking Gaits,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Stylianos Piperakis'), arxiv.Result.Author('Michael Maravgakis'), arxiv.Result.Author('Dimitrios Kanoulas'), arxiv.Result.Author('Panos Trahanias')]","In this article, we propose a deep learning framework that provides a unified
approach to the problem of leg contact detection in humanoid robot walking
gaits. Our formulation accomplishes to accurately and robustly estimate the
contact state probability for each leg (i.e., stable or slip/no contact). The
proposed framework employs solely proprioceptive sensing and although it relies
on simulated ground-truth contact data for the classification process, we
demonstrate that it generalizes across varying friction surfaces and different
legged robotic platforms and, at the same time, is readily transferred from
simulation to practice. The framework is quantitatively and qualitatively
assessed in simulation via the use of ground-truth contact data and is
contrasted against state of-the-art methods with an ATLAS, a NAO, and a TALOS
humanoid robot. Furthermore, its efficacy is demonstrated in base estimation
with a real TALOS humanoid. To reinforce further research endeavors, our
implementation is offered as an open-source ROS/Python package, coined Legged
Contact Detection (LCD).",-0.2812847,-0.02183541,0.042640142,A
9508,RESULTS                             further research endeavors.,"The latter modules are also
                                                                   released as open-source ROS/C++ packages to reinforce
                           IV.","In the current section, we present quantitative and quali-         Next, we’ve commanded each robot to continuously walk
tative results that demonstrate the accuracy and efﬁcacy of        over multiple surfaces with varying friction coefﬁcients from
the proposed framework both in simulation and real world           0.05 to 1.2, for approximately 10 minutes, to record the
experiments.",2022-07-30 17:19:47+00:00,Robust Contact State Estimation in Humanoid Walking Gaits,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Stylianos Piperakis'), arxiv.Result.Author('Michael Maravgakis'), arxiv.Result.Author('Dimitrios Kanoulas'), arxiv.Result.Author('Panos Trahanias')]","In this article, we propose a deep learning framework that provides a unified
approach to the problem of leg contact detection in humanoid robot walking
gaits. Our formulation accomplishes to accurately and robustly estimate the
contact state probability for each leg (i.e., stable or slip/no contact). The
proposed framework employs solely proprioceptive sensing and although it relies
on simulated ground-truth contact data for the classification process, we
demonstrate that it generalizes across varying friction surfaces and different
legged robotic platforms and, at the same time, is readily transferred from
simulation to practice. The framework is quantitatively and qualitatively
assessed in simulation via the use of ground-truth contact data and is
contrasted against state of-the-art methods with an ATLAS, a NAO, and a TALOS
humanoid robot. Furthermore, its efficacy is demonstrated in base estimation
with a real TALOS humanoid. To reinforce further research endeavors, our
implementation is offered as an open-source ROS/Python package, coined Legged
Contact Detection (LCD).",-0.25341046,0.014760095,0.22106725,A_centroid
9509,"[14] M. F. Fallon, M. Antone, N. Roy, and S. Teller, “Drift-free Humanoid
To reinforce further research endeavours we released LCD                             State Estimation Fusing Kinematic, Inertial and LIDAR Sensing,” in
as an open-source ROS/Python package [25].","Although the latter
                                                                rely on simulated ground-truth contact data for the training
                                                                process, LCD generalizes across robotic platforms and can
be readily transferred from simulation to real world setups.",IEEE-RAS Intl.,2022-07-30 17:19:47+00:00,Robust Contact State Estimation in Humanoid Walking Gaits,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Stylianos Piperakis'), arxiv.Result.Author('Michael Maravgakis'), arxiv.Result.Author('Dimitrios Kanoulas'), arxiv.Result.Author('Panos Trahanias')]","In this article, we propose a deep learning framework that provides a unified
approach to the problem of leg contact detection in humanoid robot walking
gaits. Our formulation accomplishes to accurately and robustly estimate the
contact state probability for each leg (i.e., stable or slip/no contact). The
proposed framework employs solely proprioceptive sensing and although it relies
on simulated ground-truth contact data for the classification process, we
demonstrate that it generalizes across varying friction surfaces and different
legged robotic platforms and, at the same time, is readily transferred from
simulation to practice. The framework is quantitatively and qualitatively
assessed in simulation via the use of ground-truth contact data and is
contrasted against state of-the-art methods with an ATLAS, a NAO, and a TALOS
humanoid robot. Furthermore, its efficacy is demonstrated in base estimation
with a real TALOS humanoid. To reinforce further research endeavors, our
implementation is offered as an open-source ROS/Python package, coined Legged
Contact Detection (LCD).",-0.21608981,0.16100103,0.007346239,A
9642,"The competition had a systems
                                       believe are fundamental open problems, that are likely to require            track and a virtual track, and included three main events: the
                                       further research to break through.","Third, we outline what we                underground environments.","Finally, we provide a list of             Tunnel circuit event, the Urban circuit event, and the Finals.",2022-08-02 23:54:48+00:00,Present and Future of SLAM in Extreme Underground Environments,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Lukas Bernreiter'), arxiv.Result.Author('Harel Biggie'), arxiv.Result.Author('Gavin Catt'), arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Arghya Chatterjee'), arxiv.Result.Author('Christopher E. Denniston'), arxiv.Result.Author('Simon-Pierre Deschênes'), arxiv.Result.Author('Kyle Harlow'), arxiv.Result.Author('Shehryar Khattak'), arxiv.Result.Author('Lucas Nogueira'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Pavel Petráček'), arxiv.Result.Author('Matěj Petrlík'), arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Vít Krátký'), arxiv.Result.Author('Shibo Zhao'), arxiv.Result.Author('Ali-akbar Agha-mohammadi'), arxiv.Result.Author('Kostas Alexis'), arxiv.Result.Author('Christoffer Heckman'), arxiv.Result.Author('Kasra Khosoussi'), arxiv.Result.Author('Navinda Kottege'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Marco Hutter'), arxiv.Result.Author('Fred Pauling'), arxiv.Result.Author('François Pomerleau'), arxiv.Result.Author('Martin Saska'), arxiv.Result.Author('Sebastian Scherer'), arxiv.Result.Author('Roland Siegwart'), arxiv.Result.Author('Jason L. Williams'), arxiv.Result.Author('Luca Carlone')]","This paper reports on the state of the art in underground SLAM by discussing
different SLAM strategies and results across six teams that participated in the
three-year-long SubT competition. In particular, the paper has four main goals.
First, we review the algorithms, architectures, and systems adopted by the
teams; particular emphasis is put on lidar-centric SLAM solutions (the go-to
approach for virtually all teams in the competition), heterogeneous multi-robot
operation (including both aerial and ground robots), and real-world underground
operation (from the presence of obscurants to the need to handle tight
computational constraints). We do not shy away from discussing the dirty
details behind the different SubT SLAM systems, which are often omitted from
technical papers. Second, we discuss the maturity of the field by highlighting
what is possible with the current SLAM systems and what we believe is within
reach with some good systems engineering. Third, we outline what we believe are
fundamental open problems, that are likely to require further research to break
through. Finally, we provide a list of open-source SLAM implementations and
datasets that have been produced during the SubT challenge and related efforts,
and constitute a useful resource for researchers and practitioners.",0.20123509,-0.055447645,-0.02369844,C
9643,"problems, that are likely to require further research to break
                                                                  through (Section V).","on rough terrains induces noise in inertial sensors, due to the   Third, we outline what we believe are fundamental open
aggressive 6-DoF motion and high-frequency vibrations.","Finally, we provide a list of open-source
   Even when the sensors themselves perform to speciﬁcations,     SLAM implementations and datasets that have been produced
these environments create further challenges for SLAM al-         during the SubT challenge and related efforts, and constitute
gorithms.",2022-08-02 23:54:48+00:00,Present and Future of SLAM in Extreme Underground Environments,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Lukas Bernreiter'), arxiv.Result.Author('Harel Biggie'), arxiv.Result.Author('Gavin Catt'), arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Arghya Chatterjee'), arxiv.Result.Author('Christopher E. Denniston'), arxiv.Result.Author('Simon-Pierre Deschênes'), arxiv.Result.Author('Kyle Harlow'), arxiv.Result.Author('Shehryar Khattak'), arxiv.Result.Author('Lucas Nogueira'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Pavel Petráček'), arxiv.Result.Author('Matěj Petrlík'), arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Vít Krátký'), arxiv.Result.Author('Shibo Zhao'), arxiv.Result.Author('Ali-akbar Agha-mohammadi'), arxiv.Result.Author('Kostas Alexis'), arxiv.Result.Author('Christoffer Heckman'), arxiv.Result.Author('Kasra Khosoussi'), arxiv.Result.Author('Navinda Kottege'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Marco Hutter'), arxiv.Result.Author('Fred Pauling'), arxiv.Result.Author('François Pomerleau'), arxiv.Result.Author('Martin Saska'), arxiv.Result.Author('Sebastian Scherer'), arxiv.Result.Author('Roland Siegwart'), arxiv.Result.Author('Jason L. Williams'), arxiv.Result.Author('Luca Carlone')]","This paper reports on the state of the art in underground SLAM by discussing
different SLAM strategies and results across six teams that participated in the
three-year-long SubT competition. In particular, the paper has four main goals.
First, we review the algorithms, architectures, and systems adopted by the
teams; particular emphasis is put on lidar-centric SLAM solutions (the go-to
approach for virtually all teams in the competition), heterogeneous multi-robot
operation (including both aerial and ground robots), and real-world underground
operation (from the presence of obscurants to the need to handle tight
computational constraints). We do not shy away from discussing the dirty
details behind the different SubT SLAM systems, which are often omitted from
technical papers. Second, we discuss the maturity of the field by highlighting
what is possible with the current SLAM systems and what we believe is within
reach with some good systems engineering. Third, we outline what we believe are
fundamental open problems, that are likely to require further research to break
through. Finally, we provide a list of open-source SLAM implementations and
datasets that have been produced during the SubT challenge and related efforts,
and constitute a useful resource for researchers and practitioners.",-0.044070866,0.30274713,0.12204451,B
9644,"1335–1379, 2016.
engineering, and what is yet to be solved and likely requires
further research.","1335–1379, 2021.
art, and the state-of-the-practice in SLAM in extreme subter-
ranean environments and reports on what can be considered                       [16] L. E. Parker, D. Rus, and G. S. Sukhatme, “Multiple mobile robot
solved problems, what can be solved with some good systems                            systems,” Springer handbook of robotics, pp.","We reviewed algorithms, architectures, and                    [17] P. Y. Lajoie, B. Ramtoula, F. Wu, and G. Beltrame, “Towards collabo-
systems adopted by six teams that participated in the DARPA                           rative simultaneous localization and mapping: a survey of the current
Subterranean (SubT) Challenge, with particular emphasis on                            research landscape,” ArXiv Preprint: 2108.08325, 2021.",2022-08-02 23:54:48+00:00,Present and Future of SLAM in Extreme Underground Environments,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kamak Ebadi'), arxiv.Result.Author('Lukas Bernreiter'), arxiv.Result.Author('Harel Biggie'), arxiv.Result.Author('Gavin Catt'), arxiv.Result.Author('Yun Chang'), arxiv.Result.Author('Arghya Chatterjee'), arxiv.Result.Author('Christopher E. Denniston'), arxiv.Result.Author('Simon-Pierre Deschênes'), arxiv.Result.Author('Kyle Harlow'), arxiv.Result.Author('Shehryar Khattak'), arxiv.Result.Author('Lucas Nogueira'), arxiv.Result.Author('Matteo Palieri'), arxiv.Result.Author('Pavel Petráček'), arxiv.Result.Author('Matěj Petrlík'), arxiv.Result.Author('Andrzej Reinke'), arxiv.Result.Author('Vít Krátký'), arxiv.Result.Author('Shibo Zhao'), arxiv.Result.Author('Ali-akbar Agha-mohammadi'), arxiv.Result.Author('Kostas Alexis'), arxiv.Result.Author('Christoffer Heckman'), arxiv.Result.Author('Kasra Khosoussi'), arxiv.Result.Author('Navinda Kottege'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Marco Hutter'), arxiv.Result.Author('Fred Pauling'), arxiv.Result.Author('François Pomerleau'), arxiv.Result.Author('Martin Saska'), arxiv.Result.Author('Sebastian Scherer'), arxiv.Result.Author('Roland Siegwart'), arxiv.Result.Author('Jason L. Williams'), arxiv.Result.Author('Luca Carlone')]","This paper reports on the state of the art in underground SLAM by discussing
different SLAM strategies and results across six teams that participated in the
three-year-long SubT competition. In particular, the paper has four main goals.
First, we review the algorithms, architectures, and systems adopted by the
teams; particular emphasis is put on lidar-centric SLAM solutions (the go-to
approach for virtually all teams in the competition), heterogeneous multi-robot
operation (including both aerial and ground robots), and real-world underground
operation (from the presence of obscurants to the need to handle tight
computational constraints). We do not shy away from discussing the dirty
details behind the different SubT SLAM systems, which are often omitted from
technical papers. Second, we discuss the maturity of the field by highlighting
what is possible with the current SLAM systems and what we believe is within
reach with some good systems engineering. Third, we outline what we believe are
fundamental open problems, that are likely to require further research to break
through. Finally, we provide a list of open-source SLAM implementations and
datasets that have been produced during the SubT challenge and related efforts,
and constitute a useful resource for researchers and practitioners.",-0.06071907,0.349346,0.103728354,B
9772,"The relation between F˜t(D1), F˜t(D3)

                                                                                                can’t be established this way and needs further study.","Using  the  equivalence  of   norms  and       1  =  0.88Γ−1

                                                                                                                                                   Σ2

                                                                                                for our example, it can be seen that F˜t(D1) ⊂ F˜t(D2),

                                                                                                F˜t(D3) ⊂ F˜t(D2).","The

                                                                                                increased conservatism for RMPC, however, results in fewer

     2Experiments were run on a computer with a Intel i9-9900K CPU, 32                          constraint violations and its LP formulation of collision
GB RAM, and a RTX 2080 Ti GPU.",2022-08-06 14:58:16+00:00,Collision Avoidance for Dynamic Obstacles with Uncertain Predictions using Model Predictive Control,cs.RO,"['cs.RO', 'math.OC']","[arxiv.Result.Author('Siddharth H. Nair'), arxiv.Result.Author('Eric H. Tseng'), arxiv.Result.Author('Francesco Borrelli')]","We propose a Model Predictive Control (MPC) for collision avoidance between
an autonomous agent and dynamic obstacles with uncertain predictions. The
collision avoidance constraints are imposed by enforcing positive distance
between convex sets representing the agent and the obstacles, and tractably
reformulating them using Lagrange duality. This approach allows for smooth
collision avoidance constraints even for polytopes, which otherwise require
mixed-integer or non-smooth constraints. We consider three widely used
descriptions of the uncertain obstacle position: 1) Arbitrary distribution with
polytopic support, 2) Gaussian distributions and 3) Arbitrary distribution with
first two moments known. For each case we obtain deterministic reformulations
of the collision avoidance constraints. The proposed MPC formulation optimizes
over feedback policies to reduce conservatism in satisfying the collision
avoidance constraints. The proposed approach is validated using simulations of
traffic intersections in CARLA.",0.28467286,-0.21915671,0.07754208,C
9838,"[21] conducted further research, where
of sim2real object detection, it usually consists in generating                 they trained a deep neural network for grasp planning using only
photo-realistic images for the training dataset.",Tobin et al.,"The more the                   synthetic images and domain randomization, and achieved an
generated images resemble the real ones, the more the difference                80% success rate in a real-world environment.",2022-08-08 14:16:45+00:00,Object Detection Using Sim2Real Domain Randomization for Robotic Applications,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Dániel Horváth'), arxiv.Result.Author('Gábor Erdős'), arxiv.Result.Author('Zoltán Istenes'), arxiv.Result.Author('Tomáš Horváth'), arxiv.Result.Author('Sándor Földi')]","Robots working in unstructured environments must be capable of sensing and
interpreting their surroundings. One of the main obstacles of
deep-learning-based models in the field of robotics is the lack of
domain-specific labeled data for different industrial applications. In this
article, we propose a sim2real transfer learning method based on domain
randomization for object detection with which labeled synthetic datasets of
arbitrary size and object types can be automatically generated. Subsequently, a
state-of-the-art convolutional neural network, YOLOv4, is trained to detect the
different types of industrial objects. With the proposed domain randomization
method, we could shrink the reality gap to a satisfactory level, achieving
86.32% and 97.38% mAP50 scores, respectively, in the case of zero-shot and
one-shot transfers, on our manually annotated dataset containing 190 real
images. Our solution fits for industrial use as the data generation process
takes less than 0.5 s per image and the training lasts only around 12 h, on a
GeForce RTX 2080 Ti GPU. Furthermore, it can reliably differentiate similar
classes of objects by having access to only one real image for training. To our
best knowledge, this is the only work thus far satisfying these constraints.",-0.18503603,0.19132113,-0.28360185,A
9839,"The sensors and actuators of the
to further research.","Nevertheless, a more thor-      workpieces (center points and bounding box information), as
ough evaluation of the effect of different cutouts can be subject  well as predict grasping poses.","robot and the sim2real computer vision module are connected
                                                                   in a robot control framework based on ROS [71].",2022-08-08 14:16:45+00:00,Object Detection Using Sim2Real Domain Randomization for Robotic Applications,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Dániel Horváth'), arxiv.Result.Author('Gábor Erdős'), arxiv.Result.Author('Zoltán Istenes'), arxiv.Result.Author('Tomáš Horváth'), arxiv.Result.Author('Sándor Földi')]","Robots working in unstructured environments must be capable of sensing and
interpreting their surroundings. One of the main obstacles of
deep-learning-based models in the field of robotics is the lack of
domain-specific labeled data for different industrial applications. In this
article, we propose a sim2real transfer learning method based on domain
randomization for object detection with which labeled synthetic datasets of
arbitrary size and object types can be automatically generated. Subsequently, a
state-of-the-art convolutional neural network, YOLOv4, is trained to detect the
different types of industrial objects. With the proposed domain randomization
method, we could shrink the reality gap to a satisfactory level, achieving
86.32% and 97.38% mAP50 scores, respectively, in the case of zero-shot and
one-shot transfers, on our manually annotated dataset containing 190 real
images. Our solution fits for industrial use as the data generation process
takes less than 0.5 s per image and the training lasts only around 12 h, on a
GeForce RTX 2080 Ti GPU. Furthermore, it can reliably differentiate similar
classes of objects by having access to only one real image for training. To our
best knowledge, this is the only work thus far satisfying these constraints.",-0.28406993,0.15605482,0.013762556,A
9840,The dataset can be used for further research.,"as a community dataset, which non of the previous literature
did.","As highlighted by Colledanchise and Natale [50] behavior
                                                                   tree implementations are less mature than state machine
    Bagnell et al.",2022-08-08 15:21:29+00:00,Behavior Trees and State Machines in Robotics Applications,cs.RO,"['cs.RO', 'cs.SE', 'D.0; D.2.13; D.2.2']","[arxiv.Result.Author('Razan Ghzouli'), arxiv.Result.Author('Swaib Dragule'), arxiv.Result.Author('Thorsten Berger'), arxiv.Result.Author('Einar Broch Johnsen'), arxiv.Result.Author('Andrzej Wasowski')]","Autonomous robots combine a variety of skills to form increasingly complex
behaviors called missions. While the skills are often programmed at a
relatively low level of abstraction, their coordination is architecturally
separated and often expressed in higher-level languages or frameworks. State
Machines have been the go-to modeling language for decades, but recently, the
language of Behavior Trees gained attention among roboticists. Originally
designed for computer games to model autonomous actors, Behavior Trees offer an
extensible tree-based representation of missions and are praised for supporting
modular design and reuse of code. However, even though, several implementations
of the language are in use, little is known about its usage and scope in the
real world. How do concepts offered by behavior trees relate to traditional
languages, such as state machines? How are behavior tree and state machine
concepts used in applications? We present a study of the key language concepts
in Behavior Trees and their use in real-world robotic applications. We identify
behavior tree languages and compare their semantics to the most well-known
behavior modeling language in robotics: state machines. We mine open-source
repositories for robotics applications that use the languages and analyze this
usage. We find similarity aspects between the two behavior modeling languages
in terms of language design and their usage in open-source projects to
accommodate the need of robotic domain. We contribute a dataset of real-world
behavior models, hoping to inspire the community to use and further develop
this language, associated tools, and analysis techniques.",0.1836653,-0.13030687,-0.13977112,C
9856,"Diverse communities, targeted interaction experiments, and a
larger number of participants can be used to conduct further research in diﬀer-
ent settings.","Future research needs to be conducted to solidify
this possibility.","Theme 5: Community Beneﬁt
This theme suggests how this interaction could be beneﬁcial to the community.",2022-08-08 19:59:16+00:00,Share with Me: A Study on a Social Robot Collecting Mental Health Data,cs.RO,['cs.RO'],"[arxiv.Result.Author('Raida Karim'), arxiv.Result.Author('Edgar Lopez'), arxiv.Result.Author('Katelynn Oleson'), arxiv.Result.Author('Tony Li'), arxiv.Result.Author('Elin A. Björling'), arxiv.Result.Author('Maya Cakmak')]","Social robots have been used to assist with mental well-being in various ways
such as to help children with autism improve on their social skills and
executive functioning such as joint attention and bodily awareness. They are
also used to help older adults by reducing feelings of isolation and
loneliness, as well as supporting mental well-being of teens and children.
However, existing work in this sphere has only shown support for mental health
through social robots by responding interactively to human activity to help
them learn relevant skills. We hypothesize that humans can also get help from
social robots in mental well-being by releasing or sharing their mental health
data with the social robots. In this paper, we present a human-robot
interaction (HRI) study to evaluate this hypothesis. During the five-day study,
a total of fifty-five (n=55) participants shared their in-the-moment mood and
stress levels with a social robot. We saw a majority of positive results
indicating it is worth conducting future work in this direction, and the
potential of social robots to largely support mental well-being.",0.13087036,-0.29381067,-0.27214774,C
9857,"Implications: These ﬁndings point out the need to conduct further research to
evaluate how this interaction can be beneﬁcial for people or communities.",One comment towards this: “helpful to others”.,"De-
pending on in what ways these can be beneﬁcial, the design of such interactions
or used social robots could be modiﬁed for maximum beneﬁt in speciﬁc contexts.",2022-08-08 19:59:16+00:00,Share with Me: A Study on a Social Robot Collecting Mental Health Data,cs.RO,['cs.RO'],"[arxiv.Result.Author('Raida Karim'), arxiv.Result.Author('Edgar Lopez'), arxiv.Result.Author('Katelynn Oleson'), arxiv.Result.Author('Tony Li'), arxiv.Result.Author('Elin A. Björling'), arxiv.Result.Author('Maya Cakmak')]","Social robots have been used to assist with mental well-being in various ways
such as to help children with autism improve on their social skills and
executive functioning such as joint attention and bodily awareness. They are
also used to help older adults by reducing feelings of isolation and
loneliness, as well as supporting mental well-being of teens and children.
However, existing work in this sphere has only shown support for mental health
through social robots by responding interactively to human activity to help
them learn relevant skills. We hypothesize that humans can also get help from
social robots in mental well-being by releasing or sharing their mental health
data with the social robots. In this paper, we present a human-robot
interaction (HRI) study to evaluate this hypothesis. During the five-day study,
a total of fifty-five (n=55) participants shared their in-the-moment mood and
stress levels with a social robot. We saw a majority of positive results
indicating it is worth conducting future work in this direction, and the
potential of social robots to largely support mental well-being.",-0.14823166,-0.1630453,-0.13609757,A
9858,"Incorporating these feedback and conducting
further research might lead to new developments in this regard.","In general, mental health is a bit abstract and
it can be hard to design convincing short (e.g., 20-seconds) interactions to col-
lect accurate mental health data.","Theme 7: Self-Awareness
This theme shows user comments with regards to increased awareness in various
ways, such as: “more aware of my stress”, “more conscious about my overall
health, since mental health is even more important than physical health in my
opinion :)”, “I feel more self-aware”.",2022-08-08 19:59:16+00:00,Share with Me: A Study on a Social Robot Collecting Mental Health Data,cs.RO,['cs.RO'],"[arxiv.Result.Author('Raida Karim'), arxiv.Result.Author('Edgar Lopez'), arxiv.Result.Author('Katelynn Oleson'), arxiv.Result.Author('Tony Li'), arxiv.Result.Author('Elin A. Björling'), arxiv.Result.Author('Maya Cakmak')]","Social robots have been used to assist with mental well-being in various ways
such as to help children with autism improve on their social skills and
executive functioning such as joint attention and bodily awareness. They are
also used to help older adults by reducing feelings of isolation and
loneliness, as well as supporting mental well-being of teens and children.
However, existing work in this sphere has only shown support for mental health
through social robots by responding interactively to human activity to help
them learn relevant skills. We hypothesize that humans can also get help from
social robots in mental well-being by releasing or sharing their mental health
data with the social robots. In this paper, we present a human-robot
interaction (HRI) study to evaluate this hypothesis. During the five-day study,
a total of fifty-five (n=55) participants shared their in-the-moment mood and
stress levels with a social robot. We saw a majority of positive results
indicating it is worth conducting future work in this direction, and the
potential of social robots to largely support mental well-being.",0.13402718,-0.24085122,-0.21315074,C
10018,"While the
cartpole simulations we provide in the Appendix D provide some intuition for when this will be
the case, further research is needed to better understand in what scenarios we can see signiﬁcant
beneﬁts from our method.","This requires that the user have a dynamics model which captures the
primary features of the environments which affect the structure of CLFs for the system.","Nonetheless, our two hardware experiments provide encouraging initial
results which indicate that our method can rapidly learn stabilizing controllers using CLFs which
are constructed using a nominal dynamics model.",2022-08-13 20:04:17+00:00,Lyapunov Design for Robust and Efficient Robotic Reinforcement Learning,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Tyler Westenbroek'), arxiv.Result.Author('Fernando Castaneda'), arxiv.Result.Author('Ayush Agrawal'), arxiv.Result.Author('Shankar Sastry'), arxiv.Result.Author('Koushil Sreenath')]","Recent advances in the reinforcement learning (RL) literature have enabled
roboticists to automatically train complex policies in simulated environments.
However, due to the poor sample complexity of these methods, solving
reinforcement learning problems using real-world data remains a challenging
problem. This paper introduces a novel cost-shaping method which aims to reduce
the number of samples needed to learn a stabilizing controller. The method adds
a term involving a control Lyapunov function (CLF) -- an `energy-like' function
from the model-based control literature -- to typical cost formulations.
Theoretical results demonstrate the new costs lead to stabilizing controllers
when smaller discount factors are used, which is well-known to reduce sample
complexity. Moreover, the addition of the CLF term `robustifies' the search for
a stabilizing controller by ensuring that even highly sub-optimal polices will
stabilize the system. We demonstrate our approach with two hardware examples
where we learn stabilizing controllers for a cartpole and an A1 quadruped with
only seconds and a few minutes of fine-tuning data, respectively.",-0.12904897,-0.21911022,0.20545138,A
10019,"While the
cart-pole simulations we provide in the Appendix D provide some intuition for when this will be
the case, further research is needed to better understand in what scenarios we can see signiﬁcant
beneﬁts from our method.","This requires that the user has a dynamics model which captures the
primary features of the environments which affect the structure of CLFs for the system.","Nonetheless, our two hardware experiments provide encouraging initial
results which indicate that our method can rapidly learn stabilizing controllers using CLFs which
are constructed using a nominal dynamics model.",2022-08-13 20:04:17+00:00,Lyapunov Design for Robust and Efficient Robotic Reinforcement Learning,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Tyler Westenbroek'), arxiv.Result.Author('Fernando Castaneda'), arxiv.Result.Author('Ayush Agrawal'), arxiv.Result.Author('Shankar Sastry'), arxiv.Result.Author('Koushil Sreenath')]","Recent advances in the reinforcement learning (RL) literature have enabled
roboticists to automatically train complex policies in simulated environments.
However, due to the poor sample complexity of these methods, solving RL
problems using real-world data remains a challenging problem. This paper
introduces a novel cost-shaping method which aims to reduce the number of
samples needed to learn a stabilizing controller. The method adds a term
involving a Control Lyapunov Function (CLF) -- an `energy-like' function from
the model-based control literature -- to typical cost formulations. Theoretical
results demonstrate the new costs lead to stabilizing controllers when smaller
discount factors are used, which is well-known to reduce sample complexity.
Moreover, the addition of the CLF term `robustifies' the search for a
stabilizing controller by ensuring that even highly sub-optimal polices will
stabilize the system. We demonstrate our approach with two hardware examples
where we learn stabilizing controllers for a cartpole and an A1 quadruped with
only seconds and a few minutes of fine-tuning data, respectively. Furthermore,
simulation benchmark studies show that obtaining stabilizing policies by
optimizing our proposed costs requires orders of magnitude less data compared
to standard cost designs.",-0.11063522,-0.20143747,0.21400836,A
10223,"This lays the
      ground for further research that can apply SPTs to mazes and increasingly complex
      scenes without considerable loss of accuracy.","Stability of the SPT Planner Module

      Our results show comprehensively that the SPT Planner Module is stable concerning
      average action prediction accuracy for slight changes in obstacle complexity and model
      parameters ranging from CNN Encoder to the Transformer Encoder.",6.,2022-08-19 20:14:29+00:00,[Re] Differentiable Spatial Planning using Transformers,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Rohit Ranjan'), arxiv.Result.Author('Himadri Bhakta'), arxiv.Result.Author('Animesh Jha'), arxiv.Result.Author('Parv Maheshwari'), arxiv.Result.Author('Debashish Chakravarty')]","This report covers our reproduction effort of the paper 'Differentiable
Spatial Planning using Transformers' by Chaplot et al. . In this paper, the
problem of spatial path planning in a differentiable way is considered. They
show that their proposed method of using Spatial Planning Transformers
outperforms prior data-driven models and leverages differentiable structures to
learn mapping without a ground truth map simultaneously. We verify these claims
by reproducing their experiments and testing their method on new data. We also
investigate the stability of planning accuracy with maps with increased
obstacle complexity. Efforts to investigate and verify the learnings of the
Mapper module were met with failure stemming from a paucity of computational
resources and unreachable authors.",-0.019614201,0.04667381,-0.1602,C
10224,"This provides ground for further research into increased
      complexities models that might draw deeper insights and plan more accurately.","We were
      able to improve the reproduced accuracy and loss of the SPT Planner Module by 0.87%
      and 57.64%, respectively, by increasing the CNN Encoder depth, embedding size and
      Transformer Encoder complexity.","We could not train the End-to-End Mapper and Planner Module due to a paucity of
      computational resources.",2022-08-19 20:14:29+00:00,[Re] Differentiable Spatial Planning using Transformers,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Rohit Ranjan'), arxiv.Result.Author('Himadri Bhakta'), arxiv.Result.Author('Animesh Jha'), arxiv.Result.Author('Parv Maheshwari'), arxiv.Result.Author('Debashish Chakravarty')]","This report covers our reproduction effort of the paper 'Differentiable
Spatial Planning using Transformers' by Chaplot et al. . In this paper, the
problem of spatial path planning in a differentiable way is considered. They
show that their proposed method of using Spatial Planning Transformers
outperforms prior data-driven models and leverages differentiable structures to
learn mapping without a ground truth map simultaneously. We verify these claims
by reproducing their experiments and testing their method on new data. We also
investigate the stability of planning accuracy with maps with increased
obstacle complexity. Efforts to investigate and verify the learnings of the
Mapper module were met with failure stemming from a paucity of computational
resources and unreachable authors.",0.10912329,0.08774957,-0.29479152,C
10279,"We now discuss when it should not be used
                                                             0.4                                                      and which aspects still require further research.","1.0
                                                                                                                      7 Limitations of Acoustic Sensing
                       0.8
                                                                                                                      In our experiments, acoustic sensing has proven to be a
                                                             0.6                                                      simple, versatile, and robust approach to the sensorization
                                                                                                                      of soft actuators.","0.2                                                         Because the sensorization method is based on recognizing
                                                                                                                      small variations in the recorded sound, it may fail when
                                                             0.0 KNN SVC RF MLP                                       objects produce sound themselves.",2022-08-22 13:25:43+00:00,Passive and Active Acoustic Sensing for Soft Pneumatic Actuators,cs.RO,['cs.RO'],"[arxiv.Result.Author('Vincent Wall'), arxiv.Result.Author('Gabriel Zöller'), arxiv.Result.Author('Oliver Brock')]","We propose a sensorization method for soft pneumatic actuators that uses an
embedded microphone and speaker to measure different actuator properties. The
physical state of the actuator determines the specific modulation of sound as
it travels through the structure. Using simple machine learning, we create a
computational sensor that infers the corresponding state from sound recordings.
We demonstrate the acoustic sensor on a soft pneumatic continuum actuator and
use it to measure contact locations, contact forces, object materials, actuator
inflation, and actuator temperature. We show that the sensor is reliable
(average classification rate for six contact locations of 93%), precise (mean
spatial accuracy of 3.7 mm), and robust against common disturbances like
background noise. Finally, we compare different sounds and learning methods and
achieve best results with 20 ms of white noise and a support vector classifier
as the sensor model.",-0.12968256,-0.07613631,0.1970906,A
10315,"Thus, further research on
                                        users embodying a physical robot is warranted.","It has been shown that
                                        regular, non-immersive screens do not make users feel as present as
                                        an Head-Mounted Display (HMD) [27].","Besides a more immersive screen, another piece shown to increase
                                        the feeling of presence is the ability to embody a robot that one can
                                        move, instead of a stationary camera [24].",2022-08-22 21:37:49+00:00,Leaning-Based Control of an Immersive-Telepresence Robot,cs.RO,"['cs.RO', 'cs.HC', 'cs.MM']","[arxiv.Result.Author('Joona Halkola'), arxiv.Result.Author('Markku Suomalainen'), arxiv.Result.Author('Basak Sakcak'), arxiv.Result.Author('Katherine J. Mimnaugh'), arxiv.Result.Author('Juho Kalliokoski'), arxiv.Result.Author('Alexis P. Chambers'), arxiv.Result.Author('Timo Ojala'), arxiv.Result.Author('Steven M. LaValle')]","In this paper, we present an implementation of a leaning-based control of a
differential drive telepresence robot and a user study in simulation, with the
goal of bringing the same functionality to a real telepresence robot. The
participants used a balance board to control the robot and viewed the virtual
environment through a head-mounted display. The main motivation for using a
balance board as the control device stems from Virtual Reality (VR) sickness;
even small movements of your own body matching the motions seen on the screen
decrease the sensory conflict between vision and vestibular organs, which lies
at the heart of most theories regarding the onset of VR sickness. To test the
hypothesis that the balance board as a control method would be less sickening
than using joysticks, we designed a user study (N=32, 15 women) in which the
participants drove a simulated differential drive robot in a virtual
environment with either a Nintendo Wii Balance Board or joysticks. However, our
pre-registered main hypotheses were not supported; the joystick did not cause
any more VR sickness on the participants than the balance board, and the board
proved to be statistically significantly more difficult to use, both
subjectively and objectively. Analyzing the open-ended questions revealed these
results to be likely connected, meaning that the difficulty of use seemed to
affect sickness; even unlimited training time before the test did not make the
use as easy as the familiar joystick. Thus, making the board easier to use is a
key to enable its potential; we present a few possibilities towards this goal.",-0.30475616,0.0030050573,-0.033690557,A
10326,"Furthermore,           Data Acquisition Using a Synchronized In-Air Imaging Sonar Sensor
the task ahead requires further research on using 3D-sonar               Network,” in IEEE International Conference on Intelligent Robots and
sensors to assist autonomous agents in completing their tasks.","1, p. e54076,
theoretical foundation completed we can now implement an                 1 2013.
online real-time system on a real-world mobile platform and         [5] R. Kerstens, D. Laurijssen, G. Schouten, and J. Steckel, “3D Point Cloud
perform a full analysis of the navigation system.",Systems.,2022-08-23 09:10:22+00:00,Adaptive Acoustic Flow-Based Navigation with 3D Sonar Sensor Fusion,cs.RO,['cs.RO'],"[arxiv.Result.Author('Wouter Jansen'), arxiv.Result.Author('Dennis Laurijssen'), arxiv.Result.Author('Jan Steckel')]","Navigating spatially varied and dynamic environments is one of the key tasks
for autonomous agents. In this paper we present a novel method of navigating a
mobile platform with one or multiple 3D-sonar sensors. Moving a mobile platform
and subsequently any 3D-sonar sensor on it, will create signature variations
over time of the echoed reflections in the sensor readings. An approach is
presented to create a predictive model of these signature variations for any
motion type. Furthermore, the model is adaptive and works for any position and
orientation of one or multiple sonar sensors on a mobile platform. We propose
to use this adaptive model and fuse all sensory readings to create a layered
control system allowing a mobile platform to perform a set of primitive motions
such as collision avoidance, obstacle avoidance, wall following and corridor
following behaviours to navigate an environment with dynamically moving objects
within it. This paper describes the underlying theoretical base of the entire
navigation model and validates it in a simulated environment with results that
shows the system is stable and delivers expected behaviour for several tested
spatial configurations of one or multiple sonar sensors that can complete an
autonomous navigation task.",-0.0050030565,0.4658212,0.1086019,B
10442,"We hope that our novel trafﬁc behavior simulation method, along with the evaluation
protocol and data interface software, can serve as a foundation for further research on this topic.","Finally, as part of our core
contributions, we develop and open source a software tool that uniﬁes data formats across different
AV datasets and allows users to transform scenarios from existing datasets into interactive simulation
environments.","2 Related work

Trafﬁc simulation.",2022-08-26 02:17:54+00:00,BITS: Bi-level Imitation for Traffic Simulation,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Danfei Xu'), arxiv.Result.Author('Yuxiao Chen'), arxiv.Result.Author('Boris Ivanovic'), arxiv.Result.Author('Marco Pavone')]","Simulation is the key to scaling up validation and verification for robotic
systems such as autonomous vehicles. Despite advances in high-fidelity physics
and sensor simulation, a critical gap remains in simulating realistic behaviors
of road users. This is because, unlike simulating physics and graphics,
devising first principle models for human-like behaviors is generally
infeasible. In this work, we take a data-driven approach and propose a method
that can learn to generate traffic behaviors from real-world driving logs. The
method achieves high sample efficiency and behavior diversity by exploiting the
bi-level hierarchy of driving behaviors by decoupling the traffic simulation
problem into high-level intent inference and low-level driving behavior
imitation. The method also incorporates a planning module to obtain stable
long-horizon behaviors. We empirically validate our method, named Bi-level
Imitation for Traffic Simulation (BITS), with scenarios from two large-scale
driving datasets and show that BITS achieves balanced traffic simulation
performance in realism, diversity, and long-horizon stability. We also explore
ways to evaluate behavior realism and introduce a suite of evaluation metrics
for traffic simulation. Finally, as part of our core contributions, we develop
and open source a software tool that unifies data formats across different
driving datasets and converts scenes from existing datasets into interactive
simulation environments. For additional information and videos, see
https://sites.google.com/view/nvr-bits2022/home",0.29116368,-0.04356985,-0.08085339,C
10502,"Section 8 states the con- ized GMM (TP-GMM) [30], Probabilistic

clusion and proposes further research direc- Movement Primitives (ProMP) [31], Ker-

tions.",7.,"nelized Movement Primitive (KMP) [32],

                        and Gaussian Process (GP) [33].",2022-08-28 18:34:38+00:00,Learning Stable Robotic Skills on Riemannian Manifolds,cs.RO,['cs.RO'],"[arxiv.Result.Author('Matteo Saveriano'), arxiv.Result.Author('Fares J. Abu-Dakka'), arxiv.Result.Author('Ville Kyrki')]","In this paper, we propose an approach to learn stable dynamical systems
evolving on Riemannian manifolds. The approach leverages a data-efficient
procedure to learn a diffeomorphic transformation that maps simple stable
dynamical systems onto complex robotic skills. By exploiting mathematical tools
from differential geometry, the method ensures that the learned skills fulfill
the geometric constraints imposed by the underlying manifolds, such as unit
quaternion (UQ) for orientation and SPD for stiffness matrices, while
preserving the convergence to a given target. The proposed approach is firstly
tested in simulation on a public benchmark, obtained by projecting Cartesian
data into UQ and SPD manifolds, and compared with existing approaches. Apart
from evaluating the approach on a public benchmark, several experiments were
performed on a real robot performing bottle stacking in different conditions
and a drilling task in cooperation with a human operator. The evaluation shows
promising results in terms of learning accuracy and task adaptation
capabilities.",0.024562301,-0.16128004,-0.017608713,C
10503,"Section 8
                                                                           states the conclusion and proposes further research di-
                                                                           rections.",7.,2.,2022-08-28 18:34:38+00:00,Learning Stable Robotic Skills on Riemannian Manifolds,cs.RO,['cs.RO'],"[arxiv.Result.Author('Matteo Saveriano'), arxiv.Result.Author('Fares J. Abu-Dakka'), arxiv.Result.Author('Ville Kyrki')]","In this paper, we propose an approach to learn stable dynamical systems
evolving on Riemannian manifolds. The approach leverages a data-efficient
procedure to learn a diffeomorphic transformation that maps simple stable
dynamical systems onto complex robotic skills. By exploiting mathematical tools
from differential geometry, the method ensures that the learned skills fulfill
the geometric constraints imposed by the underlying manifolds, such as unit
quaternion (UQ) for orientation and symmetric positive definite (SPD) matrices
for impedance, while preserving the convergence to a given target. The proposed
approach is firstly tested in simulation on a public benchmark, obtained by
projecting Cartesian data into UQ and SPD manifolds, and compared with existing
approaches. Apart from evaluating the approach on a public benchmark, several
experiments were performed on a real robot performing bottle stacking in
different conditions and a drilling task in cooperation with a human operator.
The evaluation shows promising results in terms of learning accuracy and task
adaptation capabilities.",0.24219732,-0.36355948,-0.24497166,C
10548,further research is needed.,"We deﬁne three situations when testing                    than those achieved using the other three scenes; hence,
the policy in the real world as [2].","Moreover, in the experiment
                                                                              setup, we assumed that the target orientation is known for
TABLE III: PERFORMANCES IN REAL ENVIRONMENT SETUPS                            the insertion task, which simpliﬁes the task.",2022-08-30 09:38:53+00:00,Sim-to-Real Transfer of Robotic Assembly with Visual Inputs Using CycleGAN and Force Control,cs.RO,['cs.RO'],"[arxiv.Result.Author('Chengjie Yuan'), arxiv.Result.Author('Yunlei Shi'), arxiv.Result.Author('Qian Feng'), arxiv.Result.Author('Chunyang Chang'), arxiv.Result.Author('Zhaopeng Chen'), arxiv.Result.Author('Alois Christian Knoll'), arxiv.Result.Author('Jianwei Zhang')]","Recently, deep reinforcement learning (RL) has shown some impressive
successes in robotic manipulation applications. However, training robots in the
real world is nontrivial owing to sample efficiency and safety concerns.
Sim-to-real transfer is proposed to address the aforementioned concerns but
introduces a new issue called the reality gap. In this work, we introduce a
sim-to-real learning framework for vision-based assembly tasks and perform
training in a simulated environment by employing inputs from a single camera to
address the aforementioned issues. We present a domain adaptation method based
on cycle-consistent generative adversarial networks (CycleGAN) and a force
control transfer approach to bridge the reality gap. We demonstrate that the
proposed framework trained in a simulated environment can be successfully
transferred to a real peg-in-hole setup.",-0.048067942,-0.16926642,-0.18020694,C
10565,"spots, etc., is not shown, and a further study on its limitations
                                                                     is required to use it in safety-critical tasks.","While the approach shows
input sets for real-world problems, like perception, renders         promise, its performance under common distortions like bright
reachability analysis impractical for the DNN used in AV.","Various LiDAR
   Analyzability and veriﬁability are the crucial components of      based geometric algorithms were considered as part of this
the certiﬁcation process of safety-critical systems [30], [31].",2022-08-30 17:15:35+00:00,Verifiable Obstacle Detection,cs.RO,"['cs.RO', 'cs.CV', 'cs.SY', 'eess.SY', 'D.2.4; I.2.9; I.4.8']","[arxiv.Result.Author('Ayoosh Bansal'), arxiv.Result.Author('Hunmin Kim'), arxiv.Result.Author('Simon Yu'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Naira Hovakimyan'), arxiv.Result.Author('Marco Caccamo'), arxiv.Result.Author('Lui Sha')]","Perception of obstacles remains a critical safety concern for autonomous
vehicles. Real-world collisions have shown that the autonomy faults leading to
fatal collisions originate from obstacle existence detection. Open source
autonomous driving implementations show a perception pipeline with complex
interdependent Deep Neural Networks. These networks are not fully verifiable,
making them unsuitable for safety-critical tasks.
  In this work, we present a safety verification of an existing LiDAR based
classical obstacle detection algorithm. We establish strict bounds on the
capabilities of this obstacle detection algorithm. Given safety standards, such
bounds allow for determining LiDAR sensor properties that would reliably
satisfy the standards. Such analysis has as yet been unattainable for neural
network based perception systems. We provide a rigorous analysis of the
obstacle detection system with empirical results based on real-world sensor
data.",0.17150123,0.15828854,-0.054304674,B
10566,"The max curve ﬁts in Figures 6a, 6b and 6c are conservative       Therefore further research is warranted to improve such phys-
linear approximations.","This is not surprising given the low min-         Veriﬁable Algorithms: Despite the necessity of DNN in AV,
imum height bounds determined in Section VI and Figure 6.         this work shows that there is a role for analyzable algorithms.","Ignoring the max curve ﬁts, the actual     ical model backed veriﬁable algorithms.",2022-08-30 17:15:35+00:00,Verifiable Obstacle Detection,cs.RO,"['cs.RO', 'cs.CV', 'cs.SY', 'eess.SY', 'D.2.4; I.2.9; I.4.8']","[arxiv.Result.Author('Ayoosh Bansal'), arxiv.Result.Author('Hunmin Kim'), arxiv.Result.Author('Simon Yu'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Naira Hovakimyan'), arxiv.Result.Author('Marco Caccamo'), arxiv.Result.Author('Lui Sha')]","Perception of obstacles remains a critical safety concern for autonomous
vehicles. Real-world collisions have shown that the autonomy faults leading to
fatal collisions originate from obstacle existence detection. Open source
autonomous driving implementations show a perception pipeline with complex
interdependent Deep Neural Networks. These networks are not fully verifiable,
making them unsuitable for safety-critical tasks.
  In this work, we present a safety verification of an existing LiDAR based
classical obstacle detection algorithm. We establish strict bounds on the
capabilities of this obstacle detection algorithm. Given safety standards, such
bounds allow for determining LiDAR sensor properties that would reliably
satisfy the standards. Such analysis has as yet been unattainable for neural
network based perception systems. We provide a rigorous analysis of the
obstacle detection system with empirical results based on real-world sensor
data.",0.23375356,-0.023822563,-0.04262518,C
10725,"utilize the
                                       further study the obstacle avoidance and mutual-robot-collision            Bernstein polynomials to transform the motion planning into a
                                       avoidance in the motion planning.","In [17], Cichella et al.","Finally, numerical simulation            discrete optimization approximately.",2022-09-02 11:26:12+00:00,Simultaneous Position and Orientation Planning of Nonholonomic Multi-Robot Systems: A Dynamic Vector Field Approach,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Zhongkui Li')]","This paper considers the simultaneous position and orientation planning of
nonholonomic multirobot systems. Different from common researches which only
focus on final position constraints, we model the nonholonomic mobile robot as
a rigid body and introduce the orientation as well as position constraints for
the robot's final states. In other words, robots should not only reach the
specified positions, but also point to the desired orientations simultaneously.
The challenge of this problem lies in the underactuation of full-state motion
planning, since three states need to be planned by mere two control inputs. To
this end, we propose a dynamic vector field (DVF) based on the rigid body
modeling. Specifically, the dynamics of the robot orientation are brought into
the vector field, implying that the vector field is not static on the 2-D plane
anymore, but a dynamic one varying with the attitude angle. Hence, each robot
can move along the integral curve of the DVF to arrive at the desired position,
and in the meantime, the attitude angle can converge to the specified value
following the orientation dynamics. Subsequently, by designing a circular
vector field under the framework of the DVF, we further study the obstacle
avoidance and mutual-robot-collision avoidance in the motion planning. Finally,
numerical simulation examples are provided to verify the effectiveness of the
proposed methodology.",-0.13264725,-0.050318472,0.07871551,A
10755,"In this work, we simultaneously
consider the position and orientation planning of the dual-arm free-ﬂoating space
robot to track the target rotating at diﬀerent speeds within a speciﬁc range,
which is a further study on existing research.","But all these methods
are only applied to the single-arm space robot.","3
3 The Kinematic Model of Dual-Arm Free-Floating

     Space Robot

The dual-arm free-ﬂoating space robot comprises a base satellite and two n-Dof
manipulators.",2022-09-03 14:20:17+00:00,Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Yuxue Cao'), arxiv.Result.Author('Shengjie Wang'), arxiv.Result.Author('Xiang Zheng'), arxiv.Result.Author('Wenke Ma'), arxiv.Result.Author('Xinru Xie'), arxiv.Result.Author('Lei Liu')]","Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the problem, we propose a novel
algorithm, EfficientLPT, to facilitate RL-based methods to improve planning
accuracy efficiently. Our core contributions are constructing a mixed policy
with prior knowledge guidance and introducing infinite norm to build a more
reasonable reward function. Furthermore, our method successfully captures a
rotating object with different spinning speeds.",-0.21223688,0.049536325,0.25329137,A
10874,"We
further study the effects of opportunities and communication                     In robotics, like in many animals, the knowledge is pro-
range on group performance, knowledge spread, and functional                  cedural that deﬁnes the robot’s ability to perform a given
heterogeneity in a group of agents, presenting interesting insights.",and improved group performance in various scenarios.,task by synthesizing information from sensory data [4].,2022-09-07 02:17:04+00:00,KT-BT: A Framework for Knowledge Transfer Through Behavior Trees in Multi-Robot Systems,cs.RO,"['cs.RO', 'cs.AI', 'cs.MA']","[arxiv.Result.Author('Sanjay Sarma Oruganti Venkata'), arxiv.Result.Author('Ramviyas Parasuraman'), arxiv.Result.Author('Ramana Pidaparti')]","Multi-Robot and Multi-Agent Systems demonstrate collective (swarm)
intelligence through systematic and distributed integration of local behaviors
in a group. Agents sharing knowledge about the mission and environment can
enhance performance at individual and mission levels. However, this is
difficult to achieve, partly due to the lack of a generic framework for
transferring part of the known knowledge (behaviors) between agents. This paper
presents a new knowledge representation framework and a transfer strategy
called KT-BT: Knowledge Transfer through Behavior Trees. The KT-BT framework
follows a query-response-update mechanism through an online Behavior Tree
framework, where agents broadcast queries for unknown conditions and respond
with appropriate knowledge using a condition-action-control sub-flow. We embed
a novel grammar structure called stringBT that encodes knowledge, enabling
behavior sharing. We theoretically investigate the properties of the KT-BT
framework in achieving homogeneity of high knowledge across the entire group
compared to a heterogeneous system without the capability of sharing their
knowledge. We extensively verify our framework in a simulated multi-robot
search and rescue problem. The results show successful knowledge transfers and
improved group performance in various scenarios. We further study the effects
of opportunities and communication range on group performance, knowledge
spread, and functional heterogeneity in a group of agents, presenting
interesting insights.",-0.21494704,-0.035936043,-0.1991794,A
10932,"[16] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A Survey
                                                                                      on Mobile Edge Computing: The Communication Perspective,” IEEE
   The work at hand motivates further research in both mobile                         Communications Surveys Tutorials, vol.","Furthermore, the                         autonomous driving,” MCC’12 - Proceedings of the 1st ACM Mobile
released mqtt_client ROS package contributes not only                                 Cloud Computing Workshop, 08 2012.
to automated driving applications, but also to the robotics and
IoT ecosystems as a whole.","19, no.",2022-09-08 08:13:15+00:00,Enabling Connectivity for Automated Mobility: A Novel MQTT-based Interface Evaluated in a 5G Case Study on Edge-Cloud Lidar Object Detection,cs.RO,"['cs.RO', 'cs.NI']","[arxiv.Result.Author('Lennart Reiher'), arxiv.Result.Author('Bastian Lampe'), arxiv.Result.Author('Timo Woopen'), arxiv.Result.Author('Raphael van Kempen'), arxiv.Result.Author('Till Beemelmanns'), arxiv.Result.Author('Lutz Eckstein')]","Enabling secure and reliable high-bandwidth lowlatency connectivity between
automated vehicles and external servers, intelligent infrastructure, and other
road users is a central step in making fully automated driving possible. The
availability of data interfaces, which allow this kind of connectivity, has the
potential to distinguish artificial agents' capabilities in connected,
cooperative, and automated mobility systems from the capabilities of human
operators, who do not possess such interfaces. Connected agents can for example
share data to build collective environment models, plan collective behavior,
and learn collectively from the shared data that is centrally combined. This
paper presents multiple solutions that allow connected entities to exchange
data. In particular, we propose a new universal communication interface which
uses the Message Queuing Telemetry Transport (MQTT) protocol to connect agents
running the Robot Operating System (ROS). Our work integrates methods to assess
the connection quality in the form of various key performance indicators in
real-time. We compare a variety of approaches that provide the connectivity
necessary for the exemplary use case of edge-cloud lidar object detection in a
5G network. We show that the mean latency between the availability of
vehicle-based sensor measurements and the reception of a corresponding object
list from the edge-cloud is below 87 ms. All implemented solutions are made
open-source and free to use. Source code is available at
https://github.com/ika-rwth-aachen/ros-v2x-benchmarking-suite.",0.2736573,0.20777695,0.12105125,B
10987,"Therefore, it is necessary to further study the
                                                                                                               power consumption for the UAV in a 3-D ﬂight and establish
(11)      and         the  speed      V,       the      required        power  P    i  (V  ,    T  i  )  and   the corresponding power consumption model for generic ﬂight.","the ﬂight status of rising and moving forward at
   Then, according to (3), given the corresponding thrust in                                                   the same time.","a              a
   i            i                                                                                                 Prior to modeling the generic power consumption model,
P  d  (V  ,  T  d  )  for  the  i-th        rotor       in     vertical     ascent     and      vertical       the following factors should be known.",2022-09-09 06:05:17+00:00,Modelling Power Consumptions for Multi-rotor UAVs,cs.RO,['cs.RO'],"[arxiv.Result.Author('Hao Gong'), arxiv.Result.Author('Baoqi Huang'), arxiv.Result.Author('Bing Jia'), arxiv.Result.Author('Hansu Dai')]","Unmanned aerial vehicles (UAVs) have various advantages, but their practical
applications are influenced by their limited energy. Therefore, it is important
to manage their power consumption and also important to establish corresponding
power consumption models. However, most of existing works either establish
theoretical power consumption models for fixed-wing UAVs and single-rotor UAVs,
or provide heuristic power consumption models for multi-rotor UAVs without
rigorous mathematical derivations. This paper aims to establish theoretical
power consumption models for multi-rotor UAVs. To be specific, the closed-form
power consumption models for a multi-rotor UAV in three flight statuses, i.e.,
forward flight, vertical ascent and vertical descent, are derived by leveraging
the relationship between single-rotor UAVs and multi-rotor UAVs in terms of
power consumptions. On this basis, a generic flight power consumption model for
the UAV in a three-dimensional (3-D) scenario is obtained. Extensive
experiments are conducted by using DJI M210 and a mobile app made by DJI Mobile
SDK in real scenarios, and confirm the correctness and effectiveness of these
models; in addition, simulations are performed to further investigate the
effect of the rotor numbers on the power consumption for the UAV. The proposed
power consumption models not only reveal how the power consumption of
multi-rotor UAVs are affected by various factors, but also pave the way for
introducing other novel applications.",0.057710942,-0.15932249,0.25992113,C
11040,"The framework reported in this paper presents promis-
ing insights and opens up multiple directions for further research.","In the context of able-bodied humans using our HMI with a simulated robot
arm to perform reaching tasks requiring actuation of a DoF in the wrist, we demonstrated a signiﬁ-
cant improvement in performance, and a better understanding of the robot’s operation, in comparison
with a state of the art data-driven baseline.","Speciﬁcally, we will conduct
experiments with additional human subjects and thoroughly explore the impact of different repre-
sentational choices in the muscle-tendon models.",2022-09-11 21:21:24+00:00,Toward a Framework for Adaptive Impedance Control of an Upper-limb Prosthesis,cs.RO,['cs.RO'],"[arxiv.Result.Author('Laura Ferrante'), arxiv.Result.Author('Mohan Sridharan'), arxiv.Result.Author('Claudio Zito'), arxiv.Result.Author('Dario Farina')]","This paper describes a novel framework for a human-machine interface that can
be used to control an upper-limb prosthesis. The objective is to estimate the
human's motor intent from noisy surface electromyography signals and to execute
the motor intent on the prosthesis (i.e., the robot) even in the presence of
previously unseen perturbations. The framework includes muscle-tendon models
for each degree of freedom, a method for learning the parameter values of
models used to estimate the user's motor intent, and a variable impedance
controller that uses the stiffness and damping values obtained from the muscle
models to adapt the prosthesis' motion trajectory and dynamics. We
experimentally evaluate our framework in the context of able-bodied humans
using a simulated version of the human-machine interface to perform reaching
tasks that primarily actuate one degree of freedom in the wrist, and consider
external perturbations in the form of a uniform force field that pushes the
wrist away from the target. We demonstrate that our framework provides the
desired adaptive performance, and substantially improves performance in
comparison with a data-driven baseline.",-0.33151403,-0.11041224,-0.055416487,A
11041,Our framework provides interesting insights and opens up multiple directions for further research.,"Instead,
our computational framework provides a coherent representation of the dynamics of the MTUs and
that of the robot, leading to improved controllability.","For example, the conditional mutual information between qr and qf given the stiffness or damping
provides an initial indication of the contribution of joint stiffness and damping to system stability.",2022-09-11 21:21:24+00:00,Toward a Framework for Adaptive Impedance Control of an Upper-limb Prosthesis,cs.RO,['cs.RO'],"[arxiv.Result.Author('Laura Ferrante'), arxiv.Result.Author('Mohan Sridharan'), arxiv.Result.Author('Claudio Zito'), arxiv.Result.Author('Dario Farina')]","Adapting upper-limb impedance (i.e., stiffness, damping, inertia) is
essential for humans interacting with dynamic environments for executing
grasping or manipulation tasks. On the other hand, control methods designed for
state-of-the-art upper-limb prostheses infer motor intent from surface
electromyography (sEMG) signals in terms of joint kinematics, but they fail to
infer and use the underlying impedance properties of the limb. We present a
framework that allows a human user to simultaneously control the kinematics,
stiffness, and damping of a simulated robot through wrist's flexion-extension.
The framework includes muscle-tendon units and a forward dynamics block to
estimate the motor intent from sEMG signals, and a variable impedance
controller that implements the estimated intent on the robot, allowing the user
to adapt the robot's kinematics and dynamics online. We evaluate our framework
with 8 able-bodied subjects and an amputee during reaching tasks performed in
free space, and in the presence of unexpected external perturbations that
require adaptation of the wrist impedance to ensure stable interaction with the
environment. We experimentally demonstrate that our approach outperforms a
data-driven baseline in terms of its ability to adapt to external
perturbations, overall controllability, and feedback from participants.",-0.2838349,-0.14827849,0.27489698,A
11166,"This new
   The tools for labeling surgical context based on video          representation of surgical tasks enables the incorporation of
data and automated translation of context labels to motion         surgical context into surgical procedure modeling which is
primitives as well as the aggregated dataset with context and      missing from the previously proposed models such as grammar
motion primitive labels are made publicly available at [url] to    graphs and Hidden Markov models [1] where lower level
facilitate further research and collaboration in this area.","automata with the states representing the surgical context and
                                                                   the transitions representing the motion primitives.",actions are obscured by hidden states.,2022-09-14 05:25:19+00:00,COMPASS: A Formal Framework and Aggregate Dataset for Generalized Surgical Procedure Modeling,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kay Hutchinson'), arxiv.Result.Author('Ian Reyes'), arxiv.Result.Author('Zongyu Li'), arxiv.Result.Author('Homa Alemzadeh')]","Objective: We propose a formal framework for modeling surgical tasks using a
unified set of motion primitives (MPs) as the basic surgical actions to enable
more objective labeling and aggregation of different datasets and training
generalized models for surgical action recognition.
  Methods: We use our framework to create the COntext and Motion Primitive
Aggregate Surgical Set (COMPASS), including six dry-lab surgical tasks from
three publicly-available datasets (JIGSAWS, DESK, and ROSMA) with kinematic and
video data and context and MP labels. Methods for labeling surgical context and
automatic translation to MPs are presented. We propose the Leave-One-Task-Out
(LOTO) cross validation method to evaluate a model's ability to generalize to
an unseen task.
  Results: Our context labeling method achieves near-perfect agreement between
consensus labels from crowd-sourcing and expert surgeons. Segmentation of tasks
to MPs enables the generation of separate left and right transcripts and
significantly improves LOTO performance. We find that MP segmentation models
perform best if trained on tasks with the same context and/or tasks from the
same dataset.
  Conclusion: The proposed framework enables high-quality labeling of surgical
data based on context and fine-grained MPs. Modeling surgical tasks with MPs
enables the aggregation of different datasets for training action recognition
models that can generalize better to unseen tasks than models trained at the
gesture level.
  Significance: Our formal framework and aggregate dataset can support the
development of models and algorithms for surgical process analysis, skill
assessment, error detection, and autonomy.",-0.014206979,0.07967979,-0.27041698,C
11167,"https://github.com/UVA-DSA/COMPASS to facilitate
                                                                   further research and collaboration in this area.","In all these works, the granularity of the actions can   and motion primitive labels are made publicly available at
vary from the sub-gesture to the task level (see Figure 1).","Yet, there is still no formal framework that deﬁnes a
standard set of surgical actions and their relations to gestures                                II.",2022-09-14 05:25:19+00:00,COMPASS: A Formal Framework and Aggregate Dataset for Generalized Surgical Procedure Modeling,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kay Hutchinson'), arxiv.Result.Author('Ian Reyes'), arxiv.Result.Author('Zongyu Li'), arxiv.Result.Author('Homa Alemzadeh')]","Objective: We propose a formal framework for modeling surgical tasks using a
unified set of motion primitives (MPs) as the basic surgical actions to enable
more objective labeling, aggregation of different datasets, and training
generalized models for surgical action recognition.
  Methods: We use our framework to create the COntext and Motion Primitive
Aggregate Surgical Set (COMPASS), including six dry-lab surgical tasks from
three publicly-available datasets (JIGSAWS, DESK, and ROSMA) with kinematic and
video data and context and MP labels. Methods for labeling surgical context and
automatic translation to MPs are presented. We propose the Leave-One-Task-Out
(LOTO) cross validation method to evaluate a model's ability to generalize to
an unseen task.
  Results: Our context labeling method achieves near-perfect agreement between
consensus labels from crowd-sourcing and expert surgeons. Segmentation of tasks
to MPs enables the generation of separate left and right transcripts and
significantly improves LOTO performance. We find that MP segmentation models
perform best if trained on tasks with the same context and/or tasks from the
same dataset.
  Conclusion: The proposed framework enables high quality labeling of surgical
data based on context and fine-grained MPs. Modeling surgical tasks with MPs
enables the aggregation of different datasets for training action recognition
models that can generalize better to unseen tasks than models trained at the
gesture level.
  Significance: Our formal framework and aggregate dataset can support the
development of models and algorithms for surgical process analysis, skill
assessment, error detection, and autonomy.",-0.14295533,-0.04569003,-0.1771648,A
11299,"However, there are still some
problems that need further study.","A two-stage method is also
proposed by using the techniques of regularization and patching.",We have the following ﬁnal remarks.,2022-09-16 11:58:38+00:00,A regularization-patching dual quaternion optimization method for solving the hand-eye calibration problem,cs.RO,"['cs.RO', 'math.OC']","[arxiv.Result.Author('Zhongming Chen'), arxiv.Result.Author('Chen Ling'), arxiv.Result.Author('Liqun Qi'), arxiv.Result.Author('Hong Yan')]","The hand-eye calibration problem is an important application problem in robot
research. Based on the 2-norm of dual quaternion vectors, we propose a new dual
quaternion optimization method for the hand-eye calibration problem. The dual
quaternion optimization problem is decomposed to two quaternion optimization
subproblems. The first quaternion optimization subproblem governs the rotation
of the robot hand. It can be solved efficiently by the eigenvalue decomposition
or singular value decomposition. If the optimal value of the first quaternion
optimization subproblem is zero, then the system is rotationwise noiseless,
i.e., there exists a ``perfect'' robot hand motion which meets all the testing
poses rotationwise exactly. In this case, we apply the regularization technique
for solving the second subproblem to minimize the distance of the translation.
Otherwise we apply the patching technique to solve the second quaternion
optimization subproblem. Then solving the second quaternion optimization
subproblem turns out to be solving a quadratically constrained quadratic
program. In this way, we give a complete description for the solution set of
hand-eye calibration problems. This is new in the hand-eye calibration
literature. The numerical results are also presented to show the efficiency of
the proposed method.",0.34013537,-0.13657704,-0.025586769,C
11346,"This architecture and its
extension to hierarchical learning, PointNet++ [13], enabled                            Feature Extraction
further research on learning local descriptors directly from                         Keypoint Detection
point clouds [14], [6], [7].","{jiaruit, torroba, yipingx, johnf}@kth.se
PointNet from [12], which allowed neural networks to work                                                         Concat
directly with geometric point clouds.","[14] proposed a novel N-tuple loss                      Feature Embedding
based on the contrastive loss from [15], combining point
pair features and global context to learn a globally-aware       Fig.",2022-09-18 15:02:51+00:00,Data-driven Loop Closure Detection in Bathymetric Point Clouds for Underwater SLAM,cs.RO,['cs.RO'],"[arxiv.Result.Author('Jiarui Tan'), arxiv.Result.Author('Ignacio Torroba'), arxiv.Result.Author('Yiping Xie'), arxiv.Result.Author('John Folkesson')]","Simultaneous localization and mapping (SLAM) frameworks for autonomous
navigation rely on robust data association to identify loop closures for
back-end trajectory optimization. In the case of autonomous underwater vehicles
(AUVs) equipped with multibeam echosounders (MBES), data association is
particularly challenging due to the scarcity of identifiable landmarks in the
seabed, the large drift in dead-reckoning navigation estimates to which AUVs
are prone and the low resolution characteristic of MBES data. Deep learning
solutions to loop closure detection have shown excellent performance on data
from more structured environments. However, their transfer to the seabed domain
is not immediate and efforts to port them are hindered by the lack of
bathymetric datasets. Thus, in this paper we propose a neural network
architecture aimed to showcase the potential of adapting such techniques to
correspondence matching in bathymetric data. We train our framework on real
bathymetry from an AUV mission and evaluate its performance on the tasks of
loop closure detection and coarse point cloud alignment. Finally, we show its
potential against a more traditional method and release both its implementation
and the dataset used.",0.077572465,0.37693068,-0.3053841,B
11347,"The module Tj is deﬁned as:                    being considered as a further research topic, to enable mul-
                                                                   tiple agents to perform concurrent transitions and is beyond
Tj {pj , ej , qj }                                       (9)       the scope of the current work.","Multi-port modules are currently
and Pin ∩ Pout = ∅.","We have to also note here
                                                                   that the additional effort in casting the problem as a module
where pj ∈ Pin, ej ∈ ES and qj ∈ Pout.",2022-09-18 17:27:08+00:00,Autonomous Task Planning for Heterogeneous Multi-Agent Systems,cs.RO,"['cs.RO', 'cs.FL']","[arxiv.Result.Author('Anatoli A. Tziola'), arxiv.Result.Author('Savvas G. Loizou')]","This paper presents a solution to the automatic task planning problem for
multi-agent systems. A formal framework is developed based on the
Nondeterministic Finite Automata with $\epsilon$-transitions, where given the
capabilities, constraints and failure modes of the agents involved, an initial
state of the system and a task specification, an optimal solution is generated
that satisfies the system constraints and the task specification. The resulting
solution is guaranteed to be complete and optimal; moreover a heuristic
solution that offers significant reduction of the computational requirements
while relaxing the completeness and optimality requirements is proposed. The
constructed system model is independent from the initial condition and the task
specification, alleviating the need to repeat the costly pre-processing cycle
for solving other scenarios, while allowing the incorporation of failure modes
on-the-fly. Two case studies are provided: a simple one to showcase the
concepts of the proposed methodology and a more elaborate one to demonstrate
the effectiveness and validity of the methodology.",0.18887334,-0.20503566,0.11375391,C
11367,"To promote further research,
                                        Province, China, 215123.                                                         we make our algorithm open-source.","2Peng Cheng Laboratory, No.2, Xingke 1st Street, Nanshan, Shenzhen,
                                        China, 518055, cuijq@pcl.ac.cn                                               3) A series of experiments on the EuRoC and TUM-VI
                                           3Institute of Nano-Tech and Nano-Bionics, Chinese Academy of Sci-             public datasets are performed to verify the performance
                                        ences, 398 Ruoshui Road, Suzhou Industrial Park, Suzhou City, Jiangsu            of the proposed method.",II.,2022-09-19 08:03:03+00:00,LGC-Net: A Lightweight Gyroscope Calibration Network for Efficient Attitude Estimation,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Yaohua Liu'), arxiv.Result.Author('Wei Liang'), arxiv.Result.Author('Jinqiang Cui')]","This paper presents a lightweight, efficient calibration neural network model
for denoising low-cost microelectromechanical system (MEMS) gyroscope and
estimating the attitude of a robot in real-time. The key idea is extracting
local and global features from the time window of inertial measurement units
(IMU) measurements to regress the output compensation components for the
gyroscope dynamically. Following a carefully deduced mathematical calibration
model, LGC-Net leverages the depthwise separable convolution to capture the
sectional features and reduce the network model parameters. The Large kernel
attention is designed to learn the long-range dependencies and feature
representation better. The proposed algorithm is evaluated in the EuRoC and
TUM-VI datasets and achieves state-of-the-art on the (unseen) test sequences
with a more lightweight model structure. The estimated orientation with our
LGC-Net is comparable with the top-ranked visual-inertial odometry systems,
although it does not adopt vision sensors. We make our method open-source at:
https://github.com/huazai665/LGC-Net",0.1559436,-0.057312615,-0.077177875,C
11393,"Future
directions will include further studying and expanding the types of robotic control tasks ActPC can tackle, investigating
how ActPC can tackle the partially observable Markov decision process formulation of robotic control environments,
and crucially generalizing ActPC to deal with the domain adaptation problem inherent to creating an embodied robotic
agent for tackling real-world tasks.","ActPC, while potentially
powerful as our results indicate, is not without its limitations including other biological elements that are still missing,
such as neural communication through spike trains (although related work has demonstrated that neural generative
coding can be reformulated as a spiking neural system [38], a direction we intend to explore in future work).","References

   [1] S. B. Furber, “Brain-inspired computing,” IET Computers & Digital Techniques, vol.",2022-09-19 16:49:32+00:00,Active Predicting Coding: Brain-Inspired Reinforcement Learning for Sparse Reward Robotic Control Problems,cs.RO,"['cs.RO', 'cs.LG', 'cs.NE']","[arxiv.Result.Author('Alexander Ororbia'), arxiv.Result.Author('Ankur Mali')]","In this article, we propose a backpropagation-free approach to robotic
control through the neuro-cognitive computational framework of neural
generative coding (NGC), designing an agent built completely from powerful
predictive coding/processing circuits that facilitate dynamic, online learning
from sparse rewards, embodying the principles of planning-as-inference.
Concretely, we craft an adaptive agent system, which we call active predictive
coding (ActPC), that balances an internally-generated epistemic signal (meant
to encourage intelligent exploration) with an internally-generated instrumental
signal (meant to encourage goal-seeking behavior) to ultimately learn how to
control various simulated robotic systems as well as a complex robotic arm
using a realistic robotics simulator, i.e., the Surreal Robotics Suite, for the
block lifting task and can pick-and-place problems. Notably, our experimental
results demonstrate that our proposed ActPC agent performs well in the face of
sparse (extrinsic) reward signals and is competitive with or outperforms
several powerful backprop-based RL approaches.",-0.21074632,-0.055252947,-0.10046849,A
11404,"The map encoder takes the predicted maps and turns
                                        trained modules, opening doors to further research for tasks               it into a compact representation which is shared with the
                                        that remain elusive with classical methods.","This map comes from another
                                        be seamlessly combined with other learning methods to learn                differentiable module we call Differentiable Map Predictor
                                        end-to-end, resulting in a better solution than the individually           (DMP).",other agents using a Graph Neural Network (GNN) [11].,2022-09-19 18:34:49+00:00,D2CoPlan: A Differentiable Decentralized Planner for Multi-Robot Coverage,cs.RO,['cs.RO'],"[arxiv.Result.Author('Vishnu Dutt Sharma'), arxiv.Result.Author('Lifeng Zhou'), arxiv.Result.Author('Pratap Tokekar')]","Centralized approaches for multi-robot coverage planning problems suffer from
the lack of scalability. Learning-based distributed algorithms provide a
scalable avenue in addition to bringing data-oriented feature generation
capabilities to the table, allowing integration with other learning-based
approaches. To this end, we present a learning-based, differentiable
distributed coverage planner (D2COPL A N) which scales efficiently in runtime
and number of agents compared to the expert algorithm, and performs on par with
the classical distributed algorithm. In addition, we show that D2COPlan can be
seamlessly combined with other learning methods to learn end-to-end, resulting
in a better solution than the individually trained modules, opening doors to
further research for tasks that remain elusive with classical methods.",0.062376216,0.1854533,-0.24520734,B
11464,"the COVID-19          of both aspects, further research should go into how the different
pandemic.","To address the limitations
tions where a global event restricts mobility, e.g.",methods can complement each other.,2022-09-20 15:20:09+00:00,Exploring Human-robot Interaction by Simulating Robots,cs.RO,"['cs.RO', 'cs.HC']","[arxiv.Result.Author('Khaled Kassem'), arxiv.Result.Author('Florian Michahelles')]","As collaborative robots enter industrial shop floors, logistics, and
manufacturing, rapid and flexible evaluation of human-machine interaction has
become more important. The availability of consumer headsets for virtual and
augmented realities has lowered the barrier of entry for virtual environments.
In this paper, we explore the different aspects of using such environments for
simulating robots in user studies and present the first findings from our own
research work. Finally, we recommend directions for applying and using
simulation in human-robot interaction.",0.2863968,-0.21932828,-0.0072694235,C
11499,"A prerequisite for planning is to acquire
                                        and points to promising new directions for further research.","For              about the possible long-term effects of actions on future states,
                                        POMDP algorithm designers, the survey provides new insights                observations, and reward using state transition, observation,
                                        into the unique challenges of applying POMDPs to robot systems             and reward models.",these models.,2022-09-21 13:24:20+00:00,Partially Observable Markov Decision Processes in Robotics: A Survey,cs.RO,"['cs.RO', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Mikko Lauri'), arxiv.Result.Author('David Hsu'), arxiv.Result.Author('Joni Pajarinen')]","Noisy sensing, imperfect control, and environment changes are defining
characteristics of many real-world robot tasks. The partially observable Markov
decision process (POMDP) provides a principled mathematical framework for
modeling and solving robot decision and control tasks under uncertainty. Over
the last decade, it has seen many successful applications, spanning
localization and navigation, search and tracking, autonomous driving,
multi-robot systems, manipulation, and human-robot interaction. This survey
aims to bridge the gap between the development of POMDP models and algorithms
at one end and application to diverse robot decision tasks at the other. It
analyzes the characteristics of these tasks and connects them with the
mathematical and algorithmic properties of the POMDP framework for effective
modeling and solution. For practitioners, the survey provides some of the key
task characteristics in deciding when and how to apply POMDPs to robot tasks
successfully. For POMDP algorithm designers, the survey provides new insights
into the unique challenges of applying POMDPs to robot systems and points to
promising new directions for further research.",-0.13344982,-0.079133615,0.008658929,A
11848,"However, the difﬁculties
                                        imitation learning techniques outperform traditional imitation        in sim-to-real transfer and ensuring safety remain open for
                                        learning methods and can greatly improve the performance of           further study [1].",We show that interactive           off between safety and performance.,"reinforcement learning policies by bootstrapping thanks to its
                                        better sample efﬁciency.",2022-09-29 19:49:13+00:00,A Benchmark Comparison of Imitation Learning-based Control Policies for Autonomous Racing,cs.RO,['cs.RO'],"[arxiv.Result.Author('Xiatao Sun'), arxiv.Result.Author('Mingyan Zhou'), arxiv.Result.Author('Zhijun Zhuang'), arxiv.Result.Author('Shuo Yang'), arxiv.Result.Author('Johannes Betz'), arxiv.Result.Author('Rahul Mangharam')]","Autonomous racing with scaled race cars has gained increasing attention as an
effective approach for developing perception, planning and control algorithms
for safe autonomous driving at the limits of the vehicle's handling. To train
agile control policies for autonomous racing, learning-based approaches largely
utilize reinforcement learning, albeit with mixed results. In this study, we
benchmark a variety of imitation learning policies for racing vehicles that are
applied directly or for bootstrapping reinforcement learning both in simulation
and on scaled real-world environments. We show that interactive imitation
learning techniques outperform traditional imitation learning methods and can
greatly improve the performance of reinforcement learning policies by
bootstrapping thanks to its better sample efficiency. Our benchmarks provide a
foundation for future research on autonomous racing using Imitation Learning
and Reinforcement Learning.",-0.12787625,-0.15001066,-0.1949561,A
11897,"To further study the importance of a learned dense reward in online RL, we compare to using
ground-truth task sparse reward coupled with VIP’s visual representation, VIP (Sparse).","visual reward and representation for learning a closed-loop
policy.","The RL
algorithm we use is natural policy gradient (NPG) (Kakade, 2001).",2022-09-30 18:14:07+00:00,VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yecheng Jason Ma'), arxiv.Result.Author('Shagun Sodhani'), arxiv.Result.Author('Dinesh Jayaraman'), arxiv.Result.Author('Osbert Bastani'), arxiv.Result.Author('Vikash Kumar'), arxiv.Result.Author('Amy Zhang')]","Reward and representation learning are two long-standing challenges for
learning an expanding set of robot manipulation skills from sensory
observations. Given the inherent cost and scarcity of in-domain, task-specific
robot data, learning from large, diverse, offline human videos has emerged as a
promising path towards acquiring a generally useful visual representation for
control; however, how these human videos can be used for general-purpose reward
learning remains an open question. We introduce
$\textbf{V}$alue-$\textbf{I}$mplicit $\textbf{P}$re-training (VIP), a
self-supervised pre-trained visual representation capable of generating dense
and smooth reward functions for unseen robotic tasks. VIP casts representation
learning from human videos as an offline goal-conditioned reinforcement
learning problem and derives a self-supervised dual goal-conditioned
value-function objective that does not depend on actions, enabling pre-training
on unlabeled human videos. Theoretically, VIP can be understood as a novel
implicit time contrastive objective that generates a temporally smooth
embedding, enabling the value function to be implicitly defined via the
embedding distance, which can then be used to construct the reward for any
goal-image specified downstream task. Trained on large-scale Ego4D human videos
and without any fine-tuning on in-domain, task-specific data, VIP's frozen
representation can provide dense visual reward for an extensive set of
simulated and $\textbf{real-robot}$ tasks, enabling diverse reward-based visual
control methods and significantly outperforming all prior pre-trained
representations. Notably, VIP can enable simple, $\textbf{few-shot}$ offline RL
on a suite of real-world robot tasks with as few as 20 trajectories.",0.045899782,-0.06414485,-0.20818418,C
12053,"Therefore, the HCM technique is more suitable for
    To further study the potential of the proposed HCM                        untethered robots.","is also proportional to the weight and the light weight of the
                                                                              tethered robot leads to more vibration and collision that make
B. Tethered dual-HCM biped robot                                              the robot harder to maintain a stable motion thus lowering the
                                                                              speed.","It is also interesting to see that the tethered
strategy in soft robotics, we design and fabricate a similar                  robotic gait with only one HCM buckling is faster than the
tethered biped robot as in Fig 6.",2022-10-05 15:53:59+00:00,Fast Untethered Soft Robotic Crawler with Elastic Instability,cs.RO,['cs.RO'],"[arxiv.Result.Author('Zechen Xiong'), arxiv.Result.Author('Yufeng Su'), arxiv.Result.Author('Hod Lipson')]","High-speed locomotion of animals gives them tremendous advantages in
exploring, hunting, and escaping from predators in varying environments.
Enlightened by the fast-running gait of mammals like cheetahs and wolves, we
designed and fabricated a single-servo-driving untethered soft robot that is
capable of galloping at a speed of 313 mm/s or 1.56 body length per second
(BL/s), 5.2 times and 2.6 times faster than the reported fastest predecessors
in mm/s and BL/s, respectively, in literature. An in-plane prestressed hair
clip mechanism (HCM) made up of semi-rigid materials like plastic is used as
the supporting chassis, the compliant spine, and the muscle force amplifier of
the robot at the same time, enabling the robot to be rapid and strong. The
influence of factors including actuation frequency, substrates,
tethering/untethering, and symmetric/asymmetric actuation is explored with
experiments. Based on previous work, this paper further demonstrated the
potential of HCM in addressing the speed problem of soft robots.",-0.29138958,-0.17539254,0.30566996,A
12081,"Thus, further research in realistic cost functions is                         pp.","Furthermore, the ratio of different sources is                         Engineers, Part I: Journal of Systems and Control Engineering, 2011,
unclear.",831–849.,2022-10-06 08:37:52+00:00,Meta Reinforcement Learning for Optimal Design of Legged Robots,cs.RO,"['cs.RO', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Álvaro Belmonte-Baeza'), arxiv.Result.Author('Joonho Lee'), arxiv.Result.Author('Giorgio Valsecchi'), arxiv.Result.Author('Marco Hutter')]","The process of robot design is a complex task and the majority of design
decisions are still based on human intuition or tedious manual tuning. A more
informed way of facing this task is computational design methods where design
parameters are concurrently optimized with corresponding controllers. Existing
approaches, however, are strongly influenced by predefined control rules or
motion templates and cannot provide end-to-end solutions. In this paper, we
present a design optimization framework using model-free meta reinforcement
learning, and its application to the optimizing kinematics and actuator
parameters of quadrupedal robots. We use meta reinforcement learning to train a
locomotion policy that can quickly adapt to different designs. This policy is
used to evaluate each design instance during the design optimization. We
demonstrate that the policy can control robots of different designs to track
random velocity commands over various rough terrains. With controlled
experiments, we show that the meta policy achieves close-to-optimal performance
for each design instance after adaptation. Lastly, we compare our results
against a model-based baseline and show that our approach allows higher
performance while not being constrained by predefined motions or gait patterns.",0.17445561,-0.35637534,0.17509772,C
12371,"To accelerate further research in         long-horizon rearrangements such as “put the blocks into a smiley
                                        this setting, we accordingly provide our associated recipe, dataset,     face with green eyes” that require multiple minutes of precise
                                        models, hardware environment description, simulated analogue             coordinated control (Figure 5, left).","1, a) that by certain metrics operate at an order of magnitude     natural-language feedback, the robot can accomplish complex
                                        larger scale than prior works.","We also find that real-time
                                        environment, and a research benchmark for language conditioned           language competency unlocks new capabilities like simultaneous,
                                        manipulation (Fig.",2022-10-12 17:03:41+00:00,Interactive Language: Talking to Robots in Real Time,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Corey Lynch'), arxiv.Result.Author('Ayzaan Wahid'), arxiv.Result.Author('Jonathan Tompson'), arxiv.Result.Author('Tianli Ding'), arxiv.Result.Author('James Betker'), arxiv.Result.Author('Robert Baruch'), arxiv.Result.Author('Travis Armstrong'), arxiv.Result.Author('Pete Florence')]","We present a framework for building interactive, real-time, natural
language-instructable robots in the real world, and we open source related
assets (dataset, environment, benchmark, and policies). Trained with behavioral
cloning on a dataset of hundreds of thousands of language-annotated
trajectories, a produced policy can proficiently execute an order of magnitude
more commands than previous works: specifically we estimate a 93.5% success
rate on a set of 87,000 unique natural language strings specifying raw
end-to-end visuo-linguo-motor skills in the real world. We find that the same
policy is capable of being guided by a human via real-time language to address
a wide range of precise long-horizon rearrangement goals, e.g. ""make a smiley
face out of blocks"". The dataset we release comprises nearly 600,000
language-labeled trajectories, an order of magnitude larger than prior
available datasets. We hope the demonstrated results and associated assets
enable further advancement of helpful, capable, natural-language-interactable
robots. See videos at https://interactive-language.github.io.",-0.20838754,-0.014372896,-0.13359329,A
12372,"θ

                                                                                                                          V. LANGUAGE-TABLE: DATASETS AND ENVIRONMENT

                                                                                                                          To facilitate further research in language-conditioned
                                                                                                                       visuomotor learning, we release Language-Table, which consists
                                                                                                                       of (i) a suite of datasets and (ii) a simulated multi-task language
                                                                                                                       conditioned control environment and benchmark.","as in [9], [49].",Dataset.,2022-10-12 17:03:41+00:00,Interactive Language: Talking to Robots in Real Time,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Corey Lynch'), arxiv.Result.Author('Ayzaan Wahid'), arxiv.Result.Author('Jonathan Tompson'), arxiv.Result.Author('Tianli Ding'), arxiv.Result.Author('James Betker'), arxiv.Result.Author('Robert Baruch'), arxiv.Result.Author('Travis Armstrong'), arxiv.Result.Author('Pete Florence')]","We present a framework for building interactive, real-time, natural
language-instructable robots in the real world, and we open source related
assets (dataset, environment, benchmark, and policies). Trained with behavioral
cloning on a dataset of hundreds of thousands of language-annotated
trajectories, a produced policy can proficiently execute an order of magnitude
more commands than previous works: specifically we estimate a 93.5% success
rate on a set of 87,000 unique natural language strings specifying raw
end-to-end visuo-linguo-motor skills in the real world. We find that the same
policy is capable of being guided by a human via real-time language to address
a wide range of precise long-horizon rearrangement goals, e.g. ""make a smiley
face out of blocks"". The dataset we release comprises nearly 600,000
language-labeled trajectories, an order of magnitude larger than prior
available datasets. We hope the demonstrated results and associated assets
enable further advancement of helpful, capable, natural-language-interactable
robots. See videos at https://interactive-language.github.io.",-0.14552554,-0.20059362,-0.316755,A
12445,"All our software and datasets

                                        are freely available for further research.","Finally, we benchmark the

                                        impact of the critical design parameters - the number of

                                        cameras and the overlap between their FoV that deﬁne the

                                        camera conﬁguration for SLAM.",I.,2022-10-13 19:42:04+00:00,Design and Evaluation of a Generic Visual SLAM Framework for Multi-Camera Systems,cs.RO,['cs.RO'],"[arxiv.Result.Author('Pushyami Kaveti'), arxiv.Result.Author('Arvind Thamilchelvan'), arxiv.Result.Author('Hanumant Singh')]","Multi-camera systems have been shown to improve the accuracy and robustness
of SLAM estimates, yet state-of-the-art SLAM systems predominantly support
monocular or stereo setups. This paper presents a generic sparse visual SLAM
framework capable of running on any number of cameras and in any arrangement.
Our SLAM system uses the generalized camera model, which allows us to represent
an arbitrary multi-camera system as a single imaging device. Additionally, it
takes advantage of the overlapping fields of view (FoV) by extracting
cross-matched features across cameras in the rig. This limits the linear rise
in the number of features with the number of cameras and keeps the
computational load in check while enabling an accurate representation of the
scene. We evaluate our method in terms of accuracy, robustness, and run time on
indoor and outdoor datasets that include challenging real-world scenarios such
as narrow corridors, featureless spaces, and dynamic objects. We show that our
system can adapt to different camera configurations and allows real-time
execution for typical robotic applications. Finally, we benchmark the impact of
the critical design parameters - the number of cameras and the overlap between
their FoV that define the camera configuration for SLAM. All our software and
datasets are freely available for further research.",0.087530896,0.26610458,-0.005175459,B
12468,"Concave ﬁelds, 2.5D terrains
with a brute force optimizer; the route planner, with three     and capacitated vehicles provide a good starting point for
                                                                discussion and further research.","Future developments are supported and maintained
has four modules, which are: the headlands generator, with      by the ﬁrst author of this paper, with the collaboration of
a constant width headlands generator; the swath generator,      the open-source community.","ACKNOWLEDGEMENT                                   [18] S. de Bruin, P. Lerink, I. J.",2022-10-14 14:09:29+00:00,Fields2Cover: An open-source coverage path planning library for unmanned agricultural vehicles,cs.RO,"['cs.RO', 'cs.CG']","[arxiv.Result.Author('Gonzalo Mier'), arxiv.Result.Author('João Valente'), arxiv.Result.Author('Sytze de Bruin')]","This paper describes Fields2Cover, a novel open source library for coverage
path planning (CPP) for agricultural vehicles. While there are several CPP
solutions nowadays, there have been limited efforts to unify them into an open
source library and provide benchmarking tools to compare their performance.
Fields2Cover provides a framework for planning coverage paths, developing novel
techniques, and benchmarking state-of-the-art algorithms. The library features
a modular and extensible architecture that supports various vehicles and can be
used for a variety of applications, including farms. Its core modules are: a
headland generator, a swath generator, a route planner and a path planner. An
interface to the Robot Operating System (ROS) is also supplied as an add-on. In
this paper, the functionalities of the library for planning a coverage path in
agriculture are demonstrated using 8 state-of-the-art methods and 7 objective
functions in simulation and field experiments.",0.070621006,0.111051455,0.2760628,B
12497,"However, regression meth-
tropy minimization as the surrogate objective for the patrolling       ods have been shown to be very sensitive to acquisition policies

                                                                   14
and require further research.","This improvement in entropy mini-
ment Learning framework that optimizes a convolutional policy          mization is translated into lower model errors and higher detec-
based on a visual state of the informative scenario using the en-      tion rates of contamination peaks.",The convergence of the regression                            https://doi.org/10.1016/j.robot.2020.103691.,2022-10-12 07:33:46+00:00,Censored Deep Reinforcement Patrolling with Information Criterion for Monitoring Large Water Resources using Autonomous Surface Vehicles,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Samuel Yanes Luis'), arxiv.Result.Author('Daniel Gutiérrez Reina'), arxiv.Result.Author('Sergio Toral Marín')]","Monitoring and patrolling large water resources is a major challenge for
conservation. The problem of acquiring data of an underlying environment that
usually changes within time involves a proper formulation of the information.
The use of Autonomous Surface Vehicles equipped with water quality sensor
modules can serve as an early-warning system agents for contamination
peak-detection, algae blooms monitoring, or oil-spill scenarios. In addition to
information gathering, the vehicle must plan routes that are free of obstacles
on non-convex maps. This work proposes a framework to obtain a collision-free
policy that addresses the patrolling task for static and dynamic scenarios.
Using information gain as a measure of the uncertainty reduction over data, it
is proposed a Deep Q-Learning algorithm improved by a Q-Censoring mechanism for
model-based obstacle avoidance. The obtained results demonstrate the usefulness
of the proposed algorithm for water resource monitoring for static and dynamic
scenarios. Simulations showed the use of noise-networks are a good choice for
enhanced exploration, with 3 times less redundancy in the paths. Previous
coverage strategies are also outperformed both in the accuracy of the obtained
contamination model by a 13% on average and by a 37% in the detection of
dangerous contamination peaks. Finally, these results indicate the
appropriateness of the proposed framework for monitoring scenarios with
autonomous vehicles.",0.037107997,-0.019570779,-0.07246841,C
12498,"It can
be seen, that for every component and task only very few KG-based approaches
have been developed, suggesting much potential for further research in this area.",2.,"5 Conclusions

While AD has made tremendous progress over the last few years, many questions
remain still unanswered.",2022-09-30 15:47:19+00:00,A Survey on Knowledge Graph-based Methods for Automated Driving,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.SI']","[arxiv.Result.Author('Juergen Luettin'), arxiv.Result.Author('Sebastian Monka'), arxiv.Result.Author('Cory Henson'), arxiv.Result.Author('Lavdim Halilaj')]","Automated driving is one of the most active research areas in computer
science. Deep learning methods have made remarkable breakthroughs in machine
learning in general and in automated driving (AD)in particular. However, there
are still unsolved problems to guarantee reliability and safety of automated
systems, especially to effectively incorporate all available information and
knowledge in the driving task. Knowledge graphs (KG) have recently gained
significant attention from both industry and academia for applications that
benefit by exploiting structured, dynamic, and relational data. The complexity
of graph-structured data with complex relationships and inter-dependencies
between objects has posed significant challenges to existing machine learning
algorithms. However, recent progress in knowledge graph embeddings and graph
neural networks allows to applying machine learning to graph-structured data.
Therefore, we motivate and discuss the potential benefit of KGs applied to the
main tasks of AD including 1) ontologies 2) perception, 3) scene understanding,
4) motion planning, and 5) validation. Then, we survey, analyze and categorize
ontologies and KG-based approaches for AD. We discuss current research
challenges and propose promising future research directions for KG-based
solutions for AD.",0.10651961,-0.23084536,-0.091471285,C
12593,"To support
                                        further research in this domain, we present the INSANE data sets       Existing Unmanned Aerial Vehicle (UAV) data sets focus
                                        - a collection of versatile Micro Aerial Vehicle (MAV) data sets    on isolated topics such as Visual Inertial Odometry (VIO),
                                        for cross-environment localization.",and robust localization being a key prerequisite.,"The data sets provide various   indoor navigation, or vehicle control with aspects to energy
                                        scenarios with multiple stages of difﬁculty for localization meth-  efﬁciency as presented by [1].",2022-10-17 14:06:17+00:00,INSANE: Cross-Domain UAV Data Sets with Increased Number of Sensors for developing Advanced and Novel Estimators,cs.RO,['cs.RO'],"[arxiv.Result.Author('Christian Brommer'), arxiv.Result.Author('Alessandro Fornasier'), arxiv.Result.Author('Martin Scheiber'), arxiv.Result.Author('Jeff Delaune'), arxiv.Result.Author('Roland Brockers'), arxiv.Result.Author('Jan Steinbrener'), arxiv.Result.Author('Stephan Weiss')]","For real-world applications, autonomous mobile robotic platforms must be
capable of navigating safely in a multitude of different and dynamic
environments with accurate and robust localization being a key prerequisite. To
support further research in this domain, we present the INSANE data sets - a
collection of versatile Micro Aerial Vehicle (MAV) data sets for
cross-environment localization. The data sets provide various scenarios with
multiple stages of difficulty for localization methods. These scenarios range
from trajectories in the controlled environment of an indoor motion capture
facility, to experiments where the vehicle performs an outdoor maneuver and
transitions into a building, requiring changes of sensor modalities, up to
purely outdoor flight maneuvers in a challenging Mars analog environment to
simulate scenarios which current and future Mars helicopters would need to
perform. The presented work aims to provide data that reflects real-world
scenarios and sensor effects. The extensive sensor suite includes various
sensor categories, including multiple Inertial Measurement Units (IMUs) and
cameras. Sensor data is made available as raw measurements and each data set
provides highly accurate ground truth, including the outdoor experiments where
a dual Real-Time Kinematic (RTK) Global Navigation Satellite System (GNSS)
setup provides sub-degree and centimeter accuracy (1-sigma). The sensor suite
also includes a dedicated high-rate IMU to capture all the vibration dynamics
of the vehicle during flight to support research on novel machine
learning-based sensor signal enhancement methods for improved localization. The
data sets and post-processing tools are available at:
https://sst.aau.at/cns/datasets",-0.025651153,0.40972435,0.13349907,B
12614,"[3] formulate the problem as constrained optimization,
                                        In addition to Neural Contact Fields, we also release our        where different constraints must be deﬁned to solve for
                                        YCB-Extrinsic-Contact dataset of simulated extrinsic contact     parameters that localize external contact, such as position
                                        interactions to enable further research in this area.",al.,"Project    of a point, direction of a line, or normal direction of a plane.",2022-10-17 17:52:43+00:00,Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Carolina Higuera'), arxiv.Result.Author('Siyuan Dong'), arxiv.Result.Author('Byron Boots'), arxiv.Result.Author('Mustafa Mukadam')]","We present Neural Contact Fields, a method that brings together neural fields
and tactile sensing to address the problem of tracking extrinsic contact
between object and environment. Knowing where the external contact occurs is a
first step towards methods that can actively control it in facilitating
downstream manipulation tasks. Prior work for localizing environmental contacts
typically assume a contact type (e.g. point or line), does not capture
contact/no-contact transitions, and only works with basic geometric-shaped
objects. Neural Contact Fields are the first method that can track arbitrary
multi-modal extrinsic contacts without making any assumptions about the contact
type. Our key insight is to estimate the probability of contact for any 3D
point in the latent space of object shapes, given vision-based tactile inputs
that sense the local motion resulting from the external contact. In
experiments, we find that Neural Contact Fields are able to localize multiple
contact patches without making any assumptions about the geometry of the
contact, and capture contact/no-contact transitions for known categories of
objects with unseen shapes in unseen environment configurations. In addition to
Neural Contact Fields, we also release our YCB-Extrinsic-Contact dataset of
simulated extrinsic contact interactions to enable further research in this
area. Project repository: https://github.com/carolinahiguera/NCF",-0.070943356,-0.055606715,-0.07088162,A
12615,"This       sensing for packing four basic objects shapes in a box,
dataset provides a simulation benchmark that we hope will           with the hypothesis that different error directions will result
enable further research in extrinsic contact perception.","Re-
from the TACTO simulator [6], and extrinsic contact ground-         garding their usage in manipulation tasks, [9] uses tactile
truth per time step of each training/testing trajectory.",in distinguishable tactile imprints.,2022-10-17 17:52:43+00:00,Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Carolina Higuera'), arxiv.Result.Author('Siyuan Dong'), arxiv.Result.Author('Byron Boots'), arxiv.Result.Author('Mustafa Mukadam')]","We present Neural Contact Fields, a method that brings together neural fields
and tactile sensing to address the problem of tracking extrinsic contact
between object and environment. Knowing where the external contact occurs is a
first step towards methods that can actively control it in facilitating
downstream manipulation tasks. Prior work for localizing environmental contacts
typically assume a contact type (e.g. point or line), does not capture
contact/no-contact transitions, and only works with basic geometric-shaped
objects. Neural Contact Fields are the first method that can track arbitrary
multi-modal extrinsic contacts without making any assumptions about the contact
type. Our key insight is to estimate the probability of contact for any 3D
point in the latent space of object shapes, given vision-based tactile inputs
that sense the local motion resulting from the external contact. In
experiments, we find that Neural Contact Fields are able to localize multiple
contact patches without making any assumptions about the geometry of the
contact, and capture contact/no-contact transitions for known categories of
objects with unseen shapes in unseen environment configurations. In addition to
Neural Contact Fields, we also release our YCB-Extrinsic-Contact dataset of
simulated extrinsic contact interactions to enable further research in this
area. Project repository: https://github.com/carolinahiguera/NCF",-0.22890809,-0.031874996,-0.1475012,A
12616,"We show key snapshots of a test
   To evaluate our NCF and to encourage further research in     trajectory collected with unseen mugs, bottles, bowls, and
extrinsic contact modeling through tactile sensing, we open     new scenarios in simulation.","4 we show qualitative results of our pipeline
                                                                tracking extrinsic contact.","From a qualitative point of view,
source the YCB-Extrinsic-Contact dataset.",2022-10-17 17:52:43+00:00,Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing,cs.RO,"['cs.RO', 'cs.CV']","[arxiv.Result.Author('Carolina Higuera'), arxiv.Result.Author('Siyuan Dong'), arxiv.Result.Author('Byron Boots'), arxiv.Result.Author('Mustafa Mukadam')]","We present Neural Contact Fields, a method that brings together neural fields
and tactile sensing to address the problem of tracking extrinsic contact
between object and environment. Knowing where the external contact occurs is a
first step towards methods that can actively control it in facilitating
downstream manipulation tasks. Prior work for localizing environmental contacts
typically assume a contact type (e.g. point or line), does not capture
contact/no-contact transitions, and only works with basic geometric-shaped
objects. Neural Contact Fields are the first method that can track arbitrary
multi-modal extrinsic contacts without making any assumptions about the contact
type. Our key insight is to estimate the probability of contact for any 3D
point in the latent space of object shapes, given vision-based tactile inputs
that sense the local motion resulting from the external contact. In
experiments, we find that Neural Contact Fields are able to localize multiple
contact patches without making any assumptions about the geometry of the
contact, and capture contact/no-contact transitions for known categories of
objects with unseen shapes in unseen environment configurations. In addition to
Neural Contact Fields, we also release our YCB-Extrinsic-Contact dataset of
simulated extrinsic contact interactions to enable further research in this
area. Project repository: https://github.com/carolinahiguera/NCF",-0.12857759,-0.068906374,-0.050162397,A
12626,"These ﬁndings provide insights for           (especially when visualization is in trajectory form) over both
further research into human-centered visual design options in        visual or haptic only in the shared navigation task.","Our results
path, possibly because it provided speciﬁc corrections on what       revealed that more operators preferred multi-modal assistance
control input to apply.","While we
assistive systems.",2022-10-17 20:23:32+00:00,Multimodal Shared Autonomy for Social Navigation Assistance of Telepresence Robots,cs.RO,"['cs.RO', 'cs.HC']","[arxiv.Result.Author('Kenechukwu C. Mbanisi'), arxiv.Result.Author('Michael A. Gennert')]","Mobile telepresence robots (MTRs) have become increasingly popular in the
expanding world of remote work, providing new avenues for people to actively
participate in activities at a distance. However, humans operating MTRs often
have difficulty navigating in densely populated environments due to limited
situation awareness and narrow field-of-view, which reduces user acceptance and
satisfaction. Shared autonomy in navigation has been studied primarily in
static environments or in situations where only one pedestrian interacts with
the robot. We present a multimodal shared autonomy approach, leveraging visual
and haptic guidance, to provide navigation assistance for remote operators in
densely-populated environments. It uses a modified form of reciprocal velocity
obstacles for generating safe control inputs while taking social proxemics
constraints into account. Two different visual guidance designs, as well as
haptic force rendering, were proposed to convey safe control input. We
conducted a user study to compare the merits and limitations of multimodal
navigation assistance to haptic or visual assistance alone on a shared
navigation task. The study involved 15 participants operating a virtual
telepresence robot in a virtual hall with moving pedestrians, using the
different assistance modalities. We evaluated navigation performance,
transparency and cooperation, as well as user preferences. Our results showed
that participants preferred multimodal assistance with a visual guidance
trajectory over haptic or visual modalities alone, although it had no impact on
navigation performance. Additionally, we found that visual guidance
trajectories conveyed a higher degree of understanding and cooperation than
equivalent haptic cues in a navigation task.",-0.2327397,0.0109236315,-0.10286717,A
12627,"We conclude that
performance [44, 45], with low human trust in the robot              further research is needed to validate these ﬁndings in a real-
partner degrading task performance.","tion assistance systems within the complex context of socially-
Research shows that human-robot trust inﬂuences joint task           aware navigation in cluttered environments.","Conversely, higher trust         world context.",2022-10-17 20:23:32+00:00,Multimodal Shared Autonomy for Social Navigation Assistance of Telepresence Robots,cs.RO,"['cs.RO', 'cs.HC']","[arxiv.Result.Author('Kenechukwu C. Mbanisi'), arxiv.Result.Author('Michael A. Gennert')]","Mobile telepresence robots (MTRs) have become increasingly popular in the
expanding world of remote work, providing new avenues for people to actively
participate in activities at a distance. However, humans operating MTRs often
have difficulty navigating in densely populated environments due to limited
situation awareness and narrow field-of-view, which reduces user acceptance and
satisfaction. Shared autonomy in navigation has been studied primarily in
static environments or in situations where only one pedestrian interacts with
the robot. We present a multimodal shared autonomy approach, leveraging visual
and haptic guidance, to provide navigation assistance for remote operators in
densely-populated environments. It uses a modified form of reciprocal velocity
obstacles for generating safe control inputs while taking social proxemics
constraints into account. Two different visual guidance designs, as well as
haptic force rendering, were proposed to convey safe control input. We
conducted a user study to compare the merits and limitations of multimodal
navigation assistance to haptic or visual assistance alone on a shared
navigation task. The study involved 15 participants operating a virtual
telepresence robot in a virtual hall with moving pedestrians, using the
different assistance modalities. We evaluated navigation performance,
transparency and cooperation, as well as user preferences. Our results showed
that participants preferred multimodal assistance with a visual guidance
trajectory over haptic or visual modalities alone, although it had no impact on
navigation performance. Additionally, we found that visual guidance
trajectories conveyed a higher degree of understanding and cooperation than
equivalent haptic cues in a navigation task.",-0.12978636,0.087148964,-0.10129194,A
12744,"We develop various forms

of comparative feedback, and further study how they can be collected actively to improve data-

eﬃciency, which is crucial in robotics, especially because the data are coming from human users.","feedback to learn the objectives, where human users are asked

to compare multiple trajectories of a robot based on their preferences.","For these, we bridge ideas from machine learning, information theory, human-robot interaction,

optimization and control theory.",2022-10-19 21:34:51+00:00,Learning Preferences for Interactive Autonomy,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'stat.ML']",[arxiv.Result.Author('Erdem Bıyık')],"When robots enter everyday human environments, they need to understand their
tasks and how they should perform those tasks. To encode these, reward
functions, which specify the objective of a robot, are employed. However,
designing reward functions can be extremely challenging for complex tasks and
environments. A promising approach is to learn reward functions from humans.
Recently, several robot learning works embrace this approach and leverage human
demonstrations to learn the reward functions. Known as inverse reinforcement
learning, this approach relies on a fundamental assumption: humans can provide
near-optimal demonstrations to the robot. Unfortunately, this is rarely the
case: human demonstrations to the robot are often suboptimal due to various
reasons, e.g., difficulty of teleoperation, robot having high degrees of
freedom, or humans' cognitive limitations.
  This thesis is an attempt towards learning reward functions from human users
by using other, more reliable data modalities. Specifically, we study how
reward functions can be learned using comparative feedback, in which the human
user compares multiple robot trajectories instead of (or in addition to)
providing demonstrations. To this end, we first propose various forms of
comparative feedback, e.g., pairwise comparisons, best-of-many choices,
rankings, scaled comparisons; and describe how a robot can use these various
forms of human feedback to infer a reward function, which may be parametric or
non-parametric. Next, we propose active learning techniques to enable the robot
to ask for comparison feedback that optimizes for the expected information that
will be gained from that user feedback. Finally, we demonstrate the
applicability of our methods in a wide variety of domains, ranging from
autonomous driving simulations to home robotics, from standard reinforcement
learning benchmarks to lower-body exoskeletons.",-0.19920665,0.005300861,-0.118059635,A
12745,"These parameters were selected
because exoskeleton users frequently suggested modiﬁcations to SL, SD, and PR in prior work
(see https://sites.google.com/view/roial-icra2021), and we wanted to further study the re-
lationship between PR and PP.","These periodic gaits are parameterized by various features, and this studies focuses on
four: step length (SL) in meters, step duration (SD) in seconds, maximum pelvis roll (PR) in
degrees, and maximum pelvis pitch (PP) in degrees (Figure 3.4).","We discretized these parameters into bins of sizes 10, 7, 5, and 5,
respectively, resulting in 1,750 trajectories within a 4D gait (or feature) space.",2022-10-19 21:34:51+00:00,Learning Preferences for Interactive Autonomy,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'stat.ML']",[arxiv.Result.Author('Erdem Bıyık')],"When robots enter everyday human environments, they need to understand their
tasks and how they should perform those tasks. To encode these, reward
functions, which specify the objective of a robot, are employed. However,
designing reward functions can be extremely challenging for complex tasks and
environments. A promising approach is to learn reward functions from humans.
Recently, several robot learning works embrace this approach and leverage human
demonstrations to learn the reward functions. Known as inverse reinforcement
learning, this approach relies on a fundamental assumption: humans can provide
near-optimal demonstrations to the robot. Unfortunately, this is rarely the
case: human demonstrations to the robot are often suboptimal due to various
reasons, e.g., difficulty of teleoperation, robot having high degrees of
freedom, or humans' cognitive limitations.
  This thesis is an attempt towards learning reward functions from human users
by using other, more reliable data modalities. Specifically, we study how
reward functions can be learned using comparative feedback, in which the human
user compares multiple robot trajectories instead of (or in addition to)
providing demonstrations. To this end, we first propose various forms of
comparative feedback, e.g., pairwise comparisons, best-of-many choices,
rankings, scaled comparisons; and describe how a robot can use these various
forms of human feedback to infer a reward function, which may be parametric or
non-parametric. Next, we propose active learning techniques to enable the robot
to ask for comparison feedback that optimizes for the expected information that
will be gained from that user feedback. Finally, we demonstrate the
applicability of our methods in a wide variety of domains, ranging from
autonomous driving simulations to home robotics, from standard reinforcement
learning benchmarks to lower-body exoskeletons.",-0.062406294,-0.21433556,0.0136075355,A
12799,"The Hermit crab rover by ‘The
and there is signiﬁcant further research eﬀort needed.","The development of           entries, and echoes the considerable interest in bee-
bird-inspired robots is an active area of research [9],  inspired robots seen in the aerial robotics ﬁeld, where
and while many design concepts have been proposed,       one of the smallest and best-known robots is the
practical application demonstrations remain scarce       Harvard Robobee [11].","Yak Collective’ (ﬁgure A5) is a scavenger robot, which
The proposed ‘Ersters’ robot by Elizabeth Ivanova        gathers scrap material from its surroundings to build
(ﬁgure A2) is an excellent idea that identiﬁes an        itself a protective shell.",2022-10-20 17:49:19+00:00,The Natural Robotics Contest: Crowdsourced Biomimetic Design,cs.RO,['cs.RO'],"[arxiv.Result.Author('Robert Siddall'), arxiv.Result.Author('Raphael Zufferey'), arxiv.Result.Author('Sophie Armanini'), arxiv.Result.Author('Ketao Zhang'), arxiv.Result.Author('Sina Sareh'), arxiv.Result.Author('Elisavetha Sergeev')]","Biomimetic and Bioinspired design is not only a potent resource for
roboticists looking to develop robust engineering systems or understand the
natural world. It is also a uniquely accessible entry point into science and
technology. Every person on Earth constantly interacts with nature, and most
people have an intuitive sense of animal and plant behavior, even without
realizing it. The Natural Robotics Contest is novel piece of science
communication that takes advantage of this intuition, and creates an
opportunity for anyone with an interest in nature or robotics to submit their
idea and have it turned into a real engineering system. In this paper we will
discuss the competition's submissions, which show how the public thinks of
nature as well as the problems people see as most pressing for engineers to
solve. We will then show our design process from the winning submitted concept
sketch through to functioning robot, to offer a case study in biomimetic robot
design. The winning design is a robotic fish which uses gill structures to
filter out microplastics. This was fabricated into an open source robot with a
novel 3D printed gill design. By presenting the competition and the winning
entry we hope to foster further interest in nature-inspired design, and
increase the interplay between nature and engineering in the minds of readers.",-0.27187878,0.06817421,0.16271114,A
12826,"A further research
    would help in exploring the possibilities for co-evolution of desired robotic structure and
    its assembly planning.","• So far, co-evolution or morphology has been utilized for robotic design.","• For a human being emotions play a significant role during decision making along with
    intelligence.",2022-10-21 03:11:15+00:00,Accessible Survey of Evolutionary Robotics and Potential Future Research Directions,cs.RO,['cs.RO'],[arxiv.Result.Author('Hari Mohan Pandey')],"This paper reviews various Evolutionary Approaches applied to the domain of
Evolutionary Robotics with the intention of resolving difficult problems in the
areas of robotic design and control. Evolutionary Robotics is a fast-growing
field that has attracted substantial research attention in recent years. The
paper thus collates recent findings along with some anticipated applications.
The reviewed literature is organized systematically to give a categorical
overview of recent developments and is presented in tabulated form for quick
reference. We discuss the outstanding potentialities and challenges that exist
in robotics from an ER perspective, with the belief that these will be have the
capacity to be addressed in the near future via the application of evolutionary
approaches. The primary objective of this study is to explore the applicability
of Evolutionary Approaches in robotic application development. We believe that
this study will enable the researchers to utilize Evolutionary Approaches to
solve complex outstanding problems in robotics.",-0.24242635,-0.13817136,-0.006017534,A
12827,"Author have highlighted various challenges which creates
room for further research in future.","Hence, these research areas require further
investigation for the improvement.","References

1.",2022-10-21 03:11:15+00:00,Accessible Survey of Evolutionary Robotics and Potential Future Research Directions,cs.RO,['cs.RO'],[arxiv.Result.Author('Hari Mohan Pandey')],"This paper reviews various Evolutionary Approaches applied to the domain of
Evolutionary Robotics with the intention of resolving difficult problems in the
areas of robotic design and control. Evolutionary Robotics is a fast-growing
field that has attracted substantial research attention in recent years. The
paper thus collates recent findings along with some anticipated applications.
The reviewed literature is organized systematically to give a categorical
overview of recent developments and is presented in tabulated form for quick
reference. We discuss the outstanding potentialities and challenges that exist
in robotics from an ER perspective, with the belief that these will be have the
capacity to be addressed in the near future via the application of evolutionary
approaches. The primary objective of this study is to explore the applicability
of Evolutionary Approaches in robotic application development. We believe that
this study will enable the researchers to utilize Evolutionary Approaches to
solve complex outstanding problems in robotics.",0.27855074,-0.30748886,-0.15506437,C
12917,"By comparing the performance of the DaXBench tasks across these three methods, we
aim to further study the beneﬁts and limitations of applying the differentiable physics to RL.","The computation graph of SHAC is illustrated
in Figure 4a.",Overall Performance.,2022-10-24 09:33:59+00:00,Benchmarking Deformable Object Manipulation with Differentiable Physics,cs.RO,['cs.RO'],"[arxiv.Result.Author('Siwei Chen'), arxiv.Result.Author('Cunjun Yu'), arxiv.Result.Author('Yiqing Xu'), arxiv.Result.Author('Linfeng Li'), arxiv.Result.Author('Xiao Ma'), arxiv.Result.Author('Zhongwen Xu'), arxiv.Result.Author('David Hsu')]","Deformable Object Manipulation (DOM) is of significant importance to both
daily and industrial applications. Recent successes in differentiable physics
simulators allow learning algorithms to train a policy with analytic gradients
through environment dynamics, which significantly facilitates the development
of DOM algorithms. However, existing DOM benchmarks are either
single-object-based or non-differentiable. This leaves the questions of 1) how
a task-specific algorithm performs on other tasks and 2) how a
differentiable-physics-based algorithm compares with the non-differentiable
ones in general. In this work, we present DaXBench, a differentiable DOM
benchmark with a wide object and task coverage. DaXBench includes 9 challenging
high-fidelity simulated tasks, covering rope, cloth, and liquid manipulation
with various difficulty levels. To better understand the performance of general
algorithms on different DOM tasks, we conduct comprehensive experiments over
representative DOM methods, ranging from planning to imitation learning and
reinforcement learning. In addition, we provide careful empirical studies of
existing decision-making algorithms based on differentiable physics, and
discuss their limitations, as well as potential future directions.",0.22924021,-0.16967592,0.09169589,C
13007,"5 The YCB-Slide dataset

To evaluate MidasTouch, and enable further research in tactile sensing, we introduce our YCB-Slide
dataset (more in Appendix D).",xHt .,"It comprises of DIGIT sliding interactions on the 10 YCB objects
from Section 4.",2022-10-25 17:55:09+00:00,MidasTouch: Monte-Carlo inference over distributions across sliding touch,cs.RO,['cs.RO'],"[arxiv.Result.Author('Sudharshan Suresh'), arxiv.Result.Author('Zilin Si'), arxiv.Result.Author('Stuart Anderson'), arxiv.Result.Author('Michael Kaess'), arxiv.Result.Author('Mustafa Mukadam')]","We present MidasTouch, a tactile perception system for online global
localization of a vision-based touch sensor sliding on an object surface. This
framework takes in posed tactile images over time, and outputs an evolving
distribution of sensor pose on the object's surface, without the need for
visual priors. Our key insight is to estimate local surface geometry with
tactile sensing, learn a compact representation for it, and disambiguate these
signals over a long time horizon. The backbone of MidasTouch is a Monte-Carlo
particle filter, with a measurement model based on a tactile code network
learned from tactile simulation. This network, inspired by LIDAR place
recognition, compactly summarizes local surface geometries. These generated
codes are efficiently compared against a precomputed tactile codebook
per-object, to update the pose distribution. We further release the YCB-Slide
dataset of real-world and simulated forceful sliding interactions between a
vision-based tactile sensor and standard YCB objects. While single-touch
localization can be inherently ambiguous, we can quickly localize our sensor by
traversing salient surface geometries. Project page:
https://suddhu.github.io/midastouch-tactile/",-0.15936233,-0.12617539,-0.14143322,A
13050,"The deployment of the
that the success rate decreases mildly when the standard          student policy on a real drone still requires further research
deviation increases, which proves that our policy is robust       on transfer learning or adaptive learning.",We can observe    robustness of our vision-based policy.,"Although relaxing
                                                                  the need for globally-consistent position information about
                                                                  the drone and the gate, the student policy still relies on part
                                                                  of the vehicle’s state to predict the control commands.",2022-10-26 19:03:17+00:00,Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Jiawei Fu'), arxiv.Result.Author('Yunlong Song'), arxiv.Result.Author('Yan Wu'), arxiv.Result.Author('Fisher Yu'), arxiv.Result.Author('Davide Scaramuzza')]","Autonomous drones can operate in remote and unstructured environments,
enabling various real-world applications. However, the lack of effective
vision-based algorithms has been a stumbling block to achieving this goal.
Existing systems often require hand-engineered components for state estimation,
planning, and control. Such a sequential design involves laborious tuning,
human heuristics, and compounding delays and errors. This paper tackles the
vision-based autonomous-drone-racing problem by learning deep sensorimotor
policies. We use contrastive learning to extract robust feature representations
from the input images and leverage a two-stage learning-by-cheating framework
for training a neural network policy. The resulting policy directly infers
control commands with feature representations learned from raw images, forgoing
the need for globally-consistent state estimation, trajectory planning, and
handcrafted control design. Our experimental results indicate that our
vision-based policy can achieve the same level of racing performance as the
state-based policy while being robust against different visual disturbances and
distractors. We believe this work serves as a stepping-stone toward developing
intelligent vision-based autonomous systems that control the drone purely from
image inputs, like human pilots.",-0.14223889,0.077347256,-0.15081556,A
13068,"By leveraging open-source simulations and
implementations, we hope that this work and our insights can provide a basis for further research into
safe and robust RL, especially for robot control.","We also note that traditional “vanilla”
agents show similar performance to the robust RL agents even when disturbances are injected, despite
not being explicitly designed with this purpose in mind.","2 Background

In RL, an agent, in our case, a robot, performs an action and receives feedback (reward) from the
environment on how well it is doing at the environment’s task, perceives the updated state of the
environment resulting from the action taken and repeats the process, learning over time to improve the
actions it takes to maximise reward collection (and this to correctly perform the task).",2022-10-27 06:15:26+00:00,Characterising the Robustness of Reinforcement Learning for Continuous Control using Disturbance Injection,cs.RO,['cs.RO'],"[arxiv.Result.Author('Catherine R. Glossop'), arxiv.Result.Author('Jacopo Panerati'), arxiv.Result.Author('Amrit Krishnan'), arxiv.Result.Author('Zhaocong Yuan'), arxiv.Result.Author('Angela P. Schoellig')]","In this study, we leverage the deliberate and systematic fault-injection
capabilities of an open-source benchmark suite to perform a series of
experiments on state-of-the-art deep and robust reinforcement learning
algorithms. We aim to benchmark robustness in the context of continuous action
spaces -- crucial for deployment in robot control. We find that robustness is
more prominent for action disturbances than it is for disturbances to
observations and dynamics. We also observe that state-of-the-art approaches
that are not explicitly designed to improve robustness perform at a level
comparable to that achieved by those that are. Our study and results are
intended to provide insight into the current state of safe and robust
reinforcement learning and a foundation for the advancement of the field, in
particular, for deployment in robotic systems.",-0.22534627,-0.07632084,0.047828544,A
13074,"Section VII states the conclusions and proposes
structure of the underlying space, which makes the learning                  further research directions.","V and show an experiment in a typical in-
DoFs are related by particular constraints arising from the                  dustrial task.","problem more difﬁcult as the learner has also to fulﬁll those
constraints.",2022-10-27 08:02:41+00:00,Learning Deep Robotic Skills on Riemannian manifolds,cs.RO,['cs.RO'],"[arxiv.Result.Author('Weitao Wang'), arxiv.Result.Author('Matteo Saveriano'), arxiv.Result.Author('Fares J. Abu-Dakka')]","In this paper, we propose RiemannianFlow, a deep generative model that allows
robots to learn complex and stable skills evolving on Riemannian manifolds.
Examples of Riemannian data in robotics include stiffness (symmetric and
positive definite matrix (SPD)) and orientation (unit quaternion (UQ))
trajectories. For Riemannian data, unlike Euclidean ones, different dimensions
are interconnected by geometric constraints which have to be properly
considered during the learning process. Using distance preserving mappings, our
approach transfers the data between their original manifold and the tangent
space, realizing the removing and re-fulfilling of the geometric constraints.
This allows to extend existing frameworks to learn stable skills from
Riemannian data while guaranteeing the stability of the learning results. The
ability of RiemannianFlow to learn various data patterns and the stability of
the learned models are experimentally shown on a dataset of manifold motions.
Further, we analyze from different perspectives the robustness of the model
with different hyperparameter combinations. It turns out that the model's
stability is not affected by different hyperparameters, a proper combination of
the hyperparameters leads to a significant improvement (up to 27.6%) of the
model accuracy. Last, we show the effectiveness of RiemannianFlow in a real
peg-in-hole (PiH) task where we need to generate stable and consistent position
and orientation trajectories for the robot starting from different initial
poses.",0.08346914,-0.17318779,-0.24953738,C
13263,"115
12 Research Challenges and Opportunities

After discussing the diﬀerent aspects involved in IIL, regarding ML algorithmic features,
ways of interaction, interfaces, human factors, and evaluation considerations, we discuss
some of the problems in the domain that represent a challenge and are potentially interesting
directions for further research that could make the use of IIL for transferring the human
knowledge to the machines more eﬀectively.","An exception is the NIST benchmark, which provides a standardized
set of easily reproducible table-top tasks to be used for evaluating real-world robots.",• Successfully combining multiple feedback modalities.,2022-10-31 12:36:03+00:00,Interactive Imitation Learning in Robotics: A Survey,cs.RO,['cs.RO'],"[arxiv.Result.Author('Carlos Celemin'), arxiv.Result.Author('Rodrigo Pérez-Dattari'), arxiv.Result.Author('Eugenio Chisari'), arxiv.Result.Author('Giovanni Franzese'), arxiv.Result.Author('Leandro de Souza Rosa'), arxiv.Result.Author('Ravi Prakash'), arxiv.Result.Author('Zlatan Ajanović'), arxiv.Result.Author('Marta Ferraz'), arxiv.Result.Author('Abhinav Valada'), arxiv.Result.Author('Jens Kober')]","Interactive Imitation Learning (IIL) is a branch of Imitation Learning (IL)
where human feedback is provided intermittently during robot execution allowing
an online improvement of the robot's behavior. In recent years, IIL has
increasingly started to carve out its own space as a promising data-driven
alternative for solving complex robotic tasks. The advantages of IIL are its
data-efficient, as the human feedback guides the robot directly towards an
improved behavior, and its robustness, as the distribution mismatch between the
teacher and learner trajectories is minimized by providing feedback directly
over the learner's trajectories. Nevertheless, despite the opportunities that
IIL presents, its terminology, structure, and applicability are not clear nor
unified in the literature, slowing down its development and, therefore, the
research of innovative formulations and discoveries. In this article, we
attempt to facilitate research in IIL and lower entry barriers for new
practitioners by providing a survey of the field that unifies and structures
it. In addition, we aim to raise awareness of its potential, what has been
accomplished and what are still open research questions. We organize the most
relevant works in IIL in terms of human-robot interaction (i.e., types of
feedback), interfaces (i.e., means of providing feedback), learning (i.e.,
models learned from feedback and function approximators), user experience
(i.e., human perception about the learning process), applications, and
benchmarks. Furthermore, we analyze similarities and differences between IIL
and RL, providing a discussion on how the concepts offline, online, off-policy
and on-policy learning should be transferred to IIL from the RL literature. We
particularly focus on robotic applications in the real world and discuss their
implications, limitations, and promising future areas of research.",-0.13126458,0.12667257,-0.10629083,A
13285,"Additionally,
                                                                as the lower body of the robot was not considered in this
   In the next experiment, we used the full neural network      study because of the balancing problem, further study is
architecture, but trained it on data from only one interaction  needed to generate behaviors that include the lower body
scenario each time.",that it can be applied to other types of robots.,"For each interaction scenario, the model    movements, such as moving forward or backward.",2022-11-02 07:38:52+00:00,Nonverbal Social Behavior Generation for Social Robots Using End-to-End Learning,cs.RO,['cs.RO'],"[arxiv.Result.Author('Woo-Ri Ko'), arxiv.Result.Author('Minsu Jang'), arxiv.Result.Author('Jaeyeon Lee'), arxiv.Result.Author('Jaehong Kim')]","To provide effective and enjoyable human-robot interaction, it is important
for social robots to exhibit nonverbal behaviors, such as a handshake or a hug.
However, the traditional approach of reproducing pre-coded motions allows users
to easily predict the reaction of the robot, giving the impression that the
robot is a machine rather than a real agent. Therefore, we propose a neural
network architecture based on the Seq2Seq model that learns social behaviors
from human-human interactions in an end-to-end manner. We adopted a generative
adversarial network to prevent invalid pose sequences from occurring when
generating long-term behavior. To verify the proposed method, experiments were
performed using the humanoid robot Pepper in a simulated environment. Because
it is difficult to determine success or failure in social behavior generation,
we propose new metrics to calculate the difference between the generated
behavior and the ground-truth behavior. We used these metrics to show how
different network architectural choices affect the performance of behavior
generation, and we compared the performance of learning multiple behaviors and
that of learning a single behavior. We expect that our proposed method can be
used not only with home service robots, but also for guide robots, delivery
robots, educational robots, and virtual robots, enabling the users to enjoy and
effectively interact with the robots.",-0.2788348,-0.112923175,-0.047493853,A
13296,"Therefore, further research will
be performed to achieve conclusions regarding whether or not
an AV should mimic driver behavior.","Other unknown factors might also contribute to the discrep-
ancy in some of the results.","ACKNOWLEDGEMENT

   This work was supported by the Austrian Science Fund
(FWF), within the project ”Interaction of autonomous and
manually-controlled vehicles (IAMCV)”, number P 34485-N.

                            REFERENCES

[1] C. Olaverri-Monreal, “Autonomous vehicles and smart mobility related
     technologies,” Infocommunications Journal, vol.",2022-11-02 13:40:47+00:00,Implementation of Road Safety Perception in Autonomous Vehicles in a Lane Change Scenario,cs.RO,['cs.RO'],"[arxiv.Result.Author('Enrico Del Re'), arxiv.Result.Author('Cristina Olaverri-Monreal')]","Understanding human driving behavior is crucial to develop autonomous
vehicles' algorithms. However, most low level automation, such as the one in
advanced driving assistance systems (ADAS), is based on objective safety
measures, which are not always aligned with what the drivers perceive as safe
and their correspondent driving behavior. Finding the bridge between the
subjective perception and objective safety measures has been analyzed in this
paper focusing specifically on lane-change scenarios. Results showed
statistically significant differences between what is perceived as safe by
drivers and objective metrics depending on the specific maneuver and location
of drivers.",0.009386141,-0.04306518,0.029390238,A
13491,"However, further research effort is
needed to i) further increase the total calorie content that can be provided (by developing an edible
fuselage, edible tail wing, etc.)","Additionally, the scalability of the proposed design can
load other nutritional values required by an endangered person.","and improve the nutrient composition (i.e., ratios of macro-nutrients
such as protein, carbohydrates and lipids) to meet the requirements defined by WHO for human
nutrition; ii) store additional payload, such as water, inside an edible container without significantly
increasing the surface area exposed to the air; iii) investigate and incorporate edible sensors or edible
electronics to increase the mass ratio of the biodegradable/edible part of the drone; iv) explore other
methods to fabricate the wing or drone, for example, by 3D printing, to improve fabrication efficiency
and reduce time.",2022-11-08 10:43:52+00:00,Towards edible drones for rescue missions: design and flight of nutritional wings,cs.RO,['cs.RO'],"[arxiv.Result.Author('Bokeon Kwak'), arxiv.Result.Author('Jun Shintake'), arxiv.Result.Author('Lu Zhang'), arxiv.Result.Author('Dario Floreano')]","Drones have shown to be useful aerial vehicles for unmanned transport
missions such as food and medical supply delivery. This can be leveraged to
deliver life-saving nutrition and medicine for people in emergency situations.
However, commercial drones can generally only carry 10 % - 30 % of their own
mass as payload, which limits the amount of food delivery in a single flight.
One novel solution to noticeably increase the food-carrying ratio of a drone,
is recreating some structures of a drone, such as the wings, with edible
materials. We thus propose a drone, which is no longer only a food transporting
aircraft, but itself is partially edible, increasing its food-carrying mass
ratio to 50 %, owing to its edible wings. Furthermore, should the edible drone
be left behind in the environment after performing its task in an emergency
situation, it will be more biodegradable than its non-edible counterpart,
leaving less waste in the environment. Here we describe the choice of materials
and scalable design of edible wings, and validate the method in a
flight-capable prototype that can provide 300 kcal and carry a payload of 80 g
of water.",-0.045050293,-0.06535189,0.117919385,A
13541,"Nevertheless, there are clearly settings where high grasp success
      rates for novel objects are important and further research is clearly needed to achieve this.","In these applications, it should be possible to reach grasp
      success rates near 100%.",5.2.,2022-11-09 13:57:58+00:00,"Grasp Learning: Models, Methods, and Performance",cs.RO,['cs.RO'],[arxiv.Result.Author('Robert Platt')],"Grasp learning has become an exciting and important topic in robotics. Just a
few years ago, the problem of grasping novel objects from unstructured piles of
clutter was considered a serious research challenge. Now, it is a capability
that is quickly becoming incorporated into industrial supply chain automation.
How did that happen? What is the current state of the art in robotic grasp
learning, what are the different methodological approaches, and what machine
learning models are used? This review attempts to give an overview of the
current state of the art of grasp learning research.",-0.2680078,-0.06992113,-0.10998227,A
13581,"Therefore,                        max TCS
there currently seems to be no model that a trajectory planning
algorithm in any scenario can rely on to substantially increase    
the effectiveness of an autonomous vehicle’s driving style,
necessitating further research into such models.","However, especially for the unintuitive      tS = TS(XT ) =                      else
and safety-critical edge cases, most models often performed
only slightly better than a random predictor at best.","tC (max T )  TCC = ∅  (A.3)
                                                                   tC = TC (XT ) =                     else
                             APPENDIX A
 DETAILING THE FRAMEWORK FOR BENCHMARKING GAP                                            min TCC

                             ACCEPTANCE MODELS                     

A.",2022-11-10 09:59:38+00:00,Benchmark for Models Predicting Human Behavior in Gap Acceptance Scenarios,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Julian Frederik Schumann'), arxiv.Result.Author('Jens Kober'), arxiv.Result.Author('Arkady Zgonnikov')]","Autonomous vehicles currently suffer from a time-inefficient driving style
caused by uncertainty about human behavior in traffic interactions. Accurate
and reliable prediction models enabling more efficient trajectory planning
could make autonomous vehicles more assertive in such interactions. However,
the evaluation of such models is commonly oversimplistic, ignoring the
asymmetric importance of prediction errors and the heterogeneity of the
datasets used for testing. We examine the potential of recasting interactions
between vehicles as gap acceptance scenarios and evaluating models in this
structured environment. To that end, we develop a framework facilitating the
evaluation of any model, by any metric, and in any scenario. We then apply this
framework to state-of-the-art prediction models, which all show themselves to
be unreliable in the most safety-critical situations.",0.109652445,-0.09764184,0.045346636,C
13586,"SVGP BATHYMETRIC MAPPING
framework publicly available to encourage further research.","Two examples of these are [3], with
                                        a maximum a posteriori (MAP) approach to solving SLAM
                                        and [4], based on a Rao-Blackwellized Particle Filter (RBPF)

                                           This work was supported by Stiftelsen for Strategisk Forskning (SSF)
                                        through the Swedish Maritime Robotics Centre (SMaRC) (IRC15-0046)

                                           The authors are with 1 the Robotics Perception and Learning Division and
                                        2 the Naval Architecture Division at KTH Royal Institute of Technology,
                                        SE-100 44 Stockholm, Sweden{torroba, mcella, aldot, nrol,
                                        johnf}@kth.se
collected with two AUVs, one of them live, and we make the                   III.","This section introduces large-scale bathymetric modelling
                      II.",2022-11-10 14:21:48+00:00,Online Stochastic Variational Gaussian Process Mapping for Large-Scale SLAM in Real Time,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Ignacio Torroba'), arxiv.Result.Author('Marco Chella'), arxiv.Result.Author('Aldo Teran'), arxiv.Result.Author('Niklas Rolleberg'), arxiv.Result.Author('John Folkesson')]","Autonomous underwater vehicles (AUVs) are becoming standard tools for
underwater exploration and seabed mapping in both scientific and industrial
applications \cite{graham2022rapid, stenius2022system}. Their capacity to dive
untethered allows them to reach areas inaccessible to surface vessels and to
collect data more closely to the seafloor, regardless of the water depth.
However, their navigation autonomy remains bounded by the accuracy of their
dead reckoning (DR) estimate of their global position, severely limited in the
absence of a priori maps of the area and GPS signal. Global localization
systems equivalent to the later exists for the underwater domain, such as LBL
or USBL. However they involve expensive external infrastructure and their
reliability decreases with the distance to the AUV, making them unsuitable for
deep sea surveys.",-0.12707685,0.3047339,0.02135368,B
13587,"Additionally, the full framework has been released
to encourage further research on the topic 1.","Aaai/iaai, 593598, 2002.
its use.","[10] Ignacio Torroba, Christopher Iliffe Sprague, and John Folkesson.",2022-11-10 14:21:48+00:00,Online Stochastic Variational Gaussian Process Mapping for Large-Scale SLAM in Real Time,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Ignacio Torroba'), arxiv.Result.Author('Marco Chella'), arxiv.Result.Author('Aldo Teran'), arxiv.Result.Author('Niklas Rolleberg'), arxiv.Result.Author('John Folkesson')]","Autonomous underwater vehicles (AUVs) are becoming standard tools for
underwater exploration and seabed mapping in both scientific and industrial
applications \cite{graham2022rapid, stenius2022system}. Their capacity to dive
untethered allows them to reach areas inaccessible to surface vessels and to
collect data more closely to the seafloor, regardless of the water depth.
However, their navigation autonomy remains bounded by the accuracy of their
dead reckoning (DR) estimate of their global position, severely limited in the
absence of a priori maps of the area and GPS signal. Global localization
systems equivalent to the later exists for the underwater domain, such as LBL
or USBL. However they involve expensive external infrastructure and their
reliability decreases with the distance to the AUV, making them unsuitable for
deep sea surveys.",0.17922369,-0.33926183,-0.22030985,C
13645,"The simulation results are given in the video                   It is necessary to do further study on their impacts in the
attachment.",and slopes.,"Overall, the simulation results have shown high                 future.",2022-11-11 14:19:40+00:00,The Simplest Balance Controller for Dynamic Walking,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Linqi Ye'), arxiv.Result.Author('Xueqian Wang'), arxiv.Result.Author('Houde Liu'), arxiv.Result.Author('Bin Liang')]","Humans can balance very well during walking, even when perturbed. But it
seems difficult to achieve robust walking for bipedal robots. Here we describe
the simplest balance controller that leads to robust walking for a linear
inverted pendulum (LIP) model. The main idea is to use a linear function of the
body velocity to determine the next foot placement, which we call linear foot
placement control (LFPC). By using the Poincar\'e map, a balance criterion is
derived, which shows that LFPC is stable when the velocity-feedback coefficient
is located in a certain range. And that range is much bigger when stepping
faster, which indicates ""faster stepping, easier to balance"". We show that
various gaits can be generated by adjusting the controller parameters in LFPC.
Particularly, a dead-beat controller is discovered that can lead to
steady-state walking in just one step. The effectiveness of LFPC is verified
through Matlab simulation as well as V-REP simulation for both 2D and 3D
walking. The main feature of LFPC is its simplicity and inherent robustness,
which may help us understand the essence of how to maintain balance in dynamic
walking.",-0.00024006749,-0.23020087,0.17132857,A
13647,"To further study this, a 4-segment 2D soft robot is          trajectory.","(3), it is also     variables for each robot segment are calculated, and using the
possible to design trajectories for a robot with a constant tip     model-based controller, the robot is controlled on the desired
angle.","To control the robot, a combination of feedforward
chosen to follow the desired trajectory of Xd = 175+50cos(t)        and feedback control, as explained in section(II-B) has been
and Yd = 100 + 50sin(t).",2022-11-11 15:01:59+00:00,A geometric approach towards inverse kinematics of soft extensible pneumatic actuators intended for trajectory tracking,cs.RO,['cs.RO'],"[arxiv.Result.Author('Mahboubeh Keyvanara'), arxiv.Result.Author('Arman Goshtasbi'), arxiv.Result.Author('Irene A. Kuling')]","Soft robots are interesting examples of hyper-redundancy in robotics,
however, the nonlinear continuous dynamics of these robots and the use of
hyper-elastic and visco-elastic materials makes modeling of these robots more
complicated. This study presents a geometric Inverse Kinematic (IK) model for
trajectory tracking of multi-segment extensible soft robots, where, each
segment of the soft actuator is geometrically approximated with multiple rigid
links connected with rotary and prismatic joints. Using optimization methods,
the desired configuration variables of the soft actuator for the desired
end-effector positions are obtained. Also, the redundancy of the robot is
applied for second task applications, such as tip angle control. The model's
performance is investigated through simulations, numerical benchmarks, and
experimental validations and results show lower computational costs and higher
accuracy compared to most existing methods. The method is easy to apply to
multi segment soft robots, both in 2D and 3D. As a case study, a fully
3D-printed soft robot manipulator is tested using a control unit and the model
predictions show good agreement with the experimental results.",-0.32663393,-0.119736895,0.37397373,A
13679,"While our work has substantially advanced OOD-IL, we believe
that this problem merits further study.",Limitations and future work.,"One primary limitation of our current method is that our
method may not afford the computational cost of transferability learning when the number of clusters
increases to hundreds or thousands.",2022-11-13 07:45:06+00:00,Out-of-Dynamics Imitation Learning from Multimodal Demonstrations,cs.RO,"['cs.RO', 'cs.LG']","[arxiv.Result.Author('Yiwen Qiu'), arxiv.Result.Author('Jialong Wu'), arxiv.Result.Author('Zhangjie Cao'), arxiv.Result.Author('Mingsheng Long')]","Existing imitation learning works mainly assume that the demonstrator who
collects demonstrations shares the same dynamics as the imitator. However, the
assumption limits the usage of imitation learning, especially when collecting
demonstrations for the imitator is difficult. In this paper, we study
out-of-dynamics imitation learning (OOD-IL), which relaxes the assumption to
that the demonstrator and the imitator have the same state spaces but could
have different action spaces and dynamics. OOD-IL enables imitation learning to
utilize demonstrations from a wide range of demonstrators but introduces a new
challenge: some demonstrations cannot be achieved by the imitator due to the
different dynamics. Prior works try to filter out such demonstrations by
feasibility measurements, but ignore the fact that the demonstrations exhibit a
multimodal distribution since the different demonstrators may take different
policies in different dynamics. We develop a better transferability measurement
to tackle this newly-emerged challenge. We firstly design a novel
sequence-based contrastive clustering algorithm to cluster demonstrations from
the same mode to avoid the mutual interference of demonstrations from different
modes, and then learn the transferability of each demonstration with an
adversarial-learning based algorithm in each cluster. Experiment results on
several MuJoCo environments, a driving environment, and a simulated robot
environment show that the proposed transferability measurement more accurately
finds and down-weights non-transferable demonstrations and outperforms prior
works on the final imitation learning performance. We show the videos of our
experiment results on our website.",0.2754259,-0.068963856,-0.12018862,C
13685,"To further study this sim-
virtual constraints with the distributed MPC at the high level.","Here, (a) and (c) depict the evolution of       generated terrains and disturbances.","In addition, (b)  ilar robust stability behavior, Fig.",2022-11-13 14:26:32+00:00,Layered Control for Cooperative Locomotion of Two Quadrupedal Robots: Centralized and Distributed Approaches,cs.RO,"['cs.RO', 'math.OC']","[arxiv.Result.Author('Jeeseop Kim'), arxiv.Result.Author('Randall T Fawcett'), arxiv.Result.Author('Vinay R Kamidi'), arxiv.Result.Author('Aaron D Ames'), arxiv.Result.Author('Kaveh Akbari Hamed')]","This paper presents a layered control approach for real-time trajectory
planning and control of robust cooperative locomotion by two holonomically
constrained quadrupedal robots. A novel interconnected network of reduced-order
models, based on the single rigid body (SRB) dynamics, is developed for
trajectory planning purposes. At the higher level of the control architecture,
two different model predictive control (MPC) algorithms are proposed to address
the optimal control problem of the interconnected SRB dynamics: centralized and
distributed MPCs. The distributed MPC assumes two local quadratic programs that
share their optimal solutions according to a one-step communication delay and
an agreement protocol. At the lower level of the control scheme, distributed
nonlinear controllers are developed to impose the full-order dynamics to track
the prescribed reduced-order trajectories generated by MPCs. The effectiveness
of the control approach is verified with extensive numerical simulations and
experiments for the robust and cooperative locomotion of two holonomically
constrained A1 robots with different payloads on variable terrains and in the
presence of disturbances. It is shown that the distributed MPC has a
performance similar to that of the centralized MPC, while the computation time
is reduced significantly.",0.097198874,-0.09931014,0.32263854,C
13704,"As a further research
opportunity the findings from each threat emulation event can be used to develop new security controls, and the
implementation of which can be measured in the context of the human-robot trust relationship.","As [15]
states, continued offensive security activities need to be undertaken against robotic systems.","7
Conference acronym ’XX, 2023, Woodstock, NY  Adam Haskard, Damith Herath, and Zena Assaad

4.4 Secure Robotics Control Layer: Robotic Networks

Yaacoub et al.",2022-11-14 07:04:22+00:00,Secure Robotics: A Definition and a Brief Review from a Cybersecurity Control and Implementation Methodology Perspective,cs.RO,"['cs.RO', 'cs.CR']","[arxiv.Result.Author('Adam Haskard'), arxiv.Result.Author('Damith Herath'), arxiv.Result.Author('Zena Assaad')]","Secure robotics is a multi-disciplinary endeavour for improving the
cybersecurity posture of robotic and embodied Artificial Intelligence systems.
The article surveys emerging concepts and ideas encapsulating the notion of
secure robotics and identifies five Secure Robotics Cybersecurity Control
Implementation Layers as a crucial starting point for consideration by
practitioners. It also recognises the need for further studies on the
relationship between Human-robot trust and the implementation of established
and novel cybersecurity controls.",-0.1225159,-0.020709341,0.11937431,A
13740,"This will help the community understand the shape of the ”envelope”
of ofﬂine methods and identify directions for further research.","By using
the same set of tasks across two tracks we will be able to directly compare ofﬂine methods with semi-
ofﬂine and non-ofﬂine ones.","Driving SMARTS puts heavy emphasis on bringing challenges of real-world autonomous driving
R&D to the doorstep of the ML researchers such that they can devise innovative solutions without
being entangled in the complex practicalities of data preprocessing, scenario construction, evalua-
tion, and real-world system integration.",2022-11-14 17:10:53+00:00,NeurIPS 2022 Competition: Driving SMARTS,cs.RO,"['cs.RO', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Amir Rasouli'), arxiv.Result.Author('Randy Goebel'), arxiv.Result.Author('Matthew E. Taylor'), arxiv.Result.Author('Iuliia Kotseruba'), arxiv.Result.Author('Soheil Alizadeh'), arxiv.Result.Author('Tianpei Yang'), arxiv.Result.Author('Montgomery Alban'), arxiv.Result.Author('Florian Shkurti'), arxiv.Result.Author('Yuzheng Zhuang'), arxiv.Result.Author('Adam Scibior'), arxiv.Result.Author('Kasra Rezaee'), arxiv.Result.Author('Animesh Garg'), arxiv.Result.Author('David Meger'), arxiv.Result.Author('Jun Luo'), arxiv.Result.Author('Liam Paull'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Xinyu Wang'), arxiv.Result.Author('Xi Chen')]","Driving SMARTS is a regular competition designed to tackle problems caused by
the distribution shift in dynamic interaction contexts that are prevalent in
real-world autonomous driving (AD). The proposed competition supports
methodologically diverse solutions, such as reinforcement learning (RL) and
offline learning methods, trained on a combination of naturalistic AD data and
open-source simulation platform SMARTS. The two-track structure allows focusing
on different aspects of the distribution shift. Track 1 is open to any method
and will give ML researchers with different backgrounds an opportunity to solve
a real-world autonomous driving challenge. Track 2 is designed for strictly
offline learning methods. Therefore, direct comparisons can be made between
different methods with the aim to identify new promising research directions.
The proposed setup consists of 1) realistic traffic generated using real-world
data and micro simulators to ensure fidelity of the scenarios, 2) framework
accommodating diverse methods for solving the problem, and 3) baseline method.
As such it provides a unique opportunity for the principled investigation into
various aspects of autonomous vehicle deployment.",0.27665788,0.056904633,-0.06937286,C
13789,"Our further research
   Moreover, a bar chart for 10 epochs is presented in                  will be devoted to improving the result of LiePoseNet
Fig.","Nonetheless, our result is not accurate enough compared
                                                                        to PoseNet trained during 300 epochs.",4.,2022-11-15 20:11:46+00:00,LiePoseNet: Heterogeneous Loss Function Based on Lie Group for Significant Speed-up of PoseNet Training Process,cs.RO,"['cs.RO', '68T40']","[arxiv.Result.Author('Mikhail Kurenkov'), arxiv.Result.Author('Ivan Kalinov'), arxiv.Result.Author('Dzmitry Tsetserukou')]","Visual localization is an essential modern technology for robotics and
computer vision. Popular approaches for solving this task are image-based
methods. Nowadays, these methods have low accuracy and a long training time.
The reasons are the lack of rigid-body and projective geometry awareness,
landmark symmetry, and homogeneous error assumption. We propose a heterogeneous
loss function based on concentrated Gaussian distribution with the Lie group to
overcome these difficulties. Following our experiment, the proposed method
allows us to speed up the training process significantly (from 300 to 10
epochs) with acceptable error values.",0.19551575,0.12672493,-0.22126912,B
13816,"While prone
to compounding errors at test time [47], BC has shown surprising effectiveness when compared
to more complex learning-based algorithms in some robotic manipulation contexts [48, 49], which
motivates further study on how it can be done with point cloud observations.","BC performs supervised learning to learn a policy π with parameters
θ such that the predicted action aˆt = πθ(ot) is close to the ground truth label a∗t .","In future work, we will
explore combining our method with other imitation learning algorithms [50, 51, 52].",2022-11-16 16:02:48+00:00,ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds,cs.RO,['cs.RO'],"[arxiv.Result.Author('Daniel Seita'), arxiv.Result.Author('Yufei Wang'), arxiv.Result.Author('Sarthak J. Shetty'), arxiv.Result.Author('Edward Yao Li'), arxiv.Result.Author('Zackory Erickson'), arxiv.Result.Author('David Held')]","Point clouds are a widely available and canonical data modality which convey
the 3D geometry of a scene. Despite significant progress in classification and
segmentation from point clouds, policy learning from such a modality remains
challenging, and most prior works in imitation learning focus on learning
policies from images or state information. In this paper, we propose a novel
framework for learning policies from point clouds for robotic manipulation with
tools. We use a novel neural network, ToolFlowNet, which predicts dense
per-point flow on the tool that the robot controls, and then uses the flow to
derive the transformation that the robot should execute. We apply this
framework to imitation learning of challenging deformable object manipulation
tasks with continuous movement of tools, including scooping and pouring, and
demonstrate significantly improved performance over baselines which do not use
flow. We perform 50 physical scooping experiments with ToolFlowNet and attain
82% scooping success. See https://tinyurl.com/toolflownet for supplementary
material.",-0.27216002,0.1407254,-0.20240325,A
13837,"2,
facilitate further research in this area.","We share our dataset and predictive models to                           appearance and task,” International Journal of Social Robotics, vol.",no.,2022-11-17 08:05:53+00:00,Charting Visual Impression of Robot Hands,cs.RO,"['cs.RO', 'cs.HC']","[arxiv.Result.Author('Hasti Seifi'), arxiv.Result.Author('Steven A. Vasquez'), arxiv.Result.Author('Hyunyoung Kim'), arxiv.Result.Author('Pooyan Fazli')]","A wide variety of robotic hands have been designed to date. Yet, we do not
know how users perceive these hands and feel about interacting with them. To
inform hand design for social robots, we compiled a dataset of 73 robot hands
and ran an online study, in which 160 users rated their impressions of the
hands using 17 rating scales. Next, we developed 17 regression models that can
predict user ratings (e.g., humanlike) from the design features of the hands
(e.g., number of fingers). The models have less than a 10-point error in
predicting the user ratings on a 0-100 scale. The shape of the fingertips,
color scheme, and size of the hands influence the user ratings the most. We
present simple guidelines to improve user impression of robot hands and outline
remaining questions for future work.",-0.2214778,0.06389264,-0.28245437,A
14115,"The results above stress the importance of further studying human perceptions during HRC in order to inform
AI methods for achieving explainability and transparency [Vouros, 2022].","Another factor
that impacts the collaboration of humans with automated artiﬁcial systems is the ability for prediciton [Sahaï et al.,
2017].","The goal of the present study was to endow a UR3 cobot in real-world and in real-time with TL capabilities and to
study the impact of TL on the performance of the team, the speed of learning, as well as the impact of HRC on human’s
subjective perceptions regarding the robot, the collaboration and their selves.",2022-11-23 16:02:00+00:00,Enhancing team performance with transfer-learning during real-world human-robot collaboration,cs.RO,"['cs.RO', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Athanasios C. Tsitos'), arxiv.Result.Author('Maria Dagioglou')]","Socially aware robots should be able, among others, to support fluent
human-robot collaboration in tasks that require interdependent actions in order
to be solved. Towards enhancing mutual performance, collaborative robots should
be equipped with adaptation and learning capabilities. However, co-learning can
be a time consuming procedure. For this reason, transferring knowledge from an
expert could potentially boost the overall team performance. In the present
study, transfer learning was integrated in a deep Reinforcement Learning (dRL)
agent. In a real-time and real-world set-up, two groups of participants had to
collaborate with a cobot under two different conditions of dRL agents; one that
was transferring knowledge and one that did not. A probabilistic policy reuse
method was used for the transfer learning (TL). The results showed that there
was a significant difference between the performance of the two groups; TL
halved the time needed for the training of new participants to the task.
Moreover, TL also affected the subjective performance of the teams and enhanced
the perceived fluency. Finally, in many cases the objective performance metrics
did not correlate with the subjective ones providing interesting insights about
the design of transparent and explainable cobot behaviour.",-0.067944594,-0.018645069,-0.16182667,A
14128,"formance of different methods and Marknet with different
                                                                       parameters more comprehensively and accurately, we com-
   To further study the localization performance of the irreg-         pute the precision, recall, and loss of the results as follows:
ular markers in contact areas by different methods, the test
dataset is divided into two parts: one named easy is labelled          precision = PT /(PT + PF)  (9)
by OpenCV only, containing 43107 markers, and the other
named hard is labelled by manual correction, containing 271            recall = PT /T             (10)

                                                                       loss = D/PT                (11)

   1https://docs.opencv.org/3.4/d0/d7a/classcv_1_                      in which (PT ) is the number of correctly predicted points,
1SimpleBlobDetector.html                                               and (PF) is the number of wrongly predicted points.","Marknet dataset is randomly divided into three parts of 6:2:2,            3) Performance indicator: In order to evaluate the per-
which are used as train, validation and test datasets.","T
                                                                       represents the actual number of markers, and D is the sum of
                                                                       the squares of the distances between all correctly predicted
                                                                       points and their ground-truth labels.",2022-11-24 05:54:57+00:00,Real-Time Marker Localization Learning for GelStereo Tactile Sensing,cs.RO,['cs.RO'],"[arxiv.Result.Author('Shujuan Liu'), arxiv.Result.Author('Shaowei Cui'), arxiv.Result.Author('Chaofan Zhang'), arxiv.Result.Author('Yinghao Cai'), arxiv.Result.Author('Shuo Wang')]","Visuotactile sensing technology is becoming more popular in tactile sensing,
but the effectiveness of the existing marker detection localization methods
remains to be further explored. Instead of contour-based blob detection, this
paper presents a learning-based marker localization network for GelStereo
visuotactile sensing called Marknet. Specifically, the Marknet presents a grid
regression architecture to incorporate the distribution of the GelStereo
markers. Furthermore, a marker rationality evaluator (MRE) is modelled to
screen suitable prediction results. The experimental results show that the
Marknet combined with MRE achieves 93.90% precision for irregular markers in
contact areas, which outperforms the traditional contour-based blob detection
method by a large margin of 42.32%. Meanwhile, the proposed learning-based
marker localization method can achieve better real-time performance beyond the
blob detection interface provided by the OpenCV library through GPU
acceleration, which we believe will lead to considerable perceptual sensitivity
gains in various robotic manipulation tasks.",0.25613895,0.0017184522,-0.19010845,C
14228,"Despite the progress, further research and testing are cru-       Other causes are model mismatch, noise, or actuation
                                        cial to realize the advantages of DRL, be implementable in        delays.","faults in the simulator and overestimates its performance
                                                                                                          compared to the target domain (Muratore et al., 2021).","real world and automotive industry standard (You et al.,
                                                                                                          In this paper, we propose a DRL training and testing
                                           This work is part of FOCETA project that has received funding  environment relying on Digital Twin (DT).",2022-11-27 16:08:32+00:00,Reinforcement Learning from Simulation to Real World Autonomous Driving using Digital Twin,cs.RO,['cs.RO'],"[arxiv.Result.Author('Kevin Voogd'), arxiv.Result.Author('Jean Pierre Allamaa'), arxiv.Result.Author('Javier Alonso-Mora'), arxiv.Result.Author('Tong Duy Son')]","Reinforcement learning (RL) is a promising solution for autonomous vehicles
to deal with complex and uncertain traffic environments. The RL training
process is however expensive, unsafe, and time consuming. Algorithms are often
developed first in simulation and then transferred to the real world, leading
to a common sim2real challenge that performance decreases when the domain
changes. In this paper, we propose a transfer learning process to minimize the
gap by exploiting digital twin technology, relying on a systematic and
simultaneous combination of virtual and real world data coming from vehicle
dynamics and traffic scenarios. The model and testing environment are evolved
from model, hardware to vehicle in the loop and proving ground testing stages,
similar to standard development cycle in automotive industry. In particular, we
also integrate other transfer learning techniques such as domain randomization
and adaptation in each stage. The simulation and real data are gradually
incorporated to accelerate and make the transfer learning process more robust.
The proposed RL methodology is applied to develop a path following steering
controller for an autonomous electric vehicle. After learning and deploying the
real-time RL control policy on the vehicle, we obtained satisfactory and safe
control performance already from the first deployment, demonstrating the
advantages of the proposed digital twin based learning process.",0.22387968,-0.07557131,0.10001276,C
14495,"Beside
the seminal work of Bouabdallah, the collaboration of Robert Mahony, Vi-
jay Kumar and Peter Corke yielded several papers that attracted substantial
attention and paved the path for further research in the ﬁeld of UAV con-
trol.","It was argued that this was the consequence of switching nature of the sliding
mode control that caused unwanted oscillations within UAV control.","The methods proposed by the abovementioned authors have been vali-
dated both in simulated environment and on real UAVs.",2022-12-02 14:43:25+00:00,"Octocopter Design: Modelling, Control and Motion Planning",cs.RO,['cs.RO'],"[arxiv.Result.Author('Nedim Osmic'), arxiv.Result.Author('Adnan Tahirovic'), arxiv.Result.Author('Bakir Lacevic')]","This book provides a solution to the control and motion planning design for
an octocopter system. It includes a particular choice of control and motion
planning algorithms which is based on the authors' previous research work, so
it can be used as a reference design guidance for students, researchers as well
as autonomous vehicles hobbyists. The control is constructed based on a fault
tolerant approach aiming to increase the chances of the system to detect and
isolate a potential failure in order to produce feasible control signals to the
remaining active motors. The used motion planning algorithm is risk-aware by
means that it takes into account the constraints related to the fault-dependant
and mission-related maneuverability analysis of the octocopter system during
the planning stage. Such a planner generates only those reference trajectories
along which the octocopter system would be safe and capable of good tracking in
case of a single motor fault and of majority of double motor fault scenarios.
The control and motion planning algorithms presented in the book aim to
increase the overall reliability of the system for completing the mission.",-0.11053016,-0.17302673,0.33131373,A
14526,"In Section II, we start with a formal problem        in the ǫ-clearance2 free space Fǫ, parametrized by a scalar
description for the design of time governors for safe path             path parameter s P rsmin, smaxs Ď R, which can be either au-
following, which shall serve a basis for further research on           tomatically generated by a standard off-the-shelf path planner
this topic.","This paper introduces a new generic time governor frame-                                            ω
work for online feedback time parametrization of a reference              For path-following control, we consider a Lipschitz-conti-
path for provably correct and safe path-following control              nuous collision-free reference path ppsq : rsmin, smaxs Ñ Fǫ
around obstacles.","In Section III, we present a generic time governor         [4] or manually speciﬁed by the user such that ppsq P Fǫ for
design approach using feedback motion prediction and discuss           all s P rsmin, smaxs.",2022-12-02 20:54:52+00:00,Time Governors for Safe Path-Following Control,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY', '93C85, 68T40, 70Q05', 'I.2.9; I.2.8']",[arxiv.Result.Author('Ömür Arslan')],"Safe and smooth robot motion around obstacles is an essential skill for
autonomous robots, especially when operating around people and other robots.
Conventionally, due to real-time operation requirements and onboard computation
limitations, many robot motion planning and control methods follow a two-step
approach: first construct a (e.g., piecewise linear) collision-free reference
path for a simplified robot model, and then execute the reference plan via
path-following control for a more accurate and complex robot model. A challenge
of such a decoupled robot motion planning and control method for highly dynamic
robotic systems is ensuring the safety of path-following control as well as the
successful completion of the reference plan. In this paper, we introduce a
novel dynamical systems approach for online closed-loop time parametrization,
called $\textit{a time governor}$, of a reference path for provably correct and
safe path-following control based on feedback motion prediction, where the
safety of robot motion under path-following control is continuously monitored
using predicted robot motion. After introducing the general framework of time
governors for safe path following, we present an example application for the
fully actuated high-order robot dynamics using
proportional-and-higher-order-derivative (PhD) path-following control whose
feedback motion prediction is performed by Lyapunov ellipsoids and Vandemonde
simplexes. In numerical simulations, we investigate the role of reference
position and velocity feedback, and motion prediction on path-following
performance and robot motion.",0.07565468,-0.06472602,0.3422777,C
14647,"Furthermore, further research on       [11] A.","In the future, we plan to use bigger UAVs to carry             183--190, 2021.
actual packages and integrate the recharging station with
the current scenario.",A. T. S. E. &.,2022-12-06 21:54:58+00:00,UAVs for Industries and Supply Chain Management,cs.RO,['cs.RO'],"[arxiv.Result.Author('Shrutarv Awasthi'), arxiv.Result.Author('Nils Gramse'), arxiv.Result.Author('Dr. Christopher Reining'), arxiv.Result.Author('Moritz Roidl')]","This work aims at showing that it is feasible and safe to use a swarm of
Unmanned Aerial Vehicles (UAVs) indoors alongside humans. UAVs are increasingly
being integrated under the Industry 4.0 framework. UAV swarms are primarily
deployed outdoors in civil and military applications, but the opportunities for
using them in manufacturing and supply chain management are immense. There is
extensive research on UAV technology, e.g., localization, control, and computer
vision, but less research on the practical application of UAVs in industry. UAV
technology could improve data collection and monitoring, enhance
decision-making in an Internet of Things framework and automate time-consuming
and redundant tasks in the industry. However, there is a gap between the
technological developments of UAVs and their integration into the supply chain.
Therefore, this work focuses on automating the task of transporting packages
utilizing a swarm of small UAVs operating alongside humans. MoCap system, ROS,
and unity are used for localization, inter-process communication and
visualization. Multiple experiments are performed with the UAVs in wander and
swarm mode in a warehouse like environment.",0.1668233,-0.040756136,0.19795987,C
14785,"The effects of ITS
architecture we build can serve as the testbed for further research    applications on CV or pedestrian can be investigated through
and development of ITS applications on CV and pedestrian.","The proposed framework is expected to              technology that projects all the traffic participants into a digital
provide guidance to the future Digital Twin research, and the          road network in a real-time manner.",simulation in the digital world.,2022-12-08 18:36:38+00:00,Towards Next Generation of Pedestrian and Connected Vehicle In-the-loop Research: A Digital Twin Simulation Framework,cs.RO,['cs.RO'],"[arxiv.Result.Author('Zijin Wang'), arxiv.Result.Author('Ou Zheng'), arxiv.Result.Author('Liangding Li'), arxiv.Result.Author('Mohamed Abdel-Aty'), arxiv.Result.Author('Carolina Cruz-Neira'), arxiv.Result.Author('Zubayer Islam')]","Digital Twin is an emerging technology that replicates real-world entities
into a digital space. It has attracted increasing attention in the
transportation field and many researchers are exploring its future applications
in the development of Intelligent Transportation System (ITS) technologies.
Connected vehicles (CVs) and pedestrians are among the major traffic
participants in ITS. However, the usage of Digital Twin in research involving
both CV and pedestrian remains largely unexplored. In this study, a Digital
Twin framework for CV and pedestrian in-the-loop simulation is proposed. The
proposed framework consists of the physical world, the digital world, and data
transmission in between. The features for the entities (CV and pedestrian) that
need digital twined are divided into external state and internal state, and the
attributes in each state are described. We also demonstrate a sample
architecture under the proposed Digital Twin framework, which is based on
Carla-Sumo Co-simulation and Cave automatic virtual environment (CAVE). The
proposed framework is expected to provide guidance to the future Digital Twin
research, and the architecture we build can serve as the testbed for further
research and development of ITS applications on CV and pedestrian.",0.25694513,0.110732295,-0.036209997,B
14790,"Therefore, 3D localization
is worth further researching, which is highly important for drone applications.","Theoreti-

cally, it is claimed that most of them can be extended to the 3D case, but in practice, localization
in the vertical axis comes with a much higher error than the x-y axis.","This issue is
discussed more deeply in [13,38].",2022-12-09 21:54:33+00:00,A Review of Radio Frequency Based Localization for Aerial and Ground Robots with 5G Future Perspectives,cs.RO,['cs.RO'],"[arxiv.Result.Author('Meisam Kabiri'), arxiv.Result.Author('Claudio Cimarelli'), arxiv.Result.Author('Hriday Bavle'), arxiv.Result.Author('Jose Luis Sanchez-Lopez'), arxiv.Result.Author('Holger Voos')]","Efficient localization plays a vital role in many modern applications of
Unmanned Ground Vehicles (UGV) and Unmanned aerial vehicles (UAVs), which would
contribute to improved control, safety, power economy, etc. The ubiquitous 5G
NR (New Radio) cellular network will provide new opportunities for enhancing
localization of UAVs and UGVs. In this paper, we review the radio frequency
(RF) based approaches for localization. We review the RF features that can be
utilized for localization and investigate the current methods suitable for
Unmanned vehicles under two general categories: range-based and fingerprinting.
The existing state-of-the-art literature on RF-based localization for both UAVs
and UGVs is examined, and the envisioned 5G NR for localization enhancement,
and the future research direction are explored.",-0.06603793,0.3492424,0.1014672,B
14791,"The impact of handover is also worth further researching,
especially for high mobility scenarios.","In the uplink,
which is crucial for ofﬂoading, the data rate in 5G NR is supposed to be signiﬁcantly improved,
but [122] recorded the maximum of 67 Mbit/s in the uplink for a ﬂying drone showing no
improvement compared to 4G.","[122] shows that the handover rate between LTE and 5G
is too high, which is unacceptable.",2022-12-09 21:54:33+00:00,A Review of Radio Frequency Based Localization for Aerial and Ground Robots with 5G Future Perspectives,cs.RO,['cs.RO'],"[arxiv.Result.Author('Meisam Kabiri'), arxiv.Result.Author('Claudio Cimarelli'), arxiv.Result.Author('Hriday Bavle'), arxiv.Result.Author('Jose Luis Sanchez-Lopez'), arxiv.Result.Author('Holger Voos')]","Efficient localization plays a vital role in many modern applications of
Unmanned Ground Vehicles (UGV) and Unmanned aerial vehicles (UAVs), which would
contribute to improved control, safety, power economy, etc. The ubiquitous 5G
NR (New Radio) cellular network will provide new opportunities for enhancing
localization of UAVs and UGVs. In this paper, we review the radio frequency
(RF) based approaches for localization. We review the RF features that can be
utilized for localization and investigate the current methods suitable for
Unmanned vehicles under two general categories: range-based and fingerprinting.
The existing state-of-the-art literature on RF-based localization for both UAVs
and UGVs is examined, and the envisioned 5G NR for localization enhancement,
and the future research direction are explored.",0.31309277,-0.07183312,0.16322434,C
14792,"Therefore, 3D localization is worth further researching, which is highly important for drone
applications.","Theoretically, it is claimed that most
of them can be extended to the 3D case, but in practice, localization in the vertical axis comes with a much higher
error than the x-y axis.","This issue is discussed more deeply in [13, 38].",2022-12-09 21:54:33+00:00,A Review of Radio Frequency Based Localization for Aerial and Ground Robots with 5G Future Perspectives,cs.RO,['cs.RO'],"[arxiv.Result.Author('Meisam Kabiri'), arxiv.Result.Author('Claudio Cimarelli'), arxiv.Result.Author('Hriday Bavle'), arxiv.Result.Author('Jose Luis Sanchez-Lopez'), arxiv.Result.Author('Holger Voos')]","Efficient localization plays a vital role in many modern applications of
Unmanned Ground Vehicles (UGV) and Unmanned aerial vehicles (UAVs), which would
contribute to improved control, safety, power economy, etc. The ubiquitous 5G
NR (New Radio) cellular network will provide new opportunities for enhancing
localization of UAVs and UGVs. In this paper, we review the radio frequency
(RF) based approaches for localization. We review the RF features that can be
utilized for localization and investigate the current methods suitable for
Unmanned vehicles under two general categories: range-based and fingerprinting.
The existing state-of-the-art literature on RF-based localization for both UAVs
and UGVs is examined, and the envisioned 5G NR for localization enhancement,
and the future research direction are explored.",-0.06659575,0.35328338,0.10469022,B
14793,"The impact of handover is also worth further researching, especially for high mobility
scenarios.","In the uplink, which is crucial for ofﬂoading, the data rate in 5G NR is supposed to
be signiﬁcantly improved, but [122] recorded the maximum of 67 Mbit/s in the uplink for a ﬂying drone showing no
improvement compared to 4G.","[122] shows that the handover rate between LTE and 5G is too high, which is unacceptable.",2022-12-09 21:54:33+00:00,A Review of Radio Frequency Based Localization for Aerial and Ground Robots with 5G Future Perspectives,cs.RO,['cs.RO'],"[arxiv.Result.Author('Meisam Kabiri'), arxiv.Result.Author('Claudio Cimarelli'), arxiv.Result.Author('Hriday Bavle'), arxiv.Result.Author('Jose Luis Sanchez-Lopez'), arxiv.Result.Author('Holger Voos')]","Efficient localization plays a vital role in many modern applications of
Unmanned Ground Vehicles (UGV) and Unmanned aerial vehicles (UAVs), which would
contribute to improved control, safety, power economy, etc. The ubiquitous 5G
NR (New Radio) cellular network will provide new opportunities for enhancing
localization of UAVs and UGVs. In this paper, we review the radio frequency
(RF) based approaches for localization. We review the RF features that can be
utilized for localization and investigate the current methods suitable for
Unmanned vehicles under two general categories: range-based and fingerprinting.
The existing state-of-the-art literature on RF-based localization for both UAVs
and UGVs is examined, and the envisioned 5G NR for localization enhancement,
and the future research direction are explored.",0.31309277,-0.07183312,0.16322434,C
14817,"4 and labeled using lowercase letters to further study velocity                    prototyped the design and the snapshots of the ﬂight tests
vector ﬁelds.",4.,These vector ﬁelds can reveal information                            using an external power supply is shown in Fig.,2022-12-10 20:13:51+00:00,Wake-Based Locomotion Gait Design for Aerobat,cs.RO,"['cs.RO', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Eric Sihite'), arxiv.Result.Author('Alireza Ramezani')]","Flying animals, such as bats, fly through their fluidic environment as they
create air jets and form wake structures downstream of their flight path. Bats,
in particular, dynamically morph their highly flexible and dexterous armwing to
manipulate their fluidic environment which is key to their agility and flight
efficiency. This paper presents the theoretical and numerical analysis of the
wake-structure-based gait design inspired by bat flight for flapping robots
using the notion of reduced-order models and unsteady aerodynamic model
incorporating Wagner function. The objective of this paper is to introduce the
notion of gait design for flapping robots by systematically searching the
design space in the context of optimization. The solution found using our gait
design framework was used to design and test a flapping robot.",0.095211186,-0.19183204,0.13362518,C
14905,"We hope to encourage further study focused on                     hardens and is able to maintain a grip via the interaction
                                        active vibrational control of jamming in soft robotics to improve           between the target object, the granular material, and the soft
                                        performance and increase diversity of potential applications.","The resultant stress from the atmospheric pressure
                                        is also dependent on temporal properties of the induced                     causes a jamming phase transition to occur; the gripper
                                        waveform.",membrane.,2022-12-13 11:26:23+00:00,Active Vibration Fluidization for Granular Jamming Grippers,cs.RO,"['cs.RO', 'cond-mat.soft']","[arxiv.Result.Author('Cameron Coombe'), arxiv.Result.Author('James Brett'), arxiv.Result.Author('Raghav Mishra'), arxiv.Result.Author('Gary W. Delaney'), arxiv.Result.Author('David Howard')]","Granular jamming has recently become popular in soft robotics with widespread
applications including industrial gripping, surgical robotics and haptics.
Previous work has investigated the use of various techniques that exploit the
nature of granular physics to improve jamming performance, however this is
generally underrepresented in the literature compared to its potential impact.
We present the first research that exploits vibration-based fluidisation
actively (e.g., during a grip) to elicit bespoke performance from granular
jamming grippers. We augment a conventional universal gripper with a
computer-controllled audio exciter, which is attached to the gripper via a 3D
printed mount, and build an automated test rig to allow large-scale data
collection to explore the effects of active vibration. We show that vibration
in soft jamming grippers can improve holding strength. In a series of studies,
we show that frequency and amplitude of the waveforms are key determinants to
performance, and that jamming performance is also dependent on temporal
properties of the induced waveform. We hope to encourage further study focused
on active vibrational control of jamming in soft robotics to improve
performance and increase diversity of potential applications.",-0.30727065,-0.17239673,0.26933327,A
15053,"From an ethical point of view, we actively con-
nation is admissible would be subject to further research.","However, to what extent such a cultural determi-                  met.","sider a risk threshold here, which we denote as the
                                                                            maximum acceptable risk.",2022-12-16 16:53:51+00:00,An Ethical Trajectory Planning Algorithm for Autonomous Vehicles,cs.RO,"['cs.RO', 'cs.CY']","[arxiv.Result.Author('Maximilian Geisslinger'), arxiv.Result.Author('Franziska Poszler'), arxiv.Result.Author('Markus Lienkamp')]","With the rise of AI and automation, moral decisions are being put into the
hands of algorithms that were formerly the preserve of humans. In autonomous
driving, a variety of such decisions with ethical implications are made by
algorithms for behavior and trajectory planning. Therefore, we present an
ethical trajectory planning algorithm with a framework that aims at a fair
distribution of risk among road users. Our implementation incorporates a
combination of five essential ethical principles: minimization of the overall
risk, priority for the worst-off, equal treatment of people, responsibility,
and maximum acceptable risk. To the best of the authors' knowledge, this is the
first ethical algorithm for trajectory planning of autonomous vehicles in line
with the 20 recommendations from the EU Commission expert group and with
general applicability to various traffic situations. We showcase the ethical
behavior of our algorithm in selected scenarios and provide an empirical
analysis of the ethical principles in 2000 scenarios. The code used in this
research is available as open-source software.",0.1120794,-0.3397984,-0.07466285,C
15219,"Therefore, the closed-loop control of            The contributions of this work are threefold:
position and velocity during the attitude tracking stage still          1) A trajectory planner based on waypoint optimization
needs further study.","This causes their accuracies to be suscep-
tible to disturbances.","using reachability analysis is proposed to adapt to uncertainties
                                                                     in target predictions and tracking control.",2022-12-21 07:56:33+00:00,Perching on Moving Inclined Surfaces using Uncertainty Tolerant Planner and Thrust Regulation,cs.RO,['cs.RO'],"[arxiv.Result.Author('Sensen Liu'), arxiv.Result.Author('Wenkang Hu'), arxiv.Result.Author('Zhaoying Wang'), arxiv.Result.Author('Wei Dong'), arxiv.Result.Author('Xinjun Sheng')]","Quadrotors with the ability to perch on moving inclined surfaces can save
energy and extend their travel distance by leveraging ground vehicles.
Achieving dynamic perching places high demands on the performance of trajectory
planning and terminal state accuracy in SE(3). However, in the perching
process, uncertainties in target surface prediction, tracking control and
external disturbances may cause trajectory planning failure or lead to
unacceptable terminal errors. To address these challenges, we first propose a
trajectory planner that considers adaptation to uncertainties in target
prediction and tracking control. To facilitate this work, the reachable set of
quadrotors' states is first analyzed. The states whose reachable sets possess
the largest coverage probability for uncertainty targets, are defined as
optimal waypoints. Subsequently, an approach to seek local optimal waypoints
for static and moving uncertainty targets is proposed. A real-time trajectory
planner based on optimized waypoints is developed accordingly. Secondly, thrust
regulation is also implemented in the terminal attitude tracking stage to
handle external disturbances. When a quadrotor's attitude is commanded to align
with target surfaces, the thrust is optimized to minimize terminal errors. This
makes the terminal position and velocity be controlled in closed-loop manner.
Therefore, the resistance to disturbances and terminal accuracy is improved.
Extensive simulation experiments demonstrate that our methods can improve the
accuracy of terminal states under uncertainties. The success rate is
approximately increased by $50\%$ compared to the two-end planner without
thrust regulation. Perching on the rear window of a car is also achieved using
our proposed heterogeneous cooperation system outdoors. This validates the
feasibility and practicality of our methods.",-0.11225017,0.002182924,0.24444756,A
15262,"Our macro-discrete results show that we get improved control over
the model, and the idealized microscopic examples support that this is a fruitful avenue
for further study.","In addition, to counteract known uncertainty during
robot team deployment, we propose a feedback control strategy which follows a speciﬁed
desired distribution.","Future work includes understanding diﬀerent types of nonlinear models,
and further characterizing the uncertainty by seeking upper and lower bounds on the
macroscopic model.",2022-12-22 01:39:46+00:00,Stochastic Nonlinear Ensemble Modeling and Control for Robot Team Environmental Monitoring,cs.RO,['cs.RO'],"[arxiv.Result.Author('Victoria Edwards'), arxiv.Result.Author('Thales C. Silva'), arxiv.Result.Author('M. Ani Hsieh')]","We seek methods to model, control, and analyze robot teams performing
environmental monitoring tasks. During environmental monitoring, the goal is to
have teams of robots collect various data throughout a fixed region for
extended periods of time. Standard bottom-up task assignment methods do not
scale as the number of robots and task locations increases and require
computationally expensive replanning. Alternatively, top-down methods have been
used to combat computational complexity, but most have been limited to the
analysis of methods which focus on transition times between tasks. In this
work, we study a class of nonlinear macroscopic models which we use to control
a time-varying distribution of robots performing different tasks throughout an
environment. Our proposed ensemble model and control maintains desired
time-varying populations of robots by leveraging naturally occurring
interactions between robots performing tasks. We validate our approach at
multiple fidelity levels including experimental results, suggesting the
effectiveness of our approach to perform environmental monitoring.",-0.27925557,-0.0922901,0.15730818,A
15341,"Despite the existence of unstructured elements such
as differences in the visual system and the robot controller,       As future work, we aim to apply our demonstration-guided
several assumptions of the dynamics model, as well as            approach to a broader range of manipulation tasks (namely,
immeasurable friction, the feedback controller is able to        beyond pushing problems), which requires further study on
cope with these different mismatches and track the reference     how to extract constraints from demonstration, and how to
trajectory successfully.",target.,Fig.,2022-12-24 20:51:13+00:00,Demonstration-guided Optimal Control for Long-term Non-prehensile Planar Manipulation,cs.RO,['cs.RO'],"[arxiv.Result.Author('Teng Xue'), arxiv.Result.Author('Hakan Girgin'), arxiv.Result.Author('Teguh Santoso Lembono'), arxiv.Result.Author('Sylvain Calinon')]","Long-term non-prehensile planar manipulation is a challenging task for robot
planning and feedback control. It is characterized by underactuation, hybrid
control, and contact uncertainty. One main difficulty is to determine contact
points and directions, which involves joint logic and geometrical reasoning in
the modes of the dynamics model. To tackle this issue, we propose a
demonstration-guided hierarchical optimization framework to achieve offline
task and motion planning (TAMP). Our work extends the formulation of the
dynamics model of the pusher-slider system to include separation mode with face
switching cases, and solves a warm-started TAMP problem by exploiting human
demonstrations. We show that our approach can cope well with the local minima
problems currently present in the state-of-the-art solvers and determine a
valid solution to the task. We validate our results in simulation and
demonstrate its applicability on a pusher-slider system with real Franka Emika
robot in the presence of external disturbances.",-0.4214043,-0.07593177,0.06395766,A
