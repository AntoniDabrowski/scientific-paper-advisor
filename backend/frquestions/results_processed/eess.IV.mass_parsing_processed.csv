Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
13,"However, there are no
such public datasets of other imaging modalities available for further research
and development of AI solutions for multi-modality datasets.","Other than DFU images, researchers have used
other imaging modalities such as infrared, Magnetic Resonance Imaging (MRI),
and ﬂuorescence imaging for the management of DFU.","In many research studies, thermal infrared imaging has been proven to be a
useful technique in the clinical management of DFU.",2022-01-01 10:45:07+00:00,Development of Diabetic Foot Ulcer Datasets: An Overview,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Moi Hoon Yap'), arxiv.Result.Author('Connah Kendrick'), arxiv.Result.Author('Neil D. Reeves'), arxiv.Result.Author('Manu Goyal'), arxiv.Result.Author('Joseph M. Pappachan'), arxiv.Result.Author('Bill Cassidy')]","This paper provides conceptual foundation and procedures used in the
development of diabetic foot ulcer datasets over the past decade, with a
timeline to demonstrate progress. We conduct a survey on data capturing methods
for foot photographs, an overview of research in developing private and public
datasets, the related computer vision tasks (detection, segmentation and
classification), the diabetic foot ulcer challenges and the future direction of
the development of the datasets. We report the distribution of dataset users by
country and year. Our aim is to share the technical challenges that we
encountered together with good practices in dataset development, and provide
motivation for other researchers to participate in data sharing in this domain.",-0.16526942,0.004547038,0.17237142,C
123,"patches with a sliding stride of 12 × 128 × 128 to get the           To further study the impact of domain difference on the
prediction for the whole CT. Data augmentation in the training       performance of transfer learning, we conduct an ablation study
stage includes random cropping, ﬂipping and re-scaling, and          on our proposed SVD-Net.",Visual comparison of our proposed methods with competing pre-training methods and training from scratch on the LITS dataset.,"Speciﬁcally, by keeping the pre-
no test-time augmentation is implemented.",2022-01-05 03:11:21+00:00,Advancing 3D Medical Image Analysis with Variable Dimension Transform based Supervised 3D Pre-training,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Shu Zhang'), arxiv.Result.Author('Zihao Li'), arxiv.Result.Author('Hong-Yu Zhou'), arxiv.Result.Author('Jiechao Ma'), arxiv.Result.Author('Yizhou Yu')]","The difficulties in both data acquisition and annotation substantially
restrict the sample sizes of training datasets for 3D medical imaging
applications. As a result, constructing high-performance 3D convolutional
neural networks from scratch remains a difficult task in the absence of a
sufficient pre-training parameter. Previous efforts on 3D pre-training have
frequently relied on self-supervised approaches, which use either predictive or
contrastive learning on unlabeled data to build invariant 3D representations.
However, because of the unavailability of large-scale supervision information,
obtaining semantically invariant and discriminative representations from these
learning frameworks remains problematic. In this paper, we revisit an
innovative yet simple fully-supervised 3D network pre-training framework to
take advantage of semantic supervisions from large-scale 2D natural image
datasets. With a redesigned 3D network architecture, reformulated natural
images are used to address the problem of data scarcity and develop powerful 3D
representations. Comprehensive experiments on four benchmark datasets
demonstrate that the proposed pre-trained models can effectively accelerate
convergence while also improving accuracy for a variety of 3D medical imaging
tasks such as classification, segmentation and detection. In addition, as
compared to training from scratch, it can save up to 60% of annotation efforts.
On the NIH DeepLesion dataset, it likewise achieves state-of-the-art detection
performance, outperforming earlier self-supervised and fully-supervised
pre-training approaches, as well as methods that do training from scratch. To
facilitate further development of 3D medical models, our code and pre-trained
model weights are publicly available at https://github.com/urmagicsmine/CSPR.",-0.018299397,0.27143386,0.09932627,A
245,"Our plug-and-play GP-module
                                                                    can also be applied to existing segmentation methods to fur-
                                                                    ther improve their performance with fewer parameters and
                                                                    fewer FLOPs, which shed the light on the further research.","We test our method on three dataset
                                                                    and visualize several feature maps to proof the effectiveness
                                                                    and efﬁciency of our method.",4.3.,2022-01-07 19:53:15+00:00,GPU-Net: Lightweight U-Net with more diverse features,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Heng Yu'), arxiv.Result.Author('Di Fan'), arxiv.Result.Author('Weihu Song')]","Image segmentation is an important task in the medical image field and many
convolutional neural networks (CNNs) based methods have been proposed, among
which U-Net and its variants show promising performance. In this paper, we
propose GP-module and GPU-Net based on U-Net, which can learn more diverse
features by introducing Ghost module and atrous spatial pyramid pooling (ASPP).
Our method achieves better performance with more than 4 times fewer parameters
and 2 times fewer FLOPs, which provides a new potential direction for future
research. Our plug-and-play module can also be applied to existing segmentation
methods to further improve their performance.",0.10576069,0.044838175,-0.20934397,A
423,"However, it is beyond the scope of this work and
   The penalty function formulated for this work consists of              calls for further research.",DISCUSSION                                 problems.,L1-norm continuity term and L2-norm data term.,2022-01-12 08:58:19+00:00,ALTRUIST: Alternating Direction Method of Multipliers for Total Variation Regularization in Ultrasound Strain Imaging,eess.IV,"['eess.IV', 'physics.med-ph']","[arxiv.Result.Author('Md Ashikuzzaman'), arxiv.Result.Author('Hassan Rivaz')]","Ultrasound strain imaging, which delineates mechanical properties to detect
tissue abnormalities, involves estimating the time-delay between two
radio-frequency (RF) frames collected before and after tissue deformation. The
existing regularized optimization-based time-delay estimation (TDE) techniques
suffer from at least one of the following drawbacks: 1) The regularizer is not
aligned with tissue deformation physics due to taking only the first-order
displacement derivative into account. 2) The L2-norm of the displacement
derivatives, which oversmoothes the estimated time-delay, is utilized as the
regularizer. 3) The absolute value function's sharp corner should be
approximated by a smooth function to facilitate the optimization of L1-norm.
Herein, to resolve these shortcomings, we propose employing the alternating
direction method of multipliers (ADMM) for optimizing a novel cost function
consisting of L2-norm data fidelity term, L1-norm first- and second-order
spatial continuity terms. ADMM empowers the proposed algorithm to use different
techniques for optimizing different parts of the cost function and obtain
high-contrast strain images with smooth background and sharp boundaries. We
name our technique ADMM for totaL variaTion RegUlarIzation in ultrasound STrain
imaging (ALTRUIST). In extensive simulation, phantom, and in vivo experiments,
ALTRUIST substantially outperforms GLUE, OVERWIND, and L1-SOUL, three
recently-published TDE algorithms, both qualitatively and quantitatively.
ALTRUIST yields 89%, 88%, and 26% improvements of contrast-to-noise ratio over
L1-SOUL for simulated, phantom, and in vivo liver cancer datasets,
respectively. We will publish the ALTRUIST code after the acceptance of this
paper at http://code.sonography.ai.",0.17004293,-0.14827359,-0.15696886,B
424,"However, these modiﬁcations
are beyond the scope of this work and call for further research.","Another option is to acquire RF frames
with known loading conditions and incorporate FEM in an al-
ternating optimization process.","This work adopts a manual parameter-selection scheme to
optimize the performance of ALTRUIST.",2022-01-12 08:58:19+00:00,Ultrasound Strain Imaging using ADMM,eess.IV,"['eess.IV', 'physics.med-ph']","[arxiv.Result.Author('Md Ashikuzzaman'), arxiv.Result.Author('Hassan Rivaz')]","Ultrasound strain imaging, which delineates mechanical properties to detect
tissue abnormalities, involves estimating the time-delay between two
radio-frequency (RF) frames collected before and after tissue deformation. The
existing regularized optimization-based time-delay estimation (TDE) techniques
suffer from at least one of the following drawbacks: 1) The regularizer is not
aligned with tissue deformation physics due to taking only the first-order
displacement derivative into account. 2) The L2-norm of the displacement
derivatives, which oversmooths the estimated time-delay, is utilized as the
regularizer. 3) The absolute value function should be approximated by a smooth
function to facilitate the optimization of L1-norm. Herein, to resolve these
shortcomings, we propose employing the alternating direction method of
multipliers (ADMM) for optimizing a novel cost function consisting of L2-norm
data fidelity term and L1-norm first- and second-order spatial continuity
terms. ADMM empowers the proposed algorithm to use different techniques for
optimizing different parts of the cost function and obtain high-contrast strain
images with smooth background and sharp boundaries. We name our technique ADMM
for totaL variaTion RegUlarIzation in ultrasound STrain imaging (ALTRUIST). In
extensive simulation, phantom, and in vivo experiments, ALTRUIST substantially
outperforms GLUE, OVERWIND, and L1-SOUL, three recently-published TDE
algorithms, both qualitatively and quantitatively. ALTRUIST yields 118%, 104%,
and 72% improvements of contrast-to-noise ratio over L1-SOUL for simulated,
phantom, and in vivo liver cancer datasets, respectively. We will publish the
ALTRUIST code after the acceptance of this paper at http://code.sonography.ai.",0.3230493,-0.114161715,-0.14409325,B
879,"Improving
marker localization might need changes to the forward model         Additionally, we set the x and y components of the deforma-
used, an aspect that needs further research; however, in our        tion ﬁeld to zero.","These small weighted
markers were removed with a further thresholding step, where         Dt,z(r, P ) = (P0 + P1x + P2y + P3x2 + P4y2 + P5xy)t (31)
markers with weights less than 0.1 were discarded.","It is probable that our assumed deformation
experiments, marker localization did not have a signiﬁcant          ﬁeld was insufﬁcient to model sample deformation in the
effect on deformation estimation accuracy, as seen from the         experimental data.",2022-01-21 14:03:32+00:00,SparseAlign: A Super-Resolution Algorithm for Automatic Marker Localization and Deformation Estimation in Cryo-Electron Tomography,eess.IV,"['eess.IV', 'cs.CV', 'cs.NA', 'math.NA', 'math.OC', 'q-bio.QM', '65K10, 65M32']","[arxiv.Result.Author('Poulami Somanya Ganguly'), arxiv.Result.Author('Felix Lucka'), arxiv.Result.Author('Holger Kohr'), arxiv.Result.Author('Erik Franken'), arxiv.Result.Author('Hermen Jan Hupkes'), arxiv.Result.Author('K Joost Batenburg')]","Tilt-series alignment is crucial to obtaining high-resolution reconstructions
in cryo-electron tomography. Beam-induced local deformation of the sample is
hard to estimate from the low-contrast sample alone, and often requires
fiducial gold bead markers. The state-of-the-art approach for deformation
estimation uses (semi-)manually labelled marker locations in projection data to
fit the parameters of a polynomial deformation model. Manually-labelled marker
locations are difficult to obtain when data are noisy or markers overlap in
projection data. We propose an alternative mathematical approach for
simultaneous marker localization and deformation estimation by extending a
grid-free super-resolution algorithm first proposed in the context of
single-molecule localization microscopy. Our approach does not require labelled
marker locations; instead, we use an image-based loss where we compare the
forward projection of markers with the observed data. We equip this marker
localization scheme with an additional deformation estimation component and
solve for a reduced number of deformation parameters. Using extensive numerical
studies on marker-only samples, we show that our approach automatically finds
markers and reliably estimates sample deformation without labelled marker data.
We further demonstrate the applicability of our approach for a broad range of
model mismatch scenarios, including experimental electron tomography data of
gold markers on ice.",0.0472762,-0.16467445,0.11386917,C
928,"Considering the need for a critical
approach to chest classiﬁcation and analysis of biases[23],[22],[20], we carry out
further research into the analysis of the deep learning model decisions using the
COVIDx dataset.","They create a map of lung areas important for COVID-19 detec-
tion and claim a production-ready solution.","Recently, a new patch-based learning technique [3], [10], [25] emerged as a
successful method for robust model learning and generalization.",2022-01-23 20:35:45+00:00,POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tomasz Szczepański'), arxiv.Result.Author('Arkadiusz Sitek'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Szymon Płotka')]","A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available (https://github.com/tomek1911/POTHER).",-0.28895766,0.44306034,-0.10442303,A_centroid
929,"Considering the need for a critical
approach to chest classiﬁcation and analysis of biases[23],[22],[20], we carry out
further research into the analysis of the deep learning model decisions using the
COVIDx dataset.","They create a map of lung areas important for COVID-19 detec-
tion and claim a production-ready solution.","Recently, a new patch-based learning technique [3], [10], [25] emerged as a
successful method for robust model learning and generalization.",2022-01-23 20:35:45+00:00,POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tomasz Szczepański'), arxiv.Result.Author('Arkadiusz Sitek'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Szymon Płotka')]","A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available (https://github.com/tomek1911/POTHER).",-0.28895766,0.44306034,-0.10442303,A
930,"Considering the need for a critical
approach to chest classiﬁcation and analysis of biases[23],[22],[20], we carry out
further research into the analysis of the deep learning model decisions using the
COVIDx dataset.","They create a map of lung areas important for COVID-19 detec-
tion and claim a production-ready solution.","Recently, a new patch-based learning technique [3], [10], [25] emerged as a
successful method for robust model learning and generalization.",2022-01-23 20:35:45+00:00,POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tomasz Szczepański'), arxiv.Result.Author('Arkadiusz Sitek'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Szymon Płotka')]","A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available at (https://github.com/tomek1911/POTHER).",-0.28895766,0.44306034,-0.10442303,A
931,"Considering the need for a critical
approach to chest classiﬁcation and analysis of biases[23],[22],[20], we carry out
further research into the analysis of the deep learning model decisions using the
COVIDx dataset.","They create a map of lung areas important for COVID-19 detec-
tion and claim a production-ready solution.","Recently, a new patch-based learning technique [3], [10], [25] emerged as a
successful method for robust model learning and generalization.",2022-01-23 20:35:45+00:00,POTHER: Patch-Voted Deep Learning-Based Chest X-ray Bias Analysis for COVID-19 Detection,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tomasz Szczepański'), arxiv.Result.Author('Arkadiusz Sitek'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Szymon Płotka')]","A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available at (https://github.com/tomek1911/POTHER).",-0.28895766,0.44306034,-0.10442303,A
932,"Considering the need for a critical
approach to chest classiﬁcation and analysis of biases[23],[22],[20], we carry out
further research into the analysis of the deep learning model decisions using the
COVIDx dataset.","They create a map of lung areas important for COVID-19 detec-
tion and claim a production-ready solution.","Recently, a new patch-based learning technique [3], [10], [25] emerged as a
successful method for robust model learning and generalization.",2022-01-23 20:35:45+00:00,POTHER: Patch-Voted Deep Learning-Based Chest X-ray Bias Analysis for COVID-19 Detection,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tomasz Szczepański'), arxiv.Result.Author('Arkadiusz Sitek'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Szymon Płotka')]","A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available at (https://github.com/tomek1911/POTHER).",-0.28895766,0.44306034,-0.10442303,A
1016,"Based on our evaluations, we identify challenges and
deﬁne requirements for further research in this ﬁeld to help physicians make objective
wound assessments, support treatment decisions, and ultimately improve patient quality
of life.","We also investigated the behavior of US intensities in
diﬀerent sections of the wound.","2 Material and methods

2.1 Dataset

All human subjects research was done with approval from the Internal Review Board at
the University of California, San Diego.",2022-01-25 18:12:54+00:00,Initial Investigations Towards Non-invasive Monitoring of Chronic Wound Healing Using Deep Learning and Ultrasound Imaging,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Maja Schlereth'), arxiv.Result.Author('Daniel Stromer'), arxiv.Result.Author('Yash Mantri'), arxiv.Result.Author('Jason Tsujimoto'), arxiv.Result.Author('Katharina Breininger'), arxiv.Result.Author('Andreas Maier'), arxiv.Result.Author('Caesar Anderson'), arxiv.Result.Author('Pranav S. Garimella'), arxiv.Result.Author('Jesse V. Jokerst')]","Chronic wounds including diabetic and arterial/venous insufficiency injuries
have become a major burden for healthcare systems worldwide. Demographic
changes suggest that wound care will play an even bigger role in the coming
decades. Predicting and monitoring response to therapy in wound care is
currently largely based on visual inspection with little information on the
underlying tissue. Thus, there is an urgent unmet need for innovative
approaches that facilitate personalized diagnostics and treatments at the
point-of-care. It has been recently shown that ultrasound imaging can monitor
response to therapy in wound care, but this work required onerous manual image
annotations. In this study, we present initial results of a deep learning-based
automatic segmentation of cross-sectional wound size in ultrasound images and
identify requirements and challenges for future research on this application.
Evaluation of the segmentation results underscores the potential of the
proposed deep learning approach to complement non-invasive imaging with Dice
scores of 0.34 (U-Net, FCN) and 0.27 (ResNet-U-Net) but also highlights the
need for improving robustness further. We conclude that deep learning-supported
analysis of non-invasive ultrasound images is a promising area of research to
automatically extract cross-sectional wound size and depth information with
potential value in monitoring response to therapy.",-0.31702042,-0.23564786,-0.10972031,C
1086,"Although our method
                                                                  obtained excellent fusion performance, the generality of the
      Method   Time(s)(↓)  Parameters(m)(↓)                       already trained model on different hyperspectral datasets with
       GSA       4.7202             —                             different spectral bands still needs further research.","In addition,
AVERAGE TESTING TIME AND MODEL PARAMETERS ON                      this fusion network could be adapted to other ratios’ fusion
                                                                  tasks for it achieved excellent generalization performance on
                          CHIKUSEI DATASET                        different fusion ratios’ experiments.","SFIM     17.9695             —
                 4.3358             —                             ACKNOWLEDGEMENT
     Wavelet    12.5180             —                                The authors would like to thank Baidu AI Studio for
MTF GLP HPM     43.9069             —
                 0.0048            0.13                           providing the computing power supports.",2022-01-27 04:37:23+00:00,Unmixing based PAN guided fusion network for hyperspectral imagery,eess.IV,['eess.IV'],"[arxiv.Result.Author('Shuangliang Li'), arxiv.Result.Author('Yugang Tian'), arxiv.Result.Author('Hao Xia'), arxiv.Result.Author('Qingwei Liu')]","The hyperspectral image (HSI) has been widely used in many applications due
to its fruitful spectral information. However, the limitation of imaging
sensors has reduced its spatial resolution that causes detail loss. One
solution is to fuse the low spatial resolution hyperspectral image (LR-HSI) and
the panchromatic image (PAN) with inverse features to get the high-resolution
hyperspectral image (HR-HSI). Most of the existing fusion methods just focus on
small fusion ratios like 4 or 6, which might be impractical for some large
ratios' HSI and PAN image pairs. Moreover, the ill-posedness of restoring
detail information in HSI with hundreds of bands from PAN image with only one
band has not been solved effectively, especially under large fusion ratios.
Therefore, a lightweight unmixing-based pan-guided fusion network (Pgnet) is
proposed to mitigate this ill-posedness and improve the fusion performance
significantly. Note that the fusion process of the proposed network is under
the projected low-dimensional abundance subspace with an extremely large fusion
ratio of 16. Furthermore, based on the linear and nonlinear relationships
between the PAN intensity and abundance, an interpretable PAN detail inject
network (PDIN) is designed to inject the PAN details into the abundance feature
efficiently. Comprehensive experiments on simulated and real datasets
demonstrate the superiority and generality of our method over several
state-of-the-art (SOTA) methods qualitatively and quantitatively (The codes in
pytorch and paddle versions and dataset could be available at
https://github.com/rs-lsl/Pgnet). (This is a improved version compared with the
publication in Tgrs with the modification in the deduction of the PDIN block.)",0.15472044,-0.019351099,-0.2445278,B
1127,"As we could not demonstrate a beneﬁt in preliminary experiments yet,
further research will have to investigate the potential of combining PA and US images
and the limitations of this strategy under a more careful preprocessing scheme.","Speciﬁcally, it is able to
enrich the imaging data with functional information beyond the morphological structures
visible in US.","The dataset in the current study was acquired from a small study population with a
comparatively simple network architecture and further evaluation has to be performed to
validate our ﬁndings.",2022-01-27 16:37:19+00:00,Automatic Classification of Neuromuscular Diseases in Children Using Photoacoustic Imaging,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Maja Schlereth'), arxiv.Result.Author('Daniel Stromer'), arxiv.Result.Author('Katharina Breininger'), arxiv.Result.Author('Alexandra Wagner'), arxiv.Result.Author('Lina Tan'), arxiv.Result.Author('Andreas Maier'), arxiv.Result.Author('Ferdinand Knieling')]","Neuromuscular diseases (NMDs) cause a significant burden for both healthcare
systems and society. They can lead to severe progressive muscle weakness,
muscle degeneration, contracture, deformity and progressive disability. The
NMDs evaluated in this study often manifest in early childhood. As subtypes of
disease, e.g. Duchenne Muscular Dystropy (DMD) and Spinal Muscular Atrophy
(SMA), are difficult to differentiate at the beginning and worsen quickly, fast
and reliable differential diagnosis is crucial. Photoacoustic and ultrasound
imaging has shown great potential to visualize and quantify the extent of
different diseases. The addition of automatic classification of such image data
could further improve standard diagnostic procedures. We compare deep
learning-based 2-class and 3-class classifiers based on VGG16 for
differentiating healthy from diseased muscular tissue. This work shows
promising results with high accuracies above 0.86 for the 3-class problem and
can be used as a proof of concept for future approaches for earlier diagnosis
and therapeutic monitoring of NMDs.",-0.066784576,0.0688659,0.2570183,A
1128,"US and PA could potentially
be used to monitor treatment process and medication response to support personalized
treatment planning and assessment if further research conﬁrms predictability of severity
for each disease.","It can therefore be postulated that the disease progression also has
clear impact both on the measured US and PA signal.","Despite the slightly lower performance of PA compared to US, PA’s
inherent properties will make it an interesting choice for future applications.",2022-01-27 16:37:19+00:00,Automatic Classification of Neuromuscular Diseases in Children Using Photoacoustic Imaging,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Maja Schlereth'), arxiv.Result.Author('Daniel Stromer'), arxiv.Result.Author('Katharina Breininger'), arxiv.Result.Author('Alexandra Wagner'), arxiv.Result.Author('Lina Tan'), arxiv.Result.Author('Andreas Maier'), arxiv.Result.Author('Ferdinand Knieling')]","Neuromuscular diseases (NMDs) cause a significant burden for both healthcare
systems and society. They can lead to severe progressive muscle weakness,
muscle degeneration, contracture, deformity and progressive disability. The
NMDs evaluated in this study often manifest in early childhood. As subtypes of
disease, e.g. Duchenne Muscular Dystropy (DMD) and Spinal Muscular Atrophy
(SMA), are difficult to differentiate at the beginning and worsen quickly, fast
and reliable differential diagnosis is crucial. Photoacoustic and ultrasound
imaging has shown great potential to visualize and quantify the extent of
different diseases. The addition of automatic classification of such image data
could further improve standard diagnostic procedures. We compare deep
learning-based 2-class and 3-class classifiers based on VGG16 for
differentiating healthy from diseased muscular tissue. This work shows
promising results with high accuracies above 0.86 for the 3-class problem and
can be used as a proof of concept for future approaches for earlier diagnosis
and therapeutic monitoring of NMDs.",-0.32325256,-0.37294716,-0.19114704,C
1148,"Instead, our paper has questioned the traditional practice of
assigning hard labels for histopathology image classiﬁcation and proposed a simple yet
effective alternative that invites further research in this direction.","As such, we do not consider
our encouraging empirical results as validation of our label smoothing methods for
all histopathology tasks.","In this paper, we have proposed two well-motivated sets of label smoothing meth-
ods for improving the calibration and accuracy of histopathology image classiﬁers:
agreement-aware label smoothing, which uses annotator agreement data, and conﬁdence-
aware label smoothing, which uses model conﬁdence.",2022-01-28 00:13:09+00:00,Calibrating Histopathology Image Classifiers using Label Smoothing,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Jerry Wei'), arxiv.Result.Author('Lorenzo Torresani'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Saeed Hassanpour')]","The classification of histopathology images fundamentally differs from
traditional image classification tasks because histopathology images naturally
exhibit a range of diagnostic features, resulting in a diverse range of
annotator agreement levels. However, examples with high annotator disagreement
are often either assigned the majority label or discarded entirely when
training histopathology image classifiers. This widespread practice often
yields classifiers that do not account for example difficulty and exhibit poor
model calibration. In this paper, we ask: can we improve model calibration by
endowing histopathology image classifiers with inductive biases about example
difficulty?
  We propose several label smoothing methods that utilize per-image annotator
agreement. Though our methods are simple, we find that they substantially
improve model calibration, while maintaining (or even improving) accuracy. For
colorectal polyp classification, a common yet challenging task in
gastrointestinal pathology, we find that our proposed agreement-aware label
smoothing methods reduce calibration error by almost 70%. Moreover, we find
that using model confidence as a proxy for annotator agreement also improves
calibration and accuracy, suggesting that datasets without multiple annotators
can still benefit from our proposed label smoothing methods via our proposed
confidence-aware label smoothing methods.
  Given the importance of calibration (especially in histopathology image
analysis), the improvements from our proposed techniques merit further
exploration and potential implementation in other histopathology image
classification tasks.",-0.070116594,0.027993966,-0.0014231894,A
1179,"proposes something similar, but further research is needed.","Only the work in [153]    to 0.83, 0.82, 0.70 and 0.69, respectively.","4) Pose estimation: In [164], fetal pose is estimated by
   A uniﬁed framework to biometry estimation from multiple        localizing 16 landmarks, including joints.",2022-01-28 17:22:44+00:00,A Review on Deep-Learning Algorithms for Fetal Ultrasound-Image Analysis,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Maria Chiara Fiorentino'), arxiv.Result.Author('Francesca Pia Villani'), arxiv.Result.Author('Mariachiara Di Cosmo'), arxiv.Result.Author('Emanuele Frontoni'), arxiv.Result.Author('Sara Moccia')]","Deep-learning (DL) algorithms are becoming the standard for processing
ultrasound (US) fetal images. Despite a large number of survey papers already
present in this field, most of them are focusing on a broader area of
medical-image analysis or not covering all fetal US DL applications. This paper
surveys the most recent work in the field, with a total of 145 research papers
published after 2017. Each paper is analyzed and commented on from both the
methodology and application perspective. We categorized the papers in (i) fetal
standard-plane detection, (ii) anatomical-structure analysis, and (iii)
biometry parameter estimation. For each category, main limitations and open
issues are presented. Summary tables are included to facilitate the comparison
among the different approaches. Publicly-available datasets and performance
metrics commonly used to assess algorithm performance are summarized, too. This
paper ends with a critical summary of the current state of the art on DL
algorithms for fetal US image analysis and a discussion on current challenges
that have to be tackled by researchers working in the field to translate the
research methodology into the actual clinical practice.",-0.10996796,-0.12806167,0.13583021,C
1317,"On the contrary, adopting
We perform several ablation experiments in the Binary Air-         the result of the geodesic distance branch to repair the result
way Segmentation Dataset to further study the effect of each       of the ﬁnetune branch (G2F) is more appropriate.","Ablation Study                                                better continuity in space and can produce results with fewer
                                                                   fractures than the ﬁnetune branch.",component in our segmentation framework.,2022-01-29 09:05:46+00:00,BREAK: Bronchi Reconstruction by gEodesic transformation And sKeleton embedding,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Weihao Yu'), arxiv.Result.Author('Hao Zheng'), arxiv.Result.Author('Minghui Zhang'), arxiv.Result.Author('Hanxiao Zhang'), arxiv.Result.Author('Jiayuan Sun'), arxiv.Result.Author('Jie Yang')]","Airway segmentation is critical for virtual bronchoscopy and computer-aided
pulmonary disease analysis. In recent years, convolutional neural networks
(CNNs) have been widely used to delineate the bronchial tree. However, the
segmentation results of the CNN-based methods usually include many
discontinuous branches, which need manual repair in clinical use. A major
reason for the breakages is that the appearance of the airway wall can be
affected by the lung disease as well as the adjacency of the vessels, while the
network tends to overfit to these special patterns in the training set. To
learn robust features for these areas, we design a multi-branch framework that
adopts the geodesic distance transform to capture the intensity changes between
airway lumen and wall. Another reason for the breakages is the intra-class
imbalance. Since the volume of the peripheral bronchi may be much smaller than
the large branches in an input patch, the common segmentation loss is not
sensitive to the breakages among the distal branches. Therefore, in this paper,
a breakage-sensitive regularization term is designed and can be easily combined
with other loss functions. Extensive experiments are conducted on publicly
available datasets. Compared with state-of-the-art methods, our framework can
detect more branches while maintaining competitive segmentation performance.",0.05970691,-0.0748395,0.038734715,C
1397,"On further studying the
mathematics behind classification, it came into light that the most sophisticated method to achieve the desired
results was to use Deep Learning models.","Within the scope of this research, we found that classification using K-nearest Neighbours or Support Vector
Machines or even Decision Trees yielded low precision as well as accuracies.","We tried various mathematical models with and without the use of
Transfer Learning, but in the end, it was concluded that the depth and quality of activation provided by
pretrained models, were of no-match, hence we paired our knowledge of mathematics and created a Dense
Convolutional Network model which gave us accuracy of over 86.6%.",2022-02-01 17:11:41+00:00,Classification of Skin Cancer Images using Convolutional Neural Networks,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG', 'I.4.9; J.3']","[arxiv.Result.Author('Kartikeya Agarwal'), arxiv.Result.Author('Tismeet Singh')]","Skin cancer is the most common human malignancy(American Cancer Society)
which is primarily diagnosed visually, starting with an initial clinical
screening and followed potentially by dermoscopic(related to skin) analysis, a
biopsy and histopathological examination. Skin cancer occurs when errors
(mutations) occur in the DNA of skin cells. The mutations cause the cells to
grow out of control and form a mass of cancer cells. The aim of this study was
to try to classify images of skin lesions with the help of convolutional neural
networks. The deep neural networks show humongous potential for image
classification while taking into account the large variability exhibited by the
environment. Here we trained images based on the pixel values and classified
them on the basis of disease labels. The dataset was acquired from an Open
Source Kaggle Repository(Kaggle Dataset)which itself was acquired from
ISIC(International Skin Imaging Collaboration) Archive. The training was
performed on multiple models accompanied with Transfer Learning. The highest
model accuracy achieved was over 86.65%. The dataset used is publicly available
to ensure credibility and reproducibility of the aforementioned result.",-0.030366603,0.4055955,-0.24898998,A
1406,"for point cloud compression, thus stimulating further research.","may also provide additional information including the laser         Despite these studies, however, there is no accepted standard
                                         intensity, scan angle, and reﬂectance properties of the surface.","In the last decades, LiDARs have been extensively applied
                                         to different research ﬁelds, including agriculture (e.g., for          Based on the above introduction, in this paper we provide
                                         topographic analysis and prediction of soil properties), military   a comparison between 2D and 3D compression methods for
                                         (e.g., for ground surveillance, navigation, search and rescue)      point clouds, shedding light on the most promising scheme(s)
                                         and architecture (e.g., for detecting subtle topographic fea-       to guarantee accurate though efﬁcient compression before
                                         tures).",2022-02-01 19:29:32+00:00,Point Cloud Compression for Efficient Data Broadcasting: A Performance Comparison,eess.IV,"['eess.IV', 'cs.NI']","[arxiv.Result.Author('Francesco Nardo'), arxiv.Result.Author('Davide Peressoni'), arxiv.Result.Author('Paolo Testolina'), arxiv.Result.Author('Marco Giordani'), arxiv.Result.Author('Andrea Zanella')]","The worldwide commercialization of fifth generation (5G) wireless networks
and the exciting possibilities offered by connected and autonomous vehicles
(CAVs) are pushing toward the deployment of heterogeneous sensors for tracking
dynamic objects in the automotive environment. Among them, Light Detection and
Ranging (LiDAR) sensors are witnessing a surge in popularity as their
application to vehicular networks seem particularly promising. LiDARs can
indeed produce a three-dimensional (3D) mapping of the surrounding environment,
which can be used for object detection, recognition, and topography. These data
are encoded as a point cloud which, when transmitted, may pose significant
challenges to the communication systems as it can easily congest the wireless
channel. Along these lines, this paper investigates how to compress point
clouds in a fast and efficient way. Both 2D- and a 3D-oriented approaches are
considered, and the performance of the corresponding techniques is analyzed in
terms of (de)compression time, efficiency, and quality of the decompressed
frame compared to the original. We demonstrate that, thanks to the matrix form
in which LiDAR frames are saved, compression methods that are typically applied
for 2D images give equivalent results, if not better, than those specifically
designed for 3D point clouds.",0.23744196,-0.032074332,0.1730282,B
1445,"Moreover, although the new proposed setup improved the performance of the deep neural
network under investigation, further research is still required to design more generalized
simulation setups.","Currently, the exact GT for our measured data is not available.","For example, the presented setup is a 2D model, and therefore oﬀ-
axis reﬂections from z-direction are not modeled due to high computational eﬀort and
memory requirements.",2022-02-01 11:09:35+00:00,Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Farnaz Khun Jush'), arxiv.Result.Author('Markus Biele'), arxiv.Result.Author('Peter M. Dueppenbecker'), arxiv.Result.Author('Andreas Maier')]","Ultrasound b-mode imaging is a qualitative approach and diagnostic quality
strongly depends on operators' training and experience. Quantitative approaches
can provide information about tissue properties; therefore, can be used for
identifying various tissue types, e.g., speed-of-sound in the tissue can be
used as a biomarker for tissue malignancy, especially in breast imaging. Recent
studies showed the possibility of speed-of-sound reconstruction using deep
neural networks that are fully trained on simulated data. However, because of
the ever present domain shift between simulated and measured data, the
stability and performance of these models in real setups are still under
debate. In this study, we investigated the impacts of training data diversity
on the robustness of these networks by using multiple kinds of geometrical and
natural simulated phantom structures. On the simulated data, we investigated
the performance of the networks on out-of-domain echogenicity, geometries, and
in the presence of noise. We further inspected the stability of employing such
tissue modeling in a real data acquisition setup. We demonstrated that training
the network with a joint set of datasets including both geometrical and natural
tissue models improves the stability of the predicted speed-of-sound values
both on simulated and measured data.",0.111694865,0.3110062,-0.07668335,A
1518,"As such it should focus on two
areas: impact (preferably in the practical, say, clinical context) and unresolved questions that
should be subjected to further research.","CONCLUSIONS
This section should offer a “takeaway message” for the reader.","[C-1] Concise presentation

The conclusion does not just reiterate the sentiments provided throughout the manuscript.",2022-02-03 21:46:59+00:00,Best Practices and Scoring System on Reviewing A.I. based Medical Imaging Papers: Part 1 Classification,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Timothy L. Kline'), arxiv.Result.Author('Felipe Kitamura'), arxiv.Result.Author('Ian Pan'), arxiv.Result.Author('Amine M. Korchi'), arxiv.Result.Author('Neil Tenenholtz'), arxiv.Result.Author('Linda Moy'), arxiv.Result.Author('Judy Wawira Gichoya'), arxiv.Result.Author('Igor Santos'), arxiv.Result.Author('Steven Blumer'), arxiv.Result.Author('Misha Ysabel Hwang'), arxiv.Result.Author('Kim-Ann Git'), arxiv.Result.Author('Abishek Shroff'), arxiv.Result.Author('Elad Walach'), arxiv.Result.Author('George Shih'), arxiv.Result.Author('Steve Langer')]","With the recent advances in A.I. methodologies and their application to
medical imaging, there has been an explosion of related research programs
utilizing these techniques to produce state-of-the-art classification
performance. Ultimately, these research programs culminate in submission of
their work for consideration in peer reviewed journals. To date, the criteria
for acceptance vs. rejection is often subjective; however, reproducible science
requires reproducible review. The Machine Learning Education Sub-Committee of
SIIM has identified a knowledge gap and a serious need to establish guidelines
for reviewing these studies. Although there have been several recent papers
with this goal, this present work is written from the machine learning
practitioners standpoint. In this series, the committee will address the best
practices to be followed in an A.I.-based study and present the required
sections in terms of examples and discussion of what should be included to make
the studies cohesive, reproducible, accurate, and self-contained. This first
entry in the series focuses on the task of image classification. Elements such
as dataset curation, data pre-processing steps, defining an appropriate
reference standard, data partitioning, model architecture and training are
discussed. The sections are presented as they would be detailed in a typical
manuscript, with content describing the necessary information that should be
included to make sure the study is of sufficient quality to be considered for
publication. The goal of this series is to provide resources to not only help
improve the review process for A.I.-based medical imaging papers, but to
facilitate a standard for the information that is presented within all
components of the research study. We hope to provide quantitative metrics in
what otherwise may be a qualitative review process.",-0.29462403,-0.43013924,-0.13666262,C
1574,"(CVPR),
available real-world old photo dataset that includes 200 pairs                        June 2019.
of authentic legacy photographs with corresponding “ground
truth” repaired by Photoshop experts, which we believe will                     [17] Z. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, and F. Wen,
facilitate further research on deep learning-based old photo                          “Bringing old photos back to life,” in Proc.",Pattern Recognit.,IEEE Conf.,2022-02-05 17:48:15+00:00,ROMNet: Renovate the Old Memories,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Runsheng Xu'), arxiv.Result.Author('Zhengzhong Tu'), arxiv.Result.Author('Yuanqi Du'), arxiv.Result.Author('Xiaoyu Dong'), arxiv.Result.Author('Jinlong Li'), arxiv.Result.Author('Zibo Meng'), arxiv.Result.Author('Jiaqi Ma'), arxiv.Result.Author('Hongkai YU')]","Renovating the memories in old photos is an intriguing research topic in
computer vision fields. These legacy images often suffer from severe and
commingled degradations such as cracks, noise, and color-fading, while lack of
large-scale paired old photo datasets makes this restoration task very
challenging. In this work, we present a novel reference-based end-to-end
learning framework that can jointly repair and colorize the degraded legacy
pictures. Specifically, the proposed framework consists of three modules: a
restoration sub-network for degradation restoration, a similarity sub-network
for color histogram matching and transfer, and a colorization subnet that
learns to predict the chroma elements of the images conditioned on chromatic
reference signals. The whole system takes advantage of the color histogram
priors in a given reference image, which vastly reduces the dependency on
large-scale training data. Apart from the proposed method, we also create, to
our knowledge, the first public and real-world old photo dataset with paired
ground truth for evaluating old photo restoration models, wherein each old
photo is paired with a manually restored pristine image by PhotoShop experts.
Our extensive experiments conducted on both synthetic and real-world datasets
demonstrate that our method significantly outperforms state-of-the-arts both
quantitatively and qualitatively.",0.17012069,0.34502333,0.047182675,A
1612,"How to better preserve the detail information in sand
the reason why both of them have higher CIE94 and CIEDE2000                     dust removal task needs further research and discussion.","In Figures 1 and 5, the results of CIDC           may cause local information distorted in the reconstruction
[11] and TTFIO [1] still exist the issue of color distortion, that is           images.",metrics.,2022-02-07 09:48:09+00:00,A comprehensive benchmark analysis for sand dust image reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Yazhong Si'), arxiv.Result.Author('Fan Yang'), arxiv.Result.Author('Ya Guo'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Yipu Yang')]","Numerous sand dust image enhancement algorithms have been proposed in recent
years. To our best acknowledge, however, most methods evaluated their
performance with no-reference way using few selected real-world images from
internet. It is unclear how to quantitatively analysis the performance of the
algorithms in a supervised way and how we could gauge the progress in the
field. Moreover, due to the absence of large-scale benchmark datasets, there
are no well-known reports of data-driven based method for sand dust image
enhancement up till now. To advance the development of deep learning-based
algorithms for sand dust image reconstruction, while enabling supervised
objective evaluation of algorithm performance. In this paper, we presented a
comprehensive perceptual study and analysis of real-world sand dust images,
then constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training
Convolutional Neural Networks (CNNs) and evaluating algorithms performance. In
addition, we adopted the existing image transformation neural network trained
on SIRB as baseline to illustrate the generalization of SIRB for training CNNs.
Finally, we conducted the qualitative and quantitative evaluation to
demonstrate the performance and limitations of the state-of-the-arts (SOTA),
which shed light on future research in sand dust image reconstruction.",0.27342585,-0.11178465,0.1056371,B
1701,"(2021) can potentially alleviate this problem and presents an interesting
direction for further research.","Also, advanced post-processing of the residual maps as in Mun˜oz-
Ram´ırez et al.","Lately, self-supervised methods that use artiﬁcial anomalies
for training became popular (Tan et al., 2020, 2021) and need further investigation.",2022-02-08 12:45:10+00:00,On the Pitfalls of Using the Residual Error as Anomaly Score,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Felix Meissen'), arxiv.Result.Author('Benedikt Wiestler'), arxiv.Result.Author('Georgios Kaissis'), arxiv.Result.Author('Daniel Rueckert')]","Many current state-of-the-art methods for anomaly localization in medical
images rely on calculating a residual image between a potentially anomalous
input image and its ""healthy"" reconstruction. As the reconstruction of the
unseen anomalous region should be erroneous, this yields large residuals as a
score to detect anomalies in medical images. However, this assumption does not
take into account residuals resulting from imperfect reconstructions of the
machine learning models used. Such errors can easily overshadow residuals of
interest and therefore strongly question the use of residual images as scoring
function. Our work explores this fundamental problem of residual images in
detail. We theoretically define the problem and thoroughly evaluate the
influence of intensity and texture of anomalies against the effect of imperfect
reconstructions in a series of experiments. Code and experiments are available
under https://github.com/FeliMe/residual-score-pitfalls",-0.10786697,0.096108384,-0.1402837,A
1925,"A good direction
for further research might be to use the embryonic volume error during training.","We observed that cases with a high relative embryonic volume error (EVerror >
Q3) had clear visual explanation for the failing of the registration: either the embryo lying
on the image border, against the uterine wall or the image had low quality.","Having
the embryonic volume available during training might simplify ﬁnding the embryo.",2022-02-14 10:40:51+00:00,Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('W. A. P. Bastiaansen'), arxiv.Result.Author('M. Rousian'), arxiv.Result.Author('R. P. M. Steegers-Theunissen'), arxiv.Result.Author('W. J. Niessen'), arxiv.Result.Author('A. H. J. Koning'), arxiv.Result.Author('S. Klein')]","Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight pregnancies were selected as atlas images. We evaluated different
fusion strategies to incorporate multiple atlases: 1) training the framework
using atlas images from a single pregnancy, 2) training the framework with data
of all available atlases and 3) ensembling of the frameworks trained per
pregnancy. To evaluate the performance, we calculated the Dice score over the
test set. We found that training the framework using all available atlases
outperformed ensembling and gave similar results compared to the best of all
frameworks trained on a single subject. Furthermore, we found that selecting
images from the four atlases closest in GA out of all available atlases,
regardless of the individual quality, gave the best results with a median Dice
score of 0.72. We conclude that our framework can accurately segment and
spatially align the embryo in first trimester 3D US images and is robust for
the variation in quality that existed in the available atlases. Our code is
publicly available at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.",-0.1698415,0.011013612,0.19437435,A
1926,"Finally, another interesting topic for further research is applying our framework to other
problems.","Having
the embryonic volume available during training might simplify ﬁnding the embryo.","We created a ﬂexible framework that easily can be adapted to work with or
without landmarks and with or without multiple atlas images.",2022-02-14 10:40:51+00:00,Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('W. A. P. Bastiaansen'), arxiv.Result.Author('M. Rousian'), arxiv.Result.Author('R. P. M. Steegers-Theunissen'), arxiv.Result.Author('W. J. Niessen'), arxiv.Result.Author('A. H. J. Koning'), arxiv.Result.Author('S. Klein')]","Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight pregnancies were selected as atlas images. We evaluated different
fusion strategies to incorporate multiple atlases: 1) training the framework
using atlas images from a single pregnancy, 2) training the framework with data
of all available atlases and 3) ensembling of the frameworks trained per
pregnancy. To evaluate the performance, we calculated the Dice score over the
test set. We found that training the framework using all available atlases
outperformed ensembling and gave similar results compared to the best of all
frameworks trained on a single subject. Furthermore, we found that selecting
images from the four atlases closest in GA out of all available atlases,
regardless of the individual quality, gave the best results with a median Dice
score of 0.72. We conclude that our framework can accurately segment and
spatially align the embryo in first trimester 3D US images and is robust for
the variation in quality that existed in the available atlases. Our code is
publicly available at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.",-0.14401749,0.12393712,0.027544549,A
1927,"To overcome this limitation, a good direction for further research to improve the
alignment results are to either add additional landmarks, add the embryonic volume error
to the loss, or incorporate the segmentations obtained by nn-UNet to improve the alignment
results.","This resulted in a wide spread in the results that
was propagated from the aﬃne to the nonrigid network for all metrics, and all atlas fusion
strategies.","These segmentations can be incorporated either a priori by segmenting the embryo
before alignment, or can be added to the loss as supervision.",2022-02-14 10:40:51+00:00,Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('W. A. P. Bastiaansen'), arxiv.Result.Author('M. Rousian'), arxiv.Result.Author('R. P. M. Steegers-Theunissen'), arxiv.Result.Author('W. J. Niessen'), arxiv.Result.Author('A. H. J. Koning'), arxiv.Result.Author('S. Klein')]","Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight subjects were selected as atlas. We evaluated different fusion
strategies to incorporate multiple atlases: 1) training the framework using
atlas images from a single subject, 2) training the framework with data of all
available atlases and 3) ensembling of the frameworks trained per subject. To
evaluate the performance, we calculated the Dice score over the test set. We
found that training the framework using all available atlases outperformed
ensembling and gave similar results compared to the best of all frameworks
trained on a single subject. Furthermore, we found that selecting images from
the four atlases closest in GA out of all available atlases, regardless of the
individual quality, gave the best results with a median Dice score of 0.72. We
conclude that our framework can accurately segment and spatially align the
embryo in first trimester 3D US images and is robust for the variation in
quality that existed in the available atlases. Our code is publicly available
at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.",-0.0733652,0.09436871,-0.011125986,A
1928,"The extension to non-singleton pregnancies is an interesting topic for further research,
which was not considered here.","Furthermore, dividing the images per week GA might negatively inﬂuence the results of the
images with a GA of for example 8+6, where the atlases for week 9 might be more similar
then the ones for week 8.","For our framework, this implies that it should be able to
deal with the presence of another (partial) embryo, or the possibility to align them both
independently.",2022-02-14 10:40:51+00:00,Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('W. A. P. Bastiaansen'), arxiv.Result.Author('M. Rousian'), arxiv.Result.Author('R. P. M. Steegers-Theunissen'), arxiv.Result.Author('W. J. Niessen'), arxiv.Result.Author('A. H. J. Koning'), arxiv.Result.Author('S. Klein')]","Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight subjects were selected as atlas. We evaluated different fusion
strategies to incorporate multiple atlases: 1) training the framework using
atlas images from a single subject, 2) training the framework with data of all
available atlases and 3) ensembling of the frameworks trained per subject. To
evaluate the performance, we calculated the Dice score over the test set. We
found that training the framework using all available atlases outperformed
ensembling and gave similar results compared to the best of all frameworks
trained on a single subject. Furthermore, we found that selecting images from
the four atlases closest in GA out of all available atlases, regardless of the
individual quality, gave the best results with a median Dice score of 0.72. We
conclude that our framework can accurately segment and spatially align the
embryo in first trimester 3D US images and is robust for the variation in
quality that existed in the available atlases. Our code is publicly available
at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.",-0.10095422,-0.057744294,-0.019242505,C
1929,"Finally, another interesting topic for further research is applying our framework to other
problems.","However, we used the
proposed framework to segment and spatially align the embryonic brain for the development
of a spatio-temporal model that can be used to explore correlations between maternal
periconceptional health and brain growth and development (Bastiaansen et al., 2022).","We created a ﬂexible framework that easily can be adapted to work with or
without landmarks and with or without multiple atlas images.",2022-02-14 10:40:51+00:00,Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('W. A. P. Bastiaansen'), arxiv.Result.Author('M. Rousian'), arxiv.Result.Author('R. P. M. Steegers-Theunissen'), arxiv.Result.Author('W. J. Niessen'), arxiv.Result.Author('A. H. J. Koning'), arxiv.Result.Author('S. Klein')]","Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight subjects were selected as atlas. We evaluated different fusion
strategies to incorporate multiple atlases: 1) training the framework using
atlas images from a single subject, 2) training the framework with data of all
available atlases and 3) ensembling of the frameworks trained per subject. To
evaluate the performance, we calculated the Dice score over the test set. We
found that training the framework using all available atlases outperformed
ensembling and gave similar results compared to the best of all frameworks
trained on a single subject. Furthermore, we found that selecting images from
the four atlases closest in GA out of all available atlases, regardless of the
individual quality, gave the best results with a median Dice score of 0.72. We
conclude that our framework can accurately segment and spatially align the
embryo in first trimester 3D US images and is robust for the variation in
quality that existed in the available atlases. Our code is publicly available
at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.",-0.16884457,-0.019311545,0.08321014,A
1968,"However,       The “Features” column in both tables denotes the encoders for
further research is necessary to accurately validate the biolog-    the patch-level representation: SUPERVISE-ResNet50, SWAV-
ical meaning of these sets of prototypical patterns.",prototypical patterns are histologically meaningful.,ResNet50 or ﬁne-tuned/retrained a CNN (Tuned).,2022-02-14 19:40:47+00:00,Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Quoc Dang Vu'), arxiv.Result.Author('Kashif Rajpoot'), arxiv.Result.Author('Shan E Ahmed Raza'), arxiv.Result.Author('Nasir Rajpoot')]","Diagnostic, prognostic and therapeutic decision-making of cancer in pathology
clinics can now be carried out based on analysis of multi-gigapixel tissue
images, also known as whole-slide images (WSIs). Recently, deep convolutional
neural networks (CNNs) have been proposed to derive unsupervised WSI
representations; these are attractive as they rely less on expert annotation
which is cumbersome. However, a major trade-off is that higher predictive power
generally comes at the cost of interpretability, posing a challenge to their
clinical use where transparency in decision-making is generally expected. To
address this challenge, we present a handcrafted framework based on deep CNN
for constructing holistic WSI-level representations. Building on recent
findings about the internal working of the Transformer in the domain of natural
language processing, we break down its processes and handcraft them into a more
transparent framework that we term as the Handcrafted Histological Transformer
or H2T. Based on our experiments involving various datasets consisting of a
total of 5,306 WSIs, the results demonstrate that H2T based holistic WSI-level
representations offer competitive performance compared to recent
state-of-the-art methods and can be readily utilized for various downstream
analysis tasks. Finally, our results demonstrate that the H2T framework can be
up to 14 times faster than the Transformer models.",-0.13755342,0.024202378,-0.27921635,A
1969,"However,
obtained from SWAV-ResNet50 [51], we extracted 16 proto-           further research is necessary to accurately validate the biolog-
typical patterns for each set.",Using patch-level representations        prototypical patterns are histologically meaningful.,While the number of prototypical    ical meaning of these sets of prototypical patterns.,2022-02-14 19:40:47+00:00,Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Quoc Dang Vu'), arxiv.Result.Author('Kashif Rajpoot'), arxiv.Result.Author('Shan E Ahmed Raza'), arxiv.Result.Author('Nasir Rajpoot')]","Diagnostic, prognostic and therapeutic decision-making of cancer in pathology
clinics can now be carried out based on analysis of multi-gigapixel tissue
images, also known as whole-slide images (WSIs). Recently, deep convolutional
neural networks (CNNs) have been proposed to derive unsupervised WSI
representations; these are attractive as they rely less on expert annotation
which is cumbersome. However, a major trade-off is that higher predictive power
generally comes at the cost of interpretability, posing a challenge to their
clinical use where transparency in decision-making is generally expected. To
address this challenge, we present a handcrafted framework based on deep CNN
for constructing holistic WSI-level representations. Building on recent
findings about the internal working of the Transformer in the domain of natural
language processing, we break down its processes and handcraft them into a more
transparent framework that we term as the Handcrafted Histological Transformer
or H2T. Based on our experiments involving various datasets consisting of a
total of 5,306 WSIs, the results demonstrate that H2T based holistic WSI-level
representations offer competitive performance compared to recent
state-of-the-art methods and can be readily utilized for various downstream
analysis tasks. Finally, our results demonstrate that the H2T framework can be
up to 14 times faster than the Transformer models.",-0.18048681,-0.093320444,-0.2535448,C
2206,"Moving forward, the architecture of CAGAN might be able to be applied to other image-to-image tasks, such as image
enhancing, and semantic segmentation, which is worthy of our further research.","As a result, we succeed in acquiring
precise, detailed, and artifact-free sparse-view CT images through our pipeline.","Also, extreme sparsity of projections
would lead to much loss of information and cause severe artifacts beyond any repairing algorithms.",2022-02-19 14:04:59+00:00,A Lightweight Dual-Domain Attention Framework for Sparse-View CT Reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Chang Sun'), arxiv.Result.Author('Ken Deng'), arxiv.Result.Author('Yitong Liu'), arxiv.Result.Author('Hongwen Yang')]","Computed Tomography (CT) plays an essential role in clinical diagnosis. Due
to the adverse effects of radiation on patients, the radiation dose is expected
to be reduced as low as possible. Sparse sampling is an effective way, but it
will lead to severe artifacts on the reconstructed CT image, thus sparse-view
CT image reconstruction has been a prevailing and challenging research area.
With the popularity of mobile devices, the requirements for lightweight and
real-time networks are increasing rapidly. In this paper, we design a novel
lightweight network called CAGAN, and propose a dual-domain reconstruction
pipeline for parallel beam sparse-view CT. CAGAN is an adversarial
auto-encoder, combining the Coordinate Attention unit, which preserves the
spatial information of features. Also, the application of Shuffle Blocks
reduces the parameters by a quarter without sacrificing its performance. In the
Radon domain, the CAGAN learns the mapping between the interpolated data and
fringe-free projection data. After the restored Radon data is reconstructed to
an image, the image is sent into the second CAGAN trained for recovering the
details, so that a high-quality image is obtained. Experiments indicate that
the CAGAN strikes an excellent balance between model complexity and
performance, and our pipeline outperforms the DD-Net and the DuDoNet.",0.142426,0.27194393,0.32407618,A
2268,"Evaluation on the BraTS’18
multimodal brain MRI dataset suggests that the method is promising and opens
new avenues for further research.","We show that our
network is not only scalable to any number of input modalities, but also capable
of picking up style variations within each modality.","MIST GAN  9

References

 1.",2022-02-21 17:50:40+00:00,MIST GAN: Modality Imputation Using Style Transfer for MRI,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Jaya Chandra Raju'), arxiv.Result.Author('Kompella Subha Gayatri'), arxiv.Result.Author('Keerthi Ram'), arxiv.Result.Author('Rajeswaran Rangasami'), arxiv.Result.Author('Rajoo Ramachandran'), arxiv.Result.Author('Mohansankar Sivaprakasam')]","MRI entails a great amount of cost, time and effort for the generation of all
the modalities that are recommended for efficient diagnosis and treatment
planning. Recent advancements in deep learning research show that generative
models have achieved substantial improvement in the aspects of style transfer
and image synthesis. In this work, we formulate generating the missing MR
modality from existing MR modalities as an imputation problem using style
transfer. With a multiple-to-one mapping, we model a network that accommodates
domain specific styles in generating the target image. We analyse the style
diversity both within and across MR modalities. Our model is tested on the
BraTS'18 dataset and the results obtained are observed to be on par with the
state-of-the-art in terms of visual metrics, SSIM and PSNR. After being
evaluated by two expert radiologists, we show that our model is efficient,
extendable, and suitable for clinical applications.",-0.05564628,0.25005424,0.015601737,A
2291,"Another directions of further research
may may include the extension of the proposed approach to non-sparse, non-convolutional
features and generalization to other types of tomography problems.","A rigorous convergence analysis
of the presented scheme remains an open issue.","Also, multiple feature
reconstruction (similar to the method [14, 34]) seems to be an interesting future research
direction.",2022-02-22 08:37:14+00:00,Feature reconstruction from incomplete tomographic data without detour,eess.IV,"['eess.IV', 'cs.CV', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Simon Göppel'), arxiv.Result.Author('Jürgen Frikel'), arxiv.Result.Author('Markus Haltmeier')]","In this paper, we consider the problem of feature reconstruction from
incomplete x-ray CT data. Such problems occurs, e.g., as a result of dose
reduction in the context medical imaging. Since image reconstruction from
incomplete data is a severely ill-posed problem, the reconstructed images may
suffer from characteristic artefacts or missing features, and significantly
complicate subsequent image processing tasks (e.g., edge detection or
segmentation). In this paper, we introduce a novel framework for the robust
reconstruction of convolutional image features directly from CT data, without
the need of computing a reconstruction firs. Within our framework we use
non-linear (variational) regularization methods that can be adapted to a
variety of feature reconstruction tasks and to several limited data situations
. In our numerical experiments, we consider several instances of edge
reconstructions from angularly undersampled data and show that our approach is
able to reliably reconstruct feature maps in this case.",0.08656207,0.090468496,0.38708144,A
2365,"Neural Architecture Search (NAS), i.e., the automated design of neural network architectures, can eﬀectively
                                          and eﬃciently search through this space of possible network architecture designs and ﬁnd a network that is
                                          highly tailored to the task at hand.4 While research on NAS for medical image segmentation has not been
                                          as elaborate as for natural image classiﬁcation, it has already shown promising results by outperforming the
                                          SotA architectures.5–7 In our opinion, further research on NAS for medical image segmentation can make its
                                          contributions even more signiﬁcant.","This gives rise
                                          to an inconceivably large amount of network architecture design possibilities, which is impossible to manually
                                          navigate through in an exhaustive fashion, or even by means of intelligent design, while ensuring the best choices
                                          are made.","NAS involves three key components: (1) The search space (the set of all possible networks given the speciﬁed
                                          architectural constraints); (2) the search algorithm (the algorithm to navigate the search space); (3) performance
                                          estimation (the choices made to score a network’s performance, such that these networks can be ranked by the
                                          search algorithm).",2022-02-23 10:32:35+00:00,Mixed-Block Neural Architecture Search for Medical Image Segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Martijn M. A. Bosma'), arxiv.Result.Author('Arkadiy Dushatskiy'), arxiv.Result.Author('Monika Grewal'), arxiv.Result.Author('Tanja Alderliesten'), arxiv.Result.Author('Peter A. N. Bosman')]","Deep Neural Networks (DNNs) have the potential for making various clinical
procedures more time-efficient by automating medical image segmentation. Due to
their strong, in some cases human-level, performance, they have become the
standard approach in this field. The design of the best possible medical image
segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS),
i.e., the automation of neural network design, has been shown to have the
capability to outperform manually designed networks for various tasks. However,
the existing NAS methods for medical image segmentation have explored a quite
limited range of types of DNN architectures that can be discovered. In this
work, we propose a novel NAS search space for medical image segmentation
networks. This search space combines the strength of a generalised
encoder-decoder structure, well known from U-Net, with network blocks that have
proven to have a strong performance in image classification tasks. The search
is performed by looking for the best topology of multiple cells simultaneously
with the configuration of each cell within, allowing for interactions between
topology and cell-level attributes. From experiments on two publicly available
datasets, we find that the networks discovered by our proposed NAS method have
better performance than well-known handcrafted segmentation networks, and
outperform networks found with other NAS approaches that perform only topology
search, and topology-level search followed by cell-level search.",-0.07383727,0.25957876,-0.05931686,A
2366,"Overall, the results indicate that further research into search space reﬁnement, allowing
to exploit key features of what accounts for good deep learning performance, may yet push the boundaries of
what can be achieved with deep neural networks for medical image processing.","The experiments in this paper show the added
value of this approach.","REFERENCES

 [1] Bernard, O. et al., “Deep learning techniques for automatic MRI cardiac multi-structures segmentation and
      diagnosis,” IEEE Transactions on Medical Imaging 37(11), 2514–2525 (2018).",2022-02-23 10:32:35+00:00,Mixed-Block Neural Architecture Search for Medical Image Segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Martijn M. A. Bosma'), arxiv.Result.Author('Arkadiy Dushatskiy'), arxiv.Result.Author('Monika Grewal'), arxiv.Result.Author('Tanja Alderliesten'), arxiv.Result.Author('Peter A. N. Bosman')]","Deep Neural Networks (DNNs) have the potential for making various clinical
procedures more time-efficient by automating medical image segmentation. Due to
their strong, in some cases human-level, performance, they have become the
standard approach in this field. The design of the best possible medical image
segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS),
i.e., the automation of neural network design, has been shown to have the
capability to outperform manually designed networks for various tasks. However,
the existing NAS methods for medical image segmentation have explored a quite
limited range of types of DNN architectures that can be discovered. In this
work, we propose a novel NAS search space for medical image segmentation
networks. This search space combines the strength of a generalised
encoder-decoder structure, well known from U-Net, with network blocks that have
proven to have a strong performance in image classification tasks. The search
is performed by looking for the best topology of multiple cells simultaneously
with the configuration of each cell within, allowing for interactions between
topology and cell-level attributes. From experiments on two publicly available
datasets, we find that the networks discovered by our proposed NAS method have
better performance than well-known handcrafted segmentation networks, and
outperform networks found with other NAS approaches that perform only topology
search, and topology-level search followed by cell-level search.",-0.21060547,0.36744088,0.061737455,A
2409,"Making our approach insightful to end
users (e.g., explicitly showing diﬀerent types of variation in data) and validating the practical added value (i.e.,
reducing required time for scan delineation) is an interesting and important question for further research.","Our algorithm does not distinguish between types of variation,
and it only performs the partitioning to maximize segmentation quality.","As mentioned in Section 4, a larger number of produced segmentation variants leads to better performance
of our approach.",2022-02-24 13:35:34+00:00,Data variation-aware medical image segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Arkadiy Dushatskiy'), arxiv.Result.Author('Gerry Lowe'), arxiv.Result.Author('Peter A. N. Bosman'), arxiv.Result.Author('Tanja Alderliesten')]","Deep learning algorithms have become the golden standard for segmentation of
medical imaging data. In most works, the variability and heterogeneity of real
clinical data is acknowledged to still be a problem. One way to automatically
overcome this is to capture and exploit this variation explicitly. Here, we
propose an approach that improves on our previous work in this area and explain
how it potentially can improve clinical acceptance of (semi-)automatic
segmentation methods. In contrast to a standard neural network that produces
one segmentation, we propose to use a multi-pathUnet network that produces
multiple segmentation variants, presumably corresponding to the variations that
reside in the dataset. Different paths of the network are trained on disjoint
data subsets. Because a priori it may be unclear what variations exist in the
data, the subsets should be automatically determined. This is achieved by
searching for the best data partitioning with an evolutionary optimization
algorithm. Because each network path can become more specialized when trained
on a more homogeneous data subset, better segmentation quality can be achieved.
In practical usage, various automatically produced segmentations can be
presented to a medical expert, from which the preferred segmentation can be
selected. In experiments with a real clinical dataset of CT scans with prostate
segmentations, our approach provides an improvement of several percentage
points in terms of Dice and surface Dice coefficients compared to when all
network paths are trained on all training data. Noticeably, the largest
improvement occurs in the upper part of the prostate that is known to be most
prone to inter-observer segmentation variation.",0.045037635,-0.069369584,-0.06405811,C
2420,"We conclude that using random labels during training can potentially be a way to automatically detect data leakage, but
that it requires further research.","For the other datasets, the p-values were much
larger.","Discussion

Dataset split should be carefully designed to avoid overlap between training and testing sets.",2022-02-21 14:08:42+00:00,Inflation of test accuracy due to data leakage in deep learning-based classification of OCT images,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Iulian Emil Tampu'), arxiv.Result.Author('Anders Eklund'), arxiv.Result.Author('Neda Haj-Hosseini')]","In the application of deep learning on optical coherence tomography (OCT)
data, it is common to train classification networks using 2D images originating
from volumetric data. Given the micrometer resolution of OCT systems,
consecutive images are often very similar in both visible structures and noise.
Thus, an inappropriate data split can result in overlap between the training
and testing sets, with a large portion of the literature overlooking this
aspect. In this study, the effect of improper dataset splitting on model
evaluation is demonstrated for three classification tasks using three OCT
open-access datasets extensively used, Kermany's and Srinivasan's ophthalmology
datasets, and AIIMS breast tissue dataset. Results show that the classification
performance is inflated by 0.07 up to 0.43 in terms of Matthews Correlation
Coefficient (accuracy: 5% to 30%) for models tested on datasets with improper
splitting, highlighting the considerable effect of dataset handling on model
evaluation. This study intends to raise awareness on the importance of dataset
splitting given the increased research interest in implementing deep learning
on OCT data.",-0.10252545,0.03236365,-0.39558792,A
2455,"Overall, our work shows
promising results which could motivate further research in this direction.","In practice, to reconstruct breathing
sequences of arbitrary length, this would mean acquisition times of around
6 min, which is a reasonable time in clinical practice.","We believe our method shows a way for predicted true real-time 4D MRI
techniques and provides a solution to reduce the acquisition time and eﬀort
for retrospective reconstruction approaches.",2022-02-25 11:34:25+00:00,Predicting 4D Liver MRI for MR-guided Interventions,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Gino Gulamhussene'), arxiv.Result.Author('Anneke Meyer'), arxiv.Result.Author('Marko Rak'), arxiv.Result.Author('Oleksii Bashkanov'), arxiv.Result.Author('Jazan Omari'), arxiv.Result.Author('Maciej Pech'), arxiv.Result.Author('Christian Hansen')]","Organ motion poses an unresolved challenge in image-guided interventions. In
the pursuit of solving this problem, the research field of time-resolved
volumetric magnetic resonance imaging (4D MRI) has evolved. However, current
techniques are unsuitable for most interventional settings because they lack
sufficient temporal and/or spatial resolution or have long acquisition times.
In this work, we propose a novel approach for real-time, high-resolution 4D MRI
with large fields of view for MR-guided interventions. To this end, we trained
a convolutional neural network (CNN) end-to-end to predict a 3D liver MRI that
correctly predicts the liver's respiratory state from a live 2D navigator MRI
of a subject. Our method can be used in two ways: First, it can reconstruct
near real-time 4D MRI with high quality and high resolution (209x128x128 matrix
size with isotropic 1.8mm voxel size and 0.6s/volume) given a dynamic
interventional 2D navigator slice for guidance during an intervention. Second,
it can be used for retrospective 4D reconstruction with a temporal resolution
of below 0.2s/volume for motion analysis and use in radiation therapy. We
report a mean target registration error (TRE) of 1.19 $\pm$0.74mm, which is
below voxel size. We compare our results with a state-of-the-art retrospective
4D MRI reconstruction. Visual evaluation shows comparable quality. We show that
small training sizes with short acquisition times down to 2min can already
achieve promising results and 24min are sufficient for high quality results.
Because our method can be readily combined with earlier methods, acquisition
time can be further decreased while also limiting quality loss. We show that an
end-to-end, deep learning formulation is highly promising for 4D MRI
reconstruction.",-0.061136663,-0.09569612,0.3426613,C
2633,"4 Discussion

In this study, we report the development and the respective performance of
popular deep learning models for the annotation of a large dataset of time-lapse
videos of embryo development that we propose to make publicly available for
the sake of facilitated and improved further research in the ﬁeld.","This highlights that our database allows the models to be trained with
16 phases without being biased when reducing the settings to what can be found
in the literature.","We chose three
architectures for our experiments.",2022-03-01 15:13:21+00:00,Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Tristan Gomez'), arxiv.Result.Author('Magalie Feyeux'), arxiv.Result.Author('Nicolas Normand'), arxiv.Result.Author('Laurent David'), arxiv.Result.Author('Perrine Paul-Gilloteaux'), arxiv.Result.Author('Thomas Fréour'), arxiv.Result.Author('Harold Mouchère')]","An important limitation to the development of Artificial Intelligence
(AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a
public reference benchmark to train and evaluate deep learning (DL) models. In
this work, we describe a fully annotated dataset of 756 videos of developing
embryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D
architectures to our dataset and demonstrate that they overperform algorithmic
approaches to automatically annotate stage development phases. Altogether, we
propose the first public benchmark that will allow the community to evaluate
morphokinetic models. This is the first step towards deep learning-powered IVF.
Of note, we propose highly detailed annotations with 16 different development
phases, including early cell division phases, but also late cell divisions,
phases after morulation, and very early phases, which have never been used
before. We postulate that this original approach will help improve the overall
performance of deep learning approaches on time-lapse videos of embryo
development, ultimately benefiting infertile patients with improved clinical
success rates (Code and data are available at
https://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git).",-0.14157559,0.4052258,-0.21742639,A
2634,"4 Discussion

In this study, we propose a large dataset of time-lapse videos of embryo devel-
opment and make it publicly available for the sake of facilitated and improved
further research in the ﬁeld.","Also, the baseline models achieved good performance which demon-

                               13
strates that our dataset is suﬃcient in size and quality to train and evaluate
deep learning models.","This dataset is accompanied by detailed morpho-
kinetic annotations and custom metrics.",2022-03-01 15:13:21+00:00,Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Tristan Gomez'), arxiv.Result.Author('Magalie Feyeux'), arxiv.Result.Author('Nicolas Normand'), arxiv.Result.Author('Laurent David'), arxiv.Result.Author('Perrine Paul-Gilloteaux'), arxiv.Result.Author('Thomas Fréour'), arxiv.Result.Author('Harold Mouchère')]","An important limitation to the development of Artificial Intelligence
(AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a
public reference benchmark to train and evaluate deep learning (DL) models. In
this work, we describe a fully annotated dataset of 704 videos of developing
embryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D
architectures to our dataset and demonstrate that they overperform algorithmic
approaches to automatically annotate stage development phases. Altogether, we
propose the first public benchmark that will allow the community to evaluate
morphokinetic models. This is the first step towards deep learning-powered IVF.
Of note, we propose highly detailed annotations with 16 different development
phases, including early cell division phases, but also late cell divisions,
phases after morulation, and very early phases, which have never been used
before. We postulate that this original approach will help improve the overall
performance of deep learning approaches on time-lapse videos of embryo
development, ultimately benefiting infertile patients with improved clinical
success rates (Code and data are available at
https://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git).",-0.089123726,0.33049548,-0.13267745,A
2672,"Therefore, we proceed with this intermediate model as an
optimum model for further study.","The result for
depth and breadth optimization indicates that the average accuracy of the intermediate model is >0.85 (including all the
classes), whereas it is <0.85 for shallow and deep networks.","Considering the intermediate model has the optimum breadth and depth, the further
optimization of the parameters and the results are as shown in Figure 5a.",2022-03-02 07:09:29+00:00,Machine learning based lens-free imaging technique for field-portable cytometry,eess.IV,"['eess.IV', 'cs.AI']","[arxiv.Result.Author('Rajkumar Vaghashiya'), arxiv.Result.Author('Sanghoon Shin'), arxiv.Result.Author('Varun Chauhan'), arxiv.Result.Author('Kaushal Kapadiya'), arxiv.Result.Author('Smit Sanghavi'), arxiv.Result.Author('Sungkyu Seo2'), arxiv.Result.Author('Mohendra Roy')]","Lens-free Shadow Imaging Technique (LSIT) is a well-established technique for
the characterization of microparticles and biological cells. Due to its
simplicity and cost-effectiveness, various low-cost solutions have been
evolved, such as automatic analysis of complete blood count (CBC), cell
viability, 2D cell morphology, 3D cell tomography, etc. The developed auto
characterization algorithm so far for this custom-developed LSIT cytometer was
based on the hand-crafted features of the cell diffraction patterns from the
LSIT cytometer, that were determined from our empirical findings on thousands
of samples of individual cell types, which limit the system in terms of
induction of a new cell type for auto classification or characterization.
Further, its performance is suffering from poor image (cell diffraction
pattern) signatures due to its small signal or background noise. In this work,
we address these issues by leveraging the artificial intelligence-powered auto
signal enhancing scheme such as denoising autoencoder and adaptive cell
characterization technique based on the transfer of learning in deep neural
networks. The performance of our proposed method shows an increase in accuracy
>98% along with the signal enhancement of >5 dB for most of the cell types,
such as Red Blood Cell (RBC) and White Blood Cell (WBC). Furthermore, the model
is adaptive to learn new type of samples within a few learning iterations and
able to successfully classify the newly introduced sample along with the
existing other sample types.",0.1246503,0.32721362,-0.16116142,A
2673,"Therefore, we proceed with this intermediate model as an
optimum model for further study.","The result for
depth and breadth optimization indicates that the average accuracy of the intermediate model is >0.85 (including all the
classes), whereas it is <0.85 for shallow and deep networks.","Considering the intermediate model has the optimum breadth and depth, the further
optimization of the parameters and the results are as shown in Figure 5a.",2022-03-02 07:09:29+00:00,Machine learning based lens-free imaging technique for field-portable cytometry,eess.IV,"['eess.IV', 'cs.AI']","[arxiv.Result.Author('Rajkumar Vaghashiya'), arxiv.Result.Author('Sanghoon Shin'), arxiv.Result.Author('Varun Chauhan'), arxiv.Result.Author('Kaushal Kapadiya'), arxiv.Result.Author('Smit Sanghavi'), arxiv.Result.Author('Sungkyu Seo'), arxiv.Result.Author('Mohendra Roy')]","Lens-free Shadow Imaging Technique (LSIT) is a well-established technique for
the characterization of microparticles and biological cells. Due to its
simplicity and cost-effectiveness, various low-cost solutions have been
evolved, such as automatic analysis of complete blood count (CBC), cell
viability, 2D cell morphology, 3D cell tomography, etc. The developed auto
characterization algorithm so far for this custom-developed LSIT cytometer was
based on the hand-crafted features of the cell diffraction patterns from the
LSIT cytometer, that were determined from our empirical findings on thousands
of samples of individual cell types, which limit the system in terms of
induction of a new cell type for auto classification or characterization.
Further, its performance is suffering from poor image (cell diffraction
pattern) signatures due to its small signal or background noise. In this work,
we address these issues by leveraging the artificial intelligence-powered auto
signal enhancing scheme such as denoising autoencoder and adaptive cell
characterization technique based on the transfer of learning in deep neural
networks. The performance of our proposed method shows an increase in accuracy
>98% along with the signal enhancement of >5 dB for most of the cell types,
such as Red Blood Cell (RBC) and White Blood Cell (WBC). Furthermore, the model
is adaptive to learn new type of samples within a few learning iterations and
able to successfully classify the newly introduced sample along with the
existing other sample types.",0.1246503,0.32721362,-0.16116142,A
2689,"Ex-
we believe that it is more valuable to direct further research          amples are joint modeling of myocardium, LV and RV, 3D car-
towards a 3D version of our approach.","Therefore,             concept can as well be applied to other segmentation tasks.","Since the parameter re-           diac segmentation or application of the method on other clinical
gression showed to be more sensitive to the number of samples,          data, e.g.",2022-03-02 13:20:30+00:00,Shape constrained CNN for segmentation guided prediction of myocardial shape and pose parameters in cardiac MRI,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Sofie Tilborghs'), arxiv.Result.Author('Jan Bogaert'), arxiv.Result.Author('Frederik Maes')]","Semantic segmentation using convolutional neural networks (CNNs) is the
state-of-the-art for many medical image segmentation tasks including myocardial
segmentation in cardiac MR images. However, the predicted segmentation maps
obtained from such standard CNN do not allow direct quantification of regional
shape properties such as regional wall thickness. Furthermore, the CNNs lack
explicit shape constraints, occasionally resulting in unrealistic
segmentations. In this paper, we use a CNN to predict shape parameters of an
underlying statistical shape model of the myocardium learned from a training
set of images. Additionally, the cardiac pose is predicted, which allows to
reconstruct the myocardial contours. The integrated shape model regularizes the
predicted contours and guarantees realistic shapes. We enforce robustness of
shape and pose prediction by simultaneously performing pixel-wise semantic
segmentation during training and define two loss functions to impose
consistency between the two predicted representations: one distance-based loss
and one overlap-based loss. We evaluated the proposed method in a 5-fold cross
validation on an in-house clinical dataset with 75 subjects and on the ACDC and
LVQuan19 public datasets. We show the benefits of simultaneous semantic
segmentation and the two newly defined loss functions for the prediction of
shape parameters. Our method achieved a correlation of 99% for left ventricular
(LV) area on the three datasets, between 91% and 97% for myocardial area,
98-99% for LV dimensions and between 80% and 92% for regional wall thickness.",-0.21449265,-0.09476642,0.095564365,C
2690,"The proposed method of DERDO creates a vast amount                           Picture Coding Symposium (PCS), Nu¨rnberg, Germany, Dec 2016.
of possible further research topics: Research can focus on
improving the decoding energy model or the training process               [13] E. Kalali, Y. Adibelli, and I. Hamzaoglu, “A high performance and low
of the speciﬁc energies.","[12] C. Herglotz, Y. Wen, B. Dai, M. Kra¨nzler, and A. Kaup, “A bitstream
                                                                                feature based model for video decoding energy estimation,” in Proc.","Furthermore, in the DERDO process,                     energy intra prediction hardware for HEVC video decoding,” in Con-
the transmission energy can be considered explicitly and new                    ference on Design and Architectures for Signal and Image Processing
encoder speed-up methods targeting the decoder energy con-                      (DASIP), Oct 2012, pp.",2022-03-02 13:36:41+00:00,Decoding-Energy-Rate-Distortion Optimization for Video Coding,eess.IV,['eess.IV'],"[arxiv.Result.Author('Christian Herglotz'), arxiv.Result.Author('Andreas Heindel'), arxiv.Result.Author('André Kaup')]","This paper presents a method for generating coded video bit streams requiring
less decoding energy than conventionally coded bit streams. To this end, we
propose extending the standard rate-distortion optimization approach to also
consider the decoding energy. In the encoder, the decoding energy is estimated
during runtime using a feature-based energy model. These energy estimates are
then used to calculate decoding-energy-rate-distortion costs that are minimized
by the encoder. This ultimately leads to optimal trade-offs between these three
parameters. Therefore, we introduce the mathematical theory for describing
decoding-energy-rate-distortion optimization and the proposed encoder algorithm
is explained in detail. For rate-energy control, a new encoder parameter is
introduced. Finally, measurements of the software decoding process for
HEVC-coded bit streams are performed. Results show that this approach can lead
to up to 30% of decoding energy reduction at a constant visual objective
quality when accepting a bitrate increase at the same order of magnitude.",0.5396545,0.007782541,-0.07706617,B
2714,will further study this issue in our future work.,8).,"The pro-
                                                                    posed method can be embedded in imaging devices such as
5.3.",2022-03-02 13:30:56+00:00,Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Zhilu Zhang'), arxiv.Result.Author('Ruohao Wang'), arxiv.Result.Author('Hongzhi Zhang'), arxiv.Result.Author('Yunjin Chen'), arxiv.Result.Author('Wangmeng Zuo')]","In this paper, we consider two challenging issues in reference-based
super-resolution (RefSR), (i) how to choose a proper reference image, and (ii)
how to learn real-world RefSR in a self-supervised manner. Particularly, we
present a novel self-supervised learning approach for real-world image SR from
observations at dual camera zooms (SelfDZSR). For the first issue, the more
zoomed (telephoto) image can be naturally leveraged as the reference to guide
the SR of the lesser zoomed (short-focus) image. For the second issue, SelfDZSR
learns a deep network to obtain the SR result of short-focal image and with the
same resolution as the telephoto image. For this purpose, we take the telephoto
image instead of an additional high-resolution image as the supervision
information and select a patch from it as the reference to super-resolve the
corresponding short-focus image patch. To mitigate the effect of various
misalignment between the short-focus low-resolution (LR) image and telephoto
ground-truth (GT) image, we design a degradation model and map the GT to a
pseudo-LR image aligned with GT. Then the pseudo-LR and LR image can be fed
into the proposed adaptive spatial transformer networks (AdaSTN) to deform the
LR features. During testing, SelfDZSR can be directly deployed to super-solve
the whole short-focus image with the reference of telephoto image. Experiments
show that our method achieves better quantitative and qualitative performance
against state-of-the-arts. The code and pre-trained models will be publicly
available.",0.20530623,-0.15571508,0.34469074,B
2977,"As a result, as shown      facts that commonly exist in both SoTA and ours methods
in Table 3, such a model can still achieve comparable per-      and require further study.","Hence we train a model without amplifying the input       signals in each color channel, leading to those color arti-
raw images with the predeﬁned ratio.","formance, with only a slight decrease in PSNR and SSIM.",2022-03-08 12:22:31+00:00,Learning to Erase the Bayer-Filter to See in the Dark,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Xingbo Dong'), arxiv.Result.Author('Wanyan Xu'), arxiv.Result.Author('Zhihui Miao'), arxiv.Result.Author('Lan Ma'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Jiewen Yang'), arxiv.Result.Author('Zhe Jin'), arxiv.Result.Author('Andrew Beng Jin Teoh'), arxiv.Result.Author('Jiajun Shen')]","Low-light image enhancement - a pervasive but challenging problem, plays a
central role in enhancing the visibility of an image captured in a poor
illumination environment. Due to the fact that not all photons can pass the
Bayer-Filter on the sensor of the color camera, in this work, we first present
a De-Bayer-Filter simulator based on deep neural networks to generate a
monochrome raw image from the colored raw image. Next, a fully convolutional
network is proposed to achieve the low-light image enhancement by fusing
colored raw data with synthesized monochrome raw data. Channel-wise attention
is also introduced to the fusion process to establish a complementary
interaction between features from colored and monochrome raw images. To train
the convolutional networks, we propose a dataset with monochrome and color raw
pairs named Mono-Colored Raw paired dataset (MCR) collected by using a
monochrome camera without Bayer-Filter and a color camera with Bayer-Filter.
The proposed pipeline take advantages of the fusion of the virtual monochrome
and the color raw images and our extensive experiments indicate that
significant improvement can be achieved by leveraging raw sensor data and
data-driven learning.",0.28378403,-0.030324904,-0.11663586,B
2978,"signals in each color channel, leading to those color arti-
                                                              facts that commonly exist in both SoTA and ours methods
Baseline                      DBF  SSIM            DBLE       and require further study.",Ablation study on the MCR dataset.,"Baseline wo CA [8]   PSNR (dB)     0.8254  PSNR (dB) SSIM
Baseline wo ratio                  0.7948                     6.",2022-03-08 12:22:31+00:00,Abandoning the Bayer-Filter to See in the Dark,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Xingbo Dong'), arxiv.Result.Author('Wanyan Xu'), arxiv.Result.Author('Zhihui Miao'), arxiv.Result.Author('Lan Ma'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Jiewen Yang'), arxiv.Result.Author('Zhe Jin'), arxiv.Result.Author('Andrew Beng Jin Teoh'), arxiv.Result.Author('Jiajun Shen')]","Low-light image enhancement - a pervasive but challenging problem, plays a
central role in enhancing the visibility of an image captured in a poor
illumination environment. Due to the fact that not all photons can pass the
Bayer-Filter on the sensor of the color camera, in this work, we first present
a De-Bayer-Filter simulator based on deep neural networks to generate a
monochrome raw image from the colored raw image. Next, a fully convolutional
network is proposed to achieve the low-light image enhancement by fusing
colored raw data with synthesized monochrome raw data. Channel-wise attention
is also introduced to the fusion process to establish a complementary
interaction between features from colored and monochrome raw images. To train
the convolutional networks, we propose a dataset with monochrome and color raw
pairs named Mono-Colored Raw paired dataset (MCR) collected by using a
monochrome camera without Bayer-Filter and a color camera with Bayer-Filter.
The proposed pipeline take advantages of the fusion of the virtual monochrome
and the color raw images and our extensive experiments indicate that
significant improvement can be achieved by leveraging raw sensor data and
data-driven learning.",0.0842192,-0.3789839,-0.13136858,B
2996,"Recently researchers are focusing on the gene sequence data as it is a wide area and there is always room for further research
and results.","So, this gives future
researchers an opportunity to use Attention to improve the accuracy of deep learning models.","There are several opportunities for future researchers to contribute by merging multiple gene sequencing datasets to predict
additional outcomes with larger dataset.",2022-03-08 13:51:17+00:00,Breast cancer detection using artificial intelligence techniques: A systematic literature review,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Ali Bou Nassif'), arxiv.Result.Author('Manar Abu Talib'), arxiv.Result.Author('Qassim Nasir'), arxiv.Result.Author('Yaman Afadar'), arxiv.Result.Author('Omar Elgendy')]","Cancer is one of the most dangerous diseases to humans, and yet no permanent
cure has been developed for it. Breast cancer is one of the most common cancer
types. According to the National Breast Cancer foundation, in 2020 alone, more
than 276,000 new cases of invasive breast cancer and more than 48,000
non-invasive cases were diagnosed in the US. To put these figures in
perspective, 64% of these cases are diagnosed early in the disease's cycle,
giving patients a 99% chance of survival. Artificial intelligence and machine
learning have been used effectively in detection and treatment of several
dangerous diseases, helping in early diagnosis and treatment, and thus
increasing the patient's chance of survival. Deep learning has been designed to
analyze the most important features affecting detection and treatment of
serious diseases. For example, breast cancer can be detected using genes or
histopathological imaging. Analysis at the genetic level is very expensive, so
histopathological imaging is the most common approach used to detect breast
cancer. In this research work, we systematically reviewed previous work done on
detection and treatment of breast cancer using genetic sequencing or
histopathological imaging with the help of deep learning and machine learning.
We also provide recommendations to researchers who will work in this field",-0.12387315,0.3683139,-0.3232785,A
3042,"The related codes will
   be released for further research.",comprehensive comparisons.,Degradation pool.,2022-03-09 11:30:38+00:00,Learning the Degradation Distribution for Blind Image Super-Resolution,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Zhengxiong Luo'), arxiv.Result.Author('Yan Huang'), arxiv.Result.Author('Shang Li'), arxiv.Result.Author('Liang Wang'), arxiv.Result.Author('Tieniu Tan')]","Synthetic high-resolution (HR) \& low-resolution (LR) pairs are widely used
in existing super-resolution (SR) methods. To avoid the domain gap between
synthetic and test images, most previous methods try to adaptively learn the
synthesizing (degrading) process via a deterministic model. However, some
degradations in real scenarios are stochastic and cannot be determined by the
content of the image. These deterministic models may fail to model the random
factors and content-independent parts of degradations, which will limit the
performance of the following SR models. In this paper, we propose a
probabilistic degradation model (PDM), which studies the degradation
$\mathbf{D}$ as a random variable, and learns its distribution by modeling the
mapping from a priori random variable $\mathbf{z}$ to $\mathbf{D}$. Compared
with previous deterministic degradation models, PDM could model more diverse
degradations and generate HR-LR pairs that may better cover the various
degradations of test images, and thus prevent the SR model from over-fitting to
specific ones. Extensive experiments have demonstrated that our degradation
model can help the SR model achieve better performance on different datasets.
The source codes are released at \url{git@github.com:greatlog/UnpairedSR.git}.",0.25869745,-0.27799836,-0.296745,B
3139,"Other directions of further research include
more principled ways to integrate spatial smoothness priors, and experimenting with diﬀerent com-
partment models to explain the data.","Future work will consider integrating this approach with cerebral blood ﬂow estimation from
arterial spin labelling data [Alsop et al., 2015], which will permit straightforward inference of the
cerebral metabolic rate of Oxygen [An et al., 2001].","Acknowledgments: We acknowledge funding from the University of Brighton Rising Stars
initiative and support from Brighton and Sussex Medical School.",2022-03-11 10:47:16+00:00,Flexible Amortized Variational Inference in qBOLD MRI,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']","[arxiv.Result.Author('Ivor J. A. Simpson'), arxiv.Result.Author('Ashley McManamon'), arxiv.Result.Author('Alan J. Stone'), arxiv.Result.Author('Nicholas P. Blockley'), arxiv.Result.Author('Alessandro Colasanti'), arxiv.Result.Author('Mara Cercignani')]","Streamlined qBOLD acquisitions enable experimentally straightforward
observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred;
however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume
(DBV) are more ambiguously determined from the data. As such, existing
inference methods tend to yield very noisy and underestimated OEF maps, while
overestimating DBV.
  This work describes a novel probabilistic machine learning approach that can
infer plausible distributions of OEF and DBV. Initially, we create a model that
produces informative voxelwise prior distribution based on synthetic training
data. Contrary to prior work, we model the joint distribution of OEF and DBV
through a scaled multivariate logit-Normal distribution, which enables the
values to be constrained within a plausible range. The prior distribution model
is used to train an efficient amortized variational Bayesian inference model.
This model learns to infer OEF and DBV by predicting real image data, with few
training data required, using the signal equations as a forward model.
  We demonstrate that our approach enables the inference of smooth OEF and DBV
maps, with a physiologically plausible distribution that can be adapted through
specification of an informative prior distribution. Other benefits include
model comparison (via the evidence lower bound) and uncertainty quantification
for identifying image artefacts. Results are demonstrated on a small study
comparing subjects undergoing hyperventilation and at rest. We illustrate that
the proposed approach allows measurement of gray matter differences in OEF and
DBV and enables voxelwise comparison between conditions, where we observe
significant increases in OEF and $R_2^\prime$ during hyperventilation.",-0.11538671,-0.22411521,0.0970376,C
3140,"Other directions of further research include
more principled ways to integrate spatial smoothness priors, and experimenting with diﬀerent com-
partment models to explain the data.","Future work will consider integrating this approach with cerebral blood ﬂow estimation from
arterial spin labelling data [Alsop et al., 2015], which will permit straightforward inference of the
cerebral metabolic rate of Oxygen [An et al., 2001].","Acknowledgments: We acknowledge funding from the University of Brighton Rising Stars
initiative and support from Brighton and Sussex Medical School.",2022-03-11 10:47:16+00:00,Flexible Amortized Variational Inference in qBOLD MRI,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']","[arxiv.Result.Author('Ivor J. A. Simpson'), arxiv.Result.Author('Ashley McManamon'), arxiv.Result.Author('Balázs Örzsik'), arxiv.Result.Author('Alan J. Stone'), arxiv.Result.Author('Nicholas P. Blockley'), arxiv.Result.Author('Iris Asllani'), arxiv.Result.Author('Alessandro Colasanti'), arxiv.Result.Author('Mara Cercignani')]","Streamlined qBOLD acquisitions enable experimentally straightforward
observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred;
however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume
(DBV) are more ambiguously determined from the data. As such, existing
inference methods tend to yield very noisy and underestimated OEF maps, while
overestimating DBV.
  This work describes a novel probabilistic machine learning approach that can
infer plausible distributions of OEF and DBV. Initially, we create a model that
produces informative voxelwise prior distribution based on synthetic training
data. Contrary to prior work, we model the joint distribution of OEF and DBV
through a scaled multivariate logit-Normal distribution, which enables the
values to be constrained within a plausible range. The prior distribution model
is used to train an efficient amortized variational Bayesian inference model.
This model learns to infer OEF and DBV by predicting real image data, with few
training data required, using the signal equations as a forward model.
  We demonstrate that our approach enables the inference of smooth OEF and DBV
maps, with a physiologically plausible distribution that can be adapted through
specification of an informative prior distribution. Other benefits include
model comparison (via the evidence lower bound) and uncertainty quantification
for identifying image artefacts. Results are demonstrated on a small study
comparing subjects undergoing hyperventilation and at rest. We illustrate that
the proposed approach allows measurement of gray matter differences in OEF and
DBV and enables voxelwise comparison between conditions, where we observe
significant increases in OEF and $R_2^\prime$ during hyperventilation.",-0.11538671,-0.22411521,0.0970376,C
3308,"human prior leads to efﬁcient representation learning,
       which suggests promising further research                     Another learning paradigm to effectively counter earlier
                                                                  stated data scarcity challenges is self-supervised learning.","Simple
                                                                  augmentations refer to one or more of the including rotation,
   5) Presents initial empirical support showing that reducing    ﬂipping, cropping, shifting, and zooming.","6) Empirical evidence on cross-magniﬁcation evaluation         It refers to methods that use pseudo-labels retrieved from
       to demonstrate magniﬁcation invariant representation       structural properties of data itself to learn representations.",2022-03-15 07:54:20+00:00,Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Prakash Chandra Chhipa'), arxiv.Result.Author('Richa Upadhyay'), arxiv.Result.Author('Gustav Grund Pihlgren'), arxiv.Result.Author('Rajkumar Saini'), arxiv.Result.Author('Seiichi Uchida'), arxiv.Result.Author('Marcus Liwicki')]","This work presents a novel self-supervised pre-training method to learn
efficient representations without labels on histopathology medical images
utilizing magnification factors. Other state-of-theart works mainly focus on
fully supervised learning approaches that rely heavily on human annotations.
However, the scarcity of labeled and unlabeled data is a long-standing
challenge in histopathology. Currently, representation learning without labels
remains unexplored for the histopathology domain. The proposed method,
Magnification Prior Contrastive Similarity (MPCS), enables self-supervised
learning of representations without labels on small-scale breast cancer dataset
BreakHis by exploiting magnification factor, inductive transfer, and reducing
human prior. The proposed method matches fully supervised learning
state-of-the-art performance in malignancy classification when only 20% of
labels are used in fine-tuning and outperform previous works in fully
supervised learning settings. It formulates a hypothesis and provides empirical
evidence to support that reducing human-prior leads to efficient representation
learning in self-supervision. The implementation of this work is available
online on GitHub -
https://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method",0.030039448,0.2133874,-0.11909993,A
3423,"Our code could be applied to large repositories of
OA cases for further research on the questions of surgery appropriateness and outcomes.","We believe that the combination of
the state-of-the-art performance of our algorithm, thorough evaluation with multiple datasets, and public availability of
the code will allow for more rapid advances in OA imaging research.","To our knowledge there is no
publicly available software of this type, until now.",2022-03-16 19:54:47+00:00,Automated Grading of Radiographic Knee Osteoarthritis Severity Combined with Joint Space Narrowing,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Keyu Li'), arxiv.Result.Author('Roy J. Colglazier'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Michael Lebhar'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('Rob J. French'), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jikai Zhang'), arxiv.Result.Author('Christine Park'), arxiv.Result.Author('Maciej A. Mazurowski')]","The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical ""black box"" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.",-0.26770228,-0.14327885,0.17782779,C_centroid
3424,"It remains a question of how to get rid of these potentially misleading cases during training, or
perform further study on these misclassiﬁed cases.","As such, it is possible that in certain cases in the training set, the ground truth KL
grades were erroneous.","Further studies could also consider incorporating additional OA
features, such as medial tibial attrition, medial tibial sclerosis, and lateral femoral sclerosis.",2022-03-16 19:54:47+00:00,Automated Grading of Radiographic Knee Osteoarthritis Severity Combined with Joint Space Narrowing,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Keyu Li'), arxiv.Result.Author('Roy J. Colglazier'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Michael Lebhar'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('Rob J. French'), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jikai Zhang'), arxiv.Result.Author('Christine Park'), arxiv.Result.Author('Maciej A. Mazurowski')]","The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical ""black box"" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.",-0.16283932,-0.20308487,-0.09493174,C
3425,Our software has been made publicly available and easy to use for further research.,"Thorough evaluation with
three datasets and a comparison to the performance of multiple experienced readers showed that our algorithm performs
at the level of radiologists.","13
                    arXiv Template                                             A PREPRINT

References

Osteoarthritis (OA) | Arthritis | CDC.",2022-03-16 19:54:47+00:00,Automated Grading of Radiographic Knee Osteoarthritis Severity Combined with Joint Space Narrowing,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Keyu Li'), arxiv.Result.Author('Roy J. Colglazier'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Michael Lebhar'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('Rob J. French'), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jikai Zhang'), arxiv.Result.Author('Christine Park'), arxiv.Result.Author('Maciej A. Mazurowski')]","The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical ""black box"" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.",-0.25441965,-0.16026258,-0.06165064,C
3426,"Our code could be applied to large repositories of
OA cases for further research on the questions of surgery appropriateness and outcomes.","We believe that the combination of
the state-of-the-art performance of our algorithm, thorough evaluation with multiple datasets, and public availability of
the code will allow for more rapid advances in OA imaging research.","To our knowledge there is no
publicly available software of this type, until now.",2022-03-16 19:54:47+00:00,Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Keyu Li'), arxiv.Result.Author('Roy J. Colglazier'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Michael Lebhar'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('Rob J. French'), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jikai Zhang'), arxiv.Result.Author('Christine Park'), arxiv.Result.Author('Maciej A. Mazurowski')]","The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical ""black box"" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.",-0.26770228,-0.14327885,0.17782779,C
3427,"It remains a question of how to get rid of these potentially misleading cases during training, or
perform further study on these misclassiﬁed cases.","As such, it is possible that in certain cases in the training set, the ground truth KL
grades were erroneous.","Further studies could also consider incorporating additional OA
features, such as medial tibial attrition, medial tibial sclerosis, and lateral femoral sclerosis.",2022-03-16 19:54:47+00:00,Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Keyu Li'), arxiv.Result.Author('Roy J. Colglazier'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Michael Lebhar'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('Rob J. French'), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jikai Zhang'), arxiv.Result.Author('Christine Park'), arxiv.Result.Author('Maciej A. Mazurowski')]","The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical ""black box"" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.",-0.16283932,-0.20308487,-0.09493174,C
3428,"Our software has been made publicly available and easy to use for
further research.","Thorough
evaluation with three datasets and a comparison to the performance of multiple experienced readers showed that our
algorithm performs at the level of radiologists.","References

Osteoarthritis (OA) | Arthritis | CDC.",2022-03-16 19:54:47+00:00,Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Keyu Li'), arxiv.Result.Author('Roy J. Colglazier'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Michael Lebhar'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('Rob J. French'), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jikai Zhang'), arxiv.Result.Author('Christine Park'), arxiv.Result.Author('Maciej A. Mazurowski')]","The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical ""black box"" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.",-0.2920171,-0.21675967,-0.059393123,C
3429,"Rotation invariance: To further study rotation equivariance and invariance
properties in our 3D-UCaps, we trained our network without any rotation data
augmentation.",Best view in zoom.,"During testing, we choose an axis to rotate the volume, and apply
the rotation with angle values ﬁxed to 5, 10, 15, .., 90 degrees.",2022-03-16 22:02:37+00:00,3D-UCaps: 3D Capsules Unet for Volumetric Image Segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Tan Nguyen'), arxiv.Result.Author('Binh-Son Hua'), arxiv.Result.Author('Ngan Le')]","Medical image segmentation has been so far achieving promising results with
Convolutional Neural Networks (CNNs). However, it is arguable that in
traditional CNNs, its pooling layer tends to discard important information such
as positions. Moreover, CNNs are sensitive to rotation and affine
transformation. Capsule network is a data-efficient network design proposed to
overcome such limitations by replacing pooling layers with dynamic routing and
convolutional strides, which aims to preserve the part-whole relationships.
Capsule network has shown a great performance in image recognition and natural
language processing, but applications for medical image segmentation,
particularly volumetric image segmentation, has been limited. In this work, we
propose 3D-UCaps, a 3D voxel-based Capsule network for medical volumetric image
segmentation. We build the concept of capsules into a CNN by designing a
network with two pathways: the first pathway is encoded by 3D Capsule blocks,
whereas the second pathway is decoded by 3D CNNs blocks. 3D-UCaps, therefore
inherits the merits from both Capsule network to preserve the spatial
relationship and CNNs to learn visual representation. We conducted experiments
on various datasets to demonstrate the robustness of 3D-UCaps including
iSeg-2017, LUNA16, Hippocampus, and Cardiac, where our method outperforms
previous Capsule networks and 3D-Unets.",0.09464494,0.1480505,0.047834955,A
3460,"Finally, we conclude on our contribution and possible further research.",We also discuss important practical questions related to the proposed method.,"2 Related Work

In the last years there is a growing interest in self-supervised learning (SSL).",2022-03-17 14:58:15+00:00,Using the Order of Tomographic Slices as a Prior for Neural Networks Pre-Training,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Yaroslav Zharov'), arxiv.Result.Author('Alexey Ershov'), arxiv.Result.Author('Tilo Baumbach'), arxiv.Result.Author('Vincent Heuveline')]","The technical advances in Computed Tomography (CT) allow to obtain immense
amounts of 3D data. For such datasets it is very costly and time-consuming to
obtain the accurate 3D segmentation markup to train neural networks. The
annotation is typically done for a limited number of 2D slices, followed by an
interpolation. In this work, we propose a pre-training method SortingLoss. It
performs pre-training on slices instead of volumes, so that a model could be
fine-tuned on a sparse set of slices, without the interpolation step. Unlike
general methods (e.g. SimCLR or Barlow Twins), the task specific methods (e.g.
Transferable Visual Words) trade broad applicability for quality benefits by
imposing stronger assumptions on the input data. We propose a relatively mild
assumption -- if we take several slices along some axis of a volume, structure
of the sample presented on those slices, should give a strong clue to
reconstruct the correct order of those slices along the axis. Many biomedical
datasets fulfill this requirement due to the specific anatomy of a sample and
pre-defined alignment of the imaging setup. We examine the proposed method on
two datasets: medical CT of lungs affected by COVID-19 disease, and
high-resolution synchrotron-based full-body CT of model organisms (Medaka
fish). We show that the proposed method performs on par with SimCLR, while
working 2x faster and requiring 1.5x less memory. In addition, we present the
benefits in terms of practical scenarios, especially the applicability to the
pre-training of large models and the ability to localize samples within volumes
in an unsupervised setup.",0.017093012,0.0141845085,-0.2798689,A
3548,"Also, we are interested in further studying the limits of the
generalization of LCS to previously unseen labels.","For instance,
there might be better representations for the conditioned label and better ways to present
it as an input to the model.","8
                                        Label conditioned segmentation

References

Mugahed A Al-Antari, Mohammed A Al-Masni, Mun-Taek Choi, Seung-Moo Han, and
   Tae-Seong Kim.",2022-03-17 22:21:10+00:00,Label conditioned segmentation,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Tianyu Ma'), arxiv.Result.Author('Benjamin C. Lee'), arxiv.Result.Author('Mert R. Sabuncu')]","Semantic segmentation is an important task in computer vision that is often
tackled with convolutional neural networks (CNNs). A CNN learns to produce
pixel-level predictions through training on pairs of images and their
corresponding ground-truth segmentation labels. For segmentation tasks with
multiple classes, the standard approach is to use a network that computes a
multi-channel probabilistic segmentation map, with each channel representing
one class. In applications where the image grid size (e.g., when it is a 3D
volume) and/or the number of labels is relatively large, the standard
(baseline) approach can become prohibitively expensive for our computational
resources. In this paper, we propose a simple yet effective method to address
this challenge. In our approach, the segmentation network produces a
single-channel output, while being conditioned on a single class label, which
determines the output class of the network. Our method, called label
conditioned segmentation (LCS), can be used to segment images with a very large
number of classes, which might be infeasible for the baseline approach. We also
demonstrate in the experiments that label conditioning can improve the accuracy
of a given backbone architecture, likely, thanks to its parameter efficiency.
Finally, as we show in our results, an LCS model can produce previously unseen
fine-grained labels during inference time, when only coarse labels were
available during training. We provide all of our code here:
https://github.com/tym002/Label-conditioned-segmentation",0.028525222,-0.09638463,-0.2299104,C
3720,"For further research we might adapt Tsallis–Havrda–Charvat
                       binary cross-entropy to a categorical cross-entropy.","This makes the Tsallis–Havrda–Charvat formula the best candidate for further
                       research on these datasets.","This would allow for making multi-class
                       predictions, including estimating the time between the end of cancer treatment and recurrence.",2022-03-22 13:51:23+00:00,A Quantitative Comparison between Shannon and Tsallis Havrda Charvat Entropies Applied to Cancer Outcome Prediction,eess.IV,"['eess.IV', 'cs.LG']","[arxiv.Result.Author('Thibaud Brochet'), arxiv.Result.Author('Jérôme Lapuyade-Lahorgue'), arxiv.Result.Author('Pierre Vera'), arxiv.Result.Author('Su Ruan')]","In this paper, we propose to quantitatively compare loss functions based on
parameterized Tsallis-Havrda-Charvat entropy and classical Shannon entropy for
the training of a deep network in the case of small datasets which are usually
encountered in medical applications. Shannon cross-entropy is widely used as a
loss function for most neural networks applied to the segmentation,
classification and detection of images. Shannon entropy is a particular case of
Tsallis-Havrda-Charvat entropy. In this work, we compare these two entropies
through a medical application for predicting recurrence in patients with
head-neck and lung cancers after treatment. Based on both CT images and patient
information, a multitask deep neural network is proposed to perform a
recurrence prediction task using cross-entropy as a loss function and an image
reconstruction task. Tsallis-Havrda-Charvat cross-entropy is a parameterized
cross entropy with the parameter $\alpha$. Shannon entropy is a particular case
of Tsallis-Havrda-Charvat entropy for $\alpha$ = 1. The influence of this
parameter on the final prediction results is studied. In this paper, the
experiments are conducted on two datasets including in total 580 patients, of
whom 434 suffered from head-neck cancers and 146 from lung cancers. The results
show that Tsallis-Havrda-Charvat entropy can achieve better performance in
terms of prediction accuracy with some values of $\alpha$.",-0.16360955,-0.16331568,-0.32826304,C
3994,3D          direction for further study.,"Further investigation into why the CAT modules do not work
   Our cross-slice attention method has shown its superiority     on frameworks without skip connections and how to make
to other methods for the following reasons: 2D networks           similar ideas work on these architectures would be a good
cannot acquire useful information from nearby slices.","convolution approaches do not perform well due to the
anisotropy of the MRI data (i.e., through-plane resolution sub-      In this work, we only considered T2WI, but in practice, as
stantially lower than in-plane resolution), which is problematic  suggested by previous works [40] [10], performing prostate
in learning the 3D shape.",2022-03-29 00:50:54+00:00,CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Alex Ling Yu Hung'), arxiv.Result.Author('Haoxin Zheng'), arxiv.Result.Author('Qi Miao'), arxiv.Result.Author('Steven S. Raman'), arxiv.Result.Author('Demetri Terzopoulos'), arxiv.Result.Author('Kyunghyun Sung')]","Prostate cancer is the second leading cause of cancer death among men in the
United States. The diagnosis of prostate MRI often relies on the accurate
prostate zonal segmentation. However, state-of-the-art automatic segmentation
methods often fail to produce well-contained volumetric segmentation of the
prostate zones since certain slices of prostate MRI, such as base and apex
slices, are harder to segment than other slices. This difficulty can be
overcome by accounting for the cross-slice relationship of adjacent slices, but
current methods do not fully learn and exploit such relationships. In this
paper, we propose a novel cross-slice attention mechanism, which we use in a
Transformer module to systematically learn the cross-slice relationship at
different scales. The module can be utilized in any existing learning-based
segmentation framework with skip connections. Experiments show that our
cross-slice attention is able to capture the cross-slice information in
prostate zonal segmentation and improve the performance of current
state-of-the-art methods. Our method improves segmentation accuracy in the
peripheral zone, such that the segmentation results are consistent across all
the prostate slices (apex, mid-gland, and base).",-0.06409175,0.13777879,0.14706284,A
4099,"We hope our eﬀort provides the community a point of departure for
further research into adaptive multi-coil acquisition.","We perform extensive evaluation on Cartesian sam-
pling for 2D MRI using the multi-coil fastMRI knee dataset (Knoll et al., 2020b) on 4×
and 8× acceleration.","Our experiments1 show that:

    • On the 8× setup, our learned policies improve ∼ 2% in SSIM over the strongest
       baseline, highlighting the ability of potentially-adaptive k-space acquisition to improve
       MRI reconstruction under high acceleration factors.",2022-03-30 15:23:23+00:00,On learning adaptive acquisition policies for undersampled multi-coil MRI reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Tim Bakker'), arxiv.Result.Author('Matthew Muckley'), arxiv.Result.Author('Adriana Romero-Soriano'), arxiv.Result.Author('Michal Drozdzal'), arxiv.Result.Author('Luis Pineda')]","Most current approaches to undersampled multi-coil MRI reconstruction focus
on learning the reconstruction model for a fixed, equidistant acquisition
trajectory. In this paper, we study the problem of joint learning of the
reconstruction model together with acquisition policies. To this end, we extend
the End-to-End Variational Network with learnable acquisition policies that can
adapt to different data points. We validate our model on a coil-compressed
version of the large scale undersampled multi-coil fastMRI dataset using two
undersampling factors: $4\times$ and $8\times$. Our experiments show on-par
performance with the learnable non-adaptive and handcrafted equidistant
strategies at $4\times$, and an observed improvement of more than $2\%$ in SSIM
at $8\times$ acceleration, suggesting that potentially-adaptive $k$-space
acquisition trajectories can improve reconstructed image quality for larger
acceleration factors. However, and perhaps surprisingly, our best performing
policies learn to be explicitly non-adaptive.",-0.03068763,-0.09956275,0.16677986,C
4100,"However, this framework aiming at maintaining a                A robust regularized optimization-based TDE scheme called
proper balance between motion continuity and discontinuity is       rGLUE for quasi-static ultrasound elastography has been
beyond the scope of this work and calls for further research.","noise statistics as a reference to re-estimate the displacement
ﬁeld with an adaptive distribution of regularization parameter                                 V. CONCLUSION
values.",proposed.,2022-03-30 15:27:10+00:00,Incorporating Gradient Similarity for Robust Time Delay Estimation in Ultrasound Elastography,eess.IV,"['eess.IV', 'physics.med-ph']","[arxiv.Result.Author('Md Ashikuzzaman'), arxiv.Result.Author('Timothy J. Hall'), arxiv.Result.Author('Hassan Rivaz')]","Energy-based ultrasound elastography techniques minimize a regularized cost
function consisting of data and continuity terms to obtain local displacement
estimates based on the local time-delay estimation (TDE) between
radio-frequency (RF) frames. The data term associated with the existing
techniques takes only the amplitude similarity into account and hence is not
sufficiently robust to the outlier samples present in the RF frames under
consideration. This drawback creates noticeable artifacts in the strain image.
To resolve this issue, we propose to formulate the data function as a linear
combination of the amplitude and gradient similarity constraints. We estimate
the adaptive weight concerning each similarity term following an iterative
scheme. Finally, we optimize the non-linear cost function in an efficient
manner to convert the problem to a sparse system of linear equations which are
solved for millions of variables. We call our technique rGLUE: robust data term
in GLobal Ultrasound Elastography. rGLUE has been validated using simulation,
phantom, in vivo liver, and breast datasets. In all of our experiments, rGLUE
substantially outperforms the recent elastography methods both visually and
quantitatively. For simulated, phantom, and in vivo datasets, respectively,
rGLUE achieves 107%, 18%, and 23% improvements of signal-to-noise ratio (SNR)
and 61%, 19%, and 25% improvements of contrast-to-noise ratio (CNR) over GLUE,
a recently-published elastography algorithm.",0.09248813,-0.11647107,0.26879656,C
4256,"A further study is needed to integrate the optical eﬀects of diﬀraction and
aberration in a display and human eye into the discussion of the pixel resolution and the rendered
depth, although an initial attempt has been made by Huang and Hua by extending their study [14].","Speciﬁcally, the pixel sampling determines the depth range in which the best pixel resolution can be
obtained for the viewer.",The last important limitation to be pointed out is the paraxial approximation.,2022-04-02 15:37:39+00:00,Optical modelling of accommodative light field display system and prediction of human eye responses,eess.IV,"['eess.IV', 'cs.HC', 'physics.optics']","[arxiv.Result.Author('Yuta Miyanishi'), arxiv.Result.Author('Erdem Sahin'), arxiv.Result.Author('Atanas Gotchev')]","The spatio-angular resolution of a light field (LF) display is a crucial
factor for delivering adequate spatial image quality and eliciting an
accommodation response. Previous studies have modelled retinal image formation
with an LF display and evaluated whether accommodation would be evoked
correctly. The models were mostly based on ray-tracing and a schematic eye
model, which pose computational complexity and inaccurately represent the human
eye population's behaviour. We propose an efficient wave-optics-based framework
to model the human eye and a general LF display. With the model, we simulated
the retinal point spread function (PSF) of a point rendered by an LF display at
various depths to characterise the retinal image quality. Additionally,
accommodation responses to rendered LF images were estimated by computing the
visual Strehl ratio based on the optical transfer function (VSOTF) from the
PSFs. We assumed an ideal LF display that had an infinite spatial resolution
and was free from optical aberrations in the simulation. We tested images
rendered at 0--4 dioptres of depths having angular resolutions of up to 4x4
viewpoints within a pupil. The simulation predicted small and constant
accommodation errors, which contradict the findings of previous studies. An
evaluation of the optical resolution of the rendered retinal image suggested a
trade-off between the maximum resolution achievable and the depth range of a
rendered image where in-focus resolution is kept high. The proposed framework
can be used to evaluate the upper bound of the optical performance of an LF
display for realistically aberrated eyes, which may help to find an optimal
spatio-angular resolution required to render a high quality 3D scene.",0.3529173,-0.26282334,0.2939772,B
4345,"It is
this combination of visible quality loss and widespread use that makes
JPEG ideal for further study.","At the same time,
JPEG does suﬀer from some extreme artifacts in many conditions.","JPEG Domain Residual Learning

N we develop a general method for performing residual net-                     .",2022-04-04 18:24:25+00:00,The First Principles of Deep Learning and Compression,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",[arxiv.Result.Author('Max Ehrlich')],"The deep learning revolution incited by the 2012 Alexnet paper has been
transformative for the field of computer vision. Many problems which were
severely limited using classical solutions are now seeing unprecedented
success. The rapid proliferation of deep learning methods has led to a sharp
increase in their use in consumer and embedded applications. One consequence of
consumer and embedded applications is lossy multimedia compression which is
required to engineer the efficient storage and transmission of data in these
real-world scenarios. As such, there has been increased interest in a deep
learning solution for multimedia compression which would allow for higher
compression ratios and increased visual quality.
  The deep learning approach to multimedia compression, so called Learned
Multimedia Compression, involves computing a compressed representation of an
image or video using a deep network for the encoder and the decoder. While
these techniques have enjoyed impressive academic success, their industry
adoption has been essentially non-existent. Classical compression techniques
like JPEG and MPEG are too entrenched in modern computing to be easily
replaced. This dissertation takes an orthogonal approach and leverages deep
learning to improve the compression fidelity of these classical algorithms.
This allows the incredible advances in deep learning to be used for multimedia
compression without threatening the ubiquity of the classical methods.
  The key insight of this work is that methods which are motivated by first
principles, i.e., the underlying engineering decisions that were made when the
compression algorithms were developed, are more effective than general methods.
By encoding prior knowledge into the design of the algorithm, the flexibility,
performance, and/or accuracy are improved at the cost of generality...",0.31545636,0.21273902,0.11253019,B
4374,"into the weighting matrix principle to further reduce the ACS
                                                                  lines is worth investigation in further study.","Therefore, the two-              other strategies such as the virtual conjugate coil (VCC) [53]
layer CNNs are selected for reconstruction in this work.",Fig.,2022-04-05 04:28:06+00:00,Multi-Weight Respecification of Scan-specific Learning for Parallel Imaging,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Hui Tao'), arxiv.Result.Author('Haifeng Wang'), arxiv.Result.Author('Shanshan Wang'), arxiv.Result.Author('Dong Liang'), arxiv.Result.Author('Xiaoling Xu'), arxiv.Result.Author('Qiegen Liu')]","Parallel imaging is widely used in magnetic resonance imaging as an
acceleration technology. Traditional linear reconstruction methods in parallel
imaging often suffer from noise amplification. Recently, a non-linear robust
artificial-neural-network for k-space interpolation (RAKI) exhibits superior
noise resilience over other linear methods. However, RAKI performs poorly at
high acceleration rates, and needs a large amount of autocalibration signals as
the training samples. In order to tackle these issues, we propose a
multi-weight method that implements multiple weighting matrices on the
undersampled data, named as MW-RAKI. Enforcing multiple weighted matrices on
the measurements can effectively reduce the influence of noise and increase the
data constraints. Furthermore, we incorporate the strategy of multiple
weighting matrixes into a residual version of RAKI, and form
MW-rRAKI.Experimental compari-sons with the alternative methods demonstrated
noticeably better reconstruction performances, particularly at high
acceleration rates.",0.18910384,0.19359806,0.17137453,A
4422,"For example, we intend to use it to identify participants in the
UKBB with RPD for further research, including genetic analysis, where quantification is key.","Of note is the improved sensitivity of the model with the increased number of
average false positive lesions per B-scan, the importance of which depends on the settings
under which the model is used.","To
that end, a small number of false positives (i.e., high specificity) is required.",2022-04-05 14:02:45+00:00,A deep learning framework for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Roy Schwartz'), arxiv.Result.Author('Hagar Khalid'), arxiv.Result.Author('Sandra Liakopoulos'), arxiv.Result.Author('Yanling Ouyang'), arxiv.Result.Author('Coen de Vente'), arxiv.Result.Author('Cristina González-Gonzalo'), arxiv.Result.Author('Aaron Y. Lee'), arxiv.Result.Author('Robyn Guymer'), arxiv.Result.Author('Emily Y. Chew'), arxiv.Result.Author('Catherine Egan'), arxiv.Result.Author('Zhichao Wu'), arxiv.Result.Author('Himeesh Kumar'), arxiv.Result.Author('Joseph Farrington'), arxiv.Result.Author('Clara I. Sánchez'), arxiv.Result.Author('Adnan Tufail')]","Purpose - To develop and validate a deep learning (DL) framework for the
detection and quantification of drusen and reticular pseudodrusen (RPD) on
optical coherence tomography scans.
  Design - Development and validation of deep learning models for
classification and feature segmentation.
  Methods - A DL framework was developed consisting of a classification model
and an out-of-distribution (OOD) detection model for the identification of
ungradable scans; a classification model to identify scans with drusen or RPD;
and an image segmentation model to independently segment lesions as RPD or
drusen. Data were obtained from 1284 participants in the UK Biobank (UKBB) with
a self-reported diagnosis of age-related macular degeneration (AMD) and 250
UKBB controls. Drusen and RPD were manually delineated by five retina
specialists. The main outcome measures were sensitivity, specificity, area
under the ROC curve (AUC), kappa, accuracy and intraclass correlation
coefficient (ICC).
  Results - The classification models performed strongly at their respective
tasks (0.95, 0.93, and 0.99 AUC, respectively, for the ungradable scans
classifier, the OOD model, and the drusen and RPD classification model). The
mean ICC for drusen and RPD area vs. graders was 0.74 and 0.61, respectively,
compared with 0.69 and 0.68 for intergrader agreement. FROC curves showed that
the model's sensitivity was close to human performance.
  Conclusions - The models achieved high classification and segmentation
performance, similar to human performance. Application of this robust framework
will further our understanding of RPD as a separate entity from drusen in both
research and clinical settings.",-0.2974446,-0.28161132,-0.13031557,C
4546,"This analysis of the literature shows that the development of AI-based mod-
els predicting the outcomes of COVID-19 patients still deserves further research
eﬀorts, since in the multimodal scenario only simple methods are employed with
no interest in understanding when, which and how the neural networks archi-
tectures should be used to ﬁnd the appropriate fusion.","The best results are obtained by the ETE
approach, whose results are reported and discussed in section 6.","We decided to use Soda
et al.",2022-04-07 23:07:33+00:00,"Multi-objective optimization determines when, which and how to fuse deep networks: an application to predict COVID-19 outcomes",eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Valerio Guarrasi'), arxiv.Result.Author('Paolo Soda')]","The COVID-19 pandemic has caused millions of cases and deaths and the
AI-related scientific community, after being involved with detecting COVID-19
signs in medical images, has been now directing the efforts towards the
development of methods that can predict the progression of the disease. This
task is multimodal by its very nature and, recently, baseline results achieved
on the publicly available AIforCOVID dataset have shown that chest X-ray scans
and clinical information are useful to identify patients at risk of severe
outcomes. While deep learning has shown superior performance in several medical
fields, in most of the cases it considers unimodal data only. In this respect,
when, which and how to fuse the different modalities is an open challenge in
multimodal deep learning. To cope with these three questions here we present a
novel approach optimizing the setup of a multimodal end-to-end model. It
exploits Pareto multi-objective optimization working with a performance metric
and the diversity score of multiple candidate unimodal neural networks to be
fused. We test our method on the AIforCOVID dataset, attaining state-of-the-art
results, not only outperforming the baseline performance but also being robust
to external validation. Moreover, exploiting XAI algorithms we figure out a
hierarchy among the modalities and we extract the features' intra-modality
importance, enriching the trust on the predictions made by the model.",-0.18513444,0.085837,-0.1603684,A
4670,further research in this direction.,"Robustness is not a property that can be     we cannot single out speciﬁc architectures in terms of a
taken for granted, but requires dedicated efforts to be achieved   particularly high robustness but see a general necessity for
and incorporated into the state of the art.","We would like to highlight that, for a sincere evaluation                                 IV.",2022-04-11 12:26:19+00:00,From CNNs to Vision Transformers -- A Comprehensive Evaluation of Deep Learning Models for Histopathology,eess.IV,['eess.IV'],"[arxiv.Result.Author('Maximilian Springenberg'), arxiv.Result.Author('Annika Frommholz'), arxiv.Result.Author('Markus Wenzel'), arxiv.Result.Author('Eva Weicken'), arxiv.Result.Author('Jackie Ma'), arxiv.Result.Author('Nils Strodthoff')]","While machine learning is currently transforming the field of histopathology,
the domain lacks a comprehensive evaluation of state-of-the-art models based on
essential but complementary quality requirements beyond a mere classification
accuracy. In order to fill this gap, we conducted an extensive evaluation by
benchmarking a wide range of classification models, including recent vision
transformers, convolutional neural networks and hybrid models comprising
transformer and convolutional models. We thoroughly tested the models on five
widely used histopathology datasets containing whole slide images of breast,
gastric, and colorectal cancer and developed a novel approach using an
image-to-image translation model to assess the robustness of a cancer
classification model against stain variations. Further, we extended existing
interpretability methods to previously unstudied models and systematically
reveal insights of the models' classification strategies that allow for
plausibility checks and systematic comparisons. The study resulted in specific
model recommendations for practitioners as well as putting forward a general
methodology to quantify a model's quality according to complementary
requirements that can be transferred to future model architectures.",0.22655654,-0.08197762,-0.055248413,B
4735,"Third, we further study the robustness of our methods                      The elasticity map of chicken heart tissue, B-Mode ultrasound
and show the standard deviation of predictions at each pixel                  image and cross-section of the phantom is given in Figure 11.","This is
                                                                              consistent for all binarization thresholds as shown in Figure 10.","IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL.",2022-04-11 08:30:58+00:00,Ultrasound Shear Wave Elasticity Imaging with Spatio-Temporal Deep Learning,eess.IV,"['eess.IV', 'cs.LG', 'physics.med-ph']","[arxiv.Result.Author('Maximilian Neidhardt'), arxiv.Result.Author('Marcel Bengs'), arxiv.Result.Author('Sarah Latus'), arxiv.Result.Author('Stefan Gerlach'), arxiv.Result.Author('Christian J. Cyron'), arxiv.Result.Author('Johanna Sprenger'), arxiv.Result.Author('Alexander Schlaefer')]","Ultrasound shear wave elasticity imaging is a valuable tool for quantifying
the elastic properties of tissue. Typically, the shear wave velocity is derived
and mapped to an elasticity value, which neglects information such as the shape
of the propagating shear wave or push sequence characteristics. We present 3D
spatio-temporal CNNs for fast local elasticity estimation from ultrasound data.
This approach is based on retrieving elastic properties from shear wave
propagation within small local regions. A large training data set is acquired
with a robot from homogeneous gelatin phantoms ranging from 17.42 kPa to 126.05
kPa with various push locations. The results show that our approach can
estimate elastic properties on a pixelwise basis with a mean absolute error of
5.01+-4.37 kPa. Furthermore, we estimate local elasticity independent of the
push location and can even perform accurate estimates inside the push region.
For phantoms with embedded inclusions, we report a 53.93% lower MAE (7.50 kPa)
and on the background of 85.24% (1.64 kPa) compared to a conventional shear
wave method. Overall, our method offers fast local estimations of elastic
properties with small spatio-temporal window sizes.",-0.046374757,-0.1350794,0.32192552,C
4736,"20                             20
10                             10                                                Third, we further study the robustness of our methods
                                                                              and show the standard deviation of predictions at each pixel
 0                              0                                             for the complete FOV for the spatio-temporal CNN and the
      37.55 56.04 72.64 97.22           23456                                 ToF approach using push one, four and seven, see Figure 8.","40                             40                                             of 31 compared to the smallest input size of 5 × 5 pixels (∼
30                             30                                             0.32 × 0.4 mm).","Note that push one and seven are completely removed from
Fig.",2022-04-11 08:30:58+00:00,Ultrasound Shear Wave Elasticity Imaging with Spatio-Temporal Deep Learning,eess.IV,"['eess.IV', 'cs.LG', 'physics.med-ph']","[arxiv.Result.Author('Maximilian Neidhardt'), arxiv.Result.Author('Marcel Bengs'), arxiv.Result.Author('Sarah Latus'), arxiv.Result.Author('Stefan Gerlach'), arxiv.Result.Author('Christian J. Cyron'), arxiv.Result.Author('Johanna Sprenger'), arxiv.Result.Author('Alexander Schlaefer')]","Ultrasound shear wave elasticity imaging is a valuable tool for quantifying
the elastic properties of tissue. Typically, the shear wave velocity is derived
and mapped to an elasticity value, which neglects information such as the shape
of the propagating shear wave or push sequence characteristics. We present 3D
spatio-temporal CNNs for fast local elasticity estimation from ultrasound data.
This approach is based on retrieving elastic properties from shear wave
propagation within small local regions. A large training data set is acquired
with a robot from homogeneous gelatin phantoms ranging from 17.42 kPa to 126.05
kPa with various push locations. The results show that our approach can
estimate elastic properties on a pixelwise basis with a mean absolute error of
5.01+-4.37 kPa. Furthermore, we estimate local elasticity independent of the
push location and can even perform accurate estimates inside the push region.
For phantoms with embedded inclusions, we report a 53.93% lower MAE (7.50 kPa)
and on the background of 85.24% (1.64 kPa) compared to a conventional shear
wave method. Overall, our method offers fast local estimations of elastic
properties with small spatio-temporal window sizes.",0.23032741,0.21083948,0.06267004,A
4946,"It             searchers are inspired to conduct further research within the
would be very interesting to know the differences and re-               scope of Transformers in CL and medical image segmenta-
sults when applied in a class incremental setup instead.","The first is our reduced scope, as we only
focus on domain incremental learning and one anatomy                       Given our promising findings, we hope that other re-
for our analysis due to limited computational resources.",tion.,2022-04-17 16:13:04+00:00,Continual Hippocampus Segmentation with Transformers,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Amin Ranem'), arxiv.Result.Author('Camila González'), arxiv.Result.Author('Anirban Mukhopadhyay')]","In clinical settings, where acquisition conditions and patient populations
change over time, continual learning is key for ensuring the safe use of deep
neural networks. Yet most existing work focuses on convolutional architectures
and image classification. Instead, radiologists prefer to work with
segmentation models that outline specific regions-of-interest, for which
Transformer-based architectures are gaining traction. The self-attention
mechanism of Transformers could potentially mitigate catastrophic forgetting,
opening the way for more robust medical image segmentation. In this work, we
explore how recently-proposed Transformer mechanisms for semantic segmentation
behave in sequential learning scenarios, and analyse how best to adapt
continual learning strategies for this setting. Our evaluation on hippocampus
segmentation shows that Transformer mechanisms mitigate catastrophic forgetting
for medical image segmentation compared to purely convolutional architectures,
and demonstrates that regularising ViT modules should be done with caution.",-0.077907726,0.0058242334,-0.036983352,C
4992,"3, given all
experimentally set the rate as 0.9, and we further study the        model weights, we aim to optimize the routing matrix R to
effects on different choices of τ in our experiments.",As shown in Fig.,"For the       calculate the outside personalized model weight wo from the
local gradient descent, in practice, we iteratively conduct the     routing space W = R·[w1, · · · , wk, w¯] .",2022-04-16 08:26:19+00:00,IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Meirui Jiang'), arxiv.Result.Author('Hongzheng Yang'), arxiv.Result.Author('Chen Cheng'), arxiv.Result.Author('Qi Dou')]","Federated learning (FL) allows multiple medical institutions to
collaboratively learn a global model without centralizing all clients data. It
is difficult, if possible at all, for such a global model to commonly achieve
optimal performance for each individual client, due to the heterogeneity of
medical data from various scanners and patient demographics. This problem
becomes even more significant when deploying the global model to unseen clients
outside the FL with new distributions not presented during federated training.
To optimize the prediction accuracy of each individual client for critical
medical tasks, we propose a novel unified framework for both Inside and Outside
model Personalization in FL (IOP-FL). Our inside personalization is achieved by
a lightweight gradient-based approach that exploits the local adapted model for
each client, by accumulating both the global gradients for common knowledge and
local gradients for client-specific optimization. Moreover, and importantly,
the obtained local personalized models and the global model can form a diverse
and informative routing space to personalize a new model for outside FL
clients. Hence, we design a new test-time routing scheme inspired by the
consistency loss with a shape constraint to dynamically incorporate the models,
given the distribution information conveyed by the test data. Our extensive
experimental results on two medical image segmentation tasks present
significant improvements over SOTA methods on both inside and outside
personalization, demonstrating the great potential of our IOP-FL scheme for
clinical practice. Code will be released at https://github.com/med-air/IOP-FL.",0.27716762,0.047901645,-0.337088,B
4993,"In the following, we will illustrate the design of
we further study the personalization on new data outside            losses to optimize the coefﬁcients in the routing matrix.","For another case,
C. Test-time Routing for Outside Personalization                    the R coordinates all candidates in the model set to generate
                                                                    better parameters that ﬁt test data distribution as closely as
   With the obtained models during the inside personalization,      possible.","the federation, which is more challenging due to the unseen
distribution and unavailable labels.",2022-04-16 08:26:19+00:00,IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Meirui Jiang'), arxiv.Result.Author('Hongzheng Yang'), arxiv.Result.Author('Chen Cheng'), arxiv.Result.Author('Qi Dou')]","Federated learning (FL) allows multiple medical institutions to
collaboratively learn a global model without centralizing all clients data. It
is difficult, if possible at all, for such a global model to commonly achieve
optimal performance for each individual client, due to the heterogeneity of
medical data from various scanners and patient demographics. This problem
becomes even more significant when deploying the global model to unseen clients
outside the FL with new distributions not presented during federated training.
To optimize the prediction accuracy of each individual client for critical
medical tasks, we propose a novel unified framework for both Inside and Outside
model Personalization in FL (IOP-FL). Our inside personalization is achieved by
a lightweight gradient-based approach that exploits the local adapted model for
each client, by accumulating both the global gradients for common knowledge and
local gradients for client-specific optimization. Moreover, and importantly,
the obtained local personalized models and the global model can form a diverse
and informative routing space to personalize a new model for outside FL
clients. Hence, we design a new test-time routing scheme inspired by the
consistency loss with a shape constraint to dynamically incorporate the models,
given the distribution information conveyed by the test data. Our extensive
experimental results on two medical image segmentation tasks present
significant improvements over SOTA methods on both inside and outside
personalization, demonstrating the great potential of our IOP-FL scheme for
clinical practice. Code will be released at https://github.com/med-air/IOP-FL.",0.1696682,-0.046252906,-0.34507042,B
4994,"In speciﬁc, we
achieve the highest average results on prostate segmentation by               Effects of the accumulation rate: We further study how
increasing 5.26% in Dice over Ensemble.","test data, thus achieving superior performance.",and 3.47% in Dice                  the accumulation rate τ affects the performance of our method.,2022-04-16 08:26:19+00:00,IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Meirui Jiang'), arxiv.Result.Author('Hongzheng Yang'), arxiv.Result.Author('Chen Cheng'), arxiv.Result.Author('Qi Dou')]","Federated learning (FL) allows multiple medical institutions to
collaboratively learn a global model without centralizing all clients data. It
is difficult, if possible at all, for such a global model to commonly achieve
optimal performance for each individual client, due to the heterogeneity of
medical data from various scanners and patient demographics. This problem
becomes even more significant when deploying the global model to unseen clients
outside the FL with new distributions not presented during federated training.
To optimize the prediction accuracy of each individual client for critical
medical tasks, we propose a novel unified framework for both Inside and Outside
model Personalization in FL (IOP-FL). Our inside personalization is achieved by
a lightweight gradient-based approach that exploits the local adapted model for
each client, by accumulating both the global gradients for common knowledge and
local gradients for client-specific optimization. Moreover, and importantly,
the obtained local personalized models and the global model can form a diverse
and informative routing space to personalize a new model for outside FL
clients. Hence, we design a new test-time routing scheme inspired by the
consistency loss with a shape constraint to dynamically incorporate the models,
given the distribution information conveyed by the test data. Our extensive
experimental results on two medical image segmentation tasks present
significant improvements over SOTA methods on both inside and outside
personalization, demonstrating the great potential of our IOP-FL scheme for
clinical practice. Code will be released at https://github.com/med-air/IOP-FL.",-0.07566708,-0.17338032,-0.025451813,C
5254,"Additionally, GAN-based networks have been used for pelvic        Most of this research, on the other hand, is based on private
imaging [30], while further research has presented methods        information.",Several approaches have been created based on these datasets.,"To deal with inflexible registration of MRI-
for registering cardiac MRI and CT images [132].",2022-04-24 19:34:00+00:00,Deep Learning for Medical Image Registration: A Comprehensive Review,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Subrato Bharati'), arxiv.Result.Author('M. Rubaiyat Hossain Mondal'), arxiv.Result.Author('Prajoy Podder'), arxiv.Result.Author('V. B. Surya Prasath')]","Image registration is a critical component in the applications of various
medical image analyses. In recent years, there has been a tremendous surge in
the development of deep learning (DL)-based medical image registration models.
This paper provides a comprehensive review of medical image registration.
Firstly, a discussion is provided for supervised registration categories, for
example, fully supervised, dual supervised, and weakly supervised registration.
Next, similarity-based as well as generative adversarial network (GAN)-based
registration are presented as part of unsupervised registration. Deep iterative
registration is then described with emphasis on deep similarity-based and
reinforcement learning-based registration. Moreover, the application areas of
medical image registration are reviewed. This review focuses on monomodal and
multimodal registration and associated imaging, for instance, X-ray, CT scan,
ultrasound, and MRI. The existing challenges are highlighted in this review,
where it is shown that a major challenge is the absence of a training dataset
with known transformations. Finally, a discussion is provided on the promising
future research areas in the field of DL-based medical image registration.",-0.19599614,0.14915378,0.2234276,A
5263,"Then we will further study the possibility of
The results show that there is still a long way to go for cur-                  formulating accurate clinical treatment plans for breast can-
rent methods before clinical application, which also proves                     cer using synthetic IHC images.",IHC images.,the importance of the BCI dataset in further research.,2022-04-25 04:00:47+00:00,BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Shengjie Liu'), arxiv.Result.Author('Chuang Zhu'), arxiv.Result.Author('Feng Xu'), arxiv.Result.Author('Xinyu Jia'), arxiv.Result.Author('Zhongyue Shi'), arxiv.Result.Author('Mulan Jin')]","The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.",-0.2052771,-0.23201388,0.070526056,C
5264,the importance of the BCI dataset in further research.,"Then we will further study the possibility of
The results show that there is still a long way to go for cur-                  formulating accurate clinical treatment plans for breast can-
rent methods before clinical application, which also proves                     cer using synthetic IHC images.","Acknowledgement
6.",2022-04-25 04:00:47+00:00,BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Shengjie Liu'), arxiv.Result.Author('Chuang Zhu'), arxiv.Result.Author('Feng Xu'), arxiv.Result.Author('Xinyu Jia'), arxiv.Result.Author('Zhongyue Shi'), arxiv.Result.Author('Mulan Jin')]","The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.",-0.2021913,-0.22743046,0.06389285,C
5265,"Then we will further study the possibility of
The results show that there is still a long way to go for cur-                  formulating accurate clinical treatment plans for breast can-
rent methods before clinical application, which also proves                     cer using synthetic IHC images.",IHC images.,the importance of the BCI dataset in further research.,2022-04-25 04:00:47+00:00,BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Shengjie Liu'), arxiv.Result.Author('Chuang Zhu'), arxiv.Result.Author('Feng Xu'), arxiv.Result.Author('Xinyu Jia'), arxiv.Result.Author('Zhongyue Shi'), arxiv.Result.Author('Mulan Jin')]","The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.",-0.2052771,-0.23201388,0.070526056,C
5266,the importance of the BCI dataset in further research.,"Then we will further study the possibility of
The results show that there is still a long way to go for cur-                  formulating accurate clinical treatment plans for breast can-
rent methods before clinical application, which also proves                     cer using synthetic IHC images.","Acknowledgement
6.",2022-04-25 04:00:47+00:00,BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Shengjie Liu'), arxiv.Result.Author('Chuang Zhu'), arxiv.Result.Author('Feng Xu'), arxiv.Result.Author('Xinyu Jia'), arxiv.Result.Author('Zhongyue Shi'), arxiv.Result.Author('Mulan Jin')]","The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.",-0.2021913,-0.22743046,0.06389285,C
5346,"Therefore, a comparison of bigger cell size prescription
maps must be conducted in further research.","Alternative map grids with approximately 10×10 ft cell size would have been
useful for better efficacy.","Furthermore, we suggest that the
sprayer would have done a poor job if the sprayer speed was operated at 12 mph
or more.",2022-04-26 16:21:21+00:00,UAS Imagery and Computer Vision for Site-Specific Weed Control in Corn,eess.IV,['eess.IV'],"[arxiv.Result.Author('Ranjan Sapkota'), arxiv.Result.Author('Paulo Flores')]","Currently, weed control in a corn field is performed by a blanket application
of herbicides which do not consider spatial distribution information of weeds
and also uses an extensive amount of chemical herbicides. In order to reduce
the amount of chemicals, we used drone based high-resolution imagery and
computer-vision techniwue to perform site-specific weed control in corn.",0.013282578,-0.24961722,-0.003231315,C
5491,"This raises the question, for further research, of whether
reconstructions when using TG initialization on CIFAR-100          speciﬁc conﬁgurations perform better on speciﬁc types of
                                                                   images.","However, it is still unclear if there is a correlation between the
Using this adaptive measure results in the most accurate           image data distribution and optimal choice of conﬁguration.","(MSE: 5.71e-3, SSIM: 0.850), while still keeping a low

number of non-converging images (6).",2022-04-25 20:23:25+00:00,Analysing the Influence of Attack Configurations on the Reconstruction of Medical Images in Federated Learning,eess.IV,"['eess.IV', 'cs.CR', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Mads Emil Dahlgaard'), arxiv.Result.Author('Morten Wehlast Jørgensen'), arxiv.Result.Author('Niels Asp Fuglsang'), arxiv.Result.Author('Hiba Nassar')]","The idea of federated learning is to train deep neural network models
collaboratively and share them with multiple participants without exposing
their private training data to each other. This is highly attractive in the
medical domain due to patients' privacy records. However, a recently proposed
method called Deep Leakage from Gradients enables attackers to reconstruct data
from shared gradients. This study shows how easy it is to reconstruct images
for different data initialization schemes and distance measures. We show how
data and model architecture influence the optimal choice of initialization
scheme and distance measure configurations when working with single images. We
demonstrate that the choice of initialization scheme and distance measure can
significantly increase convergence speed and quality. Furthermore, we find that
the optimal attack configuration depends largely on the nature of the target
image distribution and the complexity of the model architecture.",0.26910242,-0.09934729,0.19300403,B
5523,"Obviously, C(k,l) and Dk are independent of the                length effects have to be considered and further research is
                                                                         necessary for determining the required bit-depth of the tables
input signal and the residual.","For a hardware implementation or
                                                                         an implementation on a digital signal processor, ﬁnite-word
products.","Hence, they only have to be               and the impact of ﬁxed-point arithmetic.",2022-04-27 12:47:30+00:00,A Fast Algorithm for Selective Signal Extrapolation with Arbitrary Basis Functions,eess.IV,['eess.IV'],"[arxiv.Result.Author('Jürgen Seiler'), arxiv.Result.Author('André Kaup')]","Signal extrapolation is an important task in digital signal processing for
extending known signals into unknown areas. The Selective Extrapolation is a
very effective algorithm to achieve this. Thereby, the extrapolation is
obtained by generating a model of the signal to be extrapolated as weighted
superposition of basis functions. Unfortunately, this algorithm is
computationally very expensive and, up to now, efficient implementations exist
only for basis function sets that emanate from discrete transforms. Within the
scope of this contribution, a novel efficient solution for Selective
Extrapolation is presented for utilization with arbitrary basis functions. The
proposed algorithm mathematically behaves identically to the original Selective
Extrapolation, but is several decades faster. Furthermore, it is able to
outperform existent fast transform domain algorithms which are limited to basis
function sets that belong to the corresponding transform. With that, the novel
algorithm allows for an efficient use of arbitrary basis functions, even if
they are only numerically defined.",0.27368325,-0.29986915,-0.18300088,B
5524,"53,
door for further research on carrying out the extrapolation                     no.","on Broadcasting, vol.","1, pp.",2022-04-27 12:47:30+00:00,A Fast Algorithm for Selective Signal Extrapolation with Arbitrary Basis Functions,eess.IV,['eess.IV'],"[arxiv.Result.Author('Jürgen Seiler'), arxiv.Result.Author('André Kaup')]","Signal extrapolation is an important task in digital signal processing for
extending known signals into unknown areas. The Selective Extrapolation is a
very effective algorithm to achieve this. Thereby, the extrapolation is
obtained by generating a model of the signal to be extrapolated as weighted
superposition of basis functions. Unfortunately, this algorithm is
computationally very expensive and, up to now, efficient implementations exist
only for basis function sets that emanate from discrete transforms. Within the
scope of this contribution, a novel efficient solution for Selective
Extrapolation is presented for utilization with arbitrary basis functions. The
proposed algorithm mathematically behaves identically to the original Selective
Extrapolation, but is several decades faster. Furthermore, it is able to
outperform existent fast transform domain algorithms which are limited to basis
function sets that belong to the corresponding transform. With that, the novel
algorithm allows for an efficient use of arbitrary basis functions, even if
they are only numerically defined.",0.23854601,-0.25594196,-0.11010739,B
5914,Sec-      searchers continued to carry out further research on the basis.,"In the follow-up development, many re-
   The remainder of this paper is organized as follows.",tion II briefly reviews some related PI methods in k-space        Liu et al.,2022-05-08 14:28:20+00:00,WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Zongjiang Tu'), arxiv.Result.Author('Die Liu'), arxiv.Result.Author('Xiaoqing Wang'), arxiv.Result.Author('Chen Jiang'), arxiv.Result.Author('Minghui Zhang'), arxiv.Result.Author('Qiegen Liu'), arxiv.Result.Author('Dong Liang')]","Parallel Imaging (PI) is one of the most im-portant and successful
developments in accelerating magnetic resonance imaging (MRI). Recently deep
learning PI has emerged as an effective technique to accelerate MRI.
Nevertheless, most approaches have so far been based image domain. In this
work, we propose to explore the k-space domain via robust generative modeling
for flexible PI reconstruction, coined weight-k-space generative model (WKGM).
Specifically, WKGM is a generalized k-space domain model, where the k-space
weighting technology and high-dimensional space strategy are efficiently
incorporated for score-based generative model training, resulting in good and
robust reconstruction. In addition, WKGM is flexible and thus can
synergistically combine various traditional k-space PI models, generating
learning-based priors to produce high-fidelity reconstructions. Experimental
results on datasets with varying sampling patterns and acceleration factors
demonstrate that WKGM can attain state-of-the-art reconstruction results under
the well-learned k-space generative prior.",0.20499134,-0.12807566,-0.16239738,B
5915,"In the follow-up development, many re-
    is utilized in the k-space domain reconstruction proce-       searchers continued to carry out further research on the basis.","variational autoencoders (VAEs) as a tool for describing the
⚫ Flexible Reconstruction Scheme: The generative model            data density prior.","dure in an iterative manner, enabling flexible incorpora-     Liu et al.",2022-05-08 14:28:20+00:00,WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Zongjiang Tu'), arxiv.Result.Author('Die Liu'), arxiv.Result.Author('Xiaoqing Wang'), arxiv.Result.Author('Chen Jiang'), arxiv.Result.Author('Minghui Zhang'), arxiv.Result.Author('Shanshan Wang'), arxiv.Result.Author('Qiegen Liu'), arxiv.Result.Author('Dong Liang')]","Deep learning based parallel Imaging (PI) has made great progresses in recent
years to accelerate magnetic resonance imaging (MRI). Nevertheless, the
performanc-es and robustness of existing methods can still be im-proved. In
this work, we propose to explore the k-space domain learning via robust
generative modeling for flexible PI reconstruction, coined weight-k-space
genera-tive model (WKGM). Specifically, WKGM is a general-ized k-space domain
model, where the k-space weighting technology and high-dimensional space
augmentation design are efficiently incorporated for score-based gen-erative
model training, resulting in good and robust re-constructions. In addition,
WKGM is flexible and thus can be synergistically combined with various
traditional k-space PI models, generating learning-based priors to produce
high-fidelity reconstructions. Experimental re-sults on datasets with varying
sampling patterns and ac-celeration factors demonstrate that WKGM can attain
state-of-the-art reconstruction results with the well-learned k-space
generative prior.",0.23071453,0.22393373,0.08793782,A
5916,"In the follow-up development, many re-
                                                               searchers continued to carry out further research on the basis.","[33] utilized the
                                                               variational autoencoders (VAEs) as a tool for describing the
                                                               data density prior.",Liu et al.,2022-05-08 14:28:20+00:00,WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Zongjiang Tu'), arxiv.Result.Author('Die Liu'), arxiv.Result.Author('Xiaoqing Wang'), arxiv.Result.Author('Chen Jiang'), arxiv.Result.Author('Minghui Zhang'), arxiv.Result.Author('Shanshan Wang'), arxiv.Result.Author('Qiegen Liu'), arxiv.Result.Author('Dong Liang')]","Deep learning based parallel imaging (PI) has made great progresses in recent
years to accelerate magnetic resonance imaging (MRI). Nevertheless, it still
has some limitations, such as the robustness and flexibility of existing
methods have great deficiency. In this work, we propose a method to explore the
k-space domain learning via robust generative modeling for flexible
calibration-less PI reconstruction, coined weight-k-space generative model
(WKGM). Specifically, WKGM is a generalized k-space domain model, where the
k-space weighting technology and high-dimensional space augmentation design are
efficiently incorporated for score-based generative model training, resulting
in good and robust reconstructions. In addition, WKGM is flexible and thus can
be synergistically combined with various traditional k-space PI models, which
can make full use of the correlation between multi-coil data and
realizecalibration-less PI. Even though our model was trained on only 500
images, experimental results with varying sampling patterns and acceleration
factors demonstrate that WKGM can attain state-of-the-art reconstruction
results with the well-learned k-space generative prior.",0.11861495,0.12830383,-0.14163251,A
5917,"In the follow-up development, many re-
                                                               searchers continued to carry out further research on the basis.","[33] utilized the
                                                               variational autoencoders (VAEs) as a tool for describing the
                                                               data density prior.",Liu et al.,2022-05-08 14:28:20+00:00,WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Zongjiang Tu'), arxiv.Result.Author('Die Liu'), arxiv.Result.Author('Xiaoqing Wang'), arxiv.Result.Author('Chen Jiang'), arxiv.Result.Author('Pengwen Zhu'), arxiv.Result.Author('Minghui Zhang'), arxiv.Result.Author('Shanshan Wang'), arxiv.Result.Author('Dong Liang'), arxiv.Result.Author('Qiegen Liu')]","Deep learning based parallel imaging (PI) has made great progresses in recent
years to accelerate magnetic resonance imaging (MRI). Nevertheless, it still
has some limitations, such as the robustness and flexibility of existing
methods have great deficiency. In this work, we propose a method to explore the
k-space domain learning via robust generative modeling for flexible
calibration-less PI reconstruction, coined weight-k-space generative model
(WKGM). Specifically, WKGM is a generalized k-space domain model, where the
k-space weighting technology and high-dimensional space augmentation design are
efficiently incorporated for score-based generative model training, resulting
in good and robust reconstructions. In addition, WKGM is flexible and thus can
be synergistically combined with various traditional k-space PI models, which
can make full use of the correlation between multi-coil data and
realizecalibration-less PI. Even though our model was trained on only 500
images, experimental results with varying sampling patterns and acceleration
factors demonstrate that WKGM can attain state-of-the-art reconstruction
results with the well-learned k-space generative prior.",0.11861495,0.12830383,-0.14163251,A
5984,"Thus, although OCTA devices are not as common as OCT
devices in all medical centers, they may be potentially helpful and worthy of further research
for MA analysis.","In addition to the above and as stated in [51], MAs can also be examined in terms of flow,
location and capillary density.","References

1.",2022-05-10 06:43:01+00:00,Automatic Detection of Microaneurysms in OCT Images Using Bag of Features,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Elahe Sadat Kazemi Nasab'), arxiv.Result.Author('Ramin Almasi'), arxiv.Result.Author('Bijan Shoushtarian'), arxiv.Result.Author('Ehsan Golkar'), arxiv.Result.Author('Hossein Rabbani')]","Diabetic Retinopathy (DR) caused by diabetes occurs as a result of changes in
the retinal vessels and causes visual impairment. Microaneurysms (MAs) are the
early clinical signs of DR, whose timely diagnosis can help detecting DR in the
early stages of its development. It has been observed that MAs are more common
in the inner retinal layers compared to the outer retinal layers in eyes
suffering from DR. Optical Coherence Tomography (OCT) is a noninvasive imaging
technique that provides a cross-sectional view of the retina and it has been
used in recent years to diagnose many eye diseases. As a result, in this paper
has attempted to identify areas with MA from normal areas of the retina using
OCT images. This work is done using the dataset collected from FA and OCT
images of 20 patients with DR. In this regard, firstly Fluorescein Angiography
(FA) and OCT images were registered. Then the MA and normal areas were
separated and the features of each of these areas were extracted using the Bag
of Features (BOF) approach with Speeded-Up Robust Feature (SURF) descriptor.
Finally, the classification process was performed using a multilayer perceptron
network. For each of the criteria of accuracy, sensitivity, specificity, and
precision, the obtained results were 96.33%, 97.33%, 95.4%, and 95.28%,
respectively. Utilizing OCT images to detect MAsautomatically is a new idea and
the results obtained as preliminary research in this field are promising .",-0.09500329,-0.37082914,0.1190801,C
5996,"However, further research is needed since adversarial training is under-explored in medical imaging inter-
pretability.","The experiments on the dermatology dataset
HAM10000 [132] showed that saliency maps on the robust model are significantly sharper and visually coherent
than those obtained from the standard trained model.","Specifically, applying the above-referred findings to other datasets and network architectures becomes
necessary to perceive the generalization capability of the methods.",2022-05-10 09:28:14+00:00,Explainable Deep Learning Methods in Medical Diagnosis: A Survey,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Cristiano Patrício'), arxiv.Result.Author('João C. Neves'), arxiv.Result.Author('Luís F. Teixeira')]","The remarkable success of deep learning has prompted interest in its
application to medical diagnosis. Even tough state-of-the-art deep learning
models have achieved human-level accuracy on the classification of different
types of medical data, these models are hardly adopted in clinical workflows,
mainly due to their lack of interpretability. The black-box-ness of deep
learning models has raised the need for devising strategies to explain the
decision process of these models, leading to the creation of the topic of
eXplainable Artificial Intelligence (XAI). In this context, we provide a
thorough survey of XAI applied to medical diagnosis, including visual, textual,
and example-based explanation methods. Moreover, this work reviews the existing
medical imaging datasets and the existing metrics for evaluating the quality of
the explanations . Complementary to most existing surveys, we include a
performance comparison among a set of report generation-based methods. Finally,
the major challenges in applying XAI to medical imaging are also discussed.",-0.20938006,0.36591643,0.044106193,A
5997,"However, further research is needed since adversarial training is under-explored in medical imaging inter-
pretability.","The experiments
on the dermatology dataset HAM10000 [133] showed that saliency maps of the robust model are significantly
sharper and visually more coherent than those obtained from the standard trained model.","Specifically, applying the above-referred findings to other datasets and network architectures becomes
necessary to perceive the generalization capability of the methods.",2022-05-10 09:28:14+00:00,Explainable Deep Learning Methods in Medical Imaging Diagnosis: A Survey,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Cristiano Patrício'), arxiv.Result.Author('João C. Neves'), arxiv.Result.Author('Luís F. Teixeira')]","The remarkable success of deep learning has prompted interest in its
application to medical imaging diagnosis. Even though state-of-the-art deep
learning models have achieved human-level accuracy on the classification of
different types of medical data, these models are hardly adopted in clinical
workflows, mainly due to their lack of interpretability. The black-box-ness of
deep learning models has raised the need for devising strategies to explain the
decision process of these models, leading to the creation of the topic of
eXplainable Artificial Intelligence (XAI). In this context, we provide a
thorough survey of XAI applied to medical imaging diagnosis, including visual,
textual, example-based and concept-based explanation methods. Moreover, this
work reviews the existing medical imaging datasets and the existing metrics for
evaluating the quality of the explanations. In addition, we include a
performance comparison among a set of report generation-based methods. Finally,
the major challenges in applying XAI to medical imaging and the future research
directions on the topic are also discussed.",-0.22014606,0.37437624,0.03389474,A
6147,"These val-   further researched whether the ﬂuctuating detection perfor-
ues show the same tendency as in the intra coding case listed  mance depending on the coding order is negatively affecting
in Table 2 and extensively discussed in [7].","It has to be
signiﬁcantly lower compared to the other metrics.",practical applications.,2022-05-13 08:54:40+00:00,Evaluation of Video Coding for Machines without Ground Truth,eess.IV,"['eess.IV', 'I.4.2']","[arxiv.Result.Author('Kristian Fischer'), arxiv.Result.Author('Markus Hofbauer'), arxiv.Result.Author('Christopher Kuhn'), arxiv.Result.Author('Eckehard Steinbach'), arxiv.Result.Author('André Kaup')]","In the emerging field of video coding for machines, video datasets with
pristine video quality and high-quality annotations are required for a
comprehensive evaluation. However, existing video datasets with detailed
annotations are severely limited in size and video quality. Thus, current
methods have to either evaluate their codecs on still images or on already
compressed data. To mitigate this problem, we propose an evaluation method
based on pseudo ground-truth data from the field of semantic segmentation to
the evaluation of video coding for machines. Through extensive evaluation, this
paper shows that the proposed ground-truth-agnostic evaluation method results
in an acceptable absolute measurement error below 0.7 percentage points on the
Bjontegaard Delta Rate compared to using the true ground truth for mid-range
bitrates. We evaluate on the three tasks of semantic segmentation, instance
segmentation, and object detection. Lastly, we utilize the
ground-truth-agnostic method to measure the coding performances of the VVC
compared against HEVC on the Cityscapes sequences. This reveals that the coding
position has a significant influence on the task performance.",0.111488774,-0.30768996,-0.14006117,B
6306,"Doctors’ segmentation results with the assistance of BPSegSys

After all the experimentation discussed above, we further study the role of BPSegSys in assisting doctors in
brachial plexus identiﬁcation.","YGY dataset          IoU       Doctor B  Doctor C            BPSegSys  Variance      BPSegSys
BK300 dataset
Mixed-dataset        Doctor A  0.4641    0.5205              0.5238    Doctor        0.02806
                               0.3929    0.4704              0.4715                  0.02764
                     0.4288    0.4323    0.4979              0.5029    0.04623       0.02361
                     0.4195                                            0.03939
                     0.4242                                            0.04042

2.4.","We selecte Doctor A’s folds with lower scores in the YGY dataset and BK3000
dataset and then aske Doctor A to relabel the data considering the BPSegSys segmentation results.",2022-05-17 07:23:28+00:00,Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Binbin Zhu'), arxiv.Result.Author('Lingsi Kong'), arxiv.Result.Author('Jianlin Wang'), arxiv.Result.Author('Bin Gao'), arxiv.Result.Author('Jianhua Wang'), arxiv.Result.Author('Dingcheng Tian'), arxiv.Result.Author('Yudong Yao')]","Ultrasound-guided nerve block anesthesia (UGNB) is a high-tech visual nerve
block anesthesia method that can observe the target nerve and its surrounding
structures, the puncture needle's advancement, and local anesthetics spread in
real-time. The key in UGNB is nerve identification. With the help of deep
learning methods, the automatic identification or segmentation of nerves can be
realized, assisting doctors in completing nerve block anesthesia accurately and
efficiently. Here, we establish a public dataset containing 320 ultrasound
images of brachial plexus (BP). Three experienced doctors jointly produce the
BP segmentation ground truth and label brachial plexus trunks. We design a
brachial plexus segmentation system (BPSegSys) based on deep learning. BPSegSys
achieves experienced-doctor-level nerve identification performance in various
experiments. We evaluate BPSegSys' performance in terms of
intersection-over-union (IoU), a commonly used performance measure for
segmentation experiments. Considering three dataset groups in our established
public dataset, the IoU of BPSegSys are 0.5238, 0.4715, and 0.5029,
respectively, which exceed the IoU 0.5205, 0.4704, and 0.4979 of experienced
doctors. In addition, we show that BPSegSys can help doctors identify brachial
plexus trunks more accurately, with IoU improvement up to 27%, which has
significant clinical application value.",-0.2561263,-0.27095732,-0.14002627,C
6375,"perspective to study images with higher accuracy and is         Substitute I(u, v) in Equation14 with Equation 16, we

worth further research.",(j) and (l) are corresponding residues of (i) and (k).,"have,

APPENDIX A                                                        I˜ = Imn sinc(u − m)sinc(v − m)·
CORRECTION OF THE IMAGE SENSOR’S SAMPLING
EFFECT                                                                   mn

The discrete image I˜ij sampled by an image sensor is the             H(i − u)H(j − v)dudv
convolution of continuous image I(u, v) with 2D rectangu-
lar function H(u)H(v) , which is described by                       = Imn sinc(u − m) H(i − u)du·

                                                                         mn

                                                                          sinc(v − m) H(j − v)dv

     I˜ij = I(u, v)H(i − u)H(j − v)dudv,   (14)                   = RimRjnImn

                                                                      mn

where H(x) is the rectangular function                            =                     RimImn Rnj ,

                                                                       n             m

                         1, 0 ≤ |x|< 1/2.",2022-05-18 15:57:11+00:00,Field Distortion Model Based on Fredholm Integral,eess.IV,"['eess.IV', 'astro-ph.IM']","[arxiv.Result.Author('Yunqi Sun'), arxiv.Result.Author('Jianfeng Zhou')]","Field distortion is widespread in imaging systems. If it cannot be measured
and corrected well, it will affect the accuracy of photogrammetry. To this end,
we proposed a general field distortion model based on Fredholm integration,
which uses a reconstructed high-resolution reference point spread function
(PSF) and two sets of 4-variable polynomials to describe an imaging system. The
model includes the point-to-point positional distortion from the object space
to the image space and the deformation of the PSF so that we can measure an
actual field distortion with arbitrary accuracy. We also derived the formula
required for correcting the sampling effect of the image sensor. Through
numerical simulation, we verify the effectiveness of the model and
reconstruction algorithm. This model will have potential application in
high-precision image calibration, photogrammetry and astrometry.",0.33093107,-0.037332,0.3109547,B
6440,"The presented method shows great potential
                                                to facilitate further research in the domain of artiﬁcial medical image generation.","Obtained results demonstrate the suitability of the
                                                proposed adversarial approach for the accurate generation of high-quality CT images.","Index Terms

                                                     conditional generative adversarial networks, CT, deep learning, generative adversarial networks, medical image generation,
                                                unsupervised deep learning

                                                                                                            I.",2022-05-19 20:29:25+00:00,Generation of Artificial CT Images using Patch-based Conditional Generative Adversarial Networks,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Marija Habijan'), arxiv.Result.Author('Irena Galic')]","Deep learning has a great potential to alleviate diagnosis and prognosis for
various clinical procedures. However, the lack of a sufficient number of
medical images is the most common obstacle in conducting image-based analysis
using deep learning. Due to the annotations scarcity, semi-supervised
techniques in the automatic medical analysis are getting high attention.
Artificial data augmentation and generation techniques such as generative
adversarial networks (GANs) may help overcome this obstacle. In this work, we
present an image generation approach that uses generative adversarial networks
with a conditional discriminator where segmentation masks are used as
conditions for image generation. We validate the feasibility of GAN-enhanced
medical image generation on whole heart computed tomography (CT) images and its
seven substructures, namely: left ventricle, right ventricle, left atrium,
right atrium, myocardium, pulmonary arteries, and aorta. Obtained results
demonstrate the suitability of the proposed adversarial approach for the
accurate generation of high-quality CT images. The presented method shows great
potential to facilitate further research in the domain of artificial medical
image generation.",-0.12834619,0.3942645,0.22448987,A
6441,"The two main limitations of this work, low generability since we only used one dataset and modality and lack of appropriate
quantitative evaluation, are good starting points for further research.","Obtained results demonstrate the suitability of
the proposed adversarial approach for the accurate generation of high-quality CT images.","Therefore, future work on this topic includes the use
of a number of datasets of different organs of the human body as well as the use of different imaging modalities to prove
the generality of the proposed approach.",2022-05-19 20:29:25+00:00,Generation of Artificial CT Images using Patch-based Conditional Generative Adversarial Networks,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Marija Habijan'), arxiv.Result.Author('Irena Galic')]","Deep learning has a great potential to alleviate diagnosis and prognosis for
various clinical procedures. However, the lack of a sufficient number of
medical images is the most common obstacle in conducting image-based analysis
using deep learning. Due to the annotations scarcity, semi-supervised
techniques in the automatic medical analysis are getting high attention.
Artificial data augmentation and generation techniques such as generative
adversarial networks (GANs) may help overcome this obstacle. In this work, we
present an image generation approach that uses generative adversarial networks
with a conditional discriminator where segmentation masks are used as
conditions for image generation. We validate the feasibility of GAN-enhanced
medical image generation on whole heart computed tomography (CT) images and its
seven substructures, namely: left ventricle, right ventricle, left atrium,
right atrium, myocardium, pulmonary arteries, and aorta. Obtained results
demonstrate the suitability of the proposed adversarial approach for the
accurate generation of high-quality CT images. The presented method shows great
potential to facilitate further research in the domain of artificial medical
image generation.",-0.10369578,0.40300965,0.2557417,A
6442,"Moreover, the proposed approach could alleviate further research by incorporating cross-modality images, such as mapping
from CT to PET data.","Some other cases, such as tumors, may
be more challenging to analyze even by an expert, and artiﬁcially created images may be too unsafe for algorithms training.","This may reduce the risk factor, and the cost of medical image acquisition which will provide better
diagnostic decision-making.",2022-05-19 20:29:25+00:00,Generation of Artificial CT Images using Patch-based Conditional Generative Adversarial Networks,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Marija Habijan'), arxiv.Result.Author('Irena Galic')]","Deep learning has a great potential to alleviate diagnosis and prognosis for
various clinical procedures. However, the lack of a sufficient number of
medical images is the most common obstacle in conducting image-based analysis
using deep learning. Due to the annotations scarcity, semi-supervised
techniques in the automatic medical analysis are getting high attention.
Artificial data augmentation and generation techniques such as generative
adversarial networks (GANs) may help overcome this obstacle. In this work, we
present an image generation approach that uses generative adversarial networks
with a conditional discriminator where segmentation masks are used as
conditions for image generation. We validate the feasibility of GAN-enhanced
medical image generation on whole heart computed tomography (CT) images and its
seven substructures, namely: left ventricle, right ventricle, left atrium,
right atrium, myocardium, pulmonary arteries, and aorta. Obtained results
demonstrate the suitability of the proposed adversarial approach for the
accurate generation of high-quality CT images. The presented method shows great
potential to facilitate further research in the domain of artificial medical
image generation.",-0.24877816,0.1424665,0.25279474,A
6547,Both issues represent interesting ﬁelds for further research.,"For this to be the case, new architectural designs need to be explored.",Acknowledgments.,2022-05-22 17:52:02+00:00,Improving AMD diagnosis by the simultaneous identification of associated retinal lesions,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('José Morano'), arxiv.Result.Author('Álvaro S. Hervella'), arxiv.Result.Author('José Rouco'), arxiv.Result.Author('Jorge Novo'), arxiv.Result.Author('José I. Fernández-Vigo'), arxiv.Result.Author('Marcos Ortega')]","Age-related Macular Degeneration (AMD) is the predominant cause of blindness
in developed countries, specially in elderly people. Moreover, its prevalence
is increasing due to the global population ageing. In this scenario, early
detection is crucial to avert later vision impairment. Nonetheless,
implementing large-scale screening programmes is usually not viable, since the
population at-risk is large and the analysis must be performed by expert
clinicians. Also, the diagnosis of AMD is considered to be particularly
difficult, as it is characterized by many different lesions that, in many
cases, resemble those of other macular diseases. To overcome these issues,
several works have proposed automatic methods for the detection of AMD in
retinography images, the most widely used modality for the screening of the
disease. Nowadays, most of these works use Convolutional Neural Networks (CNNs)
for the binary classification of images into AMD and non-AMD classes. In this
work, we propose a novel approach based on CNNs that simultaneously performs
AMD diagnosis and the classification of its potential lesions. This latter
secondary task has not yet been addressed in this domain, and provides
complementary useful information that improves the diagnosis performance and
helps understanding the decision. A CNN model is trained using retinography
images with image-level labels for both AMD and lesion presence, which are
relatively easy to obtain. The experiments conducted in several public datasets
show that the proposed approach improves the detection of AMD, while achieving
satisfactory results in the identification of most lesions.",0.16726379,-0.25219187,-0.18292807,B
6643,"However, to establish the clinical relevance, further research
relating the adjusted Agatston scores with CHD events is warranted.","The results showed an improved agreement for the proposed method, compared
to automatic clinical calcium scoring.","Future research on evaluation of the proposed method in non-ECG-synchronized against cal-
cium scoring in dedicated calcium scoring CT is needed.",2022-05-24 10:59:32+00:00,Generative Models for Reproducible Coronary Calcium Scoring,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Sanne G. M. van Velzen'), arxiv.Result.Author('Bob D. de Vos'), arxiv.Result.Author('Julia M. H. Noothout'), arxiv.Result.Author('Helena M. Verkooijen'), arxiv.Result.Author('Max A. Viergever'), arxiv.Result.Author('Ivana Išgum')]","Purpose: Coronary artery calcium (CAC) score, i.e. the amount of CAC
quantified in CT, is a strong and independent predictor of coronary heart
disease (CHD) events. However, CAC scoring suffers from limited interscan
reproducibility, which is mainly due to the clinical definition requiring
application of a fixed intensity level threshold for segmentation of
calcifications. This limitation is especially pronounced in
non-ECG-synchronized CT where lesions are more impacted by cardiac motion and
partial volume effects. Therefore, we propose a CAC quantification method that
does not require a threshold for segmentation of CAC. Approach: Our method
utilizes a generative adversarial network where a CT with CAC is decomposed
into an image without CAC and an image showing only CAC. The method, using a
CycleGAN, was trained using 626 low-dose chest CTs and 514 radiotherapy
treatment planning CTs. Interscan reproducibility was compared to clinical
calcium scoring in radiotherapy treatment planning CTs of 1,662 patients, each
having two scans. Results: A lower relative interscan difference in CAC mass
was achieved by the proposed method: 47% compared to 89% manual clinical
calcium scoring. The intraclass correlation coefficient of Agatston scores was
0.96 for the proposed method compared to 0.91 for automatic clinical calcium
scoring. Conclusions: The increased interscan reproducibility achieved by our
method may lead to increased reliability of CHD risk categorization and
improved accuracy of CHD event prediction.",-0.19396402,-0.33104545,0.04420791,C
6998,"This method can be used to obtain good quality DWI in a shortened examination time, but
further research is necessary to improve denoising results and investigate the clinical utility of the denoised real MRI
images and ADC maps.","We showed that denoised images are significantly closer to a clean reference
than averaged images.","References

[Batson2019] Batson J., Royer L., Noise2Self: Blind Denoising by Self-Supervision, Proceedings of Machine Learning
Research, vol.",2022-06-01 08:14:35+00:00,Supervised Denoising of Diffusion-Weighted Magnetic Resonance Images Using a Convolutional Neural Network and Transfer Learning,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Jakub Jurek'), arxiv.Result.Author('Andrzej Materka'), arxiv.Result.Author('Kamil Ludwisiak'), arxiv.Result.Author('Agata Majos'), arxiv.Result.Author('Kamil Gorczewski'), arxiv.Result.Author('Kamil Cepuch'), arxiv.Result.Author('Agata Zawadzka')]","In this paper, we propose a method for denoising diffusion-weighted images
(DWI) of the brain using a convolutional neural network trained on realistic,
synthetic MR data. We compare our results to averaging of repeated scans, a
widespread method used in clinics to improve signal-to-noise ratio of MR
images. To obtain training data for transfer learning, we model, in a
data-driven fashion, the effects of echo-planar imaging (EPI): Nyquist ghosting
and ramp sampling. We introduce these effects to the digital phantom of brain
anatomy (BrainWeb). Instead of simulating pseudo-random noise with a defined
probability distribution, we perform noise scans with a brain-DWI-designed
protocol to obtain realistic noise maps. We combine them with the simulated,
noise-free EPI images. We also measure the Point Spread Function in a DW image
of an AJR-approved geometrical phantom and inter-scan movement in a brain scan
of a healthy volunteer. Their influence on image denoising and averaging of
repeated images is investigated at different signal-to-noise ratio levels.
Denoising performance is evaluated quantitatively using the simulated EPI
images and qualitatively in real EPI DWI of the brain. We show that the
application of our method allows for a significant reduction in scan time by
lowering the number of repeated scans. Visual comparisons made in the acquired
brain images indicate that the denoised single-repetition images are less noisy
than multi-repetition averaged images. We also analyse the convolutional neural
network denoiser and point out the challenges accompanying this denoising
method.",-0.07162835,-0.00796706,0.34576717,C
7426,"De Cock, I. Katsavounidis,
   A direction of further research that is worthy of exploration
is to apply RCN-Hull on encoding parameter spaces having          and A. Aaron.","[3] M. Manohara, A. Moorthy, J.",(Mar.,2022-06-10 05:11:02+00:00,Efficient Per-Shot Convex Hull Prediction By Recurrent Learning,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Somdyuti Paul'), arxiv.Result.Author('Andrey Norkin'), arxiv.Result.Author('Alan C. Bovik')]","Adaptive video streaming relies on the construction of efficient bitrate
ladders to deliver the best possible visual quality to viewers under bandwidth
constraints. The traditional method of content dependent bitrate ladder
selection requires a video shot to be pre-encoded with multiple encoding
parameters to find the optimal operating points given by the convex hull of the
resulting rate-quality curves. However, this pre-encoding step is equivalent to
an exhaustive search process over the space of possible encoding parameters,
which causes significant overhead in terms of both computation and time
expenditure. To reduce this overhead, we propose a deep learning based method
of content aware convex hull prediction. We employ a recurrent convolutional
network (RCN) to implicitly analyze the spatiotemporal complexity of video
shots in order to predict their convex hulls. A two-step transfer learning
scheme is adopted to train our proposed RCN-Hull model, which ensures
sufficient content diversity to analyze scene complexity, while also making it
possible capture the scene statistics of pristine source videos. Our
experimental results reveal that our proposed model yields better
approximations of the optimal convex hulls, and offers competitive time savings
as compared to existing approaches. On average, the pre-encoding time was
reduced by 58.0% by our method, while the average Bjontegaard delta bitrate
(BD-rate) of the predicted convex hulls against ground truth was 0.08%, while
the mean absolute deviation of the BD-rate distribution was 0.44%",0.2224167,-0.005514132,-0.16020226,B
7434,"The proposed approach encourages further research
8 Conclusion                                                directions on both algorithmic and computing archi-
                                                            tecture levels.",43× to 77× in the case of 9 × 9 inputs.,"In the ﬁrst direction, we would extend
This paper presents a GPU-accelerated computational         the SR model to handle a more challenging noise set-
framework for reconstructing high-resolution SAI from       ting, i.e., photon noise, which follows the Poisson dis-
4D LF data under mixed Gaussian-Impulse noise con-          tribution.",2022-06-09 05:23:05+00:00,A GPU-Accelerated Light-field Super-resolution Framework Based on Mixed Noise Model and Weighted Regularization,eess.IV,"['eess.IV', 'cs.CV', 'cs.PF']","[arxiv.Result.Author('Trung-Hieu Tran'), arxiv.Result.Author('Kaicong Sun'), arxiv.Result.Author('Sven Simon')]","This paper presents a GPU-accelerated computational framework for
reconstructing high resolution (HR) LF images under a mixed Gaussian-Impulse
noise condition. The main focus is on developing a high-performance approach
considering processing speed and reconstruction quality. From a statistical
perspective, we derive a joint $\ell^1$-$\ell^2$ data fidelity term for
penalizing the HR reconstruction error taking into account the mixed noise
situation. For regularization, we employ the weighted non-local total variation
approach, which allows us to effectively realize LF image prior through a
proper weighting scheme. We show that the alternating direction method of
multipliers algorithm (ADMM) can be used to simplify the computation complexity
and results in a high-performance parallel computation on the GPU Platform. An
extensive experiment is conducted on both synthetic 4D LF dataset and natural
image dataset to validate the proposed SR model's robustness and evaluate the
accelerated optimizer's performance. The experimental results show that our
approach achieves better reconstruction quality under severe mixed-noise
conditions as compared to the state-of-the-art approaches. In addition, the
proposed approach overcomes the limitation of the previous work in handling
large-scale SR tasks. While fitting within a single off-the-shelf GPU, the
proposed accelerator provides an average speedup of 2.46$\times$ and
1.57$\times$ for $\times 2$ and $\times 3$ SR tasks, respectively. In addition,
a speedup of $77\times$ is achieved as compared to CPU execution.",0.33681327,0.023216074,0.131543,B
7707,"Moreover, the
                                                                    contributions of the detected ROIs varied across different
The interpretability of the model is crucial for clinical pre-      time points and are also worthy of further study.","Furthermore, the precuneus, which is
                                                                    associated with a high level of cognitive function [37], was
5.3.3 Interpretability of the Proposed Method                       explored by PET data at most time points.","diction and can help us discover some potential information
associated with AD.",2022-06-16 09:20:41+00:00,Multi-View Imputation and Cross-Attention Network Based on Incomplete Longitudinal and Multi-Modal Data for Alzheimer's Disease Prediction,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Meiyan Huang'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Xiumei Chen'), arxiv.Result.Author('Xiaoling Zhang'), arxiv.Result.Author('Shuoling Zhou'), arxiv.Result.Author('Qianjin Feng')]","Longitudinal variations and complementary information inherent in
longitudinal and multi-modal data play an important role in Alzheimer's disease
(AD) prediction, particularly in identifying subjects with mild cognitive
impairment who are about to have AD. However, longitudinal and multi-modal data
may have missing data, which hinders the effective application of these data.
Additionally, previous longitudinal studies require existing longitudinal data
to achieve prediction, but AD prediction is expected to be conducted at
patients' baseline visit (BL) in clinical practice. Thus, we proposed a
multi-view imputation and cross-attention network (MCNet) to integrate data
imputation and AD prediction in a unified framework and achieve accurate AD
prediction. First, a multi-view imputation method combined with adversarial
learning, which can handle a wide range of missing data situations and reduce
imputation errors, was presented. Second, two cross-attention blocks were
introduced to exploit the potential associations in longitudinal and
multi-modal data. Finally, a multi-task learning model was built for data
imputation, longitudinal classification, and AD prediction tasks. When the
model was properly trained, the disease progression information learned from
longitudinal data can be leveraged by BL data to improve AD prediction. The
proposed method was tested on two independent testing sets and single-model
data at BL to verify its effectiveness and flexibility on AD prediction.
Results showed that MCNet outperformed several state-of-the-art methods.
Moreover, the interpretability of MCNet was presented. Thus, our MCNet is a
tool with a great application potential in longitudinal and multi-modal data
analysis for AD prediction. Codes are available at
https://github.com/Meiyan88/MCNET.",-0.27345237,-0.22101708,-0.09569415,C
7720,"We also use several                      [18] F. Chollet, “Xception: Deep learning with depthwise separable convolu-
SOTA baselines and benchmark them on Kvasir-Capsule to                                tions,” in Proceedings of the IEEE conference on computer vision and
streamline further research in this area.","Thus, reducing the
effort required for manual inspection.","In the future, we plan                      pattern recognition, 2017, pp.",2022-06-16 16:57:45+00:00,Video Capsule Endoscopy Classification using Focal Modulation Guided Convolutional Neural Network,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Abhishek Srivastava'), arxiv.Result.Author('Nikhil Kumar Tomar'), arxiv.Result.Author('Ulas Bagci'), arxiv.Result.Author('Debesh Jha')]","Video capsule endoscopy is a hot topic in computer vision and medicine. Deep
learning can have a positive impact on the future of video capsule endoscopy
technology. It can improve the anomaly detection rate, reduce physicians' time
for screening, and aid in real-world clinical analysis. CADx classification
system for video capsule endoscopy has shown a great promise for further
improvement. For example, detection of cancerous polyp and bleeding can lead to
swift medical response and improve the survival rate of the patients. To this
end, an automated CADx system must have high throughput and decent accuracy. In
this paper, we propose FocalConvNet, a focal modulation network integrated with
lightweight convolutional layers for the classification of small bowel
anatomical landmarks and luminal findings. FocalConvNet leverages focal
modulation to attain global context and allows global-local spatial
interactions throughout the forward pass. Moreover, the convolutional block
with its intrinsic inductive/learning bias and capacity to extract hierarchical
features allows our FocalConvNet to achieve favourable results with high
throughput. We compare our FocalConvNet with other SOTA on Kvasir-Capsule, a
large-scale VCE dataset with 44,228 frames with 13 classes of different
anomalies. Our proposed method achieves the weighted F1-score, recall and MCC}
of 0.6734, 0.6373 and 0.2974, respectively outperforming other SOTA
methodologies. Furthermore, we report the highest throughput of 148.02
images/second rate to establish the potential of FocalConvNet in a real-time
clinical environment. The code of the proposed FocalConvNet is available at
https://github.com/NoviceMAn-prog/FocalConvNet.",0.09217654,0.3645662,-0.044566248,A
7906,"Possible further research in this topic could be                [5] “LCD - Cardiac Computed Tomography and Angiography (CCTA)
                                                                        (L33423).” Medicare-Coverage-Database, www.cms.gov/medicare-
   • Low-cost, efﬁcient, and real time computational platform           coverage-database/view/lcd.aspx?LCDId=33423.","Prevention, 7 Feb. 2022, www.cdc.gov/heartdisease/facts.htm.","Accessed 12 June
      for immediate Agaston Score and bucketing score of level          2022.
      of calciﬁcation in the arteries of a patient when scan is
      clicked.",2022-06-13 20:02:02+00:00,Automated Coronary Calcium Scoring using U-Net Models through Semi-supervised Learning on Non-Gated CT Scans,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']",[arxiv.Result.Author('Sanskriti Singh')],"Every year, thousands of innocent people die due to heart attacks. Often
undiagnosed heart attacks can hit people by surprise since many current medical
plans don't cover the costs to require the searching of calcification on these
scans. Only if someone is suspected to have a heart problem, a gated CT scan is
taken, otherwise, there's no way for the patient to be aware of a possible
heart attack/disease. While nongated CT scans are more periodically taken, it
is harder to detect calcification and is usually taken for a purpose other than
locating calcification in arteries. In fact, in real time coronary artery
calcification scores are only calculated on gated CT scans, not nongated CT
scans. After training a unet model on the Coronary Calcium and chest CT's gated
scans, it received a DICE coefficient of 0.95 on its untouched test set. This
model was used to predict on nongated CT scans, performing with a mean absolute
error (MAE) of 674.19 and bucket classification accuracy of 41% (5 classes).
Through the analysis of the images and the information stored in the images,
mathematical equations were derived and used to automatically crop the images
around the location of the heart. By performing semi-supervised learning the
new cropped nongated scans were able to closely resemble gated CT scans,
improving the performance by 91% in MAE (62.38) and 23% in accuracy.",-0.19639039,-0.30439782,0.16253445,C
8007,"has inspired further research into       tion strategy and modulate the Lagrange multiplier for 1) the
                                          content-adaptive Transcoding.","In recent years, the growth in delivery of video at scale for ap-
                                          plications like Streaming & Broadcast of content (from Net-             Our contribution is thus to adjust our per-clip optimisa-
                                          ﬂix, YouTube, Disney etc.)","The perennial goal of this area       different frame types in HEVC and AV1 and 2) the image
                                          is to deliver high-quality content at increasingly lower bitrates   partitioning process of HEVC and AV1 (see Section 3).",2022-06-23 21:02:39+00:00,Frame-type Sensitive RDO Control for Content-Adaptive-encoding,eess.IV,"['eess.IV', 'eess.SP']","[arxiv.Result.Author('Vibhoothi'), arxiv.Result.Author('François Pitié'), arxiv.Result.Author('Anil Kokaram')]","Video transcoding is an increasingly important application in the streaming
media industry. It has become important to investigate the optimisation of
transcoder parameters for a single clip simply because of the immense number of
playbacks for popular clips. In this paper, we explore the use of a canned
optimiser to estimate the optimal RD tradeoff achievable for a particular clip.
We show that by adjusting the Lagrange multiplier in RD optimisation on
keyframes alone we can achieve more than 10$\times$ the previous BD-Rate gains
possible without affecting quality for any operating point.",0.6067172,0.00020245463,-0.057953484,B_centroid
8008,"has inspired further research into           Our contribution is thus to adjust our per-clip optimisa-
                                          content-adaptive Transcoding.","plications like Streaming & Broadcast of content (from Net-
                                          ﬂix, YouTube, Disney etc.)","The perennial goal of this area       tion strategy and modulate the Lagrange multiplier for 1) the
                                          is to deliver high-quality content at increasingly lower bitrates   different frame types in HEVC and AV1 and 2) the image
                                          by adapting the transcoder to the content presented, at a more      partitioning process of HEVC and AV1 (see Section 3).",2022-06-23 21:02:39+00:00,Frame-type Sensitive RDO Control for Content-Adaptive-encoding,eess.IV,"['eess.IV', 'eess.SP']","[arxiv.Result.Author('Vibhoothi'), arxiv.Result.Author('François Pitié'), arxiv.Result.Author('Anil Kokaram')]","Video transcoding is an increasingly important application in the streaming
media industry. It has become important to investigate the optimisation of
transcoder parameters for a single clip simply because of the immense number of
playbacks for popular clips. In this paper, we explore the use of a canned
optimiser to estimate the optimal RD tradeoff achievable for a particular clip.
We show that by adjusting the Lagrange multiplier in RD optimisation on
keyframes alone we can achieve more than 10$\times$ the previous BD-Rate gains
possible without affecting quality for any operating point.",0.5935744,0.024451265,-0.06841485,B
8043,"In our view, this pre-trained model is a good starting point for further research in
algorithm development.","We attribute this to the combination of the two mammographic views from both
breasts.","We also analyzed the effect of an additional pre-processing of the data on both the
performance and the fine-tuning process.",2022-06-22 15:56:17+00:00,Independent evaluation of state-of-the-art deep networks for mammography,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG', 'physics.med-ph']","[arxiv.Result.Author('Osvaldo Matias Velarde'), arxiv.Result.Author('Lucas Parra')]","Deep neural models have shown remarkable performance in image recognition
tasks, whenever large datasets of labeled images are available. The largest
datasets in radiology are available for screening mammography. Recent reports,
including in high impact journals, document performance of deep models at or
above that of trained radiologists. What is not yet known is whether
performance of these trained models is robust and replicates across datasets.
Here we evaluate performance of five published state-of-the-art models on four
publicly available mammography datasets. The limited size of public datasets
precludes retraining the model and so we are limited to evaluate those models
that have been made available with pre-trained parameters. Where test data was
available, we replicated published results. However, the trained models
performed poorly on out-of-sample data, except when based on all four standard
views of a mammographic exam. We conclude that future progress will depend on a
concerted effort to make more diverse and larger mammography datasets publicly
available. Meanwhile, results that are not accompanied by a release of trained
models for independent validation should be judged cautiously.",-0.19705388,0.1520962,0.11665593,A
8251,"We encourage further research based upon our frame-
work to enhance the diagnosis options in clinical use cases.","Altogether, we presented a framework for severity prediction as well as infection
detection and achieved good performance by applying this framework to the
ConvNeXt architecture.","References

 1.",2022-06-30 07:09:28+00:00,COVID Detection and Severity Prediction with 3D-ConvNeXt and Custom Pretrainings,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Daniel Kienzle'), arxiv.Result.Author('Julian Lorenz'), arxiv.Result.Author('Robin Schön'), arxiv.Result.Author('Katja Ludwig'), arxiv.Result.Author('Rainer Lienhart')]","Since COVID strongly affects the respiratory system, lung CT-scans can be
used for the analysis of a patients health. We introduce a neural network for
the prediction of the severity of lung damage and the detection of a
COVID-infection using three-dimensional CT-data. Therefore, we adapt the recent
ConvNeXt model to process three-dimensional data. Furthermore, we design and
analyze different pretraining methods specifically designed to improve the
models ability to handle three-dimensional CT-data. We rank 2nd in the 1st
COVID19 Severity Detection Challenge and 3rd in the 2nd COVID19 Detection
Challenge.",-0.25352448,0.15868853,-0.15559772,A
8315,"A noticeable time
   We further study the importance of estimation precision of      difference between the proposed and MCP Search is that our
the ray displacement by investigating following two extra          implementation of interpolation filtering shown in Eq.","(21)); thus, the proposed methods also
                                                                   inherit the low complexity of MCP Search.","(23) is
cases: integer-precision, fixed fractional part, 𝛼 = 𝛽 = 0 ;       not yet well-optimized as HEVC reference software [73] which
half-precision, 𝛼, 𝛽 ∈ {0; 2/4 } in additional to the quarter-     adopts the SIMD vectorization [78].",2022-07-01 16:09:19+00:00,Ray-Space Motion Compensation for Lenslet Plenoptic Video Coding,eess.IV,"['eess.IV', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Thuc Nguyen Huu'), arxiv.Result.Author('Vinh Van Duong'), arxiv.Result.Author('Jonghoon Yim'), arxiv.Result.Author('Byeungwoo Jeon')]","Plenoptic images and videos bearing rich information demand a tremendous
amount of data storage and high transmission cost. While there has been much
study on plenoptic image coding, investigations into plenoptic video coding
have been very limited. We investigate the motion compensation for plenoptic
video coding from a slightly different perspective by looking at the problem in
the ray-space domain instead of in the conventional pixel domain. Here, we
develop a novel motion compensation scheme for lenslet video under two
sub-cases of ray-space motion, that is, integer ray-space motion and fractional
ray-space motion. The proposed new scheme of light field motion-compensated
prediction is designed such that it can be easily integrated into well-known
video coding techniques such as HEVC. Experimental results compared to relevant
existing methods have shown remarkable compression efficiency with an average
gain of 19.63% and a peak gain of 29.1%.",0.3959536,-0.15735754,0.17272612,B
8331,"Moreover,                                       work
we demonstrate the characteristics of epistemic and aleatoric
uncertainties provided by the proposed framework to motivate        generated from the approximation of the posterior distribution
further research on leveraging the uncertainty information for      of the target image.","[25]           Deep Unrolling           Epistemic
method and provide epistemic and aleatoric uncertainty infor-       Proposed Framework                U-Net [48]     Epistemic & Aleatoric
mation about the reconstructed image while incorporating the        ∗Preliminary version of this   Deep Unrolling
domain-knowledge into the reconstruction process.",image reconstruction and analysis tasks.,2022-07-02 00:22:49+00:00,Uncertainty Quantification for Deep Unrolling-Based Computational Imaging,eess.IV,"['eess.IV', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Canberk Ekmekci'), arxiv.Result.Author('Mujdat Cetin')]","Deep unrolling is an emerging deep learning-based image reconstruction
methodology that bridges the gap between model-based and purely deep
learning-based image reconstruction methods. Although deep unrolling methods
achieve state-of-the-art performance for imaging problems and allow the
incorporation of the observation model into the reconstruction process, they do
not provide any uncertainty information about the reconstructed image, which
severely limits their use in practice, especially for safety-critical imaging
applications. In this paper, we propose a learning-based image reconstruction
framework that incorporates the observation model into the reconstruction task
and that is capable of quantifying epistemic and aleatoric uncertainties, based
on deep unrolling and Bayesian neural networks. We demonstrate the uncertainty
characterization capability of the proposed framework on magnetic resonance
imaging and computed tomography reconstruction problems. We investigate the
characteristics of the epistemic and aleatoric uncertainty information provided
by the proposed framework to motivate future research on utilizing uncertainty
information to develop more accurate, robust, trustworthy, uncertainty-aware,
learning-based image reconstruction and analysis methods for imaging problems.
We show that the proposed framework can provide uncertainty information while
achieving comparable reconstruction performance to state-of-the-art deep
unrolling methods.",0.11264929,0.16072297,0.31160873,A
8332,"We hope that the proposed framework and the                         [9] H. K. Aggarwal, M. P. Mani, and M. Jacob, “MoDL: Model-based
provided discussion on epistemic and aleatoric uncertainties                         deep learning architecture for inverse problems,” IEEE Transactions on
for imaging problems motivate further research on uncertainty                        Medical Imaging, vol.","problems and can be easily implemented in deep learning
frameworks.","38, no.",2022-07-02 00:22:49+00:00,Uncertainty Quantification for Deep Unrolling-Based Computational Imaging,eess.IV,"['eess.IV', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Canberk Ekmekci'), arxiv.Result.Author('Mujdat Cetin')]","Deep unrolling is an emerging deep learning-based image reconstruction
methodology that bridges the gap between model-based and purely deep
learning-based image reconstruction methods. Although deep unrolling methods
achieve state-of-the-art performance for imaging problems and allow the
incorporation of the observation model into the reconstruction process, they do
not provide any uncertainty information about the reconstructed image, which
severely limits their use in practice, especially for safety-critical imaging
applications. In this paper, we propose a learning-based image reconstruction
framework that incorporates the observation model into the reconstruction task
and that is capable of quantifying epistemic and aleatoric uncertainties, based
on deep unrolling and Bayesian neural networks. We demonstrate the uncertainty
characterization capability of the proposed framework on magnetic resonance
imaging and computed tomography reconstruction problems. We investigate the
characteristics of the epistemic and aleatoric uncertainty information provided
by the proposed framework to motivate future research on utilizing uncertainty
information to develop more accurate, robust, trustworthy, uncertainty-aware,
learning-based image reconstruction and analysis methods for imaging problems.
We show that the proposed framework can provide uncertainty information while
achieving comparable reconstruction performance to state-of-the-art deep
unrolling methods.",-0.03486495,0.3129626,0.18116064,A
8333,"motivate further research on uncertainty characterization for
                                                                               imaging problems and on leveraging the uncertainty informa-
                          VI.","We hope
For the sake of brevity, we have not discussed this variant;                   that the proposed framework and the provided discussion on
however, a brief discussion on that variant is provided in the                 epistemic and aleatoric uncertainties for imaging problems
Supplementary Material.",CONCLUSION                                       tion for image reconstruction and analysis tasks.,2022-07-02 00:22:49+00:00,Uncertainty Quantification for Deep Unrolling-Based Computational Imaging,eess.IV,"['eess.IV', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Canberk Ekmekci'), arxiv.Result.Author('Mujdat Cetin')]","Deep unrolling is an emerging deep learning-based image reconstruction
methodology that bridges the gap between model-based and purely deep
learning-based image reconstruction methods. Although deep unrolling methods
achieve state-of-the-art performance for imaging problems and allow the
incorporation of the observation model into the reconstruction process, they do
not provide any uncertainty information about the reconstructed image, which
severely limits their use in practice, especially for safety-critical imaging
applications. In this paper, we propose a learning-based image reconstruction
framework that incorporates the observation model into the reconstruction task
and that is capable of quantifying epistemic and aleatoric uncertainties, based
on deep unrolling and Bayesian neural networks. We demonstrate the uncertainty
characterization capability of the proposed framework on magnetic resonance
imaging and computed tomography reconstruction problems. We investigate the
characteristics of the epistemic and aleatoric uncertainty information provided
by the proposed framework to motivate future research on utilizing uncertainty
information to develop more accurate, robust, trustworthy, uncertainty-aware,
learning-based image reconstruction and analysis methods for imaging problems.
We show that the proposed framework can provide uncertainty information while
achieving comparable reconstruction performance to state-of-the-art deep
unrolling methods.",0.081451595,-0.1041622,0.4367079,C
8492,"Nevertheless, further research is required
to improve the results, e.g., remove artifacts related to highly reﬂective scatterers, for instance, by including similar
structures in the tissue modeling, and improve training to create cleaner margins, for example, by using multi-scale
approaches.","AutoSpeed on the measured data predicted SoS values in range
1535 ± 6 m/s and 1561 ± 11 m/s in the background region and inside the inclusion area, respectively, where the
corresponding expected values are 1520 ± 10 m/s and 1580 ± 20 m/s.","Additionally, more research is required to transfer and prove the efﬁciency of such methods in clinical
setups.",2022-07-04 21:27:16+00:00,AutoSpeed: A Linked Autoencoder Approach for Pulse-Echo Speed-of-Sound Imaging for Medical Ultrasound,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Farnaz Khun Jush'), arxiv.Result.Author('Markus Biele'), arxiv.Result.Author('Peter M. Dueppenbecker'), arxiv.Result.Author('Andreas Maier')]","Quantitative ultrasound, e.g., speed-of-sound (SoS) in tissues, provides
information about tissue properties that have diagnostic value. Recent studies
showed the possibility of extracting SoS information from pulse-echo ultrasound
raw data (a.k.a. RF data) using deep neural networks that are fully trained on
simulated data. These methods take sensor domain data, i.e., RF data, as input
and train a network in an end-to-end fashion to learn the implicit mapping
between the RF data domain and SoS domain. However, such networks are prone to
overfitting to simulated data which results in poor performance and instability
when tested on measured data. We propose a novel method for SoS mapping
employing learned representations from two linked autoencoders. We test our
approach on simulated and measured data acquired from human breast mimicking
phantoms. We show that SoS mapping is possible using linked autoencoders. The
proposed method has a Mean Absolute Percentage Error (MAPE) of 2.39% on the
simulated data. On the measured data, the predictions of the proposed method
are close to the expected values with MAPE of 1.1%. Compared to an end-to-end
trained network, the proposed method shows higher stability and
reproducibility.",-0.18550235,-0.20968524,0.13760035,C
8517,"We believe that the results in this work are an important step in this
direction, and they lay the foundation for further research on this topic.","Further study of the diﬀerences between these two domains and the conceptual
reasons for why they arise could lead to helpful guidelines for deep learning with
radiology.","References

 1.",2022-07-06 16:33:07+00:00,The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Nicholas Konz'), arxiv.Result.Author('Hanxue Gu'), arxiv.Result.Author('Haoyu Dong'), arxiv.Result.Author('Maciej A. Mazurowski')]","The manifold hypothesis is a core mechanism behind the success of deep
learning, so understanding the intrinsic manifold structure of image data is
central to studying how neural networks learn from the data. Intrinsic dataset
manifolds and their relationship to learning difficulty have recently begun to
be studied for the common domain of natural images, but little such research
has been attempted for radiological images. We address this here. First, we
compare the intrinsic manifold dimensionality of radiological and natural
images. We also investigate the relationship between intrinsic dimensionality
and generalization ability over a wide range of datasets. Our analysis shows
that natural image datasets generally have a higher number of intrinsic
dimensions than radiological images. However, the relationship between
generalization ability and intrinsic dimensionality is much stronger for
medical images, which could be explained as radiological images having
intrinsic features that are more difficult to learn. These results give a more
principled underpinning for the intuition that radiological images can be more
challenging to apply deep learning to than natural image datasets common to
machine learning research. We believe rather than directly applying models
developed for natural images to the radiological imaging domain, more care
should be taken to developing architectures and algorithms that are more
tailored to the specific characteristics of this domain. The research shown in
our paper, demonstrating these characteristics and the differences from natural
images, is an important first step in this direction.",-0.2273831,0.22411504,-0.038696826,A
8757,"[15] ITU-T (formerly CCITT) and ISO/IEC JTC1, “Generic coding of
                                                                                        moving pictures and associated audio information - Part 2: Video,”
   There are several open issues for further study, some of                             ITU-T Recommendation H.262 - ISO/IEC 13818-2 (MPEG-2), Nov.
which have been mentioned before.","1993.
bridge of both optimization to promote the improvement of
coding performance together, so it has great potential.","Firstly, we can add ad-                               1994.
vanced schemes of the two frameworks into our framework to
further realize the potential of hybrid optimization.",2022-07-12 14:36:52+00:00,Towards Hybrid-Optimization Video Coding,eess.IV,['eess.IV'],"[arxiv.Result.Author('Shuai Huo'), arxiv.Result.Author('Dong Liu'), arxiv.Result.Author('Li Li'), arxiv.Result.Author('Siwei Ma'), arxiv.Result.Author('Feng Wu'), arxiv.Result.Author('Wen Gao')]","Video coding is a mathematical optimization problem of rate and distortion
essentially. To solve this complex optimization problem, two popular video
coding frameworks have been developed: block-based hybrid video coding and
end-to-end learned video coding. If we rethink video coding from the
perspective of optimization, we find that the existing two frameworks represent
two directions of optimization solutions. Block-based hybrid coding represents
the discrete optimization solution because those irrelevant coding modes are
discrete in mathematics. It searches for the best one among multiple starting
points (i.e. modes). However, the search is not efficient enough. On the other
hand, end-to-end learned coding represents the continuous optimization solution
because the gradient descent is based on a continuous function. It optimizes a
group of model parameters efficiently by the numerical algorithm. However,
limited by only one starting point, it is easy to fall into the local optimum.
To better solve the optimization problem, we propose to regard video coding as
a hybrid of the discrete and continuous optimization problem, and use both
search and numerical algorithm to solve it. Our idea is to provide multiple
discrete starting points in the global space and optimize the local optimum
around each point by numerical algorithm efficiently. Finally, we search for
the global optimum among those local optimums. Guided by the hybrid
optimization idea, we design a hybrid optimization video coding framework,
which is built on continuous deep networks entirely and also contains some
discrete modes. We conduct a comprehensive set of experiments. Compared to the
continuous optimization framework, our method outperforms pure learned video
coding methods. Meanwhile, compared to the discrete optimization framework, our
method achieves comparable performance to HEVC reference software HM16.10 in
PSNR.",0.47252622,-0.052629482,-0.09910127,B
9216,"18: Sample of generated volumes plus subspheres results and relative isocenters
for the improved α-GAN (diﬀerent angles are illustrated)

results obtained in this work can beneﬁt further research on radiation therapy
treatment planning by generating high-quality synthetic datasets since the con-
nected volumes share many similarities with the tumors.","Tumor      Subs 1  Subs 2    Subs 3  Subs 4  Subs 5    Subs 6       Subs 7

                       (a) Sample 1 with 7 subspheres

    24 53 6            3 6 425                   24 53 6                       Tumor
      71                  17                       71                          Subcenter
                                                                        3 6 1 425
                                                                            7

                       (b) Isocenters as the centers of the subspheres

    Tumor      Subs 1  Subs 2    Subs 3  Subs 4  Subs 5    Subs 6       Subs 7

                       (c) Sample 2 with 7 subspheres

    2 34675 1           2346 57                  2 346                        Tumor
                       1                             75 1                     Subcenter
                                                                        1 46 57
                                                                         23

                                (d) Isocenters as the centers of the subspheres

Fig.","That is, the connectivity
between tumor voxels is a crucial aspect that can be preserved using our proposed
GAN architecture.",2022-07-13 16:39:47+00:00,Improved $α$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Sanaz Mohammadjafari'), arxiv.Result.Author('Mucahit Cevik'), arxiv.Result.Author('Ayse Basar')]","Generative Adversarial Networks (GANs) have gained significant attention in
several computer vision tasks for generating high-quality synthetic data.
Various medical applications including diagnostic imaging and radiation therapy
can benefit greatly from synthetic data generation due to data scarcity in the
domain. However, medical image data is typically kept in 3D space, and
generative models suffer from the curse of dimensionality issues in generating
such synthetic data. In this paper, we investigate the potential of GANs for
generating connected 3D volumes. We propose an improved version of 3D
$\alpha$-GAN by incorporating various architectural enhancements. On a
synthetic dataset of connected 3D spheres and ellipsoids, our model can
generate fully connected 3D shapes with similar geometrical characteristics to
that of training data. We also show that our 3D GAN model can successfully
generate high-quality 3D tumor volumes and associated treatment specifications
(e.g., isocenter locations). Similar moment invariants to the training data as
well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN
implicitly learns the training data distribution, and generates
realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it
a valuable source for generating synthetic medical image data that can help
future research in this domain.",-0.05677186,0.13801295,0.15084568,A
9294,"Color blindness would be an
                                                                            additional criteria for further research.",adjacent colors are approximately even.,"We made the HSV-gradient
   The pros and cons of the rainbow color scheme have been widely           for the different hues as smooth and equal as possible (see Fig.",2022-07-13 08:48:43+00:00,Color Coding of Large Value Ranges Applied to Meteorological Data,eess.IV,"['eess.IV', 'cs.CV', 'cs.GR']","[arxiv.Result.Author('Daniel Braun'), arxiv.Result.Author('Kerstin Ebell'), arxiv.Result.Author('Vera Schemann'), arxiv.Result.Author('Laura Pelchmann'), arxiv.Result.Author('Susanne Crewell'), arxiv.Result.Author('Rita Borgo'), arxiv.Result.Author('Tatiana von Landesberger')]","This paper presents a novel color scheme designed to address the challenge of
visualizing data series with large value ranges, where scale transformation
provides limited support. We focus on meteorological data, where the presence
of large value ranges is common. We apply our approach to meteorological
scatterplots, as one of the most common plots used in this domain area. Our
approach leverages the numerical representation of mantissa and exponent of the
values to guide the design of novel ""nested"" color schemes, able to emphasize
differences between magnitudes. Our user study evaluates the new designs, the
state of the art color scales and representative color schemes used in the
analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess
accuracy, time and confidence in the context of discrimination (comparison) and
interpretation (reading) tasks. Our proposed color scheme significantly
outperforms the others in interpretation tasks, while showing comparable
performances in discrimination tasks.",0.12818936,-0.29799357,-0.12978642,B
9295,"Color blindness would be an
                                                                            additional criteria for further research.",adjacent colors are approximately even.,"We made the HSV-gradient
   The pros and cons of the rainbow color scheme have been widely           for the different hues as smooth and equal as possible (see Fig.",2022-07-13 08:48:43+00:00,Color Coding of Large Value Ranges Applied to Meteorological Data,eess.IV,"['eess.IV', 'cs.CV', 'cs.GR']","[arxiv.Result.Author('Daniel Braun'), arxiv.Result.Author('Kerstin Ebell'), arxiv.Result.Author('Vera Schemann'), arxiv.Result.Author('Laura Pelchmann'), arxiv.Result.Author('Susanne Crewell'), arxiv.Result.Author('Rita Borgo'), arxiv.Result.Author('Tatiana von Landesberger')]","This paper presents a novel color scheme designed to address the challenge of
visualizing data series with large value ranges, where scale transformation
provides limited support. We focus on meteorological data, where the presence
of large value ranges is common. We apply our approach to meteorological
scatterplots, as one of the most common plots used in this domain area. Our
approach leverages the numerical representation of mantissa and exponent of the
values to guide the design of novel ""nested"" color schemes, able to emphasize
differences between magnitudes. Our user study evaluates the new designs, the
state of the art color scales and representative color schemes used in the
analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess
accuracy, time and confidence in the context of discrimination (comparison) and
interpretation (reading) tasks. Our proposed color scheme significantly
outperforms the others in interpretation tasks, while showing comparable
performances in discrimination tasks.",0.12818936,-0.29799357,-0.12978642,B
9308,"We also
did not consider a knee in a given visit if for that visit the knee was missing JSN or osteophyte
grades, since our further study will include JSN and osteophytes estimation and this process only
excludes less than 10% of all samples.","For
example, if a patient had TKA performed on right knee after V2, we included right and left knee
images from V0, V1, V2 and included only left knee’s images from all visits after V2.","Finally, if for a given visit paired PA and LAT views were missing, we also did not consider
such knee for that visit.",2022-07-25 20:35:17+00:00,Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Albert Swiecicki'), arxiv.Result.Author('Nianyi Li'), arxiv.Result.Author(""Jonathan O'Donnell""), arxiv.Result.Author('Nicholas Said'), arxiv.Result.Author('Jichen Yang'), arxiv.Result.Author('Richard C. Mather'), arxiv.Result.Author('William A. Jiranek'), arxiv.Result.Author('Maciej A. Mazurowski')]","A fully-automated deep learning algorithm matched performance of radiologists
in assessment of knee osteoarthritis severity in radiographs using the
Kellgren-Lawrence grading system.
  To develop an automated deep learning-based algorithm that jointly uses
Posterior-Anterior (PA) and Lateral (LAT) views of knee radiographs to assess
knee osteoarthritis severity according to the Kellgren-Lawrence grading system.
  We used a dataset of 9739 exams from 2802 patients from Multicenter
Osteoarthritis Study (MOST). The dataset was divided into a training set of
2040 patients, a validation set of 259 patients and a test set of 503 patients.
A novel deep learning-based method was utilized for assessment of knee OA in
two steps: (1) localization of knee joints in the images, (2) classification
according to the KL grading system. Our method used both PA and LAT views as
the input to the model. The scores generated by the algorithm were compared to
the grades provided in the MOST dataset for the entire test set as well as
grades provided by 5 radiologists at our institution for a subset of the test
set.
  The model obtained a multi-class accuracy of 71.90% on the entire test set
when compared to the ratings provided in the MOST dataset. The quadratic
weighted Kappa coefficient for this set was 0.9066. The average quadratic
weighted Kappa between all pairs of radiologists from our institution who took
a part of study was 0.748. The average quadratic-weighted Kappa between the
algorithm and the radiologists at our institution was 0.769.
  The proposed model performed demonstrated equivalency of KL classification to
MSK radiologists, but clearly superior reproducibility. Our model also agreed
with radiologists at our institution to the same extent as the radiologists
with each other. The algorithm could be used to provide reproducible assessment
of knee osteoarthritis severity.",-0.15317512,-0.34976852,0.114584595,C
9315,"Furthermore, the core idea of KH-CVS is to fuse frames with diﬀerent exposure properties, thus
how to construct more eﬀective sampling strategies is worth further studying.","For the hardware system,
KH-CVS is compatible with deep optics [10,11] to improve spatial resolution, or it can be adopted
to other spatio-temporal compressive imaging systems, such as CUP [65] and COSUP [57].","KH-CVS, we
believe, will open up new study avenues in the future.",2022-07-26 03:27:17+00:00,Key frames assisted hybrid encoding for photorealistic compressive video sensing,eess.IV,['eess.IV'],"[arxiv.Result.Author('Honghao Huang'), arxiv.Result.Author('Jiajie Teng'), arxiv.Result.Author('Yu Liang'), arxiv.Result.Author('Chengyang Hu'), arxiv.Result.Author('Minghua Chen'), arxiv.Result.Author('Sigang Yang'), arxiv.Result.Author('Hongwei Chen')]","Snapshot compressive imaging (SCI) encodes high-speed scene video into a
snapshot measurement and then computationally makes reconstructions, allowing
for efficient high-dimensional data acquisition. Numerous algorithms, ranging
from regularization-based optimization and deep learning, are being
investigated to improve reconstruction quality, but they are still limited by
the ill-posed and information-deficient nature of the standard SCI paradigm. To
overcome these drawbacks, we propose a new key frames assisted hybrid encoding
paradigm for compressive video sensing, termed KH-CVS, that alternatively
captures short-exposure key frames without coding and long-exposure encoded
compressive frames to jointly reconstruct photorealistic video. With the use of
optical flow and spatial warping, a deep convolutional neural network framework
is constructed to integrate the benefits of these two types of frames.
Extensive experiments on both simulations and real data from the prototype we
developed verify the superiority of the proposed method.",0.29498404,-0.044998664,0.17652854,B
9357,"Kaggle’s objective is to provide free and high-quality open-source datasets from various
fields of study to further research in data analytics and machine learning.","E. Training Datasets

    Training datasets were obtained from Kaggle, an open-source research community
owned by Google LLC, which specializes in data analytics and machine learning research.","The dataset used is entitled “Chest X-ray Images for Pneumonia Detection with Deep
Learning” published by Tolga Dincer (2020), a data scientist from Connecticut, United
States.",2022-07-27 04:55:29+00:00,Applied Computer Vision on 2-Dimensional Lung X-Ray Images for Assisted Medical Diagnosis of Pneumonia,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Ralph Joseph S. D. Ligueran'), arxiv.Result.Author('Manuel Luis C. Delos Santos'), arxiv.Result.Author('Dr. Ronaldo S. Tinio'), arxiv.Result.Author('Emmanuel H. Valencia')]","This study focuses on the application of a specific subfield of artificial
intelligence referred to as computer vision in the analysis of 2-dimensional
lung x-ray images for the assisted medical diagnosis of ordinary pneumonia.
  A convolutional neural network algorithm was implemented in a Python-coded,
Flask-based web application that can analyze x-ray images for the detection of
ordinary pneumonia. Since convolutional neural network algorithms rely on
machine learning for the identification and detection of patterns, a technique
referred to as transfer learning was implemented to train the neural network in
the identification and detection of patterns within the dataset. Open-source
lung x-ray images were used as training data to create a knowledge base that
served as the core element of the web application and the experimental design
employed a 5-Trial Confirmatory Test for the validation of the web application.
  The results of the 5-Trial Confirmatory Test show the calculation of
Diagnostic Precision Percentage per Trial, General Diagnostic Precision
Percentage, and General Diagnostic Error Percentage while the Confusion Matrix
further shows the relationship between the label and the corresponding
diagnosis result of the web application on each test images.
  The developed web application can be used by medical practitioners in
A.I.-assisted diagnosis of ordinary pneumonia, and by researchers in the fields
of computer science and bioinformatics.",-0.21769401,0.2754193,-0.16042088,A
9365,"Preliminary results show promise in the use on real data using models
trained with synthetic data, but further research and evidence is needed.","This approach addresses
the limitations in medical data availability, data privacy, and cost of expert an-
notation.","The main limitation of our study is that it is based on the anatomical vari-
ability of a synthetic cohort of healthy subjects.",2022-07-27 10:05:46+00:00,Efficient Pix2Vox++ for 3D Cardiac Reconstruction from 2D echo views,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('David Stojanovski'), arxiv.Result.Author('Uxio Hermida'), arxiv.Result.Author('Marica Muffoletto'), arxiv.Result.Author('Pablo Lamata'), arxiv.Result.Author('Arian Beqiri'), arxiv.Result.Author('Alberto Gomez')]","Accurate geometric quantification of the human heart is a key step in the
diagnosis of numerous cardiac diseases, and in the management of cardiac
patients. Ultrasound imaging is the primary modality for cardiac imaging,
however acquisition requires high operator skill, and its interpretation and
analysis is difficult due to artifacts. Reconstructing cardiac anatomy in 3D
can enable discovery of new biomarkers and make imaging less dependent on
operator expertise, however most ultrasound systems only have 2D imaging
capabilities. We propose both a simple alteration to the Pix2Vox++ networks for
a sizeable reduction in memory usage and computational complexity, and a
pipeline to perform reconstruction of 3D anatomy from 2D standard cardiac
views, effectively enabling 3D anatomical reconstruction from limited 2D data.
We evaluate our pipeline using synthetically generated data achieving accurate
3D whole-heart reconstructions (peak intersection over union score > 0.88) from
just two standard anatomical 2D views of the heart. We also show preliminary
results using real echo images.",-0.3869906,-0.020451734,-0.092697024,C
9775,"Therefore, further research is needed to
ogists.","We showed that        showing that deployment of these models has improved
the system signiﬁcantly improved agreement among radiol-         patient outcomes.","To the best of our knowledge, we are the ﬁrst to         validate the model prospectively and determine its utility
show that a DL system trained on a large-scale, annotated        in clinical settings.",2022-08-06 17:03:49+00:00,An Accurate and Explainable Deep Learning System Improves Interobserver Agreement in the Interpretation of Chest Radiograph,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Hieu H. Pham'), arxiv.Result.Author('Ha Q. Nguyen'), arxiv.Result.Author('Hieu T. Nguyen'), arxiv.Result.Author('Linh T. Le'), arxiv.Result.Author('Lam Khanh')]","Recent artificial intelligence (AI) algorithms have achieved
radiologist-level performance on various medical classification tasks. However,
only a few studies addressed the localization of abnormal findings from CXR
scans, which is essential in explaining the image-level classification to
radiologists. We introduce in this paper an explainable deep learning system
called VinDr-CXR that can classify a CXR scan into multiple thoracic diseases
and, at the same time, localize most types of critical findings on the image.
VinDr-CXR was trained on 51,485 CXR scans with radiologist-provided bounding
box annotations. It demonstrated a comparable performance to experienced
radiologists in classifying 6 common thoracic diseases on a retrospective
validation set of 3,000 CXR scans, with a mean area under the receiver
operating characteristic curve (AUROC) of 0.967 (95% confidence interval [CI]:
0.958-0.975). The VinDr-CXR was also externally validated in independent
patient cohorts and showed its robustness. For the localization task with 14
types of lesions, our free-response receiver operating characteristic (FROC)
analysis showed that the VinDr-CXR achieved a sensitivity of 80.2% at the rate
of 1.0 false-positive lesion identified per scan. A prospective study was also
conducted to measure the clinical impact of the VinDr-CXR in assisting six
experienced radiologists. The results indicated that the proposed system, when
used as a diagnosis supporting tool, significantly improved the agreement
between radiologists themselves with an increase of 1.5% in mean Fleiss' Kappa.
We also observed that, after the radiologists consulted VinDr-CXR's
suggestions, the agreement between each of them and the system was remarkably
increased by 3.3% in mean Cohen's Kappa.",-0.29579365,-0.195391,-0.24117662,C
9776,"Another area for further study is the
quantiﬁcation of ﬁber-level characteristics (e.g., ﬁber density and orientation).","Hence, a future direction of this work could explore the
possibility of integrating the priors within the training framework for further
reduction of FPs, and improving the method to reduce the variation (indicated
by standard deviation) in our results.","5 Conclusions

In this work, we proposed an end-to-end automated, anatomy-constrained self
supervised learning tool for accurate detection of ﬁber bundles on macaque tracer
Fiber bundle detection on macaque tracing data  9

data.",2022-08-06 19:17:02+00:00,Constrained self-supervised method with temporal ensembling for fiber bundle detection on anatomic tracing data,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Vaanathi Sundaresan'), arxiv.Result.Author('Julia F. Lehman'), arxiv.Result.Author('Sean Fitzgibbon'), arxiv.Result.Author('Saad Jbabdi'), arxiv.Result.Author('Suzanne N. Haber'), arxiv.Result.Author('Anastasia Yendiki')]","Anatomic tracing data provides detailed information on brain circuitry
essential for addressing some of the common errors in diffusion MRI
tractography. However, automated detection of fiber bundles on tracing data is
challenging due to sectioning distortions, presence of noise and artifacts and
intensity/contrast variations. In this work, we propose a deep learning method
with a self-supervised loss function that takes anatomy-based constraints into
account for accurate segmentation of fiber bundles on the tracer sections from
macaque brains. Also, given the limited availability of manual labels, we use a
semi-supervised training technique for efficiently using unlabeled data to
improve the performance, and location constraints for further reduction of
false positives. Evaluation of our method on unseen sections from a different
macaque yields promising results with a true positive rate of ~0.90. The code
for our method is available at
https://github.com/v-sundaresan/fiberbundle_seg_tracing.",-0.050855555,0.0866468,-0.00840528,A
9949,"We also include 3D-UNet
and UNETR for 3D segmentation tasks for further research.","2.2 Medical image segmentation

Most methods are designed for 2D segmentation tasks.",2D-UNet.,2022-08-11 02:51:14+00:00,OpenMedIA: Open-Source Medical Image Analysis Toolbox and Benchmark under Heterogeneous AI Computing Platforms,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Jia-Xin Zhuang'), arxiv.Result.Author('Xiansong Huang'), arxiv.Result.Author('Yang Yang'), arxiv.Result.Author('Jiancong Chen'), arxiv.Result.Author('Yue Yu'), arxiv.Result.Author('Wei Gao'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Jie Chen'), arxiv.Result.Author('Tong Zhang')]","In this paper, we present OpenMedIA, an open-source toolbox library
containing a rich set of deep learning methods for medical image analysis under
heterogeneous Artificial Intelligence (AI) computing platforms. Various medical
image analysis methods, including 2D$/$3D medical image classification,
segmentation, localisation, and detection, have been included in the toolbox
with PyTorch and$/$or MindSpore implementations under heterogeneous NVIDIA and
Huawei Ascend computing systems. To our best knowledge, OpenMedIA is the first
open-source algorithm library providing compared PyTorch and MindSp",-0.116266765,-0.054536097,0.22394705,C
9950,"We also include 3D-UNet
and UNETR for 3D segmentation tasks for further research.","2.2 Medical image segmentation

Most methods are designed for 2D segmentation tasks.",2D-UNet.,2022-08-11 02:51:14+00:00,OpenMedIA: Open-Source Medical Image Analysis Toolbox and Benchmark under Heterogeneous AI Computing Platforms,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Jia-Xin Zhuang'), arxiv.Result.Author('Xiansong Huang'), arxiv.Result.Author('Yang Yang'), arxiv.Result.Author('Jiancong Chen'), arxiv.Result.Author('Yue Yu'), arxiv.Result.Author('Wei Gao'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Jie Chen'), arxiv.Result.Author('Tong Zhang')]","In this paper, we present OpenMedIA, an open-source toolbox library
containing a rich set of deep learning methods for medical image analysis under
heterogeneous Artificial Intelligence (AI) computing platforms. Various medical
image analysis methods, including 2D/3D medical image classification,
segmentation, localisation, and detection, have been included in the toolbox
with PyTorch and/or MindSpore implementations under heterogeneous NVIDIA and
Huawei Ascend computing systems. To our best knowledge, OpenMedIA is the first
open-source algorithm library providing compared PyTorch and MindSpore
implementations and results on several benchmark datasets. The source codes and
models are available at https://git.openi.org.cn/OpenMedIA.",-0.116266765,-0.054536097,0.22394705,C
9973,"EDL
uncertainty estimation resembled blurred input,        frameworks with higher segmentation accuracy
while the sUEO of EDL (wDICE) became 0.002             are worthy of further study.",The metrics for            segmentation performance of raw images.,"In addition, since the
higher than that of EDL (DICE).",2022-08-11 21:04:15+00:00,Region-Based Evidential Deep Learning to Quantify Uncertainty and Improve Robustness of Brain Tumor Segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hao Li'), arxiv.Result.Author('Yang Nan'), arxiv.Result.Author('Javier Del Ser'), arxiv.Result.Author('Guang Yang')]","Despite recent advances in the accuracy of brain tumor segmentation, the
results still suffer from low reliability and robustness. Uncertainty
estimation is an efficient solution to this problem, as it provides a measure
of confidence in the segmentation results. The current uncertainty estimation
methods based on quantile regression, Bayesian neural network, ensemble, and
Monte Carlo dropout are limited by their high computational cost and
inconsistency. In order to overcome these challenges, Evidential Deep Learning
(EDL) was developed in recent work but primarily for natural image
classification. In this paper, we proposed a region-based EDL segmentation
framework that can generate reliable uncertainty maps and robust segmentation
results. We used the Theory of Evidence to interpret the output of a neural
network as evidence values gathered from input features. Following Subjective
Logic, evidence was parameterized as a Dirichlet distribution, and predicted
probabilities were treated as subjective opinions. To evaluate the performance
of our model on segmentation and uncertainty estimation, we conducted
quantitative and qualitative experiments on the BraTS 2020 dataset. The results
demonstrated the top performance of the proposed method in quantifying
segmentation uncertainty and robustly segmenting tumors. Furthermore, our
proposed new framework maintained the advantages of low computational cost and
easy implementation and showed the potential for clinical application.",0.15543652,-0.0650287,0.13850564,B
10009,"To provide a guide for transformer-based medical image analysis, we write this review and believe
it can contribute a lot to further research.","Existing published reviews [23, 24, 25, 26, 27] all concentrate on analyzing medical images using the
CNN-based method.",The overall workﬂow of our work is illustrated in Figure 1.,2022-08-13 13:13:41+00:00,Medical image analysis based on transformer: A Review,eess.IV,"['eess.IV', 'cs.CV', 'I.2.1, I.2.10, I.4.9, I.5.4, J.0']","[arxiv.Result.Author('Zhaoshan Liu'), arxiv.Result.Author('Lei Shen')]","The transformer has dominated the natural language processing (NLP) field for
a long time. Recently, the transformer-based method is adopt into the computer
vision (CV) field and shows promising results. As an important branch of the CV
field, medical image analysis joins the wave of the transformer-based method
rightfully. In this paper, we illustrate the principle of the attention
mechanism, and the detailed structures of the transformer, and depict how the
transformer is adopted into the CV field. We organize the transformer-based
medical image analysis applications in the sequence of different CV tasks,
including classification, segmentation, synthesis, registration, localization,
detection, captioning, and denoising. For the mainstream classification and
segmentation tasks, we further divided the corresponding works based on
different medical imaging modalities. We include thirteen modalities and more
than twenty objects in our work. We also visualize the proportion that each
modality and object occupy to give the readers an intuitive impression. We hope
our work can contribute to the development of transformer-based medical image
analysis in the future.",-0.16537902,0.2007373,0.28301007,A
10010,"We summarize the collected works that evaluated a total
of thirteen modalities and more than twenty objects as well as identify areas that further research
can focus on.","In this review, we illustrate the detailed structure of the transformer
and summarize its applications in the sequence of different medical image analysis tasks, such as
classiﬁcation, segmentation, and so on.","With a detailed summary, critical review, and insightful perspective, we believe our
review may contribute to the development of transformer-based medical image analysis.",2022-08-13 13:13:41+00:00,Medical image analysis based on transformer: A Review,eess.IV,"['eess.IV', 'cs.CV', 'I.2.m, I.4.9, I.5.4, J.0']","[arxiv.Result.Author('Zhaoshan Liu'), arxiv.Result.Author('Qiujie Lv'), arxiv.Result.Author('Chau Hung Lee'), arxiv.Result.Author('Lei Shen')]","The transformer has dominated the natural language processing (NLP) field for
a long time. Recently, the transformer-based method has been adopted into the
computer vision (CV) field and shows promising results. As an important branch
of the CV field, medical image analysis joins the wave of the transformer-based
method rightfully. In this review, we illustrate the principle of the attention
mechanism, and the detailed structures of the transformer, and depict how the
transformer is adopted into medical image analysis. We organize the
transformer-based medical image analysis applications in a sequence of
different tasks, including classification, segmentation, synthesis,
registration, localization, detection, captioning, and denoising. For the
mainstream classification and segmentation tasks, we further divided the
corresponding works based on different medical imaging modalities. The datasets
corresponding to the related works are also organized. We include thirteen
modalities and more than twenty objects in our work.",-0.18467787,0.053045172,0.24511153,C
10086,"We hope our work inspires
further research on the fully end-to-end camera ISP network.","Experimental results demonstrated that RBN with KD shows a noticeable per-
formance increase over the alternative approaches.","Acknowledgements
This work was supported by the National Research Foundation of Korea (NRF)
grant funded by the Korea Government (MSIT) (No.",2022-08-16 09:51:57+00:00,RAWtoBit: A Fully End-to-end Camera ISP Network,eess.IV,['eess.IV'],"[arxiv.Result.Author('Wooseok Jeong'), arxiv.Result.Author('Seung-Won Jung')]","Image compression is an essential and last processing unit in the camera
image signal processing (ISP) pipeline. While many studies have been made to
replace the conventional ISP pipeline with a single end-to-end optimized deep
learning model, image compression is barely considered as a part of the model.
In this paper, we investigate the designing of a fully end-to-end optimized
camera ISP incorporating image compression. To this end, we propose RAWtoBit
network (RBN) that can effectively perform both tasks simultaneously. RBN is
further improved with a novel knowledge distillation scheme by introducing two
teacher networks specialized in each task. Extensive experiments demonstrate
that our proposed method significantly outperforms alternative approaches in
terms of rate-distortion trade-off.",0.39368647,-0.057639424,-0.13997214,B
10151,"We hope that our contribution can help the application ﬁeld to further research
the domain and utilise multi-frame information blending for single frame segmentation in deep medical image
analysis more widely.","We provided detailed quantitative and qualitative insights and publish source code and
network weights with this paper.","ACKNOWLEDGMENTS

The institutional Ethics Committee approved data usage and research application (REF: 11277).",2022-08-17 14:28:58+00:00,Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Chengxi Zeng'), arxiv.Result.Author('Xinyu Yang'), arxiv.Result.Author('Majid Mirmehdi'), arxiv.Result.Author('Alberto M Gambaruto'), arxiv.Result.Author('Tilo Burghardt')]","We propose Video-TransUNet, a deep architecture for instance segmentation in
medical CT videos constructed by integrating temporal feature blending into the
TransUNet deep learning framework. In particular, our approach amalgamates
strong frame representation via a ResNet CNN backbone, multi-frame feature
blending via a Temporal Context Module (TCM), non-local attention via a Vision
Transformer, and reconstructive capabilities for multiple targets via a
UNet-based convolutional-deconvolutional architecture with multiple heads. We
show that this new network design can significantly outperform other
state-of-the-art systems when tested on the segmentation of bolus and
pharynx/larynx in Videofluoroscopic Swallowing Study (VFSS) CT sequences. On
our VFSS2022 dataset it achieves a dice coefficient of $0.8796\%$ and an
average surface distance of $1.0379$ pixels. Note that tracking the pharyngeal
bolus accurately is a particularly important application in clinical practice
since it constitutes the primary method for diagnostics of swallowing
impairment. Our findings suggest that the proposed model can indeed enhance the
TransUNet architecture via exploiting temporal information and improving
segmentation performance by a significant margin. We publish key source code,
network weights, and ground truth annotations for simplified performance
reproduction.",-0.10819958,0.13802807,0.016032929,C
10152,"We hope that our contribution can help the application ﬁeld to further research
the domain and utilise multi-frame information blending for single frame segmentation in deep medical image
analysis more widely.","We provided detailed quantitative and qualitative insights and publish source code and
network weights with this paper.","ACKNOWLEDGMENTS

The institutional Ethics Committee approved data usage and research application (REF: 11277).",2022-08-17 14:28:58+00:00,Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Chengxi Zeng'), arxiv.Result.Author('Xinyu Yang'), arxiv.Result.Author('Majid Mirmehdi'), arxiv.Result.Author('Alberto M Gambaruto'), arxiv.Result.Author('Tilo Burghardt')]","We propose Video-TransUNet, a deep architecture for instance segmentation in
medical CT videos constructed by integrating temporal feature blending into the
TransUNet deep learning framework. In particular, our approach amalgamates
strong frame representation via a ResNet CNN backbone, multi-frame feature
blending via a Temporal Context Module (TCM), non-local attention via a Vision
Transformer, and reconstructive capabilities for multiple targets via a
UNet-based convolutional-deconvolutional architecture with multiple heads. We
show that this new network design can significantly outperform other
state-of-the-art systems when tested on the segmentation of bolus and
pharynx/larynx in Videofluoroscopic Swallowing Study (VFSS) CT sequences. On
our VFSS2022 dataset it achieves a dice coefficient of $0.8796\%$ and an
average surface distance of $1.0379$ pixels. Note that tracking the pharyngeal
bolus accurately is a particularly important application in clinical practice
since it constitutes the primary method for diagnostics of swallowing
impairment. Our findings suggest that the proposed model can indeed enhance the
TransUNet architecture via exploiting temporal information and improving
segmentation performance by a significant margin. We publish key source code,
network weights, and ground truth annotations for simplified performance
reproduction.",-0.10819958,0.13802807,0.016032929,C
10153,"We hope that our contribution can help the application ﬁeld to further research
the domain and utilise multi-frame information blending for single frame segmentation in deep medical image
analysis more widely.","We provided detailed quantitative and qualitative insights and publish source code and
network weights with this paper.","ACKNOWLEDGMENTS

The institutional Ethics Committee approved data usage and research application (REF: 11277).",2022-08-17 14:28:58+00:00,Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Chengxi Zeng'), arxiv.Result.Author('Xinyu Yang'), arxiv.Result.Author('Majid Mirmehdi'), arxiv.Result.Author('Alberto M Gambaruto'), arxiv.Result.Author('Tilo Burghardt')]","We propose Video-TransUNet, a deep architecture for instance segmentation in
medical CT videos constructed by integrating temporal feature blending into the
TransUNet deep learning framework. In particular, our approach amalgamates
strong frame representation via a ResNet CNN backbone, multi-frame feature
blending via a Temporal Context Module (TCM), non-local attention via a Vision
Transformer, and reconstructive capabilities for multiple targets via a
UNet-based convolutional-deconvolutional architecture with multiple heads. We
show that this new network design can significantly outperform other
state-of-the-art systems when tested on the segmentation of bolus and
pharynx/larynx in Videofluoroscopic Swallowing Study (VFSS) CT sequences. On
our VFSS2022 dataset it achieves a dice coefficient of 0.8796 and an average
surface distance of 1.0379 pixels. Note that tracking the pharyngeal bolus
accurately is a particularly important application in clinical practice since
it constitutes the primary method for diagnostics of swallowing impairment. Our
findings suggest that the proposed model can indeed enhance the TransUNet
architecture via exploiting temporal information and improving segmentation
performance by a significant margin. We publish key source code, network
weights, and ground truth annotations for simplified performance reproduction.",-0.10819958,0.13802807,0.016032929,C
10351,has inspired further research into content-adaptive transcoding.,"INTRODUCTION

                                          In recent years, the growth in delivery of video at scale for broadcast and streaming applications (from Netﬂix,
                                          YouTube, Disney etc.)","The goal is to deliver
                                          high-quality content at progressively lower bitrates by adapting the transcoder for each input at a ﬁne-grained
                                          level of control.",2022-08-23 18:34:26+00:00,Direct Optimisation of $\boldsymbolλ$ for HDR Content Adaptive Transcoding in AV1,eess.IV,"['eess.IV', 'cs.MM', 'eess.SP']","[arxiv.Result.Author('Vibhoothi'), arxiv.Result.Author('François Pitiè'), arxiv.Result.Author('Angeliki Katsenou'), arxiv.Result.Author('Daniel Joseph Ringis'), arxiv.Result.Author('Yeping Su'), arxiv.Result.Author('Neil Birkbeck'), arxiv.Result.Author('Jessie Lin'), arxiv.Result.Author('Balu Adsumilli'), arxiv.Result.Author('Anil Kokaram')]","Since the adoption of VP9 by Netflix in 2016, royalty-free coding standards
continued to gain prominence through the activities of the AOMedia consortium.
AV1, the latest open source standard, is now widely supported. In the early
years after standardisation, HDR video tends to be under served in open source
encoders for a variety of reasons including the relatively small amount of true
HDR content being broadcast and the challenges in RD optimisation with that
material. AV1 codec optimisation has been ongoing since 2020 including
consideration of the computational load. In this paper, we explore the idea of
direct optimisation of the Lagrangian $\lambda$ parameter used in the rate
control of the encoders to estimate the optimal Rate-Distortion trade-off
achievable for a High Dynamic Range signalled video clip. We show that by
adjusting the Lagrange multiplier in the RD optimisation process on a
frame-hierarchy basis, we are able to increase the Bjontegaard difference rate
gains by more than 3.98$\times$ on average without visually affecting the
quality.",0.5811963,0.0278412,-0.19135691,B
10352,has inspired further research into content-adaptive transcoding.,"INTRODUCTION

                                         In recent years, the growth in delivery of video at scale for broadcast and streaming applications (from Netﬂix,
                                         YouTube, Disney etc.)","The goal is to deliver
                                         high-quality content at progressively lower bitrates by adapting the transcoder for each input at a ﬁne-grained
                                         level of control.",2022-08-23 18:34:26+00:00,Direct Optimisation of $\boldsymbolλ$ for HDR Content Adaptive Transcoding in AV1,eess.IV,"['eess.IV', 'cs.MM', 'eess.SP']","[arxiv.Result.Author('Vibhoothi'), arxiv.Result.Author('François Pitié'), arxiv.Result.Author('Angeliki Katsenou'), arxiv.Result.Author('Daniel Joseph Ringis'), arxiv.Result.Author('Yeping Su'), arxiv.Result.Author('Neil Birkbeck'), arxiv.Result.Author('Jessie Lin'), arxiv.Result.Author('Balu Adsumilli'), arxiv.Result.Author('Anil Kokaram')]","Since the adoption of VP9 by Netflix in 2016, royalty-free coding standards
continued to gain prominence through the activities of the AOMedia consortium.
AV1, the latest open source standard, is now widely supported. In the early
years after standardisation, HDR video tends to be under served in open source
encoders for a variety of reasons including the relatively small amount of true
HDR content being broadcast and the challenges in RD optimisation with that
material. AV1 codec optimisation has been ongoing since 2020 including
consideration of the computational load. In this paper, we explore the idea of
direct optimisation of the Lagrangian $\lambda$ parameter used in the rate
control of the encoders to estimate the optimal Rate-Distortion trade-off
achievable for a High Dynamic Range signalled video clip. We show that by
adjusting the Lagrange multiplier in the RD optimisation process on a
frame-hierarchy basis, we are able to increase the Bjontegaard difference rate
gains by more than 3.98$\times$ on average without visually affecting the
quality.",0.5811963,0.0278412,-0.19135691,B
10409,"How-
ever, to make a final verdict on this matter, it is necessary to conduct further research
into different aggregation algorithms, privacy preserving techniques, and even defense
mechanisms against adversarial attacks.","Achieved results confirmed that the generation of
skin lesions in a distributed setup can lead to similar performance with respect to the
quality and diversity of generated samples, with a significant faster convergence.","6 Conclusions

GAN-based augmentation is an extensively explored technique for medical imaging ap-
plications, especially in the case of very rare diseases.",2022-08-24 15:59:39+00:00,GAN-based generative modelling for dermatological applications -- comparative study,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Sandra Carrasco Limeros'), arxiv.Result.Author('Sylwia Majchrowska'), arxiv.Result.Author('Mohamad Khir Zoubi'), arxiv.Result.Author('Anna Rosén'), arxiv.Result.Author('Juulia Suvilehto'), arxiv.Result.Author('Lisa Sjöblom'), arxiv.Result.Author('Magnus Kjellberg')]","The lack of sufficiently large open medical databases is one of the biggest
challenges in AI-powered healthcare. Synthetic data created using Generative
Adversarial Networks (GANs) appears to be a good solution to mitigate the
issues with privacy policies. The other type of cure is decentralized protocol
across multiple medical institutions without exchanging local data samples. In
this paper, we explored unconditional and conditional GANs in centralized and
decentralized settings. The centralized setting imitates studies on large but
highly unbalanced skin lesion dataset, while the decentralized one simulates a
more realistic hospital scenario with three institutions. We evaluated models'
performance in terms of fidelity, diversity, speed of training, and predictive
ability of classifiers trained on the generated synthetic data. In addition we
provided explainability through exploration of latent space and embeddings
projection focused both on global and local explanations. Calculated distance
between real images and their projections in the latent space proved the
authenticity and generalization of trained GANs, which is one of the main
concerns in this type of applications. The open source code for conducted
studies is publicly available at
\url{https://github.com/aidotse/stylegan2-ada-pytorch}.",-0.12312944,0.2329171,0.11132124,A
10485,"As we can observe in                       serve that although our approach improves upon previous
Table 6, on CLIC images (around 2048 × 1365), the actual                          method, it is still far from (a lower bound of) the theoretical
bit rate is very close to the estimated bit rate, sometimes                       limit, and further research is required to approach the limit
even smaller than estimated ones, for example when λ =                            of compression.","We ob-
will asymptotically decreases to zero.",2048.,2022-08-27 17:15:38+00:00,Lossy Image Compression with Quantized Hierarchical VAEs,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhihao Duan'), arxiv.Result.Author('Ming Lu'), arxiv.Result.Author('Zhan Ma'), arxiv.Result.Author('Fengqing Zhu')]","Recent work has shown a strong theoretical connection between variational
autoencoders (VAEs) and the rate distortion theory. Motivated by this, we
consider the problem of lossy image compression from the perspective of
generative modeling. Starting from ResNet VAEs, which are originally designed
for data (image) distribution modeling, we redesign their latent variable model
using a quantization-aware posterior and prior, enabling easy quantization and
entropy coding for image compression. Along with improved neural network
blocks, we present a powerful and efficient class of lossy image coders,
outperforming previous methods on natural image (lossy) compression. Our model
compresses images in a coarse-to-fine fashion and supports parallel encoding
and decoding, leading to fast execution on GPUs.",0.49448407,-0.13459074,0.08816883,B
10752,"The results reported in this paper constitute an interesting departure point
for further research.","Finally, we intro-
duced a rigorous manual delineation process that was followed by radiologists
to provide ground-truth segmentations, together with additional information
related to their conﬁdence and analysis time.","The follow-up steps for our segmentation algorithm would
be to evaluate its robustness for longitudinal assessment and its ability to ade-
quately identify disease progression or response during the course of a number
of drug treatments.",2022-09-03 11:41:42+00:00,Deep learning automates bidimensional and volumetric tumor burden measurement from MRI in pre- and post-operative glioblastoma patients,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Jakub Nalepa'), arxiv.Result.Author('Krzysztof Kotowski'), arxiv.Result.Author('Bartosz Machura'), arxiv.Result.Author('Szymon Adamski'), arxiv.Result.Author('Oskar Bozek'), arxiv.Result.Author('Bartosz Eksner'), arxiv.Result.Author('Bartosz Kokoszka'), arxiv.Result.Author('Tomasz Pekala'), arxiv.Result.Author('Mateusz Radom'), arxiv.Result.Author('Marek Strzelczak'), arxiv.Result.Author('Lukasz Zarudzki'), arxiv.Result.Author('Agata Krason'), arxiv.Result.Author('Filippo Arcadu'), arxiv.Result.Author('Jean Tessier')]","Tumor burden assessment by magnetic resonance imaging (MRI) is central to the
evaluation of treatment response for glioblastoma. This assessment is complex
to perform and associated with high variability due to the high heterogeneity
and complexity of the disease. In this work, we tackle this issue and propose a
deep learning pipeline for the fully automated end-to-end analysis of
glioblastoma patients. Our approach simultaneously identifies tumor
sub-regions, including the enhancing tumor, peritumoral edema and surgical
cavity in the first step, and then calculates the volumetric and bidimensional
measurements that follow the current Response Assessment in Neuro-Oncology
(RANO) criteria. Also, we introduce a rigorous manual annotation process which
was followed to delineate the tumor sub-regions by the human experts, and to
capture their segmentation confidences that are later used while training the
deep learning models. The results of our extensive experimental study performed
over 760 pre-operative and 504 post-operative adult patients with glioma
obtained from the public database (acquired at 19 sites in years 2021-2020) and
from a clinical treatment trial (47 and 69 sites for pre-/post-operative
patients, 2009-2011) and backed up with thorough quantitative, qualitative and
statistical analysis revealed that our pipeline performs accurate segmentation
of pre- and post-operative MRIs in a fraction of the manual delineation time
(up to 20 times faster than humans). The bidimensional and volumetric
measurements were in strong agreement with expert radiologists, and we showed
that RANO measurements are not always sufficient to quantify tumor burden.",-0.33006194,-0.16550593,-0.04712411,C
10816,"We hope these empirical results could help
further researchers.","In our experiments, deeper U-Net shows better perfor-
mance than vanilla and thinner ones.","References

1.",2022-09-06 06:53:41+00:00,An evaluation of U-Net in Renal Structure Segmentation,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Ziyan Huang'), arxiv.Result.Author('Jin Ye'), arxiv.Result.Author('Can Tu'), arxiv.Result.Author('Yuncheng Yang'), arxiv.Result.Author('Shiyi Du'), arxiv.Result.Author('Zhongying Deng'), arxiv.Result.Author('Chenglong Ma'), arxiv.Result.Author('Jingqi Niu'), arxiv.Result.Author('Junjun He')]","Renal structure segmentation from computed tomography angiography~(CTA) is
essential for many computer-assisted renal cancer treatment applications.
Kidney PArsing~(KiPA 2022) Challenge aims to build a fine-grained
multi-structure dataset and improve the segmentation of multiple renal
structures. Recently, U-Net has dominated the medical image segmentation. In
the KiPA challenge, we evaluated several U-Net variants and selected the best
models for the final submission.",0.14234027,-0.12154016,-0.28581524,B
10881,"Also, further research is required to quantify completeness.","However,
    there is a limited progress in terms of identifying extensive concepts for medical tasks.",5.,2022-09-07 04:44:56+00:00,Deep Learning for Medical Imaging From Diagnosis Prediction to its Counterfactual Explanation,eess.IV,"['eess.IV', 'cs.CV']",[arxiv.Result.Author('Sumedha Singla')],"Deep neural networks (DNN) have achieved unprecedented performance in
computer-vision tasks almost ubiquitously in business, technology, and science.
While substantial efforts are made to engineer highly accurate architectures
and provide usable model explanations, most state-of-the-art approaches are
first designed for natural vision and then translated to the medical domain.
This dissertation seeks to address this gap by proposing novel architectures
that integrate the domain-specific constraints of medical imaging into the DNN
model and explanation design.",-0.29356894,-0.19959642,-0.2136459,C
10906,"• Reduced peripapillary vessel density, another parameter that requires
       further research, is discussed in Ferrari et al.","The fovea can be seen in Figure 5 as the dark spot in the center
       where it marks the center of the retina (Bear et al., 2016; Augustin and
       Atorf, 2022).",(2017).,2022-09-07 08:27:10+00:00,A Survey on Automated Diagnosis of Alzheimer's Disease Using Optical Coherence Tomography and Angiography,eess.IV,"['eess.IV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yasemin Turkan'), arxiv.Result.Author('F. Boray Tek')]","Retinal optical coherence tomography (OCT) and optical coherence tomography
angiography (OCTA) are promising tools for the (early) diagnosis of Alzheimer's
disease (AD). These non-invasive imaging techniques are cost-effective and more
accessible than alternative neuroimaging tools. However, interpreting and
classifying multi-slice scans produced by OCT devices is time-consuming and
challenging even for trained practitioners.
  There are surveys on machine learning and deep learning approaches concerning
the automated analysis of OCT scans for various diseases such as glaucoma.
However, the current literature lacks an extensive survey on the diagnosis of
Alzheimer's disease or cognitive impairment using OCT or OCTA. This has
motivated us to do a comprehensive survey aimed at machine/deep learning
scientists or practitioners who require an introduction to the problem. The
paper contains 1) an introduction to the medical background of Alzheimer's
Disease and Cognitive Impairment and their diagnosis using OCT and OCTA imaging
modalities, 2) a review of various technical proposals for the problem and the
sub-problems from an automated analysis perspective, 3) a systematic review of
the recent deep learning studies and available OCT/OCTA datasets directly aimed
at the diagnosis of Alzheimer's Disease and Cognitive Impairment. For the
latter, we used Publish or Perish Software to search for the relevant studies
from various sources such as Scopus, PubMed, and Web of Science. We followed
the PRISMA approach to screen an initial pool of 3073 references and determined
ten relevant studies (N=10, out of 3073) that directly targeted AD diagnosis.
We identified the lack of open OCT/OCTA datasets (about Alzheimer's disease) as
the main issue that is impeding the progress in the field.",-0.054133687,-0.22664592,0.14249584,C
10907,"However, other parameters such as the macula and
       choroid layer size require further research before manufacturers incor-
       porate these options into future devices.","New OCT devices can measure RNFL thickness
       (Schott, 2020).","Blood vessel segmentation is

                                               17
  another parameter used to diagnose Alzheimer’s disease.",2022-09-07 08:27:10+00:00,A Survey on Automated Diagnosis of Alzheimer's Disease Using Optical Coherence Tomography and Angiography,eess.IV,"['eess.IV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yasemin Turkan'), arxiv.Result.Author('F. Boray Tek')]","Retinal optical coherence tomography (OCT) and optical coherence tomography
angiography (OCTA) are promising tools for the (early) diagnosis of Alzheimer's
disease (AD). These non-invasive imaging techniques are cost-effective and more
accessible than alternative neuroimaging tools. However, interpreting and
classifying multi-slice scans produced by OCT devices is time-consuming and
challenging even for trained practitioners.
  There are surveys on machine learning and deep learning approaches concerning
the automated analysis of OCT scans for various diseases such as glaucoma.
However, the current literature lacks an extensive survey on the diagnosis of
Alzheimer's disease or cognitive impairment using OCT or OCTA. This has
motivated us to do a comprehensive survey aimed at machine/deep learning
scientists or practitioners who require an introduction to the problem. The
paper contains 1) an introduction to the medical background of Alzheimer's
Disease and Cognitive Impairment and their diagnosis using OCT and OCTA imaging
modalities, 2) a review of various technical proposals for the problem and the
sub-problems from an automated analysis perspective, 3) a systematic review of
the recent deep learning studies and available OCT/OCTA datasets directly aimed
at the diagnosis of Alzheimer's Disease and Cognitive Impairment. For the
latter, we used Publish or Perish Software to search for the relevant studies
from various sources such as Scopus, PubMed, and Web of Science. We followed
the PRISMA approach to screen an initial pool of 3073 references and determined
ten relevant studies (N=10, out of 3073) that directly targeted AD diagnosis.
We identified the lack of open OCT/OCTA datasets (about Alzheimer's disease) as
the main issue that is impeding the progress in the field.",-0.1424587,-0.30215555,0.036726788,C
10918,"While it outperforms
Model with normalization            0.08     87.22         200     JPEG compression using DFT, there are still other variations of
Model with normalization and noise  0.111    56.74          30     JPEG, that needs further research.","TABLE I: Comparison of proposed model with different
                              settings

Model (variations)                  Loss   Accuracy  Epoches                                  V. CONCLUSION
Model                               0.162   80.236          15
Model                               0.092   85.352         100        An autoencoder-based image compression technique is pre-
Model                               0.089   82.307         200     sented in the paper, which uses a lower-dimensional represen-
Model with batch size 32            0.13    84.269         200     tation to reconstruct the image with less reconstruction loss by
Model with normalization            0.07     85.94         100     capturing the most signiﬁcant elements.","Among others, the impact of
                                                                   noise as a regularization method deserves further investigation.",2022-08-26 12:46:16+00:00,Convolutional Neural Network (CNN) to reduce construction loss in JPEG compression,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']",[arxiv.Result.Author('Suman Kunwar')],"In recent decades, digital image processing has gained enormous popularity.
Consequently, a number of data compression strategies have been put forth, with
the goal of minimizing the amount of information required to represent images.
Among them, JPEG compression is one of the most popular methods that has been
widely applied in multimedia and digital applications. The periodic nature of
DFT makes it impossible to meet the periodic condition of an image's opposing
edges without producing severe artifacts, which lowers the image's perceptual
visual quality. On the other hand, deep learning has recently achieved
outstanding results for applications like speech recognition, image reduction,
and natural language processing. Convolutional Neural Networks (CNN) have
received more attention than most other types of deep neural networks. The use
of convolution in feature extraction results in a less redundant feature map
and a smaller dataset, both of which are crucial for image compression. In this
work, an effective image compression method is purposed using autoencoders. The
study's findings revealed a number of important trends that suggested better
reconstruction along with good compression can be achieved using autoencoders.",0.3707105,0.1661264,0.1638799,B
11036,"With the increase of τ , the PMF of r is quantized
    In Table 5, we further study the relationships between           by larger bins and becomes coarser.","ˆr with pˆϕ(ˆr|u, Cˆr, τ ) (with SQRC) resulting in lower bit
                                                                     rates.","Thus, the compression
different network architectures of RC and lossless image             performance with SQRC approaches the oracle.",2022-09-11 12:11:56+00:00,Deep Lossy Plus Residual Coding for Lossless and Near-lossless Image Compression,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Yuanchao Bai'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Kai Wang'), arxiv.Result.Author('Xiangyang Ji'), arxiv.Result.Author('Xiaolin Wu'), arxiv.Result.Author('Wen Gao')]","Lossless and near-lossless image compression is of paramount importance to
professional users in many technical fields, such as medicine, remote sensing,
precision engineering and scientific research. But despite rapidly growing
research interests in learning-based image compression, no published method
offers both lossless and near-lossless modes. In this paper, we propose a
unified and powerful deep lossy plus residual (DLPR) coding framework for both
lossless and near-lossless image compression. In the lossless mode, the DLPR
coding system first performs lossy compression and then lossless coding of
residuals. We solve the joint lossy and residual compression problem in the
approach of VAEs, and add autoregressive context modeling of the residuals to
enhance lossless compression performance. In the near-lossless mode, we
quantize the original residuals to satisfy a given $\ell_\infty$ error bound,
and propose a scalable near-lossless compression scheme that works for variable
$\ell_\infty$ bounds instead of training multiple networks. To expedite the
DLPR coding, we increase the degree of algorithm parallelization by a novel
design of coding context, and accelerate the entropy coding with adaptive
residual interval. Experimental results demonstrate that the DLPR coding system
achieves both the state-of-the-art lossless and near-lossless image compression
performance with competitive coding speed.",0.5415151,-0.03474277,-0.048681155,B
11037,"16, we further study the effects of different λ’s on  5                                         = 0.06
                                                                                                            = 0.03
the near-lossless image compression performance.",In Fig.,"Though                                                     = 0.001
λ = 0 leads to the best lossless compression performance, it      4                                         =0

is unsuitable for near-lossless compression since the residual    3
quantization (9) is adopted on the r = x.",2022-09-11 12:11:56+00:00,Deep Lossy Plus Residual Coding for Lossless and Near-lossless Image Compression,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Yuanchao Bai'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Kai Wang'), arxiv.Result.Author('Xiangyang Ji'), arxiv.Result.Author('Xiaolin Wu'), arxiv.Result.Author('Wen Gao')]","Lossless and near-lossless image compression is of paramount importance to
professional users in many technical fields, such as medicine, remote sensing,
precision engineering and scientific research. But despite rapidly growing
research interests in learning-based image compression, no published method
offers both lossless and near-lossless modes. In this paper, we propose a
unified and powerful deep lossy plus residual (DLPR) coding framework for both
lossless and near-lossless image compression. In the lossless mode, the DLPR
coding system first performs lossy compression and then lossless coding of
residuals. We solve the joint lossy and residual compression problem in the
approach of VAEs, and add autoregressive context modeling of the residuals to
enhance lossless compression performance. In the near-lossless mode, we
quantize the original residuals to satisfy a given $\ell_\infty$ error bound,
and propose a scalable near-lossless compression scheme that works for variable
$\ell_\infty$ bounds instead of training multiple networks. To expedite the
DLPR coding, we increase the degree of algorithm parallelization by a novel
design of coding context, and accelerate the entropy coding with adaptive
residual interval. Experimental results demonstrate that the DLPR coding system
achieves both the state-of-the-art lossless and near-lossless image compression
performance with competitive coding speed.",0.46525484,-0.052805692,0.1264416,B
11244,"Comparing Figure 4
h) and i) one can observe, that the ball is distorted with struc-         Focus of further research will be to incorporate a scene
tured artifacts in h).",ﬁner details than the other algorithms.,"These artifacts are caused by the motion       change detection algorithm in addition, to prevent negative
within the sequence, as the net, which is visible in previous         effects due to image content from neighboring scenes within
and succeeding frames, contributes to the model up to a cer-          a sequence.",2022-09-15 11:45:05+00:00,Motion-Adapted Three-Dimensional Frequency Selective Extrapolation,eess.IV,['eess.IV'],"[arxiv.Result.Author('Andreas Spruck'), arxiv.Result.Author('Markus Jonscher'), arxiv.Result.Author('JÜrgen Seiler'), arxiv.Result.Author('André Kaup')]","It has been shown, that high resolution images can be acquired using a low
resolution sensor with non-regular sampling. Therefore, post-processing is
necessary. In terms of video data, not only the spatial neighborhood can be
used to assist the reconstruction, but also the temporal neighborhood. A
popular and well performing algorithm for this kind of problem is the
three-dimensional frequency selective extrapolation (3D-FSE) for which a motion
adapted version is introduced in this paper. This proposed extension solves the
problem of changing content within the area considered by the 3D-FSE, which is
caused by motion within the sequence. Because of this motion, it may happen
that regions are emphasized during the reconstruction that are not present in
the original signal within the considered area. By that, false content is
introduced into the extrapolated sequence, which affects the resulting image
quality negatively. The novel extension, presented in the following,
incorporates motion data of the sequence in order to adapt the algorithm
accordingly, and compensates changing content, resulting in gains of up to 1.75
dB compared to the existing 3D-FSE.",0.31062412,-0.16712269,0.11820759,B
11297,"Also, we propose the ﬁrst end-to-end deep 3D matting network and implement a solid 3D
                                          medical image matting benchmark, which will be released to encourage further research1.","We then adapt the four selected state-of-the-art
                                          2D image matting algorithms to 3D scenes and further customize the methods for CT images.","Keywords: 3D Matting, Pulmonary nodules, Soft Segmentation, Thoracic CT, Uncertainty

                                          1.",2022-09-16 10:18:59+00:00,3D Matting: A Soft Segmentation Method Applied in Computed Tomography,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Lin Wang'), arxiv.Result.Author('Xiufen Ye'), arxiv.Result.Author('Donghao Zhang'), arxiv.Result.Author('Wanji He'), arxiv.Result.Author('Lie Ju'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Wei Feng'), arxiv.Result.Author('Kaimin Song'), arxiv.Result.Author('Xin Zhao'), arxiv.Result.Author('Zongyuan Ge')]","Three-dimensional (3D) images, such as CT, MRI, and PET, are common in
medical imaging applications and important in clinical diagnosis. Semantic
ambiguity is a typical feature of many medical image labels. It can be caused
by many factors, such as the imaging properties, pathological anatomy, and the
weak representation of the binary masks, which brings challenges to accurate 3D
segmentation. In 2D medical images, using soft masks instead of binary masks
generated by image matting to characterize lesions can provide rich semantic
information, describe the structural characteristics of lesions more
comprehensively, and thus benefit the subsequent diagnoses and analyses. In
this work, we introduce image matting into the 3D scenes to describe the
lesions in 3D medical images. The study of image matting in 3D modality is
limited, and there is no high-quality annotated dataset related to 3D matting,
therefore slowing down the development of data-driven deep-learning-based
methods. To address this issue, we constructed the first 3D medical matting
dataset and convincingly verified the validity of the dataset through quality
control and downstream experiments in lung nodules classification. We then
adapt the four selected state-of-the-art 2D image matting algorithms to 3D
scenes and further customize the methods for CT images. Also, we propose the
first end-to-end deep 3D matting network and implement a solid 3D medical image
matting benchmark, which will be released to encourage further research.",-0.11432367,0.20812716,0.31765664,A
11444,These two issues represent interesting ﬁelds for further research.,"Also, it constitutes an ad hoc stage that does
not allow to train the networks in a truly end-to-end setting (from the original data to the
target decisions).","Declaration of competing interest

    The authors declare that they have no known competing ﬁnancial interests or personal
relationships that could have appeared to inﬂuence the work reported in this paper.",2022-09-20 09:54:01+00:00,Simultaneous segmentation and classification of the retinal arteries and veins from color fundus images,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('José Morano'), arxiv.Result.Author('Álvaro S. Hervella'), arxiv.Result.Author('Jorge Novo'), arxiv.Result.Author('José Rouco')]","The study of the retinal vasculature is a fundamental stage in the screening
and diagnosis of many diseases. A complete retinal vascular analysis requires
to segment and classify the blood vessels of the retina into arteries and veins
(A/V). Early automatic methods approached these segmentation and classification
tasks in two sequential stages. However, currently, these tasks are approached
as a joint semantic segmentation task, as the classification results highly
depend on the effectiveness of the vessel segmentation. In that regard, we
propose a novel approach for the simultaneous segmentation and classification
of the retinal A/V from eye fundus images. In particular, we propose a novel
method that, unlike previous approaches, and thanks to a novel loss, decomposes
the joint task into three segmentation problems targeting arteries, veins and
the whole vascular tree. This configuration allows to handle vessel crossings
intuitively and directly provides accurate segmentation masks of the different
target vascular trees. The provided ablation study on the public Retinal Images
vessel Tree Extraction (RITE) dataset demonstrates that the proposed method
provides a satisfactory performance, particularly in the segmentation of the
different structures. Furthermore, the comparison with the state of the art
shows that our method achieves highly competitive results in A/V
classification, while significantly improving vascular segmentation. The
proposed multi-segmentation method allows to detect more vessels and better
segment the different structures, while achieving a competitive classification
performance. Also, in these terms, our approach outperforms the approaches of
various reference works. Moreover, in contrast with previous approaches, the
proposed method allows to directly detect the vessel crossings, as well as
preserving the continuity of A/V at these complex locations.",0.009436222,0.02062083,-0.39225698,A
11447,"This framework shows promise for
building new AI models generalisable across clinical centres with limited data acquired in challenging
and heterogeneous conditions and calls for further research to develop new solutions for usability of
AI in countries with less resources and, consequently, in higher need of clinical support.","In particular, the model can be re-aligned
and optimised to boost the performance on African populations by increasing the recall to 0.92±0.04
and at the same time maintaining a high precision across centres.","Keywords: Artiﬁcial intelligence, low-resource settings, deep learning, domain generalisation,
ultrasound imaging, transfer learning.",2022-09-20 10:56:09+00:00,Generalisability of deep learning models in low-resource imaging settings: A fetal ultrasound study in 5 African countries,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Carla Sendra-Balcells'), arxiv.Result.Author('Víctor M. Campello'), arxiv.Result.Author('Jordina Torrents-Barrena'), arxiv.Result.Author('Yahya Ali Ahmed'), arxiv.Result.Author('Mustafa Elattar'), arxiv.Result.Author('Benard Ohene Botwe'), arxiv.Result.Author('Pempho Nyangulu'), arxiv.Result.Author('William Stones'), arxiv.Result.Author('Mohammed Ammar'), arxiv.Result.Author('Lamya Nawal Benamer'), arxiv.Result.Author('Harriet Nalubega Kisembo'), arxiv.Result.Author('Senai Goitom Sereke'), arxiv.Result.Author('Sikolia Z. Wanyonyi'), arxiv.Result.Author('Marleen Temmerman'), arxiv.Result.Author('Kamil Mikolaj'), arxiv.Result.Author('Martin Grønnebæk Tolsgaard'), arxiv.Result.Author('Karim Lekadir')]","Most artificial intelligence (AI) research have concentrated in high-income
countries, where imaging data, IT infrastructures and clinical expertise are
plentiful. However, slower progress has been made in limited-resource
environments where medical imaging is needed. For example, in Sub-Saharan
Africa the rate of perinatal mortality is very high due to limited access to
antenatal screening. In these countries, AI models could be implemented to help
clinicians acquire fetal ultrasound planes for diagnosis of fetal
abnormalities. So far, deep learning models have been proposed to identify
standard fetal planes, but there is no evidence of their ability to generalise
in centres with limited access to high-end ultrasound equipment and data. This
work investigates different strategies to reduce the domain-shift effect for a
fetal plane classification model trained on a high-resource clinical centre and
transferred to a new low-resource centre. To that end, a classifier trained
with 1,792 patients from Spain is first evaluated on a new centre in Denmark in
optimal conditions with 1,008 patients and is later optimised to reach the same
performance in five African centres (Egypt, Algeria, Uganda, Ghana and Malawi)
with 25 patients each. The results show that a transfer learning approach can
be a solution to integrate small-size African samples with existing large-scale
databases in developed countries. In particular, the model can be re-aligned
and optimised to boost the performance on African populations by increasing the
recall to $0.92 \pm 0.04$ and at the same time maintaining a high precision
across centres. This framework shows promise for building new AI models
generalisable across clinical centres with limited data acquired in challenging
and heterogeneous conditions and calls for further research to develop new
solutions for usability of AI in countries with less resources.",-0.3468349,0.21586853,-0.17777774,A
11448,"[18] evaluated the
maturity of state-of-the-art CNNs to automatically classify 2D maternal fetal US and released a
large open-source dataset to promote further research on the matter.","More recently, Burgos-Artizzu et al.","While some AI tools are being
implemented to recognise multiple views in fetal imaging [19], others focus their work on speciﬁc
anatomical structures [20, 21, 22].",2022-09-20 10:56:09+00:00,Generalisability of deep learning models in low-resource imaging settings: A fetal ultrasound study in 5 African countries,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Carla Sendra-Balcells'), arxiv.Result.Author('Víctor M. Campello'), arxiv.Result.Author('Jordina Torrents-Barrena'), arxiv.Result.Author('Yahya Ali Ahmed'), arxiv.Result.Author('Mustafa Elattar'), arxiv.Result.Author('Benard Ohene Botwe'), arxiv.Result.Author('Pempho Nyangulu'), arxiv.Result.Author('William Stones'), arxiv.Result.Author('Mohammed Ammar'), arxiv.Result.Author('Lamya Nawal Benamer'), arxiv.Result.Author('Harriet Nalubega Kisembo'), arxiv.Result.Author('Senai Goitom Sereke'), arxiv.Result.Author('Sikolia Z. Wanyonyi'), arxiv.Result.Author('Marleen Temmerman'), arxiv.Result.Author('Kamil Mikolaj'), arxiv.Result.Author('Martin Grønnebæk Tolsgaard'), arxiv.Result.Author('Karim Lekadir')]","Most artificial intelligence (AI) research have concentrated in high-income
countries, where imaging data, IT infrastructures and clinical expertise are
plentiful. However, slower progress has been made in limited-resource
environments where medical imaging is needed. For example, in Sub-Saharan
Africa the rate of perinatal mortality is very high due to limited access to
antenatal screening. In these countries, AI models could be implemented to help
clinicians acquire fetal ultrasound planes for diagnosis of fetal
abnormalities. So far, deep learning models have been proposed to identify
standard fetal planes, but there is no evidence of their ability to generalise
in centres with limited access to high-end ultrasound equipment and data. This
work investigates different strategies to reduce the domain-shift effect for a
fetal plane classification model trained on a high-resource clinical centre and
transferred to a new low-resource centre. To that end, a classifier trained
with 1,792 patients from Spain is first evaluated on a new centre in Denmark in
optimal conditions with 1,008 patients and is later optimised to reach the same
performance in five African centres (Egypt, Algeria, Uganda, Ghana and Malawi)
with 25 patients each. The results show that a transfer learning approach can
be a solution to integrate small-size African samples with existing large-scale
databases in developed countries. In particular, the model can be re-aligned
and optimised to boost the performance on African populations by increasing the
recall to $0.92 \pm 0.04$ and at the same time maintaining a high precision
across centres. This framework shows promise for building new AI models
generalisable across clinical centres with limited data acquired in challenging
and heterogeneous conditions and calls for further research to develop new
solutions for usability of AI in countries with less resources.",-0.20055963,0.27745336,0.12485561,A
11449,"Moreover, it also en-
courages for further research to develop highly generalisable solutions for usability of AI in countries
with less resources and consequently in higher need of clinical support.","This framework shows promise for generalisability across
multi-centre African datasets with challenging and heterogeneous conditions.","One of the main limitations of the present study is that the datasets obtained in Africa are not
complete, since they are less abundant and more diﬃcult to compile.",2022-09-20 10:56:09+00:00,Generalisability of deep learning models in low-resource imaging settings: A fetal ultrasound study in 5 African countries,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Carla Sendra-Balcells'), arxiv.Result.Author('Víctor M. Campello'), arxiv.Result.Author('Jordina Torrents-Barrena'), arxiv.Result.Author('Yahya Ali Ahmed'), arxiv.Result.Author('Mustafa Elattar'), arxiv.Result.Author('Benard Ohene Botwe'), arxiv.Result.Author('Pempho Nyangulu'), arxiv.Result.Author('William Stones'), arxiv.Result.Author('Mohammed Ammar'), arxiv.Result.Author('Lamya Nawal Benamer'), arxiv.Result.Author('Harriet Nalubega Kisembo'), arxiv.Result.Author('Senai Goitom Sereke'), arxiv.Result.Author('Sikolia Z. Wanyonyi'), arxiv.Result.Author('Marleen Temmerman'), arxiv.Result.Author('Kamil Mikolaj'), arxiv.Result.Author('Martin Grønnebæk Tolsgaard'), arxiv.Result.Author('Karim Lekadir')]","Most artificial intelligence (AI) research have concentrated in high-income
countries, where imaging data, IT infrastructures and clinical expertise are
plentiful. However, slower progress has been made in limited-resource
environments where medical imaging is needed. For example, in Sub-Saharan
Africa the rate of perinatal mortality is very high due to limited access to
antenatal screening. In these countries, AI models could be implemented to help
clinicians acquire fetal ultrasound planes for diagnosis of fetal
abnormalities. So far, deep learning models have been proposed to identify
standard fetal planes, but there is no evidence of their ability to generalise
in centres with limited access to high-end ultrasound equipment and data. This
work investigates different strategies to reduce the domain-shift effect for a
fetal plane classification model trained on a high-resource clinical centre and
transferred to a new low-resource centre. To that end, a classifier trained
with 1,792 patients from Spain is first evaluated on a new centre in Denmark in
optimal conditions with 1,008 patients and is later optimised to reach the same
performance in five African centres (Egypt, Algeria, Uganda, Ghana and Malawi)
with 25 patients each. The results show that a transfer learning approach can
be a solution to integrate small-size African samples with existing large-scale
databases in developed countries. In particular, the model can be re-aligned
and optimised to boost the performance on African populations by increasing the
recall to $0.92 \pm 0.04$ and at the same time maintaining a high precision
across centres. This framework shows promise for building new AI models
generalisable across clinical centres with limited data acquired in challenging
and heterogeneous conditions and calls for further research to develop new
solutions for usability of AI in countries with less resources.",-0.27682847,-0.0002818997,-0.288391,C
11764,"Furthermore, the mathematical proofs of the reconstruc-
tion conditions of deep learning-based CS methods are a problem worthy of
further study.","For future work, we will apply the residual network to obtain the multi-scale
feature information of the image, adopt the weighted fusion method to fuse the
feature information of diﬀerent scales to further improve the quality of image
reconstruction, and move our experiment implementations to the latest deep
learning frameworks.","Acknowledgments

The research work of this paper were supported by the National Natural Sci-
ence Foundation of China (No.",2022-09-28 01:11:56+00:00,Image Compressed Sensing with Multi-scale Dilated Convolutional Neural Network,eess.IV,"['eess.IV', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Zhifeng Wang'), arxiv.Result.Author('Zhenghui Wang'), arxiv.Result.Author('Chunyan Zeng'), arxiv.Result.Author('Yan Yu'), arxiv.Result.Author('Xiangkui Wan')]","Deep Learning (DL) based Compressed Sensing (CS) has been applied for better
performance of image reconstruction than traditional CS methods. However, most
existing DL methods utilize the block-by-block measurement and each measurement
block is restored separately, which introduces harmful blocking effects for
reconstruction. Furthermore, the neuronal receptive fields of those methods are
designed to be the same size in each layer, which can only collect single-scale
spatial information and has a negative impact on the reconstruction process.
This paper proposes a novel framework named Multi-scale Dilated Convolution
Neural Network (MsDCNN) for CS measurement and reconstruction. During the
measurement period, we directly obtain all measurements from a trained
measurement network, which employs fully convolutional structures and is
jointly trained with the reconstruction network from the input image. It
needn't be cut into blocks, which effectively avoids the block effect. During
the reconstruction period, we propose the Multi-scale Feature Extraction (MFE)
architecture to imitate the human visual system to capture multi-scale features
from the same feature map, which enhances the image feature extraction ability
of the framework and improves the performance of image reconstruction. In the
MFE, there are multiple parallel convolution channels to obtain multi-scale
feature information. Then the multi-scale features information is fused and the
original image is reconstructed with high quality. Our experimental results
show that the proposed method performs favorably against the state-of-the-art
methods in terms of PSNR and SSIM.",0.14198354,0.38951796,0.08215931,A
11880,"The table shows           and need for further research targeting energy efﬁciency in
which parameters inﬂuence the energy consumption of the                  online video, which is essential for sustainable deployment of
considered end-user devices, the networks, and the server tasks          online video technology in the future.","These examples show the high potential
of such dependencies is provided in Table VII.",on the provider side.,2022-09-30 12:04:30+00:00,Sweet Streams are Made of This: The System Engineer's View on Energy Efficiency in Video Communications,eess.IV,['eess.IV'],"[arxiv.Result.Author('Christian Herglotz'), arxiv.Result.Author('Matthias Kränzler'), arxiv.Result.Author('Robert Schober'), arxiv.Result.Author('André Kaup')]","In recent years, the global use of online video services has increased
rapidly. Today, a manifold of applications, such as video streaming, video
conferencing, live broadcasting, and social networks, make use of this
technology. A recent study found that the development and the success of these
services had as a consequence that, nowadays, more than 1% of the global
greenhouse-gas emissions are related to online video, with growth rates close
to 10% per year. This article reviews the latest findings concerning energy
consumption of online video from the system engineer's perspective, where the
system engineer is the designer and operator of a typical online video service.
We discuss all relevant energy sinks, highlight dependencies with
quality-of-service variables as well as video properties, review energy
consumption models for different devices from the literature, and aggregate
these existing models into a global model for the overall energy consumption of
a generic online video service. Analyzing this model and its implications, we
find that end-user devices and video encoding have the largest potential for
energy savings. Finally, we provide an overview of recent advances in energy
efficiency improvement for video streaming and propose future research
directions for energy-efficient video streaming services.",0.4238546,-0.2165621,-0.22280465,B
11966,The other employed                                                   to be conﬁrmed by further study.,"However, this needs
MFNet performing the best in most cases.","DL-based method, DVF, received the highest overall DMOS
values, which may be due to the assumption of simple linear                                                       V. EVALUATION OF OBJECTIVE QUALITY METRICS
motion between adjacent frames in the DVF approach, i.e.",2022-10-03 11:15:05+00:00,BVI-VFI: A Video Quality Database for Video Frame Interpolation,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Duolikun Danier'), arxiv.Result.Author('Fan Zhang'), arxiv.Result.Author('David Bull')]","Video frame interpolation (VFI) is a fundamental research topic in video
processing, which is currently attracting increased attention across the
research community. While the development of more advanced VFI algorithms has
been extensively researched, there remains little understanding of how humans
perceive the quality of interpolated content and how well existing objective
quality assessment methods perform when measuring the perceived quality. In
order to narrow this research gap, we have developed a new video quality
database named BVI-VFI, which contains 540 distorted sequences generated by
applying five commonly used VFI algorithms to 36 diverse source videos with
various spatial resolutions and frame rates. We collected more than 10,800
quality ratings for these videos through a large scale subjective study
involving 189 human subjects. Based on the collected subjective scores, we
further analysed the influence of VFI algorithms and frame rates on the
perceptual quality of interpolated videos. Moreover, we benchmarked the
performance of 28 classic and state-of-the-art objective image/video quality
metrics on the new database, and demonstrated the urgent requirement for more
accurate bespoke quality assessment methods for VFI. To facilitate further
research in this area, we have made BVI-VFI publicly available at
https://github.com/danielism97/BVI-VFI-database.",0.3085646,-0.12822872,-0.056892682,B
11967,"The other employed DL-based method,                                                   to be conﬁrmed by further study.","However, this needs
the best in most cases.","DVF, received the highest overall DMOS values, which may
be due to the assumption of simple linear motion between                                                          V. EVALUATION OF OBJECTIVE QUALITY METRICS
adjacent frames in the DVF approach, i.e.",2022-10-03 11:15:05+00:00,BVI-VFI: A Video Quality Database for Video Frame Interpolation,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Duolikun Danier'), arxiv.Result.Author('Fan Zhang'), arxiv.Result.Author('David Bull')]","Video frame interpolation (VFI) is a fundamental research topic in video
processing, which is currently attracting increased attention across the
research community. While the development of more advanced VFI algorithms has
been extensively researched, there remains little understanding of how humans
perceive the quality of interpolated content and how well existing objective
quality assessment methods perform when measuring the perceived quality. In
order to narrow this research gap, we have developed a new video quality
database named BVI-VFI, which contains 540 distorted sequences generated by
applying five commonly used VFI algorithms to 36 diverse source videos with
various spatial resolutions and frame rates. We collected more than 10,800
quality ratings for these videos through a large scale subjective study
involving 189 human subjects. Based on the collected subjective scores, we
further analysed the influence of VFI algorithms and frame rates on the
perceptual quality of interpolated videos. Moreover, we benchmarked the
performance of 28 classic and state-of-the-art objective image/video quality
metrics on the new database, and demonstrated the urgent requirement for more
accurate bespoke quality assessment methods for VFI. To facilitate further
research in this area, we have made BVI-VFI publicly available at
https://github.com/danielism97/BVI-VFI-database.",0.2659769,-0.24356724,-0.0053813476,B
11994,"In the further research, we should explore a new method to generate pseudo labels from the
image itself according to the characteristics of spectral CT, so that the network can learn noise
distribution more effectively.","Third, we use the similarity between adjacent pixels to construct training pairs.","In conclusion, we reconstruct spectral CT by combining deep learning with iterative
reconstruction methods, and the reconstruction performance on narrow energy channels has
been verified.",2022-10-03 03:07:33+00:00,Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep Reconstruction without Reference,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('onghui Li'), arxiv.Result.Author('Peng He'), arxiv.Result.Author('Peng Feng'), arxiv.Result.Author('Xiaodong Guo'), arxiv.Result.Author('Weiwen Wu'), arxiv.Result.Author('Hengyong Yu')]","The photon-counting detector (PCD) based spectral computed tomography
attracts much more attentions since it has the capability to provide more
accurate identification and quantitative analysis for biomedical materials. The
limited number of photons within narrow energy-bin leads to low signal-noise
ratio data. The existing supervised deep reconstruction networks for CT
reconstruction are difficult to address these challenges. In this paper, we
propose an iterative deep reconstruction network to synergize model and data
priors into a unified framework, named as Spectral2Spectral. Our
Spectral2Spectral employs an unsupervised deep training strategy to obtain
high-quality images from noisy data with an end-to-end fashion. The structural
similarity prior within image-spectral domain is refined as a regularization
term to further constrain the network training. The weights of neural network
are automatically updated to capture image features and structures with
iterative process. Three large-scale preclinical datasets experiments
demonstrate that the Spectral2spectral reconstruct better image quality than
other state-of-the-art methods.",0.15125392,0.34936556,0.21993807,A
12059,"RESULTS

We evaluated the models both quantitatively and qualitatively, to account for the diﬀerent aspects which could
be of interest considering the further study of the obtained segmentations.",3.,Table 1.,2022-10-05 17:35:56+00:00,A deep learning model for brain vessel segmentation in 3DRA with arteriovenous malformations,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Camila García'), arxiv.Result.Author('Yibin Fang'), arxiv.Result.Author('Jianmin Liu'), arxiv.Result.Author('Ana Paula Narata'), arxiv.Result.Author('José Ignacio Orlando'), arxiv.Result.Author('Ignacio Larrabide')]","Segmentation of brain arterio-venous malformations (bAVMs) in 3D rotational
angiographies (3DRA) is still an open problem in the literature, with high
relevance for clinical practice. While deep learning models have been applied
for segmenting the brain vasculature in these images, they have never been used
in cases with bAVMs. This is likely caused by the difficulty to obtain
sufficiently annotated data to train these approaches. In this paper we
introduce a first deep learning model for blood vessel segmentation in 3DRA
images of patients with bAVMs. To this end, we densely annotated 5 3DRA volumes
of bAVM cases and used these to train two alternative 3DUNet-based
architectures with different segmentation objectives. Our results show that the
networks reach a comprehensive coverage of relevant structures for bAVM
analysis, much better than what is obtained using standard methods. This is
promising for achieving a better topological and morphological characterisation
of the bAVM structures of interest. Furthermore, the models have the ability to
segment venous structures even when missing in the ground truth labelling,
which is relevant for planning interventional treatments. Ultimately, these
results could be used as more reliable first initial guesses, alleviating the
cumbersome task of creating manual labels.",0.01832421,-0.21583506,-0.12796292,C
12233,"We used two clinical variables, AHT diagnosis and Functional
Status Scale (FSS) score, to arrive at the conclusion that CE is highly correlated with overall outcome and
that further study is needed to determine whether CE is a biomarker of AHT.","In addition, we perform
analysis of our system’s output to determine the association of CE with Abusive Head Trauma (AHT) – a
type of traumatic brain injury (TBI) associated with abuse – and overall functional outcome and in-hospital
mortality of infants and young children.","With that, this paper introduces
a simple yet powerful deep learning-based solution for automated CE classification.",2022-10-06 14:21:44+00:00,Deep Learning Mixture-of-Experts Approach for Cytotoxic Edema Assessment in Infants and Children,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Henok Ghebrechristos'), arxiv.Result.Author('Stence Nicholas'), arxiv.Result.Author('David Mirsky'), arxiv.Result.Author('Gita Alaghband'), arxiv.Result.Author('Manh Huynh'), arxiv.Result.Author('Zackary Kromer'), arxiv.Result.Author('Ligia Batista'), arxiv.Result.Author('Brent ONeill'), arxiv.Result.Author('Steven Moulton'), arxiv.Result.Author('Daniel M. Lindberg')]","This paper presents a deep learning framework for image classification aimed
at increasing predictive performance for Cytotoxic Edema (CE) diagnosis in
infants and children. The proposed framework includes two 3D network
architectures optimized to learn from two types of clinical MRI data , a trace
Diffusion Weighted Image (DWI) and the calculated Apparent Diffusion
Coefficient map (ADC). This work proposes a robust and novel solution based on
volumetric analysis of 3D images (using pixels from time slices) and 3D
convolutional neural network (CNN) models. While simple in architecture, the
proposed framework shows significant quantitative results on the domain
problem. We use a dataset curated from a Childrens Hospital Colorado (CHCO)
patient registry to report a predictive performance F1 score of 0.91 at
distinguishing CE patients from children with severe neurologic injury without
CE. In addition, we perform analysis of our systems output to determine the
association of CE with Abusive Head Trauma (AHT) , a type of traumatic brain
injury (TBI) associated with abuse , and overall functional outcome and in
hospital mortality of infants and young children. We used two clinical
variables, AHT diagnosis and Functional Status Scale (FSS) score, to arrive at
the conclusion that CE is highly correlated with overall outcome and that
further study is needed to determine whether CE is a biomarker of AHT. With
that, this paper introduces a simple yet powerful deep learning based solution
for automated CE classification. This solution also enables an indepth analysis
of progression of CE and its correlation to AHT and overall neurologic outcome,
which in turn has the potential to empower experts to diagnose and mitigate AHT
during early stages of a childs life.",-0.23638651,0.05716627,-0.22466214,A
12234,We believe our work paves the way to further study the stated hypothesis in detail.,"Further study and incorporation of more clinical data and variables is necessary to conclude whether CE is
a biomarker of AHT.","6 FUNDING

This work was supported by the Colorado TBI Trust Fund (MINDSOURCE).",2022-10-06 14:21:44+00:00,Deep Learning Mixture-of-Experts Approach for Cytotoxic Edema Assessment in Infants and Children,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Henok Ghebrechristos'), arxiv.Result.Author('Stence Nicholas'), arxiv.Result.Author('David Mirsky'), arxiv.Result.Author('Gita Alaghband'), arxiv.Result.Author('Manh Huynh'), arxiv.Result.Author('Zackary Kromer'), arxiv.Result.Author('Ligia Batista'), arxiv.Result.Author('Brent ONeill'), arxiv.Result.Author('Steven Moulton'), arxiv.Result.Author('Daniel M. Lindberg')]","This paper presents a deep learning framework for image classification aimed
at increasing predictive performance for Cytotoxic Edema (CE) diagnosis in
infants and children. The proposed framework includes two 3D network
architectures optimized to learn from two types of clinical MRI data , a trace
Diffusion Weighted Image (DWI) and the calculated Apparent Diffusion
Coefficient map (ADC). This work proposes a robust and novel solution based on
volumetric analysis of 3D images (using pixels from time slices) and 3D
convolutional neural network (CNN) models. While simple in architecture, the
proposed framework shows significant quantitative results on the domain
problem. We use a dataset curated from a Childrens Hospital Colorado (CHCO)
patient registry to report a predictive performance F1 score of 0.91 at
distinguishing CE patients from children with severe neurologic injury without
CE. In addition, we perform analysis of our systems output to determine the
association of CE with Abusive Head Trauma (AHT) , a type of traumatic brain
injury (TBI) associated with abuse , and overall functional outcome and in
hospital mortality of infants and young children. We used two clinical
variables, AHT diagnosis and Functional Status Scale (FSS) score, to arrive at
the conclusion that CE is highly correlated with overall outcome and that
further study is needed to determine whether CE is a biomarker of AHT. With
that, this paper introduces a simple yet powerful deep learning based solution
for automated CE classification. This solution also enables an indepth analysis
of progression of CE and its correlation to AHT and overall neurologic outcome,
which in turn has the potential to empower experts to diagnose and mitigate AHT
during early stages of a childs life.",-0.29221526,-0.40732992,-0.14145444,C
12262,"Therefore, with further research, the model can be improved so that
underdeveloped countries without access to quality doctors or nurses can still
receive the healthcare they need in a timely and aﬀordable fashion.","Another important ad-
vancement could focus on applying this technology and methodology to other
lung-based diseases that can be diagnosed through chest x-rays, such as tuber-
culosis.","9
6 Literature Cited

ARASARATNAM, A. and HUMPHREYS, G. Emerging economies drive frugal
innovation.",2022-10-10 21:38:54+00:00,Using Deep Learning to Improve Early Diagnosis of Pneumonia in Underdeveloped Countries,eess.IV,"['eess.IV', 'cs.CV']",[arxiv.Result.Author('Kyler Larsen')],"As advancements in technology and medicine are being made, many countries are
still unable to access quality medical care due to cost and lack of qualified
medical personnel. This discrepancy in healthcare has caused many preventable
deaths, either due to lack of detection or lack of care. One of the most
prevalent diseases in the world is pneumonia, an infection of the lungs that
killed 2.56 million people worldwide in 2017. In this same year, the United
States recorded a pneumonia death rate of 15.88 people per 100000 in
population, while much of Sub-Saharan Africa, such as Chad and Guinea,
experienced death rates of over 150 people per 100000. In sub-Saharan Africa,
there is an extreme shortage of doctors and nurses, estimated to be around 2.4
million. The hypothesis being tested is that a deep learning model can receive
input in the form of an x-ray and produce a diagnosis with the equivalent
accuracy of a physician, compared to a prediagnosed image. The model used in
this project is a modified convolutional neural network. The model was trained
on a set of 2000 x-ray images that have predetermined normal and abnormal lung
findings, and then tested on a set of 400 images that contains evenly split
images of pneumonia and healthy lungs. For each computer-run test, data was
collected on a base measurement of accuracy, as well as more specific metrics
such as specificity and sensitivity. Results show that the algorithm tested was
able to accurately identify abnormal lung findings an average of 82.5% of the
time. The model achieved a maximum specificity of 98.5% and a maximum
sensitivity of 90% separately, and the highest simultaneous values of these two
metrics was a sensitivity of 90% and a specificity of 78.5%. This research can
be further improved by testing other deep learning models as well as machine
learning models to improve the metric scores and chance of correct diagnoses.",-0.2126078,-0.17672545,-0.03543713,C
12269,The dataset and codes will be released to encourage further research1.,"The validity of the dataset was veriﬁed through clinicians’ assessments and
                                          downstream experiments.","Keywords: 3D Matting, Pulmonary nodules, Soft Segmentation, Thoracic CT, Uncertainty
                                          2022 MSC: 00-01, 99-00

                                          Abbreviations and Symbols

                                              The following abbreviations are used in this manuscript:

                                              1Url for codes and dataset: https://github.com/wangsssky/3DMatting.",2022-10-11 02:40:18+00:00,3D Matting: A Benchmark Study on Soft Segmentation Method for Pulmonary Nodules Applied in Computed Tomography,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Lin Wang'), arxiv.Result.Author('Xiufen Ye'), arxiv.Result.Author('Donghao Zhang'), arxiv.Result.Author('Wanji He'), arxiv.Result.Author('Lie Ju'), arxiv.Result.Author('Yi Luo'), arxiv.Result.Author('Huan Luo'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Wei Feng'), arxiv.Result.Author('Kaimin Song'), arxiv.Result.Author('Xin Zhao'), arxiv.Result.Author('Zongyuan Ge')]","Usually, lesions are not isolated but are associated with the surrounding
tissues. For example, the growth of a tumour can depend on or infiltrate into
the surrounding tissues. Due to the pathological nature of the lesions, it is
challenging to distinguish their boundaries in medical imaging. However, these
uncertain regions may contain diagnostic information. Therefore, the simple
binarization of lesions by traditional binary segmentation can result in the
loss of diagnostic information. In this work, we introduce the image matting
into the 3D scenes and use the alpha matte, i.e., a soft mask, to describe
lesions in a 3D medical image. The traditional soft mask acted as a training
trick to compensate for the easily mislabelled or under-labelled ambiguous
regions. In contrast, 3D matting uses soft segmentation to characterize the
uncertain regions more finely, which means that it retains more structural
information for subsequent diagnosis and treatment. The current study of image
matting methods in 3D is limited. To address this issue, we conduct a
comprehensive study of 3D matting, including both traditional and
deep-learning-based methods. We adapt four state-of-the-art 2D image matting
algorithms to 3D scenes and further customize the methods for CT images to
calibrate the alpha matte with the radiodensity. Moreover, we propose the first
end-to-end deep 3D matting network and implement a solid 3D medical image
matting benchmark. Its efficient counterparts are also proposed to achieve a
good performance-computation balance. Furthermore, there is no high-quality
annotated dataset related to 3D matting, slowing down the development of
data-driven deep-learning-based methods. To address this issue, we construct
the first 3D medical matting dataset. The validity of the dataset was verified
through clinicians' assessments and downstream experiments.",-0.27226353,-0.07779202,0.18530026,C
12548,"Given the surprising result with the
first carpal bone, further study should be conducted using the other projections.","We had other projections (dorsopalmar, dorsal
55º lateral to palmaromedial oblique, flexed lateromedial, and flexed dorsal 60º proximal
dorsodistal oblique) and they were not used in this study because the first carpal bone was
harder to detect given bones superimposed or juxtaposed.","Second, we
only used radiographs provided by a single institution.",2022-10-17 04:02:30+00:00,How many radiographs are needed to re-train a deep learning system for object detection?,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Raniere Silva'), arxiv.Result.Author('Khizar Hayat'), arxiv.Result.Author('Christopher M Riggs'), arxiv.Result.Author('Michael Doube')]","Background: Object detection in radiograph computer vision has largely
benefited from progress in deep convolutional neural networks and can, for
example, annotate a radiograph with a box around a knee joint or intervertebral
disc. Is deep learning capable of detect small (less than 1% of the image) in
radiographs? And how many radiographs do we need use when re-training a deep
learning model?
  Methods: We annotated 396 radiographs of left and right carpi dorsal 75
medial to palmarolateral oblique (DMPLO) projection with the location of
radius, proximal row of carpal bones, distal row of carpal bones, accessory
carpal bone, first carpal bone (if present), and metacarpus (metacarpal II,
III, and IV). The radiographs and respective annotations were splited into sets
that were used to leave-one-out cross-validation of models created using
transfer learn from YOLOv5s.
  Results: Models trained using 96 radiographs or more achieved precision,
recall and mAP above 0.95, including for the first carpal bone, when trained
for 32 epochs. The best model needed the double of epochs to learn to detect
the first carpal bone compared with the other bones.
  Conclusions: Free and open source state of the art object detection models
based on deep learning can be re-trained for radiograph computer vision
applications with 100 radiographs and achieved precision, recall and mAP above
0.95.",-0.12497075,-0.41415325,0.15060389,C
13033,"2: An end-to-end pipeline for quantifying LDD in the lumbar spine using Pﬁrrmann grading

    models available to beneﬁt further research.","Corresponding Author: narasimharao.kowlagi@oulu.ﬁ
512 × 512                                                             224 × 224 × 4

                L1-L2                                                                        Grade 2
                L2-L3                                                                        Grade 3
                L3-L4                                                                        Grade 4
                L4-L5                                                                        Grade 5

                   L5-S1                                                             Pfirmmann grade

4 input slices                Segmentation and                        5 cropped
       FPN                    localization results                    disc images

                EfficientNet                                     MLP

Fig.",2.3.,2022-10-26 10:12:21+00:00,A Stronger Baseline For Automatic Pfirrmann Grading Of Lumbar Spine MRI Using Deep Learning,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Narasimharao Kowlagi'), arxiv.Result.Author('Huy Hoang Nguyen'), arxiv.Result.Author('Terence McSweeney'), arxiv.Result.Author('Simo Saarakkala'), arxiv.Result.Author('Juhani määttä'), arxiv.Result.Author('Jaro Karppinen'), arxiv.Result.Author('Aleksei Tiulpin')]","This paper addresses the challenge of grading visual features in lumbar spine
MRI using Deep Learning. Such a method is essential for the automatic
quantification of structural changes in the spine, which is valuable for
understanding low back pain. Multiple recent studies investigated different
architecture designs, and the most recent success has been attributed to the
use of transformer architectures. In this work, we argue that with a well-tuned
three-stage pipeline comprising semantic segmentation, localization, and
classification, convolutional networks outperform the state-of-the-art
approaches. We conducted an ablation study of the existing methods in a
population cohort, and report performance generalization across various
subgroups. Our code is publicly available to advance research on disc
degeneration and low back pain.",-0.123270616,-0.16506514,0.07324962,C
13102,"Then,
                                                              we further study the inﬂuences of the self-attention and
                                                              DwConv layers in the PI block.","Ablation studies of modules and strategies

                                                                 In this section, we ﬁrst investigate the impact of the
                                                              proposed PI block by comparing it against Swin Trans-
                                                              former (ST) block and ConvNeXt (CNX) block.","Next, we demonstrate
                                                              the performance gains of ImageNet-based pre-training
                                                              over training from scratch.",2022-10-27 16:00:04+00:00,UNet-2022: Exploring Dynamics in Non-isomorphic Architecture,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Jiansen Guo'), arxiv.Result.Author('Hong-Yu Zhou'), arxiv.Result.Author('Liansheng Wang'), arxiv.Result.Author('Yizhou Yu')]","Recent medical image segmentation models are mostly hybrid, which integrate
self-attention and convolution layers into the non-isomorphic architecture.
However, one potential drawback of these approaches is that they failed to
provide an intuitive explanation of why this hybrid combination manner is
beneficial, making it difficult for subsequent work to make improvements on top
of them. To address this issue, we first analyze the differences between the
weight allocation mechanisms of the self-attention and convolution. Based on
this analysis, we propose to construct a parallel non-isomorphic block that
takes the advantages of self-attention and convolution with simple
parallelization. We name the resulting U-shape segmentation model as UNet-2022.
In experiments, UNet-2022 obviously outperforms its counterparts in a range
segmentation tasks, including abdominal multi-organ segmentation, automatic
cardiac diagnosis, neural structures segmentation, and skin lesion
segmentation, sometimes surpassing the best performing baseline by 4%.
Specifically, UNet-2022 surpasses nnUNet, the most recognized segmentation
model at present, by large margins. These phenomena indicate the potential of
UNet-2022 to become the model of choice for medical image segmentation.",0.13355975,0.24288163,-0.14164516,A
13294,"We believe that our
2a) reduce noise compared to the noisy low-dose image and       released open-source helical CT rebinning and differentiable
outperform the respective model only using a post-processing    reconstruction framework can enable further research on self-
denoising operator (1c, 2c).","strate quantitatively and qualitatively that dual-domain de-
The dual-domain models employing denoising operators in         noising is beneﬁcial over solely reconstruction image denois-
both the projection and image domain simultaneously (1a,        ing as conducted in many recent works.",supervised and dual-domain CT pipelines.,2022-11-02 13:37:59+00:00,On the Benefit of Dual-domain Denoising in a Self-supervised Low-dose CT Setting,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Fabian Wagner'), arxiv.Result.Author('Mareike Thies'), arxiv.Result.Author('Laura Pfaff'), arxiv.Result.Author('Oliver Aust'), arxiv.Result.Author('Sabrina Pechmann'), arxiv.Result.Author('Daniela Weidner'), arxiv.Result.Author('Noah Maul'), arxiv.Result.Author('Maximilian Rohleder'), arxiv.Result.Author('Mingxuan Gu'), arxiv.Result.Author('Jonas Utz'), arxiv.Result.Author('Felix Denzinger'), arxiv.Result.Author('Andreas Maier')]","Computed tomography (CT) is routinely used for three-dimensional non-invasive
imaging. Numerous data-driven image denoising algorithms were proposed to
restore image quality in low-dose acquisitions. However, considerably less
research investigates methods already intervening in the raw detector data due
to limited access to suitable projection data or correct reconstruction
algorithms. In this work, we present an end-to-end trainable CT reconstruction
pipeline that contains denoising operators in both the projection and the image
domain and that are optimized simultaneously without requiring ground-truth
high-dose CT data. Our experiments demonstrate that including an additional
projection denoising operator improved the overall denoising performance by
82.4-94.1%/12.5-41.7% (PSNR/SSIM) on abdomen CT and 1.5-2.9%/0.4-0.5%
(PSNR/SSIM) on XRM data relative to the low-dose baseline. We make our entire
helical CT reconstruction framework publicly available that contains a raw
projection rebinning step to render helical projection data suitable for
differentiable fan-beam reconstruction operators and end-to-end learning.",0.11051563,0.13376728,0.39879656,A
13295,"We believe that our
the distinct noise distribution in the projection data, which   released open-source helical CT rebinning and differentiable
constitutes a considerably easier denoising task compared       reconstruction framework can enable further research on self-
to complex noise removal on the reconstruction.","noising is beneﬁcial over solely reconstruction image denois-
The beneﬁt of projection denoising can be explained through     ing as conducted in many recent works.",We hope         supervised and dual-domain CT pipelines.,2022-11-02 13:37:59+00:00,On the Benefit of Dual-domain Denoising in a Self-supervised Low-dose CT Setting,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Fabian Wagner'), arxiv.Result.Author('Mareike Thies'), arxiv.Result.Author('Laura Pfaff'), arxiv.Result.Author('Oliver Aust'), arxiv.Result.Author('Sabrina Pechmann'), arxiv.Result.Author('Daniela Weidner'), arxiv.Result.Author('Noah Maul'), arxiv.Result.Author('Maximilian Rohleder'), arxiv.Result.Author('Mingxuan Gu'), arxiv.Result.Author('Jonas Utz'), arxiv.Result.Author('Felix Denzinger'), arxiv.Result.Author('Andreas Maier')]","Computed tomography (CT) is routinely used for three-dimensional non-invasive
imaging. Numerous data-driven image denoising algorithms were proposed to
restore image quality in low-dose acquisitions. However, considerably less
research investigates methods already intervening in the raw detector data due
to limited access to suitable projection data or correct reconstruction
algorithms. In this work, we present an end-to-end trainable CT reconstruction
pipeline that contains denoising operators in both the projection and the image
domain and that are optimized simultaneously without requiring ground-truth
high-dose CT data. Our experiments demonstrate that including an additional
projection denoising operator improved the overall denoising performance by
82.4-94.1%/12.5-41.7% (PSNR/SSIM) on abdomen CT and 1.5-2.9%/0.4-0.5%
(PSNR/SSIM) on XRM data relative to the low-dose baseline. We make our entire
helical CT reconstruction framework publicly available that contains a raw
projection rebinning step to render helical projection data suitable for
differentiable fan-beam reconstruction operators and end-to-end learning.",0.13597079,0.114694685,0.46884954,A
13388,The clearance of contrast from the feto-placental system still requires further research.,"Nonetheless, this technique has signiﬁcant drawbacks as it requires an exogenous contrast.","To
that end, an imaging technique which does not include any safety concerns for the mother
and fetus is more pertinent.",2022-11-01 08:41:30+00:00,PIPPI2021: An Approach to Automated Diagnosis and Texture Analysis of the Fetal Liver & Placenta in Fetal Growth Restriction,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Aya Mutaz Zeidan'), arxiv.Result.Author('Paula Ramirez Gilliland'), arxiv.Result.Author('Ashay Patel'), arxiv.Result.Author('Zhanchong Ou'), arxiv.Result.Author('Dimitra Flouri'), arxiv.Result.Author('Nada Mufti'), arxiv.Result.Author('Kasia Maksym'), arxiv.Result.Author('Rosalind Aughwane'), arxiv.Result.Author('Sebastien Ourselin'), arxiv.Result.Author('Anna David'), arxiv.Result.Author('Andrew Melbourne')]","Fetal growth restriction (FGR) is a prevalent pregnancy condition
characterised by failure of the fetus to reach its genetically predetermined
growth potential. We explore the application of model fitting techniques,
linear regression machine learning models, deep learning regression, and
Haralick textured features from multi-contrast MRI for multi-fetal organ
analysis of FGR. We employed T2 relaxometry and diffusion-weighted MRI datasets
(using a combined T2-diffusion scan) for 12 normally grown and 12 FGR
gestational age (GA) matched pregnancies. We applied the Intravoxel Incoherent
Motion Model and novel multi-compartment models for MRI fetal analysis, which
exhibit potential to provide a multi-organ FGR assessment, overcoming the
limitations of empirical indicators - such as abnormal artery Doppler findings
- to evaluate placental dysfunction. The placenta and fetal liver presented key
differentiators between FGR and normal controls (decreased perfusion, abnormal
fetal blood motion and reduced fetal blood oxygenation. This may be associated
with the preferential shunting of the fetal blood towards the fetal brain.
These features were further explored to determine their role in assessing FGR
severity, by employing simple machine learning models to predict FGR diagnosis
(100\% accuracy in test data, n=5), GA at delivery, time from MRI scan to
delivery, and baby weight. Moreover, we explored the use of deep learning to
regress the latter three variables. Image texture analysis of the fetal organs
demonstrated prominent textural variations in the placental perfusion fractions
maps between the groups (p$<$0.0009), and spatial differences in the incoherent
fetal capillary blood motion in the liver (p$<$0.009). This research serves as
a proof-of-concept, investigating the effect of FGR on fetal organs.",-0.07694232,-0.27938598,0.32226497,C
13402,This is an interesting topic for further study.,"Such problem can be possibly
resolved by optimizing the PTQ for all layers at once which however may need excessive amount of
computations.",We notice that Sun et al.,2022-11-05 08:50:29+00:00,Rate-Distortion Optimized Post-Training Quantization for Learned Image Compression,eess.IV,['eess.IV'],"[arxiv.Result.Author('Junqi Shi'), arxiv.Result.Author('Ming Lu'), arxiv.Result.Author('Fangdong Chen'), arxiv.Result.Author('Shiliang Pu'), arxiv.Result.Author('Zhan Ma')]","Quantizing floating-point neural network to its fixed-point representation is
crucial for Learned Image Compression (LIC) because it ensures the decoding
consistency for interoperability and reduces space-time complexity for
implementation. Existing solutions often have to retrain the network for model
quantization which is time consuming and impractical. This work suggests the
use of Post-Training Quantization (PTQ) to directly process pretrained,
off-the-shelf LIC models. We theoretically prove that minimizing the mean
squared error (MSE) in PTQ is sub-optimal for compression task and thus develop
a novel Rate-Distortion (R-D) Optimized PTQ (RDO-PTQ) to best retain the
compression performance. Such RDO-PTQ just needs to compress few images (e.g.,
10) to optimize the transformation of weight, bias, and activation of
underlying LIC model from its native 32-bit floating-point (FP32) format to
8-bit fixed-point (INT8) precision for fixed-point inference onwards.
Experiments reveal outstanding efficiency of the proposed method on different
LICs, showing the closest coding performance to their floating-point
counterparts. And, our method is a lightweight and plug-and-play approach
without any need of model retraining which is attractive to practitioners.",0.30360305,-0.08031662,-0.107127205,B
13403,"An interesting topic
for further study is to reduce the quantization loss at high bitrates.","More importantly, our method did not require
any model retraining, and offered a push-bottom solution for all existing LICs.","9
REFERENCES

Johannes Balle´, Nick Johnston, and David Minnen.",2022-11-05 08:50:29+00:00,Rate-Distortion Optimized Post-Training Quantization for Learned Image Compression,eess.IV,['eess.IV'],"[arxiv.Result.Author('Junqi Shi'), arxiv.Result.Author('Ming Lu'), arxiv.Result.Author('Fangdong Chen'), arxiv.Result.Author('Shiliang Pu'), arxiv.Result.Author('Zhan Ma')]","Quantizing floating-point neural network to its fixed-point representation is
crucial for Learned Image Compression (LIC) because it ensures the decoding
consistency for interoperability and reduces space-time complexity for
implementation. Existing solutions often have to retrain the network for model
quantization which is time consuming and impractical. This work suggests the
use of Post-Training Quantization (PTQ) to directly process pretrained,
off-the-shelf LIC models. We theoretically prove that minimizing the mean
squared error (MSE) in PTQ is sub-optimal for compression task and thus develop
a novel Rate-Distortion (R-D) Optimized PTQ (RDO-PTQ) to best retain the
compression performance. Such RDO-PTQ just needs to compress few images (e.g.,
10) to optimize the transformation of weight, bias, and activation of
underlying LIC model from its native 32-bit floating-point (FP32) format to
8-bit fixed-point (INT8) precision for fixed-point inference onwards.
Experiments reveal outstanding efficiency of the proposed method on different
LICs, showing the closest coding performance to their floating-point
counterparts. And, our method is a lightweight and plug-and-play approach
without any need of model retraining which is attractive to practitioners.",0.4915495,0.01690921,-0.16963062,B
13413,"These results have paved the way              learning models in classifying stall and non-stall brain capil-
                                         for further research to improve the memory functions of an             laries.","Experiments             we propose a pipeline of data-speciﬁc preprocessing and mul-
                                         showed that the memory function of the mouse improved as               timodal fusion techniques to improve the performance of deep
                                         blood ﬂow increased [3].",Our contributions here are three-fold.,2022-11-06 13:22:05+00:00,A Sequence Agnostic Multimodal Preprocessing for Clogged Blood Vessel Detection in Alzheimer's Diagnosis,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Partho Ghosh'), arxiv.Result.Author('Md. Abrar Istiak'), arxiv.Result.Author('Mir Sayeed Mohammad'), arxiv.Result.Author('Swapnil Saha'), arxiv.Result.Author('Uday Kamal')]","Successful identification of blood vessel blockage is a crucial step for
Alzheimer's disease diagnosis. These blocks can be identified from the spatial
and time-depth variable Two-Photon Excitation Microscopy (TPEF) images of the
brain blood vessels using machine learning methods. In this study, we propose
several preprocessing schemes to improve the performance of these methods. Our
method includes 3D-point cloud data extraction from image modality and their
feature-space fusion to leverage complementary information inherent in
different modalities. We also enforce the learned representation to be
sequence-order invariant by utilizing bi-direction dataflow. Experimental
results on The Clog Loss dataset show that our proposed method consistently
outperforms the state-of-the-art preprocessing methods in stalled and
non-stalled vessel classification.",-0.10871135,0.060090017,-0.17813072,A
13534,"The implementation code and volunteer data1 will be made pub-
                                                  licly available ensuring reproducibility and further research.","Interestingly, with the proposed approach,
                                                  explicit within-sequence loss that encourages consistency in composing
                                                  transformations or minimises accumulated error may no longer be re-
                                                  quired.","Keywords: 3D freehand US, transformation estimation, multi-task learning,
                                         sequence encoding

                                         1 https://github.com/ucl-candi/freehand
2  Q. Li et al.",2022-11-09 13:18:35+00:00,Trackerless freehand ultrasound with sequence modelling and auxiliary transformation over past and future frames,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Qi Li'), arxiv.Result.Author('Ziyi Shen'), arxiv.Result.Author('Qian Li'), arxiv.Result.Author('Dean C Barratt'), arxiv.Result.Author('Thomas Dowrick'), arxiv.Result.Author('Matthew J Clarkson'), arxiv.Result.Author('Tom Vercauteren'), arxiv.Result.Author('Yipeng Hu')]","Three-dimensional (3D) freehand ultrasound (US) reconstruction without a
tracker can be advantageous over its two-dimensional or tracked counterparts in
many clinical applications. In this paper, we propose to estimate 3D spatial
transformation between US frames from both past and future 2D images, using
feed-forward and recurrent neural networks (RNNs). With the temporally
available frames, a further multi-task learning algorithm is proposed to
utilise a large number of auxiliary transformation-predicting tasks between
them. Using more than 40,000 US frames acquired from 228 scans on 38 forearms
of 19 volunteers in a volunteer study, the hold-out test performance is
quantified by frame prediction accuracy, volume reconstruction overlap,
accumulated tracking error and final drift, based on ground-truth from an
optical tracker. The results show the importance of modelling the
temporal-spatially correlated input frames as well as output transformations,
with further improvement owing to additional past and/or future frames. The
best performing model was associated with predicting transformation between
moderately-spaced frames, with an interval of less than ten frames at 20 frames
per second (fps). Little benefit was observed by adding frames more than one
second away from the predicted transformation, with or without LSTM-based RNNs.
Interestingly, with the proposed approach, explicit within-sequence loss that
encourages consistency in composing transformations or minimises accumulated
error may no longer be required. The implementation code and volunteer data
will be made publicly available ensuring reproducibility and further research.",0.13348307,0.18310893,-0.17325446,A
13635,"In
further research, generative adversarial networks (GAN) can be used for generative modelling.","Implementation
of this classiﬁcation can ease the patients with PCO such that their number of hospital visit will be minimized.","GAN is capable of
learning the regularities and patterns of input data and generating new data using generator and discriminator model
[26].",2022-11-11 10:36:42+00:00,Treatment classification of posterior capsular opacification (PCO) using automated ground truths,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Raisha Shrestha'), arxiv.Result.Author('Waree Kongprawechnon'), arxiv.Result.Author('Teesid Leelasawassuk'), arxiv.Result.Author('Nattapon Wongcumchang'), arxiv.Result.Author('Oliver Findl'), arxiv.Result.Author('Nino Hirnschall')]","Determination of treatment need of posterior capsular opacification (PCO)--
one of the most common complication of cataract surgery -- is a difficult
process due to its local unavailability and the fact that treatment is provided
only after PCO occurs in the central visual axis. In this paper we propose a
deep learning (DL)-based method to first segment PCO images then classify the
images into \textit{treatment required} and \textit{not yet required} cases in
order to reduce frequent hospital visits. To train the model, we prepare a
training image set with ground truths (GT) obtained from two strategies: (i)
manual and (ii) automated. So, we have two models: (i) Model 1 (trained with
image set containing manual GT) (ii) Model 2 (trained with image set containing
automated GT). Both models when evaluated on validation image set gave Dice
coefficient value greater than 0.8 and intersection-over-union (IoU) score
greater than 0.67 in our experiments. Comparison between gold standard GT and
segmented results from our models gave a Dice coefficient value greater than
0.7 and IoU score greater than 0.6 for both the models showing that automated
ground truths can also result in generation of an efficient model. Comparison
between our classification result and clinical classification shows 0.98
F2-score for outputs from both the models.",-0.1365056,0.3481897,-0.107485265,A
13646,"We hope our work inspires further research in the ﬁeld of uncertainty and out of
distribution detection particularly in the ﬁeld of medical imaging.","We see multiple avenues of future research including fast prediction of uncertainty estimates, further
evaluation of the quality of uncertainty, and evaluating these methods on more realistic medical
imaging settings.","References

[1] Karim Armanious, Chenming Jiang, Sherif Abdulatif, Thomas Küstner, Sergios Gatidis, and
     Bin Yang.",2022-11-11 14:45:16+00:00,Disentangled Uncertainty and Out of Distribution Detection in Medical Generative Models,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Kumud Lakara'), arxiv.Result.Author('Matias Valdenegro-Toro')]","Trusting the predictions of deep learning models in safety critical settings
such as the medical domain is still not a viable option. Distentangled
uncertainty quantification in the field of medical imaging has received little
attention. In this paper, we study disentangled uncertainties in image to image
translation tasks in the medical domain. We compare multiple uncertainty
quantification methods, namely Ensembles, Flipout, Dropout, and DropConnect,
while using CycleGAN to convert T1-weighted brain MRI scans to T2-weighted
brain MRI scans. We further evaluate uncertainty behavior in the presence of
out of distribution data (Brain CT and RGB Face Images), showing that epistemic
uncertainty can be used to detect out of distribution inputs, which should
increase reliability of model outputs.",-0.1044493,-0.06153837,0.35610506,C
13761,"Still, a limited number of studies
address this concern (see Section 3.2), rendering it suitable for further research.","In addition, due to the nature of diffusion
models, they are a strong candidate for denoising medical images in diverse modalities.","For clariﬁcation, sonography uses a high
frequency to view inside a body, thus the transmitted ultrasound beams weaken as they penetrate the tissue.",2022-11-14 23:50:52+00:00,Diffusion Models for Medical Image Analysis: A Comprehensive Survey,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Amirhossein Kazerouni'), arxiv.Result.Author('Ehsan Khodapanah Aghdam'), arxiv.Result.Author('Moein Heidari'), arxiv.Result.Author('Reza Azad'), arxiv.Result.Author('Mohsen Fayyaz'), arxiv.Result.Author('Ilker Hacihaliloglu'), arxiv.Result.Author('Dorit Merhof')]","Denoising diffusion models, a class of generative models, have garnered
immense interest lately in various deep-learning problems. A diffusion
probabilistic model defines a forward diffusion stage where the input data is
gradually perturbed over several steps by adding Gaussian noise and then learns
to reverse the diffusion process to retrieve the desired noise-free data from
noisy data samples. Diffusion models are widely appreciated for their strong
mode coverage and quality of the generated samples despite their known
computational burdens. Capitalizing on the advances in computer vision, the
field of medical imaging has also observed a growing interest in diffusion
models. To help the researcher navigate this profusion, this survey intends to
provide a comprehensive overview of diffusion models in the discipline of
medical image analysis. Specifically, we introduce the solid theoretical
foundation and fundamental concepts behind diffusion models and the three
generic diffusion modelling frameworks: diffusion probabilistic models,
noise-conditioned score networks, and stochastic differential equations. Then,
we provide a systematic taxonomy of diffusion models in the medical domain and
propose a multi-perspective categorization based on their application, imaging
modality, organ of interest, and algorithms. To this end, we cover extensive
applications of diffusion models in the medical domain. Furthermore, we
emphasize the practical use case of some selected approaches, and then we
discuss the limitations of the diffusion models in the medical domain and
propose several directions to fulfill the demands of this field. Finally, we
gather the overviewed studies with their available open-source implementations
at
https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.",-0.053330988,-0.20928815,0.39740646,C
13882,"We publicly share source code to facilitate               channel coding (JSCC) schemes for many decades [4]–[6],
                                          further research and reproducibility.","the reconstructed images compared to orthogonal transmission
                                          employing current DeepJSCC approaches particularly for low                     Although researchers have been investigating joint source-
                                          bandwidth ratios.","these schemes did not ﬁnd application in practical systems due
                                                                                                                      to their high complexity, and poor performance with practical
                                             Index Terms—Joint source-channel coding, non-orthogonal                  sources and channel distributions.",2022-11-17 22:36:03+00:00,Distributed Deep Joint Source-Channel Coding over a Multiple Access Channel,eess.IV,"['eess.IV', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT']","[arxiv.Result.Author('Selim F. Yilmaz'), arxiv.Result.Author('Can Karamanli'), arxiv.Result.Author('Deniz Gunduz')]","We consider distributed image transmission over a noisy multiple access
channel (MAC) using deep joint source-channel coding (DeepJSCC). It is known
that Shannon's separation theorem holds when transmitting independent sources
over a MAC in the asymptotic infinite block length regime. However, we are
interested in the practical finite block length regime, in which case separate
source and channel coding is known to be suboptimal. We introduce a novel joint
image compression and transmission scheme, where the devices send their
compressed image representations in a non-orthogonal manner. While
non-orthogonal multiple access (NOMA) is known to achieve the capacity region,
to the best of our knowledge, non-orthogonal joint source channel coding (JSCC)
scheme for practical systems has not been studied before. Through extensive
experiments, we show significant improvements in terms of the quality of the
reconstructed images compared to orthogonal transmission employing current
DeepJSCC approaches particularly for low bandwidth ratios. We publicly share
source code to facilitate further research and reproducibility.",0.50140506,0.04053126,-0.016691845,B
13883,"Therefore, it is inherently a MTL
   5) To facilitate further research and reproducibility, we        problem.","are superposed onto a joint representation, and the decoder
                                                                    reconstructs multiple images.","In distributed compression [19], typically different
       provide the source code of our framework and simulations     encoding and decoding functions are employed for each of the
       on github.com/ipc-lab/deepjscc-noma.",2022-11-17 22:36:03+00:00,Distributed Deep Joint Source-Channel Coding over a Multiple Access Channel,eess.IV,"['eess.IV', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT']","[arxiv.Result.Author('Selim F. Yilmaz'), arxiv.Result.Author('Can Karamanli'), arxiv.Result.Author('Deniz Gunduz')]","We consider distributed image transmission over a noisy multiple access
channel (MAC) using deep joint source-channel coding (DeepJSCC). It is known
that Shannon's separation theorem holds when transmitting independent sources
over a MAC in the asymptotic infinite block length regime. However, we are
interested in the practical finite block length regime, in which case separate
source and channel coding is known to be suboptimal. We introduce a novel joint
image compression and transmission scheme, where the devices send their
compressed image representations in a non-orthogonal manner. While
non-orthogonal multiple access (NOMA) is known to achieve the capacity region,
to the best of our knowledge, non-orthogonal joint source channel coding (JSCC)
scheme for practical systems has not been studied before. Through extensive
experiments, we show significant improvements in terms of the quality of the
reconstructed images compared to orthogonal transmission employing current
DeepJSCC approaches particularly for low bandwidth ratios. We publicly share
source code to facilitate further research and reproducibility.",0.3902011,0.17943299,-0.049459606,B
13907,"Moreover, attention mechanism was demonstrated with potential in
segmentation when combining with convolutional networks [8, 11], which de-
serves further research.","[24] introduced the squeeze & excitation
structure to nnU-Net and successfully improved their performance in last year’s
challenge.","We recognize that our pseudo labelling approach is not
perfect despite slight improved performance in testing, since combined original
training and predicted testing set will deepen model learning and introduce in-
correct information from wrong cases in testing set.",2022-11-18 10:31:26+00:00,Joint nnU-Net and Radiomics Approaches for Segmentation and Prognosis of Head and Neck Cancers with PET/CT images,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hui Xu'), arxiv.Result.Author('Yihao Li'), arxiv.Result.Author('Wei Zhao'), arxiv.Result.Author('Gwenolé Quellec'), arxiv.Result.Author('Lijun Lu'), arxiv.Result.Author('Mathieu Hatt')]","Automatic segmentation of head and neck cancer (HNC) tumors and lymph nodes
plays a crucial role in the optimization treatment strategy and prognosis
analysis. This study aims to employ nnU-Net for automatic segmentation and
radiomics for recurrence-free survival (RFS) prediction using pretreatment
PET/CT images in multi-center HNC cohort. A multi-center HNC dataset with 883
patients (524 patients for training, 359 for testing) was provided in HECKTOR
2022. A bounding box of the extended oropharyngeal region was retrieved for
each patient with fixed size of 224 x 224 x 224 $mm^{3}$. Then 3D nnU-Net
architecture was adopted to automatic segmentation of primary tumor and lymph
nodes synchronously.Based on predicted segmentation, ten conventional features
and 346 standardized radiomics features were extracted for each patient. Three
prognostic models were constructed containing conventional and radiomics
features alone, and their combinations by multivariate CoxPH modelling. The
statistical harmonization method, ComBat, was explored towards reducing
multicenter variations. Dice score and C-index were used as evaluation metrics
for segmentation and prognosis task, respectively. For segmentation task, we
achieved mean dice score around 0.701 for primary tumor and lymph nodes by 3D
nnU-Net. For prognostic task, conventional and radiomics models obtained the
C-index of 0.658 and 0.645 in the test set, respectively, while the combined
model did not improve the prognostic performance with the C-index of 0.648.",-0.027588274,0.343853,-0.22119069,A
13908,"The further study aimed to explore model
performance with and without harmonization need to be performed in further
nnU-Net and radiomics for segmentation and prognosis  11

work.","Given the limitation
of submitted models, we cannot provide an explicit comparison to demonstrate
the eﬀect of Combat harmonization.","Radiomics features without harmonization combined with two conven-
tional features above did not improve the performance in testing that may be
explained by the obstacle of high sensitivity of radiomics features.",2022-11-18 10:31:26+00:00,Joint nnU-Net and Radiomics Approaches for Segmentation and Prognosis of Head and Neck Cancers with PET/CT images,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Hui Xu'), arxiv.Result.Author('Yihao Li'), arxiv.Result.Author('Wei Zhao'), arxiv.Result.Author('Gwenolé Quellec'), arxiv.Result.Author('Lijun Lu'), arxiv.Result.Author('Mathieu Hatt')]","Automatic segmentation of head and neck cancer (HNC) tumors and lymph nodes
plays a crucial role in the optimization treatment strategy and prognosis
analysis. This study aims to employ nnU-Net for automatic segmentation and
radiomics for recurrence-free survival (RFS) prediction using pretreatment
PET/CT images in multi-center HNC cohort. A multi-center HNC dataset with 883
patients (524 patients for training, 359 for testing) was provided in HECKTOR
2022. A bounding box of the extended oropharyngeal region was retrieved for
each patient with fixed size of 224 x 224 x 224 $mm^{3}$. Then 3D nnU-Net
architecture was adopted to automatic segmentation of primary tumor and lymph
nodes synchronously.Based on predicted segmentation, ten conventional features
and 346 standardized radiomics features were extracted for each patient. Three
prognostic models were constructed containing conventional and radiomics
features alone, and their combinations by multivariate CoxPH modelling. The
statistical harmonization method, ComBat, was explored towards reducing
multicenter variations. Dice score and C-index were used as evaluation metrics
for segmentation and prognosis task, respectively. For segmentation task, we
achieved mean dice score around 0.701 for primary tumor and lymph nodes by 3D
nnU-Net. For prognostic task, conventional and radiomics models obtained the
C-index of 0.658 and 0.645 in the test set, respectively, while the combined
model did not improve the prognostic performance with the C-index of 0.648.",-0.17327935,0.0044382736,-0.12872845,C
13952,"The network can process the original picture      further study was focused on detecting HER2 from tissue
without laborious image preparation.","Hence,
architectures.","This paper presents a       slices to mimic the typical HER2 status detection method
modified deep learning architecture based on ImageNet weight     better in [21-27].",2022-11-19 13:09:14+00:00,convoHER2: A Deep Neural Network for Multi-Stage Classification of HER2 Breast Cancer,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('M. F. Mridha'), arxiv.Result.Author('Md. Kishor Morol'), arxiv.Result.Author('Md. Asraf Ali'), arxiv.Result.Author('Md Sakib Hossain Shovon')]","Generally, human epidermal growth factor 2 (HER2) breast cancer is more
aggressive than other kinds of breast cancer. Currently, HER2 breast cancer is
detected using expensive medical tests are most expensive. Therefore, the aim
of this study was to develop a computational model named convoHER2 for
detecting HER2 breast cancer with image data using convolution neural network
(CNN). Hematoxylin and eosin (H&E) and immunohistochemical (IHC) stained images
has been used as raw data from the Bayesian information criterion (BIC)
benchmark dataset. This dataset consists of 4873 images of H&E and IHC. Among
all images of the dataset, 3896 and 977 images are applied to train and test
the convoHER2 model, respectively. As all the images are in high resolution, we
resize them so that we can feed them in our convoHER2 model. The cancerous
samples images are classified into four classes based on the stage of the
cancer (0+, 1+, 2+, 3+). The convoHER2 model is able to detect HER2 cancer and
its grade with accuracy 85% and 88% using H&E images and IHC images,
respectively. The outcomes of this study determined that the HER2 cancer
detecting rates of the convoHER2 model are much enough to provide better
diagnosis to the patient for recovering their HER2 breast cancer in future.",-0.092742905,0.35835785,0.04710454,A
14025,"This
finding deserves further study with a larger sample size.","These include neurological conditions history, psychological/psychiatric conditions.","Statistical comparison of the prediction
results obtained from different feature sets provides insights about the importance and
effectiveness of features.",2022-11-18 19:23:00+00:00,Towards Automatic Prediction of Outcome in Treatment of Cerebral Aneurysms,eess.IV,"['eess.IV', 'cs.CV', 'physics.med-ph']","[arxiv.Result.Author('Ashutosh Jadhav'), arxiv.Result.Author('Satyananda Kashyap'), arxiv.Result.Author('Hakan Bulu'), arxiv.Result.Author('Ronak Dholakia'), arxiv.Result.Author('Amon Y. Liu'), arxiv.Result.Author('Tanveer Syeda-Mahmood'), arxiv.Result.Author('William R. Patterson'), arxiv.Result.Author('Hussain Rangwala'), arxiv.Result.Author('Mehdi Moradi')]","Intrasaccular flow disruptors treat cerebral aneurysms by diverting the blood
flow from the aneurysm sac. Residual flow into the sac after the intervention
is a failure that could be due to the use of an undersized device, or to
vascular anatomy and clinical condition of the patient. We report a machine
learning model based on over 100 clinical and imaging features that predict the
outcome of wide-neck bifurcation aneurysm treatment with an intravascular
embolization device. We combine clinical features with a diverse set of common
and novel imaging measurements within a random forest model. We also develop
neural network segmentation algorithms in 2D and 3D to contour the sac in
angiographic images and automatically calculate the imaging features. These
deliver 90% overlap with manual contouring in 2D and 83% in 3D. Our predictive
model classifies complete vs. partial occlusion outcomes with an accuracy of
75.31%, and weighted F1-score of 0.74.",-0.2494653,-0.16231295,-0.27604297,C
14133,"To promote and enable further research in this domain, we have published the ACROBAT (AutomatiC
Registration Of Breast cAncer Tissue) data set15, which consists of 4,212 WSIs from 1,153 female
primary breast cancer patients.","However, there is currently a lack of publicly available data sets that
include WSIs from H&E-stained tissue sections with matched IHC-stained tissue from the same
tumour, despite the importance of IHC for pathological diagnosis.","For each patient, the data set contains one WSI of H&E stained tissue
and up to four WSIs with tissue that was stained with the routine diagnostic IHC markers ER, PGR,
HER2 and KI67.",2022-11-24 14:16:36+00:00,ACROBAT -- a multi-stain breast cancer histological whole-slide-image data set from routine diagnostics for computational pathology,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Philippe Weitz'), arxiv.Result.Author('Masi Valkonen'), arxiv.Result.Author('Leslie Solorzano'), arxiv.Result.Author('Circe Carr'), arxiv.Result.Author('Kimmo Kartasalo'), arxiv.Result.Author('Constance Boissin'), arxiv.Result.Author('Sonja Koivukoski'), arxiv.Result.Author('Aino Kuusela'), arxiv.Result.Author('Dusan Rasic'), arxiv.Result.Author('Yanbo Feng'), arxiv.Result.Author('Sandra Kristiane Sinius Pouplier'), arxiv.Result.Author('Abhinav Sharma'), arxiv.Result.Author('Kajsa Ledesma Eriksson'), arxiv.Result.Author('Leena Latonen'), arxiv.Result.Author('Anne-Vibeke Laenkholm'), arxiv.Result.Author('Johan Hartman'), arxiv.Result.Author('Pekka Ruusuvuori'), arxiv.Result.Author('Mattias Rantalainen')]","The analysis of FFPE tissue sections stained with haematoxylin and eosin
(H&E) or immunohistochemistry (IHC) is an essential part of the pathologic
assessment of surgically resected breast cancer specimens. IHC staining has
been broadly adopted into diagnostic guidelines and routine workflows to
manually assess status and scoring of several established biomarkers, including
ER, PGR, HER2 and KI67. However, this is a task that can also be facilitated by
computational pathology image analysis methods. The research in computational
pathology has recently made numerous substantial advances, often based on
publicly available whole slide image (WSI) data sets. However, the field is
still considerably limited by the sparsity of public data sets. In particular,
there are no large, high quality publicly available data sets with WSIs of
matching IHC and H&E-stained tissue sections. Here, we publish the currently
largest publicly available data set of WSIs of tissue sections from surgical
resection specimens from female primary breast cancer patients with matched
WSIs of corresponding H&E and IHC-stained tissue, consisting of 4,212 WSIs from
1,153 patients. The primary purpose of the data set was to facilitate the
ACROBAT WSI registration challenge, aiming at accurately aligning H&E and IHC
images. For research in the area of image registration, automatic quantitative
feedback on registration algorithm performance remains available through the
ACROBAT challenge website, based on more than 37,000 manually annotated
landmark pairs from 13 annotators. Beyond registration, this data set has the
potential to enable many different avenues of computational pathology research,
including stain-guided learning, virtual staining, unsupervised pre-training,
artefact detection and stain-independent models.",-0.24035627,-0.17593119,0.009110976,C
14297,"There are two major challenges in this
approach and it is recommended that further research should be undertaken in the
following areas: The first is creating image objects related to soil moisture variable and
the second is calculating soil moisture values for each image object.","The results demonstrated that
exploiting the segment-based approach in the synergetic use of Sentinel-1, Sentinel-2 and
SAMP data improved the accuracy of soil moisture estimation by 25% (as measured by
R2) compared to the pixel-based approach.","Acknowledgments
The authors would like to thank Claudia Notarnicola and Felix Greifeneder for their
support.",2022-11-29 05:23:33+00:00,Segment-based fusion of multi-sensor multi-scale satellite soil moisture retrievals,eess.IV,"['eess.IV', 'cs.AI']","[arxiv.Result.Author('Reza Attarzadeh'), arxiv.Result.Author('Hossein Bagheri'), arxiv.Result.Author('Iman Khosravi'), arxiv.Result.Author('Saeid Niazmardi'), arxiv.Result.Author('Davood Akbarid')]","Synergetic use of sensors for soil moisture retrieval is attracting
considerable interest due to the different advantages of different sensors.
Active, passive, and optic data integration could be a comprehensive solution
for exploiting the advantages of different sensors aimed at preparing soil
moisture maps. Typically, pixel-based methods are used for multi-sensor fusion.
Since, different applications need different scales of soil moisture maps,
pixel-based approaches are limited for this purpose. Object-based image
analysis employing an image object instead of a pixel could help us to meet
this need. This paper proposes a segment-based image fusion framework to
evaluate the possibility of preparing a multi-scale soil moisture map through
integrated Sentinel-1, Sentinel-2, and Soil Moisture Active Passive (SMAP)
data. The results confirmed that the proposed methodology was able to improve
soil moisture estimation in different scales up to 20% better compared to
pixel-based fusion approach.",0.15625346,0.03728053,0.12578768,B
14399,"It means that the segmentation module might learn incorrect patterns     [4] F. Yokota, Y. Otake, M. Takao, T. Ogawa, T. Okada, N. Sugano, and
in the ﬁne tuning stage and needs to further study, which is beyond           Y. Sato, “Automated muscle segmentation from CT images of the hip
the scope of this manuscript.","901–911, 2017.
noise, prediction errors cannot be found only based on entropy maps.","and thigh using a hierarchical multi-atlas method,” International Journal
                                                                              of Computer Assisted Radiology and Surgery, vol.",2022-11-30 19:04:17+00:00,Single Slice Thigh CT Muscle Group Segmentation with Domain Adaptation and Self-Training,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Qi Yang'), arxiv.Result.Author('Xin Yu'), arxiv.Result.Author('Ho Hin Lee'), arxiv.Result.Author('Leon Y. Cai'), arxiv.Result.Author('Kaiwen Xu'), arxiv.Result.Author('Shunxing Bao'), arxiv.Result.Author('Yuankai Huo'), arxiv.Result.Author('Ann Zenobia Moore'), arxiv.Result.Author('Sokratis Makrogiannis'), arxiv.Result.Author('Luigi Ferrucci'), arxiv.Result.Author('Bennett A. Landman')]","Objective: Thigh muscle group segmentation is important for assessment of
muscle anatomy, metabolic disease and aging. Many efforts have been put into
quantifying muscle tissues with magnetic resonance (MR) imaging including
manual annotation of individual muscles. However, leveraging publicly available
annotations in MR images to achieve muscle group segmentation on single slice
computed tomography (CT) thigh images is challenging.
  Method: We propose an unsupervised domain adaptation pipeline with
self-training to transfer labels from 3D MR to single CT slice. First, we
transform the image appearance from MR to CT with CycleGAN and feed the
synthesized CT images to a segmenter simultaneously. Single CT slices are
divided into hard and easy cohorts based on the entropy of pseudo labels
inferenced by the segmenter. After refining easy cohort pseudo labels based on
anatomical assumption, self-training with easy and hard splits is applied to
fine tune the segmenter.
  Results: On 152 withheld single CT thigh images, the proposed pipeline
achieved a mean Dice of 0.888(0.041) across all muscle groups including
sartorius, hamstrings, quadriceps femoris and gracilis. muscles
  Conclusion: To our best knowledge, this is the first pipeline to achieve
thigh imaging domain adaptation from MR to CT. The proposed pipeline is
effective and robust in extracting muscle groups on 2D single slice CT thigh
images.The container is available for public use at
https://github.com/MASILab/DA_CT_muscle_seg",-0.23298761,-0.008744668,0.13595867,C
14433,Both issues represent interesting ﬁelds for further research.,"However, it would be interesting to extend it to multi-pathology studies,
more similar to real screening scenarios.","Declaration of competing interest

    The authors declare that they have no known competing ﬁnancial interests or personal relationships that
could have appeared to inﬂuence the work reported in this paper.",2022-12-01 15:11:56+00:00,Weakly-supervised detection of AMD-related lesions in color fundus images using explainable deep learning,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('José Morano'), arxiv.Result.Author('Álvaro S. Hervella'), arxiv.Result.Author('José Rouco'), arxiv.Result.Author('Jorge Novo'), arxiv.Result.Author('José I. Fernández-Vigo'), arxiv.Result.Author('Marcos Ortega')]","Age-related macular degeneration (AMD) is a degenerative disorder affecting
the macula, a key area of the retina for visual acuity. Nowadays, it is the
most frequent cause of blindness in developed countries. Although some
promising treatments have been developed, their effectiveness is low in
advanced stages. This emphasizes the importance of large-scale screening
programs. Nevertheless, implementing such programs for AMD is usually
unfeasible, since the population at risk is large and the diagnosis is
challenging. All this motivates the development of automatic methods. In this
sense, several works have achieved positive results for AMD diagnosis using
convolutional neural networks (CNNs). However, none incorporates explainability
mechanisms, which limits their use in clinical practice. In that regard, we
propose an explainable deep learning approach for the diagnosis of AMD via the
joint identification of its associated retinal lesions. In our proposal, a CNN
is trained end-to-end for the joint task using image-level labels. The provided
lesion information is of clinical interest, as it allows to assess the
developmental stage of AMD. Additionally, the approach allows to explain the
diagnosis from the identified lesions. This is possible thanks to the use of a
CNN with a custom setting that links the lesions and the diagnosis.
Furthermore, the proposed setting also allows to obtain coarse lesion
segmentation maps in a weakly-supervised way, further improving the
explainability. The training data for the approach can be obtained without much
extra work by clinicians. The experiments conducted demonstrate that our
approach can identify AMD and its associated lesions satisfactorily, while
providing adequate coarse segmentation maps for most common lesions.",-0.39246717,-0.32903334,-0.05394141,C
14434,Both issues represent interesting ﬁelds for further research.,"However, it would be interesting to extend it to multi-pathology studies,
more similar to real screening scenarios.","Declaration of competing interest

    The authors declare that they have no known competing ﬁnancial interests or personal relationships that
could have appeared to inﬂuence the work reported in this paper.",2022-12-01 15:11:56+00:00,Weakly-supervised detection of AMD-related lesions in color fundus images using explainable deep learning,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('José Morano'), arxiv.Result.Author('Álvaro S. Hervella'), arxiv.Result.Author('José Rouco'), arxiv.Result.Author('Jorge Novo'), arxiv.Result.Author('José I. Fernández-Vigo'), arxiv.Result.Author('Marcos Ortega')]","Age-related macular degeneration (AMD) is a degenerative disorder affecting
the macula, a key area of the retina for visual acuity. Nowadays, it is the
most frequent cause of blindness in developed countries. Although some
promising treatments have been developed, their effectiveness is low in
advanced stages. This emphasizes the importance of large-scale screening
programs. Nevertheless, implementing such programs for AMD is usually
unfeasible, since the population at risk is large and the diagnosis is
challenging. All this motivates the development of automatic methods. In this
sense, several works have achieved positive results for AMD diagnosis using
convolutional neural networks (CNNs). However, none incorporates explainability
mechanisms, which limits their use in clinical practice. In that regard, we
propose an explainable deep learning approach for the diagnosis of AMD via the
joint identification of its associated retinal lesions. In our proposal, a CNN
is trained end-to-end for the joint task using image-level labels. The provided
lesion information is of clinical interest, as it allows to assess the
developmental stage of AMD. Additionally, the approach allows to explain the
diagnosis from the identified lesions. This is possible thanks to the use of a
CNN with a custom setting that links the lesions and the diagnosis.
Furthermore, the proposed setting also allows to obtain coarse lesion
segmentation maps in a weakly-supervised way, further improving the
explainability. The training data for the approach can be obtained without much
extra work by clinicians. The experiments conducted demonstrate that our
approach can identify AMD and its associated lesions satisfactorily, while
providing adequate coarse segmentation maps for most common lesions.",-0.39246717,-0.32903334,-0.05394141,C
14488,"REVIEW OF SOFT CONTEXT FORMATION
                                         further research was the incorporation of a screen content
                                         coding (SCC) extension in the HEVC standard [1], [2], which                The SCF method is based on the principal of ideal entropy
                                         integrates dedicated encoding tools for screen content, such            coding.","An
                                         indication of the recognition of this issue and a trigger for                    II.","The general processing order is visualized in Figure 1.
                                         as palette coding [3], adaptive color transform [4], or intra           The pixels are scanned in raster-scan order and for each pixel
                                         block copy [5].",2022-12-02 12:10:20+00:00,Optimization of Probability Distributions for Residual Coding of Screen Content,eess.IV,['eess.IV'],"[arxiv.Result.Author('Hannah Och'), arxiv.Result.Author('Tilo Strutz'), arxiv.Result.Author('André Kaup')]","Probability distribution modeling is the basis for most competitive methods
for lossless coding of screen content. One such state-of-the-art method is
known as soft context formation (SCF). For each pixel to be encoded, a
probability distribution is estimated based on the neighboring pattern and the
occurrence of that pattern in the already encoded image. Using an arithmetic
coder, the pixel color can thus be encoded very efficiently, provided that the
current color has been observed before in association with a similar pattern.
If this is not the case, the color is instead encoded using a color palette or,
if it is still unknown, via residual coding. Both palette-based coding and
residual coding have significantly worse compression efficiency than coding
based on soft context formation. In this paper, the residual coding stage is
improved by adaptively trimming the probability distributions for the residual
error. Furthermore, an enhanced probability modeling for indicating a new color
depending on the occurrence of new colors in the neighborhood is proposed.
These modifications result in a bitrate reduction of up to 2.9% on average.
Compared to HEVC (HM-16.21 + SCM-8.8) and FLIF, the improved SCF method saves
on average about 11% and 18% rate, respectively.",0.38098088,-0.09619857,-0.037038583,B
14908,"Accord-
                                          itating further research.","cedure is also cast as embedding-level multiple instance learn-
                                               Source code is available at https://github.com/liupei101/AdvMIL for facil-  ing (MIL) (Ilse et al., 2018; Carbonneau et al., 2018).","ing to the type of patch correlation, the mainstream MIL-based
                                              ∗Corresponding author: L. Ji (jiluping@uestc.edu.cn).",2022-12-13 12:02:05+00:00,AdvMIL: Adversarial Multiple Instance Learning for the Survival Analysis on Whole-Slide Images,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Pei Liu'), arxiv.Result.Author('Luping Ji'), arxiv.Result.Author('Feng Ye'), arxiv.Result.Author('Bo Fu')]","The survival analysis on histological whole-slide images (WSIs) is one of the
most important means to estimate patient prognosis. Although many
weakly-supervised deep learning models have been developed for gigapixel WSIs,
their potential is generally restricted by classical survival analysis rules
and fully-supervision requirements. As a result, these models provide patients
only with a completely-certain point estimation of time-to-event, and they
could only learn from the well-annotated WSI data currently at a small scale.
To tackle these problems, we propose a novel adversarial multiple instance
learning (AdvMIL) framework. This framework is based on adversarial
time-to-event modeling, and it integrates the multiple instance learning (MIL)
that is much necessary for WSI representation learning. It is a plug-and-play
one, so that most existing WSI-based models with embedding-level MIL networks
can be easily upgraded by applying this framework, gaining the improved ability
of survival distribution estimation and semi-supervised learning. Our extensive
experiments show that AdvMIL could not only bring performance improvement to
mainstream WSI models at a relatively low computational cost, but also enable
these models to learn from unlabeled data with semi-supervised learning. Our
AdvMIL framework could promote the research of time-to-event modeling in
computational pathology with its novel paradigm of adversarial MIL.",0.03608048,0.023547763,-0.30048132,A
14956,"As UDA is transferable to arbitrary medical imaging tasks, it may provide a springboard
for further research across institutions, imaging devices, acquisitions and patient groups.","Our application of UDA significantly
improved the phenotype classification performance, making downstream clinical research on local patient
groups possible.","Acknowledgements

We would like to acknowledge Ben Chi Yin Choi and Cherry Cheuk Nam Cheng for assistance in patient
recruitment and MRI exams.",2022-12-14 04:26:32+00:00,Unsupervised Domain Adaptation for Automated Knee Osteoarthritis Phenotype Classification,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Junru Zhong'), arxiv.Result.Author('Yongcheng Yao'), arxiv.Result.Author('Donal G. Cahill'), arxiv.Result.Author('Fan Xiao'), arxiv.Result.Author('Siyue Li'), arxiv.Result.Author('Jack Lee'), arxiv.Result.Author('Kevin Ki-Wai Ho'), arxiv.Result.Author('Michael Tim-Yun Ong'), arxiv.Result.Author('James F. Griffith'), arxiv.Result.Author('Weitian Chen')]","Purpose: The aim of this study was to demonstrate the utility of unsupervised
domain adaptation (UDA) in automated knee osteoarthritis (OA) phenotype
classification using a small dataset (n=50). Materials and Methods: For this
retrospective study, we collected 3,166 three-dimensional (3D) double-echo
steady-state magnetic resonance (MR) images from the Osteoarthritis Initiative
dataset and 50 3D turbo/fast spin-echo MR images from our institute (in 2020
and 2021) as the source and target datasets, respectively. For each patient,
the degree of knee OA was initially graded according to the MRI Osteoarthritis
Knee Score (MOAKS) before being converted to binary OA phenotype labels. The
proposed UDA pipeline included (a) pre-processing, which involved automatic
segmentation and region-of-interest cropping; (b) source classifier training,
which involved pre-training phenotype classifiers on the source dataset; (c)
target encoder adaptation, which involved unsupervised adaption of the source
encoder to the target encoder and (d) target classifier validation, which
involved statistical analysis of the target classification performance
evaluated by the area under the receiver operating characteristic curve
(AUROC), sensitivity, specificity and accuracy. Additionally, a classifier was
trained without UDA for comparison. Results: The target classifier trained with
UDA achieved improved AUROC, sensitivity, specificity and accuracy for both
knee OA phenotypes compared with the classifier trained without UDA.
Conclusion: The proposed UDA approach improves the performance of automated
knee OA phenotype classification for small target datasets by utilising a
large, high-quality source dataset for training. The results successfully
demonstrated the advantages of the UDA approach in classification on small
datasets.",-0.35926723,-0.0417678,0.029583918,C
14989,"Notably, the spatial               reconstruction will also be the subject of further research.",11.,"resolution is lower than that of the data in the retrospective
study, so the image quality in this experiment is not as good                                         REFERENCES
as in the former images.",2022-12-15 03:04:48+00:00,Universal Generative Modeling in Dual-domain for Dynamic MR Imaging,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Chuanming Yu'), arxiv.Result.Author('Yu Guan'), arxiv.Result.Author('Ziwen Ke'), arxiv.Result.Author('Dong Liang'), arxiv.Result.Author('Qiegen Liu')]","Dynamic magnetic resonance image reconstruction from incomplete k-space data
has generated great research interest due to its capability to reduce scan
time. Never-theless, the reconstruction problem is still challenging due to its
ill-posed nature. Recently, diffusion models espe-cially score-based generative
models have exhibited great potential in algorithm robustness and usage
flexi-bility. Moreover, the unified framework through the variance exploding
stochastic differential equation (VE-SDE) is proposed to enable new sampling
methods and further extend the capabilities of score-based gener-ative models.
Therefore, by taking advantage of the uni-fied framework, we proposed a k-space
and image Du-al-Domain collaborative Universal Generative Model (DD-UGM) which
combines the score-based prior with low-rank regularization penalty to
reconstruct highly under-sampled measurements. More precisely, we extract prior
components from both image and k-space domains via a universal generative model
and adaptively handle these prior components for faster processing while
maintaining good generation quality. Experimental comparisons demonstrated the
noise reduction and detail preservation abilities of the proposed method. Much
more than that, DD-UGM can reconstruct data of differ-ent frames by only
training a single frame image, which reflects the flexibility of the proposed
model.",0.14196518,-0.19112892,0.29000008,B
15142,"This performance level could
                                                                 serve as a starting point for further research and reﬁnement, but
 Race/Gender         Class     Precision  Recall  F1-score       further improvements are needed to reach a performance level
      White      Reduced EF       0.64     0.62     0.63         for clinical deployment.","Our best model shows good performance with a
A COMPARISON OF RESNET50 PERFORMANCE ACROSS RACIAL GROUPS        misclassiﬁcation rate 31.2%.","Among our contributions, we show
      Black      Preserved EF     0.72     0.74     0.73         how improving model size can increase model performance
                 Reduced EF       0.38     0.32     0.35         and that data augmentation can improve model robustness.",2022-12-19 21:18:27+00:00,Predicting Ejection Fraction from Chest X-rays Using Computer Vision for Diagnosing Heart Failure,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Walt Williams'), arxiv.Result.Author('Rohan Doshi'), arxiv.Result.Author('Yanran Li'), arxiv.Result.Author('Kexuan Liang')]","Heart failure remains a major public health challenge with growing costs.
Ejection fraction (EF) is a key metric for the diagnosis and management of
heart failure however estimation of EF using echocardiography remains expensive
for the healthcare system and subject to intra/inter operator variability.
While chest x-rays (CXR) are quick, inexpensive, and require less expertise,
they do not provide sufficient information to the human eye to estimate EF.
This work explores the efficacy of computer vision techniques to predict
reduced EF solely from CXRs. We studied a dataset of 3488 CXRs from the MIMIC
CXR-jpg (MCR) dataset. Our work establishes benchmarks using multiple
state-of-the-art convolutional neural network architectures. The subsequent
analysis shows increasing model sizes from 8M to 23M parameters improved
classification performance without overfitting the dataset. We further show how
data augmentation techniques such as CXR rotation and random cropping further
improves model performance another ~5%. Finally, we conduct an error analysis
using saliency maps and Grad-CAMs to better understand the failure modes of
convolutional models on this task.",-0.151099,0.002898777,-0.31204373,A
15257,"For this experiments CSi-        and further research is needed to determine the correlations that could
GAN pretrained on each of the Dk datasets were used.","The comparison
Input classiﬁer weights were trained wile the ones of the Cycle-        against the classiﬁcation metrics does not show a conclusive result
Consistency Network remained constant.","The results of     lead to best practices and parameter choices when training GAN
these experiments are shown in Fig.",2022-12-21 21:32:36+00:00,Semi-supervised GAN for Bladder Tissue Classification in Multi-Domain Endoscopic Images,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Jorge F. Lazo'), arxiv.Result.Author('Benoit Rosa'), arxiv.Result.Author('Michele Catellani'), arxiv.Result.Author('Matteo Fontana'), arxiv.Result.Author('Francesco A. Mistretta'), arxiv.Result.Author('Gennaro Musi'), arxiv.Result.Author('Ottavio de Cobelli'), arxiv.Result.Author('Michel de Mathelin'), arxiv.Result.Author('Elena De Momi')]","Objective: Accurate visual classification of bladder tissue during
Trans-Urethral Resection of Bladder Tumor (TURBT) procedures is essential to
improve early cancer diagnosis and treatment. During TURBT interventions, White
Light Imaging (WLI) and Narrow Band Imaging (NBI) techniques are used for
lesion detection. Each imaging technique provides diverse visual information
that allows clinicians to identify and classify cancerous lesions. Computer
vision methods that use both imaging techniques could improve endoscopic
diagnosis. We address the challenge of tissue classification when annotations
are available only in one domain, in our case WLI, and the endoscopic images
correspond to an unpaired dataset, i.e. there is no exact equivalent for every
image in both NBI and WLI domains. Method: We propose a semi-surprised
Generative Adversarial Network (GAN)-based method composed of three main
components: a teacher network trained on the labeled WLI data; a
cycle-consistency GAN to perform unpaired image-to-image translation, and a
multi-input student network. To ensure the quality of the synthetic images
generated by the proposed GAN we perform a detailed quantitative, and
qualitative analysis with the help of specialists. Conclusion: The overall
average classification accuracy, precision, and recall obtained with the
proposed method for tissue classification are 0.90, 0.88, and 0.89
respectively, while the same metrics obtained in the unlabeled domain (NBI) are
0.92, 0.64, and 0.94 respectively. The quality of the generated images is
reliable enough to deceive specialists. Significance: This study shows the
potential of using semi-supervised GAN-based classification to improve bladder
tissue classification when annotations are limited in multi-domain data.",0.06542017,0.24483973,-0.23683232,A
15258,"By
improves considerably the detection of classes which are underrep-      making available our dataset we hope to encourage further research
resented.","As      well as the acquisition of data from other imaging domains which
expected, the integration of both results in the best performance, and  could help to assess better the generalization of the method.","This behavior is more clearly noticeable in the case of the   in the ﬁeld that could motivate the clinical translation of endoscopic
NTL class which in our dataset has the smallest number of samples       image classiﬁcation.",2022-12-21 21:32:36+00:00,Semi-supervised GAN for Bladder Tissue Classification in Multi-Domain Endoscopic Images,eess.IV,"['eess.IV', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Jorge F. Lazo'), arxiv.Result.Author('Benoit Rosa'), arxiv.Result.Author('Michele Catellani'), arxiv.Result.Author('Matteo Fontana'), arxiv.Result.Author('Francesco A. Mistretta'), arxiv.Result.Author('Gennaro Musi'), arxiv.Result.Author('Ottavio de Cobelli'), arxiv.Result.Author('Michel de Mathelin'), arxiv.Result.Author('Elena De Momi')]","Objective: Accurate visual classification of bladder tissue during
Trans-Urethral Resection of Bladder Tumor (TURBT) procedures is essential to
improve early cancer diagnosis and treatment. During TURBT interventions, White
Light Imaging (WLI) and Narrow Band Imaging (NBI) techniques are used for
lesion detection. Each imaging technique provides diverse visual information
that allows clinicians to identify and classify cancerous lesions. Computer
vision methods that use both imaging techniques could improve endoscopic
diagnosis. We address the challenge of tissue classification when annotations
are available only in one domain, in our case WLI, and the endoscopic images
correspond to an unpaired dataset, i.e. there is no exact equivalent for every
image in both NBI and WLI domains. Method: We propose a semi-surprised
Generative Adversarial Network (GAN)-based method composed of three main
components: a teacher network trained on the labeled WLI data; a
cycle-consistency GAN to perform unpaired image-to-image translation, and a
multi-input student network. To ensure the quality of the synthetic images
generated by the proposed GAN we perform a detailed quantitative, and
qualitative analysis with the help of specialists. Conclusion: The overall
average classification accuracy, precision, and recall obtained with the
proposed method for tissue classification are 0.90, 0.88, and 0.89
respectively, while the same metrics obtained in the unlabeled domain (NBI) are
0.92, 0.64, and 0.94 respectively. The quality of the generated images is
reliable enough to deceive specialists. Significance: This study shows the
potential of using semi-supervised GAN-based classification to improve bladder
tissue classification when annotations are limited in multi-domain data.",-0.18062714,-0.006504759,0.1509039,C
15360,"OMSN multi-task segmentation               Besides, several publicly available OCTA segmentation
model retrained with FAROS further certifies its                 datasets with partial RVN and FAZ annotations are released to
outstanding accuracy for simultaneous RVN and FAZ                researchers for further study of OCTA images segmentation [3],
segmentation.",labeled OCTA dataset.,"[24], [25].",2022-12-26 09:32:52+00:00,OMSN and FAROS: OCTA Microstructure Segmentation Network and Fully Annotated Retinal OCTA Segmentation Dataset,eess.IV,"['eess.IV', 'cs.CV']","[arxiv.Result.Author('Peng Xiao'), arxiv.Result.Author('Xiaodong Hu'), arxiv.Result.Author('Ke Ma'), arxiv.Result.Author('Gengyuan Wang'), arxiv.Result.Author('Ziqing Feng'), arxiv.Result.Author('Yuancong Huang'), arxiv.Result.Author('Jin Yuan')]","The lack of efficient segmentation methods and fully-labeled datasets limits
the comprehensive assessment of optical coherence tomography angiography (OCTA)
microstructures like retinal vessel network (RVN) and foveal avascular zone
(FAZ), which are of great value in ophthalmic and systematic diseases
evaluation. Here, we introduce an innovative OCTA microstructure segmentation
network (OMSN) by combining an encoder-decoder-based architecture with
multi-scale skip connections and the split-attention-based residual network
ResNeSt, paying specific attention to OCTA microstructural features while
facilitating better model convergence and feature representations. The proposed
OMSN achieves excellent single/multi-task performances for RVN or/and FAZ
segmentation. Especially, the evaluation metrics on multi-task models
outperform single-task models on the same dataset. On this basis, a fully
annotated retinal OCTA segmentation (FAROS) dataset is constructed
semi-automatically, filling the vacancy of a pixel-level fully-labeled OCTA
dataset. OMSN multi-task segmentation model retrained with FAROS further
certifies its outstanding accuracy for simultaneous RVN and FAZ segmentation.",0.041634638,0.09426235,-0.091076754,A
15388,"The database was constructed to allow
                                                                                    [18] M. Olko, D. Dembeck, Y.-H. Wu, A. F. Genovese, and A. Roginska,
further research into high-order cognitive performances with an                           “Identiﬁcation of perceived sound quality attributes of 360° audiovisual
                                                                                          recordings in VR using a free verbalization method,” in Proc.","Available:
                                                                                          http://pcfarina.eng.unipr.it/Public/Jump-Videos/
in the same setting.","Audio
emphasis on real-life-like naturalistic settings for high audiovi-                        Engineering Society Convention, New York, NY, USA, Oct, 2017, pp.",2022-12-27 10:47:08+00:00,"Audiovisual Database with 360 Video and Higher-Order Ambisonics Audio for Perception, Cognition, Behavior, and QoE Evaluation Research",eess.IV,"['eess.IV', 'cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Thomas Robotham'), arxiv.Result.Author('Ashutosh Singla'), arxiv.Result.Author('Olli S. Rummukainen'), arxiv.Result.Author('Alexander Raake'), arxiv.Result.Author('Emanuël A. P. Habets')]","Research into multi-modal perception, human cognition, behavior, and
attention can benefit from high-fidelity content that may recreate
real-life-like scenes when rendered on head-mounted displays. Moreover, aspects
of audiovisual perception, cognitive processes, and behavior may complement
questionnaire-based Quality of Experience (QoE) evaluation of interactive
virtual environments. Currently, there is a lack of high-quality open-source
audiovisual databases that can be used to evaluate such aspects or systems
capable of reproducing high-quality content. With this paper, we provide a
publicly available audiovisual database consisting of twelve scenes capturing
real-life nature and urban environments with a video resolution of 7680x3840 at
60 frames-per-second and with 4th-order Ambisonics audio. These 360 video
sequences, with an average duration of 60 seconds, represent real-life settings
for systematically evaluating various dimensions of uni-/multi-modal
perception, cognition, behavior, and QoE. The paper provides details of the
scene requirements, recording approach, and scene descriptions. The database
provides high-quality reference material with a balanced focus on auditory and
visual sensory information. The database will be continuously updated with
additional scenes and further metadata such as human ratings and saliency
information.",0.17243752,-0.12878639,-0.121080555,B
