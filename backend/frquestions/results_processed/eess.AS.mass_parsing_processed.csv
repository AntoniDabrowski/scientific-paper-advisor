Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
387,"The signiﬁcant WER differences observed         Gumbel-Softmax searched TDNN-F and CNN TDNN-F sys-
       between these two systems suggest further research is       tems (Sys (9) & (11)) on the CHM subset of Hub5’00 test set
       required to improve the transferability and generalization  and Rt03S test sets when compared with the other hybrid and
       of NAS methods across different data sets and quantities.",V).,end-to-end systems (Sys (1)-(7) in Tab.,2022-01-08 07:52:01+00:00,Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Shoukang Hu'), arxiv.Result.Author('Xurong Xie'), arxiv.Result.Author('Mingyu Cui'), arxiv.Result.Author('Jiajun Deng'), arxiv.Result.Author('Shansong Liu'), arxiv.Result.Author('Jianwei Yu'), arxiv.Result.Author('Mengzhe Geng'), arxiv.Result.Author('Xunying Liu'), arxiv.Result.Author('Helen Meng')]","State-of-the-art automatic speech recognition (ASR) system development is
data and computation intensive. The optimal design of deep neural networks
(DNNs) for these systems often require expert knowledge and empirical
evaluation. In this paper, a range of neural architecture search (NAS)
techniques are used to automatically learn two types of hyper-parameters of
factored time delay neural networks (TDNN-Fs): i) the left and right splicing
context offsets; and ii) the dimensionality of the bottleneck linear projection
at each hidden layer. These techniques include the differentiable neural
architecture search (DARTS) method integrating architecture learning with
lattice-free MMI training; Gumbel-Softmax and pipelined DARTS methods reducing
the confusion over candidate architectures and improving the generalization of
architecture selection; and Penalized DARTS incorporating resource constraints
to balance the trade-off between performance and system complexity. Parameter
sharing among TDNN-F architectures allows an efficient search over up to 7^28
different systems. Statistically significant word error rate (WER) reductions
of up to 1.2% absolute and relative model size reduction of 31% were obtained
over a state-of-the-art 300-hour Switchboard corpus trained baseline LF-MMI
TDNN-F system featuring speed perturbation, i-Vector and learning hidden unit
contribution (LHUC) based speaker adaptation as well as RNNLM rescoring.
Performance contrasts on the same task against recent end-to-end systems
reported in the literature suggest the best NAS auto-configured system achieves
state-of-the-art WERs of 9.9% and 11.1% on the NIST Hub5' 00 and Rt03s test
sets respectively with up to 96% model size reduction. Further analysis using
Bayesian learning shows that ...",-0.3169089,0.045977306,-0.0028789311,B
706,"Nonetheless, our work uncovers promising observations
                                          and possibilities for further research.","We
                                          show that the human perceptual ambiguity and other challenges, such as class imbalance and lack of training data, currently
                                          limit the opportunities of these technologies for oral history archives.","Keywords: emotion recognition, sentiment analysis, language, oral history, speech emotion recognition, facial emotion
                                          recognition, annotation, ambiguity

                                                          1.",2022-01-18 10:53:07+00:00,A Study on the Ambiguity in Human Annotation of German Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis,eess.AS,"['eess.AS', 'cs.CL', 'cs.SD']","[arxiv.Result.Author('Michael Gref'), arxiv.Result.Author('Nike Matthiesen'), arxiv.Result.Author('Sreenivasa Hikkal Venugopala'), arxiv.Result.Author('Shalaka Satheesh'), arxiv.Result.Author('Aswinkumar Vijayananth'), arxiv.Result.Author('Duc Bach Ha'), arxiv.Result.Author('Sven Behnke'), arxiv.Result.Author('Joachim Köhler')]","For research in audiovisual interview archives often it is not only of
interest what is said but also how. Sentiment analysis and emotion recognition
can help capture, categorize and make these different facets searchable. In
particular, for oral history archives, such indexing technologies can be of
great interest. These technologies can help understand the role of emotions in
historical remembering. However, humans often perceive sentiments and emotions
ambiguously and subjectively. Moreover, oral history interviews have
multi-layered levels of complex, sometimes contradictory, sometimes very subtle
facets of emotions. Therefore, the question arises of the chance machines and
humans have capturing and assigning these into predefined categories. This
paper investigates the ambiguity in human perception of emotions and sentiment
in German oral history interviews and the impact on machine learning systems.
Our experiments reveal substantial differences in human perception for
different emotions. Furthermore, we report from ongoing machine learning
experiments with different modalities. We show that the human perceptual
ambiguity and other challenges, such as class imbalance and lack of training
data, currently limit the opportunities of these technologies for oral history
archives. Nonetheless, our work uncovers promising observations and
possibilities for further research.",0.2747178,0.05621579,-0.10103533,A
1823,"To further research the effect of

                                           T                                       incorporating noisy signal in the diffusion model, in Sec.","(1):
                                                                                   process that is theoretically sound.","3.3, we will

        pθ(x0, · · · , xT −1|xT ) = pθ(xt−1|xt),                              (3)  set δt according to Eq.",2022-02-10 18:58:01+00:00,Conditional Diffusion Probabilistic Model for Speech Enhancement,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Yen-Ju Lu'), arxiv.Result.Author('Zhong-Qiu Wang'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Alexander Richard'), arxiv.Result.Author('Cheng Yu'), arxiv.Result.Author('Yu Tsao')]","Speech enhancement is a critical component of many user-oriented audio
applications, yet current systems still suffer from distorted and unnatural
outputs. While generative models have shown strong potential in speech
synthesis, they are still lagging behind in speech enhancement. This work
leverages recent advances in diffusion probabilistic models, and proposes a
novel speech enhancement algorithm that incorporates characteristics of the
observed noisy speech signal into the diffusion and reverse processes. More
specifically, we propose a generalized formulation of the diffusion
probabilistic model named conditional diffusion probabilistic model that, in
its reverse process, can adapt to non-Gaussian real noises in the estimated
speech signal. In our experiments, we demonstrate strong performance of the
proposed approach compared to representative generative models, and investigate
the generalization capability of our models to other datasets with noise
characteristics unseen during training.",-0.25082207,-0.2928237,-0.20396341,B
1835,"MobileNetV2           5       24       (1, 1) 0.3M                        For use cases where even less energy per inference is targeted, we
Identity              3       -        (1, 1) -                           suggest further research into expanding the search space to use 1D
MobileNetV1           -       40       (1, 1) 0.2M                        convolutional blocks and/or combining this approach with model
MobileNetV2           3       32       (1, 1) 0.9M                        compression techniques (e.g.","We also find 1D convolutional networks
MobileNetV2-Avg-Pool  3       24       (2, 2) 1.4M                        to be significantly more energy efficient but poor in task accuracy.",weight pruning).,2022-02-09 06:10:18+00:00,Neural Architecture Search for Energy Efficient Always-on Audio Models,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Daniel T. Speckhard'), arxiv.Result.Author('Karolis Misiunas'), arxiv.Result.Author('Sagi Perel'), arxiv.Result.Author('Tenghui Zhu'), arxiv.Result.Author('Simon Carlile'), arxiv.Result.Author('Malcolm Slaney')]","Mobile and edge computing devices for always-on audio classification require
energy-efficient neural network architectures. We present a neural architecture
search (NAS) that optimizes accuracy, energy efficiency and memory usage. The
search is run on Vizier, a black-box optimization service. We present a search
strategy that uses both Bayesian and regularized evolutionary search with
particle swarms, and employs early-stopping to reduce the computational burden.
The search returns architectures for a sound-event classification dataset based
upon AudioSet with similar accuracy to MobileNetV1/V2 implementations but with
an order of magnitude less energy per inference and a much smaller memory
footprint.",-0.11725549,0.053721346,0.23635025,A
2280,"Nonethe-
less, the results of Table 5.1 demonstrated the need to perform further research of
speech recognition in collaborative learning environments.","In the case that this was not the outcome, we
considered replacing PCA with Independent Component Analysis (ICA) since it is
known to successfully remove statistical dependence between features [67].","Table 5.1: Speech recognition accuracy obtained from DeepSpeech and SVM-PCA
Classiﬁer on TIMIT and AOLME.",2022-02-21 21:25:41+00:00,Spanish and English Phoneme Recognition by Training on Simulated Classroom Audio Recordings of Collaborative Learning Environments,eess.AS,"['eess.AS', 'cs.SD']",[arxiv.Result.Author('Mario Esparza')],"Audio recordings of collaborative learning environments contain a constant
presence of cross-talk and background noise. Dynamic speech recognition between
Spanish and English is required in these environments. To eliminate the
standard requirement of large-scale ground truth, the thesis develops a
simulated dataset by transforming audio transcriptions into phonemes and using
3D speaker geometry and data augmentation to generate an acoustic simulation of
Spanish and English speech. The thesis develops a low-complexity neural network
for recognizing Spanish and English phonemes (available at
github.com/muelitas/keywordRec). When trained on 41 English phonemes, 0.099 PER
is achieved on Speech Commands. When trained on 36 Spanish phonemes and tested
on real recordings of collaborative learning environments, a 0.7208 LER is
achieved. Slightly better than Google's Speech-to-text 0.7272 LER, which used
anywhere from 15 to 1,635 times more parameters and trained on 300 to 27,500
hours of real data as opposed to 13 hours of simulated audios.",0.14065447,0.032836072,-0.13170208,B
2856,"With       racy, the model corresponding to the 12th epoch is chosen
increase in x around the change point, the CD measure           for further study.","Therefore it is hypothesized that, the ldvt distri-      ter observing the training and validation loss and accu-
bution has higher values than the ldvf distribution.","for both the measure should increase, as with increase in
number of voiced frames.",2022-03-05 08:44:59+00:00,Language vs Speaker Change: A Comparative Study,eess.AS,"['eess.AS', 'cs.SD', 'eess.SP']","[arxiv.Result.Author('Jagabandhu Mishra'), arxiv.Result.Author('S. R. Mahadeva Prasanna')]","Spoken language change detection (LCD) refers to detecting language switching
points in a multilingual speech signal. Speaker change detection (SCD) refers
to locating the speaker change points in a multispeaker speech signal. The
objective of this work is to understand the challenges in LCD task by comparing
it with SCD task. Human subjective study for change detection is performed for
LCD and SCD. This study demonstrates that LCD requires larger duration
spectro-temporal information around the change point compared to SCD. Based on
this, the work explores automatic distance based and model based LCD
approaches. The model based ones include Gaussian mixture model and universal
background model (GMM-UBM), attention, and Generative adversarial network (GAN)
based approaches. Both the human and automatic LCD tasks infer that the
performance of the LCD task improves by incorporating more and more
spectro-temporal duration.",-0.19075021,-0.034992296,-0.058856428,B
3363,"Architecture               Channels        Blocks            ASC                  UST    ASD
                                                        macro-ACC          macro-AUPRC   AUC

CNN9                 64/128/256/512        4                0.550              0.678     0.703

CNN4-Trans                 64/128          2                0.589              0.678     0.712

SE-Trans                   64/128          2                0.609              0.699     0.721

    To further study the eﬀectiveness of the Transformer encoder and SE mod-
ules, ablation experiments have been performed.","Analysis of SE and Transformer modules

Table 3: Results of ablation experiments of SE and Transformer modules.","Table 3 compares three dif-
ferent architectures with diﬀerent numbers of CNN channels and blocks, and
macro-ACC, macro-AUPRC, and AUC are implemented as evaluation measure-
ments for ASC, UST, and ASD.",2022-03-16 02:07:02+00:00,A Squeeze-and-Excitation and Transformer based Cross-task System for Environmental Sound Recognition,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Jisheng Bai'), arxiv.Result.Author('Jianfeng Chen'), arxiv.Result.Author('Mou Wang'), arxiv.Result.Author('Muhammad Saad Ayub')]","Environmental sound recognition (ESR) is an emerging research topic in audio
pattern recognition. Many tasks are presented to resort to computational
systems for ESR in real-life applications. However, current systems are usually
designed for individual tasks, and are not robust and applicable to other
tasks. Cross-task systems, which promote unified knowledge modeling across
various tasks, have not been thoroughly investigated. In this paper, we propose
a cross-task system for three different tasks of ESR: acoustic scene
classification, urban sound tagging, and anomalous sound detection. An
architecture named SE-Trans is presented that uses attention mechanism-based
Squeeze-and-Excitation and Transformer encoder modules to learn channel-wise
relationship and temporal dependencies of the acoustic features. FMix is
employed as the data augmentation method that improves the performance of ESR.
Evaluations for the three tasks are conducted on the recent databases of DCASE
challenges. The experimental results show that the proposed cross-task system
achieves state-of-the-art performance on all tasks. Further analysis
demonstrates that the proposed cross-task system can effectively utilize
acoustic knowledge across different ESR tasks.",-0.30655092,0.01058341,-0.044286277,B
3364,"Analysis of acoustic scene classiﬁcation

To further study the aforementioned contribution of Transformer and SE

modules for ASC in Sec.","xxiv
5.2.1.","5.1.2, the confusion matrices achieved by CNN9,

CNN4-Trans, and SE-Trans are shown in Fig.",2022-03-16 02:07:02+00:00,A Squeeze-and-Excitation and Transformer based Cross-task System for Environmental Sound Recognition,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Jisheng Bai'), arxiv.Result.Author('Jianfeng Chen'), arxiv.Result.Author('Mou Wang'), arxiv.Result.Author('Muhammad Saad Ayub')]","Environmental sound recognition (ESR) is an emerging research topic in audio
pattern recognition. Many tasks are presented to resort to computational
systems for ESR in real-life applications. However, current systems are usually
designed for individual tasks, and are not robust and applicable to other
tasks. Cross-task systems, which promote unified knowledge modeling across
various tasks, have not been thoroughly investigated. In this paper, we propose
a cross-task system for three different tasks of ESR: acoustic scene
classification, urban sound tagging, and anomalous sound detection. An
architecture named SE-Trans is presented that uses attention mechanism-based
Squeeze-and-Excitation and Transformer encoder modules to learn channel-wise
relationship and temporal dependencies of the acoustic features. FMix is
employed as the data augmentation method that improves the performance of ESR.
Evaluations for the three tasks are conducted on the recent databases of DCASE
challenges. The experimental results show that the proposed cross-task system
achieves state-of-the-art performance on all tasks. Further analysis
demonstrates that the proposed cross-task system can effectively utilize
acoustic knowledge across different ESR tasks.",0.07143252,-0.053030387,-0.15879816,B
4124,"The
                                         ported by NSFC 61976122.                                              comparison under these frameworks are left for further study.",This work is sup-  based encoder-decoder (AED) and RNN-transducer could be used.,2.,2022-03-31 02:28:11+00:00,Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study,eess.AS,"['eess.AS', 'cs.CL']","[arxiv.Result.Author('Keyu An'), arxiv.Result.Author('Ji Xiao'), arxiv.Result.Author('Zhijian Ou')]","Recently, the end-to-end training approach for multi-channel ASR has shown
its effectiveness, which usually consists of a beamforming front-end and a
recognition back-end. However, the end-to-end training becomes more difficult
due to the integration of multiple modules, particularly considering that
multi-channel speech data recorded in real environments are limited in size.
This raises the demand to exploit the single-channel data for multi-channel
end-to-end ASR. In this paper, we systematically compare the performance of
three schemes to exploit external single-channel data for multi-channel
end-to-end ASR, namely back-end pre-training, data scheduling, and data
simulation, under different settings such as the sizes of the single-channel
data and the choices of the front-end. Extensive experiments on CHiME-4 and
AISHELL-4 datasets demonstrate that while all three methods improve the
multi-channel end-to-end speech recognition performance, data simulation
outperforms the other two, at the cost of longer training time. Data scheduling
outperforms back-end pre-training marginally but nearly consistently,
presumably because that in the pre-training stage, the back-end tends to
overfit on the single-channel data, especially when the single-channel data
size is small.",-0.09114808,-0.041801017,-0.10215318,B
4131,We further study the technique         the paradigm.,"Furthermore, there is a uniﬁed inference process
                                          tion tasks with fewer trainable parameters than ﬁne-tuning spe-    with the original pre-trained LM for all downstream tasks in
                                          cialized downstream models.","Hence, less human labor is required in model au-
                                          in challenging sequence generation tasks.",2022-03-31 03:26:55+00:00,An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Wei-Cheng Tseng'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","Speech representations learned from Self-supervised learning (SSL) models
have been found beneficial for various speech processing tasks. However,
utilizing SSL representations usually requires fine-tuning the pre-trained
models or designing task-specific downstream models and loss functions, causing
much memory usage and human labor. On the other hand, prompting in Natural
Language Processing (NLP) is an efficient and widely used technique to leverage
pre-trained language models (LMs). Nevertheless, such a paradigm is little
studied in the speech community. We report in this paper the first exploration
of the prompt tuning paradigm for speech processing tasks based on Generative
Spoken Language Model (GSLM). Experiment results show that the prompt tuning
technique achieves competitive performance in speech classification tasks with
fewer trainable parameters than fine-tuning specialized downstream models. We
further study the technique in challenging sequence generation tasks. Prompt
tuning also demonstrates its potential, while the limitation and possible
research directions are discussed in this paper.",-0.29134512,-0.1634855,0.5403116,A
4132,"We further study the technique in challenging se-             ing paradigm signiﬁcantly improves memory and computation
                                         quence generation tasks.","Since parameters of tuned prompts are usually
                                         trainable parameters than ﬁne-tuning specialized downstream           several orders smaller than parameters of LMs [17], the prompt-
                                         models.",Prompt tuning also demonstrates its          efﬁciency.,2022-03-31 03:26:55+00:00,An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Wei-Cheng Tseng'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","Speech representations learned from Self-supervised learning (SSL) models can
benefit various speech processing tasks. However, utilizing SSL representations
usually requires fine-tuning the pre-trained models or designing task-specific
downstream models and loss functions, causing much memory usage and human
labor. Recently, prompting in Natural Language Processing (NLP) has been found
to be an efficient technique to leverage pre-trained language models (LMs).
Specifically, prompt tuning optimizes a limited number of task-specific
parameters with a fixed pre-trained model; as a result, only a small set of
parameters is needed to be stored for each task. Prompt tuning improves
computation and memory efficiency by leveraging the pre-trained LM's prediction
ability. Nevertheless, such a paradigm is little studied in the speech
community. We report in this paper the first exploration of the prompt tuning
paradigm for speech processing tasks based on Generative Spoken Language Model
(GSLM). Experiment results show that the prompt tuning technique achieves
competitive performance in speech classification tasks with fewer trainable
parameters than fine-tuning specialized downstream models. We further study the
technique in challenging sequence generation tasks. Prompt tuning also
demonstrates its potential, while the limitation and possible research
directions are discussed in this paper. The source code is available on
https://github.com/ga642381/SpeechPrompt.",-0.42737612,-0.035041332,0.4335945,C
4133,"We further study the technique in challenging se-              several orders smaller than parameters of LMs [17], the prompt-
                                          quence generation tasks.","Since parameters of tuned prompts are usually
                                          models.","Prompt tuning also demonstrates its           ing paradigm signiﬁcantly improves memory and computation
                                          potential, while the limitation and possible research directions       efﬁciency.",2022-03-31 03:26:55+00:00,SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Wei-Cheng Tseng'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","Speech representations learned from Self-supervised learning (SSL) models can
benefit various speech processing tasks. However, utilizing SSL representations
usually requires fine-tuning the pre-trained models or designing task-specific
downstream models and loss functions, causing much memory usage and human
labor. Recently, prompting in Natural Language Processing (NLP) has been found
to be an efficient technique to leverage pre-trained language models (LMs).
Specifically, prompt tuning optimizes a limited number of task-specific
parameters with a fixed pre-trained model; as a result, only a small set of
parameters is needed to be stored for each task. Prompt tuning improves
computation and memory efficiency by leveraging the pre-trained LM's prediction
ability. Nevertheless, such a paradigm is little studied in the speech
community. We report in this paper the first exploration of the prompt tuning
paradigm for speech processing tasks based on Generative Spoken Language Model
(GSLM). Experiment results show that the prompt tuning technique achieves
competitive performance in speech classification tasks with fewer trainable
parameters than fine-tuning specialized downstream models. We further study the
technique in challenging sequence generation tasks. Prompt tuning also
demonstrates its potential, while the limitation and possible research
directions are discussed in this paper. The source code is available on
https://github.com/ga642381/SpeechPrompt.",-0.39381889,-0.06138903,0.38345158,C
4173,"Results
                                                                       To further study the effects of the proposed new combination of
5.1.","Ablation study
                     5.","Objective results                                                 dynamic convolution and WadaIN, this work compares objec-
                                                                       tive results for all possible combinations, as shown in Table 4.",2022-03-31 16:46:32+00:00,Efficient Non-Autoregressive GAN Voice Conversion using VQWav2vec Features and Dynamic Convolution,eess.AS,['eess.AS'],"[arxiv.Result.Author('Mingjie Chen'), arxiv.Result.Author('Yanghao Zhou'), arxiv.Result.Author('Heyan Huang'), arxiv.Result.Author('Thomas Hain')]","It was shown recently that a combination of ASR and TTS models yield highly
competitive performance on standard voice conversion tasks such as the Voice
Conversion Challenge 2020 (VCC2020). To obtain good performance both models
require pretraining on large amounts of data, thereby obtaining large models
that are potentially inefficient in use. In this work we present a model that
is significantly smaller and thereby faster in processing while obtaining
equivalent performance. To achieve this the proposed model, Dynamic-GAN-VC
(DYGAN-VC), uses a non-autoregressive structure and makes use of vector
quantised embeddings obtained from a VQWav2vec model. Furthermore dynamic
convolution is introduced to improve speech content modeling while requiring a
small number of parameters. Objective and subjective evaluation was performed
using the VCC2020 task, yielding MOS scores of up to 3.86, and character error
rates as low as 4.3\%. This was achieved with approximately half the number of
model parameters, and up to 8 times faster decoding speed.",-0.12389821,-0.16760924,-0.20733544,B
4389,"However, further research is
                                                                                                            needed to understand the level of inﬂuence that the data (e.g.,
                                         Speech quality assessment is key to monitoring and evaluating      language, type of distortion, amount of samples) used for ﬁne-
                                         the performance of applications and services in which speech       tuning is exerting over the model’s performance.","Introduction                                  methods (e.g., MUSHRA, MOS).",is an essential component.,2022-04-05 11:57:31+00:00,Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction,eess.AS,['eess.AS'],"[arxiv.Result.Author('Helard Becerra'), arxiv.Result.Author('Alessandro Ragano'), arxiv.Result.Author('Andrew Hines')]","Recent studies have shown how self-supervised models can produce accurate
speech quality predictions. Speech representations generated by the pre-trained
wav2vec 2.0 model allows constructing robust predicting models using small
amounts of annotated data. This opens the possibility of developing strong
models in scenarios where labelled data is scarce. It is known that fine-tuning
improves the model's performance; however, it is unclear how the data (e.g.,
language, amount of samples) used for fine-tuning is influencing that
performance. In this paper, we explore how using different speech corpus to
fine-tune the wav2vec 2.0 can influence its performance. We took four speech
datasets containing degradations found in common conferencing applications and
fine-tuned wav2vec 2.0 targeting different languages and data size scenarios.
The fine-tuned models were tested across all four conferencing datasets plus an
additional dataset containing synthetic speech and they were compared against
three external baseline models. Results showed that fine-tuned models were able
to compete with baseline models. Larger fine-tune data guarantee better
performance; meanwhile, diversity in language helped the models deal with
specific languages. Further research is needed to evaluate other wav2vec 2.0
models pre-trained with multi-lingual datasets and to develop prediction models
that are more resilient to language diversity.",0.06050528,-0.09643194,0.05587537,B
4421,"This supports                    As further study, we re-trained all CNNs and QCNNs on
our hypothesis that a quaternion-value decoder is able to                    the Ravdess dataset, progressively decreasing the amount of
create embeddings that present more suitable intra-channel                   training and validation data.","As we expected, the                 C. Reducing training data
quaternion-valued decoder of the actual RH-emo outperforms
the completely real-valued version (by 2.3pp).","The size of the test set, instead,
correlations for the quaternion-valued CNNs.",2022-04-05 17:45:09+00:00,Learning Speech Emotion Representations in the Quaternion Domain,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Eric Guizzo'), arxiv.Result.Author('Tillman Weyde'), arxiv.Result.Author('Simone Scardapane'), arxiv.Result.Author('Danilo Comminiello')]","The modeling of human emotion expression in speech signals is an important,
yet challenging task. The high resource demand of speech emotion recognition
models, combined with the the general scarcity of emotion-labelled data are
obstacles to the development and application of effective solutions in this
field. In this paper, we present an approach to jointly circumvent these
difficulties. Our method, named RH-emo, is a novel semi-supervised architecture
aimed at extracting quaternion embeddings from real-valued monoaural
spectrograms, enabling the use of quaternion-valued networks for speech emotion
recognition tasks. RH-emo is a hybrid real/quaternion autoencoder network that
consists of a real-valued encoder in parallel to a real-valued emotion
classifier and a quaternion-valued decoder. On the one hand, the classifier
permits to optimize each latent axis of the embeddings for the classification
of a specific emotion-related characteristic: valence, arousal, dominance and
overall emotion. On the other hand, the quaternion reconstruction enables the
latent dimension to develop intra-channel correlations that are required for an
effective representation as a quaternion entity. We test our approach on speech
emotion recognition tasks using four popular datasets: Iemocap, Ravdess, EmoDb
and Tess, comparing the performance of three well-established real-valued CNN
architectures (AlexNet, ResNet-50, VGG) and their quaternion-valued equivalent
fed with the embeddings created with RH-emo. We obtain a consistent improvement
in the test accuracy for all datasets, while drastically reducing the
resources' demand of models. Moreover, we performed additional experiments and
ablation studies that confirm the effectiveness of our approach. The RH-emo
repository is available at: https://github.com/ispamm/rhemo.",-0.14699158,-0.007926492,0.07586551,B
4462,"strategy for this issue and other further research directions in the         Association for Computing Machinery, Bucharest, Romania, 278–286.","We will discuss a mitigation          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania).","https:
following section.",2022-04-06 14:06:04+00:00,Spectral Denoising for Microphone Classification,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('L. Cuccovillo'), arxiv.Result.Author('A. Giganti'), arxiv.Result.Author('P. Bestagini'), arxiv.Result.Author('P. Aichroth'), arxiv.Result.Author('S. Tubaro')]","In this paper, we propose the application of denoising to microphone
classification, to enable its usage on content with unfavorable noisy
conditions. We first describe the proposed integrated approach; afterwards we
discuss the baseline algorithm for microphone classification, and the various
denoising procedures which can be combined with it in the time or spectral
domain; lastly, we determine the best performing denoising procedure, and
evaluate the performance of the integrated approach with several SNR levels of
additive input noise. In comparison to the reference baseline, the proposed
method achieves an average accuracy increase of about 25% on denoised content.",-0.11081648,0.7322492,-0.07953609,C_centroid
4463,"strategy for this issue and other further research directions in the         Association for Computing Machinery, Bucharest, Romania, 278–286.","We will discuss a mitigation          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania).","https:
following section.",2022-04-06 14:06:04+00:00,Spectral Denoising for Microphone Classification,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('L. Cuccovillo'), arxiv.Result.Author('A. Giganti'), arxiv.Result.Author('P. Bestagini'), arxiv.Result.Author('P. Aichroth'), arxiv.Result.Author('S. Tubaro')]","In this paper, we propose the application of denoising to microphone
classification, to enable its usage on content with unfavorable noisy
conditions. We first describe the proposed integrated approach; afterwards we
discuss the baseline algorithm for microphone classification, and the various
denoising procedures which can be combined with it in the time or spectral
domain; lastly, we determine the best performing denoising procedure, and
evaluate the performance of the integrated approach with several SNR levels of
additive input noise. In comparison to the reference baseline, the proposed
method achieves an average accuracy increase of about 25% on denoised content.",-0.11081648,0.7322492,-0.07953609,C
4464,"gation strategy for this issue and other further research directions          Association for Computing Machinery, Bucharest, Romania, 278–286.","We will discuss a miti-          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania).","https:
in the following section.",2022-04-06 14:06:04+00:00,Spectral Denoising for Microphone Classification,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('L. Cuccovillo'), arxiv.Result.Author('A. Giganti'), arxiv.Result.Author('P. Bestagini'), arxiv.Result.Author('P. Aichroth'), arxiv.Result.Author('S. Tubaro')]","In this paper, we propose the use of denoising for microphone classification,
to enable its usage for several key application domains that involve noisy
conditions. We describe the proposed analysis pipeline and the baseline
algorithm for microphone classification, and discuss various denoising
approaches which can be applied to it within the time or spectral domain;
finally, we determine the best-performing denoising procedure, and evaluate the
performance of the overall, integrated approach with several SNR levels of
additive input noise. As a result, the proposed method achieves an average
accuracy increase of about 25% on denoised content over the reference baseline.",-0.107476234,0.73117626,-0.07208578,C
4465,"gation strategy for this issue and other further research directions          Association for Computing Machinery, Bucharest, Romania, 278–286.","We will discuss a miti-          International Conference on Multimedia Retrieval (ICMR) (Bucharest, Romania).","https:
in the following section.",2022-04-06 14:06:04+00:00,Spectral Denoising for Microphone Classification,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('L. Cuccovillo'), arxiv.Result.Author('A. Giganti'), arxiv.Result.Author('P. Bestagini'), arxiv.Result.Author('P. Aichroth'), arxiv.Result.Author('S. Tubaro')]","In this paper, we propose the use of denoising for microphone classification,
to enable its usage for several key application domains that involve noisy
conditions. We describe the proposed analysis pipeline and the baseline
algorithm for microphone classification, and discuss various denoising
approaches which can be applied to it within the time or spectral domain;
finally, we determine the best-performing denoising procedure, and evaluate the
performance of the overall, integrated approach with several SNR levels of
additive input noise. As a result, the proposed method achieves an average
accuracy increase of about 25% on denoised content over the reference baseline.",-0.107476234,0.73117626,-0.07208578,C
4507,We further study the relationship be-        to SE.,"By in-
                                         tegrating the SSL representation and spectrogram, the result can           Currently, there are only few studies applying SSL features
                                         be signiﬁcantly boosted.",Huang et al.,2022-04-07 10:22:26+00:00,Boosting Self-Supervised Embeddings for Speech Enhancement,eess.AS,['eess.AS'],"[arxiv.Result.Author('Kuo-Hsuan Hung'), arxiv.Result.Author('Szu-wei Fu'), arxiv.Result.Author('Huan-Hsin Tseng'), arxiv.Result.Author('Hsin-Tien Chiang'), arxiv.Result.Author('Yu Tsao'), arxiv.Result.Author('Chii-Wann Lin')]","Self-supervised learning (SSL) representation for speech has achieved
state-of-the-art (SOTA) performance on several downstream tasks. However, there
remains room for improvement in speech enhancement (SE) tasks. In this study,
we used a cross-domain feature to solve the problem that SSL embeddings may
lack fine-grained information to regenerate speech signals. By integrating the
SSL representation and spectrogram, the result can be significantly boosted. We
further study the relationship between the noise robustness of SSL
representation via clean-noisy distance (CN distance) and the layer importance
for SE. Consequently, we found that SSL representations with lower noise
robustness are more important. Furthermore, our experiments on the VCTK-DEMAND
dataset demonstrated that fine-tuning an SSL representation with an SE model
can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without
invoking complicated network architectures. In later experiments, the CN
distance in SSL embeddings was observed to increase after fine-tuning. These
results verify our expectations and may help design SE-related SSL training in
the future.",-0.34662342,-0.018347561,-0.525789,B
4508,We further study the relationship be-        to SE.,"By in-
                                         tegrating the SSL representation and spectrogram, the result can           Currently, there are only few studies applying SSL features
                                         be signiﬁcantly boosted.",Huang et al.,2022-04-07 10:22:26+00:00,Boosting Self-Supervised Embeddings for Speech Enhancement,eess.AS,['eess.AS'],"[arxiv.Result.Author('Kuo-Hsuan Hung'), arxiv.Result.Author('Szu-wei Fu'), arxiv.Result.Author('Huan-Hsin Tseng'), arxiv.Result.Author('Hsin-Tien Chiang'), arxiv.Result.Author('Yu Tsao'), arxiv.Result.Author('Chii-Wann Lin')]","Self-supervised learning (SSL) representation for speech has achieved
state-of-the-art (SOTA) performance on several downstream tasks. However, there
remains room for improvement in speech enhancement (SE) tasks. In this study,
we used a cross-domain feature to solve the problem that SSL embeddings may
lack fine-grained information to regenerate speech signals. By integrating the
SSL representation and spectrogram, the result can be significantly boosted. We
further study the relationship between the noise robustness of SSL
representation via clean-noisy distance (CN distance) and the layer importance
for SE. Consequently, we found that SSL representations with lower noise
robustness are more important. Furthermore, our experiments on the VCTK-DEMAND
dataset demonstrated that fine-tuning an SSL representation with an SE model
can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without
invoking complicated network architectures. In later experiments, the CN
distance in SSL embeddings was observed to increase after fine-tuning. These
results verify our expectations and may help design SE-related SSL training in
the future.",-0.34662342,-0.018347561,-0.525789,B
4558,"Figure 6
plots the mAP values as a function of the SDRi for the                  on real recordings, further research is still needed to improve
different systems in Table V. Although the number of systems
is limited, the results suggest a relationship between SDRi             extraction performance by, for example, designing simulated
and mAP values.","We observe that the mAP values increase greatly after                Second, although we demonstrated promising initial results
TSE, achieving values over 0.5 for most models.","This justiﬁes our use of mAP to evaluate
extraction performance on real mixtures when it is not possible         training data that approximate better mixing conditions of real
to compute the SDR values.",2022-04-08 07:48:45+00:00,SoundBeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Marc Delcroix'), arxiv.Result.Author('Jorge Bennasar Vázquez'), arxiv.Result.Author('Tsubasa Ochiai'), arxiv.Result.Author('Keisuke Kinoshita'), arxiv.Result.Author('Yasunori Ohishi'), arxiv.Result.Author('Shoko Araki')]","In many situations, we would like to hear desired sound events (SEs) while
being able to ignore interference. Target sound extraction (TSE) aims at
tackling this problem by estimating the sound of target SE classes in a mixture
while suppressing all other sounds. We can achieve this with a neural network
that extracts the target SEs by conditioning it on clues representing the
target SE classes. Two types of clues have been proposed, i.e., target SE class
labels and enrollment sound samples similar to the target sound. Systems based
on SE class labels can directly optimize embedding vectors representing the SE
classes, resulting in high extraction performance. However, extending these
systems to the extraction of new SE classes not encountered during training is
not easy. Enrollment-based approaches extract SEs by finding sounds in the
mixtures that share similar characteristics to the enrollment. These approaches
do not explicitly rely on SE class definitions and can thus handle new SE
classes. In this paper, we introduce a TSE framework, SoundBeam, that combines
the advantages of both approaches. We also perform an extensive evaluation of
the different TSE schemes using synthesized and real mixtures, which shows the
potential of SoundBeam.",-0.10696265,0.0017072973,0.13713738,B
4559,"Second, although we demonstrated promising initial                          “Zero-shot audio source separation through query-based learning from
results on real recordings, further research is still needed to                       weakly-labeled data,” in Proc.","Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov,
required.","of the AAAI, vol.",2022-04-08 07:48:45+00:00,SoundBeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Marc Delcroix'), arxiv.Result.Author('Jorge Bennasar Vázquez'), arxiv.Result.Author('Tsubasa Ochiai'), arxiv.Result.Author('Keisuke Kinoshita'), arxiv.Result.Author('Yasunori Ohishi'), arxiv.Result.Author('Shoko Araki')]","In many situations, we would like to hear desired sound events (SEs) while
being able to ignore interference. Target sound extraction (TSE) tackles this
problem by estimating the audio signal of the sounds of target SE classes in a
mixture of sounds while suppressing all other sounds. We can achieve this with
a neural network that extracts the target SEs by conditioning it on clues
representing the target SE classes. Two types of clues have been proposed,
i.e., target SE class labels and enrollment audio samples (or audio queries),
which are pre-recorded audio samples of sounds from the target SE classes.
Systems based on SE class labels can directly optimize embedding vectors
representing the SE classes, resulting in high extraction performance. However,
extending these systems to extract new SE classes not encountered during
training is not easy. Enrollment-based approaches extract SEs by finding sounds
in the mixtures that share similar characteristics to the enrollment audio
samples. These approaches do not explicitly rely on SE class definitions and
can thus handle new SE classes. In this paper, we introduce a TSE framework,
SoundBeam, that combines the advantages of both approaches. We also perform an
extensive evaluation of the different TSE schemes using synthesized and real
mixtures, which shows the potential of SoundBeam.",0.23540317,0.008450704,0.061485477,A
5576,"The device should be attentive only to        for reproducibility and for promotion of further research on this
                                                   the members of a given household, i.e.","Importantly, we make
                                                                                                                    our experimental pipeline, methods, and protocols public1 both
                                                • Open-set operation.",the ability to ignore     topic.,2022-04-30 15:04:56+00:00,Baselines and Protocols for Household Speaker Recognition,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Alexey Sholokhov'), arxiv.Result.Author('Xuechen Liu'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Tomi Kinnunen')]","Speaker recognition on household devices, such as smart speakers, features
several challenges: (i) robustness across a vast number of heterogeneous
domains (households), (ii) short utterances, (iii) possibly absent speaker
labels of the enrollment data (passive enrollment), and (iv) presence of
unknown persons (guests). While many commercial products exist, there is less
published research and no publicly-available evaluation protocols or
open-source baselines. Our work serves to bridge this gap by providing an
accessible evaluation benchmark derived from public resources (VoxCeleb and
ASVspoof 2019 data) along with a preliminary pool of open-source baselines.
This includes four algorithms for active enrollment (speaker labels available)
and one algorithm for passive enrollment.",-0.26131782,-0.21763946,-0.2083765,B
5577,"The device should be attentive only to        for reproducibility and for promotion of further research on this
                                                  the members of a given household, i.e.","Importantly, we make
                                                                                                                   our experimental pipeline, methods, and protocols public1 both
                                               • Open-set operation.",the ability to ignore     topic.,2022-04-30 15:04:56+00:00,Baselines and Protocols for Household Speaker Recognition,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Alexey Sholokhov'), arxiv.Result.Author('Xuechen Liu'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Tomi Kinnunen')]","Speaker recognition on household devices, such as smart speakers, features
several challenges: (i) robustness across a vast number of heterogeneous
domains (households), (ii) short utterances, (iii) possibly absent speaker
labels of the enrollment data (passive enrollment), and (iv) presence of
unknown persons (guests). While many commercial products exist, there is less
published research and no publicly-available evaluation protocols or
open-source baselines. Our work serves to bridge this gap by providing an
accessible evaluation benchmark derived from public resources (VoxCeleb and
ASVspoof 2019 data) along with a preliminary pool of open-source baselines.
This includes four algorithms for active enrollment (speaker labels available)
and one algorithm for passive enrollment.",-0.26131782,-0.21763946,-0.2083765,B
5962,"We further study where the quality gap comes from by analyzing each component in one
of the above TTS systems in Appendix.","As shown in Table 1, although the current TTS systems
can achieve close MOS with recordings, they have a large CMOS gap to recordings, with Wilcoxon
signed rank test at p-level p 0.05, which shows statistically signiﬁcant difference from human
recordings.",A.,2022-05-09 16:57:35+00:00,NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality,eess.AS,"['eess.AS', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Jiawei Chen'), arxiv.Result.Author('Haohe Liu'), arxiv.Result.Author('Jian Cong'), arxiv.Result.Author('Chen Zhang'), arxiv.Result.Author('Yanqing Liu'), arxiv.Result.Author('Xi Wang'), arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Yuanhao Yi'), arxiv.Result.Author('Lei He'), arxiv.Result.Author('Frank Soong'), arxiv.Result.Author('Tao Qin'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Tie-Yan Liu')]","Text to speech (TTS) has made rapid progress in both academia and industry in
recent years. Some questions naturally arise that whether a TTS system can
achieve human-level quality, how to define/judge human-level quality and how to
achieve it. In this paper, we answer these questions by first defining
human-level quality based on statistical significance of measurement and
describing the guidelines to judge it, and then proposing a TTS system called
NaturalSpeech that achieves human-level quality on a benchmark dataset.
Specifically, we leverage a variational autoencoder (VAE) for end-to-end text
to waveform generation, with several key designs to enhance the capacity of
prior from text and reduce the complexity of posterior from speech, including
phoneme pre-training, differentiable duration modeling, bidirectional
prior/posterior modeling, and memory mechanism in VAE. Experiment evaluations
on popular LJSpeech dataset show that our proposed NaturalSpeech achieves -0.01
CMOS (comparative mean opinion score) to human recordings on sentence level,
with Wilcoxon signed rank test at p-level p>>0.05, which demonstrates no
statistically significant difference from human recordings for the first time
on this dataset.",-0.20751677,-0.14743306,-0.027095063,B
5963,"We further study where the quality gap comes from by analyzing each component
in one of the above TTS systems in Appendix A.","As shown in Table 1, although the current TTS
systems can achieve close MOS with recordings, they have a large CMOS gap to recordings, with
Wilcoxon signed rank test at p-level p 0.05, which shows statistically signiﬁcant difference from
human recordings.",Table 1: The MOS and CMOS comparisons between previous TTS systems and human recordings.,2022-05-09 16:57:35+00:00,NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality,eess.AS,"['eess.AS', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Jiawei Chen'), arxiv.Result.Author('Haohe Liu'), arxiv.Result.Author('Jian Cong'), arxiv.Result.Author('Chen Zhang'), arxiv.Result.Author('Yanqing Liu'), arxiv.Result.Author('Xi Wang'), arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Yuanhao Yi'), arxiv.Result.Author('Lei He'), arxiv.Result.Author('Frank Soong'), arxiv.Result.Author('Tao Qin'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Tie-Yan Liu')]","Text to speech (TTS) has made rapid progress in both academia and industry in
recent years. Some questions naturally arise that whether a TTS system can
achieve human-level quality, how to define/judge that quality and how to
achieve it. In this paper, we answer these questions by first defining the
human-level quality based on the statistical significance of subjective measure
and introducing appropriate guidelines to judge it, and then developing a TTS
system called NaturalSpeech that achieves human-level quality on a benchmark
dataset. Specifically, we leverage a variational autoencoder (VAE) for
end-to-end text to waveform generation, with several key modules to enhance the
capacity of the prior from text and reduce the complexity of the posterior from
speech, including phoneme pre-training, differentiable duration modeling,
bidirectional prior/posterior modeling, and a memory mechanism in VAE.
Experiment evaluations on popular LJSpeech dataset show that our proposed
NaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human
recordings at the sentence level, with Wilcoxon signed rank test at p-level p
>> 0.05, which demonstrates no statistically significant difference from human
recordings for the first time on this dataset.",-0.19537956,-0.15507856,-0.028677603,B
6197,"(2020a;b)
      many audio samples and corresponding transcripts,            further study a way to include a hierarchical, ﬁne-grained
      which is often costly or even impossible.",Sun et al.,2) Style adap-     prosody representation.,2022-05-15 08:16:02+00:00,GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis,eess.AS,"['eess.AS', 'cs.CL', 'cs.SD']","[arxiv.Result.Author('Rongjie Huang'), arxiv.Result.Author('Yi Ren'), arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('Chenye Cui'), arxiv.Result.Author('Zhou Zhao')]","Style transfer for out-of-domain (OOD) speech synthesis aims to generate
speech samples with unseen style (e.g., speaker identity, emotion, and prosody)
derived from an acoustic reference, while facing the following challenges: 1)
The highly dynamic style features in expressive voice are difficult to model
and transfer; and 2) the TTS models should be robust enough to handle diverse
OOD conditions that differ from the source data. This paper proposes
GenerSpeech, a text-to-speech model towards high-fidelity zero-shot style
transfer of OOD custom voice. GenerSpeech decomposes the speech variation into
the style-agnostic and style-specific parts by introducing two components: 1) a
multi-level style adaptor to efficiently model a large range of style
conditions, including global speaker and emotion characteristics, and the local
(utterance, phoneme, and word-level) fine-grained prosodic representations; and
2) a generalizable content adaptor with Mix-Style Layer Normalization to
eliminate style information in the linguistic content representation and thus
improve model generalization. Our evaluations on zero-shot style transfer
demonstrate that GenerSpeech surpasses the state-of-the-art models in terms of
audio quality and style similarity. The extension studies to adaptive style
transfer further show that GenerSpeech performs robustly in the few-shot data
setting. Audio samples are available at \url{https://GenerSpeech.github.io/}",0.30628705,-0.20404422,-0.04686512,A_centroid
6198,"[41, 42] further study a way to include a hierarchical, ﬁne-grained prosody representation.",Sun et al.,Li et al.,2022-05-15 08:16:02+00:00,GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech,eess.AS,"['eess.AS', 'cs.CL', 'cs.SD']","[arxiv.Result.Author('Rongjie Huang'), arxiv.Result.Author('Yi Ren'), arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('Chenye Cui'), arxiv.Result.Author('Zhou Zhao')]","Style transfer for out-of-domain (OOD) speech synthesis aims to generate
speech samples with unseen style (e.g., speaker identity, emotion, and prosody)
derived from an acoustic reference, while facing the following challenges: 1)
The highly dynamic style features in expressive voice are difficult to model
and transfer; and 2) the TTS models should be robust enough to handle diverse
OOD conditions that differ from the source data. This paper proposes
GenerSpeech, a text-to-speech model towards high-fidelity zero-shot style
transfer of OOD custom voice. GenerSpeech decomposes the speech variation into
the style-agnostic and style-specific parts by introducing two components: 1) a
multi-level style adaptor to efficiently model a large range of style
conditions, including global speaker and emotion characteristics, and the local
(utterance, phoneme, and word-level) fine-grained prosodic representations; and
2) a generalizable content adaptor with Mix-Style Layer Normalization to
eliminate style information in the linguistic content representation and thus
improve model generalization. Our evaluations on zero-shot style transfer
demonstrate that GenerSpeech surpasses the state-of-the-art models in terms of
audio quality and style similarity. The extension studies to adaptive style
transfer further show that GenerSpeech performs robustly in the few-shot data
setting. Audio samples are available at https://GenerSpeech.github.io/",0.16638586,-0.35884058,-0.18880978,A
6935,"It is worth noting that models such as
                                          VITS [1] have used a combination of these techniques to achieve the state-of-the-art performance,
                                          however, as we will demonstrate, the synthesized speech from the current models is still perceptually
                                          distinguishable from real human speech which warrants further research.","Several approaches have been proposed to address such a problem, including methods based on
                                          variational inference [1, 4, 5, 6], ﬂow-based modeling [1, 7, 8], controlling pitch, duration and energy
                                          [9, 10], and using external prosody encoder [11, 12, 13].","In particular, the speaking
                                          styles and emotional tones of different speakers remain difﬁcult to model and incorporate adequately.",2022-05-30 21:34:40+00:00,StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Yinghao Aaron Li'), arxiv.Result.Author('Cong Han'), arxiv.Result.Author('Nima Mesgarani')]","Text-to-Speech (TTS) has recently seen great progress in synthesizing
high-quality speech owing to the rapid development of parallel TTS systems, but
producing speech with naturalistic prosodic variations, speaking styles and
emotional tones remains challenging. Moreover, since duration and speech are
generated separately, parallel TTS models still have problems finding the best
monotonic alignments that are crucial for naturalistic speech synthesis. Here,
we propose StyleTTS, a style-based generative model for parallel TTS that can
synthesize diverse speech with natural prosody from a reference speech
utterance. With novel Transferable Monotonic Aligner (TMA) and
duration-invariant data augmentation schemes, our method significantly
outperforms state-of-the-art models on both single and multi-speaker datasets
in subjective tests of speech naturalness and speaker similarity. Through
self-supervised learning of the speaking styles, our model can synthesize
speech with the same prosodic and emotional tone as any given reference speech
without the need for explicitly labeling these categories.",0.22542626,-0.31721368,0.16279405,A
6936,"We will release our source code and pre-trained
models to further research in the future directions.","Future work includes discovering a better way of selecting references

                                                           9
and predicting the most suitable style from the text.","6 Acknowledgments

We thank Gavin Mischler and Vinay Raghavan for their efforts in listening to and rating the naturalness
of the synthesized speech and providing feedback for the quality of models during the development
stage of this work.",2022-05-30 21:34:40+00:00,StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Yinghao Aaron Li'), arxiv.Result.Author('Cong Han'), arxiv.Result.Author('Nima Mesgarani')]","Text-to-Speech (TTS) has recently seen great progress in synthesizing
high-quality speech owing to the rapid development of parallel TTS systems, but
producing speech with naturalistic prosodic variations, speaking styles and
emotional tones remains challenging. Moreover, since duration and speech are
generated separately, parallel TTS models still have problems finding the best
monotonic alignments that are crucial for naturalistic speech synthesis. Here,
we propose StyleTTS, a style-based generative model for parallel TTS that can
synthesize diverse speech with natural prosody from a reference speech
utterance. With novel Transferable Monotonic Aligner (TMA) and
duration-invariant data augmentation schemes, our method significantly
outperforms state-of-the-art models on both single and multi-speaker datasets
in subjective tests of speech naturalness and speaker similarity. Through
self-supervised learning of the speaking styles, our model can synthesize
speech with the same prosodic and emotional tone as any given reference speech
without the need for explicitly labeling these categories.",0.23154914,-0.19798058,0.23952153,A
7532,"The audio samples are
                                          DCASE 2022 Challenge Task 6, we aim to inspire further research            collected following the procedure described in [8], by randomly se-
                                          into language-based audio retrieval with unconstrained textual de-         lecting those samples that were not used in the Clotho v2.",Through the sub-task B of the        caption is provided for each audio sample.,"Table 1
                                          scriptions.",2022-06-13 12:49:30+00:00,DCASE 2022 Challenge Task 6B: Language-Based Audio Retrieval,eess.AS,['eess.AS'],"[arxiv.Result.Author('Huang Xie'), arxiv.Result.Author('Samuel Lipping'), arxiv.Result.Author('Tuomas Virtanen')]","In this report, we introduce the task setup and the baseline system for the
sub-task B of the DCASE 2022 Challenge Task 6: language-based audio retrieval
subtask. For this subtask, the Clotho v2 dataset is utilized as the development
dataset, and an additional dataset consisting of 1,000 audio-caption pairs as
the evaluation dataset. We train the baseline system with the development
dataset, and evaluate it on the evaluation dataset to provide some initial
results for this subtask.",0.48571563,0.21939112,0.06177444,A
7533,"The audio samples are
                                          DCASE 2022 Challenge Task 6, we aim to inspire further research            collected following the procedure described in [8], by optimizing
                                          into language-based audio retrieval with unconstrained textual de-         the tag distribution of the selected samples.",Through the sub-task B of the        caption is provided for each audio sample.,"The samples were se-
                                          scriptions.",2022-06-13 12:49:30+00:00,DCASE 2022 Challenge Task 6B: Language-Based Audio Retrieval,eess.AS,['eess.AS'],"[arxiv.Result.Author('Huang Xie'), arxiv.Result.Author('Samuel Lipping'), arxiv.Result.Author('Tuomas Virtanen')]","In this report, we introduce the task setup and the baseline system for the
sub-task B of the DCASE 2022 Challenge Task 6: language-based audio retrieval
subtask. For this subtask, the Clotho v2 dataset is utilized as the development
dataset, and an additional dataset consisting of 1,000 audio-caption pairs as
the evaluation dataset. We train the baseline system with the development
dataset, and evaluate it on the evaluation dataset to provide some initial
results for this subtask.",0.48444083,0.2411721,0.106170624,A
7534,"audio retrieval is introduced into as Subtask 6B, which aims to in-
                                          spire further research into audio retrieval with unconstrained textual  2.3.",The data collecting procedure is explained in detail in [8].,"Clotho Retrieval Evaluation Dataset
                                          descriptions.",2022-06-13 12:49:30+00:00,Language-based Audio Retrieval Task in DCASE 2022 Challenge,eess.AS,['eess.AS'],"[arxiv.Result.Author('Huang Xie'), arxiv.Result.Author('Samuel Lipping'), arxiv.Result.Author('Tuomas Virtanen')]","Language-based audio retrieval is a task, where natural language textual
captions are used as queries to retrieve audio signals from a dataset. It has
been first introduced into DCASE 2022 Challenge as Subtask 6B of task 6, which
aims at developing computational systems to model relationships between audio
signals and free-form textual descriptions. Compared with audio captioning
(Subtask 6A), which is about generating audio captions for audio signals,
language-based audio retrieval (Subtask 6B) focuses on ranking audio signals
according to their relevance to natural language textual captions. In DCASE
2022 Challenge, the provided baseline system for Subtask 6B was significantly
outperformed, with top performance being 0.276 in mAP@10. This paper presents
the outcome of Subtask 6B in terms of submitted systems' performance and
analysis.",0.42931652,0.31367058,-0.08391164,A
7608,"As such, audio generation methods
could be used for creating misleading content and we believe that further research is required to
ensure the capabilities are used for creating a positive societal impact.","Audio generation is a signiﬁcant task
with tremendous applications, e.g., human-computer interface.","The Generative Adversarial Nets (GANs) [17] that we use for our experiments on audio generation
have a dedicated module, i.e., the discriminator, for detecting the real from the fake content.",2022-06-14 12:58:59+00:00,Adversarial Audio Synthesis with Complex-valued Polynomial Networks,eess.AS,"['eess.AS', 'cs.LG']","[arxiv.Result.Author('Yongtao Wu'), arxiv.Result.Author('Grigorios G Chrysos'), arxiv.Result.Author('Volkan Cevher')]","Time-frequency (TF) representations in audio synthesis have been increasingly
modeled with real-valued networks. However, overlooking the complex-valued
nature of TF representations can result in suboptimal performance and require
additional modules (e.g., for modeling the phase). To this end, we introduce
complex-valued polynomial networks, called APOLLO, that integrate such
complex-valued representations in a natural way. Concretely, APOLLO captures
high-order correlations of the input elements using high-order tensors as
scaling parameters. By leveraging standard tensor decompositions, we derive
different architectures and enable modeling richer correlations. We outline
such architectures and showcase their performance in audio generation across
four benchmarks. As a highlight, APOLLO results in $17.5\%$ improvement over
adversarial methods and $8.2\%$ over the state-of-the-art diffusion models on
SC09 dataset in audio generation. Our models can encourage the systematic
design of other efficient architectures on the complex field.",0.27395222,-0.15893784,0.15896401,A
7609,"As such, audio generation methods
could be used for creating misleading content and we believe that further research is required to
ensure the capabilities are used for creating a positive societal impact.","Audio generation is a signiﬁcant task
with tremendous applications, e.g., human-computer interface.","The Generative Adversarial Nets (GANs) [17] that we use for our experiments on audio generation
have a dedicated module, i.e., the discriminator, for detecting the real from the fake content.",2022-06-14 12:58:59+00:00,Adversarial Audio Synthesis with Complex-valued Polynomial Networks,eess.AS,"['eess.AS', 'cs.LG']","[arxiv.Result.Author('Yongtao Wu'), arxiv.Result.Author('Grigorios G Chrysos'), arxiv.Result.Author('Volkan Cevher')]","Time-frequency (TF) representations in audio synthesis have been increasingly
modeled with real-valued networks. However, overlooking the complex-valued
nature of TF representations can result in suboptimal performance and require
additional modules (e.g., for modeling the phase). To this end, we introduce
complex-valued polynomial networks, called APOLLO, that integrate such
complex-valued representations in a natural way. Concretely, APOLLO captures
high-order correlations of the input elements using high-order tensors as
scaling parameters. By leveraging standard tensor decompositions, we derive
different architectures and enable modeling richer correlations. We outline
such architectures and showcase their performance in audio generation across
four benchmarks. As a highlight, APOLLO results in $17.5\%$ improvement over
adversarial methods and $8.2\%$ over the state-of-the-art diffusion models on
SC09 dataset in audio generation. Our models can encourage the systematic
design of other efficient architectures on the complex field.",0.27395225,-0.15893789,0.15896411,A
7701,"[18] D. Jiang, W. Li et al., “A further study of unsupervised pretrain-
      ing for transformer based speech recognition,” in IEEE ICASSP,      [41] V. Panayotov, G. Chen et al., “Librispeech: an asr corpus based
      2021, pp.","[17] X. Chang, T. Maekaku et al., “An exploration of self-supervised
      pretrained representations for end-to-end speech recognition,”      [40] S. Kessler, B. Thomas, and S. Karout, “Continual-wav2vec2: an
      ASRU, 2021.                                                               application of continual learning for self-supervised automatic
                                                                                speech recognition,” IEEE ICASSP, 2022.",6538–6542.,2022-06-16 05:28:31+00:00,DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children's ASR,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Ruchao Fan'), arxiv.Result.Author('Abeer Alwan')]","Self-supervised learning (SSL) in the pretraining stage using un-annotated
speech data has been successful in low-resource automatic speech recognition
(ASR) tasks. However, models trained through SSL are biased to the pretraining
data which is usually different from the data used in finetuning tasks, causing
a domain shifting problem, and thus resulting in limited knowledge transfer. We
propose a novel framework, domain responsible adaptation and finetuning
(DRAFT), to reduce domain shifting in pretrained speech models through an
additional adaptation stage. In DRAFT, residual adapters (RAs) are inserted in
the pretrained model to learn domain-related information with the same SSL loss
as the pretraining stage. Only RA parameters are updated during the adaptation
stage. DRAFT is agnostic to the type of SSL method used and is evaluated with
three widely used approaches: APC, Wav2vec2.0, and HuBERT. On two child ASR
tasks (OGI and MyST databases), using SSL models trained with un-annotated
adult speech data (Librispeech), relative WER improvements of up to 19.7% are
observed when compared to the pretrained models without adaptation. Additional
experiments examined the potential of cross knowledge transfer between the two
datasets and the results are promising, showing a broader usage of the proposed
DRAFT framework.",0.12509473,0.07333414,0.111475416,A
7943,"The COVYT dataset as well as the code
for our machine learning experiments are publicly available to facilitate reproducibility and to motivate further research
comparable to the provided baselines2.","We present dataset baselines with respect to (i)
the manifestation of a COVID-19 infection in different acoustic descriptors and (ii) automatic COVID-19 detection in
various scenarios addressing several factors that inﬂuence model performance.","1The term ‘detection’ is typically used as an indication of presence of a speciﬁc event (i. e., sound event detection).",2022-06-20 16:26:51+00:00,COVYT: Introducing the Coronavirus YouTube and TikTok speech dataset featuring the same speakers with and without infection,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Anastasia Semertzidou'), arxiv.Result.Author('Meishu Song'), arxiv.Result.Author('Florian B. Pokorny'), arxiv.Result.Author('Björn W. Schuller')]","More than two years after its outbreak, the COVID-19 pandemic continues to
plague medical systems around the world, putting a strain on scarce resources,
and claiming human lives. From the very beginning, various AI-based COVID-19
detection and monitoring tools have been pursued in an attempt to stem the tide
of infections through timely diagnosis. In particular, computer audition has
been suggested as a non-invasive, cost-efficient, and eco-friendly alternative
for detecting COVID-19 infections through vocal sounds. However, like all AI
methods, also computer audition is heavily dependent on the quantity and
quality of available data, and large-scale COVID-19 sound datasets are
difficult to acquire -- amongst other reasons -- due to the sensitive nature of
such data. To that end, we introduce the COVYT dataset -- a novel COVID-19
dataset collected from public sources containing more than 8 hours of speech
from 65 speakers. As compared to other existing COVID-19 sound datasets, the
unique feature of the COVYT dataset is that it comprises both COVID-19 positive
and negative samples from all 65 speakers. We analyse the acoustic
manifestation of COVID-19 on the basis of these perfectly speaker
characteristic balanced `in-the-wild' data using interpretable audio
descriptors, and investigate several classification scenarios that shed light
into proper partitioning strategies for a fair speech-based COVID-19 detection.",0.26030076,0.04022784,0.030178215,A
7944,"These two
aspects, combined with the open-source nature of the data, make the COVYT dataset a prime basis for further research
in voice-based COVID-19 detection, which has a huge potential to massively increase future testing capacities while
saving waste at the same time.","Moreover, the presence of the same speaker with and without infection
makes it a natural candidate for personalisation approaches, which are expected to improve COVID-19 detection
performance as previous work found its symptom manifestation to have an individualised component [9].","However, the COVYT dataset inherently comes with a number of limitations, especially
as the exploited video clips were not recording with the intention to generate data for later scientiﬁc analyses.",2022-06-20 16:26:51+00:00,COVYT: Introducing the Coronavirus YouTube and TikTok speech dataset featuring the same speakers with and without infection,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Anastasia Semertzidou'), arxiv.Result.Author('Meishu Song'), arxiv.Result.Author('Florian B. Pokorny'), arxiv.Result.Author('Björn W. Schuller')]","More than two years after its outbreak, the COVID-19 pandemic continues to
plague medical systems around the world, putting a strain on scarce resources,
and claiming human lives. From the very beginning, various AI-based COVID-19
detection and monitoring tools have been pursued in an attempt to stem the tide
of infections through timely diagnosis. In particular, computer audition has
been suggested as a non-invasive, cost-efficient, and eco-friendly alternative
for detecting COVID-19 infections through vocal sounds. However, like all AI
methods, also computer audition is heavily dependent on the quantity and
quality of available data, and large-scale COVID-19 sound datasets are
difficult to acquire -- amongst other reasons -- due to the sensitive nature of
such data. To that end, we introduce the COVYT dataset -- a novel COVID-19
dataset collected from public sources containing more than 8 hours of speech
from 65 speakers. As compared to other existing COVID-19 sound datasets, the
unique feature of the COVYT dataset is that it comprises both COVID-19 positive
and negative samples from all 65 speakers. We analyse the acoustic
manifestation of COVID-19 on the basis of these perfectly speaker
characteristic balanced `in-the-wild' data using interpretable audio
descriptors, and investigate several classification scenarios that shed light
into proper partitioning strategies for a fair speech-based COVID-19 detection.",0.3101116,0.032814115,0.019163745,A
7945,"The COVYT dataset together with the provided benchmarks shall boost further research in the ﬁeld of speech-based
COVID-19 detection while ensuring reproducibility and comparability of results.","Moreover, we obtained a UAR near 70 % for the automatic classiﬁcation of speech
samples according to COVID-19 status by using pre-trained speech models.","Furthermore, as the dataset contains
samples of the same speakers with and without COVID-19 infection, we expect it to prove a valuable conduit for future
efforts in personalisation approaches that can adapt to the characteristics of individual speakers and, thus, improve
performance and reliability.",2022-06-20 16:26:51+00:00,COVYT: Introducing the Coronavirus YouTube and TikTok speech dataset featuring the same speakers with and without infection,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Anastasia Semertzidou'), arxiv.Result.Author('Meishu Song'), arxiv.Result.Author('Florian B. Pokorny'), arxiv.Result.Author('Björn W. Schuller')]","More than two years after its outbreak, the COVID-19 pandemic continues to
plague medical systems around the world, putting a strain on scarce resources,
and claiming human lives. From the very beginning, various AI-based COVID-19
detection and monitoring tools have been pursued in an attempt to stem the tide
of infections through timely diagnosis. In particular, computer audition has
been suggested as a non-invasive, cost-efficient, and eco-friendly alternative
for detecting COVID-19 infections through vocal sounds. However, like all AI
methods, also computer audition is heavily dependent on the quantity and
quality of available data, and large-scale COVID-19 sound datasets are
difficult to acquire -- amongst other reasons -- due to the sensitive nature of
such data. To that end, we introduce the COVYT dataset -- a novel COVID-19
dataset collected from public sources containing more than 8 hours of speech
from 65 speakers. As compared to other existing COVID-19 sound datasets, the
unique feature of the COVYT dataset is that it comprises both COVID-19 positive
and negative samples from all 65 speakers. We analyse the acoustic
manifestation of COVID-19 on the basis of these perfectly speaker
characteristic balanced `in-the-wild' data using interpretable audio
descriptors, and investigate several classification scenarios that shed light
into proper partitioning strategies for a fair speech-based COVID-19 detection.",0.3430578,0.05120976,0.14449579,A
8090,"These initial ﬁndings sparked further research into developing advanced front-end
features (Sahidullah, Kinnunen, and Hanilçi, 2015) (Wu et al., 2013).","Moreover, these spectral features are extracted and
processed frame-wise while the human speech production is a continuous process.","To give an idea
of the previously used architecture to detect spooﬁng attacks given in ASVspoof
2015 dataset are given in the Figure 3.1

               FIGURE 3.1: Block diagram of a typical presentation attack detection
               system mostly used in ASVspoof 2015.",2022-06-27 06:28:46+00:00,Detection of Doctored Speech: Towards an End-to-End Parametric Learn-able Filter Approach,eess.AS,"['eess.AS', 'cs.SD']",[arxiv.Result.Author('Rohit Arora')],"The Automatic Speaker Verification systems have potential in biometrics
applications for logical control access and authentication. A lot of things
happen to be at stake if the ASV system is compromised. The preliminary work
presents a comparative analysis of the wavelet and MFCC-based state-of-the-art
spoof detection techniques developed in these papers, respectively (Novoselov
et al., 2016) (Alam et al., 2016a). The results on ASVspoof 2015 justify our
inclination towards wavelet-based features instead of MFCC features. The
experiments on the ASVspoof 2019 database show the lack of credibility of the
traditional handcrafted features and give us more reason to progress towards
using end-to-end deep neural networks and more recent techniques. We use
Sincnet architecture as our baseline. We get E2E deep learning models, which we
call WSTnet and CWTnet, respectively, by replacing the Sinc layer with the
Wavelet Scattering and Continuous wavelet transform layers. The fusion model
achieved 62% and 17% relative improvement over traditional handcrafted models
and our Sincnet baseline when evaluated on the modern spoofing attacks in
ASVspoof 2019.
  The final scale distribution and the number of scales used in CWTnet are far
from optimal for the task at hand. So to solve this problem, we replaced the
CWT layer with a Wavelet Deconvolution(WD) (Khan and Yener, 2018) layer in our
CWTnet architecture. This layer calculates the Discrete-Continuous Wavelet
Transform similar to the CWTnet but also optimizes the scale parameter using
back-propagation. The WDnet model achieved 26% and 7% relative improvement over
CWTnet and Sincnet models respectively when evaluated over ASVspoof 2019
dataset. This shows that more generalized features are extracted as compared to
the features extracted by CWTnet as only the most important and relevant
frequency regions are focused upon.",0.059544478,-0.061388094,-0.14429638,B_centroid
8585,"This data en-
                                         ables further research and resource creation for these languages    1https://masakhane-io.github.io/bibleTTS/
                                         and will allow researchers to create meaningful benchmarks
                                         against non-English languages.","Finally, there have been efforts in related tasks,
                                         aligned speech corpus for ten African languages.","Table 1: Language, classiﬁcation and statistics.",2022-07-07 19:35:43+00:00,"BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus",eess.AS,"['eess.AS', 'cs.CL', 'cs.SD']","[arxiv.Result.Author('Josh Meyer'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Edresson Casanova'), arxiv.Result.Author('Alp Öktem'), arxiv.Result.Author('Daniel Whitenack Julian Weber'), arxiv.Result.Author('Salomon Kabongo'), arxiv.Result.Author('Elizabeth Salesky'), arxiv.Result.Author('Iroro Orife'), arxiv.Result.Author('Colin Leong'), arxiv.Result.Author('Perez Ogayo'), arxiv.Result.Author('Chris Emezue'), arxiv.Result.Author('Jonathan Mukiibi'), arxiv.Result.Author('Salomey Osei'), arxiv.Result.Author('Apelete Agbolo'), arxiv.Result.Author('Victor Akinode'), arxiv.Result.Author('Bernard Opoku'), arxiv.Result.Author('Samuel Olanrewaju'), arxiv.Result.Author('Jesujoba Alabi'), arxiv.Result.Author('Shamsuddeen Muhammad')]","BibleTTS is a large, high-quality, open speech dataset for ten languages
spoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned,
studio quality 48kHz single speaker recordings per language, enabling the
development of high-quality text-to-speech models. The ten languages
represented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu,
Lingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible
recordings made and released by the Open.Bible project from Biblica. We have
aligned, cleaned, and filtered the original recordings, and additionally
hand-checked a subset of the alignments for each language. We present results
for text-to-speech models with Coqui TTS. The data is released under a
commercial-friendly CC-BY-SA license.",0.13355052,0.020602474,-0.059487753,A
9280,"This is
D. Analysis on training loss curve                                 the ﬁrst time in literature an attempt was made to handle the
                                                                   limited emotion annotations available, from a machine learn-
   To further study the advantages of the proposed t-              ing perspective.","t-distribution on the ground-truth emotion annotations, which
                                                                   also accounts for the number of annotations available.","For this, we proposed and derived a KL diver-
distribution LKL (13) during the training phase, we compare        gence based loss term that aims to capture emotion annotation
the testing loss curve of (13) with the Gaussian LKL in            distribution as a t-distribution.",2022-07-25 12:38:20+00:00,Label Uncertainty Modeling and Prediction for Speech Emotion Recognition using t-Distributions,eess.AS,"['eess.AS', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Navin Raj Prabhu'), arxiv.Result.Author('Nale Lehmann-Willenbrock'), arxiv.Result.Author('Timo Gerkmann')]","As different people perceive others' emotional expressions differently, their
annotation in terms of arousal and valence are per se subjective. To address
this, these emotion annotations are typically collected by multiple annotators
and averaged across annotators in order to obtain labels for arousal and
valence. However, besides the average, also the uncertainty of a label is of
interest, and should also be modeled and predicted for automatic emotion
recognition. In the literature, for simplicity, label uncertainty modeling is
commonly approached with a Gaussian assumption on the collected annotations.
However, as the number of annotators is typically rather small due to resource
constraints, we argue that the Gaussian approach is a rather crude assumption.
In contrast, in this work we propose to model the label distribution using a
Student's t-distribution which allows us to account for the number of
annotations available. With this model, we derive the corresponding
Kullback-Leibler divergence based loss function and use it to train an
estimator for the distribution of emotion labels, from which the mean and
uncertainty can be inferred. Through qualitative and quantitative analysis, we
show the benefits of the t-distribution over a Gaussian distribution. We
validate our proposed method on the AVEC'16 dataset. Results reveal that our
t-distribution based approach improves over the Gaussian approach with
state-of-the-art uncertainty modeling results in speech-based emotion
recognition, along with an optimal and even faster convergence.",-0.012600008,-0.045945022,-0.11075297,A
11468,"audio retrieval is introduced into as Subtask 6B, which aims to in-
                                          spire further research into audio retrieval with unconstrained textual  2.3.",The data collecting procedure is explained in detail in [8].,"Clotho Retrieval Evaluation Dataset
                                          descriptions.",2022-09-20 19:51:53+00:00,Language-based Audio Retrieval Task in DCASE 2022 Challenge,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Huang Xie'), arxiv.Result.Author('Samuel Lipping'), arxiv.Result.Author('Tuomas Virtanen')]","Language-based audio retrieval is a task, where natural language textual
captions are used as queries to retrieve audio signals from a dataset. It has
been first introduced into DCASE 2022 Challenge as Subtask 6B of task 6, which
aims at developing computational systems to model relationships between audio
signals and free-form textual descriptions. Compared with audio captioning
(Subtask 6A), which is about generating audio captions for audio signals,
language-based audio retrieval (Subtask 6B) focuses on ranking audio signals
according to their relevance to natural language textual captions. In DCASE
2022 Challenge, the provided baseline system for Subtask 6B was significantly
outperformed, with top performance being 0.276 in mAP@10. This paper presents
the outcome of Subtask 6B in terms of submitted systems' performance and
analysis.",0.42931652,0.31367058,-0.08391164,A
11884,"Apart from
To further study the advantages of the proposed t-                        the size of the datasets, dataset-speciﬁc factors, such as
distribution LKL (16) during the training phase, we com-                  population demographics and social context, severely chal-
pare the testing loss curve of (16) with the Gaussian LKL in              lenge the cross-corpora performances.","5.3 Analysis on training loss curve                                       from the same dataset, and the Unmatch condition where
                                                                          the train partition is from a different dataset.","Crucial differences
MU+LU (12).",2022-09-30 12:55:43+00:00,End-to-End Label Uncertainty Modeling in Speech Emotion Recognition using Bayesian Neural Networks and Label Distribution Learning,eess.AS,"['eess.AS', 'cs.LG']","[arxiv.Result.Author('Navin Raj Prabhu'), arxiv.Result.Author('Nale Lehmann-Willenbrock'), arxiv.Result.Author('Timo Gerkman')]","To train machine learning algorithms to predict emotional expressions in
terms of arousal and valence, annotated datasets are needed. However, as
different people perceive others' emotional expressions differently, their
annotations are per se subjective. For this, annotations are typically
collected from multiple annotators and averaged to obtain ground-truth labels.
However, when exclusively trained on this averaged ground-truth, the trained
network is agnostic to the inherent subjectivity in emotional expressions. In
this work, we therefore propose an end-to-end Bayesian neural network capable
of being trained on a distribution of labels to also capture the
subjectivity-based label uncertainty. Instead of a Gaussian, we model the label
distribution using Student's t-distribution, which also accounts for the number
of annotations. We derive the corresponding Kullback-Leibler divergence loss
and use it to train an estimator for the distribution of labels, from which the
mean and uncertainty can be inferred. We validate the proposed method using two
in-the-wild datasets. We show that the proposed t-distribution based approach
achieves state-of-the-art uncertainty modeling results in speech emotion
recognition, and also consistent results in cross-corpora evaluations.
Furthermore, analyses reveal that the advantage of a t-distribution over a
Gaussian grows with increasing inter-annotator correlation and a decreasing
number of annotators.",-0.20725042,0.019356076,-0.08621923,B
12993,"Yet, further research is needed to optimize the effec-   plied to reduce the temporal dimension of input features and
                                          tive bandwidth of the captured speech signal as mid and high     to focus the signal’s discrimination solely on high frequency
                                          frequencies are missing due to the intrinsic low-pass charac-    bands.","A multiband decomposition
                                          noise pollution almost imperceptible within captured speech      using Pseudo-Quadrature Mirror Filters (PQMF) [19] is ap-
                                          [1, 2].","We therefore share a common goal with [20], but dif-
                                          teristic of the biological pathway.",2022-10-25 15:19:20+00:00,EBEN: Extreme bandwidth extension network applied to speech signals captured with noise-resilient microphones,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Julien Hauret'), arxiv.Result.Author('Thomas Joubaud'), arxiv.Result.Author('Véronique Zimpfer'), arxiv.Result.Author('Éric Bavu')]","In this paper, we present Extreme Bandwidth Extension Network (EBEN), a
generative adversarial network (GAN) that enhances audio measured with
noise-resilient microphones. This type of capture equipment suppresses ambient
noise at the expense of speech bandwidth, thereby requiring signal enhancement
techniques to recover the wideband speech signal. EBEN leverages a multiband
decomposition of the raw captured speech to decrease the data time-domain
dimensions, and give better control over the full-band signal. This multiband
representation is fed to a U-Net-like model, which adopts a combination of
feature and adversarial losses to recover an enhanced audio signal. We also
benefit from this original representation in the proposed discriminator
architecture. Our approach can achieve state-of-the-art results with a
lightweight generator and real-time compatible operation.",0.051974796,-0.16985592,0.028013166,B
13054,"Data from 7 of the 61 participants were discarded on these         phoneme-agnostic audio signals that can be used to further research
grounds to poor correlation across repeated ratings (mean intra-rater     acoustic correlates of paralinguistic vocal cues.","As such, the algorithm we describe provides a method
indicator of whether or not participants performed the task in good       for generating a synchronized dataset of affective speech, EGG, and
faith.",correlation coefﬁcient < 0.5).,2022-10-26 19:41:53+00:00,Acoustically-Driven Phoneme Removal That Preserves Vocal Affect Cues,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Camille Noufi'), arxiv.Result.Author('Jonathan Berger'), arxiv.Result.Author('Michael Frank'), arxiv.Result.Author('Karen Parker'), arxiv.Result.Author('Daniel L. Bowling')]","In this paper, we propose a method for removing linguistic information from
speech for the purpose of isolating paralinguistic indicators of affect. The
immediate utility of this method lies in clinical tests of sensitivity to vocal
affect that are not confounded by language, which is impaired in a variety of
clinical populations. The method is based on simultaneous recordings of speech
audio and electroglottographic (EGG) signals. The speech audio signal is used
to estimate the average vocal tract filter response and amplitude envelop. The
EGG signal supplies a direct correlate of voice source activity that is mostly
independent of phonetic articulation. These signals are used to create a third
signal designed to capture as much paralinguistic information from the vocal
production system as possible -- maximizing the retention of bioacoustic cues
to affect -- while eliminating phonetic cues to verbal meaning. To evaluate the
success of this method, we studied the perception of corresponding speech audio
and transformed EGG signals in an affect rating experiment with online
listeners. The results show a high degree of similarity in the perceived affect
of matched signals, indicating that our method is effective.",0.28756526,-0.20508191,-0.13428755,A
13091,"The successes motivated further research into
                                          training resource consumption both from an economical and           improving the modeling approach [6, 7] and understanding
                                          environmental point of view.","Additionally, the public avail-      reduce the amount of labeled data which is necessary to build
                                          ability of pre-trained model checkpoints is appealing to reduce     ASR systems.",the individual components [8].,2022-10-26 17:34:30+00:00,Efficient Use of Large Pre-Trained Models for Low Resource ASR,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Peter Vieting'), arxiv.Result.Author('Christoph Lüscher'), arxiv.Result.Author('Julian Dierkes'), arxiv.Result.Author('Ralf Schlüter'), arxiv.Result.Author('Hermann Ney')]","Automatic speech recognition (ASR) has been established as a well-performing
technique for many scenarios where lots of labeled data is available.
Additionally, unsupervised representation learning recently helped to tackle
tasks with limited data. Following this, hardware limitations and applications
give rise to the question how to efficiently take advantage of large pretrained
models and reduce their complexity for downstream tasks. In this work, we study
a challenging low resource conversational telephony speech corpus from the
medical domain in Vietnamese and German. We show the benefits of using
unsupervised techniques beyond simple fine-tuning of large pre-trained models,
discuss how to adapt them to a practical telephony task including bandwidth
transfer and investigate different data conditions for pre-training and
fine-tuning. We outperform the project baselines by 22% relative using
pretraining techniques. Further gains of 29% can be achieved by refinements of
architecture and training and 6% by adding 0.8 h of in-domain adaptation data.",-0.40679887,0.16371387,0.42308503,C
13092,"The successes motivated further research into
                                          training resource consumption both from an economical and           improving the modeling approach [6, 7] and understanding
                                          environmental point of view.","Additionally, the public avail-      reduce the amount of labeled data which is necessary to build
                                          ability of pre-trained model checkpoints is appealing to reduce     ASR systems.",the individual components [8].,2022-10-26 17:34:30+00:00,Efficient Use of Large Pre-Trained Models for Low Resource ASR,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Peter Vieting'), arxiv.Result.Author('Christoph Lüscher'), arxiv.Result.Author('Julian Dierkes'), arxiv.Result.Author('Ralf Schlüter'), arxiv.Result.Author('Hermann Ney')]","Automatic speech recognition (ASR) has been established as a well-performing
technique for many scenarios where lots of labeled data is available.
Additionally, unsupervised representation learning recently helped to tackle
tasks with limited data. Following this, hardware limitations and applications
give rise to the question how to efficiently take advantage of large pretrained
models and reduce their complexity for downstream tasks. In this work, we study
a challenging low resource conversational telephony speech corpus from the
medical domain in Vietnamese and German. We show the benefits of using
unsupervised techniques beyond simple fine-tuning of large pre-trained models,
discuss how to adapt them to a practical telephony task including bandwidth
transfer and investigate different data conditions for pre-training and
fine-tuning. We outperform the project baselines by 22% relative using
pretraining techniques. Further gains of 29% can be achieved by refinements of
architecture and training and 6% by adding 0.8 h of in-domain adaptation data.",-0.40679887,0.16371387,0.42308503,C
13170,"petitive, the model is language-independent and has a lot of
                                                                  scope for further research.","Though its performance is not com-
1% as compared to the Multilingual CLS + MT model.","The CLS multilingual model cas-
    Another way to go from the CLS domain to the native           caded with the machine transliteration model gave us the best
script is to ﬁnetune the models with the native script la-        relative improvement of 13.9% WER over the Monolingual
bels.",2022-10-30 04:01:26+00:00,DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common Label Set,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Arunkumar A'), arxiv.Result.Author('Mudit Batra'), arxiv.Result.Author('Umesh S')]","In a multilingual country like India, multilingual Automatic Speech
Recognition (ASR) systems have much scope. Multilingual ASR systems exhibit
many advantages like scalability, maintainability, and improved performance
over the monolingual ASR systems. However, building multilingual systems for
Indian languages is challenging since different languages use different scripts
for writing. On the other hand, Indian languages share a lot of common sounds.
Common Label Set (CLS) exploits this idea and maps graphemes of various
languages with similar sounds to common labels. Since Indian languages are
mostly phonetic, building a parser to convert from native script to CLS is
easy. In this paper, we explore various approaches to build multilingual ASR
models. We also propose a novel architecture called Encoder-Decoder-Decoder for
building multilingual systems that use both CLS and native script labels. We
also analyzed the effectiveness of CLS-based multilingual systems combined with
machine transliteration.",-0.13170955,-0.060108695,0.11577268,A
13525,"Speech Naturalness Evaluation The results shown in Table 1                Visualization on Fusion Process To further study the process of fea-
indicate that our proposed Expressive-VC can achieve the best per-        ture fusion, as shown in Fig.",gibility.,"2, three speech recordings containing
formance in speech naturalness.",2022-11-09 07:03:59+00:00,Expressive-VC: Highly Expressive Voice Conversion with Attention Fusion of Bottleneck and Perturbation Features,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Ziqian Ning'), arxiv.Result.Author('Qicong Xie'), arxiv.Result.Author('Pengcheng Zhu'), arxiv.Result.Author('Zhichao Wang'), arxiv.Result.Author('Liumeng Xue'), arxiv.Result.Author('Jixun Yao'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Mengxiao Bi')]","Voice conversion for highly expressive speech is challenging. Current
approaches struggle with the balancing between speaker similarity,
intelligibility and expressiveness. To address this problem, we propose
Expressive-VC, a novel end-to-end voice conversion framework that leverages
advantages from both neural bottleneck feature (BNF) approach and information
perturbation approach. Specifically, we use a BNF encoder and a Perturbed-Wav
encoder to form a content extractor to learn linguistic and para-linguistic
features respectively, where BNFs come from a robust pre-trained ASR model and
the perturbed wave becomes speaker-irrelevant after signal perturbation. We
further fuse the linguistic and para-linguistic features through an attention
mechanism, where speaker-dependent prosody features are adopted as the
attention query, which result from a prosody encoder with target speaker
embedding and normalized pitch and energy of source speech as input. Finally
the decoder consumes the integrated features and the speaker-dependent prosody
feature to generate the converted speech. Experiments demonstrate that
Expressive-VC is superior to several state-of-the-art systems, achieving both
high expressiveness captured from the source speech and high speaker similarity
with the target speaker; meanwhile intelligibility is well maintained.",0.16025597,-0.24243233,-0.06036161,A
13713,"Domain
adaptation techniques seem to better perform in case of cross-language datasets, paving the
way for further researches in this direction.","However our preliminary results indicate that there is the urgency of a more
realistic collection of data, that also faces the need of considering different ages.","For what concerns the lack of performance increase
applying domain adaptation models in the case of multi-age corpus, conclusions can not be
drawn, due to the peculiarity of the datasets available (where the collected speeches were
recorded by professional actors) and given the low presence of elderly people.",2022-11-14 12:39:41+00:00,Sentiment recognition of Italian elderly through domain adaptation on cross-corpus speech dataset,eess.AS,"['eess.AS', 'cs.CL']","[arxiv.Result.Author('Francesca Gasparini'), arxiv.Result.Author('Alessandra Grossi')]","The aim of this work is to define a speech emotion recognition (SER) model
able to recognize positive, neutral and negative emotions in natural
conversations of Italian elderly people. Several datasets for SER are available
in the literature. However most of them are in English or Chinese, have been
recorded while actors and actresses pronounce short phrases and thus are not
related to natural conversation. Moreover only few speeches among all the
databases are related to elderly people. Therefore, in this work, a
multi-language and multi-age corpus is considered merging a dataset in English,
that includes also elderly people, with a dataset in Italian. A general model,
trained on young and adult English actors and actresses is proposed, based on
XGBoost. Then two strategies of domain adaptation are proposed to adapt the
model either to elderly people and to Italian speakers. The results suggest
that this approach increases the classification performance, underlining also
that new datasets should be collected.",0.18648052,0.008662186,0.058016647,A
13724,"transparency for healthcare providers to help       Thus further research is needed to study the
them understand the diagnostic and other            source(s) of errors and their eﬀects on this
decisions made by DNNs (e.g.","Third, WER and CER
explainability of the deep neural network           quantify ASR performance, but identifying
(DNN) classiﬁer by providing transcripts            the nature of the errors concerned is crit-
generated by SSL ASR models, oﬀering some           ical to dementia screening and monitoring.",by show-               task.,2022-11-11 17:06:45+00:00,The Far Side of Failure: Investigating the Impact of Speech Recognition Errors on Subsequent Dementia Classification,eess.AS,"['eess.AS', 'cs.AI', 'cs.CL', 'cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Changye Li'), arxiv.Result.Author('Trevor Cohen'), arxiv.Result.Author('Serguei Pakhomov')]","Linguistic anomalies detectable in spontaneous speech have shown promise for
various clinical applications including screening for dementia and other forms
of cognitive impairment. The feasibility of deploying automated tools that can
classify language samples obtained from speech in large-scale clinical settings
depends on the ability to capture and automatically transcribe the speech for
subsequent analysis. However, the impressive performance of self-supervised
learning (SSL) automatic speech recognition (ASR) models with curated speech
data is not apparent with challenging speech samples from clinical settings.
One of the key questions for successfully applying ASR models for clinical
applications is whether imperfect transcripts they generate provide sufficient
information for downstream tasks to operate at an acceptable level of accuracy.
In this study, we examine the relationship between the errors produced by
several deep learning ASR systems and their impact on the downstream task of
dementia classification. One of our key findings is that, paradoxically, ASR
systems with relatively high error rates can produce transcripts that result in
better downstream classification accuracy than classification based on verbatim
transcripts.",-0.20979625,0.01739085,-0.11007893,B
14307,"This could be due to un-
our system but without xvector and f0 transformations, protected         controlled bias in the data but this requires further study.","However,
                                                                         we do not have a clear explanation as to why the proposed approach
We report the results for original speech, synthesised, i.e., fed into   protects better than global baseline does.","with the proposed approach, i.e., with xvector and f0 transforma-
tions as presented in Section 3, together with the global approach       4.3.2.",2022-11-29 10:20:31+00:00,Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Paul-Gauthier Noé'), arxiv.Result.Author('Xiaoxiao Miao'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Junichi Yamagishi'), arxiv.Result.Author('Jean-François Bonastre'), arxiv.Result.Author('Driss Matrouf')]","The use of modern vocoders in an analysis/synthesis pipeline allows us to
investigate high-quality voice conversion that can be used for privacy
purposes. Here, we propose to transform the speaker embedding and the pitch in
order to hide the sex of the speaker. ECAPA-TDNN-based speaker representation
fed into a HiFiGAN vocoder is protected using a neural-discriminant analysis
approach, which is consistent with the zero-evidence concept of privacy. This
approach significantly reduces the information in speech related to the
speaker's sex while preserving speech content and some consistency in the
resulting protected voices.",0.042693198,-0.24238454,0.089105204,B
14525,"Based on this
Item S1                                                     fact, we clustered the items to further study this inter-
Item S2  Reverberant solo violin                            action within diﬀerent item groups.","However, there were signiﬁcant interactions
Item ID         Description                                 for item type-ICLDD and item type-ICCD.","The items were di-
Item M1           recording                                 vided into a ﬁrst group containing solo instruments and a
Item M2                                                     second group containing instrument mixes based on the
             Reverberant solo                               similarity in their mean quality scores for each condition.",2022-12-02 20:20:16+00:00,Investigations on the Influence of Combined Inter-Aural Cue Distortions on Overall Audio Quality,eess.AS,"['eess.AS', 'cs.SD']","[arxiv.Result.Author('Pablo M. Delgado'), arxiv.Result.Author('Jürgen Herre')]","There is a considerable interest in developing algorithms that can predict
audio quality of perceptually coded signals to avoid the cost of extensive
listening tests during development time. While many established algorithms for
predicting the perceived quality of signals with monaural (timbral) distortions
are available (PEAQ, POLQA), predicting the quality degradation of stereo and
multi-channel spatial signals is still considered a challenge. Audio quality
degradation arising from spatial distortions is usually measured in terms of
well known inter-aural cue distortion measures such as Inter-aural Level
Difference Distortions (ILDD), Inter-aural Time Difference Distortions (ITDD)
and Inter-aural Cross Correlation Distortions (IACCD). However, the extent to
which their interaction influences the overall audio quality degradation in
complex signals as expressed - for example - in a multiple stimuli test is not
yet thoroughly studied. We propose a systematic approach that introduces
controlled combinations of spatial distortions on a representative set of
signals and evaluates their influence on overall perceived quality degradation
by analyzing listening test scores over said signals. From this study we derive
guidelines for designing meaningful distortion measures that consider
inter-aural cue distortion interactions.",0.058479283,-0.064209014,-0.25072655,A
14591,"A potential      between this indicator and short-term annoyance is also
limitation of our study is the absence of control stimuli,    quite weak, which warrants further research.","Our study suggests that the correlation
necessary to further apprehend this subject.",i.e.,2022-12-05 21:59:28+00:00,Standard sound emergence as a predictor of short-term annoyance from wind turbine noise,eess.AS,"['eess.AS', 'cs.SD', 'eess.SP', 'physics.class-ph']","[arxiv.Result.Author('Elise Ruaud'), arxiv.Result.Author('Guillaume Dutilleux')]","While sound emergence is used in several countries to regulate wind energy
development, there is no published evidence that it is a relevant noise
descriptor for this purpose. In the present work, we carried out two listening
tests to evaluate the merits of sound emergence. Three definitions of sound
emergence were considered: the one in ISO 1996-1, sound emergence under
audibility condition $e_{UAC}$, and spectral emergence $e_{SP}$. We also
considered the specific to residual ratio and loudness metrics. In each
listening test, the sound stimuli consisted of 48 sound stimuli at 3 A-weighted
sound pressure levels $\{30, 40, 50\}$~dB and 4 specific-to-residual ratios
$\{-10, -5, 0, +5 \}$~dB. The results lead to the conclusion that short term
annoyance is better predicted by the total sound pressure level than by sound
emergence, whatever the definition considered for the latter, or than by the
specific to residual ratio. Short-term annoyance is slightly better predicted
by $e_{UAC}$ than by $e$, while $e$ is a better predictor than $e_{SP}$. The
total sound pressure level and the loudness metrics performed similarly.
Furthermore, the results provide evidence that sound emergence is a poor
predictor of the audibility of wind turbine sounds.",-0.22198576,-0.19978747,-0.32982814,B
14718,"To serve as a foundation for further research on robust      by ﬁne-tuning a prototype model trained on a prototype ver-
speech recognition, we release inference code and models at        sion of the dataset on VoxLingua107 (Valk & Aluma¨e, 2021)
the following URL: https://github.com/openai/                      to ensure that the spoken language matches the language of
whisper.","the self-supervision or self-training techniques that have
been a mainstay of recent large-scale speech recognition           We also use an audio language detector, which was created
work.",the transcript according to CLD2.,2022-12-06 18:46:04+00:00,Robust Speech Recognition via Large-Scale Weak Supervision,eess.AS,"['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","[arxiv.Result.Author('Alec Radford'), arxiv.Result.Author('Jong Wook Kim'), arxiv.Result.Author('Tao Xu'), arxiv.Result.Author('Greg Brockman'), arxiv.Result.Author('Christine McLeavey'), arxiv.Result.Author('Ilya Sutskever')]","We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.",0.18695503,-0.08797612,0.17375475,A
14838,For      be improved for unseen speaker through further research.,"In the future, the ability to retain timbre will
systems are paired with the input samples, respectively.","Ad-
                                                                  ditionally, various disentangling tasks can be performed with
                                                                  our proposed PSDN without reliance on the parallel data.",2022-12-12 08:02:02+00:00,Non-parallel Accent Conversion using Pseudo Siamese Disentanglement Network,eess.AS,['eess.AS'],"[arxiv.Result.Author('Dongya Jia'), arxiv.Result.Author('Qiao Tian'), arxiv.Result.Author('Jiaxin Li'), arxiv.Result.Author('Yuanzhe Chen'), arxiv.Result.Author('Kainan Peng'), arxiv.Result.Author('Mingbo Ma'), arxiv.Result.Author('Yuping Wang'), arxiv.Result.Author('Yuxuan Wang')]","The main goal of accent conversion (AC) is to convert the accent of speech
into the target accent while preserving the content and timbre. Previous
reference-based methods rely on reference utterances in the inference phase,
which limits their practical application. What's more, previous reference-free
methods mostly require parallel data in the training phase. In this paper, we
propose a reference-free method based on non-parallel data from the perspective
of feature disentanglement. Pseudo Siamese Disentanglement Network (PSDN) is
proposed to disentangle the accent information from the content representation
and model the target accent. Besides, a timbre augmentation method is proposed
to enhance the ability of timbre retaining for speakers without target-accent
data. Experimental results show that the proposed system can convert the accent
of native American English speech into Indian accent with higher accentedness
(3.47) than the baseline (2.75) and input (1.19). The naturalness of converted
speech is also comparable to that of the input.",0.12101273,-0.31113195,0.09948822,A
