Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
41,"          2IIVHWUDQJHIDFWRUs   
Ablation on different s. We go on the further study of the                                
impact of different maximum offsets, i.e., the offset range                               
scale factor s in the paper.","How-
ever, replacing with more deformable attention at the early            ,PDJH1HW.$FF#         $EODWLRQVWXG\RQGLIIHUHQWRIIVHWUDQJHIDFWRUV
stages slightly decreases the accuracy.","We conduct an ablation exper-                                
iment of s ranging from 0 to 16 where 14 corresponds to                                   
the largest reasonable offset given the size of the feature                               
map (14 √ó 14 at stage 3).",2022-01-03 08:29:01+00:00,Vision Transformer with Deformable Attention,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhuofan Xia'), arxiv.Result.Author('Xuran Pan'), arxiv.Result.Author('Shiji Song'), arxiv.Result.Author('Li Erran Li'), arxiv.Result.Author('Gao Huang')]","Transformers have recently shown superior performances on various vision
tasks. The large, sometimes even global, receptive field endows Transformer
models with higher representation power over their CNN counterparts.
Nevertheless, simply enlarging receptive field also gives rise to several
concerns. On the one hand, using dense attention e.g., in ViT, leads to
excessive memory and computational cost, and features can be influenced by
irrelevant parts which are beyond the region of interests. On the other hand,
the sparse attention adopted in PVT or Swin Transformer is data agnostic and
may limit the ability to model long range relations. To mitigate these issues,
we propose a novel deformable self-attention module, where the positions of key
and value pairs in self-attention are selected in a data-dependent way. This
flexible scheme enables the self-attention module to focus on relevant regions
and capture more informative features. On this basis, we present Deformable
Attention Transformer, a general backbone model with deformable attention for
both image classification and dense prediction tasks. Extensive experiments
show that our models achieve consistently improved results on comprehensive
benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",0.3406339,0.21135488,-0.02683633,A
42,"          2IIVHWUDQJHIDFWRUs   
Ablation on different s. We go on the further study of the                                
impact of different maximum offsets, i.e., the offset range                               
scale factor s in the paper.","How-
ever, replacing with more deformable attention at the early            ,PDJH1HW.$FF#         $EODWLRQVWXG\RQGLIIHUHQWRIIVHWUDQJHIDFWRUV
stages slightly decreases the accuracy.","We conduct an ablation exper-                                
iment of s ranging from 0 to 16 where 14 corresponds to                                   
the largest reasonable offset given the size of the feature                               
map (14 √ó 14 at stage 3).",2022-01-03 08:29:01+00:00,Vision Transformer with Deformable Attention,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhuofan Xia'), arxiv.Result.Author('Xuran Pan'), arxiv.Result.Author('Shiji Song'), arxiv.Result.Author('Li Erran Li'), arxiv.Result.Author('Gao Huang')]","Transformers have recently shown superior performances on various vision
tasks. The large, sometimes even global, receptive field endows Transformer
models with higher representation power over their CNN counterparts.
Nevertheless, simply enlarging receptive field also gives rise to several
concerns. On the one hand, using dense attention e.g., in ViT, leads to
excessive memory and computational cost, and features can be influenced by
irrelevant parts which are beyond the region of interests. On the other hand,
the sparse attention adopted in PVT or Swin Transformer is data agnostic and
may limit the ability to model long range relations. To mitigate these issues,
we propose a novel deformable self-attention module, where the positions of key
and value pairs in self-attention are selected in a data-dependent way. This
flexible scheme enables the self-attention module to focus on relevant regions
and capture more informative features. On this basis, we present Deformable
Attention Transformer, a general backbone model with deformable attention for
both image classification and dense prediction tasks. Extensive experiments
show that our models achieve consistently improved results on comprehensive
benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",0.3406339,0.21135488,-0.02683633,A
61,"To further study this problem, we conduct the following experiment.","Intuitively,
such sampling variations drop when increasing the sampling resolution.","According
to the deÔ¨Ånition in Section 4.1, Ô¨Årst, on ShapeNet, we generate four datasets by
sampling diÔ¨Äerent number points of the input point cloud, ranging from n0 = 256
to 2048 points.",2022-01-03 18:05:52+00:00,Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siming Yan'), arxiv.Result.Author('Zhenpei Yang'), arxiv.Result.Author('Haoxiang Li'), arxiv.Result.Author('Li Guan'), arxiv.Result.Author('Hao Kang'), arxiv.Result.Author('Gang Hua'), arxiv.Result.Author('Qixing Huang')]","Many 3D representations (e.g., point clouds) are discrete samples of the
underlying continuous 3D surface. This process inevitably introduces sampling
variations on the underlying 3D shapes. In learning 3D representation, the
variations should be disregarded while transferable knowledge of the underlying
3D shape should be captured. This becomes a grand challenge in existing
representation learning paradigms. This paper studies autoencoding on point
clouds. The standard autoencoding paradigm forces the encoder to capture such
sampling variations as the decoder has to reconstruct the original point cloud
that has sampling variations. We introduce Implicit Autoencoder(IAE), a simple
yet effective method that addresses this challenge by replacing the point cloud
decoder with an implicit decoder. The implicit decoder outputs a continuous
representation that is shared among different point cloud sampling of the same
model. Reconstructing under the implicit representation can prioritize that the
encoder discards sampling variations, introducing more space to learn useful
features. We theoretically justify this claim under a simple linear
autoencoder. Moreover, the implicit decoder offers a rich space to design
suitable implicit representations for different tasks. We demonstrate the
usefulness of IAE across various self-supervised learning tasks for both 3D
objects and 3D scenes. Experimental results show that IAE consistently
outperforms the state-of-the-art in each task.",0.04301621,0.12698969,0.14111453,A
62,"To further study this problem, we conduct the following experiment.","Intuitively,
such sampling variations drop when increasing the sampling resolution.","According
to the deÔ¨Ånition in Section 4.1, Ô¨Årst, on ShapeNet, we generate four datasets
by sampling diÔ¨Äerent number points of the input point cloud, ranging from
n0 = 256 to 2048 points.",2022-01-03 18:05:52+00:00,Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siming Yan'), arxiv.Result.Author('Zhenpei Yang'), arxiv.Result.Author('Haoxiang Li'), arxiv.Result.Author('Li Guan'), arxiv.Result.Author('Hao Kang'), arxiv.Result.Author('Gang Hua'), arxiv.Result.Author('Qixing Huang')]","Many 3D representations (e.g., point clouds) are discrete samples of the
underlying continuous 3D surface. This process inevitably introduces sampling
variations on the underlying 3D shapes. In learning 3D representation, the
variations should be disregarded while transferable knowledge of the underlying
3D shape should be captured. This poses a grand challenge to existing
representation learning paradigms. For example, the standard autoencoding
paradigm forces the encoder to capture such sampling variations as the decoder
has to reconstruct the original point cloud. We introduce Implicit
Autoencoder(IAE), a simple yet effective method that addresses this challenge
by replacing the point cloud decoder with an implicit decoder. The implicit
decoder outputs a continuous representation that is shared among different
point cloud samplings of the same model. Reconstructing under the implicit
representation can prioritize that the encoder discards sampling variations,
introducing more space to learn useful features. We theoretically justify this
claim under a simple linear autoencoder. Moreover, our implicit decoder offers
a rich space to design suitable implicit representations for different tasks.
We demonstrate the usefulness of IAE across various self-supervised learning
tasks for both 3D objects and 3D scenes. Experimental results show that IAE
consistently outperforms the state-of-the-art in each task.",0.04301621,0.12698969,0.14111453,A
63,"We show two experiments to further study why IAE
Different Implicit Functions.","Further Experimental Analysis
than all explicit decoders.",We also investigate several              shows better result than explicit autoencoders.,2022-01-03 18:05:52+00:00,IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siming Yan'), arxiv.Result.Author('Zhenpei Yang'), arxiv.Result.Author('Haoxiang Li'), arxiv.Result.Author('Li Guan'), arxiv.Result.Author('Hao Kang'), arxiv.Result.Author('Gang Hua'), arxiv.Result.Author('Qixing Huang')]","Autoencoding has been a popular topic across many fields and recently emerged
in the 3D domain. However, many 3D representations (e.g., point clouds) are
discrete samples of the underlying continuous 3D surface which makes them
different from other data modalities. This process inevitably introduces
sampling variations on the underlying 3D shapes. In learning 3D representation,
a desirable goal is to disregard such sampling variations while focusing on
capturing transferable knowledge of the underlying 3D shape. This aim poses a
grand challenge to existing representation learning paradigms. For example, the
standard autoencoding paradigm forces the encoder to capture such sampling
variations as the decoder has to reconstruct the original point cloud. In this
paper, we introduce the Implicit Autoencoder(IAE). This simple yet effective
method addresses this challenge by replacing the point cloud decoder with an
implicit decoder. The implicit decoder can output a continuous representation
that is shared among different point cloud samplings of the same model.
Reconstructing under the implicit representation can prioritize that the
encoder discards sampling variations, introducing appropriate inductive bias to
learn more generalizable feature representations. We validate this claim via
experimental analysis. Moreover, our implicit decoder offers excellent
flexibility in designing suitable implicit representations for different tasks.
We demonstrate the usefulness of IAE across various self-supervised learning
tasks for both 3D objects and 3D scenes. Experimental results show that IAE
consistently outperforms the state-of-the-art in each task.",0.117327005,-0.08964565,0.11738825,C
64,"To further study this problem, we conduct
learning performance on the SUN RGB-D dataset.","The model with a              sampling variations drop when increasing the sampling
maximum cropping size of 50% achieves the best transfer                resolution.",Note that               the following experiment.,2022-01-03 18:05:52+00:00,IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siming Yan'), arxiv.Result.Author('Zhenpei Yang'), arxiv.Result.Author('Haoxiang Li'), arxiv.Result.Author('Li Guan'), arxiv.Result.Author('Hao Kang'), arxiv.Result.Author('Gang Hua'), arxiv.Result.Author('Qixing Huang')]","Autoencoding has been a popular topic across many fields and recently emerged
in the 3D domain. However, many 3D representations (e.g., point clouds) are
discrete samples of the underlying continuous 3D surface which makes them
different from other data modalities. This process inevitably introduces
sampling variations on the underlying 3D shapes. In learning 3D representation,
a desirable goal is to disregard such sampling variations while focusing on
capturing transferable knowledge of the underlying 3D shape. This aim poses a
grand challenge to existing representation learning paradigms. For example, the
standard autoencoding paradigm forces the encoder to capture such sampling
variations as the decoder has to reconstruct the original point cloud. In this
paper, we introduce the Implicit Autoencoder(IAE). This simple yet effective
method addresses this challenge by replacing the point cloud decoder with an
implicit decoder. The implicit decoder can output a continuous representation
that is shared among different point cloud samplings of the same model.
Reconstructing under the implicit representation can prioritize that the
encoder discards sampling variations, introducing appropriate inductive bias to
learn more generalizable feature representations. We validate this claim via
experimental analysis. Moreover, our implicit decoder offers excellent
flexibility in designing suitable implicit representations for different tasks.
We demonstrate the usefulness of IAE across various self-supervised learning
tasks for both 3D objects and 3D scenes. Experimental results show that IAE
consistently outperforms the state-of-the-art in each task.",-0.074732944,0.082577825,0.25810862,B
70,"While further research is required, this paper establishes
the potential of using Deep Neural Network based Vision-Language modelling for on-
device ML for night scene perception challenges.","Experiments on the
trained model shows this AI is able to caption any low light image that is randomly
downloaded from the internet.","Thus, the paper establishes the po-
tential for AI in safety of women & visual impaired users, especially at night.",2022-01-04 04:21:07+00:00,Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety,cs.CV,"['cs.CV', 'cs.CL', 'cs.LG', 'I.2.0']","[arxiv.Result.Author('Rajagopal A'), arxiv.Result.Author('Nirmala V'), arxiv.Result.Author('Arun Muthuraj Vedamanickam')]","There is amazing progress in Deep Learning based models for Image captioning
and Low Light image enhancement. For the first time in literature, this paper
develops a Deep Learning model that translates night scenes to sentences,
opening new possibilities for AI applications in the safety of visually
impaired women. Inspired by Image Captioning and Visual Question Answering, a
novel Interactive Image Captioning is developed. A user can make the AI focus
on any chosen person of interest by influencing the attention scoring.
Attention context vectors are computed from CNN feature vectors and
user-provided start word. The Encoder-Attention-Decoder neural network learns
to produce captions from low brightness images. This paper demonstrates how
women safety can be enabled by researching a novel AI capability in the
Interactive Vision-Language model for perception of the environment in the
night.",-0.22591636,-0.14153282,-0.057235204,C
71,"To encourage further research by inter-
ested researchers in Vision-Language modelling for promoting safety of women, the
source code is made available in open-source.","Thus an Interactive Image Captioning AI for Explainable night scene
understanding is demonstrated experimentally.","15

References

1.",2022-01-04 04:21:07+00:00,Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety,cs.CV,"['cs.CV', 'cs.CL', 'cs.LG', 'I.2.0']","[arxiv.Result.Author('Rajagopal A'), arxiv.Result.Author('Nirmala V'), arxiv.Result.Author('Arun Muthuraj Vedamanickam')]","There is amazing progress in Deep Learning based models for Image captioning
and Low Light image enhancement. For the first time in literature, this paper
develops a Deep Learning model that translates night scenes to sentences,
opening new possibilities for AI applications in the safety of visually
impaired women. Inspired by Image Captioning and Visual Question Answering, a
novel Interactive Image Captioning is developed. A user can make the AI focus
on any chosen person of interest by influencing the attention scoring.
Attention context vectors are computed from CNN feature vectors and
user-provided start word. The Encoder-Attention-Decoder neural network learns
to produce captions from low brightness images. This paper demonstrates how
women safety can be enabled by researching a novel AI capability in the
Interactive Vision-Language model for perception of the environment in the
night.",-0.20587444,-0.054857023,-0.29176354,C
73,"We hope this new base-           only 3.3B FLOPs, which is signiÔ¨Åcantly better than the
                                       line will be helpful to the further research and application       original TNT-S [11] and Swin-T [22].","SpeciÔ¨Åcally, PyramidTNT-S
                                       formances than the previous state-of-the-art vision trans-         yields 82.0% ImageNet classiÔ¨Åcation top-1 accuracy with
                                       formers such as Swin Transformer.","For COCO detec-
                                       of vision transformer.",2022-01-04 04:56:57+00:00,PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kai Han'), arxiv.Result.Author('Jianyuan Guo'), arxiv.Result.Author('Yehui Tang'), arxiv.Result.Author('Yunhe Wang')]","Transformer networks have achieved great progress for computer vision tasks.
Transformer-in-Transformer (TNT) architecture utilizes inner transformer and
outer transformer to extract both local and global representations. In this
work, we present new TNT baselines by introducing two advanced designs: 1)
pyramid architecture, and 2) convolutional stem. The new ""PyramidTNT""
significantly improves the original TNT by establishing hierarchical
representations. PyramidTNT achieves better performances than the previous
state-of-the-art vision transformers such as Swin Transformer. We hope this new
baseline will be helpful to the further research and application of vision
transformer. Code will be available at
https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch.",-0.048195794,0.152978,0.24991778,B
74,"We hope this new baseline will be helpful to the
                                                                                                          further research and application of vision transformer.",models.,1.,2022-01-04 04:56:57+00:00,PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kai Han'), arxiv.Result.Author('Jianyuan Guo'), arxiv.Result.Author('Yehui Tang'), arxiv.Result.Author('Yunhe Wang')]","Transformer networks have achieved great progress for computer vision tasks.
Transformer-in-Transformer (TNT) architecture utilizes inner transformer and
outer transformer to extract both local and global representations. In this
work, we present new TNT baselines by introducing two advanced designs: 1)
pyramid architecture, and 2) convolutional stem. The new ""PyramidTNT""
significantly improves the original TNT by establishing hierarchical
representations. PyramidTNT achieves better performances than the previous
state-of-the-art vision transformers such as Swin Transformer. We hope this new
baseline will be helpful to the further research and application of vision
transformer. Code will be available at
https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch.",-0.055913553,0.30086523,0.08556327,B
100,"RELATED WORK                         of GAN for style transfer and feature space data augmentation
                                                                  is also a problem worthy of further study.","Of course, the use
                         II.","A. Depression recognition and gait
                                                                     Rotation, noise injection and random erasing are commonly
   At present, the clinical diagnosis of depression is mainly     image augmentation methods.",2022-01-04 12:53:29+00:00,Data Augmentation for Depression Detection Using Skeleton-Based Gait Information,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jingjing Yang'), arxiv.Result.Author('Haifeng Lu'), arxiv.Result.Author('Chengming Li'), arxiv.Result.Author('Xiping Hu'), arxiv.Result.Author('Bin Hu')]","In recent years, the incidence of depression is rising rapidly worldwide, but
large-scale depression screening is still challenging. Gait analysis provides a
non-contact, low-cost, and efficient early screening method for depression.
However, the early screening of depression based on gait analysis lacks
sufficient effective sample data. In this paper, we propose a skeleton data
augmentation method for assessing the risk of depression. First, we propose
five techniques to augment skeleton data and apply them to depression and
emotion datasets. Then, we divide augmentation methods into two types
(non-noise augmentation and noise augmentation) based on the mutual information
and the classification accuracy. Finally, we explore which augmentation
strategies can capture the characteristics of human skeleton data more
effectively. Experimental results show that the augmented training data set
that retains more of the raw skeleton data properties determines the
performance of the detection model. Specifically, rotation augmentation and
channel mask augmentation make the depression detection accuracy reach 92.15%
and 91.34%, respectively.",-0.06627321,-0.025725417,0.08220279,C
101,"Journal of Big Data, 6(1):1‚Äì48, 2019.
time, we will further study the relationship between gait
                                                                       [11] Zeshan Hussain, Francisco Gimenez, Darvin Yi, and Daniel Rubin.",At the same                 augmentation for deep learning.,"Dif-
                                                                             ferential data augmentation techniques for medical imaging classiÔ¨Åcation
                                                                             tasks.",2022-01-04 12:53:29+00:00,Data Augmentation for Depression Detection Using Skeleton-Based Gait Information,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jingjing Yang'), arxiv.Result.Author('Haifeng Lu'), arxiv.Result.Author('Chengming Li'), arxiv.Result.Author('Xiping Hu'), arxiv.Result.Author('Bin Hu')]","In recent years, the incidence of depression is rising rapidly worldwide, but
large-scale depression screening is still challenging. Gait analysis provides a
non-contact, low-cost, and efficient early screening method for depression.
However, the early screening of depression based on gait analysis lacks
sufficient effective sample data. In this paper, we propose a skeleton data
augmentation method for assessing the risk of depression. First, we propose
five techniques to augment skeleton data and apply them to depression and
emotion datasets. Then, we divide augmentation methods into two types
(non-noise augmentation and noise augmentation) based on the mutual information
and the classification accuracy. Finally, we explore which augmentation
strategies can capture the characteristics of human skeleton data more
effectively. Experimental results show that the augmented training data set
that retains more of the raw skeleton data properties determines the
performance of the detection model. Specifically, rotation augmentation and
channel mask augmentation make the depression detection accuracy reach 92.15%
and 91.34%, respectively.",-0.09681842,-0.108686626,0.119948514,C
131,"Even with all the research that has been done in SLR, many inadequacies need to be
 dealt with by further research.","In continuous SLR, the system is able to recognize and
 translate whole sentences instead of a single gesture [33][34].","Some of the issues and challenges that need to be worked
 on are as follows [33][2][4][6].",2022-01-05 07:13:03+00:00,Sign Language Recognition System using TensorFlow Object Detection API,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Sharvani Srivastava'), arxiv.Result.Author('Amisha Gangwar'), arxiv.Result.Author('Richa Mishra'), arxiv.Result.Author('Sudhakar Singh')]","Communication is defined as the act of sharing or exchanging information,
ideas or feelings. To establish communication between two people, both of them
are required to have knowledge and understanding of a common language. But in
the case of deaf and dumb people, the means of communication are different.
Deaf is the inability to hear and dumb is the inability to speak. They
communicate using sign language among themselves and with normal people but
normal people do not take seriously the importance of sign language. Not
everyone possesses the knowledge and understanding of sign language which makes
communication difficult between a normal person and a deaf and dumb person. To
overcome this barrier, one can build a model based on machine learning. A model
can be trained to recognize different gestures of sign language and translate
them into English. This will help a lot of people in communicating and
conversing with deaf and dumb people. The existing Indian Sing Language
Recognition systems are designed using machine learning algorithms with single
and double-handed gestures but they are not real-time. In this paper, we
propose a method to create an Indian Sign Language dataset using a webcam and
then using transfer learning, train a TensorFlow model to create a real-time
Sign Language Recognition system. The system achieves a good level of accuracy
even with a limited size dataset.",0.11100521,-0.10499759,-0.27115327,A
132,"Even with all the research that has been done in SLR, many inadequacies need to be
 dealt with by further research.","In continuous SLR, the system is able to recognize and
 translate whole sentences instead of a single gesture [33][34].","Some of the issues and challenges that need to be worked
 on are as follows [33][2][4][6].",2022-01-05 07:13:03+00:00,Sign Language Recognition System using TensorFlow Object Detection API,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Sharvani Srivastava'), arxiv.Result.Author('Amisha Gangwar'), arxiv.Result.Author('Richa Mishra'), arxiv.Result.Author('Sudhakar Singh')]","Communication is defined as the act of sharing or exchanging information,
ideas or feelings. To establish communication between two people, both of them
are required to have knowledge and understanding of a common language. But in
the case of deaf and dumb people, the means of communication are different.
Deaf is the inability to hear and dumb is the inability to speak. They
communicate using sign language among themselves and with normal people but
normal people do not take seriously the importance of sign language. Not
everyone possesses the knowledge and understanding of sign language which makes
communication difficult between a normal person and a deaf and dumb person. To
overcome this barrier, one can build a model based on machine learning. A model
can be trained to recognize different gestures of sign language and translate
them into English. This will help a lot of people in communicating and
conversing with deaf and dumb people. The existing Indian Sing Language
Recognition systems are designed using machine learning algorithms with single
and double-handed gestures but they are not real-time. In this paper, we
propose a method to create an Indian Sign Language dataset using a webcam and
then using transfer learning, train a TensorFlow model to create a real-time
Sign Language Recognition system. The system achieves a good level of accuracy
even with a limited size dataset.",0.11100521,-0.10499759,-0.27115327,A
143,"We share these resources with the research com-     per proposes a pipeline to provide an automated process
                                             munity to facilitate further research in this interesting    of reading tables from PDFs and utilize them as a weak
                                             direction.","With spreadsheets as weak supervision, this pa-
                                             parsing.",supervision source for DL systems.,2022-01-05 15:21:06+00:00,TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets,cs.CV,['cs.CV'],"[arxiv.Result.Author('Susie Xi Rao'), arxiv.Result.Author('Johannes Rausch'), arxiv.Result.Author('Peter Egger'), arxiv.Result.Author('Ce Zhang')]","Tables have been an ever-existing structure to store data. There exist now
different approaches to store tabular data physically. PDFs, images,
spreadsheets, and CSVs are leading examples. Being able to parse table
structures and extract content bounded by these structures is of high
importance in many applications. In this paper, we devise TableParser, a system
capable of parsing tables in both native PDFs and scanned images with high
precision. We have conducted extensive experiments to show the efficacy of
domain adaptation in developing such a tool. Moreover, we create TableAnnotator
and ExcelAnnotator, which constitute a spreadsheet-based weak supervision
mechanism and a pipeline to enable table parsing. We share these resources with
the research community to facilitate further research in this interesting
direction.",0.20316075,-0.083020985,-0.25954974,A
156,"In this section, we further study
step by step ablation studies of our proposed system.","D. Ablation Experiments                                              3) Conditional vs Unconditional: The experiments in the
                                                                  last section show the effectiveness of the conditional model in
   1) Step by Step Ablation Studies: In this section, we provide  Ô¨Ånetuning downstream tasks.",The         how the condition Ô¨Çag affects the generation performance.,2022-01-06 11:00:52+00:00,Self-Training Vision Language BERTs with a Unified Conditional Model,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Xiaofeng Yang'), arxiv.Result.Author('Fengmao Lv'), arxiv.Result.Author('Fayao Liu'), arxiv.Result.Author('Guosheng Lin')]","Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.",0.4965949,0.030883268,-0.11133222,A
157,"Characterization of
                  anisotropy is however subject of further research.","In this case, the parameters Œª and ŒΩ depend on the angle œï.","In inspection systems for industrial quality control, line cameras are often used to
                  scan the material continuously in the production line, see e. g. [65, 66].",2022-01-06 11:03:27+00:00,An unambiguous cloudiness index for nonwovens,cs.CV,"['cs.CV', 'eess.IV', '62M40, 62P30', 'I.4.7; I.4.9']","[arxiv.Result.Author('Michael Godehardt'), arxiv.Result.Author('Ali Moghiseh'), arxiv.Result.Author('Christine Oetjen'), arxiv.Result.Author('Joachim Ohser'), arxiv.Result.Author('Katja Schladitz')]","Cloudiness or formation is a concept routinely used in industry to address
deviations from homogeneity in nonwovens and papers. Measuring a cloudiness
index based on image data is a common task in industrial quality assurance. The
two most popular ways of quantifying cloudiness are based on power spectrum or
correlation function on the one hand or the Laplacian pyramid on the other
hand. Here, we recall the mathematical basis of the first approach
comprehensively, derive a cloudiness index, and demonstrate its practical
estimation. We prove that the Laplacian pyramid as well as other quantities
characterizing cloudiness like the range of interaction and the intensity of
small-angle scattering are very closely related to the power spectrum. Finally,
we show that the power spectrum is easy to be measured image analytically and
carries more information than the alternatives.",0.18640585,0.37255162,0.010386452,A
158,"Characterization of

                  anisotropy is however subject of further research.","In this case, the parameters Œª and ŒΩ depend on the angle œï.","In inspection systems for industrial quality control, line cameras are often used to

                  scan the material continuously in the production line, see e. g. [65, 66].",2022-01-06 11:03:27+00:00,An unambiguous cloudiness index for nonwovens,cs.CV,"['cs.CV', 'eess.IV', '62M40, 62P30', 'I.4.7; I.4.9']","[arxiv.Result.Author('Michael Godehardt'), arxiv.Result.Author('Ali Moghiseh'), arxiv.Result.Author('Christine Oetjen'), arxiv.Result.Author('Joachim Ohser'), arxiv.Result.Author('Katja Schladitz')]","Cloudiness or formation is a concept routinely used in industry to address
deviations from homogeneity in nonwovens and papers. Measuring a cloudiness
index based on image data is a common task in industrial quality assurance. The
two most popular ways of quantifying cloudiness are based on power spectrum or
correlation function on the one hand or the Laplacian pyramid on the other
hand. Here, we recall the mathematical basis of the first approach
comprehensively, derive a cloudiness index, and demonstrate its practical
estimation. We prove that the Laplacian pyramid as well as other quantities
characterizing cloudiness like the range of interaction and the intensity of
small-angle scattering are very closely related to the power spectrum. Finally,
we show that the power spectrum is easy to be measured image analytically and
carries more information than the alternatives.",0.18640585,0.37255162,0.010386452,A
172,"As those models gives high performance, we
expect that these results will help for further research.","We trained those
models with noiseless or few noise images so this model will be unable to give the best
performance if the images have noise.","Moreover, it will help to develop
an image application through the application, general people can classify the edible
local spinach.",2022-01-06 15:10:41+00:00,Deep Learning Based Classification System For Recognizing Local Spinach,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Mirajul Islam'), arxiv.Result.Author('Nushrat Jahan Ria'), arxiv.Result.Author('Jannatul Ferdous Ani'), arxiv.Result.Author('Abu Kaisar Mohammad Masum'), arxiv.Result.Author('Sheikh Abujar'), arxiv.Result.Author('Syed Akhter Hossain')]","A deep learning model gives an incredible result for image processing by
studying from the trained dataset. Spinach is a leaf vegetable that contains
vitamins and nutrients. In our research, a Deep learning method has been used
that can automatically identify spinach and this method has a dataset of a
total of five species of spinach that contains 3785 images. Four Convolutional
Neural Network (CNN) models were used to classify our spinach. These models
give more accurate results for image classification. Before applying these
models there is some preprocessing of the image data. For the preprocessing of
data, some methods need to happen. Those are RGB conversion, filtering, resize
& rescaling, and categorization. After applying these methods image data are
pre-processed and ready to be used in the classifier algorithms. The accuracy
of these classifiers is in between 98.68% - 99.79%. Among those models, VGG16
achieved the highest accuracy of 99.79%.",0.021576498,-0.035526767,0.057641104,C
234,"We further study (iv) the probability threshold hy-                                         erations, instantiated from mouthing-only (M) annotations,
perparameter for text-based retrieval via sign recognition.","We notice
embedding aggregation mechanisms, and (iii) text embed-                                            substantial improvements from each of our SPOT-ALIGN it-
dings.","expanded Ô¨Årst in the number of annotations within the same
We also highlight (v) the importance of having a sign lan-                                         vocabulary size of 1079, then expanded in the number of
guage aligned subtitle data by experimenting with using the                                        sign categories with 1887-way classiÔ¨Åcation.",2022-01-07 15:22:18+00:00,Sign Language Video Retrieval with Free-Form Textual Queries,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Amanda Duarte'), arxiv.Result.Author('Samuel Albanie'), arxiv.Result.Author('Xavier Gir√≥-i-Nieto'), arxiv.Result.Author('G√ºl Varol')]","Systems that can efficiently search collections of sign language videos have
been highlighted as a useful application of sign language technology. However,
the problem of searching videos beyond individual keywords has received limited
attention in the literature. To address this gap, in this work we introduce the
task of sign language retrieval with free-form textual queries: given a written
query (e.g., a sentence) and a large collection of sign language videos, the
objective is to find the signing video in the collection that best matches the
written query. We propose to tackle this task by learning cross-modal
embeddings on the recently introduced large-scale How2Sign dataset of American
Sign Language (ASL). We identify that a key bottleneck in the performance of
the system is the quality of the sign video embedding which suffers from a
scarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a
framework for interleaving iterative rounds of sign spotting and feature
alignment to expand the scope and scale of available training data. We validate
the effectiveness of SPOT-ALIGN for learning a robust sign video embedding
through improvements in both sign recognition and the proposed video retrieval
task.",0.03649372,-0.27188128,-0.27947584,C
235,"We further study (iv) the probability threshold hy-
secutive iterations.","4 (left),        embedding aggregation mechanisms, and (iii) text embed-
where we observe a significant increase in yield across con-         dings.",To enable evaluation of sign recogni-           perparameter for text-based retrieval via sign recognition.,2022-01-07 15:22:18+00:00,Sign Language Video Retrieval with Free-Form Textual Queries,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Amanda Duarte'), arxiv.Result.Author('Samuel Albanie'), arxiv.Result.Author('Xavier Gir√≥-i-Nieto'), arxiv.Result.Author('G√ºl Varol')]","Systems that can efficiently search collections of sign language videos have
been highlighted as a useful application of sign language technology. However,
the problem of searching videos beyond individual keywords has received limited
attention in the literature. To address this gap, in this work we introduce the
task of sign language retrieval with free-form textual queries: given a written
query (e.g., a sentence) and a large collection of sign language videos, the
objective is to find the signing video in the collection that best matches the
written query. We propose to tackle this task by learning cross-modal
embeddings on the recently introduced large-scale How2Sign dataset of American
Sign Language (ASL). We identify that a key bottleneck in the performance of
the system is the quality of the sign video embedding which suffers from a
scarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a
framework for interleaving iterative rounds of sign spotting and feature
alignment to expand the scope and scale of available training data. We validate
the effectiveness of SPOT-ALIGN for learning a robust sign video embedding
through improvements in both sign recognition and the proposed video retrieval
task.",0.1343005,-0.2029928,-0.14345178,C
242,"from sparse images captured under wildly different condi-
tions, which is commonly seen in online image collections             We will release our code, pre-trained models, and train-
with individual images taken with varying lightings, cam-          ing datasets upon publication to facilitate further research
eras, environments, and poses.","Our object capture          ‚Ä¢ Extensive evaluations, comparisons and results using
and rendering approach builds upon neural radiance Ô¨Åelds             these and other established datasets demonstrating the
with several key features that enable high-Ô¨Ådelity capture           state-of-the-art results obtained by our approach.",The only expected anno-             effort in this area.,2022-01-07 16:45:15+00:00,NeROIC: Neural Rendering of Objects from Online Image Collections,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhengfei Kuang'), arxiv.Result.Author('Kyle Olszewski'), arxiv.Result.Author('Menglei Chai'), arxiv.Result.Author('Zeng Huang'), arxiv.Result.Author('Panos Achlioptas'), arxiv.Result.Author('Sergey Tulyakov')]","We present a novel method to acquire object representations from online image
collections, capturing high-quality geometry and material properties of
arbitrary objects from photographs with varying cameras, illumination, and
backgrounds. This enables various object-centric rendering applications such as
novel-view synthesis, relighting, and harmonized background composition from
challenging in-the-wild input. Using a multi-stage approach extending neural
radiance fields, we first infer the surface geometry and refine the coarsely
estimated initial camera parameters, while leveraging coarse foreground object
masks to improve the training efficiency and geometry quality. We also
introduce a robust normal estimation technique which eliminates the effect of
geometric noise while retaining crucial details. Lastly, we extract surface
material properties and ambient illumination, represented in spherical
harmonics with extensions that handle transient elements, e.g. sharp shadows.
The union of these components results in a highly modular and efficient object
acquisition framework. Extensive evaluations and comparisons demonstrate the
advantages of our approach in capturing high-quality geometry and appearance
properties useful for rendering applications.",-0.32962,0.0241945,0.15294367,B
257,"is significantly better than other loss functions, although its per-
                                                                        formance on Wikipedia and NUS-WIDE datasets is slightly lower
   To further study how the superiority of CLIP4CMR is gener-           than the linear regression loss.","From the results, we can see that the overall
precision-recall results are consistent with the mAP scores, where      performance of the prototype contrastive loss on the four datasets
our CLIP4CMR obviously outperforms all the baseline methods.","For the prir-wise losses, we can
ated, we further show the distributions of the intra-class distances    observe that the performance of modality-invariant loss is very
and inter-class distances across different modalities in the test       poor, which shows the necessity of considering negative samples
set, as shown in Figure (3).",2022-01-08 06:00:22+00:00,A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval,cs.CV,"['cs.CV', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Zhixiong Zeng'), arxiv.Result.Author('Wenji Mao')]","Cross-Modal Retrieval (CMR) is an important research topic across multimodal
computing and information retrieval, which takes one type of data as the query
to retrieve relevant data of another type, and has been widely used in many
real-world applications. Recently, the vision-language pre-trained model
represented by CLIP has demonstrated its superiority of learning visual and
textual representations and its impressive performance on various vision and
language related tasks. Although CLIP as well as the previous pre-trained
models have shown great performance improvement in unsupervised CMR, the
performance and impact of these pre-trained models on supervised CMR were
rarely explored due to the lack of multimodal class-level associations.
  In this paper, we take CLIP as the current representative vision-language
pre-trained model to conduct a comprehensive empirical study and provide
insights on its performance and impact on supervised CMR. To this end, we first
propose a novel model CLIP4CMR (\textbf{CLIP For} supervised
\textbf{C}ross-\textbf{M}odal \textbf{R}etrieval) that employs pre-trained CLIP
as backbone network to perform supervised CMR. We then revisit the existing
loss function design in CMR, including the most common pair-wise losses,
class-wise losses and hybrid ones, and provide insights on applying CLIP.
Moreover, we investigate several concerned issues in supervised CMR and provide
new perspectives for this field via CLIP4CMR, including the robustness to
modality imbalance and the sensitivity to hyper-parameters. Extensive
experimental results show that the CLIP4CMR achieves SOTA results with
significant improvements on the benchmark datasets Wikipedia, NUS-WIDE,
Pascal-Sentence and XmediaNet. Our data and codes are publicly available at
https://github.com/zhixiongz/CLIP4CMR.",0.15566073,-0.088084646,0.11129325,A
258,"To further study how the superiority                                                                                                                                 training protocol, parameter choice and random seed for a relatively
of CLIP4CMR is generated, we further examine the distributions                                                                                                                                     objective comparison.","Specifically, we unify the model architecture of CLIP4CMR,
4.2.2 Visualization Analysis.","We compare three popular pair-wise losses
of the intra-class image-text distances and inter-class image-text                                                                                                                                 namely modality-invariant loss (i.e., ML), contrastive loss (i.e., CL)
distances in the test set.",2022-01-08 06:00:22+00:00,A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval,cs.CV,"['cs.CV', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Zhixiong Zeng'), arxiv.Result.Author('Wenji Mao')]","Cross-Modal Retrieval (CMR) is an important research topic across multimodal
computing and information retrieval, which takes one type of data as the query
to retrieve relevant data of another type. It has been widely used in many
real-world applications. Recently, the vision-language pre-trained models
represented by CLIP demonstrate its superiority in learning the visual and
textual representations and gain impressive performance on various vision and
language related tasks. Although CLIP as well as the previous pre-trained
models have shown great performance improvement in the unsupervised CMR, the
performance and impact of these pre-trained models on the supervised CMR were
rarely explored due to the lack of common representation for the multimodal
class-level associations. In this paper, we take CLIP as the current
representative vision-language pre-trained model to conduct a comprehensive
empirical study. We evaluate its performance and impact on the supervised CMR,
and attempt to answer several key research questions. To this end, we first
propose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal
Retrieval) that employs the pre-trained CLIP as backbone network to perform the
supervised CMR. Then by means of the CLIP4CMR framework, we revisit the design
of different learning objectives in current CMR methods to provide new insights
on model design. Moreover, we investigate the most concerned aspects in
applying CMR, including the robustness to modality imbalance and sensitivity to
hyper-parameters, to provide new perspectives for practical applications.
Through extensive experiments, we show that CLIP4CMR achieves the SOTA results
with prominent improvements on the benchmark datasets, and can be used as a
fundamental framework to empirically study the key research issues of the
supervised CMR, with significant implications for model design and practical
considerations.",0.071867414,-0.17775977,0.08954273,C
267,"Certainly, further research can be done, since
related but also for time series analyses.","However,
that the images are linked to individual plants at multiple    the discrimination between the Ô¨Årst 5 degrees seems to be
dates enables this dataset to be used not only for spatially   challenging.",Figure 14 shows a   this is beyond the scope of this work.,2022-01-08 21:14:07+00:00,Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision,cs.CV,"['cs.CV', 'cs.CE', 'cs.LG']","[arxiv.Result.Author('Maurice G√ºnder'), arxiv.Result.Author('Facundo R. Ispizua Yamati'), arxiv.Result.Author('Jana Kierdorf'), arxiv.Result.Author('Ribana Roscher'), arxiv.Result.Author('Anne-Katrin Mahlein'), arxiv.Result.Author('Christian Bauckhage')]","UAV-based image retrieval in modern agriculture enables gathering large
amounts of spatially referenced crop image data. In large-scale experiments,
however, UAV images suffer from containing a multitudinous amount of crops in a
complex canopy architecture. Especially for the observation of temporal
effects, this complicates the recognition of individual plants over several
images and the extraction of relevant information tremendously. In this work,
we present a hands-on workflow for the automatized temporal and spatial
identification and individualization of crop images from UAVs abbreviated as
""cataloging"" based on comprehensible computer vision methods. We evaluate the
workflow on two real-world datasets. One dataset is recorded for observation of
Cercospora leaf spot - a fungal disease - in sugar beet over an entire growing
cycle. The other one deals with harvest prediction of cauliflower plants. The
plant catalog is utilized for the extraction of single plant images seen over
multiple time points. This gathers large-scale spatio-temporal image dataset
that in turn can be applied to train further machine learning models including
various data layers. The presented approach improves analysis and
interpretation of UAV data in agriculture significantly. By validation with
some reference data, our method shows an accuracy that is similar to more
complex deep learning-based recognition techniques. Our workflow is able to
automatize plant cataloging and training image extraction, especially for large
datasets.",-0.030225078,0.23068547,-0.14720023,B
268,"Certainly, further research can be done, since
related but also for time series analyses.","However,
that the images are linked to individual plants at multiple    the discrimination between the Ô¨Årst 5 degrees seems to be
dates enables this dataset to be used not only for spatially   challenging.",Figure 14 shows a   this is beyond the scope of this work.,2022-01-08 21:14:07+00:00,Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision,cs.CV,"['cs.CV', 'cs.CE', 'cs.LG']","[arxiv.Result.Author('Maurice G√ºnder'), arxiv.Result.Author('Facundo R. Ispizua Yamati'), arxiv.Result.Author('Jana Kierdorf'), arxiv.Result.Author('Ribana Roscher'), arxiv.Result.Author('Anne-Katrin Mahlein'), arxiv.Result.Author('Christian Bauckhage')]","UAV-based image retrieval in modern agriculture enables gathering large
amounts of spatially referenced crop image data. In large-scale experiments,
however, UAV images suffer from containing a multitudinous amount of crops in a
complex canopy architecture. Especially for the observation of temporal
effects, this complicates the recognition of individual plants over several
images and the extraction of relevant information tremendously. In this work,
we present a hands-on workflow for the automatized temporal and spatial
identification and individualization of crop images from UAVs abbreviated as
""cataloging"" based on comprehensible computer vision methods. We evaluate the
workflow on two real-world datasets. One dataset is recorded for observation of
Cercospora leaf spot - a fungal disease - in sugar beet over an entire growing
cycle. The other one deals with harvest prediction of cauliflower plants. The
plant catalog is utilized for the extraction of single plant images seen over
multiple time points. This gathers large-scale spatio-temporal image dataset
that in turn can be applied to train further machine learning models including
various data layers. The presented approach improves analysis and
interpretation of UAV data in agriculture significantly. By validation with
some reference data, our method shows an accuracy that is similar to more
complex deep learning-based recognition techniques. Our workflow is able to
automatize plant cataloging and training image extraction, especially for large
datasets.",-0.030225078,0.23068547,-0.14720023,B
284,"Although a lot of further research must be done in this Ô¨Åeld, the Ô¨Åndings act as the Ô¨Årst step to faster,
more accurate, less biased and less emotionally draining digital forensic investigations.",This research will beneÔ¨Åt both crime victims as well as digital forensic investigators.,2.,2022-01-09 16:25:37+00:00,Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations,cs.CV,"['cs.CV', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Thomas Grubl'), arxiv.Result.Author('Harjinder Singh Lallie')]","The precise age estimation of child sexual abuse and exploitation (CSAE)
victims is one of the most significant digital forensic challenges.
Investigators often need to determine the age of victims by looking at images
and interpreting the sexual development stages and other human characteristics.
The main priority - safeguarding children -- is often negatively impacted by a
huge forensic backlog, cognitive bias and the immense psychological stress that
this work can entail. This paper evaluates existing facial image datasets and
proposes a new dataset tailored to the needs of similar digital forensic
research contributions. This small, diverse dataset of 0 to 20-year-old
individuals contains 245 images and is merged with 82 unique images from the
FG-NET dataset, thus achieving a total of 327 images with high image diversity
and low age range density. The new dataset is tested on the Deep EXpectation
(DEX) algorithm pre-trained on the IMDB-WIKI dataset. The overall results for
young adolescents aged 10 to 15 and older adolescents/adults aged 16 to 20 are
very encouraging -- achieving MAEs as low as 1.79, but also suggest that the
accuracy for children aged 0 to 10 needs further work. In order to determine
the efficacy of the prototype, valuable input of four digital forensic experts,
including two forensic investigators, has been taken into account to improve
age estimation results. Further research is required to extend datasets both
concerning image density and the equal distribution of factors such as gender
and racial diversity.",0.1779329,0.054211944,-0.22120027,A
285,"Furthermore, there is remaining opportunity for further research in the area of coalitions against
CSAE.","In order to maximise the knowledge gain by researchers,
recent studies [68, 81] are reviewing options to apply blockchain technology to the secure distribution
of scientiÔ¨Åc datasets.",A promising approach is presented by The Technology Coalition (2020).,2022-01-09 16:25:37+00:00,Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations,cs.CV,"['cs.CV', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Thomas Grubl'), arxiv.Result.Author('Harjinder Singh Lallie')]","The precise age estimation of child sexual abuse and exploitation (CSAE)
victims is one of the most significant digital forensic challenges.
Investigators often need to determine the age of victims by looking at images
and interpreting the sexual development stages and other human characteristics.
The main priority - safeguarding children -- is often negatively impacted by a
huge forensic backlog, cognitive bias and the immense psychological stress that
this work can entail. This paper evaluates existing facial image datasets and
proposes a new dataset tailored to the needs of similar digital forensic
research contributions. This small, diverse dataset of 0 to 20-year-old
individuals contains 245 images and is merged with 82 unique images from the
FG-NET dataset, thus achieving a total of 327 images with high image diversity
and low age range density. The new dataset is tested on the Deep EXpectation
(DEX) algorithm pre-trained on the IMDB-WIKI dataset. The overall results for
young adolescents aged 10 to 15 and older adolescents/adults aged 16 to 20 are
very encouraging -- achieving MAEs as low as 1.79, but also suggest that the
accuracy for children aged 0 to 10 needs further work. In order to determine
the efficacy of the prototype, valuable input of four digital forensic experts,
including two forensic investigators, has been taken into account to improve
age estimation results. Further research is required to extend datasets both
concerning image density and the equal distribution of factors such as gender
and racial diversity.",0.22190233,-0.030721981,-0.017242376,A
290,"In contrast, the human identiÔ¨Åcation      From a model perspective, there are three key aspects
task is the least investigated task arising from the small size   that need further research: model architecture, effects of
of face biometric when captured from an aerial platform.","There are a wide range of resources, including      7.2.2 Model Development
public datasets, papers and even competitions available
addressing these tasks.","unconstrained data on model trustworthiness, prediction
The task of recognizing faces from long distance with cam-        uncertainties and model explainability.",2022-01-09 20:13:27+00:00,The State of Aerial Surveillance: A Survey,cs.CV,"['cs.CV', 'cs.AI', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Kien Nguyen'), arxiv.Result.Author('Clinton Fookes'), arxiv.Result.Author('Sridha Sridharan'), arxiv.Result.Author('Yingli Tian'), arxiv.Result.Author('Xiaoming Liu'), arxiv.Result.Author('Feng Liu'), arxiv.Result.Author('Arun Ross')]","The rapid emergence of airborne platforms and imaging sensors are enabling
new forms of aerial surveillance due to their unprecedented advantages in
scale, mobility, deployment and covert observation capabilities. This paper
provides a comprehensive overview of human-centric aerial surveillance tasks
from a computer vision and pattern recognition perspective. It aims to provide
readers with an in-depth systematic review and technical analysis of the
current state of aerial surveillance tasks using drones, UAVs and other
airborne platforms. The main object of interest is humans, where single or
multiple subjects are to be detected, identified, tracked, re-identified and
have their behavior analyzed. More specifically, for each of these four tasks,
we first discuss unique challenges in performing these tasks in an aerial
setting compared to a ground-based setting. We then review and analyze the
aerial datasets publicly available for each task, and delve deep into the
approaches in the aerial literature and investigate how they presently address
the aerial challenges. We conclude the paper with discussion on the missing
gaps and open research questions to inform future research avenues.",-0.14642641,-0.012198754,-0.17625624,B
291,"In contrast, the human identiÔ¨Åcation      From a model perspective, there are three key aspects
task is the least investigated task arising from the small size   that need further research: model architecture, effects of
of face biometric when captured from an aerial platform.","There are a wide range of resources, including      7.2.2 Model Development
public datasets, papers and even competitions available
addressing these tasks.","unconstrained data on model trustworthiness, prediction
The task of recognizing faces from long distance with cam-        uncertainties and model explainability.",2022-01-09 20:13:27+00:00,The State of Aerial Surveillance: A Survey,cs.CV,"['cs.CV', 'cs.AI', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Kien Nguyen'), arxiv.Result.Author('Clinton Fookes'), arxiv.Result.Author('Sridha Sridharan'), arxiv.Result.Author('Yingli Tian'), arxiv.Result.Author('Feng Liu'), arxiv.Result.Author('Xiaoming Liu'), arxiv.Result.Author('Arun Ross')]","The rapid emergence of airborne platforms and imaging sensors are enabling
new forms of aerial surveillance due to their unprecedented advantages in
scale, mobility, deployment and covert observation capabilities. This paper
provides a comprehensive overview of human-centric aerial surveillance tasks
from a computer vision and pattern recognition perspective. It aims to provide
readers with an in-depth systematic review and technical analysis of the
current state of aerial surveillance tasks using drones, UAVs and other
airborne platforms. The main object of interest is humans, where single or
multiple subjects are to be detected, identified, tracked, re-identified and
have their behavior analyzed. More specifically, for each of these four tasks,
we first discuss unique challenges in performing these tasks in an aerial
setting compared to a ground-based setting. We then review and analyze the
aerial datasets publicly available for each task, and delve deep into the
approaches in the aerial literature and investigate how they presently address
the aerial challenges. We conclude the paper with discussion on the missing
gaps and open research questions to inform future research avenues.",-0.14642641,-0.012198754,-0.17625624,B
303,"We again attribute          Secondly, we train CSP and Cascade RCNN on CityPersons
this to the bias present in the design of current state-of-the-art      and evaluate them on ECP [4] to further study the generaliza-
pedestrian detectors, which are tailored for speciÔ¨Åc datasets and       tion abilities of them under different diverse degrees of training
therefore limit their generalization ability.","the-art pedestrian detectors (except for BGCNet [23]) as well in
cross dataset evaluation, presented in Table 5.","Moreover, a signiÔ¨Åcant    datasets.",2022-01-10 06:00:26+00:00,"Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond",cs.CV,['cs.CV'],"[arxiv.Result.Author('Irtiza Hasan'), arxiv.Result.Author('Shengcai Liao'), arxiv.Result.Author('Jinpeng Li'), arxiv.Result.Author('Saad Ullah Akram'), arxiv.Result.Author('Ling Shao')]","Pedestrian detection is the cornerstone of many vision based applications,
starting from object tracking to video surveillance and more recently,
autonomous driving. With the rapid development of deep learning in object
detection, pedestrian detection has achieved very good performance in
traditional single-dataset training and evaluation setting. However, in this
study on generalizable pedestrian detectors, we show that, current pedestrian
detectors poorly handle even small domain shifts in cross-dataset evaluation.
We attribute the limited generalization to two main factors, the method and the
current sources of data. Regarding the method, we illustrate that biasness
present in the design choices (e.g anchor settings) of current pedestrian
detectors are the main contributing factor to the limited generalization. Most
modern pedestrian detectors are tailored towards target dataset, where they do
achieve high performance in traditional single training and testing pipeline,
but suffer a degrade in performance when evaluated through cross-dataset
evaluation. Consequently, a general object detector performs better in
cross-dataset evaluation compared with state of the art pedestrian detectors,
due to its generic design. As for the data, we show that the autonomous driving
benchmarks are monotonous in nature, that is, they are not diverse in scenarios
and dense in pedestrians. Therefore, benchmarks curated by crawling the web
(which contain diverse and dense scenarios), are an efficient source of
pre-training for providing a more robust representation. Accordingly, we
propose a progressive fine-tuning strategy which improves generalization. Code
and models cab accessed at https://github.com/hasanirtiza/Pedestron.",-0.23516443,-0.12415016,-0.06260136,B
304,"We again attribute          Secondly, we train CSP and Cascade RCNN on CityPersons
this to the bias present in the design of current state-of-the-art      and evaluate them on ECP [4] to further study the generaliza-
pedestrian detectors, which are tailored for speciÔ¨Åc datasets and       tion abilities of them under different diverse degrees of training
therefore limit their generalization ability.","the-art pedestrian detectors (except for BGCNet [23]) as well in
cross dataset evaluation, presented in Table 5.","Moreover, a signiÔ¨Åcant    datasets.",2022-01-10 06:00:26+00:00,"Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond",cs.CV,['cs.CV'],"[arxiv.Result.Author('Irtiza Hasan'), arxiv.Result.Author('Shengcai Liao'), arxiv.Result.Author('Jinpeng Li'), arxiv.Result.Author('Saad Ullah Akram'), arxiv.Result.Author('Ling Shao')]","Pedestrian detection is the cornerstone of many vision based applications,
starting from object tracking to video surveillance and more recently,
autonomous driving. With the rapid development of deep learning in object
detection, pedestrian detection has achieved very good performance in
traditional single-dataset training and evaluation setting. However, in this
study on generalizable pedestrian detectors, we show that, current pedestrian
detectors poorly handle even small domain shifts in cross-dataset evaluation.
We attribute the limited generalization to two main factors, the method and the
current sources of data. Regarding the method, we illustrate that biasness
present in the design choices (e.g anchor settings) of current pedestrian
detectors are the main contributing factor to the limited generalization. Most
modern pedestrian detectors are tailored towards target dataset, where they do
achieve high performance in traditional single training and testing pipeline,
but suffer a degrade in performance when evaluated through cross-dataset
evaluation. Consequently, a general object detector performs better in
cross-dataset evaluation compared with state of the art pedestrian detectors,
due to its generic design. As for the data, we show that the autonomous driving
benchmarks are monotonous in nature, that is, they are not diverse in scenarios
and dense in pedestrians. Therefore, benchmarks curated by crawling the web
(which contain diverse and dense scenarios), are an efficient source of
pre-training for providing a more robust representation. Accordingly, we
propose a progressive fine-tuning strategy which improves generalization. Code
and models can accessed at https://github.com/hasanirtiza/Pedestron.",-0.23516443,-0.12415016,-0.06260136,B
332,"The new paradigm
exempts the necessity of manually extract representative features from experts and also provides
paramount results regarding gait recognition, surpassing existing challenges and opening room for
further research.","A new trend on machine learning, known as deep learning, emerged in the last years as a
revolutionary tool to handle topics in image and sound processing, computer vision, and speech,
overwhelmingly outperforming virtually any baseline established until then.","Due to the increasing number of biometric and gait recognition works developed in the last years,
many authors provided studies summarizing each field‚Äôs main achievements.",2022-01-10 12:44:42+00:00,Gait Recognition Based on Deep Learning: A Survey,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Claudio Filipi Gon√ßalves dos Santos'), arxiv.Result.Author('Diego de Souza Oliveira'), arxiv.Result.Author('Leandro A. Passos'), arxiv.Result.Author('Rafael Gon√ßalves Pires'), arxiv.Result.Author('Daniel Felipe Silva Santos'), arxiv.Result.Author('Lucas Pascotti Valem'), arxiv.Result.Author('Thierry P. Moreira'), arxiv.Result.Author('Marcos Cleison S. Santana'), arxiv.Result.Author('Mateus Roder'), arxiv.Result.Author('Jo√£o Paulo Papa'), arxiv.Result.Author('Danilo Colombo')]","In general, biometry-based control systems may not rely on individual
expected behavior or cooperation to operate appropriately. Instead, such
systems should be aware of malicious procedures for unauthorized access
attempts. Some works available in the literature suggest addressing the problem
through gait recognition approaches. Such methods aim at identifying human
beings through intrinsic perceptible features, despite dressed clothes or
accessories. Although the issue denotes a relatively long-time challenge, most
of the techniques developed to handle the problem present several drawbacks
related to feature extraction and low classification rates, among other issues.
However, deep learning-based approaches recently emerged as a robust set of
tools to deal with virtually any image and computer-vision related problem,
providing paramount results for gait recognition as well. Therefore, this work
provides a surveyed compilation of recent works regarding biometric detection
through gait recognition with a focus on deep learning approaches, emphasizing
their benefits, and exposing their weaknesses. Besides, it also presents
categorized and characterized descriptions of the datasets, approaches, and
architectures employed to tackle associated constraints.",-0.19960089,-0.1797521,-0.0010880604,C
386,"The pre-
works considering DL algorithms with EEG is the following:        sented results were encouraging for further study and indicate
                                                                  the existence of a relationship between visual attention and
   ‚Ä¢ The use of convolutional neural networks (CNN) has           brain activity.",A non-exhaustive list of     average position of the centre of interest in the video.,"On the other hand, different datasets aiming to
      been considered to extract feature from EEG signals.",2022-01-11 12:16:10+00:00,Where Is My Mind (looking at)? Predicting Visual Attention from Brain Activity,cs.CV,"['cs.CV', 'cs.AI', 'eess.SP', 'q-bio.NC']","[arxiv.Result.Author('Victor Delvigne'), arxiv.Result.Author('No√© Tits'), arxiv.Result.Author('Luca La Fisca'), arxiv.Result.Author('Nathan Hubens'), arxiv.Result.Author('Antoine Maiorca'), arxiv.Result.Author('Hazem Wannous'), arxiv.Result.Author('Thierry Dutoit'), arxiv.Result.Author('Jean-Philippe Vandeborre')]","Visual attention estimation is an active field of research at the crossroads
of different disciplines: computer vision, artificial intelligence and
medicine. One of the most common approaches to estimate a saliency map
representing attention is based on the observed images. In this paper, we show
that visual attention can be retrieved from EEG acquisition. The results are
comparable to traditional predictions from observed images, which is of great
interest. For this purpose, a set of signals has been recorded and different
models have been developed to study the relationship between visual attention
and brain activity. The results are encouraging and comparable with other
approaches estimating attention with other modalities. The codes and dataset
considered in this paper have been made available at
\url{https://figshare.com/s/3e353bd1c621962888ad} to promote research in the
field.",-0.06688648,-0.0664414,-0.066801414,C
395,"mask marginalization as that in MaskFormer [6], where the                       In the ablations, we further study the effectiveness of our
                                                                                proposed components, including usage of multi-scale fea-
category prediction for pixel (h, w) is computed as                             tures, cross-scale inter-query attention, loss designs, and
                                                                                model parameter sharing.","We choose MaskFormer [6]
semantic segmentation map is obtained by probability-                           as our baseline model because of its strong performance
                                                                                among the mask-level classiÔ¨Åcation methods [6, 20, 38].","Experimental results demonstrate
               argmaxi‚àà{1,...,K} pi ¬∑ mi(h, w)                          (6)     that our model can learn useful information from multi-
                                                                                scale feature maps to deliver high quality segmentation
3.2.",2022-01-11 16:09:25+00:00,Pyramid Fusion Transformer for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zipeng Qin'), arxiv.Result.Author('Jianbo Liu'), arxiv.Result.Author('Xiaolin Zhang'), arxiv.Result.Author('Maoqing Tian'), arxiv.Result.Author('Aojun Zhou'), arxiv.Result.Author('Shuai Yi'), arxiv.Result.Author('Hongsheng Li')]","The recently proposed MaskFormer \cite{maskformer} gives a refreshed
perspective on the task of semantic segmentation: it shifts from the popular
pixel-level classification paradigm to a mask-level classification method. In
essence, it generates paired probabilities and masks corresponding to category
segments and combines them during inference for the segmentation maps. The
segmentation quality thus relies on how well the queries can capture the
semantic information for categories and their spatial locations within the
images. In our study, we find that per-mask classification decoder on top of a
single-scale feature is not effective enough to extract reliable probability or
mask. To mine for rich semantic information across the feature pyramid, we
propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask
approach semantic segmentation on top of multi-scale features. To efficiently
utilize image features of different resolutions without incurring too much
computational overheads, PFT uses a multi-scale transformer decoder with
cross-scale inter-query attention to exchange complimentary information.
Extensive experimental evaluations and ablations demonstrate the efficacy of
our framework. In particular, we achieve a 3.2 mIoU improvement on COCO-Stuff
10K dataset with ResNet-101c compared to MaskFormer. Besides, on ADE20K
validation set, our result with Swin-B backbone matches that of MaskFormer's
with a much larger Swin-L backbone in both single-scale and multi-scale
inference, achieving 54.1 mIoU and 55.3 mIoU respectively. Using a Swin-L
backbone, we achieve 56.0 mIoU single-scale result on the ADE20K validation set
and 57.2 multi-scale result, obtaining state-of-the-art performance on the
dataset.",-0.11779567,-0.09795186,0.07986861,C
396,"In the ablations,
we further study the effectiveness of our proposed components, including usage
of multi-scale features, cross-scale inter-query attention, and the design of at-
tention weight loss.","We choose MaskFormer [7] as our baseline model because of its strong perfor-
mance among the mask-level classification methods [7,40,24].","Experimental results demonstrate that our model can learn
useful information from multi-scale feature maps to deliver high quality segmen-
tation maps with our proposed multi-scale transformer decoder and optimization
for query-pixel cross-attention.",2022-01-11 16:09:25+00:00,Pyramid Fusion Transformer for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zipeng Qin'), arxiv.Result.Author('Jianbo Liu'), arxiv.Result.Author('Xiaolin Zhang'), arxiv.Result.Author('Maoqing Tian'), arxiv.Result.Author('Aojun Zhou'), arxiv.Result.Author('Shuai Yi'), arxiv.Result.Author('Hongsheng Li')]","The recently proposed MaskFormer gives a refreshed perspective on the task of
semantic segmentation: it shifts from the popular pixel-level classification
paradigm to a mask-level classification method. In essence, it generates paired
probabilities and masks corresponding to category segments and combines them
during inference for the segmentation maps. In our study, we find that per-mask
classification decoder on top of a single-scale feature is not effective enough
to extract reliable probability or mask. To mine for rich semantic information
across the feature pyramid, we propose a transformer-based Pyramid Fusion
Transformer (PFT) for per-mask approach semantic segmentation with multi-scale
features. The proposed transformer decoder performs cross-attention between the
learnable queries and each spatial feature from the feature pyramid in parallel
and uses cross-scale inter-query attention to exchange complimentary
information. We achieve competitive performance on three widely used semantic
segmentation datasets. In particular, on ADE20K validation set, our result with
Swin-B backbone surpasses that of MaskFormer's with a much larger Swin-L
backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU
and 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale
56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on
the dataset. Extensive experiments on three widely used semantic segmentation
datasets verify the effectiveness of our proposed method.",-0.02184649,-0.07733804,0.0065137986,C
397,"To further study the improve-
ments brought by our framework, we conduct experiments with different variants
of the baseline model with the same Swin-T backbone and compare our per-
formance.",Comparison with stronger baseline models.,"Specifically, we first train MaskFormer [7] with different single-scale
features of spatial shapes 1/8, 1/16, 1/32 the input image resolution respectively,
where using a 1/32 scale feature map corresponds to the original MaskFormer.",2022-01-11 16:09:25+00:00,Pyramid Fusion Transformer for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zipeng Qin'), arxiv.Result.Author('Jianbo Liu'), arxiv.Result.Author('Xiaolin Zhang'), arxiv.Result.Author('Maoqing Tian'), arxiv.Result.Author('Aojun Zhou'), arxiv.Result.Author('Shuai Yi'), arxiv.Result.Author('Hongsheng Li')]","The recently proposed MaskFormer gives a refreshed perspective on the task of
semantic segmentation: it shifts from the popular pixel-level classification
paradigm to a mask-level classification method. In essence, it generates paired
probabilities and masks corresponding to category segments and combines them
during inference for the segmentation maps. In our study, we find that per-mask
classification decoder on top of a single-scale feature is not effective enough
to extract reliable probability or mask. To mine for rich semantic information
across the feature pyramid, we propose a transformer-based Pyramid Fusion
Transformer (PFT) for per-mask approach semantic segmentation with multi-scale
features. The proposed transformer decoder performs cross-attention between the
learnable queries and each spatial feature from the feature pyramid in parallel
and uses cross-scale inter-query attention to exchange complimentary
information. We achieve competitive performance on three widely used semantic
segmentation datasets. In particular, on ADE20K validation set, our result with
Swin-B backbone surpasses that of MaskFormer's with a much larger Swin-L
backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU
and 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale
56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on
the dataset. Extensive experiments on three widely used semantic segmentation
datasets verify the effectiveness of our proposed method.",-0.023671754,0.026928237,0.15426514,C
398,"To further study the improvements
from the proposed loss, we ablate the loss weight and verify our design through
14     Qin et al.","2 row 2,
using our attention weight loss helps the cross-attention to concentrate more on
the regions corresponding to the categories.","Table 5: Analysis of query-pixel cross-                    49.0            48.7     48.7
attention weight loss.",2022-01-11 16:09:25+00:00,Pyramid Fusion Transformer for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zipeng Qin'), arxiv.Result.Author('Jianbo Liu'), arxiv.Result.Author('Xiaolin Zhang'), arxiv.Result.Author('Maoqing Tian'), arxiv.Result.Author('Aojun Zhou'), arxiv.Result.Author('Shuai Yi'), arxiv.Result.Author('Hongsheng Li')]","The recently proposed MaskFormer gives a refreshed perspective on the task of
semantic segmentation: it shifts from the popular pixel-level classification
paradigm to a mask-level classification method. In essence, it generates paired
probabilities and masks corresponding to category segments and combines them
during inference for the segmentation maps. In our study, we find that per-mask
classification decoder on top of a single-scale feature is not effective enough
to extract reliable probability or mask. To mine for rich semantic information
across the feature pyramid, we propose a transformer-based Pyramid Fusion
Transformer (PFT) for per-mask approach semantic segmentation with multi-scale
features. The proposed transformer decoder performs cross-attention between the
learnable queries and each spatial feature from the feature pyramid in parallel
and uses cross-scale inter-query attention to exchange complimentary
information. We achieve competitive performance on three widely used semantic
segmentation datasets. In particular, on ADE20K validation set, our result with
Swin-B backbone surpasses that of MaskFormer's with a much larger Swin-L
backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU
and 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale
56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on
the dataset. Extensive experiments on three widely used semantic segmentation
datasets verify the effectiveness of our proposed method.",0.14499064,-0.08375713,-0.035864767,A
399,"open the way to further research in the context of             Moglow: Probabilistic and controllable motion syn-
lightweight neural animation.",(2020).,thesis using normalising Ô¨Çows.,2022-01-11 16:39:32+00:00,Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Antoine Maiorca'), arxiv.Result.Author('Nathan Hubens'), arxiv.Result.Author('Sohaib Laraba'), arxiv.Result.Author('Thierry Dutoit')]","In the past few years, neural character animation has emerged and offered an
automatic method for animating virtual characters. Their motion is synthesized
by a neural network. Controlling this movement in real time with a user-defined
control signal is also an important task in video games for example. Solutions
based on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given
impressive results in generating and controlling various movements with
close-range interactions between the environment and the virtual character.
However, a major shortcoming of fully-connected layers is their computational
and memory cost which may lead to sub-optimized solution. In this work, we
apply pruning algorithms to compress an MLP- MoE neural network in the context
of interactive character animation, which reduces its number of parameters and
accelerates its computation time with a trade-off between this acceleration and
the synthesized motion quality. This work demonstrates that, with the same
number of experts and parameters, the pruned model produces less motion
artifacts than the dense model and the learned high-level motion features are
similar for both",0.021868441,0.054443818,-0.062162165,B
400,"open the way to further research in the context of             Moglow: Probabilistic and controllable motion syn-
lightweight neural animation.",(2020).,thesis using normalising Ô¨Çows.,2022-01-11 16:39:32+00:00,Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Antoine Maiorca'), arxiv.Result.Author('Nathan Hubens'), arxiv.Result.Author('Sohaib Laraba'), arxiv.Result.Author('Thierry Dutoit')]","In the past few years, neural character animation has emerged and offered an
automatic method for animating virtual characters. Their motion is synthesized
by a neural network. Controlling this movement in real time with a user-defined
control signal is also an important task in video games for example. Solutions
based on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given
impressive results in generating and controlling various movements with
close-range interactions between the environment and the virtual character.
However, a major shortcoming of fully-connected layers is their computational
and memory cost which may lead to sub-optimized solution. In this work, we
apply pruning algorithms to compress an MLP- MoE neural network in the context
of interactive character animation, which reduces its number of parameters and
accelerates its computation time with a trade-off between this acceleration and
the synthesized motion quality. This work demonstrates that, with the same
number of experts and parameters, the pruned model produces less motion
artifacts than the dense model and the learned high-level motion features are
similar for both",0.021868441,0.054443818,-0.062162165,B
409,"We anticipate that this method will spark further research   [15] R. Newcombe, S. Lovegrove, and A. Davison, ‚ÄúDTAM: Dense tracking
in this direction.","The foundation of       [14] C. Kerl, J. Sturm, and D. Cremers, ‚ÄúRobust odometry estimation for
our IMU initialization is delayed marginalization, which also         RGB-D cameras,‚Äù in ICRA, 2013.
enables the pose graph bundle adjustment.","The idea of delayed marginalization could          and mapping in real-time,‚Äù in ICCV, 2011.
be applied to more use cases, e.g.",2022-01-11 18:30:37+00:00,DM-VIO: Delayed Marginalization Visual-Inertial Odometry,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Lukas von Stumberg'), arxiv.Result.Author('Daniel Cremers')]","We present DM-VIO, a monocular visual-inertial odometry system based on two
novel techniques called delayed marginalization and pose graph bundle
adjustment. DM-VIO performs photometric bundle adjustment with a dynamic weight
for visual residuals. We adopt marginalization, which is a popular strategy to
keep the update time constrained, but it cannot easily be reversed, and
linearization points of connected variables have to be fixed. To overcome this
we propose delayed marginalization: The idea is to maintain a second factor
graph, where marginalization is delayed. This allows us to later readvance this
delayed graph, yielding an updated marginalization prior with new and
consistent linearization points. In addition, delayed marginalization enables
us to inject IMU information into already marginalized states. This is the
foundation of the proposed pose graph bundle adjustment, which we use for IMU
initialization. In contrast to prior works on IMU initialization, it is able to
capture the full photometric uncertainty, improving the scale estimation. In
order to cope with initially unobservable scale, we continue to optimize scale
and gravity direction in the main system after IMU initialization is complete.
We evaluate our system on the EuRoC, TUM-VI, and 4Seasons datasets, which
comprise flying drone, large-scale handheld, and automotive scenarios. Thanks
to the proposed IMU initialization, our system exceeds the state of the art in
visual-inertial odometry, even outperforming stereo-inertial methods while
using only a single camera and IMU. The code will be published at
http://vision.in.tum.de/dm-vio",-0.19971627,0.39220375,-0.060951415,B
416,"A speciÔ¨Åc object-detection model for LA in OMR
could be a promising avenue for further research, in which speciÔ¨Åc characteris-
tics of this type of documents could be exploited.","As the aforementioned results show, no model detects all the regions of in-
terest in music score images.","For example, the fact that
their general structure is regular or that their regions are usually wider than
taller.",2022-01-11 21:59:03+00:00,Region-based Layout Analysis of Music Score Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Francisco J. Castellanos'), arxiv.Result.Author('Carlos Garrido-Munoz'), arxiv.Result.Author('Antonio R√≠os-Vila'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","The Layout Analysis (LA) stage is of vital importance to the correct
performance of an Optical Music Recognition (OMR) system. It identifies the
regions of interest, such as staves or lyrics, which must then be processed in
order to transcribe their content. Despite the existence of modern approaches
based on deep learning, an exhaustive study of LA in OMR has not yet been
carried out with regard to the precision of different models, their
generalization to different domains or, more importantly, their impact on
subsequent stages of the pipeline. This work focuses on filling this gap in
literature by means of an experimental study of different neural architectures,
music document types and evaluation scenarios. The need for training data has
also led to a proposal for a new semi-synthetic data generation technique that
enables the efficient applicability of LA approaches in real scenarios. Our
results show that: (i) the choice of the model and its performance are crucial
for the entire transcription process; (ii) the metrics commonly used to
evaluate the LA stage do not always correlate with the final performance of the
OMR system, and (iii) the proposed data-generation technique enables
state-of-the-art results to be achieved with a limited set of labeled data.",-0.07213954,-0.0040220805,-0.2237593,C
436,"(2020); Sauder  unlabeled York point clouds, encouraging further research
and Sievers (2019), including PointNet Qi et al.","Finally, to further facili-
For simplicity, we faithfully follow the three baseline net-   tate the research in this research area, we also release the
works used in their original paper Wang et al.","(2017a),      exploration on this part of the data.",2022-01-12 14:48:11+00:00,SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Qingyong Hu'), arxiv.Result.Author('Bo Yang'), arxiv.Result.Author('Sheikh Khalid'), arxiv.Result.Author('Wen Xiao'), arxiv.Result.Author('Niki Trigoni'), arxiv.Result.Author('Andrew Markham')]","With the recent availability and affordability of commercial depth sensors
and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets
have been publicized to facilitate research in 3D computer vision. However,
existing datasets either cover relatively small areas or have limited semantic
annotations. Fine-grained understanding of urban-scale 3D scenes is still in
its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV
photogrammetry point cloud dataset consisting of nearly three billion points
collected from three UK cities, covering 7.6 km^2. Each point in the dataset
has been labelled with fine-grained semantic annotations, resulting in a
dataset that is three times the size of the previous existing largest
photogrammetric point cloud dataset. In addition to the more commonly
encountered categories such as road and vegetation, urban-level categories
including rail, bridge, and river are also included in our dataset. Based on
this dataset, we further build a benchmark to evaluate the performance of
state-of-the-art segmentation algorithms. In particular, we provide a
comprehensive analysis and identify several key challenges limiting urban-scale
point cloud understanding. The dataset is available at
http://point-cloud-analysis.cs.ox.ac.uk.",0.001595676,0.23006073,0.07617077,B
472,"To enable further research, we are releasing the dataset that we had prepared from the TMC challenge,
                                                  consisting of 4,380 fake and 2,563 real videos, with various video and/or audio manipulation methods
                                                  employed to produce different types of fake media.","To tackle the issue,
                                                 we have organized the Trusted Media Challenge (TMC) to explore how ArtiÔ¨Åcial Intelligence (AI)
                                                  technologies could be leveraged to combat fake media.","All the videos in the TMC dataset are accompanied
                                                 with audios and have a minimum resolution of 360p.",2022-01-13 04:32:52+00:00,AI Singapore Trusted Media Challenge Dataset,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weiling Chen'), arxiv.Result.Author('Benjamin Chua'), arxiv.Result.Author('Stefan Winkler'), arxiv.Result.Author('See Kiong Ng')]","The development of powerful deep learning technologies has brought about some
negative effects to both society and individuals. One such issue is the
emergence of fake media. To tackle the issue, we have organized the Trusted
Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies
could be leveraged to combat fake media.
  To enable further research, we are releasing the dataset that we had prepared
from the TMC challenge, consisting of 4,380 fake and 2,563 real videos, with
various video and/or audio manipulation methods employed to produce different
types of fake media. All the videos in the TMC dataset are accompanied with
audios and have a minimum resolution of 360p. The videos have various
durations, background, illumination, and may contain perturbations that mimic
transmission errors and compression.
  We have also carried out a user study to demonstrate the quality of the TMC
dataset and to compare the performance of humans and AI models. The results
showed that the TMC dataset can fool human participants in many cases, and the
winning AI models of the Trusted Media Challenge outperformed humans.
  The TMC dataset is available for research purpose upon request via
tmc-dataset@aisingapore.org.",-0.099166885,-0.09419445,-0.05587796,C
473,"As a large amount of training data including both real and fake videos are essential for training an
                                       automatic detector, we have constructed and released the TMC dataset which includes 4,380 fake and 2,563 real videos
                                       that were part of the challenge to contribute to the community for further research.","The challenge focused on the detection of audiovisual fake media, where either or both video and audio modalities
                                       may be modiÔ¨Åed.",‚àóW.,2022-01-13 04:32:52+00:00,AI Singapore Trusted Media Challenge Dataset,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weiling Chen'), arxiv.Result.Author('Benjamin Chua'), arxiv.Result.Author('Stefan Winkler'), arxiv.Result.Author('See Kiong Ng')]","The development of powerful deep learning technologies has brought about some
negative effects to both society and individuals. One such issue is the
emergence of fake media. To tackle the issue, we have organized the Trusted
Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies
could be leveraged to combat fake media.
  To enable further research, we are releasing the dataset that we had prepared
from the TMC challenge, consisting of 4,380 fake and 2,563 real videos, with
various video and/or audio manipulation methods employed to produce different
types of fake media. All the videos in the TMC dataset are accompanied with
audios and have a minimum resolution of 360p. The videos have various
durations, background, illumination, and may contain perturbations that mimic
transmission errors and compression.
  We have also carried out a user study to demonstrate the quality of the TMC
dataset and to compare the performance of humans and AI models. The results
showed that the TMC dataset can fool human participants in many cases, and the
winning AI models of the Trusted Media Challenge outperformed humans.
  The TMC dataset is available for research purpose upon request via
tmc-dataset@aisingapore.org.",-0.17201626,-0.12510945,-0.04560147,C
474,"5.2 Analysis

Similar to user studies describe in Section 4, we further study the performance of winners‚Äô models on different types of
fake media and generation methods.","They also used 3 different models to detect audio manipulation
(ResNet50), face manipulation (WSDAN [26]) and lip-sync errors (SyncNet) respectively and aggregate at the score
level using a random forest.",The results are in shown in the following tables.,2022-01-13 04:32:52+00:00,AI Singapore Trusted Media Challenge Dataset,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weiling Chen'), arxiv.Result.Author('Benjamin Chua'), arxiv.Result.Author('Stefan Winkler'), arxiv.Result.Author('See Kiong Ng')]","The development of powerful deep learning technologies has brought about some
negative effects to both society and individuals. One such issue is the
emergence of fake media. To tackle the issue, we have organized the Trusted
Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies
could be leveraged to combat fake media.
  To enable further research, we are releasing the dataset that we had prepared
from the TMC challenge, consisting of 4,380 fake and 2,563 real videos, with
various video and/or audio manipulation methods employed to produce different
types of fake media. All the videos in the TMC dataset are accompanied with
audios and have a minimum resolution of 360p. The videos have various
durations, background, illumination, and may contain perturbations that mimic
transmission errors and compression.
  We have also carried out a user study to demonstrate the quality of the TMC
dataset and to compare the performance of humans and AI models. The results
showed that the TMC dataset can fool human participants in many cases, and the
winning AI models of the Trusted Media Challenge outperformed humans.
  The TMC dataset is available for research purpose upon request via
tmc-dataset@aisingapore.org.",0.12765695,-0.1322737,-0.12748937,C
475,"To enable further research, we are releasing the dataset                         social manipulation, or celebrity porn.","These Deepfakes with high authenticity pose
                                        tificial Intelligence (AI) technologies could be leveraged to combat                         new threats and risks in the form of scams, fraud, disinformation,
                                        fake media.","In particular, fake audio-
                                        from the TMC, consists of 4,380 fake and 2,563 real videos, with var-                        visual media such as news and interviews can be used to spread
                                        ious video and audio manipulation methods employed to produce                                misinformation that may cause social fissures.",2022-01-13 04:32:52+00:00,Trusted Media Challenge Dataset and User Study,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weiling Chen'), arxiv.Result.Author('Sheng Lun Benjamin Chua'), arxiv.Result.Author('Stefan Winkler'), arxiv.Result.Author('See-Kiong Ng')]","The development of powerful deep learning technologies has brought about some
negative effects to both society and individuals. One such issue is the
emergence of fake media. To tackle the issue, we have organized the Trusted
Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies
could be leveraged to combat fake media. To enable further research, we are
releasing the dataset that we had prepared from the TMC challenge, consisting
of 4,380 fake and 2,563 real videos, with various video and/or audio
manipulation methods employed to produce different types of fake media. All the
videos in the TMC dataset are accompanied with audios and have a minimum
resolution of 360p. The videos have various durations, background,
illumination, and may contain perturbations that mimic transmission errors and
compression. We have also carried out a user study to demonstrate the quality
of the TMC dataset and to compare the performance of humans and AI models. The
results showed that the TMC dataset can fool human participants in many cases,
and the winning AI models of the Trusted Media Challenge outperformed humans.
The TMC dataset is available for research purpose upon request via
tmc-dataset@aisingapore.org.",-0.05614679,-0.11003445,-0.07986659,C
476,"For further research by the
                                                                                                                                     community, we are releasing the TMC dataset which includes 4,380
                                        ‚Ä¢ Computing methodologies ‚Üí Computer vision tasks; ‚Ä¢ Ap-                                     fake and 2,563 real videos that were part of the challenge.","In particular, we focused on
                                                                                                                                     the detection of audiovisual fake media, where either or both video
                                        CCS CONCEPTS                                                                                 and audio modalities may be modified.",plied computing ‚Üí Computer forensics.,2022-01-13 04:32:52+00:00,Trusted Media Challenge Dataset and User Study,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weiling Chen'), arxiv.Result.Author('Sheng Lun Benjamin Chua'), arxiv.Result.Author('Stefan Winkler'), arxiv.Result.Author('See-Kiong Ng')]","The development of powerful deep learning technologies has brought about some
negative effects to both society and individuals. One such issue is the
emergence of fake media. To tackle the issue, we have organized the Trusted
Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies
could be leveraged to combat fake media. To enable further research, we are
releasing the dataset that we had prepared from the TMC challenge, consisting
of 4,380 fake and 2,563 real videos, with various video and/or audio
manipulation methods employed to produce different types of fake media. All the
videos in the TMC dataset are accompanied with audios and have a minimum
resolution of 360p. The videos have various durations, background,
illumination, and may contain perturbations that mimic transmission errors and
compression. We have also carried out a user study to demonstrate the quality
of the TMC dataset and to compare the performance of humans and AI models. The
results showed that the TMC dataset can fool human participants in many cases,
and the winning AI models of the Trusted Media Challenge outperformed humans.
The TMC dataset is available for research purpose upon request via
tmc-dataset@aisingapore.org.",-0.14063343,0.0095111085,-0.06845364,C
488,We will release our code for further research and comparisons.,"Also, we can Ô¨Ånd many solutions by controlling a single parameter at the
inference phase.","References

 [1] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, ‚ÄúDeep learning for single image super-resolution:
      A brief review,‚Äù IEEE Transactions on Multimedia, vol.",2022-01-13 11:39:29+00:00,Flexible Style Image Super-Resolution using Conditional Objective,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Seung Ho Park'), arxiv.Result.Author('Young Su Moon'), arxiv.Result.Author('Nam Ik Cho')]","Recent studies have significantly enhanced the performance of single-image
super-resolution (SR) using convolutional neural networks (CNNs). While there
can be many high-resolution (HR) solutions for a given input, most existing
CNN-based methods do not explore alternative solutions during the inference. A
typical approach to obtaining alternative SR results is to train multiple SR
models with different loss weightings and exploit the combination of these
models. Instead of using multiple models, we present a more efficient method to
train a single adjustable SR model on various combinations of losses by taking
advantage of multi-task learning. Specifically, we optimize an SR model with a
conditional objective during training, where the objective is a weighted sum of
multiple perceptual losses at different feature levels. The weights vary
according to given conditions, and the set of weights is defined as a style
controller. Also, we present an architecture appropriate for this training
scheme, which is the Residual-in-Residual Dense Block equipped with spatial
feature transformation layers. At the inference phase, our trained model can
generate locally different outputs conditioned on the style control map.
Extensive experiments show that the proposed SR model produces various
desirable reconstructions without artifacts and yields comparable quantitative
performance to state-of-the-art SR methods.",-0.14363407,-0.0011853119,0.31751835,C
489,We will release our code for further research and comparisons.,"Also, we can Ô¨Ånd many solutions by controlling a single parameter at the
inference phase.","References

 [1] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, ‚ÄúDeep learning for single image super-resolution:
      A brief review,‚Äù IEEE Transactions on Multimedia, vol.",2022-01-13 11:39:29+00:00,Flexible Style Image Super-Resolution using Conditional Objective,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Seung Ho Park'), arxiv.Result.Author('Young Su Moon'), arxiv.Result.Author('Nam Ik Cho')]","Recent studies have significantly enhanced the performance of single-image
super-resolution (SR) using convolutional neural networks (CNNs). While there
can be many high-resolution (HR) solutions for a given input, most existing
CNN-based methods do not explore alternative solutions during the inference. A
typical approach to obtaining alternative SR results is to train multiple SR
models with different loss weightings and exploit the combination of these
models. Instead of using multiple models, we present a more efficient method to
train a single adjustable SR model on various combinations of losses by taking
advantage of multi-task learning. Specifically, we optimize an SR model with a
conditional objective during training, where the objective is a weighted sum of
multiple perceptual losses at different feature levels. The weights vary
according to given conditions, and the set of weights is defined as a style
controller. Also, we present an architecture appropriate for this training
scheme, which is the Residual-in-Residual Dense Block equipped with spatial
feature transformation layers. At the inference phase, our trained model can
generate locally different outputs conditioned on the style control map.
Extensive experiments show that the proposed SR model produces various
desirable reconstructions without artifacts and yields comparable quantitative
performance to state-of-the-art SR methods.",-0.14363407,-0.0011853119,0.31751835,C
490,We will release our code for further research and comparisons.,"Also, we can Ô¨Ånd many solutions by controlling a single parameter at the
inference phase.","References

 [1] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, ‚ÄúDeep learning for single image super-resolution:
      A brief review,‚Äù IEEE Transactions on Multimedia, vol.",2022-01-13 11:39:29+00:00,Flexible Style Image Super-Resolution using Conditional Objective,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Seung Ho Park'), arxiv.Result.Author('Young Su Moon'), arxiv.Result.Author('Nam Ik Cho')]","Recent studies have significantly enhanced the performance of single-image
super-resolution (SR) using convolutional neural networks (CNNs). While there
can be many high-resolution (HR) solutions for a given input, most existing
CNN-based methods do not explore alternative solutions during the inference. A
typical approach to obtaining alternative SR results is to train multiple SR
models with different loss weightings and exploit the combination of these
models. Instead of using multiple models, we present a more efficient method to
train a single adjustable SR model on various combinations of losses by taking
advantage of multi-task learning. Specifically, we optimize an SR model with a
conditional objective during training, where the objective is a weighted sum of
multiple perceptual losses at different feature levels. The weights vary
according to given conditions, and the set of weights is defined as a style
controller. Also, we present an architecture appropriate for this training
scheme, which is the Residual-in-Residual Dense Block equipped with spatial
feature transformation layers. At the inference phase, our trained model can
generate locally different outputs conditioned on the style control map.
Extensive experiments show that the proposed SR model produces various
desirable reconstructions without artifacts and yields comparable quantitative
performance to state-of-the-art SR methods.",-0.14363407,-0.0011853119,0.31751835,C
514,Code and models will be available for further research.,"Our
                                        proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a
                                        single V100 GPU device.","Index Terms‚ÄîVideo Object Detection, Vision Transformers, Scene Understanding, Video Understanding.",2022-01-13 16:17:34+00:00,TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qianyu Zhou'), arxiv.Result.Author('Xiangtai Li'), arxiv.Result.Author('Lu He'), arxiv.Result.Author('Yibo Yang'), arxiv.Result.Author('Guangliang Cheng'), arxiv.Result.Author('Yunhai Tong'), arxiv.Result.Author('Lizhuang Ma'), arxiv.Result.Author('Dacheng Tao')]","Detection Transformer (DETR) and Deformable DETR have been proposed to
eliminate the need for many hand-designed components in object detection while
demonstrating good performance as previous complex hand-crafted detectors.
However, their performance on Video Object Detection (VOD) has not been well
explored. In this paper, we present TransVOD, the first end-to-end video object
detection system based on spatial-temporal Transformer architectures. The first
goal of this paper is to streamline the pipeline of VOD, effectively removing
the need for many hand-crafted components for feature aggregation, e.g.,
optical flow model, relation networks. Besides, benefited from the object query
design in DETR, our method does not need complicated post-processing methods
such as Seq-NMS. In particular, we present a temporal Transformer to aggregate
both the spatial object queries and the feature memories of each frame. Our
temporal transformer consists of two components: Temporal Query Encoder (TQE)
to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to
obtain current frame detection results. These designs boost the strong baseline
deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID
dataset. Then, we present two improved versions of TransVOD including
TransVOD++ and TransVOD Lite. The former fuses object-level information into
object query via dynamic convolution while the latter models the entire video
clips as the output to speed up the inference time. We give detailed analysis
of all three models in the experiment part. In particular, our proposed
TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet
VID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and
accuracy trade-off with 83.7% mAP while running at around 30 FPS on a single
V100 GPU device. Code and models will be available for further research.",-0.21273491,0.2679072,0.11392824,B
515,Code and models will be available for further research.,"Our
                                        proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a
                                        single V100 GPU device.","Index Terms‚ÄîVideo Object Detection, Vision Transformers, Scene Understanding, Video Understanding.",2022-01-13 16:17:34+00:00,TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qianyu Zhou'), arxiv.Result.Author('Xiangtai Li'), arxiv.Result.Author('Lu He'), arxiv.Result.Author('Yibo Yang'), arxiv.Result.Author('Guangliang Cheng'), arxiv.Result.Author('Yunhai Tong'), arxiv.Result.Author('Lizhuang Ma'), arxiv.Result.Author('Dacheng Tao')]","Detection Transformer (DETR) and Deformable DETR have been proposed to
eliminate the need for many hand-designed components in object detection while
demonstrating good performance as previous complex hand-crafted detectors.
However, their performance on Video Object Detection (VOD) has not been well
explored. In this paper, we present TransVOD, the first end-to-end video object
detection system based on spatial-temporal Transformer architectures. The first
goal of this paper is to streamline the pipeline of VOD, effectively removing
the need for many hand-crafted components for feature aggregation, e.g.,
optical flow model, relation networks. Besides, benefited from the object query
design in DETR, our method does not need complicated post-processing methods
such as Seq-NMS. In particular, we present a temporal Transformer to aggregate
both the spatial object queries and the feature memories of each frame. Our
temporal transformer consists of two components: Temporal Query Encoder (TQE)
to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to
obtain current frame detection results. These designs boost the strong baseline
deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID
dataset. Then, we present two improved versions of TransVOD including
TransVOD++ and TransVOD Lite. The former fuses object-level information into
object query via dynamic convolution while the latter models the entire video
clips as the output to speed up the inference time. We give detailed analysis
of all three models in the experiment part. In particular, our proposed
TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet
VID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and
accuracy trade-off with 83.7% mAP while running at around 30 FPS on a single
V100 GPU device. Code and models will be available for further research.",-0.21273491,0.2679072,0.11392824,B
516,Code and models will be available for further research.,"Our
                                           proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a
                                           single V100 GPU device.","Index Terms‚ÄîVideo Object Detection, Vision Transformers, Scene Understanding, Video Understanding.",2022-01-13 16:17:34+00:00,TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qianyu Zhou'), arxiv.Result.Author('Xiangtai Li'), arxiv.Result.Author('Lu He'), arxiv.Result.Author('Yibo Yang'), arxiv.Result.Author('Guangliang Cheng'), arxiv.Result.Author('Yunhai Tong'), arxiv.Result.Author('Lizhuang Ma'), arxiv.Result.Author('Dacheng Tao')]","Detection Transformer (DETR) and Deformable DETR have been proposed to
eliminate the need for many hand-designed components in object detection while
demonstrating good performance as previous complex hand-crafted detectors.
However, their performance on Video Object Detection (VOD) has not been well
explored. In this paper, we present TransVOD, the first end-to-end video object
detection system based on spatial-temporal Transformer architectures. The first
goal of this paper is to streamline the pipeline of VOD, effectively removing
the need for many hand-crafted components for feature aggregation, e.g.,
optical flow model, relation networks. Besides, benefited from the object query
design in DETR, our method does not need complicated post-processing methods
such as Seq-NMS. In particular, we present a temporal Transformer to aggregate
both the spatial object queries and the feature memories of each frame. Our
temporal transformer consists of two components: Temporal Query Encoder (TQE)
to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to
obtain current frame detection results. These designs boost the strong baseline
deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID
dataset. Then, we present two improved versions of TransVOD including
TransVOD++ and TransVOD Lite. The former fuses object-level information into
object query via dynamic convolution while the latter models the entire video
clips as the output to speed up the inference time. We give detailed analysis
of all three models in the experiment part. In particular, our proposed
TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet
VID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and
accuracy trade-off with 83.7% mAP while running at around 30 FPS on a single
V100 GPU device. Code and models will be available for further research.",-0.21273491,0.2679072,0.11392824,B
535,"We further study our         [11] A. R. Feyjie, R. Azad, M. Pedersoli, C. Kauffman, I.",ImageNet and tiered-ImageNet benchmarks.,"B.
methods in paradigms of active and continual learning, proposing              Ayed, and J. Dolz, ‚ÄúSemi-supervised few-shot learning for
extensions for undertaking such tasks.",2022-01-13 18:59:02+00:00,"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning",cs.CV,['cs.CV'],"[arxiv.Result.Author('Peyman Bateni'), arxiv.Result.Author('Jarred Barber'), arxiv.Result.Author('Raghav Goyal'), arxiv.Result.Author('Vaden Masrani'), arxiv.Result.Author('Jan-Willem van de Meent'), arxiv.Result.Author('Leonid Sigal'), arxiv.Result.Author('Frank Wood')]","Modern deep learning requires large-scale extensively labelled datasets for
training. Few-shot learning aims to alleviate this issue by learning
effectively from few labelled examples. In previously proposed few-shot visual
classifiers, it is assumed that the feature manifold, where classifier
decisions are made, has uncorrelated feature dimensions and uniform feature
variance. In this work, we focus on addressing the limitations arising from
this assumption by proposing a variance-sensitive class of models that operates
in a low-label regime. The first method, Simple CNAPS, employs a hierarchically
regularized Mahalanobis-distance based classifier combined with a state of the
art neural adaptive feature extractor to achieve strong performance on
Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extend
this approach to a transductive learning setting, proposing Transductive CNAPS.
This transductive method combines a soft k-means parameter refinement procedure
with a two-step task encoder to achieve improved test-time classification
accuracy using unlabelled data. Transductive CNAPS achieves state of the art
performance on Meta-Dataset. Finally, we explore the use of our methods (Simple
and Transductive) for ""out of the box"" continual and active learning. Extensive
experiments on large scale benchmarks illustrate robustness and versatility of
this, relatively speaking, simple class of models. All trained model
checkpoints and corresponding source codes have been made publicly available.",-0.16430691,-0.30902755,0.07137803,C
536,"We further study our methods in paradigms of active and continual learn-
ing, proposing extensions for undertaking such tasks.","These models are able to accomplish
strong performance on Meta-Dataset, mini-ImageNet and tiered-ImageNet bench-
marks.","Future research on better continual learning of the task-wise feature manifolds
in both models can be beneÔ¨Åcial in boosting the performance.",2022-01-13 18:59:02+00:00,"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning",cs.CV,['cs.CV'],"[arxiv.Result.Author('Peyman Bateni'), arxiv.Result.Author('Jarred Barber'), arxiv.Result.Author('Raghav Goyal'), arxiv.Result.Author('Vaden Masrani'), arxiv.Result.Author('Jan-Willem van de Meent'), arxiv.Result.Author('Leonid Sigal'), arxiv.Result.Author('Frank Wood')]","Modern deep learning requires large-scale extensively labelled datasets for
training. Few-shot learning aims to alleviate this issue by learning
effectively from few labelled examples. In previously proposed few-shot visual
classifiers, it is assumed that the feature manifold, where classifier
decisions are made, has uncorrelated feature dimensions and uniform feature
variance. In this work, we focus on addressing the limitations arising from
this assumption by proposing a variance-sensitive class of models that operates
in a low-label regime. The first method, Simple CNAPS, employs a hierarchically
regularized Mahalanobis-distance based classifier combined with a state of the
art neural adaptive feature extractor to achieve strong performance on
Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extend
this approach to a transductive learning setting, proposing Transductive CNAPS.
This transductive method combines a soft k-means parameter refinement procedure
with a two-step task encoder to achieve improved test-time classification
accuracy using unlabelled data. Transductive CNAPS achieves state of the art
performance on Meta-Dataset. Finally, we explore the use of our methods (Simple
and Transductive) for ""out of the box"" continual and active learning. Extensive
experiments on large scale benchmarks illustrate robustness and versatility of
this, relatively speaking, simple class of models. All trained model
checkpoints and corresponding source codes have been made publicly available.",-0.15152875,-0.3279829,0.10274024,C
559,"Compared with the original GAN network,the cGAN adding conditional constraints to the
generator and discriminator,and it can be seen as an improvement from the unsupervised learning
generation adversarial network to the supervised learning generation adversarial network that
established a solid basis for subsequent development and further research.","Among these networks, the well-known networks including the
Conditional Generative Adversarial Networks (cGAN)[1][5], which was proposed in the same year in
2014.","In 2015, because researchers
want to see what the generative network has learned to visualize the representation information, deep
convolutional generative confrontation networks(DCGANs)[11] are proposed to clarify the
representation information of the generated images.",2022-01-14 09:00:55+00:00,Arbitrary Handwriting Image Style Transfer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kai Yang'), arxiv.Result.Author('Xiaoman Liang'), arxiv.Result.Author('Huihuang Zhao')]","This paper proposed a method to imitate handwriting style by style transfer.
We proposed an neural network model based on conditional generative adversarial
networks (cGAN) for handwriting style transfer. This paper improved the loss
function on the basis of the GAN. Compared with other handwriting imitation
methods, the handwriting style transfer's effect and efficiency have been
significantly improved. The experiments showed that the shape of the generated
Chinese characters is clear and the analysis of experimental data showed the
Generative adversarial networks showed excellent performance in handwriting
style transfer. The generated text image is closer to the real handwriting and
achieved a better performance in term of handwriting imitation.",-0.17285588,-0.2147519,0.14170288,C
577,"However, these experiments are only a few initial
attempts, and their practical versatility needs further research on more datasets and tasks.","In terms of generalization, we discussed the ability of language to transfer knowledge to new categories, because
language can describe unseen objects through the reorganization of symbols.","Our model achieved good results on simple datasets, and performance on more complex datasets still needs to improve.",2022-01-14 14:54:58+00:00,Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuqi Wang'), arxiv.Result.Author('Xu-Yao Zhang'), arxiv.Result.Author('Cheng-Lin Liu'), arxiv.Result.Author('Zhaoxiang Zhang')]","Representation is a core issue in artificial intelligence. Humans use
discrete language to communicate and learn from each other, while machines use
continuous features (like vector, matrix, or tensor in deep neural networks) to
represent cognitive patterns. Discrete symbols are low-dimensional, decoupled,
and have strong reasoning ability, while continuous features are
high-dimensional, coupled, and have incredible abstracting capabilities. In
recent years, deep learning has developed the idea of continuous representation
to the extreme, using millions of parameters to achieve high accuracies.
Although this is reasonable from the statistical perspective, it has other
major problems like lacking interpretability, poor generalization, and is easy
to be attacked. Since both paradigms have strengths and weaknesses, a better
choice is to seek reconciliation. In this paper, we make an initial attempt
towards this direction. Specifically, we propose to combine symbolism and
connectionism principles by using neural networks to derive a discrete
representation. This process is highly similar to human language, which is a
natural combination of discrete symbols and neural systems, where the brain
processes continuous signals and represents intelligence via discrete language.
To mimic this functionality, we denote our approach as machine language. By
designing an interactive environment and task, we demonstrated that machines
could generate a spontaneous, flexible, and semantic language through
cooperation. Moreover, through experiments we show that discrete language
representation has several advantages compared with continuous feature
representation, from the aspects of interpretability, generalization, and
robustness.",0.024154162,-0.39534116,-0.2529752,C
606,"[46] show the advantage
of learning-based features, further research followed that used CNN layer ac-
tivations as oÔ¨Ä-the-shelf image descriptors that appear as objective results in
retrieval tasks [47, 48].",Since Krizhevsky et al.,"Following classic retrieval approaches, such work uses
CNN to aggregate local features [49, 50].",2022-01-15 09:57:45+00:00,A Critical Analysis of Image-based Camera Pose Estimation Techniques,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Meng Xu'), arxiv.Result.Author('Youchen Wang'), arxiv.Result.Author('Bin Xu'), arxiv.Result.Author('Jun Zhang'), arxiv.Result.Author('Jian Ren'), arxiv.Result.Author('Stefan Poslad'), arxiv.Result.Author('Pengfei Xu')]","Camera, and associated with its objects within the field of view,
localization could benefit many computer vision fields, such as autonomous
driving, robot navigation, and augmented reality (AR). In this survey, we first
introduce specific application areas and the evaluation metrics for camera
localization pose according to different sub-tasks (learning-based 2D-2D task,
feature-based 2D-3D task, and 3D-3D task). Then, we review common methods for
structure-based camera pose estimation approaches, absolute pose regression and
relative pose regression approaches by critically modelling the methods to
inspire further improvements in their algorithms such as loss functions, neural
network structures. Furthermore, we summarise what are the popular datasets
used for camera localization and compare the quantitative and qualitative
results of these methods with detailed performance metrics. Finally, we discuss
future research possibilities and applications.",-0.23742428,-0.19751619,-0.0077574058,C
631,"While we expect them to
In a similar fashion, video models could greatly beneÔ¨Åt from      follow the same trend as other modalities, further research
longer time spans by leveraging memory modules (such as           is needed.","other settings [50], [52], [71], [88].","the one seen in [58]) to create recurrent Transformers and
extend VTs receptive Ô¨Åeld even further.",2022-01-16 07:31:55+00:00,Video Transformers: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Javier Selva'), arxiv.Result.Author('Anders S. Johansen'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Kamal Nasrollahi'), arxiv.Result.Author('Thomas B. Moeslund'), arxiv.Result.Author('Albert Clap√©s')]","Transformer models have shown great success modeling long-range interactions.
Nevertheless, they scale quadratically with input length and lack inductive
biases. These limitations can be further exacerbated when dealing with the high
dimensionality of video. Proper modeling of video, which can span from seconds
to hours, requires handling long-range interactions. This makes Transformers a
promising tool for solving video related tasks, but some adaptations are
required. While there are previous works that study the advances of
Transformers for vision tasks, there is none that focus on in-depth analysis of
video-specific designs. In this survey we analyse and summarize the main
contributions and trends for adapting Transformers to model video data.
Specifically, we delve into how videos are embedded and tokenized, finding a
very widspread use of large CNN backbones to reduce dimensionality and a
predominance of patches and frames as tokens. Furthermore, we study how the
Transformer layer has been tweaked to handle longer sequences, generally by
reducing the number of tokens in single attention operation. Also, we analyse
the self-supervised losses used to train Video Transformers, which to date are
mostly constrained to contrastive approaches. Finally, we explore how other
modalities are integrated with video and conduct a performance comparison on
the most common benchmark for Video Transformers (i.e., action classification),
finding them to outperform 3D CNN counterparts with equivalent FLOPs and no
significant parameter increase.",-0.035700206,0.013930904,0.07887162,C
632,This may further be seen in works reporting improved           motivate further research on this area.,"Therefore, we
training Transformers and CNN embedding layers end-to-              next analyse beneÔ¨Åts and limitations of SSL for VTs, so as to
end.","CNN-based results alone after being trained as the embed-
ding net of a Transformer [51], [81], [146], pointing towards           Traditional time-related pretext tasks (see [159] for a
CNNs beneÔ¨Åting from long-term temporal feedback pro-                complete review) are rarely used in the context of VTs.",2022-01-16 07:31:55+00:00,Video Transformers: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Javier Selva'), arxiv.Result.Author('Anders S. Johansen'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Kamal Nasrollahi'), arxiv.Result.Author('Thomas B. Moeslund'), arxiv.Result.Author('Albert Clap√©s')]","Transformer models have shown great success handling long-range interactions,
making them a promising tool for modeling video. However they lack inductive
biases and scale quadratically with input length. These limitations are further
exacerbated when dealing with the high dimensionality introduced with the
temporal dimension. While there are surveys analyzing the advances of
Transformers for vision, none focus on an in-depth analysis of video-specific
designs. In this survey we analyze main contributions and trends of works
leveraging Transformers to model video. Specifically, we delve into how videos
are handled as input-level first. Then, we study the architectural changes made
to deal with video more efficiently, reduce redundancy, re-introduce useful
inductive biases, and capture long-term temporal dynamics. In addition we
provide an overview of different training regimes and explore effective
self-supervised learning strategies for video. Finally, we conduct a
performance comparison on the most common benchmark for Video Transformers
(i.e., action classification), finding them to outperform 3D ConvNets even with
less computational complexity.",-0.045936957,-0.25941154,0.15581837,C
633,"On the one hand, long-range         MSE loss has been shown to disregard such details [180],
temporal interactions provided by Transformers boosts               [181], [182], so further research may be needed.","However, the generally used
training of Transformer layers.","Finally,
CNN‚Äôs performance in many application scenarios [33], [43],         we highlight HOG features, which provide the best com-
[46], [68], [70], [84], [120], [128], [148].",2022-01-16 07:31:55+00:00,Video Transformers: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Javier Selva'), arxiv.Result.Author('Anders S. Johansen'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Kamal Nasrollahi'), arxiv.Result.Author('Thomas B. Moeslund'), arxiv.Result.Author('Albert Clap√©s')]","Transformer models have shown great success handling long-range interactions,
making them a promising tool for modeling video. However they lack inductive
biases and scale quadratically with input length. These limitations are further
exacerbated when dealing with the high dimensionality introduced with the
temporal dimension. While there are surveys analyzing the advances of
Transformers for vision, none focus on an in-depth analysis of video-specific
designs. In this survey we analyze main contributions and trends of works
leveraging Transformers to model video. Specifically, we delve into how videos
are handled as input-level first. Then, we study the architectural changes made
to deal with video more efficiently, reduce redundancy, re-introduce useful
inductive biases, and capture long-term temporal dynamics. In addition we
provide an overview of different training regimes and explore effective
self-supervised learning strategies for video. Finally, we conduct a
performance comparison on the most common benchmark for Video Transformers
(i.e., action classification), finding them to outperform 3D ConvNets even with
less computational complexity.",-0.062152863,-0.1441494,0.27400616,C
634,"Nev-
tains visual and aural information), which could be lever-          ertheless, further research is still needed to alleviate the
aged to learn more general representations.","Video is inherently multi-modal (i.e., con-         information, may prove beneÔ¨Åcial to video modeling.",The lack of             computational burden of self-supervision in video.,2022-01-16 07:31:55+00:00,Video Transformers: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Javier Selva'), arxiv.Result.Author('Anders S. Johansen'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Kamal Nasrollahi'), arxiv.Result.Author('Thomas B. Moeslund'), arxiv.Result.Author('Albert Clap√©s')]","Transformer models have shown great success handling long-range interactions,
making them a promising tool for modeling video. However they lack inductive
biases and scale quadratically with input length. These limitations are further
exacerbated when dealing with the high dimensionality introduced with the
temporal dimension. While there are surveys analyzing the advances of
Transformers for vision, none focus on an in-depth analysis of video-specific
designs. In this survey we analyze main contributions and trends of works
leveraging Transformers to model video. Specifically, we delve into how videos
are handled as input-level first. Then, we study the architectural changes made
to deal with video more efficiently, reduce redundancy, re-introduce useful
inductive biases, and capture long-term temporal dynamics. In addition we
provide an overview of different training regimes and explore effective
self-supervised learning strategies for video. Finally, we conduct a
performance comparison on the most common benchmark for Video Transformers
(i.e., action classification), finding them to outperform 3D ConvNets even with
less computational complexity.",-0.10364109,-0.13959228,-0.18654346,C
635,"When        our contributions in this paper will entice further research
targeting video-only tasks (e.g., tracking, segmentation, clas-     in many different areas of application and boost our current
siÔ¨Åcation) we see potential in multi-modal SSL to learn             understanding of Video Transformers.","We hope
spaces that exhibit better generalization capabilities.",such spaces.,2022-01-16 07:31:55+00:00,Video Transformers: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Javier Selva'), arxiv.Result.Author('Anders S. Johansen'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Kamal Nasrollahi'), arxiv.Result.Author('Thomas B. Moeslund'), arxiv.Result.Author('Albert Clap√©s')]","Transformer models have shown great success handling long-range interactions,
making them a promising tool for modeling video. However they lack inductive
biases and scale quadratically with input length. These limitations are further
exacerbated when dealing with the high dimensionality introduced with the
temporal dimension. While there are surveys analyzing the advances of
Transformers for vision, none focus on an in-depth analysis of video-specific
designs. In this survey we analyze main contributions and trends of works
leveraging Transformers to model video. Specifically, we delve into how videos
are handled as input-level first. Then, we study the architectural changes made
to deal with video more efficiently, reduce redundancy, re-introduce useful
inductive biases, and capture long-term temporal dynamics. In addition we
provide an overview of different training regimes and explore effective
self-supervised learning strategies for video. Finally, we conduct a
performance comparison on the most common benchmark for Video Transformers
(i.e., action classification), finding them to outperform 3D ConvNets even with
less computational complexity.",-0.14552805,-0.033094175,-0.06573096,C
651,"indicate that both darker light and larger distances reduce Rs,
while the impact of different angles on Rs does not show a                2) ConÔ¨Ådence: In order to further study the effectiveness
strong regularity.",The results             that our generated AEs are robust in the real-road driving test.,"and robustness of NTA and TA, we measure the detection
                                                                      conÔ¨Ådence of each image frame.",2022-01-17 03:24:31+00:00,Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Wei Jia'), arxiv.Result.Author('Zhaojun Lu'), arxiv.Result.Author('Haichun Zhang'), arxiv.Result.Author('Zhenglin Liu'), arxiv.Result.Author('Jie Wang'), arxiv.Result.Author('Gang Qu')]","Adversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have
received a lot of attention recently. However, majority of the research on AEs
is in the digital domain and the adversarial patches are static, which is very
different from many real-world DNN applications such as Traffic Sign
Recognition (TSR) systems in autonomous vehicles. In TSR systems, object
detectors use DNNs to process streaming video in real time. From the view of
object detectors, the traffic sign`s position and quality of the video are
continuously changing, rendering the digital AEs ineffective in the physical
world.
  In this paper, we propose a systematic pipeline to generate robust physical
AEs against real-world object detectors. Robustness is achieved in three ways.
First, we simulate the in-vehicle cameras by extending the distribution of
image transformations with the blur transformation and the resolution
transformation. Second, we design the single and multiple bounding boxes
filters to improve the efficiency of the perturbation training. Third, we
consider four representative attack vectors, namely Hiding Attack, Appearance
Attack, Non-Target Attack and Target Attack.
  We perform a comprehensive set of experiments under a variety of
environmental conditions, and considering illuminations in sunny and cloudy
weather as well as at night. The experimental results show that the physical
AEs generated from our pipeline are effective and robust when attacking the
YOLO v5 based TSR system. The attacks have good transferability and can deceive
other state-of-the-art object detectors. We launched HA and NTA on a brand-new
2021 model vehicle. Both attacks are successful in fooling the TSR system,
which could be a life-threatening case for autonomous vehicles. Finally, we
discuss three defense mechanisms based on image preprocessing, AEs detection,
and model enhancing.",-0.040884264,0.36881387,0.03791133,B
691,"ùëñùëñ                  ùëñùëñ

   (5) We open-source our algorithmic implementation and ar-             ùëñ =1              ùëñ =1
        chitecture1 to benefit the research community and enable
        further study of selective sensor fusion approaches for CPS         Typically, the more information a fusion filter has, the better the
        problems.","minimum variance unbiased estimator can be derived in a batch
                                                                         manner as:
   (4) We implement our approach on an industry-standard AV
        hardware platform, the Nvidia Drive PX2, to demonstrate          ùëõ                      ùëõ
        that our approach can be practically deployed in a real AV
        with comparable energy consumption, latency, and memory          ùë¶ÀÜ(ùëõ) = [‚àëÔ∏Å ùêªùëá ùëÖ‚àí1ùêªùëñ ]‚àí1 ¬∑ ‚àëÔ∏Å ùêªùëá ùëÖ‚àí1ùë•ùëñ                                (5)
        usage to state-of-the-art methods.",performance will be.,2022-01-17 22:19:53+00:00,HydraFusion: Context-Aware Selective Sensor Fusion for Robust and Efficient Autonomous Vehicle Perception,cs.CV,['cs.CV'],"[arxiv.Result.Author('Arnav Vaibhav Malawade'), arxiv.Result.Author('Trier Mortlock'), arxiv.Result.Author('Mohammad Abdullah Al Faruque')]","Although autonomous vehicles (AVs) are expected to revolutionize
transportation, robust perception across a wide range of driving contexts
remains a significant challenge. Techniques to fuse sensor data from camera,
radar, and lidar sensors have been proposed to improve AV perception. However,
existing methods are insufficiently robust in difficult driving contexts (e.g.,
bad weather, low light, sensor obstruction) due to rigidity in their fusion
implementations. These methods fall into two broad categories: (i) early
fusion, which fails when sensor data is noisy or obscured, and (ii) late
fusion, which cannot leverage features from multiple sensors and thus produces
worse estimates. To address these limitations, we propose HydraFusion: a
selective sensor fusion framework that learns to identify the current driving
context and fuses the best combination of sensors to maximize robustness
without compromising efficiency. HydraFusion is the first approach to propose
dynamically adjusting between early fusion, late fusion, and combinations
in-between, thus varying both how and when fusion is applied. We show that, on
average, HydraFusion outperforms early and late fusion approaches by 13.66% and
14.54%, respectively, without increasing computational complexity or energy
consumption on the industry-standard Nvidia Drive PX2 AV hardware platform. We
also propose and evaluate both static and deep-learning-based context
identification strategies. Our open-source code and model implementation are
available at https://github.com/AICPS/hydrafusion.",0.10364547,0.3110275,0.10547447,A
698,"This clearly shows that the depth estimation especially for a
lateral camera placement needs further research, and we hope to improve it in future versions.","Here it is clear that there are some outliers, but
the most common estimates have an error below 50 cm.","4.5 Marker detection

Table 7: Results of the marker detection.",2022-01-18 07:54:55+00:00,"Pistol: Pupil Invisible Supportive Tool to extract Pupil, Iris, Eye Opening, Eye Movements, Pupil and Iris Gaze Vector, and 2D as well as 3D Gaze",cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Wolfgang Fuhl'), arxiv.Result.Author('Daniel Weber'), arxiv.Result.Author('Enkelejda Kasneci')]","This paper describes a feature extraction and gaze estimation software, named
Pistol that can be used with Pupil Invisible projects and other eye trackers in
the future. In offline mode, our software extracts multiple features from the
eye including, the pupil and iris ellipse, eye aperture, pupil vector, iris
vector, eye movement types from pupil and iris velocities, marker detection,
marker distance, 2D gaze estimation for the pupil center, iris center, pupil
vector, and iris vector using Levenberg Marquart fitting and neural networks.
The gaze signal is computed in 2D for each eye and each feature separately and
for both eyes in 3D also for each feature separately. We hope this software
helps other researchers to extract state-of-the-art features for their research
out of their recordings.",-0.13606247,0.40615675,-0.0927757,B
751,"This leads to slower inference speed on average     However, such methods require either large-scale manual
but better performance, as we can see in Table I.                data annotation or synthetic data generation to work on
                                                                 new objects, with long data generation and training times,
   In addition, we further study the potential negative effects  whereas our method can quickly adapt online to new objects.",observation.,"of low-accuracy pose estimates in the pseudo ground truth        Further, our method can train directly on real data in a
used in the self-supervised training process, as these bad       self-supervised way, without requiring manual annotations
pose estimates may mislead the detector.",2022-01-18 20:55:56+00:00,OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Qiao Gu'), arxiv.Result.Author('Brian Okorn'), arxiv.Result.Author('David Held')]","Real-time object pose estimation is necessary for many robot manipulation
algorithms. However, state-of-the-art methods for object pose estimation are
trained for a specific set of objects; these methods thus need to be retrained
to estimate the pose of each new object, often requiring tens of GPU-days of
training for optimal performance. \revisef{In this paper, we propose the OSSID
framework,} leveraging a slow zero-shot pose estimator to self-supervise the
training of a fast detection algorithm. This fast detector can then be used to
filter the input to the pose estimator, drastically improving its inference
speed. We show that this self-supervised training exceeds the performance of
existing zero-shot detection methods on two widely used object pose estimation
and detection datasets, without requiring any human annotations. Further, we
show that the resulting method for pose estimation has a significantly faster
inference speed, due to the ability to filter out large parts of the image.
Thus, our method for self-supervised online learning of a detector (trained
using pseudo-labels from a slow pose estimator) leads to accurate pose
estimation at real-time speeds, without requiring human annotations.
Supplementary materials and code can be found at
https://georgegu1997.github.io/OSSID/",-0.24548101,-0.07501866,-0.095363885,B
752,"This leads to slower inference speed on average     However, such methods require either large-scale manual
but better performance, as we can see in Table I.                data annotation or synthetic data generation to work on
                                                                 new objects, with long data generation and training times,
   In addition, we further study the potential negative effects  whereas our method can quickly adapt online to new objects.",observation.,"of low-accuracy pose estimates in the pseudo ground truth        Further, our method can train directly on real data in a
used in the self-supervised training process, as these bad       self-supervised way, without requiring manual annotations
pose estimates may mislead the detector.",2022-01-18 20:55:56+00:00,OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Qiao Gu'), arxiv.Result.Author('Brian Okorn'), arxiv.Result.Author('David Held')]","Real-time object pose estimation is necessary for many robot manipulation
algorithms. However, state-of-the-art methods for object pose estimation are
trained for a specific set of objects; these methods thus need to be retrained
to estimate the pose of each new object, often requiring tens of GPU-days of
training for optimal performance. In this paper, we propose the OSSID
framework, leveraging a slow zero-shot pose estimator to self-supervise the
training of a fast detection algorithm. This fast detector can then be used to
filter the input to the pose estimator, drastically improving its inference
speed. We show that this self-supervised training exceeds the performance of
existing zero-shot detection methods on two widely used object pose estimation
and detection datasets, without requiring any human annotations. Further, we
show that the resulting method for pose estimation has a significantly faster
inference speed, due to the ability to filter out large parts of the image.
Thus, our method for self-supervised online learning of a detector (trained
using pseudo-labels from a slow pose estimator) leads to accurate pose
estimation at real-time speeds, without requiring human annotations.
Supplementary materials and code can be found at
https://georgegu1997.github.io/OSSID/",-0.24548101,-0.07501866,-0.095363885,B
790,"In the Ô¨Åeld of human-machine interaction [20], the gaze signal is used and further researched for interaction with
                                      robots [112] but also other technical devices [111, 18, 23, 45].","Based on the gaze signal it is possible to extract further information like the eye movement types [44, 55, 21, 22, 31, 34].","This involves not only simple control but also collaboration
                                      in which a human communicates complex behavior to a robot or system [95].",2022-01-19 16:22:02+00:00,GroupGazer: A Tool to Compute the Gaze per Participant in Groups with integrated Calibration to Map the Gaze Online to a Screen or Beamer Projection,cs.CV,['cs.CV'],[arxiv.Result.Author('Wolfgang Fuhl')],"In this paper we present GroupGaze. It is a tool that can be used to
calculate the gaze direction and the gaze position of whole groups. GroupGazer
calculates the gaze direction of every single person in the image and allows to
map these gaze vectors to a projection like a projector. In addition to the
person-specific gaze direction, the person affiliation of each gaze vector is
stored based on the position in the image. Also, it is possible to save the
group attention after a calibration. The software is free to use and requires a
simple webcam as well as an NVIDIA GPU and the operating system Windows or
Linux.",0.036236715,0.13744697,-0.37311548,B
824,"Section 6 discusses open issues and further research
                                                                ECCV        ICCV                            directions.","6%                                               Section 5 summarizes current research progress via performance
                                                                                                            comparisons.",Section 7 concludes this paper.,2022-01-20 09:10:20+00:00,The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Aixin Sun'), arxiv.Result.Author('Wei Jing'), arxiv.Result.Author('Joey Tianyi Zhou')]","Temporal sentence grounding in videos (TSGV), a.k.a., natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate methods in different categories with their strengths
and weaknesses. Lastly, we discuss issues with the current TSGV research and
share our insights about promising research directions.",0.14479673,0.21640012,0.010386184,A
856,"If the histopathological ob-
servation failed to make a diagnosis or requires further research, observation
techniques such as IHC can be assisted.","Currently, the
intraoperative histopathological examination of sentinel lymph nodes is mainly
based on frozen sections stained with H&E [25].","IHC is very helpful to conÔ¨Årm the diag-
nosis with a high sensitivity [24] [26].",2022-01-21 05:54:14+00:00,What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoqi Li'), arxiv.Result.Author('Haoyuan Chen'), arxiv.Result.Author('Chen Li'), arxiv.Result.Author('Md Mamunur Rahaman'), arxiv.Result.Author('Xintong Li'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Xiaoyan Li'), arxiv.Result.Author('Hongzan Sun'), arxiv.Result.Author('Marcin Grzegorzek')]","In the past ten years, the computing power of machine vision (MV) has been
continuously improved, and image analysis algorithms have developed rapidly. At
the same time, histopathological slices can be stored as digital images.
Therefore, MV algorithms can provide doctors with diagnostic references. In
particular, the continuous improvement of deep learning algorithms has further
improved the accuracy of MV in disease detection and diagnosis. This paper
reviews the applications of image processing technology based on MV in lymphoma
histopathological images in recent years, including segmentation,
classification and detection. Finally, the current methods are analyzed, some
more potential methods are proposed, and further prospects are made.",0.21143913,0.14891565,-0.07198513,A
857,"The method shows that the feature association
extracted from diÔ¨Äerent methods is a potential Ô¨Åeld for further research.","In [179], features are obtained from histology images through techniques in-
cluding multidimensional fractal models and deÔ¨Åning convolutional descriptors
with diÔ¨Äerent CNN models.","8.4.3 Potential Methods of ClassiÔ¨Åcation for LHIA

In the summarized papers, in addition to the commonly used classiÔ¨Åcation
methods, there are some other potential methods.",2022-01-21 05:54:14+00:00,What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoqi Li'), arxiv.Result.Author('Haoyuan Chen'), arxiv.Result.Author('Chen Li'), arxiv.Result.Author('Md Mamunur Rahaman'), arxiv.Result.Author('Xintong Li'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Xiaoyan Li'), arxiv.Result.Author('Hongzan Sun'), arxiv.Result.Author('Marcin Grzegorzek')]","In the past ten years, the computing power of machine vision (MV) has been
continuously improved, and image analysis algorithms have developed rapidly. At
the same time, histopathological slices can be stored as digital images.
Therefore, MV algorithms can provide doctors with diagnostic references. In
particular, the continuous improvement of deep learning algorithms has further
improved the accuracy of MV in disease detection and diagnosis. This paper
reviews the applications of image processing technology based on MV in lymphoma
histopathological images in recent years, including segmentation,
classification and detection. Finally, the current methods are analyzed, some
more potential methods are proposed, and further prospects are made.",-0.08285249,0.022238612,0.018976143,C
858,"If the histopathological ob-
servation failed to make a diagnosis or requires further research, observation
techniques such as IHC can be assisted.","Currently, the
intraoperative histopathological examination of sentinel lymph nodes is mainly
based on frozen sections stained with H&E [25].","IHC is very helpful to conÔ¨Årm the diag-
nosis with a high sensitivity [24] [26].",2022-01-21 05:54:14+00:00,What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoqi Li'), arxiv.Result.Author('Haoyuan Chen'), arxiv.Result.Author('Chen Li'), arxiv.Result.Author('Md Mamunur Rahaman'), arxiv.Result.Author('Xintong Li'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Xiaoyan Li'), arxiv.Result.Author('Hongzan Sun'), arxiv.Result.Author('Marcin Grzegorzek')]","In the past ten years, the computing power of machine vision (MV) has been
continuously improved, and image analysis algorithms have developed rapidly. At
the same time, histopathological slices can be stored as digital images.
Therefore, MV algorithms can provide doctors with diagnostic references. In
particular, the continuous improvement of deep learning algorithms has further
improved the accuracy of MV in disease detection and diagnosis. This paper
reviews the applications of image processing technology based on MV in lymphoma
histopathological images in recent years, including segmentation,
classification and detection. Finally, the current methods are analyzed, some
more potential methods are proposed, and further prospects are made.",0.21143913,0.14891565,-0.07198513,A
859,"The method shows that the feature association
extracted from diÔ¨Äerent methods is a potential Ô¨Åeld for further research.","In [179], features are obtained from histology images through techniques in-
cluding multidimensional fractal models and deÔ¨Åning convolutional descriptors
with diÔ¨Äerent CNN models.","8.4.3 Potential Methods of ClassiÔ¨Åcation for LHIA

In the summarized papers, in addition to the commonly used classiÔ¨Åcation
methods, there are some other potential methods.",2022-01-21 05:54:14+00:00,What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoqi Li'), arxiv.Result.Author('Haoyuan Chen'), arxiv.Result.Author('Chen Li'), arxiv.Result.Author('Md Mamunur Rahaman'), arxiv.Result.Author('Xintong Li'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Xiaoyan Li'), arxiv.Result.Author('Hongzan Sun'), arxiv.Result.Author('Marcin Grzegorzek')]","In the past ten years, the computing power of machine vision (MV) has been
continuously improved, and image analysis algorithms have developed rapidly. At
the same time, histopathological slices can be stored as digital images.
Therefore, MV algorithms can provide doctors with diagnostic references. In
particular, the continuous improvement of deep learning algorithms has further
improved the accuracy of MV in disease detection and diagnosis. This paper
reviews the applications of image processing technology based on MV in lymphoma
histopathological images in recent years, including segmentation,
classification and detection. Finally, the current methods are analyzed, some
more potential methods are proposed, and further prospects are made.",-0.08285249,0.022238612,0.018976143,C
957,"In section 5, we conclude with a few
suggestions for further research in this area.","The best ensemble reported in this work either exceeds the performance of
the state-of-the-art in the literature or achieves similar performance on all the tested datasets.","The main contributions of this study can be summarized as follows:
ÔÇ∑ Presented is a large evaluation across eleven freely available and diverse benchmarks of the performance of common image

     manipulation methods used for data augmentation.",2022-01-24 14:12:29+00:00,Feature transforms for image data augmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Loris Nanni'), arxiv.Result.Author('Michelangelo Paci'), arxiv.Result.Author('Sheryl Brahnam'), arxiv.Result.Author('Alessandra Lumini')]","A problem with Convolutional Neural Networks (CNNs) is that they require
large datasets to obtain adequate robustness; on small datasets, they are prone
to overfitting. Many methods have been proposed to overcome this shortcoming
with CNNs. In cases where additional samples cannot easily be collected, a
common approach is to generate more data points from existing data using an
augmentation technique. In image classification, many augmentation approaches
utilize simple image manipulation algorithms. In this work, we build ensembles
on the data level by adding images generated by combining fourteen augmentation
approaches, three of which are proposed here for the first time. These novel
methods are based on the Fourier Transform (FT), the Radon Transform (RT) and
the Discrete Cosine Transform (DCT). Pretrained ResNet50 networks are finetuned
on training sets that include images derived from each augmentation method.
These networks and several fusions are evaluated and compared across eleven
benchmarks. Results show that building ensembles on the data level by combining
different data augmentation methods produce classifiers that not only compete
competitively against the state-of-the-art but often surpass the best
approaches reported in the literature.",-0.15853488,0.0055891443,0.07174144,C
958,"To develop our data-driven approach for VPR that combines both visual and
semantic knowledge, and also to enable further research in this direction, we created a
new synthetic dataset.","The other is SYNTHIA [24], which contains very dense sequences of images
that are suitable for the visual localization task [20] (‚â§ 5m from one gps coordinate to
the other) but not for the coarser place recognition task usually considered in literature
(‚â§ 25m).","This new dataset was inspired by IDDA [1], which was built from the CARLA vir-
tual simulator [7] speciÔ¨Åcally for semantic segmentation but without GPS annotations.",2022-01-24 14:13:12+00:00,Learning Semantics for Visual Place Recognition through Multi-Scale Attention,cs.CV,['cs.CV'],"[arxiv.Result.Author('Valerio Paolicelli'), arxiv.Result.Author('Antonio Tavera'), arxiv.Result.Author('Gabriele Berton'), arxiv.Result.Author('Carlo Masone'), arxiv.Result.Author('Barbara Caputo')]","In this paper we address the task of visual place recognition (VPR), where
the goal is to retrieve the correct GPS coordinates of a given query image
against a huge geotagged gallery. While recent works have shown that building
descriptors incorporating semantic and appearance information is beneficial,
current state-of-the-art methods opt for a top down definition of the
significant semantic content. Here we present the first VPR algorithm that
learns robust global embeddings from both visual appearance and semantic
content of the data, with the segmentation process being dynamically guided by
the recognition of places through a multi-scale attention module. Experiments
on various scenarios validate this new approach and demonstrate its performance
against state-of-the-art methods. Finally, we propose the first synthetic-world
dataset suited for both place recognition and segmentation tasks.",-0.32735246,0.031346314,-0.13039026,B
959,"To develop our data-driven approach for VPR that combines both visual and
semantic knowledge, and also to enable further research in this direction, we created a
new synthetic dataset.","The other is SYNTHIA [24], which contains very dense sequences of images
that are suitable for the visual localization task [20] (‚â§ 5m from one gps coordinate to
the other) but not for the coarser place recognition task usually considered in literature
(‚â§ 25m).","This new dataset was inspired by IDDA [1], which was built from the CARLA vir-
tual simulator [7] speciÔ¨Åcally for semantic segmentation but without GPS annotations.",2022-01-24 14:13:12+00:00,Learning Semantics for Visual Place Recognition through Multi-Scale Attention,cs.CV,['cs.CV'],"[arxiv.Result.Author('Valerio Paolicelli'), arxiv.Result.Author('Antonio Tavera'), arxiv.Result.Author('Carlo Masone'), arxiv.Result.Author('Gabriele Berton'), arxiv.Result.Author('Barbara Caputo')]","In this paper we address the task of visual place recognition (VPR), where
the goal is to retrieve the correct GPS coordinates of a given query image
against a huge geotagged gallery. While recent works have shown that building
descriptors incorporating semantic and appearance information is beneficial,
current state-of-the-art methods opt for a top down definition of the
significant semantic content. Here we present the first VPR algorithm that
learns robust global embeddings from both visual appearance and semantic
content of the data, with the segmentation process being dynamically guided by
the recognition of places through a multi-scale attention module. Experiments
on various scenarios validate this new approach and demonstrate its performance
against state-of-the-art methods. Finally, we propose the first synthetic-world
dataset suited for both place recognition and segmentation tasks.",-0.32735246,0.031346314,-0.13039026,B
981,"This license inhibits further research on algorithmic fairness in this way, which
is especially unfortunate because the dataset has actor-provided age and gender labels as well as
human-reviewer-provided Fitzpatrick skin types and ambient lighting conditions.","We have done this due to a potential interpretation
of the data use license agreement which limits modiÔ¨Åcations and could preclude corrupting the images
in their dataset.","Yet, due to potential
legal liability, we choose not to compare to the CCD results of Dooley et al.",2022-01-25 02:21:42+00:00,Are Commercial Face Detection Models as Biased as Academic Models?,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Samuel Dooley'), arxiv.Result.Author('George Z. Wei'), arxiv.Result.Author('Tom Goldstein'), arxiv.Result.Author('John P. Dickerson')]","As facial recognition systems are deployed more widely, scholars and
activists have studied their biases and harms. Audits are commonly used to
accomplish this and compare the algorithmic facial recognition systems'
performance against datasets with various metadata labels about the subjects of
the images. Seminal works have found discrepancies in performance by gender
expression, age, perceived race, skin type, etc. These studies and audits often
examine algorithms which fall into two categories: academic models or
commercial models. We present a detailed comparison between academic and
commercial face detection systems, specifically examining robustness to noise.
We find that state-of-the-art academic face detection models exhibit
demographic disparities in their noise robustness, specifically by having
statistically significant decreased performance on older individuals and those
who present their gender in a masculine manner. When we compare the size of
these disparities to that of commercial models, we conclude that commercial
models - in contrast to their relatively larger development budget and
industry-level fairness commitments - are always as biased or more biased than
an academic model.",0.01399759,0.003980277,-0.06535831,C
990,"We hope that our work can                  the fact that visual perception is fundamentally an ill-posed
                                        serve as a baseline and inspire further research that perform                problem under occlusion or low illumination conditions.","Such failure is due to
                                        based on millimeter wave signals.",vision tasks with radio signals.,2022-01-25 08:43:01+00:00,RFMask: A Simple Baseline for Human Silhouette Segmentation with Radio Signals,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zhi Wu'), arxiv.Result.Author('Dongheng Zhang'), arxiv.Result.Author('Chunyang Xie'), arxiv.Result.Author('Cong Yu'), arxiv.Result.Author('Jinbo Chen'), arxiv.Result.Author('Yang Hu'), arxiv.Result.Author('Yan Chen')]","Human silhouette segmentation, which is originally defined in computer
vision, has achieved promising results for understanding human activities.
However, the physical limitation makes existing systems based on optical
cameras suffer from severe performance degradation under low illumination,
smoke, and/or opaque obstruction conditions. To overcome such limitations, in
this paper, we propose to utilize the radio signals, which can traverse
obstacles and are unaffected by the lighting conditions to achieve silhouette
segmentation. The proposed RFMask framework is composed of three modules. It
first transforms RF signals captured by millimeter wave radar on two planes
into spatial domain and suppress interference with the signal processing
module. Then, it locates human reflections on RF frames and extract features
from surrounding signals with human detection module. Finally, the extracted
features from RF frames are aggregated with an attention based mask generation
module. To verify our proposed framework, we collect a dataset containing
804,760 radio frames and 402,380 camera frames with human activities under
various scenes. Experimental results show that the proposed framework can
achieve impressive human silhouette segmentation even under the challenging
scenarios(such as low light and occlusion scenarios) where traditional
optical-camera-based methods fail. To the best of our knowledge, this is the
first investigation towards segmenting human silhouette based on millimeter
wave signals. We hope that our work can serve as a baseline and inspire further
research that perform vision tasks with radio signals. The dataset and codes
will be made in public.",0.018512037,0.33507395,0.07287164,B
999,"Additionally, further study on which optimizers are better for initial and Ô¨Ånal phases
also needs to be conducted.","DualOpT training needs to be investigated more using different
initialisations, hyper-parameter tuning, and learning rates to shed more light into the differences in loss surfaces near
and far away from the solution.","Among the token-mixing architectures, we found that FNet and MLP-Mixer were not suitable for vision applications
but WaveMix and ConvMixer perform at par with the transformer models while consuming less GPU RAM.",2022-01-25 12:32:09+00:00,Convolutional Xformers for Vision,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.4.0; I.4.1; I.4.7; I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2;\n  I.5.4']","[arxiv.Result.Author('Pranav Jeevan'), arxiv.Result.Author('Amit sethi')]","Vision transformers (ViTs) have found only limited practical use in
processing images, in spite of their state-of-the-art accuracy on certain
benchmarks. The reason for their limited use include their need for larger
training datasets and more computational resources compared to convolutional
neural networks (CNNs), owing to the quadratic complexity of their
self-attention mechanism. We propose a linear attention-convolution hybrid
architecture -- Convolutional X-formers for Vision (CXV) -- to overcome these
limitations. We replace the quadratic attention with linear attention
mechanisms, such as Performer, Nystr\""omformer, and Linear Transformer, to
reduce its GPU usage. Inductive prior for image data is provided by
convolutional sub-layers, thereby eliminating the need for class token and
positional embeddings used by the ViTs. We also propose a new training method
where we use two different optimizers during different phases of training and
show that it improves the top-1 image classification accuracy across different
architectures. CXV outperforms other architectures, token mixers (e.g.
ConvMixer, FNet and MLP Mixer), transformer models (e.g. ViT, CCT, CvT and
hybrid Xformers), and ResNets for image classification in scenarios with
limited data and GPU resources (cores, RAM, power).",0.014396068,0.0041571846,0.2967767,C
1007,"The main contributions of this work are:

                                                                ‚Ä¢ A free and open-source sUAS payload for deploying real-
                                                                  time, in-Ô¨Çight AI, enabling various missions supporting
                                                                  humanitarian assistance and disaster response

                                                                ‚Ä¢ A framework achieving accurate temporal synchroniza-
                                                                  tion of imagery and GPS/INS streams for exploitation of
                                                                  visio-inertial odometry and environment mapping

                                                                ‚Ä¢ A publicly available trained model for river ice segmen-
   tation for use in further research and development            segmentation maps or detected object bounding boxes)
                                                                 within a geospatial context.","Alternatively, low-cost sUAS deploy-
                                                               ments of the ADAPT payload could allow organizations and
                                                               indigenous communities to monitor river conditions, lever-
                                                               aging real-time, in-Ô¨Çight processing to rapidly disseminate
                                                               results over low-bandwidth wireless links.","The INS reports its pose (lo-
 ‚Ä¢ A novel active learning annotation workÔ¨Çow                    cation and orientation) with respect to the world as a func-
                                                                 tion of time (‚àº100 Hz).",2022-01-25 14:51:19+00:00,ADAPT: An Open-Source sUAS Payload for Real-Time Disaster Prediction and Response with AI,cs.CV,['cs.CV'],"[arxiv.Result.Author('Daniel Davila'), arxiv.Result.Author('Joseph VanPelt'), arxiv.Result.Author('Alexander Lynch'), arxiv.Result.Author('Adam Romlein'), arxiv.Result.Author('Peter Webley'), arxiv.Result.Author('Matthew S. Brown')]","Small unmanned aircraft systems (sUAS) are becoming prominent components of
many humanitarian assistance and disaster response (HADR) operations. Pairing
sUAS with onboard artificial intelligence (AI) substantially extends their
utility in covering larger areas with fewer support personnel. A variety of
missions, such as search and rescue, assessing structural damage, and
monitoring forest fires, floods, and chemical spills, can be supported simply
by deploying the appropriate AI models. However, adoption by
resource-constrained groups, such as local municipalities, regulatory agencies,
and researchers, has been hampered by the lack of a cost-effective,
readily-accessible baseline platform that can be adapted to their unique
missions. To fill this gap, we have developed the free and open-source ADAPT
multi-mission payload for deploying real-time AI and computer vision onboard a
sUAS during local and beyond-line-of-site missions. We have emphasized a
modular design with low-cost, readily-available components, open-source
software, and thorough documentation (https://kitware.github.io/adapt/). The
system integrates an inertial navigation system, high-resolution color camera,
computer, and wireless downlink to process imagery and broadcast georegistered
analytics back to a ground station. Our goal is to make it easy for the HADR
community to build their own copies of the ADAPT payload and leverage the
thousands of hours of engineering we have devoted to developing and testing. In
this paper, we detail the development and testing of the ADAPT payload. We
demonstrate the example mission of real-time, in-flight ice segmentation to
monitor river ice state and provide timely predictions of catastrophic flooding
events. We deploy a novel active learning workflow to annotate river ice
imagery, train a real-time deep neural network for ice segmentation, and
demonstrate operation in the field.",-0.1525707,0.08118955,-0.11307517,B
1018,"However, whether using
enhanced supervision pictures to continuously improve the quality of network training will cause
more artiÔ¨Åcial traces in the Ô¨Ånal generated pictures is worth further research, In addition, it is also
worth exploring whether the supervision picture enhanced in the last cycle can be used to directly
generate the supervision picture required in the next cycle instead of continuously using the original
high score picture to generate the supervision picture required in the next cycle.","RTSR improves the performance of the super sub network by enhancing
the quality of supervision pictures and repeatedly training the network.","6.2 Texture and perceptual cost function

Texture loss is used to describe the difference in texture style between the generated image and the
reference image.",2022-01-22 03:13:11+00:00,A Review of Deep Learning Based Image Super-resolution Techniques,cs.CV,['cs.CV'],[arxiv.Result.Author('Fangyuan Zhu')],"Image super-resolution technology is the process of obtaining high-resolution
images from one or more low-resolution images. With the development of deep
learning, image super-resolution technology based on deep learning method is
emerging. This paper reviews the research progress of the application of depth
learning method in the field of image super-resolution, introduces this kind of
super-resolution work from several aspects, and looks forward to the further
application of depth learning method in the field of image super-resolution. By
collecting and counting the relevant literature on the application of depth
learning in the field of image super-resolution, we preliminarily summarizes
the application results of depth learning method in the field of image
super-resolution, and reports the latest progress of image super-resolution
technology based on depth learning method.",-0.034661755,-0.12528855,0.19355135,C
1035,"Overall, we hope that this model can serve as a solid baseline for 2D medical image segmentation
and motivate further research in medical image analysis tasks.","We conduct extensive analyses to study the robustness of our
approach, and form a more detailed understanding of desirable properties in the medical domain (i.e.,
transparency and data efÔ¨Åciency).","It also provides a new perspective on
transfer learning in medical domain, and initially shed novel insights towards understanding neural
network behavior.",2022-01-26 03:50:02+00:00,Class-Aware Generative Adversarial Transformers for Medical Image Segmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Chenyu You'), arxiv.Result.Author('Ruihan Zhao'), arxiv.Result.Author('Fenglin Liu'), arxiv.Result.Author('Siyuan Dong'), arxiv.Result.Author('Sandeep Chinchali'), arxiv.Result.Author('Ufuk Topcu'), arxiv.Result.Author('Lawrence Staib'), arxiv.Result.Author('James S. Duncan')]","Transformers have made remarkable progress towards modeling long-range
dependencies within the medical image analysis domain. However, current
transformer-based models suffer from several disadvantages: (1) existing
methods fail to capture the important features of the images due to the naive
tokenization scheme; (2) the models suffer from information loss because they
only consider single-scale feature representations; and (3) the segmentation
label maps generated by the models are not accurate enough without considering
rich semantic contexts and anatomical textures. In this work, we present
CASTformer, a novel type of generative adversarial transformers, for 2D medical
image segmentation. First, we take advantage of the pyramid structure to
construct multi-scale representations and handle multi-scale variations. We
then design a novel class-aware transformer module to better learn the
discriminative regions of objects with semantic structures. Lastly, we utilize
an adversarial training strategy that boosts segmentation accuracy and
correspondingly allows a transformer-based discriminator to capture high-level
semantically correlated contents and low-level anatomical features. Our
experiments demonstrate that CASTformer dramatically outperforms previous
state-of-the-art transformer-based approaches on three benchmarks, obtaining
2.54%-5.88% absolute improvements in Dice over previous models. Further
qualitative experiments provide a more detailed picture of the model's inner
workings, shed light on the challenges in improved transparency, and
demonstrate that transfer learning can greatly improve performance and reduce
the size of medical image datasets in training, making CASTformer a strong
starting point for downstream medical image analysis tasks.",-0.1538009,-0.15354794,0.17151424,C
1036,"Overall, we hope that this model can serve as a solid baseline for 2D medical image segmentation
and motivate further research in medical image analysis tasks.","We conduct extensive analyses to study the robustness of our approach, and
form a more detailed understanding of desirable properties in the medical domain (i.e., transparency
and data efÔ¨Åciency).","It also provides a new perspective on
transfer learning in medical domain, and initially shed novel insights towards understanding neural
network behavior.",2022-01-26 03:50:02+00:00,Class-Aware Adversarial Transformers for Medical Image Segmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Chenyu You'), arxiv.Result.Author('Ruihan Zhao'), arxiv.Result.Author('Fenglin Liu'), arxiv.Result.Author('Siyuan Dong'), arxiv.Result.Author('Sandeep Chinchali'), arxiv.Result.Author('Ufuk Topcu'), arxiv.Result.Author('Lawrence Staib'), arxiv.Result.Author('James S. Duncan')]","Transformers have made remarkable progress towards modeling long-range
dependencies within the medical image analysis domain. However, current
transformer-based models suffer from several disadvantages: (1) existing
methods fail to capture the important features of the images due to the naive
tokenization scheme; (2) the models suffer from information loss because they
only consider single-scale feature representations; and (3) the segmentation
label maps generated by the models are not accurate enough without considering
rich semantic contexts and anatomical textures. In this work, we present
CASTformer, a novel type of adversarial transformers, for 2D medical image
segmentation. First, we take advantage of the pyramid structure to construct
multi-scale representations and handle multi-scale variations. We then design a
novel class-aware transformer module to better learn the discriminative regions
of objects with semantic structures. Lastly, we utilize an adversarial training
strategy that boosts segmentation accuracy and correspondingly allows a
transformer-based discriminator to capture high-level semantically correlated
contents and low-level anatomical features. Our experiments demonstrate that
CASTformer dramatically outperforms previous state-of-the-art transformer-based
approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in
Dice over previous models. Further qualitative experiments provide a more
detailed picture of the model's inner workings, shed light on the challenges in
improved transparency, and demonstrate that transfer learning can greatly
improve performance and reduce the size of medical image datasets in training,
making CASTformer a strong starting point for downstream medical image analysis
tasks.",-0.1538009,-0.15354794,0.17151424,C
1037,"Overall, we hope that this model can serve as a solid baseline for 2D medical image segmentation
and motivate further research in medical image analysis tasks.","We conduct extensive analyses to study the robustness of our approach, and
form a more detailed understanding of desirable properties in the medical domain (i.e., transparency
and data efÔ¨Åciency).","It also provides a new perspective on
transfer learning in medical domain, and initially shed novel insights towards understanding neural
network behavior.",2022-01-26 03:50:02+00:00,Class-Aware Adversarial Transformers for Medical Image Segmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Chenyu You'), arxiv.Result.Author('Ruihan Zhao'), arxiv.Result.Author('Fenglin Liu'), arxiv.Result.Author('Siyuan Dong'), arxiv.Result.Author('Sandeep Chinchali'), arxiv.Result.Author('Ufuk Topcu'), arxiv.Result.Author('Lawrence Staib'), arxiv.Result.Author('James S. Duncan')]","Transformers have made remarkable progress towards modeling long-range
dependencies within the medical image analysis domain. However, current
transformer-based models suffer from several disadvantages: (1) existing
methods fail to capture the important features of the images due to the naive
tokenization scheme; (2) the models suffer from information loss because they
only consider single-scale feature representations; and (3) the segmentation
label maps generated by the models are not accurate enough without considering
rich semantic contexts and anatomical textures. In this work, we present
CASTformer, a novel type of adversarial transformers, for 2D medical image
segmentation. First, we take advantage of the pyramid structure to construct
multi-scale representations and handle multi-scale variations. We then design a
novel class-aware transformer module to better learn the discriminative regions
of objects with semantic structures. Lastly, we utilize an adversarial training
strategy that boosts segmentation accuracy and correspondingly allows a
transformer-based discriminator to capture high-level semantically correlated
contents and low-level anatomical features. Our experiments demonstrate that
CASTformer dramatically outperforms previous state-of-the-art transformer-based
approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in
Dice over previous models. Further qualitative experiments provide a more
detailed picture of the model's inner workings, shed light on the challenges in
improved transparency, and demonstrate that transfer learning can greatly
improve performance and reduce the size of medical image datasets in training,
making CASTformer a strong starting point for downstream medical image analysis
tasks.",-0.1538009,-0.15354794,0.17151424,C
1059,"So it              arXiv:1410.8586, 2014.
could be considered to further research about the interclass
relationships or LDL-related studies by PT-DPC or to exploit           [Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li,
a more large-scale pre-trained model beyond CLIP.","arXiv preprint
results like the second image, which seems acceptable.","Kai Li, and Li Fei-Fei.",2022-01-26 14:31:55+00:00,Learning to Compose Diversified Prompts for Image Emotion Classification,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Sinuo Deng'), arxiv.Result.Author('Lifang Wu'), arxiv.Result.Author('Ge Shi'), arxiv.Result.Author('Lehao Xing'), arxiv.Result.Author('Meng Jian'), arxiv.Result.Author('Ye Xiang')]","Contrastive Language-Image Pre-training (CLIP) represents the latest
incarnation of pre-trained vision-language models. Although CLIP has recently
shown its superior power on a wide range of downstream vision-language tasks
like Visual Question Answering, it is still underexplored for Image Emotion
Classification (IEC). Adapting CLIP to the IEC task has three significant
challenges, tremendous training objective gap between pretraining and IEC,
shared suboptimal and invariant prompts for all instances. In this paper, we
propose a general framework that shows how CLIP can be effectively applied to
IEC. We first introduce a prompt tuning method that mimics the pretraining
objective of CLIP and thus can leverage the rich image and text semantics
entailed in CLIP. Then we automatically compose instance-specific prompts by
conditioning them on the categories and image contents of instances,
diversifying prompts and avoiding suboptimal problems. Evaluations on six
widely-used affective datasets demonstrate that our proposed method outperforms
the state-of-the-art methods to a large margin (i.e., up to 9.29% accuracy gain
on EmotionROI dataset) on IEC tasks, with only a few parameters trained. Our
codes will be publicly available for research purposes.",0.18406534,-0.10159053,0.00094204955,A
1060,"Our results indicate that although some defense
methods work, they fail to do so in a tangible manner, which highlights the necessity
of further research.","Our baseline results, spanning a wide range of corruptions and defenses, show that
the discriminative ZSL models are not robust, primarily due to the severe class imbalance
and model weakness inherent to ZSL.","Our results also show that although these defense methods fail to
work, they set new high accuracies for our ZSL models.",2022-01-26 14:41:10+00:00,How Robust are Discriminatively Trained Zero-Shot Learning Models?,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Mehmet Kerim Yucel'), arxiv.Result.Author('Ramazan Gokberk Cinbis'), arxiv.Result.Author('Pinar Duygulu')]","Data shift robustness has been primarily investigated from a fully supervised
perspective, and robustness of zero-shot learning (ZSL) models have been
largely neglected. In this paper, we present novel analyses on the robustness
of discriminative ZSL to image corruptions. We subject several ZSL models to a
large set of common corruptions and defenses. In order to realize the
corruption analysis, we curate and release the first ZSL corruption robustness
datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account
the dataset characteristics, class imbalance, class transitions between seen
and unseen classes and the discrepancies between ZSL and GZSL performances. Our
results show that discriminative ZSL suffers from corruptions and this trend is
further exacerbated by the severe class imbalance and model weakness inherent
in ZSL methods. We then combine our findings with those based on adversarial
attacks in ZSL, and highlight the different effects of corruptions and
adversarial examples, such as the pseudo-robustness effect present under
adversarial attacks. We also obtain new strong baselines for both models with
the defense methods. Finally, our experiments show that although existing
methods to improve robustness somewhat work for ZSL models, they do not produce
a tangible effect.",0.2268196,-0.17761546,-0.07878165,A
1061,"The presented
results may be improved by further studying them as a function of potential biases in the training data of CLIP itself:
for example, it would be useful to understand in the Ô¨Årst place how CLIP acquired the ability to visually recognize
superimposed words and why we observed a bias favoring the superimposed words over the whole image.",A remaining question is how the CLIP model acquires the biased representations in the image encoder.,"In particular,
we can ask whether the written words are in the training data and if they are, whether there are adversarial examples in
the training data like our word-superimposed images.",2022-01-26 15:46:36+00:00,Evaluating language-biased image classification based on semantic representations,cs.CV,"['cs.CV', 'cs.CL', 'cs.LG']","[arxiv.Result.Author('Yoann Lemesle'), arxiv.Result.Author('Masataka Sawayama'), arxiv.Result.Author('Guillermo Valle-Perez'), arxiv.Result.Author('Maxime Adolphe'), arxiv.Result.Author('H√©l√®ne Sauz√©on'), arxiv.Result.Author('Pierre-Yves Oudeyer')]","Humans show language-biased image recognition for a word-embedded image,
known as picture-word interference. Such interference depends on hierarchical
semantic categories and reflects that human language processing highly
interacts with visual processing. Similar to humans, recent artificial models
jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased
image classification. Exploring whether the bias leads to interferences similar
to those observed in humans can contribute to understanding how much the model
acquires hierarchical semantic representations from joint learning of language
and vision. The present study introduces methodological tools from the
cognitive science literature to assess the biases of artificial models.
Specifically, we introduce a benchmark task to test whether words superimposed
on images can distort the image classification across different category levels
and, if it can, whether the perturbation is due to the shared semantic
representation between language and vision. Our dataset is a set of
word-embedded images and consists of a mixture of natural image datasets and
hierarchical word labels with superordinate/basic category levels. Using this
benchmark test, we evaluate the CLIP model. We show that presenting words
distorts the image classification by the model across different category
levels, but the effect does not depend on the semantic relationship between
images and embedded words. This suggests that the semantic word representation
in the CLIP visual processing is not shared with the image representation,
although the word representation strongly dominates for word-embedded images.",-0.07401294,-0.26530468,0.04328816,C
1062,"The presented results may be improved by further studying them as a function of potential
biases in the training data of CLIP itself: for example, it would be useful to understand in the Ô¨Årst
place how CLIP acquired the ability to visually recognize superimposed words and why we observed
a bias favoring the superimposed words over the whole image.","A remaining question is how the CLIP model acquires the biased representations in the image en-
coder.","In particular, we can ask whether the
written words are in the training data and if they are, whether there are adversarial examples in the
training data like our word-superimposed images.",2022-01-26 15:46:36+00:00,Language-biased image classification: evaluation based on semantic representations,cs.CV,"['cs.CV', 'cs.CL', 'cs.LG']","[arxiv.Result.Author('Yoann Lemesle'), arxiv.Result.Author('Masataka Sawayama'), arxiv.Result.Author('Guillermo Valle-Perez'), arxiv.Result.Author('Maxime Adolphe'), arxiv.Result.Author('H√©l√®ne Sauz√©on'), arxiv.Result.Author('Pierre-Yves Oudeyer')]","Humans show language-biased image recognition for a word-embedded image,
known as picture-word interference. Such interference depends on hierarchical
semantic categories and reflects that human language processing highly
interacts with visual processing. Similar to humans, recent artificial models
jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased
image classification. Exploring whether the bias leads to interference similar
to those observed in humans can contribute to understanding how much the model
acquires hierarchical semantic representations from joint learning of language
and vision. The present study introduces methodological tools from the
cognitive science literature to assess the biases of artificial models.
Specifically, we introduce a benchmark task to test whether words superimposed
on images can distort the image classification across different category levels
and, if it can, whether the perturbation is due to the shared semantic
representation between language and vision. Our dataset is a set of
word-embedded images and consists of a mixture of natural image datasets and
hierarchical word labels with superordinate/basic category levels. Using this
benchmark test, we evaluate the CLIP model. We show that presenting words
distorts the image classification by the model across different category
levels, but the effect does not depend on the semantic relationship between
images and embedded words. This suggests that the semantic word representation
in the CLIP visual processing is not shared with the image representation,
although the word representation strongly dominates for word-embedded images.",-0.0663154,-0.25078467,0.034678172,C
1091,"Other
cornerstone for further research, ICP is extremely sensitive      methods as (Huang et al.",While being a      pute the transformation only between the subsets.,"2021; Dang, Wang, and Salzmann
to noise, outliers, and initial conditions, and thus prone to     2020) offered attention mechanisms that inspect both clouds
reach a local-minima.",2022-01-27 08:40:05+00:00,Deep Confidence Guided Distance for 3D Partial Shape Registration,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dvir Ginzburg'), arxiv.Result.Author('Dan Raviv')]","We present a novel non-iterative learnable method for partial-to-partial 3D
shape registration. The partial alignment task is extremely complex, as it
jointly tries to match between points and identify which points do not appear
in the corresponding shape, causing the solution to be non-unique and ill-posed
in most cases.
  Until now, two principal methodologies have been suggested to solve this
problem: sample a subset of points that are likely to have correspondences or
perform soft alignment between the point clouds and try to avoid a match to an
occluded part. These heuristics work when the partiality is mild or when the
transformation is small but fails for severe occlusions or when outliers are
present. We present a unique approach named Confidence Guided Distance Network
(CGD-net), where we fuse learnable similarity between point embeddings and
spatial distance between point clouds, inducing an optimized solution for the
overlapping points while ignoring parts that only appear in one of the shapes.
The point feature generation is done by a self-supervised architecture that
repels far points to have different embeddings, therefore succeeds to align
partial views of shapes, even with excessive internal symmetries or acute
rotations. We compare our network to recently presented learning-based and
axiomatic methods and report a fundamental boost in performance.",0.17771012,0.19016528,0.08091695,A
1093,"When only using
improve the quality of the generated extrapolation    Swin Transformer blocks, the extended image borders
images.We further study the eÔ¨Äectiveness of our de-   have plausible structure but obvious blunt colours in
signs from qualitative results, with some examples    column (c).","We      backbone in the variant of CNN with SC&TSP suÔ¨Äers
could see that removing any design of our full model  from blur and has an obvious boundary between the
leads to worse performance, proving that our de-      input and predicted region due to long-range depen-
signs could boost the proposed model learning and     dencies not being well extracted.","When further utilizing the SC, more re-
shown in Fig.",2022-01-27 09:41:58+00:00,Generalised Image Outpainting with U-Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Penglei Gao'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Kaizhu Huang'), arxiv.Result.Author('Yujie Geng')]","While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.",-0.18292019,-0.05293854,0.29456672,C
1094,"When only using
could boost the proposed model learning and im-        Swin Transformer blocks, the extended image borders
prove the quality of the generated extrapolation im-   have plausible structure but obvious blunt colours in
ages.We further study the eÔ¨Äectiveness of our designs  column (c).","We       from blur and has an obvious boundary between the
could see that removing any design of our full model   input and predicted region due to long-range depen-
leads to worse performance, proving that our designs   dencies not being well extracted.","When further utilizing the SC, more re-
from qualitative results, with some examples shown     alistic and smooth colours are generated in the ex-
in Fig.",2022-01-27 09:41:58+00:00,Generalised Image Outpainting with U-Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Penglei Gao'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Kaizhu Huang'), arxiv.Result.Author('Yujie Geng')]","While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.",-0.04308123,0.05763875,0.25097793,C
1095,"4.4 Ablation Study                                    leads to worse performance, proving that our designs
                                                      could boost the proposed model learning and im-
  w/o Trans   FID‚Üì     IS‚Üë   PSNR‚Üë   SSIM‚Üë            prove the quality of the generated extrapolation im-
w/o SC&TSP    37.934  2.991  22.658  0.743            ages.We further study the eÔ¨Äectiveness of our designs
              29.390  3.092  23.333  0.768            from qualitative results, with some examples shown
    w/o SC    92.038  2.345  20.895  0.554            in Fig.","The edges be-

                                                          9
                                                                                               Transformer w/o SC & TSP

                                                                                               Transformer w/o SC

                                                                                               Full model

                                                                                                Transformer Transformer
                                                                                               w/o SC-& TSP w/o SC

Test          (a)     (b)            (c)              (d)          (e)          (f)

Input         GT      CNN    Transformer              Transformer  Transformer                 Full model

                      with SC & TSP w/o SC & TSP w/o TSP           w/o SC       U-Transformer

                   Figure 7: Visual results of ablation study on Scenery dataset.",7.,2022-01-27 09:41:58+00:00,Generalised Image Outpainting with U-Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Penglei Gao'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Kaizhu Huang'), arxiv.Result.Author('Yujie Geng'), arxiv.Result.Author('Yuyao Yan')]","While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.",0.25002956,0.0642822,0.15679127,A
1096,"We further study the effectiveness
the trees, rocks, and rivers predicted in column (e) of Fig.",ated extrapolation images.,"5.            of our designs from qualitative results, with some examples
The Ô¨Åne-art paintings are more complicated, having a broader               shown in Fig.",2022-01-27 09:41:58+00:00,Generalised Image Outpainting with U-Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Penglei Gao'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Kaizhu Huang'), arxiv.Result.Author('John Y. Goulermas'), arxiv.Result.Author('Yujie Geng'), arxiv.Result.Author('Yuyao Yan')]","While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.",0.01596596,0.18350299,-0.118541375,B
1097,"We
further study the eÔ¨Äectiveness of our designs from qualitative results, with
some examples shown in Fig.","We could see that removing any design of our full model leads to
worse performance, proving that our designs could boost the proposed model
learning and improve the quality of the generated extrapolation images.",8.,2022-01-27 09:41:58+00:00,Generalised Image Outpainting with U-Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Penglei Gao'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('John Y. Goulermas'), arxiv.Result.Author('Yujie Geng'), arxiv.Result.Author('Yuyao Yan'), arxiv.Result.Author('Kaizhu Huang')]","In this paper, we develop a novel transformer-based generative adversarial
neural network called U-Transformer for generalised image outpainting problem.
Different from most present image outpainting methods conducting horizontal
extrapolation, our generalised image outpainting could extrapolate visual
context all-side around a given image with plausible structure and details even
for complicated scenery, building, and art images. Specifically, we design a
generator as an encoder-to-decoder structure embedded with the popular Swin
Transformer blocks. As such, our novel neural network can better cope with
image long-range dependencies which are crucially important for generalised
image outpainting. We propose additionally a U-shaped structure and multi-view
Temporal Spatial Predictor (TSP) module to reinforce image self-reconstruction
as well as unknown-part prediction smoothly and realistically. By adjusting the
predicting step in the TSP module in the testing stage, we can generate
arbitrary outpainting size given the input sub-image. We experimentally
demonstrate that our proposed method could produce visually appealing results
for generalized image outpainting against the state-of-the-art image
outpainting approaches.",0.0986879,-0.08362835,0.048092224,C
1098,"Since Ô¨Çow
    Intermediate Ô¨Çow visualizations: We visualize the backward         and occlusion estimates from PWCNet-Bi-Occ are often not accu-
Ô¨Çow Ftr‚Üí0 estimated by QVI [50] and our approach in Figure 9.          rate and hence can create a performance bottleneck in interpola-
We notice that erroneous results in QVI‚Äôs [50] interpolated frame      tion task, further research can explore whether inclusion of RGB
is caused by incorrect estimation of the backward Ô¨Çow.","Our method
                                                                       achieves state-of-the-art results in multiple datasets.","However,        frames as input to 3D CNN can improve the performance.",2022-01-27 09:49:23+00:00,Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Saikat Dutta'), arxiv.Result.Author('Arulkumar Subramaniam'), arxiv.Result.Author('Anurag Mittal')]","Video frame interpolation aims to synthesize one or multiple frames between
two consecutive frames in a video. It has a wide range of applications
including slow-motion video generation, frame-rate up-scaling and developing
video codecs. Some older works tackled this problem by assuming per-pixel
linear motion between video frames. However, objects often follow a non-linear
motion pattern in the real domain and some recent methods attempt to model
per-pixel motion by non-linear models (e.g., quadratic). A quadratic model can
also be inaccurate, especially in the case of motion discontinuities over time
(i.e. sudden jerks) and occlusions, where some of the flow information may be
invalid or inaccurate.
  In our paper, we propose to approximate the per-pixel motion using a
space-time convolution network that is able to adaptively select the motion
model to be used. Specifically, we are able to softly switch between a linear
and a quadratic model. Towards this end, we use an end-to-end 3D CNN
encoder-decoder architecture over bidirectional optical flows and occlusion
maps to estimate the non-linear motion model of each pixel. Further, a motion
refinement module is employed to refine the non-linear motion and the
interpolated frames are estimated by a simple warping of the neighboring frames
with the estimated per-pixel motion. Through a set of comprehensive
experiments, we validate the effectiveness of our model and show that our
method outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS,
HD and GoPro).",-0.23043111,0.13646278,0.23608482,B
1099,"Since Ô¨Çow
    Intermediate Ô¨Çow visualizations: We visualize the backward         and occlusion estimates from PWCNet-Bi-Occ are often not accu-
Ô¨Çow Ftr‚Üí0 estimated by QVI [50] and our approach in Figure 9.          rate and hence can create a performance bottleneck in interpola-
We notice that erroneous results in QVI‚Äôs [50] interpolated frame      tion task, further research can explore whether inclusion of RGB
is caused by incorrect estimation of the backward Ô¨Çow.","Our method
                                                                       achieves state-of-the-art results in multiple datasets.","However,        frames as input to 3D CNN can improve the performance.",2022-01-27 09:49:23+00:00,Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Saikat Dutta'), arxiv.Result.Author('Arulkumar Subramaniam'), arxiv.Result.Author('Anurag Mittal')]","Video frame interpolation aims to synthesize one or multiple frames between
two consecutive frames in a video. It has a wide range of applications
including slow-motion video generation, frame-rate up-scaling and developing
video codecs. Some older works tackled this problem by assuming per-pixel
linear motion between video frames. However, objects often follow a non-linear
motion pattern in the real domain and some recent methods attempt to model
per-pixel motion by non-linear models (e.g., quadratic). A quadratic model can
also be inaccurate, especially in the case of motion discontinuities over time
(i.e. sudden jerks) and occlusions, where some of the flow information may be
invalid or inaccurate.
  In our paper, we propose to approximate the per-pixel motion using a
space-time convolution network that is able to adaptively select the motion
model to be used. Specifically, we are able to softly switch between a linear
and a quadratic model. Towards this end, we use an end-to-end 3D CNN
encoder-decoder architecture over bidirectional optical flows and occlusion
maps to estimate the non-linear motion model of each pixel. Further, a motion
refinement module is employed to refine the non-linear motion and the
interpolated frames are estimated by a simple warping of the neighboring frames
with the estimated per-pixel motion. Through a set of comprehensive
experiments, we validate the effectiveness of our model and show that our
method outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS,
HD and GoPro).",-0.23043111,0.13646278,0.23608482,B
1101,"Furthermore, the analysis and applicability of popular explainable artificial intelligence techniques
for ensemble learning based medical image classification pipelines with multiple models is still an open research
field and requires further research.","As future research, we plan to further analyze the impact of the
number of folds in cross-validation based Bagging techniques and extend our analysis on deep learning Boosting
approaches.","Acknowledgments
We want to thank Edmund M√ºller, Dennis Hartmann, Philip Meyer, and Peter Parys for their useful comments and
support.",2022-01-27 10:56:11+00:00,An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dominik M√ºller'), arxiv.Result.Author('I√±aki Soto-Rey'), arxiv.Result.Author('Frank Kramer')]","Novel and high-performance medical image classification pipelines are heavily
utilizing ensemble learning strategies. The idea of ensemble learning is to
assemble diverse models or multiple predictions and, thus, boost prediction
performance. However, it is still an open question to what extent as well as
which ensemble learning strategies are beneficial in deep learning based
medical image classification pipelines. In this work, we proposed a
reproducible medical image classification pipeline for analyzing the
performance impact of the following ensemble learning techniques: Augmenting,
Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing
and image augmentation methods as well as 9 deep convolution neural network
architectures. It was applied on four popular medical imaging datasets with
varying complexity. Furthermore, 12 pooling functions for combining multiple
predictions were analyzed, ranging from simple statistical functions like
unweighted averaging up to more complex learning-based functions like support
vector machines. Our results revealed that Stacking achieved the largest
performance gain of up to 13% F1-score increase. Augmenting showed consistent
improvement capabilities by up to 4% and is also applicable to single model
based pipelines. Cross-validation based Bagging demonstrated to be the most
complex ensemble learning method, which resulted in an F1-score decrease in all
analyzed datasets (up to -10%). Furthermore, we demonstrated that simple
statistical pooling functions are equal or often even better than more complex
pooling functions. We concluded that the integration of Stacking and
Augmentation ensemble learning techniques is a powerful method for any medical
image classification pipeline to improve robustness and boost performance.",-0.059371214,-0.18887368,0.09636903,C
1102,"Still, further research is needed on the
impact of fold number or sampling size on performance and model generalizability in deep learning based MIC.","For this reason, we specified our analysis on a 5-fold cross-validation.","Nevertheless, we concluded that Bagging is a powerful but complex to utilize ensemble learning technique and
that its effectiveness is highly depended on sufficient feature representation in the sampled cross-validation folds.",2022-01-27 10:56:11+00:00,An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dominik M√ºller'), arxiv.Result.Author('I√±aki Soto-Rey'), arxiv.Result.Author('Frank Kramer')]","Novel and high-performance medical image classification pipelines are heavily
utilizing ensemble learning strategies. The idea of ensemble learning is to
assemble diverse models or multiple predictions and, thus, boost prediction
performance. However, it is still an open question to what extent as well as
which ensemble learning strategies are beneficial in deep learning based
medical image classification pipelines. In this work, we proposed a
reproducible medical image classification pipeline for analyzing the
performance impact of the following ensemble learning techniques: Augmenting,
Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing
and image augmentation methods as well as 9 deep convolution neural network
architectures. It was applied on four popular medical imaging datasets with
varying complexity. Furthermore, 12 pooling functions for combining multiple
predictions were analyzed, ranging from simple statistical functions like
unweighted averaging up to more complex learning-based functions like support
vector machines. Our results revealed that Stacking achieved the largest
performance gain of up to 13% F1-score increase. Augmenting showed consistent
improvement capabilities by up to 4% and is also applicable to single model
based pipelines. Cross-validation based Bagging demonstrated significant
performance gain close to Stacking, which resulted in an F1-score increase up
to +11%. Furthermore, we demonstrated that simple statistical pooling functions
are equal or often even better than more complex pooling functions. We
concluded that the integration of ensemble learning techniques is a powerful
method for any medical image classification pipeline to improve robustness and
boost performance.",0.033874616,-0.19889933,0.13236329,C
1103,"based medical image classification pipelines with multiple models is still an open research field and requires
further research.","Furthermore, the applicability of explainable artificial intelligence techniques for ensemble learning

Preprint - April 2022  Page 12 / 16
  An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks - M√ºller et al.","Acknowledgments
We want to thank Edmund M√ºller, Dennis Hartmann, Philip Meyer, and Peter Parys for their useful comments and
support.",2022-01-27 10:56:11+00:00,An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dominik M√ºller'), arxiv.Result.Author('I√±aki Soto-Rey'), arxiv.Result.Author('Frank Kramer')]","Novel and high-performance medical image classification pipelines are heavily
utilizing ensemble learning strategies. The idea of ensemble learning is to
assemble diverse models or multiple predictions and, thus, boost prediction
performance. However, it is still an open question to what extent as well as
which ensemble learning strategies are beneficial in deep learning based
medical image classification pipelines. In this work, we proposed a
reproducible medical image classification pipeline for analyzing the
performance impact of the following ensemble learning techniques: Augmenting,
Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing
and image augmentation methods as well as 9 deep convolution neural network
architectures. It was applied on four popular medical imaging datasets with
varying complexity. Furthermore, 12 pooling functions for combining multiple
predictions were analyzed, ranging from simple statistical functions like
unweighted averaging up to more complex learning-based functions like support
vector machines. Our results revealed that Stacking achieved the largest
performance gain of up to 13% F1-score increase. Augmenting showed consistent
improvement capabilities by up to 4% and is also applicable to single model
based pipelines. Cross-validation based Bagging demonstrated significant
performance gain close to Stacking, which resulted in an F1-score increase up
to +11%. Furthermore, we demonstrated that simple statistical pooling functions
are equal or often even better than more complex pooling functions. We
concluded that the integration of ensemble learning techniques is a powerful
method for any medical image classification pipeline to improve robustness and
boost performance.",-0.100894414,-0.18164843,0.11764668,C
1146,"However, CKD does not obtain satisfactory results in

                                                                                                                                  the further study because of the insufÔ¨Åcient use of supervised

                                                                                                                                  information.","According to Eq.1, the objective formulation can                                                                    work, and have achieved certain improvement on subspace
be signiÔ¨Åed as:
                                                                                                                                  learning.","To overcome this shortcoming and improve the

                                                                                                                                  discrimination of the learned representation for cross-modal

                                                                                                                                  retrieval, the problem can be transformed to the following

                                                                                                                                  function:             min ùúÉ ùëåùëåùëá ‚àí ùëã1ùëÉ1 ( ùëã2ùëÉ2)ùëá 2ùêπ

                                                                                                                                                     ùëÉ1, ùëÉ2                                                                (6)

max tr(ùêªùêæùëã1 ùêªùêæùëã2 ) + tr(ùêªùêæùëã1 ùêªùêæùëå ) + tr(ùêªùêæùëã2 ùêªùêæùëå )                                                                                                   s.t.",2022-01-26 14:27:39+00:00,Discriminative Supervised Subspace Learning for Cross-modal Retrieval,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Haoming Zhang'), arxiv.Result.Author('Xiao-Jun Wu'), arxiv.Result.Author('Tianyang Xu'), arxiv.Result.Author('Donglin Zhang')]","Nowadays the measure between heterogeneous data is still an open problem for
cross-modal retrieval. The core of cross-modal retrieval is how to measure the
similarity between different types of data. Many approaches have been developed
to solve the problem. As one of the mainstream, approaches based on subspace
learning pay attention to learning a common subspace where the similarity among
multi-modal data can be measured directly. However, many of the existing
approaches only focus on learning a latent subspace. They ignore the full use
of discriminative information so that the semantically structural information
is not well preserved. Therefore satisfactory results can not be achieved as
expected. We in this paper propose a discriminative supervised subspace
learning for cross-modal retrieval(DS2L), to make full use of discriminative
information and better preserve the semantically structural information.
Specifically, we first construct a shared semantic graph to preserve the
semantic structure within each modality. Subsequently, the Hilbert-Schmidt
Independence Criterion(HSIC) is introduced to preserve the consistence between
feature-similarity and semantic-similarity of samples. Thirdly, we introduce a
similarity preservation term, thus our model can compensate for the
shortcomings of insufficient use of discriminative data and better preserve the
semantically structural information within each modality. The experimental
results obtained on three well-known benchmark datasets demonstrate the
effectiveness and competitiveness of the proposed method against the compared
classic subspace learning approaches.",0.09884733,-0.20349668,-0.07736714,C
1163,"In MLP-Mixer,      state-of-the-art visual recognition performances at that time
each layer mainly relies on two steps to perform informa-      and inspired further research, including ResNeXt (Xie
tion interaction: token mixing step and channel mixing step,   et al., 2017) and DenseNet (Huang et al., 2017).","They achieved
moving the need of invoking self-attention.","Afterward,
both based on MLP blocks.",2022-01-28 12:43:14+00:00,DynaMixer: A Vision MLP Architecture with Dynamic Mixing,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Ziyu Wang'), arxiv.Result.Author('Wenhao Jiang'), arxiv.Result.Author('Yiming Zhu'), arxiv.Result.Author('Li Yuan'), arxiv.Result.Author('Yibing Song'), arxiv.Result.Author('Wei Liu')]","Recently, MLP-like vision models have achieved promising performances on
mainstream visual recognition tasks. In contrast with vision transformers and
CNNs, the success of MLP-like models shows that simple information fusion
operations among tokens and channels can yield a good representation power for
deep recognition models. However, existing MLP-like models fuse tokens through
static fusion operations, lacking adaptability to the contents of the tokens to
be mixed. Thus, customary information fusion procedures are not effective
enough. To this end, this paper presents an efficient MLP-like network
architecture, dubbed DynaMixer, resorting to dynamic information fusion.
Critically, we propose a procedure, on which the DynaMixer model relies, to
dynamically generate mixing matrices by leveraging the contents of all the
tokens to be mixed. To reduce the time complexity and improve the robustness, a
dimensionality reduction technique and a multi-segment fusion mechanism are
adopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\% top-1
accuracy on the ImageNet-1K dataset without extra training data, performing
favorably against the state-of-the-art vision MLP models. When the number of
parameters is reduced to 26M, it still achieves 82.7\% top-1 accuracy,
surpassing the existing MLP-like models with a similar capacity. The
implementation of DynaMixer will be made available to the public.",-0.06252981,-0.10085924,0.09538333,C
1164,"In MLP-Mixer,      state-of-the-art visual recognition performances at that time
each layer mainly relies on two steps to perform informa-      and inspired further research, including ResNeXt (Xie
tion interaction: token mixing step and channel mixing step,   et al., 2017) and DenseNet (Huang et al., 2017).","They achieved
moving the need of invoking self-attention.","Afterward,
both based on MLP blocks.",2022-01-28 12:43:14+00:00,DynaMixer: A Vision MLP Architecture with Dynamic Mixing,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Ziyu Wang'), arxiv.Result.Author('Wenhao Jiang'), arxiv.Result.Author('Yiming Zhu'), arxiv.Result.Author('Li Yuan'), arxiv.Result.Author('Yibing Song'), arxiv.Result.Author('Wei Liu')]","Recently, MLP-like vision models have achieved promising performances on
mainstream visual recognition tasks. In contrast with vision transformers and
CNNs, the success of MLP-like models shows that simple information fusion
operations among tokens and channels can yield a good representation power for
deep recognition models. However, existing MLP-like models fuse tokens through
static fusion operations, lacking adaptability to the contents of the tokens to
be mixed. Thus, customary information fusion procedures are not effective
enough. To this end, this paper presents an efficient MLP-like network
architecture, dubbed DynaMixer, resorting to dynamic information fusion.
Critically, we propose a procedure, on which the DynaMixer model relies, to
dynamically generate mixing matrices by leveraging the contents of all the
tokens to be mixed. To reduce the time complexity and improve the robustness, a
dimensionality reduction technique and a multi-segment fusion mechanism are
adopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\% top-1
accuracy on the ImageNet-1K dataset without extra training data, performing
favorably against the state-of-the-art vision MLP models. When the number of
parameters is reduced to 26M, it still achieves 82.7\% top-1 accuracy,
surpassing the existing MLP-like models with a similar capacity. The code is
available at \url{https://github.com/ziyuwwang/DynaMixer}.",-0.06252981,-0.10085924,0.09538333,C
1165,"                                        Psychophysical Evaluation of Human Performance
                                          in Detecting Digital Face Image Manipulations

                                                                             R. Nichols, C. Rathgeb, P. Drozdowski, C. Busch
                                                                              da/sec ‚Äì Biometrics and Internet Security Research Group

                                                                                              Hochschule Darmstadt, Germany
                                                               {robert.nichols,christian.rathgeb,christoph.busch}@h-da.de

arXiv:2201.12084v1 [cs.CV] 28 Jan 2022     Abstract‚ÄîIn recent years, increasing deployment of face recog-      In light of the potential security implications, face image
                                        nition technology in security-critical settings, such as border     manipulations present a serious threat in these operational
                                        control or law enforcement, has led to considerable interest in     contexts, thus warranting further research to better understand
                                        the vulnerability of face recognition systems to attacks utilising  the process of detecting manipulated face images and human
                                        legitimate documents, which are issued on the basis of digitally    ability to do so.","105‚Äì
      123, 2016.","To contribute to a better understanding of
                                        manipulated face images.",2022-01-28 12:45:33+00:00,Psychophysical Evaluation of Human Performance in Detecting Digital Face Image Manipulations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Robert Nichols'), arxiv.Result.Author('Christian Rathgeb'), arxiv.Result.Author('Pawel Drozdowski'), arxiv.Result.Author('Christoph Busch')]","In recent years, increasing deployment of face recognition technology in
security-critical settings, such as border control or law enforcement, has led
to considerable interest in the vulnerability of face recognition systems to
attacks utilising legitimate documents, which are issued on the basis of
digitally manipulated face images. As automated manipulation and attack
detection remains a challenging task, conventional processes with human
inspectors performing identity verification remain indispensable. These
circumstances merit a closer investigation of human capabilities in detecting
manipulated face images, as previous work in this field is sparse and often
concentrated only on specific scenarios and biometric characteristics.
  This work introduces a web-based, remote visual discrimination experiment on
the basis of principles adopted from the field of psychophysics and
subsequently discusses interdisciplinary opportunities with the aim of
examining human proficiency in detecting different types of digitally
manipulated face images, specifically face swapping, morphing, and retouching.
In addition to analysing appropriate performance measures, a possible metric of
detectability is explored. Experimental data of 306 probands indicate that
detection performance is widely distributed across the population and detection
of certain types of face image manipulations is much more challenging than
others.",-0.06465426,0.058574997,-0.13306758,C
1166,"This experiment adds to a growing corpus of                      [12] B. Duchaine and K. Nakayama, ‚ÄúThe cambridge face memory test:
research showing that human detection ability in this regard
is limited; however, some individuals display extraordinary                      Results for neurologically intact individuals and an investigation of
performance, and further study of these high-performing in-
dividuals is desirable for future work.","433‚Äì445, 2015.
nipulations and the related topic concerning face comparison
and perception.","Additionally, it will be                 its validity using inverted face stimuli and prosopagnosic participants,‚Äù
important that future research further investigate the adequacy
of different test methodologies and evaluation protocols in                      Neuropsychologia, vol.",2022-01-28 12:45:33+00:00,Psychophysical Evaluation of Human Performance in Detecting Digital Face Image Manipulations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Robert Nichols'), arxiv.Result.Author('Christian Rathgeb'), arxiv.Result.Author('Pawel Drozdowski'), arxiv.Result.Author('Christoph Busch')]","In recent years, increasing deployment of face recognition technology in
security-critical settings, such as border control or law enforcement, has led
to considerable interest in the vulnerability of face recognition systems to
attacks utilising legitimate documents, which are issued on the basis of
digitally manipulated face images. As automated manipulation and attack
detection remains a challenging task, conventional processes with human
inspectors performing identity verification remain indispensable. These
circumstances merit a closer investigation of human capabilities in detecting
manipulated face images, as previous work in this field is sparse and often
concentrated only on specific scenarios and biometric characteristics.
  This work introduces a web-based, remote visual discrimination experiment on
the basis of principles adopted from the field of psychophysics and
subsequently discusses interdisciplinary opportunities with the aim of
examining human proficiency in detecting different types of digitally
manipulated face images, specifically face swapping, morphing, and retouching.
In addition to analysing appropriate performance measures, a possible metric of
detectability is explored. Experimental data of 306 probands indicate that
detection performance is widely distributed across the population and detection
of certain types of face image manipulations is much more challenging than
others.",0.122599766,-0.026537009,-0.23725015,A
1203,"In order to further study how the number of feedback
                                                                                                              users aÔ¨Äect the FoV prediction, diÔ¨Äerent number of
                                                                                                              feedback users are selected and the prediction interval is
                                                                                                              set to Œ∫0 = 0.03 on the Nvidia Titan 2060i GPU for pre-
                                                                                                        11

                                                           TABLE II
                                 The impact of the number of users sending FoV.","Furthermore, Viewport outper-
                                                                                                              forms even DeepVS and SalGAN360 as the number of
                                                                                                              user FoVs increases.","The number of users      Accuracy               Latency(s)  Computational requirement (GFLOPs)
                     2             0.8463                 0.2732                         5.2817
                     5             0.8589                 0.3101                         7.3298
                     8             0.8709                 0.3428                         9.3779
                    11             0.8941                 0.3874                         11.426

diction.",2022-01-29 08:32:19+00:00,Spherical Convolution empowered FoV Prediction in 360-degree Video Multicast with Limited FoV Feedback,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Jie Li'), arxiv.Result.Author('Ling Han'), arxiv.Result.Author('Cong Zhang'), arxiv.Result.Author('Qiyue Li'), arxiv.Result.Author('Zhi Liu')]","Field of view (FoV) prediction is critical in 360-degree video multicast,
which is a key component of the emerging Virtual Reality (VR) and Augmented
Reality (AR) applications. Most of the current prediction methods combining
saliency detection and FoV information neither take into account that the
distortion of projected 360-degree videos can invalidate the weight sharing of
traditional convolutional networks, nor do they adequately consider the
difficulty of obtaining complete multi-user FoV information, which degrades the
prediction performance. This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction framework combining
salient features extracted from 360-degree video with limited FoV feedback
information. A spherical convolution neural network (CNN) is used instead of a
traditional two-dimensional CNN to eliminate the problem of weight sharing
failure caused by video projection distortion. Specifically, salient
spatial-temporal features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback FoV information is
represented as a time-series model based on a spherical convolution-empowered
gated recurrent unit network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental results show that the
performance of the proposed method is better than other prediction methods.",0.2164027,0.13541526,0.102534756,A
1218,"Finally, in Section 6 we ana-
lyze the study‚Äôs Ô¨Åndings and provide some suggestions for further research.","Section 4, exhibits our approach, and Ô¨Ånally,
in Section 5 we present experimental results obtained with our approach on
Cityscapes and PASCAL VOC 2012 benchmarks.","2
2 Related Works

2.1 Semantic segmentation

Semantic segmentation consists in classifying each pixel of an image into a class,
where each class represents an object or a portion of the image [36].",2022-01-29 19:49:44+00:00,Self Semi Supervised Neural Architecture Search for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Lo√Øc Pauletto'), arxiv.Result.Author('Massih-Reza Amini'), arxiv.Result.Author('Nicolas Winckler')]","In this paper, we propose a Neural Architecture Search strategy based on self
supervision and semi-supervised learning for the task of semantic segmentation.
Our approach builds an optimized neural network (NN) model for this task by
jointly solving a jigsaw pretext task discovered with self-supervised learning
over unlabeled training data, and, exploiting the structure of the unlabeled
data with semi-supervised learning. The search of the architecture of the NN
model is performed by dynamic routing using a gradient descent algorithm.
Experiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the
discovered neural network is more efficient than a state-of-the-art
hand-crafted NN model with four times less floating operations.",-0.22563277,-0.043352786,-0.044681422,B
1219,"Finally, in Section 6 we ana-
lyze the study‚Äôs Ô¨Åndings and provide some suggestions for further research.","Section 4, exhibits our approach, and Ô¨Ånally,
in Section 5 we present experimental results obtained with our approach on
Cityscapes and PASCAL VOC 2012 benchmarks.","2
2 Related Works

2.1 Semantic segmentation

Semantic segmentation consists in classifying each pixel of an image into a class,
where each class represents an object or a portion of the image [36].",2022-01-29 19:49:44+00:00,Self Semi Supervised Neural Architecture Search for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Lo√Øc Pauletto'), arxiv.Result.Author('Massih-Reza Amini'), arxiv.Result.Author('Nicolas Winckler')]","In this paper, we propose a Neural Architecture Search strategy based on self
supervision and semi-supervised learning for the task of semantic segmentation.
Our approach builds an optimized neural network (NN) model for this task by
jointly solving a jigsaw pretext task discovered with self-supervised learning
over unlabeled training data, and, exploiting the structure of the unlabeled
data with semi-supervised learning. The search of the architecture of the NN
model is performed by dynamic routing using a gradient descent algorithm.
Experiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the
discovered neural network is more efficient than a state-of-the-art
hand-crafted NN model with four times less floating operations.",-0.22563277,-0.043352786,-0.044681422,B
1220,"indicate the need for further research in the optimal
Furthermore, our application is capable of storing every          implementation of deep learning for facial emotion
image uploaded or captured in order to compile an extensive       recognition.","Other models, such as VGGNet variant
utilizing our framework to test alternative FER models            architectures, have achieved higher accuracy rates and thus
proposed in the literature has not been exercised yet.","FER dataset similar to AffectNet, yet due to its relative
infancy, this functionality has not been fully realized.",2022-01-30 02:10:01+00:00,A Robust Framework for Deep Learning Approaches to Facial Emotion Recognition and Evaluation,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Nyle Siddiqui'), arxiv.Result.Author('Rushit Dave'), arxiv.Result.Author('Tyler Bauer'), arxiv.Result.Author('Thomas Reither'), arxiv.Result.Author('Dylan Black'), arxiv.Result.Author('Mitchell Hanson')]","Facial emotion recognition is a vast and complex problem space within the
domain of computer vision and thus requires a universally accepted baseline
method with which to evaluate proposed models. While test datasets have served
this purpose in the academic sphere real world application and testing of such
models lacks any real comparison. Therefore we propose a framework in which
models developed for FER can be compared and contrasted against one another in
a constant standardized fashion. A lightweight convolutional neural network is
trained on the AffectNet dataset a large variable dataset for facial emotion
recognition and a web application is developed and deployed with our proposed
framework as a proof of concept. The CNN is embedded into our application and
is capable of instant real time facial emotion recognition. When tested on the
AffectNet test set this model achieves high accuracy for emotion classification
of eight different emotions. Using our framework the validity of this model and
others can be properly tested by evaluating a model efficacy not only based on
its accuracy on a sample test dataset, but also on in the wild experiments.
Additionally, our application is built with the ability to save and store any
image captured or uploaded to it for emotion recognition, allowing for the
curation of more quality and diverse facial emotion recognition datasets.",-0.12564602,-0.2748952,0.13038835,C
1250,"With multiple useful applications of medical visual answer localization in
healthcare and consumer health education, we believe that the MedVidQA dataset and benchmarked setup can play a
valuable role for further research in this area.","The results show that the visual
answer localization is a challenging task, where the model should have the capability of inter-modal communication
to locate the relevant frames in the videos.","6 MedVidCL Benchmarking

We benchmarked our MedVidCL dataset with multiple monomodal and multimodal approaches.",2022-01-30 18:06:31+00:00,A Dataset for Medical Instructional Video Classification and Question Answering,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Deepak Gupta'), arxiv.Result.Author('Kush Attal'), arxiv.Result.Author('Dina Demner-Fushman')]","This paper introduces a new challenge and datasets to foster research toward
designing systems that can understand medical videos and provide visual answers
to natural language questions. We believe medical videos may provide the best
possible answers to many first aids, medical emergency, and medical education
questions. Toward this, we created the MedVidCL and MedVidQA datasets and
introduce the tasks of Medical Video Classification (MVC) and Medical Visual
Answer Localization (MVAL), two tasks that focus on cross-modal (medical
language and medical video) understanding. The proposed tasks and datasets have
the potential to support the development of sophisticated downstream
applications that can benefit the public and medical practitioners. Our
datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated
questions and answers timestamps from 899 videos for the MVAL task. These
datasets have been verified and corrected by medical informatics experts. We
have also benchmarked each task with the created MedVidCL and MedVidQA datasets
and proposed the multimodal learning methods that set competitive baselines for
future research.",-0.21395978,-0.09063919,-0.23145232,C
1261,"Yet the
beneÔ¨Åts and advantages of convolutional-based methods such as an easier training process (discussed more thoroughly
in ""Convolutional-based methods"" (section 2.7)) encourage further research in this Ô¨Åeld.","This shows that convolutional-based methods still
need much more improvement to reach the performance of the other methods such as the attention-based ones.","3.5 Comparing Independent Results

Many research works have reported their results independently as well as the results reported by the Microsoft COCO
servers.",2022-01-31 00:39:37+00:00,Deep Learning Approaches on Image Captioning: A Review,cs.CV,"['cs.CV', 'I.2.7; I.4']","[arxiv.Result.Author('Taraneh Ghandi'), arxiv.Result.Author('Hamidreza Pourreza'), arxiv.Result.Author('Hamidreza Mahyar')]","Automatic image captioning, which involves describing the contents of an
image, is a challenging problem with many applications in various research
fields. One notable example is designing assistants for the visually impaired.
Recently, there have been significant advances in image captioning methods
owing to the breakthroughs in deep learning. This survey paper aims to provide
a structured review of recent image captioning techniques, and their
performance, focusing mainly on deep learning methods. We also review
widely-used datasets and performance metrics, in addition to the discussions on
open problems and unsolved challenges in image captioning.",-0.16529207,-0.25547326,-0.022890415,C
1262,"Still, the
beneÔ¨Åts and advantages of convolutional-based methods, such as a more straightforward training process (discussed
more thoroughly in ""Convolutional-based methods,"" section 3.5), encourage further research in this Ô¨Åeld.","[126] falls at
the lower part of the performance list in most metrics as well, which shows that convolutional-based methods still need
much more improvement to reach the performance of the other methods, such as the attention-based ones.","5.4 Comparing Independent Results

Many research works have reported their results independently, as well as the results reported by the Microsoft COCO
servers.",2022-01-31 00:39:37+00:00,Deep Learning Approaches on Image Captioning: A Review,cs.CV,"['cs.CV', 'I.2.7; I.4']","[arxiv.Result.Author('Taraneh Ghandi'), arxiv.Result.Author('Hamidreza Pourreza'), arxiv.Result.Author('Hamidreza Mahyar')]","Automatic image captioning, which involves describing the contents of an
image, is a challenging problem with many applications in various research
fields. One notable example is designing assistants for the visually impaired.
Recently, there have been significant advances in image captioning methods
owing to the breakthroughs in deep learning. This survey paper aims to provide
a structured review of recent image captioning techniques, and their
performance, focusing mainly on deep learning methods. We also review
widely-used datasets and performance metrics, in addition to the discussions on
open problems and unsolved challenges in image captioning.",-0.11595296,-0.21688859,0.020732645,C
1281,"We also plan to further study the
calibration properties of the belief functions computed by our approach (using calibration
measures specially designed for belief functions), as well as the novelty detection capability
of our model.","This issue could be addressed
by adapting the loss function as proposed, e.g., in [62].","Acknowledgements

    This work was supported by the China Scholarship Council (No.",2022-01-31 09:34:38+00:00,Lymphoma segmentation from 3D PET-CT images using a deep evidential network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ling Huang'), arxiv.Result.Author('Su Ruan'), arxiv.Result.Author('Pierre Decazes'), arxiv.Result.Author('Thierry Denoeux')]","An automatic evidential segmentation method based on Dempster-Shafer theory
and deep learning is proposed to segment lymphomas from three-dimensional
Positron Emission Tomography (PET) and Computed Tomography (CT) images. The
architecture is composed of a deep feature-extraction module and an evidential
layer. The feature extraction module uses an encoder-decoder framework to
extract semantic feature vectors from 3D inputs. The evidential layer then uses
prototypes in the feature space to compute a belief function at each voxel
quantifying the uncertainty about the presence or absence of a lymphoma at this
location. Two evidential layers are compared, based on different ways of using
distances to prototypes for computing mass functions. The whole model is
trained end-to-end by minimizing the Dice loss function. The proposed
combination of deep feature extraction and evidential segmentation is shown to
outperform the baseline UNet model as well as three other state-of-the-art
models on a dataset of 173 patients.",0.14920872,0.04095021,-0.03638147,A
1288,"We further study the amount of redundancy in Super-features, compared to local features, in Ap-
pendix B.3.","In Appendix B.2,
we display the attention maps of Super-features, at different scales for a Ô¨Åxed Super-feature ID.","Next, we verify in Appendix B.4 that all Super-features receive training signal, as a
sanity check.",2022-01-31 12:48:42+00:00,Learning Super-Features for Image Retrieval,cs.CV,['cs.CV'],"[arxiv.Result.Author('Philippe Weinzaepfel'), arxiv.Result.Author('Thomas Lucas'), arxiv.Result.Author('Diane Larlus'), arxiv.Result.Author('Yannis Kalantidis')]","Methods that combine local and global features have recently shown excellent
performance on multiple challenging deep image retrieval benchmarks, but their
use of local features raises at least two issues. First, these local features
simply boil down to the localized map activations of a neural network, and
hence can be extremely redundant. Second, they are typically trained with a
global loss that only acts on top of an aggregation of local features; by
contrast, testing is based on local feature matching, which creates a
discrepancy between training and testing. In this paper, we propose a novel
architecture for deep image retrieval, based solely on mid-level features that
we call Super-features. These Super-features are constructed by an iterative
attention module and constitute an ordered set in which each element focuses on
a localized and discriminant image pattern. For training, they require only
image labels. A contrastive loss operates directly at the level of
Super-features and focuses on those that match across images. A second
complementary loss encourages diversity. Experiments on common landmark
retrieval benchmarks validate that Super-features substantially outperform
state-of-the-art methods when using the same number of features, and only
require a significantly smaller memory footprint to match their performance.
Code and models are available at: https://github.com/naver/FIRe.",0.033676893,-0.08681619,-0.018615834,C
1302,"Thus the observations presented here as well as in earlier
                                                                  papers indicate potential directions for further research.",investigations both in terms of computations and models.,"These
   When considering a model for power-law power spectra           directions are under investigation now.",2022-01-27 08:51:52+00:00,On scale-invariant properties in natural images and their simulations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maxim Koroteev'), arxiv.Result.Author('Kirill Aistov')]","We study samples of natural images for which a set of statistical
characteristics is computed and scale-invariant properties of samples are
demonstrated computationally. Computations of the power spectrum are carried
out and a power-law decaying power spectrum is observed on samples taken from
van Hateren images of natural scenes. We propose a dynamic model to reproduce
the observed slope in the power spectrum qualitatively. For two types of
sources for this model the behaviour of power spectrum is investigated and
scale-invariance confirmed numerically. We then discuss potential applications
of scale-invariant properties of natural images.",0.49447328,0.18177268,-0.008078197,A
1303,"Thus the observations presented here as well as in earlier
   In a spatial domain our results for natural images reproduce   papers indicate potential directions for further research.",an intuition about any fundamental property of image quality.,"These
the observations made earlier in a set of deep and thoughtful     directions are under investigation now.",2022-01-27 08:51:52+00:00,On scale-invariant properties in natural images and their simulations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maxim Koroteev'), arxiv.Result.Author('Kirill Aistov')]","We study samples of natural images for which a set of statistical
characteristics is computed and scale-invariant properties of samples are
demonstrated computationally. Computations of the power spectrum are carried
out and a power-law decaying power spectrum is observed on samples taken from
van Hateren images of natural scenes. We propose a dynamic model to reproduce
the observed slope in the power spectrum qualitatively. For two types of
sources for this model the behaviour of power spectrum is investigated and
scale-invariance confirmed numerically. We then discuss potential applications
of scale-invariant properties of natural images.",-0.15944418,0.176474,0.017371425,B
1355,"An interesting further research direction could be
the extension to multi-label segmentation.","Thereby, we show
that the output of the Layer-wise Relevance Propa-
gation (LRP) can be exploited to generate pixel-wise
segmentation masks.","Currently,
we also try to apply the proposed solution to the chal-
lenging problem of rain streak segmentation in natu-
ral images, as it is a very demanding task to generate
Table 5: Overview of the metrics for the sewer pipe cracks segmentation with our proposed LRP rule set (left) and the one
proposed by (Montavon et al., 2019) (right).",2022-02-01 10:26:10+00:00,From Explanations to Segmentation: Using Explainable AI for Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Clemens Seibold'), arxiv.Result.Author('Johannes K√ºnzel'), arxiv.Result.Author('Anna Hilsmann'), arxiv.Result.Author('Peter Eisert')]","The new era of image segmentation leveraging the power of Deep Neural Nets
(DNNs) comes with a price tag: to train a neural network for pixel-wise
segmentation, a large amount of training samples has to be manually labeled on
pixel-precision. In this work, we address this by following an indirect
solution. We build upon the advances of the Explainable AI (XAI) community and
extract a pixel-wise binary segmentation from the output of the Layer-wise
Relevance Propagation (LRP) explaining the decision of a classification
network. We show that we achieve similar results compared to an established
U-Net segmentation architecture, while the generation of the training data is
significantly simplified. The proposed method can be trained in a weakly
supervised fashion, as the training samples must be only labeled on
image-level, at the same time enabling the output of a segmentation mask. This
makes it especially applicable to a wider range of real applications where
tedious pixel-level labelling is often not possible.",-0.14997664,0.08965287,0.034030877,B
1367,We make our dataset and implementations publicly available for further research.,"Finally, we provide and evaluate strategies for algorithm selection to
                                        obtain good expected performance.","Index Terms‚ÄîAlgorithms, computer vision, graph algorithms, graph-theoretic methods, parallel algorithms, performance evaluation of
                                        algorithms and systems

                                                                                                        !",2022-02-01 14:06:27+00:00,Review of Serial and Parallel Min-Cut/Max-Flow Algorithms for Computer Vision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Patrick M. Jensen'), arxiv.Result.Author('Niels Jeppesen'), arxiv.Result.Author('Anders B. Dahl'), arxiv.Result.Author('Vedrana A. Dahl')]","Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of
problems in computer vision and thus significant effort has been put into
developing fast min-cut/max-flow algorithms. As a result, it is difficult to
choose an ideal algorithm for a given problem. Furthermore, parallel algorithms
have not been thoroughly compared. In this paper, we evaluate the
state-of-the-art serial and parallel min-cut/max-flow algorithms on the largest
set of computer vision problems yet. We focus on generic algorithms, i.e., for
unstructured graphs, but also compare with the specialized GridCut
implementation. When applicable, GridCut performs best. Otherwise, the two
pseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth
first search, achieves the overall best performance. The most memory efficient
implementation tested is the Boykov-Kolmogorov algorithm. Amongst generic
parallel algorithms, we find the bottom-up merging approach by Liu and Sun to
be best, but no method is dominant. Of the generic parallel methods, only the
parallel preflow push-relabel algorithm is able to efficiently scale with many
processors across problem sizes, and no generic parallel method consistently
outperforms serial algorithms. Finally, we provide and evaluate strategies for
algorithm selection to obtain good expected performance. We make our dataset
and implementations publicly available for further research.",0.07663714,0.07756406,0.0043175668,A
1368,algorithms and deserve further study.,"These issues reveal    handling repeated solves of graphs where capacities do not change
a major deÔ¨Åciency in the state of current parallel min-cut/max-Ô¨Çow     drastically between successive solves.","While providing good scaling
on any type of graph may be unreachable as min-cut/max-Ô¨Çow is          REFERENCES
P-complete and therefore hard to parallelize [39], computer vision
graphs often come with additional structure.",2022-02-01 14:06:27+00:00,Review of Serial and Parallel Min-Cut/Max-Flow Algorithms for Computer Vision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Patrick M. Jensen'), arxiv.Result.Author('Niels Jeppesen'), arxiv.Result.Author('Anders B. Dahl'), arxiv.Result.Author('Vedrana A. Dahl')]","Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of
problems in computer vision and thus significant effort has been put into
developing fast min-cut/max-flow algorithms. As a result, it is difficult to
choose an ideal algorithm for a given problem. Furthermore, parallel algorithms
have not been thoroughly compared. In this paper, we evaluate the
state-of-the-art serial and parallel min-cut/max-flow algorithms on the largest
set of computer vision problems yet. We focus on generic algorithms, i.e., for
unstructured graphs, but also compare with the specialized GridCut
implementation. When applicable, GridCut performs best. Otherwise, the two
pseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth
first search, achieves the overall best performance. The most memory efficient
implementation tested is the Boykov-Kolmogorov algorithm. Amongst generic
parallel algorithms, we find the bottom-up merging approach by Liu and Sun to
be best, but no method is dominant. Of the generic parallel methods, only the
parallel preflow push-relabel algorithm is able to efficiently scale with many
processors across problem sizes, and no generic parallel method consistently
outperforms serial algorithms. Finally, we provide and evaluate strategies for
algorithm selection to obtain good expected performance. We make our dataset
and implementations publicly available for further research.",-0.05839505,0.14153278,0.093604766,B
1402,"Ideally, our model should not be bounded        and body pose based action recognition demonstrate the dif-
to category-speciÔ¨Åc biases and indeed learn the nature of          Ô¨Åculty of our task using modern video classiÔ¨Åcation archi-
activity-induced energy expenditure by, e.g., understanding        tectures, highlighting the need for further research.","tions are continuous calorie values and not rigid categories,
in practice, the training set can only cover a Ô¨Ånite amount        iments with multiple state-of-the-art approaches for video-
of activity types.","the type and intensity of bodily movement produced by the
skeletal muscles.",2022-02-01 19:04:42+00:00,Should I take a walk? Estimating Energy Expenditure from Video Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Alina Roitberg'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Rainer Stiefelhagen')]","We explore the problem of automatically inferring the amount of kilocalories
used by human during physical activity from his/her video observation. To study
this underresearched task, we introduce Vid2Burn -- an omni-source benchmark
for estimating caloric expenditure from video data featuring both, high- and
low-intensity activities for which we derive energy expenditure annotations
based on models established in medical literature. In practice, a training set
would only cover a certain amount of activity types, and it is important to
validate, if the model indeed captures the essence of energy expenditure,
(e.g., how many and which muscles are involved and how intense they work)
instead of memorizing fixed values of specific activity categories seen during
training. Ideally, the models should look beyond such category-specific biases
and regress the caloric cost in videos depicting activity categories not
explicitly present during training. With this property in mind, Vid2Burn is
accompanied with a cross-category benchmark, where the task is to regress
caloric expenditure for types of physical activities not present during
training. An extensive evaluation of state-of-the-art approaches for video
recognition modified for the energy expenditure estimation task demonstrates
the difficulty of this problem, especially for new activity types at test-time,
marking a new research direction. Dataset and code are available at
https://github.com/KPeng9510/Vid2Burn.",-0.124361575,-0.019574642,-0.23210719,B
1403,"MAE is an intuitive metric reporting the mean disar-        model is evaluated in new previously unseen situations, mo-
ray between prediction and ground truth in our target units,       tivating further research of models with deeper and more
(i.e., kilocalories).","However, our experiments also
Correlation (SPC), and the Negative Logarithm Likelihood           underline the difÔ¨Åculty of estimating the energy cost if the
(NLL).","Note, that while SPC illustrates the as-     Ô¨Åne-grained understanding of physical activities.",2022-02-01 19:04:42+00:00,Should I take a walk? Estimating Energy Expenditure from Video Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Alina Roitberg'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Rainer Stiefelhagen')]","We explore the problem of automatically inferring the amount of kilocalories
used by human during physical activity from his/her video observation. To study
this underresearched task, we introduce Vid2Burn -- an omni-source benchmark
for estimating caloric expenditure from video data featuring both, high- and
low-intensity activities for which we derive energy expenditure annotations
based on models established in medical literature. In practice, a training set
would only cover a certain amount of activity types, and it is important to
validate, if the model indeed captures the essence of energy expenditure,
(e.g., how many and which muscles are involved and how intense they work)
instead of memorizing fixed values of specific activity categories seen during
training. Ideally, the models should look beyond such category-specific biases
and regress the caloric cost in videos depicting activity categories not
explicitly present during training. With this property in mind, Vid2Burn is
accompanied with a cross-category benchmark, where the task is to regress
caloric expenditure for types of physical activities not present during
training. An extensive evaluation of state-of-the-art approaches for video
recognition modified for the energy expenditure estimation task demonstrates
the difficulty of this problem, especially for new activity types at test-time,
marking a new research direction. Dataset and code are available at
https://github.com/KPeng9510/Vid2Burn.",0.1943804,0.006405285,-0.10202989,A
1404,"Ideally, our model should not be bounded        and body pose based action recognition demonstrate the dif-
to category-speciÔ¨Åc biases and indeed learn the nature of          Ô¨Åculty of our task using modern video classiÔ¨Åcation archi-
activity-induced energy expenditure by, e.g., understanding        tectures, highlighting the need for further research.","Extensive exper-
in practice, the training set can only cover a Ô¨Ånite amount        iments with multiple state-of-the-art approaches for video-
of activity types.","the type and intensity of bodily movement produced by the
skeletal muscles.",2022-02-01 19:04:42+00:00,Should I take a walk? Estimating Energy Expenditure from Video Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Alina Roitberg'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Rainer Stiefelhagen')]","We explore the problem of automatically inferring the amount of kilocalories
used by human during physical activity from his/her video observation. To study
this underresearched task, we introduce Vid2Burn -- an omni-source benchmark
for estimating caloric expenditure from video data featuring both, high- and
low-intensity activities for which we derive energy expenditure annotations
based on models established in medical literature. In practice, a training set
would only cover a certain amount of activity types, and it is important to
validate, if the model indeed captures the essence of energy expenditure,
(e.g., how many and which muscles are involved and how intense they work)
instead of memorizing fixed values of specific activity categories seen during
training. Ideally, the models should look beyond such category-specific biases
and regress the caloric cost in videos depicting activity categories not
explicitly present during training. With this property in mind, Vid2Burn is
accompanied with a cross-category benchmark, where the task is to regress
caloric expenditure for types of physical activities not present during
training. An extensive evaluation of state-of-the-art approaches for video
recognition modified for the energy expenditure estimation task demonstrates
the difficulty of this problem, especially for new activity types at test-time,
marking a new research direction. Dataset and code are available at
https://github.com/KPeng9510/Vid2Burn.",-0.145075,-0.021326195,-0.25516826,B
1405,"MAE is an intuitive metric reporting the mean disar-                model is evaluated in new previously unseen situations, mo-
ray between prediction and ground truth in our target units,               tivating further research of models with deeper and more
(i.e., kilocalories).","However, our experiments also
Correlation (SPC), and the Negative Logarithm Likelihood                   underline the difÔ¨Åculty of estimating the energy cost if the
(NLL).","Note, that while SPC illustrates the as-             Ô¨Åne-grained understanding of physical activities.",2022-02-01 19:04:42+00:00,Should I take a walk? Estimating Energy Expenditure from Video Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Alina Roitberg'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Rainer Stiefelhagen')]","We explore the problem of automatically inferring the amount of kilocalories
used by human during physical activity from his/her video observation. To study
this underresearched task, we introduce Vid2Burn -- an omni-source benchmark
for estimating caloric expenditure from video data featuring both, high- and
low-intensity activities for which we derive energy expenditure annotations
based on models established in medical literature. In practice, a training set
would only cover a certain amount of activity types, and it is important to
validate, if the model indeed captures the essence of energy expenditure,
(e.g., how many and which muscles are involved and how intense they work)
instead of memorizing fixed values of specific activity categories seen during
training. Ideally, the models should look beyond such category-specific biases
and regress the caloric cost in videos depicting activity categories not
explicitly present during training. With this property in mind, Vid2Burn is
accompanied with a cross-category benchmark, where the task is to regress
caloric expenditure for types of physical activities not present during
training. An extensive evaluation of state-of-the-art approaches for video
recognition modified for the energy expenditure estimation task demonstrates
the difficulty of this problem, especially for new activity types at test-time,
marking a new research direction. Dataset and code are available at
https://github.com/KPeng9510/Vid2Burn.",0.1943804,0.006405285,-0.10202989,A
1465,"Although
opportunities for further research remain, we believe that this approach paves the way from System1
to System 2 [40].","In the future, we plan to adapt the proposed model to video input, improve our model by changing the
intermediate layer, and output human-understandable concepts by unsupervised learning.","References

 [1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim.",2022-02-03 08:30:51+00:00,Concept Bottleneck Model with Additional Unsupervised Concepts,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yoshihide Sawada'), arxiv.Result.Author('Keigo Nakamura')]","With the increasing demands for accountability, interpretability is becoming
an essential capability for real-world AI applications. However, most methods
utilize post-hoc approaches rather than training the interpretable model. In
this article, we propose a novel interpretable model based on the concept
bottleneck model (CBM). CBM uses concept labels to train an intermediate layer
as the additional visible layer. However, because the number of concept labels
restricts the dimension of this layer, it is difficult to obtain high accuracy
with a small number of labels. To address this issue, we integrate supervised
concepts with unsupervised ones trained with self-explaining neural networks
(SENNs). By seamlessly training these two types of concepts while reducing the
amount of computation, we can obtain both supervised and unsupervised concepts
simultaneously, even for large-sized images. We refer to the proposed model as
the concept bottleneck model with additional unsupervised concepts (CBM-AUC).
We experimentally confirmed that the proposed model outperformed CBM and SENN.
We also visualized the saliency map of each concept and confirmed that it was
consistent with the semantic meanings.",-0.12394689,-0.17225188,-0.10550679,C
1531,"The
necessity of massive datasets has been a limiting factor for further research in non-convolutional
networks.","Even in the regime of moderately sized training sets, CNNs can be outperformed.","We therefore hope that our work serves as a starting point for ending the dominance of
CNNs in image processing tasks, and motivates further research in models free from hand-crafted
visual features and high inductive biases.",2022-02-04 08:36:34+00:00,Image-to-Image MLP-mixer for Image Reconstruction,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Youssef Mansour'), arxiv.Result.Author('Kang Lin'), arxiv.Result.Author('Reinhard Heckel')]","Neural networks are highly effective tools for image reconstruction problems
such as denoising and compressive sensing. To date, neural networks for image
reconstruction are almost exclusively convolutional. The most popular
architecture is the U-Net, a convolutional network with a multi-resolution
architecture. In this work, we show that a simple network based on the
multi-layer perceptron (MLP)-mixer enables state-of-the art image
reconstruction performance without convolutions and without a multi-resolution
architecture, provided that the training set and the size of the network are
moderately large. Similar to the original MLP-mixer, the image-to-image
MLP-mixer is based exclusively on MLPs operating on linearly-transformed image
patches. Contrary to the original MLP-mixer, we incorporate structure by
retaining the relative positions of the image patches. This imposes an
inductive bias towards natural images which enables the image-to-image
MLP-mixer to learn to denoise images based on fewer examples than the original
MLP-mixer. Moreover, the image-to-image MLP-mixer requires fewer parameters to
achieve the same denoising performance than the U-Net and its parameters scale
linearly in the image resolution instead of quadratically as for the original
MLP-mixer. If trained on a moderate amount of examples for denoising, the
image-to-image MLP-mixer outperforms the U-Net by a slight margin. It also
outperforms the vision transformer tailored for image reconstruction and
classical un-trained methods such as BM3D, making it a very effective tool for
image reconstruction problems.",-0.29586098,-0.23877338,0.27963734,C
1532,"We therefore hope that our work serves as a starting point for ending the dominance of
CNNs in image processing tasks, and motivates further research in models free from hand-crafted
visual features and high inductive biases.","The
necessity of massive datasets has been a limiting factor for further research in non-convolutional
networks.","6 Reproducibility statement

The code to reproduce the results in this paper is available on our Github page: https://github.",2022-02-04 08:36:34+00:00,Image-to-Image MLP-mixer for Image Reconstruction,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Youssef Mansour'), arxiv.Result.Author('Kang Lin'), arxiv.Result.Author('Reinhard Heckel')]","Neural networks are highly effective tools for image reconstruction problems
such as denoising and compressive sensing. To date, neural networks for image
reconstruction are almost exclusively convolutional. The most popular
architecture is the U-Net, a convolutional network with a multi-resolution
architecture. In this work, we show that a simple network based on the
multi-layer perceptron (MLP)-mixer enables state-of-the art image
reconstruction performance without convolutions and without a multi-resolution
architecture, provided that the training set and the size of the network are
moderately large. Similar to the original MLP-mixer, the image-to-image
MLP-mixer is based exclusively on MLPs operating on linearly-transformed image
patches. Contrary to the original MLP-mixer, we incorporate structure by
retaining the relative positions of the image patches. This imposes an
inductive bias towards natural images which enables the image-to-image
MLP-mixer to learn to denoise images based on fewer examples than the original
MLP-mixer. Moreover, the image-to-image MLP-mixer requires fewer parameters to
achieve the same denoising performance than the U-Net and its parameters scale
linearly in the image resolution instead of quadratically as for the original
MLP-mixer. If trained on a moderate amount of examples for denoising, the
image-to-image MLP-mixer outperforms the U-Net by a slight margin. It also
outperforms the vision transformer tailored for image reconstruction and
classical un-trained methods such as BM3D, making it a very effective tool for
image reconstruction problems.",-0.2587109,-0.20128343,0.2707221,C_centroid
1600,"The direction for further study is to integrate the
                                                                              multi-domain image translation with several interesting applications
                                                                              that require careful handling of multi-domain images, such as visual
                                                                              localization, 3D reconstruction, etc.","fer appearance between semantically similar images, thus leading to
                                                                              effective training.",5.,2022-02-06 14:12:34+00:00,Multi-domain Unsupervised Image-to-Image Translation with Appearance Adaptive Convolution,cs.CV,['cs.CV'],"[arxiv.Result.Author('Somi Jeong'), arxiv.Result.Author('Jiyoung Lee'), arxiv.Result.Author('Kwanghoon Sohn')]","Over the past few years, image-to-image (I2I) translation methods have been
proposed to translate a given image into diverse outputs. Despite the
impressive results, they mainly focus on the I2I translation between two
domains, so the multi-domain I2I translation still remains a challenge. To
address this problem, we propose a novel multi-domain unsupervised
image-to-image translation (MDUIT) framework that leverages the decomposed
content feature and appearance adaptive convolution to translate an image into
a target appearance while preserving the given geometric content. We also
exploit a contrast learning objective, which improves the disentanglement
ability and effectively utilizes multi-domain image data in the training
process by pairing the semantically similar images. This allows our method to
learn the diverse mappings between multiple visual domains with only a single
framework. We show that the proposed method produces visually diverse and
plausible results in multiple domains compared to the state-of-the-art methods.",-0.22411704,-0.06126026,0.0295018,C
1617,"This implies that multimodal
                                                                      fusion remains challenging with a need for further study
Method                Sup Rank@LT1CC mAP PRCRCan(k1@-S1hot)           under unsupervised clothes change re-id setting.","(‚ÄòS‚Äô: supervised,         grayscale based multimodal variant fails to beat our orig-
‚ÄòU‚Äô: unsuperivsed)                                                    inal unimodal method (#8).","Thus, we
                                                                      have not evaluated our method further by involving more
#55 PCB [35]              S  23.52  10.03  22.86                      other modalities.",2022-02-07 11:55:23+00:00,Unsupervised Long-Term Person Re-Identification with Clothes Change,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mingkun Li'), arxiv.Result.Author('Peng Xu'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jun Guo')]","We investigate unsupervised person re-identification (Re-ID) with clothes
change, a new challenging problem with more practical usability and scalability
to real-world deployment. Most existing re-id methods artificially assume the
clothes of every single person to be stationary across space and time. This
condition is mostly valid for short-term re-id scenarios since an average
person would often change the clothes even within a single day. To alleviate
this assumption, several recent works have introduced the clothes change facet
to re-id, with a focus on supervised learning person identity discriminative
representation with invariance to clothes changes. Taking a step further
towards this long-term re-id direction, we further eliminate the requirement of
person identity labels, as they are significantly more expensive and more
tedious to annotate in comparison to short-term person re-id datasets. Compared
to conventional unsupervised short-term re-id, this new problem is drastically
more challenging as different people may have similar clothes whilst the same
person can wear multiple suites of clothes over different locations and times
with very distinct appearance. To overcome such obstacles, we introduce a novel
Curriculum Person Clustering (CPC) method that can adaptively regulate the
unsupervised clustering criterion according to the clustering confidence.
Experiments on three long-term person re-id datasets show that our CPC
outperforms SOTA unsupervised re-id methods and even closely matches the
supervised re-id models.",0.030530456,0.08491274,-0.07026835,C
1618,"This implies that multimodal
‚ÄòU‚Äô: unsuperivsed)                                                    fusion remains challenging with a need for further study
                                                                      under unsupervised clothes change re-id setting.","(‚ÄòS‚Äô: supervised,         inal unimodal method (#8).","Thus, we
Method                Sup LTCC PRCC (1-Shot)                          have not evaluated our method further by involving more
                             Rank@1 mAP    Rank@1                     other modalities.",2022-02-07 11:55:23+00:00,Unsupervised Long-Term Person Re-Identification with Clothes Change,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mingkun Li'), arxiv.Result.Author('Peng Xu'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jun Guo')]","We investigate unsupervised person re-identification (Re-ID) with clothes
change, a new challenging problem with more practical usability and scalability
to real-world deployment. Most existing re-id methods artificially assume the
clothes of every single person to be stationary across space and time. This
condition is mostly valid for short-term re-id scenarios since an average
person would often change the clothes even within a single day. To alleviate
this assumption, several recent works have introduced the clothes change facet
to re-id, with a focus on supervised learning person identity discriminative
representation with invariance to clothes changes. Taking a step further
towards this long-term re-id direction, we further eliminate the requirement of
person identity labels, as they are significantly more expensive and more
tedious to annotate in comparison to short-term person re-id datasets. Compared
to conventional unsupervised short-term re-id, this new problem is drastically
more challenging as different people may have similar clothes whilst the same
person can wear multiple suites of clothes over different locations and times
with very distinct appearance. To overcome such obstacles, we introduce a novel
Curriculum Person Clustering (CPC) method that can adaptively regulate the
unsupervised clustering criterion according to the clustering confidence.
Experiments on three long-term person re-id datasets show that our CPC
outperforms SOTA unsupervised re-id methods and even closely matches the
supervised re-id models.",-0.0061898055,0.059279867,-0.10358942,C
1658,"Another noteworthy issue is that, to the best of our knowledge, there

                                                   4
is not yet a suÔ¨Éciently large scribble-based SOD dataset in the remote sensing
community, which may hinder further research of weakly supervised learning.","Thus, some
noises and background edges will be easily introduced to interference model
training.",The scribble-based salient object detection is still an under-explored topic.,2022-02-07 20:32:21+00:00,Scribble-based Boundary-aware Network for Weakly Supervised Salient Object Detection in Remote Sensing Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhou Huang'), arxiv.Result.Author('Tian-Zhu Xiang'), arxiv.Result.Author('Huai-Xin Chen'), arxiv.Result.Author('Hang Dai')]","Existing CNNs-based salient object detection (SOD) heavily depends on the
large-scale pixel-level annotations, which is labor-intensive, time-consuming,
and expensive. By contrast, the sparse annotations become appealing to the
salient object detection community. However, few efforts are devoted to
learning salient object detection from sparse annotations, especially in the
remote sensing field. In addition, the sparse annotation usually contains
scanty information, which makes it challenging to train a well-performing
model, resulting in its performance largely lagging behind the fully-supervised
models. Although some SOD methods adopt some prior cues to improve the
detection performance, they usually lack targeted discrimination of object
boundaries and thus provide saliency maps with poor boundary localization. To
this end, in this paper, we propose a novel weakly-supervised salient object
detection framework to predict the saliency of remote sensing images from
sparse scribble annotations. To implement it, we first construct the
scribble-based remote sensing saliency dataset by relabelling an existing
large-scale SOD dataset with scribbles, namely S-EOR dataset. After that, we
present a novel scribble-based boundary-aware network (SBA-Net) for remote
sensing salient object detection. Specifically, we design a boundary-aware
module (BAM) to explore the object boundary semantics, which is explicitly
supervised by the high-confidence object boundary (pseudo) labels generated by
the boundary label generation (BLG) module, forcing the model to learn features
that highlight the object structure and thus boosting the boundary localization
of objects. Then, the boundary semantics are integrated with high-level
features to guide the salient object detection under the supervision of
scribble labels.",-0.1833296,-0.051329188,-0.025918856,B
1670,"drawback may refer to a semi-supervised or weak-supervised
Therefore, we can conclude that our method could integrally     learning strategy, which we are going to further study.","An alternative way to overcome this
meaningful evaluation metric for face sketch synthesis task.","construct sketches from face photos with high-Ô¨Ådelity and rich
details and achieves the state-of-the-art performances in this                             V. CONCLUSION
task.",2022-02-08 01:51:24+00:00,MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Fan Ji'), arxiv.Result.Author('Muyi Sun'), arxiv.Result.Author('Xingqun Qi'), arxiv.Result.Author('Qi Li'), arxiv.Result.Author('Zhenan Sun')]","Face sketch synthesis has been widely used in multi-media entertainment and
law enforcement. Despite the recent developments in deep neural networks,
accurate and realistic face sketch synthesis is still a challenging task due to
the diversity and complexity of human faces. Current image-to-image
translation-based face sketch synthesis frequently encounters over-fitting
problems when it comes to small-scale datasets. To tackle this problem, we
present an end-to-end Memory Oriented Style Transfer Network (MOST-Net) for
face sketch synthesis which can produce high-fidelity sketches with limited
data. Specifically, an external self-supervised dynamic memory module is
introduced to capture the domain alignment knowledge in the long term. In this
way, our proposed model could obtain the domain-transfer ability by
establishing the durable relationship between faces and corresponding sketches
on the feature level. Furthermore, we design a novel Memory Refinement Loss (MR
Loss) for feature alignment in the memory module, which enhances the accuracy
of memory slots in an unsupervised manner. Extensive experiments on the CUFS
and the CUFSF datasets show that our MOST-Net achieves state-of-the-art
performance, especially in terms of the Structural Similarity Index(SSIM).",-0.094920255,-0.14721343,-0.016623978,C
1671,"Therefore, we can conclude that our   learning strategy, which we are going to further study.","An alternative way to overcome this
consider LPIPS as a more meaningful evaluation metric for         drawback may refer to a semi-supervised or weak-supervised
face sketch synthesis task.","method could integrally construct sketches from face photos
with high-Ô¨Ådelity and rich details and achieves state-of-the-art                             V. CONCLUSION
performances in this task.",2022-02-08 01:51:24+00:00,MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Fan Ji'), arxiv.Result.Author('Muyi Sun'), arxiv.Result.Author('Xingqun Qi'), arxiv.Result.Author('Qi Li'), arxiv.Result.Author('Zhenan Sun')]","Face sketch synthesis has been widely used in multi-media entertainment and
law enforcement. Despite the recent developments in deep neural networks,
accurate and realistic face sketch synthesis is still a challenging task due to
the diversity and complexity of human faces. Current image-to-image
translation-based face sketch synthesis frequently encounters over-fitting
problems when it comes to small-scale datasets. To tackle this problem, we
present an end-to-end Memory Oriented Style Transfer Network (MOST-Net) for
face sketch synthesis which can produce high-fidelity sketches with limited
data. Specifically, an external self-supervised dynamic memory module is
introduced to capture the domain alignment knowledge in the long term. In this
way, our proposed model could obtain the domain-transfer ability by
establishing the durable relationship between faces and corresponding sketches
on the feature level. Furthermore, we design a novel Memory Refinement Loss (MR
Loss) for feature alignment in the memory module, which enhances the accuracy
of memory slots in an unsupervised manner. Extensive experiments on the CUFS
and the CUFSF datasets show that our MOST-Net achieves state-of-the-art
performance, especially in terms of the Structural Similarity Index(SSIM).",-0.09694048,-0.10216409,-0.058385275,C
1678,"In light of this, we do not expect our method to continue to work

for many steps without further research because the samples will inevitably drift from the data

distribution.","Another limitation is that while we do show compelling results when taking

a second step, these results also suggest that the MLM is already drifting from the data distribution

and so its utility is reduced.","Intervening multiple times is necessary for understanding complicated causal inter-

actions.",2022-02-08 05:14:16+00:00,Causal Scene BERT: Improving object detection by searching for challenging groups of data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Cinjon Resnick'), arxiv.Result.Author('Or Litany'), arxiv.Result.Author('Amlan Kar'), arxiv.Result.Author('Karsten Kreis'), arxiv.Result.Author('James Lucas'), arxiv.Result.Author('Kyunghyun Cho'), arxiv.Result.Author('Sanja Fidler')]","Modern computer vision applications rely on learning-based perception modules
parameterized with neural networks for tasks like object detection. These
modules frequently have low expected error overall but high error on atypical
groups of data due to biases inherent in the training process. In building
autonomous vehicles (AV), this problem is an especially important challenge
because their perception modules are crucial to the overall system performance.
After identifying failures in AV, a human team will comb through the associated
data to group perception failures that share common causes. More data from
these groups is then collected and annotated before retraining the model to fix
the issue. In other words, error groups are found and addressed in hindsight.
Our main contribution is a pseudo-automatic method to discover such groups in
foresight by performing causal interventions on simulated scenes. To keep our
interventions on the data manifold, we utilize masked language models. We
verify that the prioritized groups found via intervention are challenging for
the object detector and show that retraining with data collected from these
groups helps inordinately compared to adding more IID data. We also plan to
release software to run interventions in simulated scenes, which we hope will
benefit the causality community.",0.30018932,-0.10340567,-0.10728968,A
1679,"In light of this, we do not expect our method to continue to work

for many steps without further research because the samples will inevitably drift from the data

distribution.","Another limitation is that while we do show compelling results when taking

a second step, these results also suggest that the MLM is already drifting from the data distribution

and so its utility is reduced.","Intervening multiple times is necessary for understanding complicated causal inter-

actions.",2022-02-08 05:14:16+00:00,Causal Scene BERT: Improving object detection by searching for challenging groups of data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Cinjon Resnick'), arxiv.Result.Author('Or Litany'), arxiv.Result.Author('Amlan Kar'), arxiv.Result.Author('Karsten Kreis'), arxiv.Result.Author('James Lucas'), arxiv.Result.Author('Kyunghyun Cho'), arxiv.Result.Author('Sanja Fidler')]","Modern computer vision applications rely on learning-based perception modules
parameterized with neural networks for tasks like object detection. These
modules frequently have low expected error overall but high error on atypical
groups of data due to biases inherent in the training process. In building
autonomous vehicles (AV), this problem is an especially important challenge
because their perception modules are crucial to the overall system performance.
After identifying failures in AV, a human team will comb through the associated
data to group perception failures that share common causes. More data from
these groups is then collected and annotated before retraining the model to fix
the issue. In other words, error groups are found and addressed in hindsight.
Our main contribution is a pseudo-automatic method to discover such groups in
foresight by performing causal interventions on simulated scenes. To keep our
interventions on the data manifold, we utilize masked language models. We
verify that the prioritized groups found via intervention are challenging for
the object detector and show that retraining with data collected from these
groups helps inordinately compared to adding more IID data. We also plan to
release software to run interventions in simulated scenes, which we hope will
benefit the causality community.",0.30018932,-0.10340567,-0.10728968,A
1688,"Those techniques may
                                       and further research directions are provided.","Finally, potential research gaps are outlined      not damaged during their evaluation.","include simple visual examinations or image/video capturing
                                                                                                               [5], creating 3D representations using lidar scanners [9], [10]
                                                                 I.",2022-02-08 08:22:26+00:00,"What's Cracking? A Review and Analysis of Deep Learning Methods for Structural Crack Segmentation, Detection and Quantification",cs.CV,['cs.CV'],"[arxiv.Result.Author('Jacob K√∂nig'), arxiv.Result.Author('Mark Jenkins'), arxiv.Result.Author('Mike Mannion'), arxiv.Result.Author('Peter Barrie'), arxiv.Result.Author('Gordon Morison')]","Surface cracks are a very common indicator of potential structural faults.
Their early detection and monitoring is an important factor in structural
health monitoring. Left untreated, they can grow in size over time and require
expensive repairs or maintenance. With recent advances in computer vision and
deep learning algorithms, the automatic detection and segmentation of cracks
for this monitoring process have become a major topic of interest. This review
aims to give researchers an overview of the published work within the field of
crack analysis algorithms that make use of deep learning. It outlines the
various tasks that are solved through applying computer vision algorithms to
surface cracks in a structural health monitoring setting and also provides
in-depth reviews of recent fully, semi and unsupervised approaches that perform
crack classification, detection, segmentation and quantification. Additionally,
this review also highlights popular datasets used for cracks and the metrics
that are used to evaluate the performance of those algorithms. Finally,
potential research gaps are outlined and further research directions are
provided.",-0.01224931,0.37333554,-0.06980299,B
1693,"This would suggest, that the    and further research the achieved robustness of the model.","In the future, we
seen in Figure 4, the supervised model has a very high false positive,
as well as a high false negative rate, while the confusion matrix       plan and extend our work to a larger number of different modalities
for the semi-supervised model in figure 5 shows a far higher false
negative rate than false positive rate.","semi-supervised model is able to more accurately detect events that
it has already seen before than the supervised model, while failing     REFERENCES
to detect events that differ strongly from the events in the training
data.",2022-02-08 10:41:41+00:00,Addressing Data Scarcity in Multimodal User State Recognition by Combining Semi-Supervised and Supervised Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hendric Vo√ü'), arxiv.Result.Author('Heiko Wersing'), arxiv.Result.Author('Stefan Kopp')]","Detecting mental states of human users is crucial for the development of
cooperative and intelligent robots, as it enables the robot to understand the
user's intentions and desires. Despite their importance, it is difficult to
obtain a large amount of high quality data for training automatic recognition
algorithms as the time and effort required to collect and label such data is
prohibitively high. In this paper we present a multimodal machine learning
approach for detecting dis-/agreement and confusion states in a human-robot
interaction environment, using just a small amount of manually annotated data.
We collect a data set by conducting a human-robot interaction study and develop
a novel preprocessing pipeline for our machine learning approach. By combining
semi-supervised and supervised architectures, we are able to achieve an average
F1-score of 81.1\% for dis-/agreement detection with a small amount of labeled
data and a large unlabeled data set, while simultaneously increasing the
robustness of the model compared to the supervised approach.",0.06884846,-0.14519435,-0.13334662,A
1702,"Detecting adversarial samples from arti-
believe this is a promising direction for further research and            facts.",We                   drew B Gardner.,"arXiv preprint arXiv:1703.00410, 2017.",2022-02-03 21:03:32+00:00,Mapping DNN Embedding Manifolds for Network Generalization Prediction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author(""Molly O'Brien""), arxiv.Result.Author('Julia Bukowski'), arxiv.Result.Author('Mathias Unberath'), arxiv.Result.Author('Aria Pezeshk'), arxiv.Result.Author('Greg Hager')]","Understanding Deep Neural Network (DNN) performance in changing conditions is
essential for deploying DNNs in safety critical applications with unconstrained
environments, e.g., perception for self-driving vehicles or medical image
analysis. Recently, the task of Network Generalization Prediction (NGP) has
been proposed to predict how a DNN will generalize in a new operating domain.
Previous NGP approaches have relied on labeled metadata and known distributions
for the new operating domains. In this study, we propose the first NGP approach
that predicts DNN performance based solely on how unlabeled images from an
external operating domain map in the DNN embedding space. We demonstrate this
technique for pedestrian, melanoma, and animal classification tasks and show
state of the art NGP in 13 of 15 NGP tasks without requiring domain knowledge.
Additionally, we show that our NGP embedding maps can be used to identify
misclassified images when the DNN performance is poor.",-0.045415528,-0.25016996,0.0021144487,C
1714,"This can be subject of
Figure 13: Visualization of direct path between images           further study from the perspective of cognitive science and
shown in Figures 12a and 12c.","The ambiguity of images was characterized based
to the path in Figure 9.                                         on the correctness of human classiÔ¨Åcations and the time it
                                                                 took for humans to classify them.",psychology.,2022-02-05 15:09:51+00:00,Decision boundaries and convex hulls in the feature space that deep learning functions learn from images,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'math.OC']",[arxiv.Result.Author('Roozbeh Yousefzadeh')],"The success of deep neural networks in image classification and learning can
be partly attributed to the features they extract from images. It is often
speculated about the properties of a low-dimensional manifold that models
extract and learn from images. However, there is not sufficient understanding
about this low-dimensional space based on theory or empirical evidence. For
image classification models, their last hidden layer is the one where images of
each class is separated from other classes and it also has the least number of
features. Here, we develop methods and formulations to study that feature space
for any model. We study the partitioning of the domain in feature space,
identify regions guaranteed to have certain classifications, and investigate
its implications for the pixel space. We observe that geometric arrangements of
decision boundaries in feature space is significantly different compared to
pixel space, providing insights about adversarial vulnerabilities, image
morphing, extrapolation, ambiguity in classification, and the mathematical
understanding of image classification models.",-0.11542413,0.02673643,-0.32885265,C
1715,"This can be the subject of further study
from the perspective of cognitive science and psychology as well as machine learning.","Ambiguity of images was characterized based on the correctness of human clas-
siÔ¨Åcations and the time it took for humans to classify them.","Identifying
ambiguous images are also useful in practice.",2022-02-05 15:09:51+00:00,Decision boundaries and convex hulls in the feature space that deep learning functions learn from images,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'math.OC']",[arxiv.Result.Author('Roozbeh Yousefzadeh')],"The success of deep neural networks in image classification and learning can
be partly attributed to the features they extract from images. It is often
speculated about the properties of a low-dimensional manifold that models
extract and learn from images. However, there is not sufficient understanding
about this low-dimensional space based on theory or empirical evidence. For
image classification models, their last hidden layer is the one where images of
each class is separated from other classes and it also has the least number of
features. Here, we develop methods and formulations to study that feature space
for any model. We study the partitioning of the domain in feature space,
identify regions guaranteed to have certain classifications, and investigate
its implications for the pixel space. We observe that geometric arrangements of
decision boundaries in feature space is significantly different compared to
pixel space, providing insights about adversarial vulnerabilities, image
morphing, extrapolation, ambiguity in classification, and the mathematical
understanding of image classification models.",-0.13027568,-0.015645063,-0.28474018,C
1793,"There are several gaps in knowledge around industrial gDT in research that follows based on
the Ô¨Åndings of this work and would beneÔ¨Åt from further research, to extend and further enhance the
developed framework.","Applicability of this research extends to part identiÔ¨Åcation in industrial catalogs, better inventory
management and improved gDT workÔ¨Çows.","Direct future research includes: (a) an improved joint network framework
with additional SOTA classiÔ¨Åcation networks and (b) shape retrieval as part of the CLOI framework
where the automatically segmented instances will be the input of the proposed method.",2022-02-10 04:47:47+00:00,Geometric Digital Twinning of Industrial Facilities: Retrieval of Industrial Shapes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eva Agapaki'), arxiv.Result.Author('Ioannis Brilakis')]","This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",0.05501472,-0.048582084,-0.12004337,A
1824,"5 Discussion

      In this section, we discuss the strengths and limitations of the proposed methods, and
outline avenues for further research.","However, the registration task
seems to be particularly challenging for both methods.","5.1 Strengths of the proposed method

1.",2022-02-10 11:44:45+00:00,A Field of Experts Prior for Adapting Neural Networks at Test Time,cs.CV,"['cs.CV', 'eess.IV', 'stat.ML']","[arxiv.Result.Author('Neerav Karani'), arxiv.Result.Author('Georg Brunner'), arxiv.Result.Author('Ertunc Erdil'), arxiv.Result.Author('Simin Fei'), arxiv.Result.Author('Kerem Tezcan'), arxiv.Result.Author('Krishna Chaitanya'), arxiv.Result.Author('Ender Konukoglu')]","Performance of convolutional neural networks (CNNs) in image analysis tasks
is often marred in the presence of acquisition-related distribution shifts
between training and test images. Recently, it has been proposed to tackle this
problem by fine-tuning trained CNNs for each test image. Such
test-time-adaptation (TTA) is a promising and practical strategy for improving
robustness to distribution shifts as it requires neither data sharing between
institutions nor annotating additional data. Previous TTA methods use a helper
model to increase similarity between outputs and/or features extracted from a
test image with those of the training images. Such helpers, which are typically
modeled using CNNs, can be task-specific and themselves vulnerable to
distribution shifts in their inputs. To overcome these problems, we propose to
carry out TTA by matching the feature distributions of test and training
images, as modelled by a field-of-experts (FoE) prior. FoEs model complicated
probability distributions as products of many simpler expert distributions. We
use 1D marginal distributions of a trained task CNN's features as experts in
the FoE model. Further, we compute principal components of patches of the task
CNN's features, and consider the distributions of PCA loadings as additional
experts. We validate the method on 5 MRI segmentation tasks (healthy tissues in
4 anatomical regions and lesions in 1 one anatomy), using data from 17 clinics,
and on a MRI registration task, using data from 3 clinics. We find that the
proposed FoE-based TTA is generically applicable in multiple tasks, and
outperforms all previous TTA methods for lesion segmentation. For healthy
tissue segmentation, the proposed method outperforms other task-agnostic
methods, but a previous TTA method which is specifically designed for
segmentation performs the best for most of the tested datasets. Our code is
publicly available.",0.1591843,0.1794027,-0.2783925,A
1896,"We further study the
PGL w/o Mix-up is 4.6% lower than the proposed model,            effectiveness of the enlarging factor Œ±, which controls the
conÔ¨Årming that replacing the source samples with pseudo-         enlarging speed of the pseudo-labeled set, shown in Table
labeled target samples progressively can alleviate the side      7.",We observe that the OS performance of           Sensitivity to Enlarging Factor Œ±.,"We note that the proposed model with a smaller value of
effect of conditional shift.",2022-02-13 01:19:41+00:00,Source-Free Progressive Graph Learning for Open-Set Domain Adaptation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yadan Luo'), arxiv.Result.Author('Zijian Wang'), arxiv.Result.Author('Zhuoxiao Chen'), arxiv.Result.Author('Zi Huang'), arxiv.Result.Author('Mahsa Baktashmotlagh')]","Open-set domain adaptation (OSDA) has gained considerable attention in many
visual recognition tasks. However, most existing OSDA approaches are limited
due to three main reasons, including: (1) the lack of essential theoretical
analysis of generalization bound, (2) the reliance on the coexistence of source
and target data during adaptation, and (3) failing to accurately estimate the
uncertainty of model predictions. We propose a Progressive Graph Learning (PGL)
framework that decomposes the target hypothesis space into the shared and
unknown subspaces, and then progressively pseudo-labels the most confident
known samples from the target domain for hypothesis adaptation. Moreover, we
tackle a more realistic source-free open-set domain adaptation (SF-OSDA)
setting that makes no assumption about the coexistence of source and target
domains, and introduce a balanced pseudo-labeling (BP-L) strategy in a
two-stage framework, namely SF-PGL. Different from PGL that applies a
class-agnostic constant threshold for all target samples for pseudo-labeling,
the SF-PGL model uniformly selects the most confident target instances from
each category at a fixed ratio. The confidence thresholds in each class are
regarded as the 'uncertainty' of learning the semantic information, which are
then used to weigh the classification loss in the adaptation step. We conducted
unsupervised and semi-supervised OSDA and SF-OSDA experiments on the benchmark
image classification and action recognition datasets. Additionally, we find
that balanced pseudo-labeling plays a significant role in improving
calibration, which makes the trained model less prone to over-confident or
under-confident predictions on the target data. Source code is available at
https://github.com/Luoyadan/SF-PGL.",0.37846395,0.030067028,0.05790688,A
1919,"We believe that the problem of poor performance in the pictures‚Äô corners
or boundary areas may be improved through a better active selection strategy, which is worthy of further research in the
future.","However, compared with the two existing
approaches, our method still achieves better results.",Figure 6.,2022-02-14 05:17:38+00:00,ADeADA: Adaptive Density-aware Active Domain Adaptationfor Semantic Segmentation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Tsung-Han Wu'), arxiv.Result.Author('Yi-Syuan Liou'), arxiv.Result.Author('Shao-Ji Yuan'), arxiv.Result.Author('Hsin-Ying Lee'), arxiv.Result.Author('Tung-I Chen'), arxiv.Result.Author('Winston H. Hsu')]","In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present ADeADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate the label efficiency, we design an
adaptive budget allocation policy, which dynamically balances the labeling
budgets among different categories as well as between density-aware and
uncertainty-based methods. Extensive experiments show that our method
outperforms existing active learning and domain adaptation baselines on two
benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than 5%
target domain annotations, our method reaches comparable results with that of
full supervision.",0.015741032,0.20056394,-0.021293998,B
1920,"We believe that the problem of poor performance in the pictures‚Äô corners
or boundary areas may be improved through a better active selection strategy, which is worthy of further research in the
future.","However, compared with the two existing
approaches, our method still achieves better results.",Figure 6.,2022-02-14 05:17:38+00:00,ADeADA: Adaptive Density-aware Active Domain Adaptation for Semantic Segmentation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Tsung-Han Wu'), arxiv.Result.Author('Yi-Syuan Liou'), arxiv.Result.Author('Shao-Ji Yuan'), arxiv.Result.Author('Hsin-Ying Lee'), arxiv.Result.Author('Tung-I Chen'), arxiv.Result.Author('Winston H. Hsu')]","In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present ADeADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate the label efficiency, we design an
adaptive budget allocation policy, which dynamically balances the labeling
budgets among different categories as well as between density-aware and
uncertainty-based methods. Extensive experiments show that our method
outperforms existing active learning and domain adaptation baselines on two
benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than 5%
target domain annotations, our method reaches comparable results with that of
full supervision.",0.015741028,0.20056398,-0.021294095,B
1921,"ing task, we believe GAMMA will be an essential starting point
for further research on this task.",2425‚Äì2433.,"Bahdanau, D., Cho, K., Bengio, Y., 2014.",2022-02-14 06:54:15+00:00,GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junde Wu'), arxiv.Result.Author('Huihui Fang'), arxiv.Result.Author('Fei Li'), arxiv.Result.Author('Huazhu Fu'), arxiv.Result.Author('Fengbin Lin'), arxiv.Result.Author('Jiongcheng Li'), arxiv.Result.Author('Lexing Huang'), arxiv.Result.Author('Qinji Yu'), arxiv.Result.Author('Sifan Song'), arxiv.Result.Author('Xingxing Xu'), arxiv.Result.Author('Yanyu Xu'), arxiv.Result.Author('Wensai Wang'), arxiv.Result.Author('Lingxiao Wang'), arxiv.Result.Author('Shuai Lu'), arxiv.Result.Author('Huiqi Li'), arxiv.Result.Author('Shihua Huang'), arxiv.Result.Author('Zhichao Lu'), arxiv.Result.Author('Chubin Ou'), arxiv.Result.Author('Xifei Wei'), arxiv.Result.Author('Bingyuan Liu'), arxiv.Result.Author('Riadh Kobbi'), arxiv.Result.Author('Xiaoying Tang'), arxiv.Result.Author('Li Lin'), arxiv.Result.Author('Qiang Zhou'), arxiv.Result.Author('Qiang Hu'), arxiv.Result.Author('Hrvoje Bogunovic'), arxiv.Result.Author('Jos√© Ignacio Orlando'), arxiv.Result.Author('Xiulan Zhang'), arxiv.Result.Author('Yanwu Xu')]","Color fundus photography and Optical Coherence Tomography (OCT) are the two
most cost-effective tools for glaucoma screening. Both two modalities of images
have prominent biomarkers to indicate glaucoma suspected. Clinically, it is
often recommended to take both of the screenings for a more accurate and
reliable diagnosis. However, although numerous algorithms are proposed based on
fundus images or OCT volumes in computer-aided diagnosis, there are still few
methods leveraging both of the modalities for the glaucoma assessment. Inspired
by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held
previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)
Challenge to encourage the development of fundus \& OCT-based glaucoma grading.
The primary task of the challenge is to grade glaucoma from both the 2D fundus
images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released
a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT
volumes, which is the first multi-modality dataset for glaucoma grading. In
addition, an evaluation framework is also established to evaluate the
performance of the submitted methods. During the challenge, 1272 results were
submitted, and finally, top-10 teams were selected to the final stage. We
analysis their results and summarize their methods in the paper. Since all
these teams submitted their source code in the challenge, a detailed ablation
study is also conducted to verify the effectiveness of the particular modules
proposed. We find many of the proposed techniques are practical for the
clinical diagnosis of glaucoma. As the first in-depth study of fundus \& OCT
multi-modality glaucoma grading, we believe the GAMMA Challenge will be an
essential starting point for future research.",0.34613115,0.15305823,-0.07526472,A
1922,"ing task, we believe GAMMA will be an essential starting point
for further research on this task.",2425‚Äì2433.,"Bahdanau, D., Cho, K., Bengio, Y., 2014.",2022-02-14 06:54:15+00:00,GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junde Wu'), arxiv.Result.Author('Huihui Fang'), arxiv.Result.Author('Fei Li'), arxiv.Result.Author('Huazhu Fu'), arxiv.Result.Author('Fengbin Lin'), arxiv.Result.Author('Jiongcheng Li'), arxiv.Result.Author('Lexing Huang'), arxiv.Result.Author('Qinji Yu'), arxiv.Result.Author('Sifan Song'), arxiv.Result.Author('Xingxing Xu'), arxiv.Result.Author('Yanyu Xu'), arxiv.Result.Author('Wensai Wang'), arxiv.Result.Author('Lingxiao Wang'), arxiv.Result.Author('Shuai Lu'), arxiv.Result.Author('Huiqi Li'), arxiv.Result.Author('Shihua Huang'), arxiv.Result.Author('Zhichao Lu'), arxiv.Result.Author('Chubin Ou'), arxiv.Result.Author('Xifei Wei'), arxiv.Result.Author('Bingyuan Liu'), arxiv.Result.Author('Riadh Kobbi'), arxiv.Result.Author('Xiaoying Tang'), arxiv.Result.Author('Li Lin'), arxiv.Result.Author('Qiang Zhou'), arxiv.Result.Author('Qiang Hu'), arxiv.Result.Author('Hrvoje Bogunovic'), arxiv.Result.Author('Jos√© Ignacio Orlando'), arxiv.Result.Author('Xiulan Zhang'), arxiv.Result.Author('Yanwu Xu')]","Color fundus photography and Optical Coherence Tomography (OCT) are the two
most cost-effective tools for glaucoma screening. Both two modalities of images
have prominent biomarkers to indicate glaucoma suspected. Clinically, it is
often recommended to take both of the screenings for a more accurate and
reliable diagnosis. However, although numerous algorithms are proposed based on
fundus images or OCT volumes in computer-aided diagnosis, there are still few
methods leveraging both of the modalities for the glaucoma assessment. Inspired
by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held
previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)
Challenge to encourage the development of fundus \& OCT-based glaucoma grading.
The primary task of the challenge is to grade glaucoma from both the 2D fundus
images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released
a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT
volumes, which is the first multi-modality dataset for glaucoma grading. In
addition, an evaluation framework is also established to evaluate the
performance of the submitted methods. During the challenge, 1272 results were
submitted, and finally, top-10 teams were selected to the final stage. We
analysis their results and summarize their methods in the paper. Since all
these teams submitted their source code in the challenge, a detailed ablation
study is also conducted to verify the effectiveness of the particular modules
proposed. We find many of the proposed techniques are practical for the
clinical diagnosis of glaucoma. As the first in-depth study of fundus \& OCT
multi-modality glaucoma grading, we believe the GAMMA Challenge will be an
essential starting point for future research.",0.34613115,0.15305823,-0.07526472,A
1923,"We have also acknowledged the limitations of the current methods

                                                  27
and discussed the need for further research in this area in our paper.","In
the future, we are considering adding explainability as one of the metrics in
our challenges to encourage the development of these technologies for clinical
applications.",5.3.,2022-02-14 06:54:15+00:00,GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junde Wu'), arxiv.Result.Author('Huihui Fang'), arxiv.Result.Author('Fei Li'), arxiv.Result.Author('Huazhu Fu'), arxiv.Result.Author('Fengbin Lin'), arxiv.Result.Author('Jiongcheng Li'), arxiv.Result.Author('Lexing Huang'), arxiv.Result.Author('Qinji Yu'), arxiv.Result.Author('Sifan Song'), arxiv.Result.Author('Xinxing Xu'), arxiv.Result.Author('Yanyu Xu'), arxiv.Result.Author('Wensai Wang'), arxiv.Result.Author('Lingxiao Wang'), arxiv.Result.Author('Shuai Lu'), arxiv.Result.Author('Huiqi Li'), arxiv.Result.Author('Shihua Huang'), arxiv.Result.Author('Zhichao Lu'), arxiv.Result.Author('Chubin Ou'), arxiv.Result.Author('Xifei Wei'), arxiv.Result.Author('Bingyuan Liu'), arxiv.Result.Author('Riadh Kobbi'), arxiv.Result.Author('Xiaoying Tang'), arxiv.Result.Author('Li Lin'), arxiv.Result.Author('Qiang Zhou'), arxiv.Result.Author('Qiang Hu'), arxiv.Result.Author('Hrvoje Bogunovic'), arxiv.Result.Author('Jos√© Ignacio Orlando'), arxiv.Result.Author('Xiulan Zhang'), arxiv.Result.Author('Yanwu Xu')]","Color fundus photography and Optical Coherence Tomography (OCT) are the two
most cost-effective tools for glaucoma screening. Both two modalities of images
have prominent biomarkers to indicate glaucoma suspected. Clinically, it is
often recommended to take both of the screenings for a more accurate and
reliable diagnosis. However, although numerous algorithms are proposed based on
fundus images or OCT volumes in computer-aided diagnosis, there are still few
methods leveraging both of the modalities for the glaucoma assessment. Inspired
by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held
previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)
Challenge to encourage the development of fundus \& OCT-based glaucoma grading.
The primary task of the challenge is to grade glaucoma from both the 2D fundus
images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released
a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT
volumes, which is the first multi-modality dataset for glaucoma grading. In
addition, an evaluation framework is also established to evaluate the
performance of the submitted methods. During the challenge, 1272 results were
submitted, and finally, top-10 teams were selected to the final stage. We
analysis their results and summarize their methods in the paper. Since all
these teams submitted their source code in the challenge, a detailed ablation
study is also conducted to verify the effectiveness of the particular modules
proposed. We find many of the proposed techniques are practical for the
clinical diagnosis of glaucoma. As the first in-depth study of fundus \& OCT
multi-modality glaucoma grading, we believe the GAMMA Challenge will be an
essential starting point for future research.",0.29280955,-0.008943588,-0.24809912,A
1986,"work performs well for the general motion area without us-
Therefore, we hope some further researchers focus on more   ing extra datasets.","These results imply that our net-
make the existing VFI models signiÔ¨Åcantly more robust.","We also demonstrated that the proposed
fundamental natures of discontinuous motions and propose    architecture and data augmentation are effective solutions to
more optimal solutions.",2022-02-15 10:17:02+00:00,Beyond the Natural Motion: Exploring Discontinuity for Video Frame Interpolation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sangjin Lee'), arxiv.Result.Author('Hyeongmin Lee'), arxiv.Result.Author('Chajin Shin'), arxiv.Result.Author('Hanbin Son'), arxiv.Result.Author('Sangyoun Lee')]","Video frame interpolation(VFI) is the task that synthesizes the intermediate
frame given two consecutive frames. Most of the previous studies have focused
on appropriate frame warping operations and refinement modules for the warped
frames. These studies have been conducted on natural videos containing only
continuous motions. However, many practical videos contain various unnatural
objects with discontinuous motions such as logos, user interfaces and
subtitles. We propose three techniques to make the existing deep learning-based
VFI architectures robust to these elements. First is a novel data augmentation
strategy called figure-text mixing(FTM) which can make the models learn
discontinuous motions during training stage without any extra dataset. Second,
we propose a simple but effective module that predicts a map called
discontinuity map(D-map), which densely distinguishes between areas of
continuous and discontinuous motions. Lastly, we propose loss functions to give
supervisions of the discontinuous motion areas which can be applied along with
FTM and D-map. We additionally collect a special test benchmark called
Graphical Discontinuous Motion(GDM) dataset consisting of some mobile games and
chatting videos. Applied to the various state-of-the-art VFI networks, our
method significantly improves the interpolation qualities on the videos from
not only GDM dataset, but also the existing benchmarks containing only
continuous motions such as Vimeo90K, UCF101, and DAVIS.",-0.17320746,0.15115613,0.029644053,B
2013,"In H.
quires further study.","Nevertheless, this problem re-          derstanding and improving fast adversarial training.","Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
                                                                    H. Lin, editors, Advances in Neural Information Process-
                                                                    ing Systems, volume 33, pages 16048‚Äì16059.",2022-02-15 14:31:17+00:00,Random Walks for Adversarial Meshes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Amir Belder'), arxiv.Result.Author('Gal Yefet'), arxiv.Result.Author('Ran Ben Izhak'), arxiv.Result.Author('Ayellet Tal')]","A polygonal mesh is the most-commonly used representation of surfaces in
computer graphics; thus, a variety of classification networks have been
recently proposed. However, while adversarial attacks are wildly researched in
2D, almost no works on adversarial meshes exist. This paper proposes a novel,
unified, and general adversarial attack, which leads to misclassification of
numerous state-of-the-art mesh classification neural networks. Our attack
approach is black-box, i.e. it has access only to the network's predictions,
but not to the network's full architecture or gradients. The key idea is to
train a network to imitate a given classification network. This is done by
utilizing random walks along the mesh surface, which gather geometric
information. These walks provide insight onto the regions of the mesh that are
important for the correct prediction of the given classification network. These
mesh regions are then modified more than other regions in order to attack the
network in a manner that is barely visible to the naked eye.",0.0049817357,-0.3333187,0.088152274,C
2025,"We hope to spur
further research in the field and hence advance model diversity and generalization to people across sensitive groups.","Our comprehensive analyses show that large models using vast amounts of data (without
requiring any annotations or labels) perform best across all subgroups defined in fairness datasets.",Our assessment is intended to be used in conjunction with qualitative analysis of models‚Äô broader impact.,2022-02-15 17:45:33+00:00,Fairness Indicators for Systematic Assessments of Visual Feature Extractors,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Priya Goyal'), arxiv.Result.Author('Adriana Romero Soriano'), arxiv.Result.Author('Caner Hazirbas'), arxiv.Result.Author('Levent Sagun'), arxiv.Result.Author('Nicolas Usunier')]","Does everyone equally benefit from computer vision systems? Answers to this
question become more and more important as computer vision systems are deployed
at large scale, and can spark major concerns when they exhibit vast performance
discrepancies between people from various demographic and social backgrounds.
Systematic diagnosis of fairness, harms, and biases of computer vision systems
is an important step towards building socially responsible systems. To initiate
an effort towards standardized fairness audits, we propose three fairness
indicators, which aim at quantifying harms and biases of visual systems. Our
indicators use existing publicly available datasets collected for fairness
evaluations, and focus on three main types of harms and bias identified in the
literature, namely harmful label associations, disparity in learned
representations of social and demographic traits, and biased performance on
geographically diverse images from across the world.We define precise
experimental protocols applicable to a wide range of computer vision models.
These indicators are part of an ever-evolving suite of fairness probes and are
not intended to be a substitute for a thorough analysis of the broader impact
of the new computer vision technologies. Yet, we believe it is a necessary
first step towards (1) facilitating the widespread adoption and mandate of the
fairness assessments in computer vision research, and (2) tracking progress
towards building socially responsible models. To study the practical
effectiveness and broad applicability of our proposed indicators to any visual
system, we apply them to off-the-shelf models built using widely adopted model
training paradigms which vary in their ability to whether they can predict
labels on a given image or only produce the embeddings. We also systematically
study the effect of data domain and model size.",0.18839699,-0.16455132,-0.20424744,A
2095,"4.2, we further study the performance on many down-
                                                                             stream tasks in computer vision including out-of-domain
   We use open source VISSL library [49] for our model                       robustness in Sec.",Pretraining the SEER model                                              Sec.,"4.2.2, Ô¨Åne-grained image recognition in
training and implement FSDP and activation checkpoint-                       Sec.",2022-02-16 22:26:47+00:00,Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Priya Goyal'), arxiv.Result.Author('Quentin Duval'), arxiv.Result.Author('Isaac Seessel'), arxiv.Result.Author('Mathilde Caron'), arxiv.Result.Author('Mannat Singh'), arxiv.Result.Author('Ishan Misra'), arxiv.Result.Author('Levent Sagun'), arxiv.Result.Author('Armand Joulin'), arxiv.Result.Author('Piotr Bojanowski')]","Discriminative self-supervised learning allows training models on any random
group of internet images, and possibly recover salient information that helps
differentiate between the images. Applied to ImageNet, this leads to object
centric features that perform on par with supervised features on most
object-centric downstream tasks. In this work, we question if using this
ability, we can learn any salient and more representative information present
in diverse unbounded set of images from across the globe. To do so, we train
models on billions of random images without any data pre-processing or prior
assumptions about what we want the model to learn. We scale our model size to
dense 10 billion parameters to avoid underfitting on a large data size. We
extensively study and validate our model performance on over 50 benchmarks
including fairness, robustness to distribution shift, geographical diversity,
fine grained recognition, image copy detection and many image classification
datasets. The resulting model, not only captures well semantic information, it
also captures information about artistic style and learns salient information
such as geolocations and multilingual word embeddings based on visual content
only. More importantly, we discover that such model is more robust, more fair,
less harmful and less biased than supervised models or models trained on object
centric datasets such as ImageNet.",-0.22851002,0.003137039,0.10242475,B
2096,"4.2, we further study the performance on many down-
                                                                             stream tasks in computer vision including out-of-domain
   We use open source VISSL library [49] for our model                       robustness in Sec.",Pretraining the SEER model                                              Sec.,"4.2.2, Ô¨Åne-grained image recognition in
training and implement FSDP and activation checkpoint-                       Sec.",2022-02-16 22:26:47+00:00,Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Priya Goyal'), arxiv.Result.Author('Quentin Duval'), arxiv.Result.Author('Isaac Seessel'), arxiv.Result.Author('Mathilde Caron'), arxiv.Result.Author('Ishan Misra'), arxiv.Result.Author('Levent Sagun'), arxiv.Result.Author('Armand Joulin'), arxiv.Result.Author('Piotr Bojanowski')]","Discriminative self-supervised learning allows training models on any random
group of internet images, and possibly recover salient information that helps
differentiate between the images. Applied to ImageNet, this leads to object
centric features that perform on par with supervised features on most
object-centric downstream tasks. In this work, we question if using this
ability, we can learn any salient and more representative information present
in diverse unbounded set of images from across the globe. To do so, we train
models on billions of random images without any data pre-processing or prior
assumptions about what we want the model to learn. We scale our model size to
dense 10 billion parameters to avoid underfitting on a large data size. We
extensively study and validate our model performance on over 50 benchmarks
including fairness, robustness to distribution shift, geographical diversity,
fine grained recognition, image copy detection and many image classification
datasets. The resulting model, not only captures well semantic information, it
also captures information about artistic style and learns salient information
such as geolocations and multilingual word embeddings based on visual content
only. More importantly, we discover that such model is more robust, more fair,
less harmful and less biased than supervised models or models trained on object
centric datasets such as ImageNet.",-0.22851002,0.003137039,0.10242475,B
2204,"[7] constructed a dedicated        is necessary to further develop image datasets
Personal Protective Equipment (PPE) image                containing workers, machines, material, and
dataset Color Helmet and Vest (CHV) composed             layouts for further research on deep learning in
of 1,330 images.",Wang et al.,They used eight YOLO                    the construction industry.,2022-02-19 09:09:23+00:00,SODA: Site Object Detection dAtaset for Deep Learning in Construction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Rui Duan'), arxiv.Result.Author('Hui Deng'), arxiv.Result.Author('Mao Tian'), arxiv.Result.Author('Yichuan Deng'), arxiv.Result.Author('Jiarui Lin')]","Computer vision-based deep learning object detection algorithms have been
developed sufficiently powerful to support the ability to recognize various
objects. Although there are currently general datasets for object detection,
there is still a lack of large-scale, open-source dataset for the construction
industry, which limits the developments of object detection algorithms as they
tend to be data-hungry. Therefore, this paper develops a new large-scale image
dataset specifically collected and annotated for the construction site, called
Site Object Detection dAtaset (SODA), which contains 15 kinds of object classes
categorized by workers, materials, machines, and layout. Firstly, more than
20,000 images were collected from multiple construction sites in different site
conditions, weather conditions, and construction phases, which covered
different angles and perspectives. After careful screening and processing,
19,846 images including 286,201 objects were then obtained and annotated with
labels in accordance with predefined categories. Statistical analysis shows
that the developed dataset is advantageous in terms of diversity and volume.
Further evaluation with two widely-adopted object detection algorithms based on
deep learning (YOLO v3/ YOLO v4) also illustrates the feasibility of the
dataset for typical construction scenarios, achieving a maximum mAP of 81.47%.
In this manner, this research contributes a large-scale image dataset for the
development of deep learning-based object detection methods in the construction
industry and sets up a performance benchmark for further evaluation of
corresponding algorithms in this area.",-0.23774914,-0.11954376,0.10891161,C
2237,"We plan to extend our method in further research to multi-view images or sparse
signals from sources not used in this paper, for example, radar, feature matching, and time-of-flight
range cameras.","This study confirms that the proposed LGSM method is suitable for the fusion of stereo images
and LiDAR data.","Acknowledgments

   We would like to thank the editors and the anonymous reviewers for their comments and
contributions, Dr. Yingsong Li for providing the source code of SGM and AD-Census, and
Guangzhou Jiantong Surveying and Mapping Geoinformation Technology Co., Ltd for providing
the Guangzhou Stereo Dataset.",2022-02-21 02:29:19+00:00,LiDAR-guided Stereo Matching with a Spatial Consistency Constraint,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Yongjun Zhang'), arxiv.Result.Author('Siyuan Zou'), arxiv.Result.Author('Xinyi Liu'), arxiv.Result.Author('Xu Huang'), arxiv.Result.Author('Yi Wan'), arxiv.Result.Author('Yongxiang Yao')]","The complementary fusion of light detection and ranging (LiDAR) data and
image data is a promising but challenging task for generating high-precision
and high-density point clouds. This study proposes an innovative LiDAR-guided
stereo matching approach called LiDAR-guided stereo matching (LGSM), which
considers the spatial consistency represented by continuous disparity or depth
changes in the homogeneous region of an image. The LGSM first detects the
homogeneous pixels of each LiDAR projection point based on their color or
intensity similarity. Next, we propose a riverbed enhancement function to
optimize the cost volume of the LiDAR projection points and their homogeneous
pixels to improve the matching robustness. Our formulation expands the
constraint scopes of sparse LiDAR projection points with the guidance of image
information to optimize the cost volume of pixels as much as possible. We
applied LGSM to semi-global matching and AD-Census on both simulated and real
datasets. When the percentage of LiDAR points in the simulated datasets was
0.16%, the matching accuracy of our method achieved a subpixel level, while
that of the original stereo matching algorithm was 3.4 pixels. The experimental
results show that LGSM is suitable for indoor, street, aerial, and satellite
image datasets and provides good transferability across semi-global matching
and AD-Census. Furthermore, the qualitative and quantitative evaluations
demonstrate that LGSM is superior to two state-of-the-art optimizing cost
volume methods, especially in reducing mismatches in difficult matching areas
and refining the boundaries of objects.",-0.11143486,0.38649255,0.0928532,B
2250,"In conclusion, our method opens
the possibility of further research in developing more robust nuclei classiÔ¨Åcation models that can scale well on all kind
of datasets.",Approaches can differ with better classiÔ¨Åers as well.,"7 Acknowledgments

This research was carried out in Indian Institute of Information Technology, Allahabad and supported by the Ministry
of Human Resource and Development, Government of India.",2022-02-21 12:39:37+00:00,Cell nuclei classification in histopathological images using hybrid OLConvNet,cs.CV,['cs.CV'],"[arxiv.Result.Author('Suvidha Tripathi'), arxiv.Result.Author('Satish Kumar Singh')]","Computer-aided histopathological image analysis for cancer detection is a
major research challenge in the medical domain. Automatic detection and
classification of nuclei for cancer diagnosis impose a lot of challenges in
developing state of the art algorithms due to the heterogeneity of cell nuclei
and data set variability. Recently, a multitude of classification algorithms
has used complex deep learning models for their dataset. However, most of these
methods are rigid and their architectural arrangement suffers from
inflexibility and non-interpretability. In this research article, we have
proposed a hybrid and flexible deep learning architecture OLConvNet that
integrates the interpretability of traditional object-level features and
generalization of deep learning features by using a shallower Convolutional
Neural Network (CNN) named as $CNN_{3L}$. $CNN_{3L}$ reduces the training time
by training fewer parameters and hence eliminating space constraints imposed by
deeper algorithms. We used F1-score and multiclass Area Under the Curve (AUC)
performance parameters to compare the results. To further strengthen the
viability of our architectural approach, we tested our proposed methodology
with state of the art deep learning architectures AlexNet, VGG16, VGG19,
ResNet50, InceptionV3, and DenseNet121 as backbone networks. After a
comprehensive analysis of classification results from all four architectures,
we observed that our proposed model works well and perform better than
contemporary complex algorithms.",0.32120863,-0.055439524,-0.11938092,A
2256,"hope that our studies spark further research about
                                              the Ô¨Çattening strategy of image reading.","Finally, we     constitutes the image.","The key information on the image can be considered as an
                                                                                                     undirected graph (Bronstein et al., 2021).",2022-02-21 13:53:04+00:00,Rethinking the Zigzag Flattening for Image Reading,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qingsong Zhao'), arxiv.Result.Author('Zhipeng Zhou'), arxiv.Result.Author('Shuguang Dou'), arxiv.Result.Author('Yangguang Li'), arxiv.Result.Author('Rui Lu'), arxiv.Result.Author('Yin Wang'), arxiv.Result.Author('Cairong Zhao')]","Sequence ordering of word vector matters a lot to text reading, which has
been proven in natural language processing (NLP). However, the rule of
different sequence ordering in computer vision (CV) was not well explored,
e.g., why the ""zigzag"" flattening (ZF) is commonly utilized as a default option
to get the image patches ordering in vision transformers (ViTs). Notably, when
decomposing multi-scale images, the ZF could not maintain the invariance of
feature point positions. To this end, we investigate the Hilbert fractal
flattening (HF) as another method for sequence ordering in CV and contrast it
against ZF. The HF has proven to be superior to other curves in maintaining
spatial locality, when performing multi-scale transformations of dimensional
space. And it can be easily plugged into most deep neural networks (DNNs).
Extensive experiments demonstrate that it can yield consistent and significant
performance boosts for a variety of architectures. Finally, we hope that our
studies spark further research about the flattening strategy of image reading.",-0.04210005,0.052050427,-0.13827847,C
2257,"Finally, we hope that our studies
                                                   spark further research about the Ô¨Çattening strategy of image reading.","Extensive experiments demonstrate that it can yield consistent
                                                   and signiÔ¨Åcant performance boosts for a variety of architectures.","1 Introduction

                                        Humans usually read text by row or by column, but how do you ‚Äúread"" a 2D image?",2022-02-21 13:53:04+00:00,Rethinking the Zigzag Flattening for Image Reading,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qingsong Zhao'), arxiv.Result.Author('Zhipeng Zhou'), arxiv.Result.Author('Shuguang Dou'), arxiv.Result.Author('Yangguang Li'), arxiv.Result.Author('Rui Lu'), arxiv.Result.Author('Yin Wang'), arxiv.Result.Author('Cairong Zhao')]","Sequence ordering of word vector matters a lot to text reading, which has
been proven in natural language processing (NLP). However, the rule of
different sequence ordering in computer vision (CV) was not well explored,
e.g., why the ""zigzag"" flattening (ZF) is commonly utilized as a default option
to get the image patches ordering in vision transformers (ViTs). Notably, when
decomposing multi-scale images, the ZF could not maintain the invariance of
feature point positions. To this end, we investigate the Hilbert fractal
flattening (HF) as another method for sequence ordering in CV and contrast it
against ZF. The HF has proven to be superior to other curves in maintaining
spatial locality, when performing multi-scale transformations of dimensional
space. And it can be easily plugged into most deep neural networks (DNNs).
Extensive experiments demonstrate that it can yield consistent and significant
performance boosts for a variety of architectures. Finally, we hope that our
studies spark further research about the flattening strategy of image reading.",-0.029320817,0.03604201,-0.03460816,C
2263,"The most popular and sample efÔ¨Åcient DRL
methods nowadays, such as SAC [23] and DrQv2 [93] do not yet have built-in procedures that aim to
tackle these issues and much further research is needed to develop agents that are both efÔ¨Åcient and
reliable.","In a multi-stage training setting, special care should be
taken on each and every training stage to ensure safety.",41,2022-02-17 09:51:32+00:00,VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Che Wang'), arxiv.Result.Author('Xufang Luo'), arxiv.Result.Author('Keith Ross'), arxiv.Result.Author('Dongsheng Li')]","We propose VRL3, a powerful data-driven framework with a simple design for
solving challenging visual deep reinforcement learning (DRL) tasks. We analyze
a number of major obstacles in taking a data-driven approach, and present a
suite of design principles, novel findings, and critical insights about
data-driven visual DRL. Our framework has three stages: in stage 1, we leverage
non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;
in stage 2, we use offline RL data (e.g. a limited number of expert
demonstrations) to convert the task-agnostic representations into more powerful
task-specific representations; in stage 3, we fine-tune the agent with online
RL. On a set of challenging hand manipulation tasks with sparse reward and
realistic visual inputs, compared to the previous SOTA, VRL3 achieves an
average of 780% better sample efficiency. And on the hardest task, VRL3 is
1220% more sample efficient (2440% when using a wider encoder) and solves the
task with only 10% of the computation. These significant results clearly
demonstrate the great potential of data-driven deep reinforcement learning.",0.24213609,-0.011245756,-0.105743326,A
2290,"This statement also
          opens up a scope of further research in the domain to determine why transfer learning works better and what
          could be improved in end-to-end learning to achieve the same amount of performance.","These results, except VGG19, shows that a marked improvement can be achieved
          with transfer learning DL features instead of using end-to-end networks for classiÔ¨Åcation.",4.,2022-02-22 06:48:50+00:00,Ensembling Handcrafted Features with Deep Features: An Analytical Study for Classification of Routine Colon Cancer Histopathological Nuclei Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Suvidha Tripathi'), arxiv.Result.Author('Satish Kumar Singh')]","The use of Deep Learning (DL) based methods in medical histopathology images
have been one of the most sought after solutions to classify, segment, and
detect diseased biopsy samples. However, given the complex nature of medical
datasets due to the presence of intra-class variability and heterogeneity, the
use of complex DL models might not give the optimal performance up to the level
which is suitable for assisting pathologists. Therefore, ensemble DL methods
with the scope of including domain agnostic handcrafted Features (HC-F)
inspired this work. We have, through experiments, tried to highlight that a
single DL network (domain-specific or state of the art pre-trained models)
cannot be directly used as the base model without proper analysis with the
relevant dataset. We have used F1-measure, Precision, Recall, AUC, and
Cross-Entropy Loss to analyse the performance of our approaches. We observed
from the results that the DL features ensemble bring a marked improvement in
the overall performance of the model, whereas, domain agnostic HC-F remains
dormant on the performance of the DL models.",0.04960335,-0.36995044,0.06439031,C
2307,Our virtual environment is released to the public for further study in the community.,"2) We build a 3D virtual environment to mimic real-world multi-object active tracking scenes, which enables
           the training of agents on active tracking tasks and also has a potential of generalization to more complicated
           scenes.","2 Related Work

In this section, we discuss related work from three aspects: traditional solutions to AOT, and deep RL for single-camera
and multi-camera AOT.",2022-02-22 13:28:40+00:00,Coordinate-Aligned Multi-Camera Collaboration for Active Multi-Object Tracking,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Zeyu Fang'), arxiv.Result.Author('Jian Zhao'), arxiv.Result.Author('Mingyu Yang'), arxiv.Result.Author('Wengang Zhou'), arxiv.Result.Author('Zhenbo Lu'), arxiv.Result.Author('Houqiang Li')]","Active Multi-Object Tracking (AMOT) is a task where cameras are controlled by
a centralized system to adjust their poses automatically and collaboratively so
as to maximize the coverage of targets in their shared visual field. In AMOT,
each camera only receives partial information from its observation, which may
mislead cameras to take locally optimal action. Besides, the global goal, i.e.,
maximum coverage of objects, is hard to be directly optimized. To address the
above issues, we propose a coordinate-aligned multi-camera collaboration system
for AMOT. In our approach, we regard each camera as an agent and address AMOT
with a multi-agent reinforcement learning solution. To represent the
observation of each agent, we first identify the targets in the camera view
with an image detector, and then align the coordinates of the targets in 3D
environment. We define the reward of each agent based on both global coverage
as well as four individual reward terms. The action policy of the agents is
derived with a value-based Q-network. To the best of our knowledge, we are the
first to study the AMOT task. To train and evaluate the efficacy of our system,
we build a virtual yet credible 3D environment, named ""Soccer Court"", to mimic
the real-world AMOT scenario. The experimental results show that our system
achieves a coverage of 71.88%, outperforming the baseline method by 8.9%.",-0.3433964,0.17345381,-0.15417656,B
2323,"tion layer in the main network takes as input the d-dimensional
Although this works well in practice, this issue is still an open  output embedding of the hypernetwork and linearly projects it
question in the general case and may be an important direction     to the size of the kernel and bias, where the parameters of the
for further research.","In this work, we tuned the       dimension and output dimension are of size d. Each convolu-
scaling factors to align the magnitudes of the separate terms.",projection are learned.,2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.04556607,-0.14902672,0.3459795,C
2324,"We Ô¨Ånd that the performance improvement achieved             We believe this work opens many interesting directions
by DHS is less for the large hypernetwork, validating the ex-     for further research.","DHS as compared to UHS, given a Ô¨Åxed hypernetwork ca-
pacity.","A straight-forward and promising future
pectation that the sampling distribution plays a more important   direction is to extend this to a higher number of loss coefÔ¨Åcient
role when the hypernetwork capacity is restricted.",2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.1970413,-0.053154815,0.25733215,A
2325,"Each
panel shows metric performance for Unet and HyperRecon-L models for varying Œª.

this issue is still an open question in the general case and may be an important direction
for further research.",Higher is better.,5.,2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.4078785,-0.010105087,0.04642672,A
2326,We believe this work opens many interesting directions for further research.,"We high-
light and address several issues related to hypernetwork training associated with the recon-
struction setting, and demonstrate empirically that our method works on two datasets and
on a variety of multi-term loss functions and reconstruction tasks.","A straight-
forward and promising future direction is to extend this to a higher number of loss coeÔ¨Écient
hyperparameters.",2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.031571303,-0.18512382,0.28290868,C
2327,"Although this works well in practice,

                                                                8
       Computing Multiple Image Reconstructions with a Single Hypernetwork

this issue is still an open question in the general case and may be an important direction
for further research.","In this work, we tuned the scaling
factors to align the magnitudes of the separate terms.",5.,2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",-0.03965384,0.122981995,0.33755144,C
2328,We believe this work opens many interesting directions for further research.,"We high-
light and address several issues related to hypernetwork training associated with the recon-
struction setting, and demonstrate empirically that our method works on two datasets and
on a variety of multi-term loss functions and reconstruction tasks.","A straight-
forward and promising future direction is to extend this to a higher number of loss coeÔ¨Écient
hyperparameters.",2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.031571303,-0.18512382,0.28290868,C
2329,"Although this works well in practice,

                                                                8
       Computing Multiple Image Reconstructions with a Single Hypernetwork

this issue is still an open question in the general case and may be an important direction
for further research.","In this work, we tuned the scaling
factors to align the magnitudes of the separate terms.",5.,2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",-0.03965384,0.122981995,0.33755144,C
2330,We believe this work opens many interesting directions for further research.,"We high-
light and address several issues related to hypernetwork training associated with the recon-
struction setting, and demonstrate empirically that our method works on two datasets and
on a variety of multi-term loss functions and reconstruction tasks.","A straight-
forward and promising future direction is to extend this to a higher number of loss coeÔ¨Écient
hyperparameters.",2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.031571303,-0.18512382,0.28290868,C
2331,"Although this works well in practice,

                                                                8
       Computing Multiple Image Reconstructions with a Single Hypernetwork

this issue is still an open question in the general case and may be an important direction
for further research.","In this work, we tuned the scaling
factors to align the magnitudes of the separate terms.",5.,2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",-0.03965384,0.122981995,0.33755144,C
2332,We believe this work opens many interesting directions for further research.,"We high-
light and address several issues related to hypernetwork training associated with the recon-
struction setting, and demonstrate empirically that our method works on two datasets and
on a variety of multi-term loss functions and reconstruction tasks.","A straight-
forward and promising future direction is to extend this to a higher number of loss coeÔ¨Écient
hyperparameters.",2022-02-22 16:27:23+00:00,Computing Multiple Image Reconstructions with a Single Hypernetwork,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Alan Q. Wang'), arxiv.Result.Author('Adrian V. Dalca'), arxiv.Result.Author('Mert R. Sabuncu')]","Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.",0.031571303,-0.18512382,0.28290868,C
2358,"In conclusion, we hope that  REFERENCES
our attempt can become a stepping stone for further research in
the field of deepfake detection focused on facemask dataset.","Furthermore, this work can this work can be further de-
difficulty in detecting NeuralTextures method whether the images      veloped to create new facemask dataset for deepfake images to
Deepfake Detection for Facial Images with Facemasks                                                             Conference‚Äô17, July 2017, Washington, DC, USA

generalize the current pandemic state.","[1] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen.",2022-02-23 09:01:27+00:00,Deepfake Detection for Facial Images with Facemasks,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Donggeun Ko'), arxiv.Result.Author('Sangjun Lee'), arxiv.Result.Author('Jinyong Park'), arxiv.Result.Author('Saebyeol Shin'), arxiv.Result.Author('Donghee Hong'), arxiv.Result.Author('Simon S. Woo')]","Hyper-realistic face image generation and manipulation have givenrise to
numerous unethical social issues, e.g., invasion of privacy,threat of security,
and malicious political maneuvering, which re-sulted in the development of
recent deepfake detection methods with the rising demands of deepfake
forensics. Proposed deepfake detection methods to date have shown remarkable
detection performance and robustness. However, none of the suggested deepfake
detection methods assessed the performance of deepfakes with the facemask
during the pandemic crisis after the outbreak of theCovid-19. In this paper, we
thoroughly evaluate the performance of state-of-the-art deepfake detection
models on the deepfakes with the facemask. Also, we propose two approaches to
enhance the masked deepfakes detection: face-patch and face-crop. The
experimental evaluations on both methods are assessed through the base-line
deepfake detection models on the various deepfake datasets. Our extensive
experiments show that, among the two methods, face-crop performs better than
the face-patch, and could be a train method for deepfake detection models to
detect fake faces with facemask in real world.",-0.15255009,-0.14736587,0.047479797,C
2410,"Additionally, since two cameras are used, it is worth          deep convolutional neural Ô¨Åelds,‚Äù IEEE transactions on
further researching estimating depth in a stereo method.","Secondly, the disparity between the      [5] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid,
two images still exists, interfering with accurate depth esti-         ‚ÄúLearning depth from single monocular images using
mation.","It            pattern analysis and machine intelligence, vol.",2022-02-24 14:06:46+00:00,Light Robust Monocular Depth Estimation For Outdoor Environment Via Monochrome And Color Camera Fusion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hyeonsoo Jang'), arxiv.Result.Author('Yeongmin Ko'), arxiv.Result.Author('Younkwan Lee'), arxiv.Result.Author('Moongu Jeon')]","Depth estimation plays a important role in SLAM, odometry, and autonomous
driving. Especially, monocular depth estimation is profitable technology
because of its low cost, memory, and computation. However, it is not a
sufficiently predicting depth map due to a camera often failing to get a clean
image because of light conditions. To solve this problem, various sensor fusion
method has been proposed. Even though it is a powerful method, sensor fusion
requires expensive sensors, additional memory, and high computational
performance.
  In this paper, we present color image and monochrome image pixel-level fusion
and stereo matching with partially enhanced correlation coefficient
maximization. Our methods not only outperform the state-of-the-art works across
all metrics but also efficient in terms of cost, memory, and computation. We
also validate the effectiveness of our design with an ablation study.",-0.28320774,0.15677083,0.14476463,B
2436,"However, further research is required to
  determine if this is also the case for MAD.","This observation is also in line with research done on the effectiveness of courses in
  facial comparisons, where the researchers suggest that examination-based training could be
  an important part of a training programme [56].","We thus answer the initial questions posed in this article:

‚Ä¢ How good are people who are checking identity or identity documents for Governments
  today, in their daily work, at detecting morphing attacks?",2022-02-24 23:46:22+00:00,Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?,cs.CV,"['cs.CV', 'cs.CY']","[arxiv.Result.Author('Sankini Rancha Godage'), arxiv.Result.Author('Fr√∏y L√∏v√•sda'), arxiv.Result.Author('Sushma Venkatesh'), arxiv.Result.Author('Kiran Raja'), arxiv.Result.Author('Raghavendra Ramachandra'), arxiv.Result.Author('Christoph Busch')]","While several works have studied the vulnerability of automated FRS and have
proposed morphing attack detection (MAD) methods, very few have focused on
studying the human ability to detect morphing attacks. The examiner/observer's
face morph detection ability is based on their observation, domain knowledge,
experience, and familiarity with the problem, and no works report the detailed
findings from observers who check identity documents as a part of their
everyday professional life. This work creates a new benchmark database of
realistic morphing attacks from 48 unique subjects leading to 400 morphed
images presented to the observers in a Differential-MAD (D-MAD) setting. Unlike
the existing databases, the newly created morphed image database has been
created with careful considerations to age, gender and ethnicity to create
realistic morph attacks. Further, unlike the previous works, we also capture
ten images from Automated Border Control (ABC) gates to mimic the realistic
D-MAD setting leading to 400 probe images in border crossing scenarios. The
newly created dataset is further used to study the ability of human observers'
ability to detect morphed images. In addition, a new dataset of 180 morphed
images is also created using the FRGCv2 dataset under the Single Image-MAD
(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a
new evaluation platform is created to conduct S-MAD and D-MAD analysis. The
benchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who
are primarily governmental employees from more than 40 countries. The analysis
provides interesting insights and points to expert observers' missing
competence and failure to detect a considerable amount of morphing attacks.
Human observers tend to detect morphed images to a lower accuracy as compared
to the automated MAD algorithms evaluated in this work.",0.024884447,-0.060303938,-0.22402854,C
2437,"However, further research is required to
  determine if this is also the case for MAD.","This observation is also in line with research done on the effectiveness of courses in
  facial comparisons, where the researchers suggest that examination-based training could be
  an important part of a training programme [56].","We thus answer the initial questions posed in this article:

‚Ä¢ How good are people who are checking identity or identity documents for Governments
  today, in their daily work, at detecting morphing attacks?",2022-02-24 23:46:22+00:00,Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?,cs.CV,"['cs.CV', 'cs.CY']","[arxiv.Result.Author('Sankini Rancha Godage'), arxiv.Result.Author('Fr√∏y L√∏v√•sda'), arxiv.Result.Author('Sushma Venkatesh'), arxiv.Result.Author('Kiran Raja'), arxiv.Result.Author('Raghavendra Ramachandra'), arxiv.Result.Author('Christoph Busch')]","While several works have studied the vulnerability of automated FRS and have
proposed morphing attack detection (MAD) methods, very few have focused on
studying the human ability to detect morphing attacks. The examiner/observer's
face morph detection ability is based on their observation, domain knowledge,
experience, and familiarity with the problem, and no works report the detailed
findings from observers who check identity documents as a part of their
everyday professional life. This work creates a new benchmark database of
realistic morphing attacks from 48 unique subjects leading to 400 morphed
images presented to the observers in a Differential-MAD (D-MAD) setting. Unlike
the existing databases, the newly created morphed image database has been
created with careful considerations to age, gender and ethnicity to create
realistic morph attacks. Further, unlike the previous works, we also capture
ten images from Automated Border Control (ABC) gates to mimic the realistic
D-MAD setting leading to 400 probe images in border crossing scenarios. The
newly created dataset is further used to study the ability of human observers'
ability to detect morphed images. In addition, a new dataset of 180 morphed
images is also created using the FRGCv2 dataset under the Single Image-MAD
(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a
new evaluation platform is created to conduct S-MAD and D-MAD analysis. The
benchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who
are primarily governmental employees from more than 40 countries. The analysis
provides interesting insights and points to expert observers' missing
competence and failure to detect a considerable amount of morphing attacks.
Human observers tend to detect morphed images to a lower accuracy as compared
to the automated MAD algorithms evaluated in this work.",0.024884507,-0.060303926,-0.22402853,C
2438,"However, further research is required to
  determine if this is also the case for MAD.","This observation is also in line with research done on the effectiveness of courses in
  facial comparisons, where the researchers suggest that examination-based training could be
  an important part of a training programme [56].","We thus answer the initial questions posed in this article:

‚Ä¢ How good are people who are checking identity or identity documents for Governments
  today, in their daily work, at detecting morphing attacks?",2022-02-24 23:46:22+00:00,Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?,cs.CV,"['cs.CV', 'cs.CY']","[arxiv.Result.Author('Sankini Rancha Godage'), arxiv.Result.Author('Fr√∏y L√∏v√•sdal'), arxiv.Result.Author('Sushma Venkatesh'), arxiv.Result.Author('Kiran Raja'), arxiv.Result.Author('Raghavendra Ramachandra'), arxiv.Result.Author('Christoph Busch')]","While several works have studied the vulnerability of automated FRS and have
proposed morphing attack detection (MAD) methods, very few have focused on
studying the human ability to detect morphing attacks. The examiner/observer's
face morph detection ability is based on their observation, domain knowledge,
experience, and familiarity with the problem, and no works report the detailed
findings from observers who check identity documents as a part of their
everyday professional life. This work creates a new benchmark database of
realistic morphing attacks from 48 unique subjects leading to 400 morphed
images presented to the observers in a Differential-MAD (D-MAD) setting. Unlike
the existing databases, the newly created morphed image database has been
created with careful considerations to age, gender and ethnicity to create
realistic morph attacks. Further, unlike the previous works, we also capture
ten images from Automated Border Control (ABC) gates to mimic the realistic
D-MAD setting leading to 400 probe images in border crossing scenarios. The
newly created dataset is further used to study the ability of human observers'
ability to detect morphed images. In addition, a new dataset of 180 morphed
images is also created using the FRGCv2 dataset under the Single Image-MAD
(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a
new evaluation platform is created to conduct S-MAD and D-MAD analysis. The
benchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who
are primarily governmental employees from more than 40 countries. The analysis
provides interesting insights and points to expert observers' missing
competence and failure to detect a considerable amount of morphing attacks.
Human observers tend to detect morphed images to a lower accuracy as compared
to the automated MAD algorithms evaluated in this work.",0.024884507,-0.060303926,-0.22402853,C
2439,"2) How to quantitatively decide
                                                                   an optimal stride conÔ¨Åguration is an interesting problem that
     AlexNet    ConÔ¨Åguration  Natural  PGD-20                      needs further research.",version will be preferred.,"From our understanding, the receptive
     VGG16                                                         Ô¨Åeld should be taken into account.",2022-02-25 00:14:59+00:00,Understanding Adversarial Robustness from Feature Maps of Convolutional Layers,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Cong Xu'), arxiv.Result.Author('Min Yang')]","The adversarial robustness of a neural network mainly relies on two factors,
one is the feature representation capacity of the network, and the other is its
resistance ability to perturbations. In this paper, we study the
anti-perturbation ability of the network from the feature maps of convolutional
layers. Our theoretical analysis discovers that larger convolutional features
before average pooling can contribute to better resistance to perturbations,
but the conclusion is not true for max pooling. Based on the theoretical
findings, we present two feasible ways to improve the robustness of existing
neural networks. The proposed approaches are very simple and only require
upsampling the inputs or modifying the stride configuration of convolution
operators. We test our approaches on several benchmark neural network
architectures, including AlexNet, VGG16, RestNet18 and PreActResNet18, and
achieve non-trivial improvements on both natural accuracy and robustness under
various attacks. Our study brings new insights into the design of robust neural
networks. The code is available at \url{https://github.com/MTandHJ/rcm}.",0.12249864,0.020532124,0.12913758,A
2465,The code has been released to facilitate further research along this line1.,"As a
        side product, we achieve a new state-of-the-art on two publicly available VQA-CP benchmarks for the VQA task.","Our prior work [24] presents a de-bias loss function for tackling the language prior problem in VQA, and this paper
extends it in the following aspects: 1) We formally define and comprehensively study the modality bias problem for
multi-modal learning, while [24] focuses on the VQA task only.",2022-02-25 13:47:09+00:00,On Modality Bias Recognition and Reduction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yangyang Guo'), arxiv.Result.Author('Liqiang Nie'), arxiv.Result.Author('Harry Cheng'), arxiv.Result.Author('Zhiyong Cheng'), arxiv.Result.Author('Mohan Kankanhalli'), arxiv.Result.Author('Alberto Del Bimbo')]","Making each modality in multi-modal data contribute is of vital importance to
learning a versatile multi-modal model. Existing methods, however, are often
dominated by one or few of modalities during model training, resulting in
sub-optimal performance. In this paper, we refer to this problem as modality
bias and attempt to study it in the context of multi-modal classification
systematically and comprehensively. After stepping into several empirical
analysis, we recognize that one modality affects the model prediction more just
because this modality has a spurious correlation with instance labels. In order
to primarily facilitate the evaluation on the modality bias problem, we
construct two datasets respectively for the colored digit recognition and video
action recognition tasks in line with the Out-of-Distribution (OoD) protocol.
Collaborating with the benchmarks in the visual question answering task, we
empirically justify the performance degradation of the existing methods on
these OoD datasets, which serves as evidence to justify the modality bias
learning. In addition, to overcome this problem, we propose a plug-and-play
loss function method, whereby the feature space for each label is adaptively
learned according to the training set statistics. Thereafter, we apply this
method on eight baselines in total to test its effectiveness. From the results
on four datasets regarding the above three tasks, our method yields remarkable
performance improvements compared with the baselines, demonstrating its
superiority on reducing the modality bias problem.",0.09282411,-0.3614248,0.034822814,C
2466,"In addition, tackling the modality bias problem
from the other two stages, i.e., multi-modal representation and fusion, demands our further research attention as well.","With the recognition of this problem in multi-modal classification, exploring its associated expression for generation
tasks will open an interesting door to increase the output diversity.","17
Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY                                                   In Submission

A PROOF FOR THE SCALING FACTOR S

Without loss of generality, let ùëÉùëñ denotes the expected minimum of the class ùëñ.",2022-02-25 13:47:09+00:00,On Modality Bias Recognition and Reduction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yangyang Guo'), arxiv.Result.Author('Liqiang Nie'), arxiv.Result.Author('Harry Cheng'), arxiv.Result.Author('Zhiyong Cheng'), arxiv.Result.Author('Mohan Kankanhalli'), arxiv.Result.Author('Alberto Del Bimbo')]","Making each modality in multi-modal data contribute is of vital importance to
learning a versatile multi-modal model. Existing methods, however, are often
dominated by one or few of modalities during model training, resulting in
sub-optimal performance. In this paper, we refer to this problem as modality
bias and attempt to study it in the context of multi-modal classification
systematically and comprehensively. After stepping into several empirical
analysis, we recognize that one modality affects the model prediction more just
because this modality has a spurious correlation with instance labels. In order
to primarily facilitate the evaluation on the modality bias problem, we
construct two datasets respectively for the colored digit recognition and video
action recognition tasks in line with the Out-of-Distribution (OoD) protocol.
Collaborating with the benchmarks in the visual question answering task, we
empirically justify the performance degradation of the existing methods on
these OoD datasets, which serves as evidence to justify the modality bias
learning. In addition, to overcome this problem, we propose a plug-and-play
loss function method, whereby the feature space for each label is adaptively
learned according to the training set statistics. Thereafter, we apply this
method on eight baselines in total to test its effectiveness. From the results
on four datasets regarding the above three tasks, our method yields remarkable
performance improvements compared with the baselines, demonstrating its
superiority on reducing the modality bias problem.",0.19244875,-0.19816011,-0.04516881,A
2467,The code has been released to facilitate further research along this line1.,"As a
        side product, we achieve a new state-of-the-art on two publicly available VQA-CP benchmarks for the VQA task.","Our prior work [26] presents a de-bias loss function for tackling the language prior problem in VQA, and this paper
extends it in the following aspects: 1) We formally define and comprehensively study the modality bias problem for
multi-modal learning, while [26] focuses on the VQA task only.",2022-02-25 13:47:09+00:00,On Modality Bias Recognition and Reduction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yangyang Guo'), arxiv.Result.Author('Liqiang Nie'), arxiv.Result.Author('Harry Cheng'), arxiv.Result.Author('Zhiyong Cheng'), arxiv.Result.Author('Mohan Kankanhalli'), arxiv.Result.Author('Alberto Del Bimbo')]","Making each modality in multi-modal data contribute is of vital importance to
learning a versatile multi-modal model. Existing methods, however, are often
dominated by one or few of modalities during model training, resulting in
sub-optimal performance. In this paper, we refer to this problem as modality
bias and attempt to study it in the context of multi-modal classification
systematically and comprehensively. After stepping into several empirical
analysis, we recognize that one modality affects the model prediction more just
because this modality has a spurious correlation with instance labels. In order
to primarily facilitate the evaluation on the modality bias problem, we
construct two datasets respectively for the colored digit recognition and video
action recognition tasks in line with the Out-of-Distribution (OoD) protocol.
Collaborating with the benchmarks in the visual question answering task, we
empirically justify the performance degradation of the existing methods on
these OoD datasets, which serves as evidence to justify the modality bias
learning. In addition, to overcome this problem, we propose a plug-and-play
loss function method, whereby the feature space for each label is adaptively
learned according to the training set statistics. Thereafter, we apply this
method on eight baselines in total to test its effectiveness. From the results
on four datasets regarding the above three tasks, our method yields remarkable
performance improvements compared with the baselines, demonstrating its
superiority on reducing the modality bias problem.",0.09382503,-0.36007184,0.03183357,C
2468,"In addition, tackling the modality bias problem
from the other two stages, i.e.,multi-modal representation and fusion, demands our further research attention as well.","With the recognition of this problem in multi-modal classification, exploring its associated expression for generation
tasks will open an interesting door to increase the output diversity.","A PROOF FOR THE SCALING FACTOR S

Without loss of generality, let ùëÉùëñ denotes the expected minimum of the class ùëñ.",2022-02-25 13:47:09+00:00,On Modality Bias Recognition and Reduction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yangyang Guo'), arxiv.Result.Author('Liqiang Nie'), arxiv.Result.Author('Harry Cheng'), arxiv.Result.Author('Zhiyong Cheng'), arxiv.Result.Author('Mohan Kankanhalli'), arxiv.Result.Author('Alberto Del Bimbo')]","Making each modality in multi-modal data contribute is of vital importance to
learning a versatile multi-modal model. Existing methods, however, are often
dominated by one or few of modalities during model training, resulting in
sub-optimal performance. In this paper, we refer to this problem as modality
bias and attempt to study it in the context of multi-modal classification
systematically and comprehensively. After stepping into several empirical
analysis, we recognize that one modality affects the model prediction more just
because this modality has a spurious correlation with instance labels. In order
to primarily facilitate the evaluation on the modality bias problem, we
construct two datasets respectively for the colored digit recognition and video
action recognition tasks in line with the Out-of-Distribution (OoD) protocol.
Collaborating with the benchmarks in the visual question answering task, we
empirically justify the performance degradation of the existing methods on
these OoD datasets, which serves as evidence to justify the modality bias
learning. In addition, to overcome this problem, we propose a plug-and-play
loss function method, whereby the feature space for each label is adaptively
learned according to the training set statistics. Thereafter, we apply this
method on eight baselines in total to test its effectiveness. From the results
on four datasets regarding the above three tasks, our method yields remarkable
performance improvements compared with the baselines, demonstrating its
superiority on reducing the modality bias problem.",0.14967826,-0.23644102,-0.055235844,C
2481,"In addition, we mathematically show that Semantic
strong basis for further research, such as data augmentation             Factorization (SeFa) [6], GANSpace [7] and regular PCA
                                                                         [11] typically achieve almost the identical results when sam-
and latent-space manipulation.","More                     introduce the independent component analysis (ICA) algo-
                                                                         rithm under an additional orthogonality constraint [10] to
importantly, the discovery of   latent codes provides a                  investigate the pre-trained weight matrix of the Ô¨Årst dense
                                                                         layer.",pling enough data for GANSpace.,2022-02-25 20:00:33+00:00,OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhenxing Zhang'), arxiv.Result.Author('Lambert Schomaker')]","Text-to-image generation intends to automatically produce a photo-realistic
image, conditioned on a textual description. It can be potentially employed in
the field of art creation, data augmentation, photo-editing, etc. Although many
efforts have been dedicated to this task, it remains particularly challenging
to generate believable, natural scenes. To facilitate the real-world
applications of text-to-image synthesis, we focus on studying the following
three issues: 1) How to ensure that generated samples are believable, realistic
or natural? 2) How to exploit the latent space of the generator to edit a
synthesized image? 3) How to improve the explainability of a text-to-image
generation framework? In this work, we constructed two novel data sets (i.e.,
the Good & Bad bird and face data sets) consisting of successful as well as
unsuccessful generated samples, according to strict criteria. To effectively
and efficiently acquire high-quality images by increasing the probability of
generating Good latent codes, we use a dedicated Good/Bad classifier for
generated images. It is based on a pre-trained front end and fine-tuned on the
basis of the proposed Good & Bad data set. After that, we present a novel
algorithm which identifies semantically-understandable directions in the latent
space of a conditional text-to-image GAN architecture by performing independent
component analysis on the pre-trained weight values of the generator.
Furthermore, we develop a background-flattening loss (BFL), to improve the
background appearance in the edited image. Subsequently, we introduce linear
interpolation analysis between pairs of keywords. This is extended into a
similar triangular `linguistic' interpolation in order to take a deep look into
what a text-to-image synthesis model has learned within the linguistic
embeddings. Our data set is available at
https://zenodo.org/record/6283798#.YhkN_ujMI2w.",0.025295326,-0.24691471,0.13352628,C
2482,"These      latent codes can be exploited for

adequate samples, since the image instances exist in a non-        further research, facilitating and extending the applicabil-

linear manifold [45].","However,       to effectively and efÔ¨Åciently identify photo-realistic samples

it is difÔ¨Åcult to directly apply a traditional classiÔ¨Åer (e.g., a  from generated images while acquiring corresponding

linear SVM) to separate realistic images adequately from in-       latent vectors.","In the meantime, we cannot train a           ity of text-to-image generation in practice.",2022-02-25 20:00:33+00:00,OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhenxing Zhang'), arxiv.Result.Author('Lambert Schomaker')]","Text-to-image generation intends to automatically produce a photo-realistic
image, conditioned on a textual description. It can be potentially employed in
the field of art creation, data augmentation, photo-editing, etc. Although many
efforts have been dedicated to this task, it remains particularly challenging
to generate believable, natural scenes. To facilitate the real-world
applications of text-to-image synthesis, we focus on studying the following
three issues: 1) How to ensure that generated samples are believable, realistic
or natural? 2) How to exploit the latent space of the generator to edit a
synthesized image? 3) How to improve the explainability of a text-to-image
generation framework? In this work, we constructed two novel data sets (i.e.,
the Good & Bad bird and face data sets) consisting of successful as well as
unsuccessful generated samples, according to strict criteria. To effectively
and efficiently acquire high-quality images by increasing the probability of
generating Good latent codes, we use a dedicated Good/Bad classifier for
generated images. It is based on a pre-trained front end and fine-tuned on the
basis of the proposed Good & Bad data set. After that, we present a novel
algorithm which identifies semantically-understandable directions in the latent
space of a conditional text-to-image GAN architecture by performing independent
component analysis on the pre-trained weight values of the generator.
Furthermore, we develop a background-flattening loss (BFL), to improve the
background appearance in the edited image. Subsequently, we introduce linear
interpolation analysis between pairs of keywords. This is extended into a
similar triangular `linguistic' interpolation in order to take a deep look into
what a text-to-image synthesis model has learned within the linguistic
embeddings. Our data set is available at
https://zenodo.org/record/6283798#.YhkN_ujMI2w.",-0.17436011,-0.07786083,0.022312636,C
2483,"points to the interpolation between three points, i.e., in the
                                                                 2-simplex, for further studying ( , ( , )) and better per-
4.3.1.","Triangular interpolation and semantic
                                                                         interpretability
basic techniques to provide insights into the explainability
                                                                     We extend the pairwise linear interpolation between two
of a text-to-image synthesis framework.",Linear interpolation and semantic                         forming data augmentation.,2022-02-25 20:00:33+00:00,OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhenxing Zhang'), arxiv.Result.Author('Lambert Schomaker')]","Text-to-image generation intends to automatically produce a photo-realistic
image, conditioned on a textual description. It can be potentially employed in
the field of art creation, data augmentation, photo-editing, etc. Although many
efforts have been dedicated to this task, it remains particularly challenging
to generate believable, natural scenes. To facilitate the real-world
applications of text-to-image synthesis, we focus on studying the following
three issues: 1) How to ensure that generated samples are believable, realistic
or natural? 2) How to exploit the latent space of the generator to edit a
synthesized image? 3) How to improve the explainability of a text-to-image
generation framework? In this work, we constructed two novel data sets (i.e.,
the Good & Bad bird and face data sets) consisting of successful as well as
unsuccessful generated samples, according to strict criteria. To effectively
and efficiently acquire high-quality images by increasing the probability of
generating Good latent codes, we use a dedicated Good/Bad classifier for
generated images. It is based on a pre-trained front end and fine-tuned on the
basis of the proposed Good & Bad data set. After that, we present a novel
algorithm which identifies semantically-understandable directions in the latent
space of a conditional text-to-image GAN architecture by performing independent
component analysis on the pre-trained weight values of the generator.
Furthermore, we develop a background-flattening loss (BFL), to improve the
background appearance in the edited image. Subsequently, we introduce linear
interpolation analysis between pairs of keywords. This is extended into a
similar triangular `linguistic' interpolation in order to take a deep look into
what a text-to-image synthesis model has learned within the linguistic
embeddings. Our data set is available at
https://zenodo.org/record/6283798#.YhkN_ujMI2w.",-0.025054999,0.044757545,-0.057607982,C
2496,"On the JHU-Crowd++ dataset, as shown in Ta-                           We further study the eÔ¨Äect of using diÔ¨Äerent k (the number
ble 6, our method outperforms the state-of-the-art method                      of nearest-neighbour), the results listed in Table 8.","On the UCF-QNRF dataset,                           5.4.2 The eÔ¨Äect of K
our method achieves the best MSE and reports compara-
ble MAE.","The pro-
GL [32] by a signiÔ¨Åcant margin of 18.9 MSE.",2022-02-26 05:21:30+00:00,An End-to-End Transformer Model for Crowd Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dingkang Liang'), arxiv.Result.Author('Wei Xu'), arxiv.Result.Author('Xiang Bai')]","Crowd localization, predicting head positions, is a more practical and
high-level task than simply counting. Existing methods employ pseudo-bounding
boxes or pre-designed localization maps, relying on complex post-processing to
obtain the head positions. In this paper, we propose an elegant, end-to-end
Crowd Localization TRansformer named CLTR that solves the task in the
regression-based paradigm. The proposed method views the crowd localization as
a direct set prediction problem, taking extracted features and trainable
embeddings as input of the transformer-decoder. To achieve good matching
results, we introduce a KMO-based Hungarian, which innovatively revisits the
label assignment from a context view instead of an independent instance view.
Extensive experiments conducted on five datasets in various data settings show
the effectiveness of our method. In particular, the proposed method achieves
the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech
Part A datasets.",0.24612918,0.061932117,0.030489834,A
2497,"Downstream Results                                           method in the Ô¨Åeld of signature veriÔ¨Åcation and paves a way
                                                                  for further research in this direction.","This work
                                                                  shows the extensive scope and applicability of the proposed
4.1.","The downstream task we considered in our work is the writer-
independent classiÔ¨Åcation of signatures into two classes: gen-                            6.",2022-02-26 06:33:25+00:00,SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Siladittya Manna'), arxiv.Result.Author('Soumitri Chattopadhyay'), arxiv.Result.Author('Saumik Bhattacharya'), arxiv.Result.Author('Umapada Pal')]","Writer independent offline signature verification is one of the most
challenging tasks in pattern recognition as there is often a scarcity of
training data. To handle such data scarcity problem, in this paper, we propose
a novel self-supervised learning (SSL) framework for writer independent offline
signature verification. To our knowledge, this is the first attempt to utilize
self-supervised setting for the signature verification task. The objective of
self-supervised representation learning from the signature images is achieved
by minimizing the cross-covariance between two random variables belonging to
different feature directions and ensuring a positive cross-covariance between
the random variables denoting the same feature direction. This ensures that the
features are decorrelated linearly and the redundant information is discarded.
Through experimental results on different data sets, we obtained encouraging
results.",0.34774947,-0.041008014,-0.09582484,A
2498,Removing color jitter as         for further research in this direction.,"Increasing the number of layers in the projector      method in the Ô¨Åeld of signature veriÔ¨Åcation and paves a way
did not improve the performance.","augmentation from the above model yielded ACC, FAR and
FRR of 83.1%, 0.11 and 0.19, respectively.",2022-02-26 06:33:25+00:00,SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Siladittya Manna'), arxiv.Result.Author('Soumitri Chattopadhyay'), arxiv.Result.Author('Saumik Bhattacharya'), arxiv.Result.Author('Umapada Pal')]","Writer independent offline signature verification is one of the most
challenging tasks in pattern recognition as there is often a scarcity of
training data. To handle such data scarcity problem, in this paper, we propose
a novel self-supervised learning (SSL) framework for writer independent offline
signature verification. To our knowledge, this is the first attempt to utilize
self-supervised setting for the signature verification task. The objective of
self-supervised representation learning from the signature images is achieved
by minimizing the cross-covariance between two random variables belonging to
different feature directions and ensuring a positive cross-covariance between
the random variables denoting the same feature direction. This ensures that the
features are decorrelated linearly and the redundant information is discarded.
Through experimental results on different data sets, we obtained encouraging
results.",0.13981764,0.28965524,0.15187742,A
2499,"However,
be solved by adding related items in the gallery set   these are either time-intensive due to the use
for training the object localization model, which      of hand-crafted features for object detection or
needs further study.",It appears that this problem can also      mance comparable to that of our model.,"employ heavy-weight neural network models to
                                                       perform the prediction, and are hence not suit-
5 Conclusions and Future                               able for large-scale applications.",2022-02-26 06:51:36+00:00,An Improved Deep Learning Approach For Product Recognition on Racks in Retail Stores,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ankit Sinha'), arxiv.Result.Author('Soham Banerjee'), arxiv.Result.Author('Pratik Chattopadhyay')]","Automated product recognition in retail stores is an important real-world
application in the domain of Computer Vision and Pattern Recognition. In this
paper, we consider the problem of automatically identifying the classes of the
products placed on racks in retail stores from an image of the rack and
information about the query/product images. We improve upon the existing
approaches in terms of effectiveness and memory requirement by developing a
two-stage object detection and recognition pipeline comprising of a
Faster-RCNN-based object localizer that detects the object regions in the rack
image and a ResNet-18-based image encoder that classifies the detected regions
into the appropriate classes. Each of the models is fine-tuned using
appropriate data sets for better prediction and data augmentation is performed
on each query image to prepare an extensive gallery set for fine-tuning the
ResNet-18-based product recognition model. This encoder is trained using a
triplet loss function following the strategy of online-hard-negative-mining for
improved prediction. The proposed models are lightweight and can be connected
in an end-to-end manner during deployment for automatically identifying each
product object placed in a rack image. Extensive experiments using Grozi-32k
and GP-180 data sets verify the effectiveness of the proposed model.",-0.32878798,-0.0796194,-0.04547058,B
2501,"Hence, further research in               detection,‚Äù in Proceedings of the IEEE conference on computer vision
this direction to reduce this accuracy gap is encouraged.","Despite these recent
efforts, the domain is less explored and the accuracy gap             [4] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao, ‚ÄúA multi-
between the semi-supervised, weakly-supervised models and                  stream bi-directional recurrent neural network for Ô¨Åne-grained action
fully supervised models is large.",and pattern recognition.,2022-02-26 09:25:44+00:00,Continuous Human Action Recognition for Human-Machine Interaction: A Review,cs.CV,"['cs.CV', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Harshala Gammulle'), arxiv.Result.Author('David Ahmedt-Aristizabal'), arxiv.Result.Author('Simon Denman'), arxiv.Result.Author('Lachlan Tychsen-Smith'), arxiv.Result.Author('Lars Petersson'), arxiv.Result.Author('Clinton Fookes')]","With advances in data-driven machine learning research, a wide variety of
prediction models have been proposed to capture spatio-temporal features for
the analysis of video streams. Recognising actions and detecting action
transitions within an input video are challenging but necessary tasks for
applications that require real-time human-machine interaction. By reviewing a
large body of recent related work in the literature, we thoroughly analyse,
explain and compare action segmentation methods and provide details on the
feature extraction and learning strategies that are used on most
state-of-the-art methods. We cover the impact of the performance of object
detection and tracking techniques on human action segmentation methodologies.
We investigate the application of such models to real-world scenarios and
discuss several limitations and key research directions towards improving
interpretability, generalisation, optimisation and deployment.",-0.2389652,-0.15711346,-0.06263288,C
2503,"aspect, contemporary reversible steganographic coding is based
                                        primarily on heuristics and therefore worth further study.",As another core          computing comes into play [11]‚Äì[15].,"While           A fundamental element of reversible steganography, in com-
                                        attempts have been made to realise automatic coding with neural         mon with lossless compression, is predictive analytics [16]‚Äì
                                        networks, perfect reversibility is still unreachable via such an        [18].",2022-02-26 13:02:32+00:00,Nonlinear Discrete Optimisation of Reversible Steganographic Coding,cs.CV,"['cs.CV', 'cs.MM']",[arxiv.Result.Author('Ching-Chun Chang')],"Authentication mechanisms are at the forefront of defending the world from
various types of cybercrime. Steganography can serve as an authentication
solution by embedding a digital signature into a carrier object to ensure the
integrity of the object and simultaneously lighten the burden of metadata
management. However, steganographic distortion, albeit generally imperceptible
to human sensory systems, might be inadmissible in fidelity-sensitive
situations. This has led to the concept of reversible steganography. A
fundamental element of reversible steganography is predictive analytics, for
which powerful neural network models have been effectively deployed. As another
core aspect, contemporary reversible steganographic coding is based primarily
on heuristics and therefore worth further study. While attempts have been made
to realise automatic coding with neural networks, perfect reversibility is
still unreachable via such an unexplainable intelligent machinery. Instead of
relying on deep learning, we aim to derive an optimal coding by means of
mathematical optimisation. In this study, we formulate reversible
steganographic coding as a nonlinear discrete optimisation problem with a
logarithmic capacity constraint and a quadratic distortion objective.
Linearisation techniques are developed to enable mixed-integer linear
programming. Experimental results validate the near-optimality of the proposed
optimisation algorithm benchmarked against a brute-force method.",0.0528593,-0.18710265,0.21770811,C
2513,"To further study the
scalability of our approach, we show the rank-1 accuracy
with diÔ¨Äerent numbers of bins in Figure 4.","This supports our
hypothesis that dividing the gallery into bins, based upon
S 2T ratio of the person, helps in preserving semantic in-
formation such as orientation, which would otherwise be
lost in the average gallery operation.","We observe
                          TABLE I: Single-sensor Tracking Performance on Three Datasets

                          Metrics                IDF1 IDP IDR FP FN IDs MOTA MOTP

Venice-2 ADL-Run.",2022-02-26 22:03:58+00:00,Orientation-Discriminative Feature Representation for Decentralized Pedestrian Tracking,cs.CV,"['cs.CV', 'cs.MA', 'cs.RO']","[arxiv.Result.Author('Vikram Shree'), arxiv.Result.Author('Carlos Diaz-Ruiz'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Bharath Hariharan'), arxiv.Result.Author('Mark Campbell')]","This paper focuses on the problem of decentralized pedestrian tracking using
a sensor network. Traditional works on pedestrian tracking usually use a
centralized framework, which becomes less practical for robotic applications
due to limited communication bandwidth. Our paper proposes a
communication-efficient, orientation-discriminative feature representation to
characterize pedestrian appearance information, that can be shared among
sensors. Building upon that representation, our work develops a cross-sensor
track association approach to achieve decentralized tracking. Extensive
evaluations are conducted on publicly available datasets and results show that
our proposed approach leads to improved performance in multi-sensor tracking.",-0.15821487,0.124738574,-0.121700704,B
2515,"This social and economic importance          pavement images has recently emerged as a subject of
                                        is further highlighted in low- and middle-income countries         interest in academia as public releases of various annotated
                                        where World Bank economists estimated in 2017 that a 10%           image datasets of road pavement distresses such as the
                                        reduction in road trafÔ¨Åc deaths would contribute to a 3.6%         2020 GRDC dataset have encouraged further research on
                                        increase in per capita real GDP of these countries over a 24-      the subject.","Given these limitations of existing
                                        Ô¨Ågures respectively as a result of a 1% increase in that           methods, the relatively more cost-effective alternative of
                                        same region‚Äôs road access to GDP, population and labor             detecting and cataloging road distresses using computer
                                        resources falling within a three hour drive of its borders over    vision algorithms trained on low-cost smartphone-captured
                                        a 22-year period [1].","year period considering the outsized mortality impact of road
                                        trafÔ¨Åc-related accidents on populations‚Äô more economically
                                        productive segments [2].",2022-02-27 04:08:00+00:00,A Computer Vision-assisted Approach to Automated Real-Time Road Infrastructure Management,cs.CV,"['cs.CV', 'cs.LG']",[arxiv.Result.Author('Philippe Heitzmann')],"Accurate automated detection of road pavement distresses is critical for the
timely identification and repair of potentially accident-inducing road hazards
such as potholes and other surface-level asphalt cracks. Deployment of such a
system would be further advantageous in low-resource environments where lack of
government funding for infrastructure maintenance typically entails heightened
risks of potentially fatal vehicular road accidents as a result of inadequate
and infrequent manual inspection of road systems for road hazards. To remedy
this, a recent research initiative organized by the Institute of Electrical and
Electronics Engineers (""IEEE"") as part of their 2020 Global Road Damage
Detection (""GRDC"") Challenge published in May 2020 a novel 21,041 annotated
image dataset of various road distresses calling upon academic and other
researchers to submit innovative deep learning-based solutions to these road
hazard detection problems. Making use of this dataset, we propose a supervised
object detection approach leveraging You Only Look Once (""YOLO"") and the Faster
R-CNN frameworks to detect and classify road distresses in real-time via a
vehicle dashboard-mounted smartphone camera, producing 0.68 F1-score
experimental results ranking in the top 5 of 121 teams that entered this
challenge as of December 2021.",-0.13836537,0.07636839,-0.1732257,B
2566,"To further study the effectiveness of the domain-level
                                                                   attention, we design the PA module to replace DA.","It suggests that the local structure regularization terms of
                                                                   the two domains provide substantial advantages in learning
                                                                   transferable and discriminative features.","SpeciÔ¨Å-
                                                                   cally, PA employs a single-head attention and regards each
                                                                   element of the given feature vectors as a patch with spatial
                                                                   size 1√ó1.",2022-02-24 02:30:15+00:00,Towards Unsupervised Domain Adaptation via Domain-Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ren Chuan-Xian'), arxiv.Result.Author('Zhai Yi-Ming'), arxiv.Result.Author('Luo You-Wei'), arxiv.Result.Author('Li Meng-Xue')]","As a vital problem in pattern analysis and machine intelligence, Unsupervised
Domain Adaptation (UDA) studies how to transfer an effective feature learner
from a labeled source domain to an unlabeled target domain. Plenty of methods
based on Convolutional Neural Networks (CNNs) have achieved promising results
in the past decades. Inspired by the success of Transformers, some methods
attempt to tackle UDA problem by adopting pure transformer architectures, and
interpret the models by applying the long-range dependency strategy at image
patch-level. However, the algorithmic complexity is high and the
interpretability seems weak. In this paper, we propose the Domain-Transformer
(DoT) for UDA, which integrates the CNN-backbones and the core attention
mechanism of Transformers from a new perspective. Specifically, a plug-and-play
domain-level attention mechanism is proposed to learn the sample correspondence
between domains. This is significantly different from existing methods which
only capture the local interactions among image patches. Instead of explicitly
modeling the distribution discrepancy from either domain-level or class-level,
DoT learns transferable features by achieving the local semantic consistency
across domains, where the domain-level attention and manifold regularization
are explored. Then, DoT is free of pseudo-labels and explicit domain
discrepancy optimization. Theoretically, DoT is connected with the optimal
transportation algorithm and statistical learning theory. The connection
provides a new insight to understand the core component of Transformers.
Extensive experiments on several benchmark datasets validate the effectiveness
of DoT.",-0.1211726,-0.28636307,-0.030487273,C
2590,"As the next subsection ex-       ing the convolution of incoming light with surface materials as an
plains, these approaches represent geometry using volumetric fields        extension for further research.","We leave decompos-
other recent, continuous representations.","Given our simplifying design choice
that inherently support multiple surface estimates at the same time to     for directly storing and optimizing static SLFs, our models allow for
facilitate optimization and also support fine and intricate geometry       direct geometry editing and simple transformations of the surface
approximations.",2022-02-28 19:37:12+00:00,ERF: Explicit Radiance Field Reconstruction From Scratch,cs.CV,"['cs.CV', 'cs.GR', 'I.3.3; I.4.5']","[arxiv.Result.Author('Samir Aroudj'), arxiv.Result.Author('Steven Lovegrove'), arxiv.Result.Author('Eddy Ilg'), arxiv.Result.Author('Tanner Schmidt'), arxiv.Result.Author('Michael Goesele'), arxiv.Result.Author('Richard Newcombe')]","We propose a novel explicit dense 3D reconstruction approach that processes a
set of images of a scene with sensor poses and calibrations and estimates a
photo-real digital model. One of the key innovations is that the underlying
volumetric representation is completely explicit in contrast to neural
network-based (implicit) alternatives. We encode scenes explicitly using clear
and understandable mappings of optimization variables to scene geometry and
their outgoing surface radiance. We represent them using hierarchical
volumetric fields stored in a sparse voxel octree. Robustly reconstructing such
a volumetric scene model with millions of unknown variables from registered
scene images only is a highly non-convex and complex optimization problem. To
this end, we employ stochastic gradient descent (Adam) which is steered by an
inverse differentiable renderer.
  We demonstrate that our method can reconstruct models of high quality that
are comparable to state-of-the-art implicit methods. Importantly, we do not use
a sequential reconstruction pipeline where individual steps suffer from
incomplete or unreliable information from previous stages, but start our
optimizations from uniformed initial solutions with scene geometry and radiance
that is far off from the ground truth. We show that our method is general and
practical. It does not require a highly controlled lab setup for capturing, but
allows for reconstructing scenes with a vast variety of objects, including
challenging ones, such as outdoor plants or furry toys. Finally, our
reconstructed scene models are versatile thanks to their explicit design. They
can be edited interactively which is computationally too costly for implicit
alternatives.",-0.08915456,0.28900543,0.21406633,B
2620,"Study on feature compression

                                                                      To further study feature compression on videos with dif-
                                                                   ferent types of generic boundaries, we conduct extensive
                                                                   experiments on Kinetics-GEBD benchmark under compres-
                                                                   sion ratios MN , ranging from 0.2 to 1 of stride 0.2.",A.2.,"Figure A
                                                                   shows that when M = 0.6N , the model achieves the best
                                                                   performance.",2022-03-01 09:31:30+00:00,Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jing Tan'), arxiv.Result.Author('Yuhong Wang'), arxiv.Result.Author('Gangshan Wu'), arxiv.Result.Author('Limin Wang')]","Generic Boundary Detection (GBD) aims at locating general boundaries that
divide videos into semantically coherent and taxonomy-free units, and could
server as an important pre-processing step for long-form video understanding.
Previous research separately handle these different-level generic boundaries
with specific designs of complicated deep networks from simple CNN to LSTM.
Instead, in this paper, our objective is to develop a general yet simple
architecture for arbitrary boundary detection in videos. To this end, we
present Temporal Perceiver, a general architecture with Transformers, offering
a unified solution to the detection of arbitrary generic boundaries. The core
design is to introduce a small set of latent feature queries as anchors to
compress the redundant input into fixed dimension via cross-attention blocks.
Thanks to this fixed number of latent units, it reduces the quadratic
complexity of attention operation to a linear form of input frames.
Specifically, to leverage the coherence structure of videos, we construct two
types of latent feature queries: boundary queries and context queries, which
handle the semantic incoherence and coherence regions accordingly. Moreover, to
guide the learning of latent feature queries, we propose an alignment loss on
cross-attention to explicitly encourage the boundary queries to attend on the
top possible boundaries. Finally, we present a sparse detection head on the
compressed representations and directly output the final boundary detection
results without any post-processing module. We test our Temporal Perceiver on a
variety of detection benchmarks, ranging from shot-level, event-level, to
scene-level GBD. Our method surpasses the previous state-of-the-art methods on
all benchmarks, demonstrating the generalization ability of our temporal
perceiver.",0.04777894,0.008126106,0.22968695,C
2626,"Available: http://arxiv.org/abs/1306.5151
   Although the proposed method has achieved better results
on multiple datasets, but it still needs further research.",[Online].,"For                                                                                                                                                                                                                                                                                                                                                                                                                                  [5] W. V. Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans, and
example, the existing unsupervised contrastive learning can                                                                                                                                                                                                                                                                                                                                                                                                                                          L. V. Gool, ‚ÄúSCAN: learning to classify images without labels,‚Äù in
effectively learn the feature representation of a single example                                                                                                                                                                                                                                                                                                                                                                                                                                     Computer Vision - ECCV 2020 - 16th European Conference, Glasgow,
without clustering the whole training dataset.",2022-03-01 13:33:00+00:00,Bridge the Gap between Supervised and Unsupervised Learning for Fine-Grained Classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiabao Wang'), arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Xiu-Shen Wei'), arxiv.Result.Author('Hang Li'), arxiv.Result.Author('Zhuang Miao'), arxiv.Result.Author('Rui Zhang')]","Unsupervised learning technology has caught up with or even surpassed
supervised learning technology in general object classification (GOC) and
person re-identification (re-ID). However, it is found that the unsupervised
learning of fine-grained visual classification (FGVC) is more challenging than
GOC and person re-ID. In order to bridge the gap between unsupervised and
supervised learning for FGVC, we investigate the essential factors (including
feature extraction, clustering, and contrastive learning) for the performance
gap between supervised and unsupervised FGVC. Furthermore, we propose a simple,
effective, and practical method, termed as UFCL, to alleviate the gap. Three
key issues are concerned and improved: First, we introduce a robust and
powerful backbone, ResNet50-IBN, which has an ability of domain adaptation when
we transfer ImageNet pre-trained models to FGVC tasks. Next, we propose to
introduce HDBSCAN instead of DBSCAN to do clustering, which can generate better
clusters for adjacent categories with fewer hyper-parameters. Finally, we
propose a weighted feature agent and its updating mechanism to do contrastive
learning by using the pseudo labels with inevitable noise, which can improve
the optimization process of learning the parameters of the network. The
effectiveness of our UFCL is verified on CUB-200-2011, Oxford-Flowers,
Oxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the
unsupervised FGVC setting, we achieve state-of-the-art results, and analyze the
key factors and the important parameters to provide a practical guidance.",-0.28183627,-0.16648403,0.052295763,C
2652,"There is
   11https://github.com/ultralytics/yolov5                                                               also an obvious gap between the location predic-
   12https://github.com/deepinsight/                                                                     tion and time prediction, showing that temporal
insightface                                                                                              reasoning in vision language learning is much un-
                                                                                                         der explored and needs further research.","Accuracy is calculated without considering                                                        taken in older times than 2010, thus not provid-
                                                                                                         ing enough supervision for our test set.","Note that
   Model  Accuracy  Example-F1                                  Model  Accuracy  Example-F1

   CLIP     11.11       44.96                                   CLIP     28.18       61.63
  CLIP+     15.72       49.74                                  CLIP+     26.49       62.68
CLIP+WIT    11.11       45.20                                CLIP+WIT    11.11       50.00
CLIP+Seg    16.46       50.52                                CLIP+Seg    26.96       62.41

  Human     86.21       92.41                                Table 4: Performance(%) for different baselines pre-
                                                             dicted towards news abstracts.",2022-03-01 21:52:08+00:00,There is a Time and Place for Reasoning Beyond the Image,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xingyu Fu'), arxiv.Result.Author('Ben Zhou'), arxiv.Result.Author('Ishaan Preetam Chandratreya'), arxiv.Result.Author('Carl Vondrick'), arxiv.Result.Author('Dan Roth')]","Images are often more significant than only the pixels to human eyes, as we
can infer, associate, and reason with contextual information from other sources
to establish a more complete picture. For example, in Figure 1, we can find a
way to identify the news articles related to the picture through segment-wise
understandings on the signs, the buildings, the crowds, and more. This tells us
the time when and the location where the image is taken, which will help us in
subsequent tasks, such as evidence retrieval for criminal activities, automatic
storyline construction, and upper-stream processing such as image clustering.
In this work, we formulate this problem and introduce TARA: a dataset with 16k
images with their associated news, time and location automatically extracted
from New York Times (NYT), and an additional 61k examples as distant
supervision from WIT. On top of the extractions, we present a crowdsourced
subset in which images are believed to be feasible to find their
spatio-temporal information for evaluation purpose. We show that there exists a
70% gap between a state-of-the-art joint model and human performance, which is
slightly filled by our proposed model that uses segment-wise reasoning,
motivating higher-level vision-language joint models that can conduct
open-ended reasoning with world knowledge.",0.023132814,-0.18320975,-0.15252629,C
2653,"also an obvious gap between the location predic-
tion and time prediction, showing that temporal              7 Conclusion
reasoning in vision language learning is much un-
der explored and needs further research.",There is            human performance.,"Note that           In this work, we introduce TARA, a new dataset
the Example-F1 value is consistently higher than             and task for spatio-temporal grounding of images
accuracy because if the model predicts the highest           that requires open-ended joint reasoning with world
two hierarchies correctly (e.g.",2022-03-01 21:52:08+00:00,There is a Time and Place for Reasoning Beyond the Image,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xingyu Fu'), arxiv.Result.Author('Ben Zhou'), arxiv.Result.Author('Ishaan Preetam Chandratreya'), arxiv.Result.Author('Carl Vondrick'), arxiv.Result.Author('Dan Roth')]","Images are often more significant than only the pixels to human eyes, as we
can infer, associate, and reason with contextual information from other sources
to establish a more complete picture. For example, in Figure 1, we can find a
way to identify the news articles related to the picture through segment-wise
understandings of the signs, the buildings, the crowds, and more. This
reasoning could provide the time and place the image was taken, which will help
us in subsequent tasks, such as automatic storyline construction, correction of
image source in intended effect photographs, and upper-stream processing such
as image clustering for certain location or time.
  In this work, we formulate this problem and introduce TARA: a dataset with
16k images with their associated news, time, and location, automatically
extracted from New York Times, and an additional 61k examples as distant
supervision from WIT. On top of the extractions, we present a crowdsourced
subset in which we believe it is possible to find the images' spatio-temporal
information for evaluation purpose. We show that there exists a $70\%$ gap
between a state-of-the-art joint model and human performance, which is slightly
filled by our proposed model that uses segment-wise reasoning, motivating
higher-level vision-language joint models that can conduct open-ended reasoning
with world knowledge. The data and code are publicly available at
https://github.com/zeyofu/TARA.",-0.22736424,-0.20451567,-0.17329606,C
2667,"In addition,
we establish the Ô¨Årst quantitative benchmark and the vi-               [9] Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, and Ran
sual Styleverse matrix for the further research.","3
from an universal and lightweight perspective.",Extensive                  He.,2022-03-02 04:23:01+00:00,Styleverse: Towards Identity Stylization across Heterogeneous Domains,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Jie Cao'), arxiv.Result.Author('JunXian Duan'), arxiv.Result.Author('Ran He')]","We propose a new challenging task namely IDentity Stylization (IDS) across
heterogeneous domains. IDS focuses on stylizing the content identity, rather
than completely swapping it using the reference identity. We use an effective
heterogeneous-network-based framework $Styleverse$ that uses a single
domain-aware generator to exploit the Metaverse of diverse heterogeneous faces,
based on the proposed dataset FS13 with limited data. FS13 means 13 kinds of
Face Styles considering diverse lighting conditions, art representations and
life dimensions. Previous similar tasks, \eg, image style transfer can handle
textural style transfer based on a reference image. This task usually ignores
the high structure-aware facial area and high-fidelity preservation of the
content. However, Styleverse intends to controllably create topology-aware
faces in the Parallel Style Universe, where the source facial identity is
adaptively styled via AdaIN guided by the domain-aware and reference-aware
style embeddings from heterogeneous pretrained models. We first establish the
IDS quantitative benchmark as well as the qualitative Styleverse matrix.
Extensive experiments demonstrate that Styleverse achieves higher-fidelity
identity stylization compared with other state-of-the-art methods.",0.31506324,0.06296407,0.1056553,A
2668,"This
   For the Ô¨Årst time, we have systematically explored and                                                                                                                           work is helpful for the further study of IDS.","strate that our approach can handle high-quality and
                                                                                                                                                                                    domain-aware IDS across heterogeneous domains.","We provide
analyzed IDS from an universal and lightweight perspec-                                                                                                                             more studies of ablation study and analysis of Styleverse as
tive.",2022-03-02 04:23:01+00:00,Styleverse: Towards Identity Stylization across Heterogeneous Domains,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Jie Cao'), arxiv.Result.Author('JunXian Duan'), arxiv.Result.Author('Ran He')]","We propose a new challenging task namely IDentity Stylization (IDS) across
heterogeneous domains. IDS focuses on stylizing the content identity, rather
than completely swapping it using the reference identity. We use an effective
heterogeneous-network-based framework $Styleverse$ that uses a single
domain-aware generator to exploit the Metaverse of diverse heterogeneous faces,
based on the proposed dataset FS13 with limited data. FS13 means 13 kinds of
Face Styles considering diverse lighting conditions, art representations and
life dimensions. Previous similar tasks, \eg, image style transfer can handle
textural style transfer based on a reference image. This task usually ignores
the high structure-aware facial area and high-fidelity preservation of the
content. However, Styleverse intends to controllably create topology-aware
faces in the Parallel Style Universe, where the source facial identity is
adaptively styled via AdaIN guided by the domain-aware and reference-aware
style embeddings from heterogeneous pretrained models. We first establish the
IDS quantitative benchmark as well as the qualitative Styleverse matrix.
Extensive experiments demonstrate that Styleverse achieves higher-fidelity
identity stylization compared with other state-of-the-art methods.",0.2204471,-0.029445488,-0.14340359,A
2683,"By investigating      log2DFT image space and the frequencies and magnitudes
the different spectral band images individually, the band that     they represent may hold vital information which helps improve
provides the best results could be used in further research.","The position of the pixels in a
discriminate more clearly between materials.",the classiÔ¨Åcation.,2022-03-02 11:39:22+00:00,Image-based material analysis of ancient historical documents,cs.CV,['cs.CV'],"[arxiv.Result.Author('Thomas Reynolds'), arxiv.Result.Author('Maruf A. Dhali'), arxiv.Result.Author('Lambert Schomaker')]","Researchers continually perform corroborative tests to classify ancient
historical documents based on the physical materials of their writing surfaces.
However, these tests, often performed on-site, requires actual access to the
manuscript objects. The procedures involve a considerable amount of time and
cost, and can damage the manuscripts. Developing a technique to classify such
documents using only digital images can be very useful and efficient. In order
to tackle this problem, this study uses images of a famous historical
collection, the Dead Sea Scrolls, to propose a novel method to classify the
materials of the manuscripts. The proposed classifier uses the two-dimensional
Fourier Transform to identify patterns within the manuscript surfaces.
Combining a binary classification system employing the transform with a
majority voting process is shown to be effective for this classification task.
This pilot study shows a successful classification percentage of up to 97% for
a confined amount of manuscripts produced from either parchment or papyrus
material. Feature vectors based on Fourier-space grid representation
outperformed a concentric Fourier-space format.",0.10856967,0.17508216,0.010299282,A
2686,"We will publish our code to facilitate
                    and Inference           and Inference                 further research on continual UDA without source data.","DNN                                      Thirdly, we show the effectiveness of CBNA across a variety
                                                                          of source/target domain combinations, where we can even
New: Continual BatchNorm Adaptation (CBNA)                                Ô¨Ånd hyperparameters which generalize across different target
                                                                          domains for a given segmentation model proving the practical
OÔ¨Ñine: Training  Online: Adaptation      Online: Adaptation               applicability of CBNA.",Source Domain    Target Domain 1         Target Domain 2                     This work is structured as follows.,2022-03-02 12:55:10+00:00,Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Marvin Klingner'), arxiv.Result.Author('Mouadh Ayache'), arxiv.Result.Author('Tim Fingscheidt')]","Environment perception in autonomous driving vehicles often heavily relies on
deep neural networks (DNNs), which are subject to domain shifts, leading to a
significantly decreased performance during DNN deployment. Usually, this
problem is addressed by unsupervised domain adaptation (UDA) approaches trained
either simultaneously on source and target domain datasets or even source-free
only on target data in an offline fashion. In this work, we further expand a
source-free UDA approach to a continual and therefore online-capable UDA on a
single-image basis for semantic segmentation. Accordingly, our method only
requires the pre-trained model from the supplier (trained in the source domain)
and the current (unlabeled target domain) camera image. Our method Continual
BatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch
normalization layers, using target domain images in an unsupervised fashion,
which yields consistent performance improvements during inference. Thereby, in
contrast to existing works, our approach can be applied to improve a DNN
continuously on a single-image basis during deployment without access to source
data, without algorithmic delay, and nearly without computational overhead. We
show the consistent effectiveness of our method across a wide variety of
source/target domain settings for semantic segmentation. As part of this work,
our code will be made publicly available.",-0.05278497,-0.27495363,0.12354072,C
2687,"We will publish our code to facilitate
                                                                          further research on continual UDA without source data.","Thirdly, we show the effectiveness of CBNA across a variety
New: Continual BatchNorm Adaptation (CBNA)                                of source/target domain combinations, where we can even
                                                                          Ô¨Ånd hyperparameters which generalize across different target
OÔ¨Ñine: Training  Online: Adaptation      Online: Adaptation               domains for a given segmentation model proving the practical
                    and Inference           and Inference                 applicability of CBNA.","Source Domain    Target Domain 1         Target Domain 2
                                                                             This work is structured as follows.",2022-03-02 12:55:10+00:00,Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Marvin Klingner'), arxiv.Result.Author('Mouadh Ayache'), arxiv.Result.Author('Tim Fingscheidt')]","Environment perception in autonomous driving vehicles often heavily relies on
deep neural networks (DNNs), which are subject to domain shifts, leading to a
significantly decreased performance during DNN deployment. Usually, this
problem is addressed by unsupervised domain adaptation (UDA) approaches trained
either simultaneously on source and target domain datasets or even source-free
only on target data in an offline fashion. In this work, we further expand a
source-free UDA approach to a continual and therefore online-capable UDA on a
single-image basis for semantic segmentation. Accordingly, our method only
requires the pre-trained model from the supplier (trained in the source domain)
and the current (unlabeled target domain) camera image. Our method Continual
BatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch
normalization layers, using target domain images in an unsupervised fashion,
which yields consistent performance improvements during inference. Thereby, in
contrast to existing works, our approach can be applied to improve a DNN
continuously on a single-image basis during deployment without access to source
data, without algorithmic delay, and nearly without computational overhead. We
show the consistent effectiveness of our method across a wide variety of
source/target domain settings for semantic segmentation. Code is available at
https://github.com/ifnspaml/CBNA.",-0.0034252983,-0.24661928,0.06507138,C
2710,"We further study two substitute mod-
likely to retrieve the accurate suspect identity.","While for our ICT, it has the global
ping results, using the outer identity to retrieve the nearest         attention mechanism at every layer so it could split the in-
neighbor (followed by comparing the inner identity) is more            ner and outer easily.","els: a) Res50-split-image, splitting images by the blend-
                                                                       ing mask and letting the model learn inner (outer) iden-
The effect of the reference set.",2022-03-02 18:59:58+00:00,Protecting Celebrities with Identity Consistency Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Ting Zhang'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Nenghai Yu'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Baining Guo')]","In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities.",-0.089811996,-0.043602474,0.04970254,C
2711,"We further study two substitute mod-
likely to retrieve the accurate suspect identity.","as the outer face is usually not manipulated in face swap-
ping results, using the outer identity to retrieve the nearest         attention mechanism at every layer so it could split the in-
neighbor (followed by comparing the inner identity) is more            ner and outer easily.","els: a) Res50-split-image, splitting images by the blend-
                                                                       ing mask and letting the model learn inner (outer) iden-
The effect of the reference set.",2022-03-02 18:59:58+00:00,Protecting Celebrities with Identity Consistency Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Ting Zhang'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Nenghai Yu'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Baining Guo')]","In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities.",-0.049844153,-0.045941647,0.067328066,C
2712,"We further study two substitute mod-        on high-level semantics is specially effective for the cases
els: a) Res50-split-image, splitting images by the blend-         where low-level based methods fail.","We show that our work based
ner and outer easily.","In addition, our ap-
ing mask and letting the model learn inner (outer) iden-          proach is further enhanced by leveraging additional identity
tity from the cropped inner (outer) face rather than learn-       information from celebrities.",2022-03-02 18:59:58+00:00,Protecting Celebrities from DeepFake with Identity Consistency Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Ting Zhang'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Nenghai Yu'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Baining Guo')]","In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities. Code will be released at
\url{https://github.com/LightDXY/ICT_DeepFake}",-0.16915664,-0.15191746,0.06157449,C
2713,"However, the results lead
naturally to other questions that will require further research before this important issue could be considered fully
explored.","10
  This article explored deep learning algorithm robustness under two-factor corruption.",These include the following.,2022-03-02 03:53:21+00:00,Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation,cs.CV,"['cs.CV', 'cs.AI', 'cs.GR']","[arxiv.Result.Author('Wei Dai'), arxiv.Result.Author('Daniel Berleant')]","Accuracies of deep learning (DL) classifiers are often unstable in that they
may change significantly when retested on adversarial images, imperfect images,
or perturbed images. This paper adds to the fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. To measure
robust DL classifiers, previous research reported on single-factor corruption.
We created comprehensive 69 benchmarking image sets, including a clean set,
sets with single factor perturbations, and sets with two-factor perturbation
conditions. The state-of-the-art two-factor perturbation includes (a) two
digital perturbations (salt & pepper noise and Gaussian noise) applied in both
sequences, and (b) one digital perturbation (salt & pepper noise) and a
geometric perturbation (rotation) applied in both sequences. Previous research
evaluating DL classifiers has often used top-1/top-5 accuracy. We innovate a
new two-dimensional, statistical matrix to evaluating robustness of DL
classifiers. Also, we introduce a new visualization tool, including minimum
accuracy, maximum accuracy, mean accuracies, and coefficient of variation (CV),
for benchmarking robustness of DL classifiers. Comparing with single factor
corruption, we first report that using two-factor perturbed images improves
both robustness and accuracy of DL classifiers. All source codes and related
image sets are shared on the Website at http://cslinux.semo.edu/david/data to
support future academic research and industry projects.",-0.0055075074,-0.25016528,0.2506369,C
2724,"Although using bimodality helps to reduce
the noise, we observe that our work, like other traditional and learning-based
algorithms, contains noise, especially in sparsely viewed regions that may need
further research.","There is usually a trade-off between
accuracy and completeness since increasing completeness also means increasing
the potential source of the noise.","It is also worth noting that in this work we have used the same
fusion pipeline as in other papers [38,44].",2022-03-02 20:25:31+00:00,DDL-MVS: Depth Discontinuity Learning for MVS Networks,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Nail Ibrahimli'), arxiv.Result.Author('Hugo Ledoux'), arxiv.Result.Author('Julian Kooij'), arxiv.Result.Author('Liangliang Nan')]","Traditional MVS methods have good accuracy but struggle with completeness,
while recently developed learning-based multi-view stereo (MVS) techniques have
improved completeness except accuracy being compromised. We propose depth
discontinuity learning for MVS methods, which further improves accuracy while
retaining the completeness of the reconstruction. Our idea is to jointly
estimate the depth and boundary maps where the boundary maps are explicitly
used for further refinement of the depth maps. We validate our idea and
demonstrate that our strategies can be easily integrated into the existing
learning-based MVS pipeline where the reconstruction depends on high-quality
depth map estimation. Extensive experiments on various datasets show that our
method improves reconstruction quality compared to baseline. Experiments also
demonstrate that the presented model and strategies have good generalization
capabilities. The source code will be available soon.",0.005794651,0.08651036,0.10868679,B
2727,"Transferring models between pinhole-, Ô¨Åsheye-, and
ary distinctions, while the baseline model is confused by      panoramic domains, fusing modalities, and solving various
the distorted shape and space, due to the lacking capacity to  tasks of 360‚ó¶ imagery are opportunities for further research.","We note that the accuracy of some classes
the segmentation results for sidewalks and pedestrians from    are still impacted by the partition boundary of panoramas at
Trans4PASS have more accurate classiÔ¨Åcations and bound-        180‚ó¶.",learn long-range contexts and distortion-aware features.,2022-03-02 23:00:32+00:00,Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Chaoxiang Ma'), arxiv.Result.Author('Simon Rei√ü'), arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Rainer Stiefelhagen')]","Panoramic images with their 360-degree directional view encompass exhaustive
information about the surrounding space, providing a rich foundation for scene
understanding. To unfold this potential in the form of robust panoramic
segmentation models, large quantities of expensive, pixel-wise annotations are
crucial for success. Such annotations are available, but predominantly for
narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal
resources for training panoramic models. Distortions and the distinct
image-feature distribution in 360-degree panoramas impede the transfer from the
annotation-rich pinhole domain and therefore come with a big dent in
performance. To get around this domain difference and bring together semantic
annotations from pinhole- and 360-degree surround-visuals, we propose to learn
object deformations and panoramic image distortions in the Deformable Patch
Embedding (DPE) and Deformable MLP (DMLP) components which blend into our
Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we
tie together shared semantics in pinhole- and panoramic feature embeddings by
generating multi-scale prototype features and aligning them in our Mutual
Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor
Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance
to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled
panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by
14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available
at https://github.com/jamycheung/Trans4PASS.",-0.37910596,0.1849401,-0.030984396,B
2728,"Transferring models between pinhole-, fisheye-, and
ary distinctions, while the baseline model is confused by      panoramic domains, fusing modalities, and solving various
the distorted shape and space, due to the lacking capacity to  tasks of 360‚ó¶ imagery are opportunities for further research.","We note that the accuracy of some classes
the segmentation results for sidewalks and pedestrians from    are still impacted by the partition boundary of panoramas at
Trans4PASS have more accurate classifications and bound-       180‚ó¶.",learn long-range contexts and distortion-aware features.,2022-03-02 23:00:32+00:00,Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Chaoxiang Ma'), arxiv.Result.Author('Simon Rei√ü'), arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Rainer Stiefelhagen')]","Panoramic images with their 360-degree directional view encompass exhaustive
information about the surrounding space, providing a rich foundation for scene
understanding. To unfold this potential in the form of robust panoramic
segmentation models, large quantities of expensive, pixel-wise annotations are
crucial for success. Such annotations are available, but predominantly for
narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal
resources for training panoramic models. Distortions and the distinct
image-feature distribution in 360-degree panoramas impede the transfer from the
annotation-rich pinhole domain and therefore come with a big dent in
performance. To get around this domain difference and bring together semantic
annotations from pinhole- and 360-degree surround-visuals, we propose to learn
object deformations and panoramic image distortions in the Deformable Patch
Embedding (DPE) and Deformable MLP (DMLP) components which blend into our
Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we
tie together shared semantics in pinhole- and panoramic feature embeddings by
generating multi-scale prototype features and aligning them in our Mutual
Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor
Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance
to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled
panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by
14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available
at https://github.com/jamycheung/Trans4PASS.",-0.3933212,0.17922346,-0.03203942,B
2738,"Finally, we explore
                                       some limitations with insightful observations and provide further research di-
                                       rection.","We thor-
                                       oughly compare the performance of various ViT algorithms and most repre-
                                       sentative CNN methods on popular benchmark datasets.","The project page along with the collections of papers are available at
                                       https://github.com/khawar512/ViT-Survey
                                       Keywords: Vision transformer, self-attention, convolutions, long-range
                                       relationship.",2022-03-03 06:17:03+00:00,Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Khawar Islam')],"Vision Transformers (ViTs) are becoming more popular and dominating technique
for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a
demanding technique in computer vision, ViTs have been successfully solved
various vision problems while focusing on long-range relationships. In this
paper, we begin by introducing the fundamental concepts and background of the
self-attention mechanism. Next, we provide a comprehensive overview of recent
top-performing ViT methods describing in terms of strength and weakness,
computational cost as well as training and testing dataset. We thoroughly
compare the performance of various ViT algorithms and most representative CNN
methods on popular benchmark datasets. Finally, we explore some limitations
with insightful observations and provide further research direction. The
project page along with the collections of papers are available at
https://github.com/khawar512/ViT-Survey",-0.22499609,-0.16905837,0.19330227,C
2739,"Finally, we explore some limitations with
                                        insightful observations and provide further research direction.","We thoroughly compare
                                        the performance of various ViT algorithms and most representative CNN meth-
                                        ods on popular benchmark datasets.","The project page
                                        along with the collections of papers are available at https://github.com/khawar-
                                        islam/ViT-Survey
                                        Keywords: Vision transformer, self-attention, convolutions, long-range
                                        relationship.",2022-03-03 06:17:03+00:00,Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Khawar Islam')],"Vision Transformers (ViTs) are becoming more popular and dominating technique
for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a
demanding technique in computer vision, ViTs have been successfully solved
various vision problems while focusing on long-range relationships. In this
paper, we begin by introducing the fundamental concepts and background of the
self-attention mechanism. Next, we provide a comprehensive overview of recent
top-performing ViT methods describing in terms of strength and weakness,
computational cost as well as training and testing dataset. We thoroughly
compare the performance of various ViT algorithms and most representative CNN
methods on popular benchmark datasets. Finally, we explore some limitations
with insightful observations and provide further research direction. The
project page along with the collections of papers are available at
https://github.com/khawar512/ViT-Survey",-0.22200921,-0.15487655,0.16493888,C
2740,"Finally, we explore some limitations with insightful         1                          3
                                       observations and provide further research direction.","We
                                       thoroughly compare the performance of various ViT algorithms           16                     49
                                       and most representative CNN methods on popular benchmark
                                       datasets.","Patch                                      49

                                                                                                                                            4                    Class token

                                                                                                              Linear Layer (Projection)

                                                                                                                                     Fig.",2022-03-03 06:17:03+00:00,Recent Advances in Vision Transformer: A Survey for Different Domains,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Khawar Islam')],"Vision Transformers (ViTs) are becoming more popular and dominating technique
for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a
demanding technique in computer vision, ViTs have been successfully solved
various vision problems while focusing on long-range relationships. In this
paper, we begin by introducing the fundamental concepts and background of the
self-attention mechanism. Next, we provide a comprehensive overview of recent
top-performing ViT methods describing in terms of strength and weakness,
computational cost as well as training and testing dataset. We thoroughly
compare the performance of various ViT algorithms and most representative CNN
methods on popular benchmark datasets. Finally, we explore some limitations
with insightful observations and provide further research direction. The
project page along with the collections of papers are available at
https://github.com/khawar512/ViT-Survey",-0.13357262,-0.20548049,0.306046,C
2741,"Finally, we explore some limitations with insightful         1                          3
                                        observations and provide further research direction.","We
                                        thoroughly compare the performance of various ViT algorithms           16                     49
                                        and most representative CNN methods on popular benchmark
                                        datasets.","Patch                                      49

                                                                                                                                             4                    Class token

                                                                                                               Linear Layer (Projection)

                                                                                                                                      Fig.",2022-03-03 06:17:03+00:00,Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Khawar Islam')],"Vision Transformers (ViTs) are becoming more popular and dominating technique
for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a
demanding technique in computer vision, ViTs have been successfully solved
various vision problems while focusing on long-range relationships. In this
paper, we begin by introducing the fundamental concepts and background of the
self-attention mechanism. Next, we provide a comprehensive overview of recent
top-performing ViT methods describing in terms of strength and weakness,
computational cost as well as training and testing dataset. We thoroughly
compare the performance of various ViT algorithms and most representative CNN
methods on popular benchmark datasets. Finally, we explore some limitations
with insightful observations and provide further research direction. The
project page along with the collections of papers are available at
https://github.com/khawar512/ViT-Survey",-0.13357262,-0.20548049,0.306046,C
2742,"To the best of our knowledge, this is the Ô¨Årst ap-
    MADDG [24]              17.02       19.68       20.87       25.02  proach using video-based transformer for face PAD which
      MDRL [30]             16.67       23.11       18.21       25.17  can serve as a new backbone for further study.","We also introduce
                            22.72       33.52       29.14       30.17  convolutions to our ViTransPAD to integrate desirable propri-
Auxiliary(Depth) [17]       27.08       44.59       31.58       40.98  eties of CNNs which can gain a good computation-accuracy
   MMD-AAE [29]             17.69        24.5       22.19       27.98  balance.","SSDG-M [31]             10.83       17.85       16.03       15.67
      ANRL [25]              8.39       21.27       16.83       15.63                          6.",2022-03-03 08:23:20+00:00,ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zuheng Ming'), arxiv.Result.Author('Zitong Yu'), arxiv.Result.Author('Musab Al-Ghadi'), arxiv.Result.Author('Muriel Visani'), arxiv.Result.Author('Muhammad MuzzamilLuqman'), arxiv.Result.Author('Jean-Christophe Burie')]","Face Presentation Attack Detection (PAD) is an important measure to prevent
spoof attacks for face biometric systems. Many works based on Convolution
Neural Networks (CNNs) for face PAD formulate the problem as an image-level
binary classification task without considering the context. Alternatively,
Vision Transformers (ViT) using self-attention to attend the context of an
image become the mainstreams in face PAD. Inspired by ViT, we propose a
Video-based Transformer for face PAD (ViTransPAD) with short/long-range
spatio-temporal attention which can not only focus on local details with short
attention within a frame but also capture long-range dependencies over frames.
Instead of using coarse image patches with single-scale as in ViT, we propose
the Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate
multi-scale patch partitions of Q, K, V feature maps to the heads of
transformer in a coarse-to-fine manner, which enables to learn a fine-grained
representation to perform pixel-level discrimination for face PAD. Due to lack
inductive biases of convolutions in pure transformers, we also introduce
convolutions to the proposed ViTransPAD to integrate the desirable properties
of CNNs by using convolution patch embedding and convolution projection. The
extensive experiments show the effectiveness of our proposed ViTransPAD with a
preferable accuracy-computation balance, which can serve as a new backbone for
face PAD.",0.021513285,0.08555351,0.23944615,C
2744,"Network performance limitations result from the in-
this survey will embrace as much literature as possible to       herent networks drawbacks that inevitably involve error ac-
provide a better understanding of current work and oÔ¨Äer ref-     cumulation from RNNs [108] and are stuck due to the lim-
erence for further research in this Ô¨Åeld.","As        which leads to a high degree of uncertainty for future human
3D human motion prediction is an emerging research Ô¨Åeld,         poses.","itation in processing standard 2D grid representations from
                                                                 CNNs.",2022-03-03 09:46:43+00:00,3D Human Motion Prediction: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kedi Lyu'), arxiv.Result.Author('Haipeng Chen'), arxiv.Result.Author('Zhenguang Liu'), arxiv.Result.Author('Beiqi Zhang'), arxiv.Result.Author('Ruili Wang')]","3D human motion prediction, predicting future poses from a given sequence, is
an issue of great significance and challenge in computer vision and machine
intelligence, which can help machines in understanding human behaviors. Due to
the increasing development and understanding of Deep Neural Networks (DNNs) and
the availability of large-scale human motion datasets, the human motion
prediction has been remarkably advanced with a surge of interest among academia
and industrial community. In this context, a comprehensive survey on 3D human
motion prediction is conducted for the purpose of retrospecting and analyzing
relevant works from existing released literature. In addition, a pertinent
taxonomy is constructed to categorize these existing approaches for 3D human
motion prediction. In this survey, relevant methods are categorized into three
categories: human pose representation, network structure design, and
\textit{prediction target}. We systematically review all relevant journal and
conference papers in the field of human motion prediction since 2015, which are
presented in detail based on proposed categorizations in this survey.
Furthermore, the outline for the public benchmark datasets, evaluation
criteria, and performance comparisons are respectively presented in this paper.
The limitations of the state-of-the-art methods are discussed as well, hoping
for paving the way for future explorations.",-0.18780202,0.0013964577,0.17981856,B
2745,"Three main problem-solving strategies
Section 6, the most advanced methods are discussed and the      are inspired among current research results, which are sum-
possible challenges for further research are listed.",In       Prediction results.,"Finally,   marized in this paper as: Human pose representations, Net-
Section 7 is the conclusion of this paper.",2022-03-03 09:46:43+00:00,3D Human Motion Prediction: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kedi Lyu'), arxiv.Result.Author('Haipeng Chen'), arxiv.Result.Author('Zhenguang Liu'), arxiv.Result.Author('Beiqi Zhang'), arxiv.Result.Author('Ruili Wang')]","3D human motion prediction, predicting future poses from a given sequence, is
an issue of great significance and challenge in computer vision and machine
intelligence, which can help machines in understanding human behaviors. Due to
the increasing development and understanding of Deep Neural Networks (DNNs) and
the availability of large-scale human motion datasets, the human motion
prediction has been remarkably advanced with a surge of interest among academia
and industrial community. In this context, a comprehensive survey on 3D human
motion prediction is conducted for the purpose of retrospecting and analyzing
relevant works from existing released literature. In addition, a pertinent
taxonomy is constructed to categorize these existing approaches for 3D human
motion prediction. In this survey, relevant methods are categorized into three
categories: human pose representation, network structure design, and
\textit{prediction target}. We systematically review all relevant journal and
conference papers in the field of human motion prediction since 2015, which are
presented in detail based on proposed categorizations in this survey.
Furthermore, the outline for the public benchmark datasets, evaluation
criteria, and performance comparisons are respectively presented in this paper.
The limitations of the state-of-the-art methods are discussed as well, hoping
for paving the way for future explorations.",-0.16258895,0.043041155,-0.30301738,B
2746,"Network performance limitations result from the in-
this survey will embrace as much literature as possible to       herent networks drawbacks that inevitably involve error ac-
provide a better understanding of current work and oÔ¨Äer ref-     cumulation from RNNs [108] and are stuck due to the lim-
erence for further research in this Ô¨Åeld.","As        which leads to a high degree of uncertainty for future human
3D human motion prediction is an emerging research Ô¨Åeld,         poses.","itation in processing standard 2D grid representations from
                                                                 CNNs.",2022-03-03 09:46:43+00:00,3D Human Motion Prediction: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kedi Lyu'), arxiv.Result.Author('Haipeng Chen'), arxiv.Result.Author('Zhenguang Liu'), arxiv.Result.Author('Beiqi Zhang'), arxiv.Result.Author('Ruili Wang')]","3D human motion prediction, predicting future poses from a given sequence, is
an issue of great significance and challenge in computer vision and machine
intelligence, which can help machines in understanding human behaviors. Due to
the increasing development and understanding of Deep Neural Networks (DNNs) and
the availability of large-scale human motion datasets, the human motion
prediction has been remarkably advanced with a surge of interest among academia
and industrial community. In this context, a comprehensive survey on 3D human
motion prediction is conducted for the purpose of retrospecting and analyzing
relevant works from existing released literature. In addition, a pertinent
taxonomy is constructed to categorize these existing approaches for 3D human
motion prediction. In this survey, relevant methods are categorized into three
categories: human pose representation, network structure design, and
\textit{prediction target}. We systematically review all relevant journal and
conference papers in the field of human motion prediction since 2015, which are
presented in detail based on proposed categorizations in this survey.
Furthermore, the outline for the public benchmark datasets, evaluation
criteria, and performance comparisons are respectively presented in this paper.
The limitations of the state-of-the-art methods are discussed as well, hoping
for paving the way for future explorations.",-0.18780202,0.0013964577,0.17981856,B
2747,"Three main problem-solving strategies
Section 6, the most advanced methods are discussed and the      are inspired among current research results, which are sum-
possible challenges for further research are listed.",In       Prediction results.,"Finally,   marized in this paper as: Human pose representations, Net-
Section 7 is the conclusion of this paper.",2022-03-03 09:46:43+00:00,3D Human Motion Prediction: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kedi Lyu'), arxiv.Result.Author('Haipeng Chen'), arxiv.Result.Author('Zhenguang Liu'), arxiv.Result.Author('Beiqi Zhang'), arxiv.Result.Author('Ruili Wang')]","3D human motion prediction, predicting future poses from a given sequence, is
an issue of great significance and challenge in computer vision and machine
intelligence, which can help machines in understanding human behaviors. Due to
the increasing development and understanding of Deep Neural Networks (DNNs) and
the availability of large-scale human motion datasets, the human motion
prediction has been remarkably advanced with a surge of interest among academia
and industrial community. In this context, a comprehensive survey on 3D human
motion prediction is conducted for the purpose of retrospecting and analyzing
relevant works from existing released literature. In addition, a pertinent
taxonomy is constructed to categorize these existing approaches for 3D human
motion prediction. In this survey, relevant methods are categorized into three
categories: human pose representation, network structure design, and
\textit{prediction target}. We systematically review all relevant journal and
conference papers in the field of human motion prediction since 2015, which are
presented in detail based on proposed categorizations in this survey.
Furthermore, the outline for the public benchmark datasets, evaluation
criteria, and performance comparisons are respectively presented in this paper.
The limitations of the state-of-the-art methods are discussed as well, hoping
for paving the way for future explorations.",-0.16258895,0.043041155,-0.30301738,B
2767,"We hope that
a speed of over 27 FPS during the tests without the accel-                                                                           our framework can inspire further researches in aerial and

    2https://www.nvidia.com/en-us/autonomous-machines/embedded-                                                                          3https://developer.nvidia.com/tensorrt
systems/jetson-agx-xavier/, https://pixhawk.org/
even general tracking with temporal contexts.","Finally, our tracker remains at                                                                            fectiveness and efÔ¨Åciency of our framework.","[11] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Potential limitations.",2022-03-03 18:04:20+00:00,TCTrack: Temporal Contexts for Aerial Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Ziyuan Huang'), arxiv.Result.Author('Liang Pan'), arxiv.Result.Author('Shiwei Zhang'), arxiv.Result.Author('Ziwei Liu'), arxiv.Result.Author('Changhong Fu')]","Temporal contexts among consecutive frames are far from been fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.",-0.03562562,0.25290054,0.021100605,B
2768,"We hope that
a speed of over 27 FPS during the tests without the accel-                                                                             our framework can inspire further researches in aerial and

    2https://www.nvidia.com/en-us/autonomous-machines/embedded-                                                                            3https://developer.nvidia.com/tensorrt
systems/jetson-agx-xavier/, https://pixhawk.org/
even general tracking with temporal contexts.","Finally, our tracker remains at                                                                              fectiveness and efÔ¨Åciency of our framework.","[11] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Potential limitations.",2022-03-03 18:04:20+00:00,TCTrack: Temporal Contexts for Aerial Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Ziyuan Huang'), arxiv.Result.Author('Liang Pan'), arxiv.Result.Author('Shiwei Zhang'), arxiv.Result.Author('Ziwei Liu'), arxiv.Result.Author('Changhong Fu')]","Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.",-0.03562562,0.25290054,0.021100605,B
2769,"We hope that
   The special challenges in the real-world tests involve dif-                                                    our framework can inspire further research in aerial and
ferent illumination, scale variation, occlusion, motion blur,                                                     even general tracking with temporal contexts.","marks and real-world tests on our UAV demonstrate the ef-
                                                                                                                  fectiveness and efÔ¨Åciency of our framework.",and low-resolution scenes.,2022-03-03 18:04:20+00:00,TCTrack: Temporal Contexts for Aerial Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Ziyuan Huang'), arxiv.Result.Author('Liang Pan'), arxiv.Result.Author('Shiwei Zhang'), arxiv.Result.Author('Ziwei Liu'), arxiv.Result.Author('Changhong Fu')]","Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.",-0.12319744,0.3517443,0.03561554,B
2777,"samples, which has inspired further research into disentan-      [C2] We demonstrate on a range of DGNs and and datasets
glement of these quantities as precision and recall [32, 45]     that polarity sampling not only allows one to move on the
metrics respectively.",3).,precision-recall Pareto frontier (Sec.,2022-03-03 20:16:49+00:00,Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ahmed Imtiaz Humayun'), arxiv.Result.Author('Randall Balestriero'), arxiv.Result.Author('Richard Baraniuk')]","We present Polarity Sampling, a theoretically justified plug-and-play method
for controlling the generation quality and diversity of pre-trained deep
generative networks DGNs). Leveraging the fact that DGNs are, or can be
approximated by, continuous piecewise affine splines, we derive the analytical
DGN output space distribution as a function of the product of the DGN's
Jacobian singular values raised to a power $\rho$. We dub $\rho$ the
$\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on
the modes ($\rho < 0$) or anti-modes ($\rho > 0$) of the DGN output-space
distribution. We demonstrate that nonzero polarity values achieve a better
precision-recall (quality-diversity) Pareto frontier than standard methods,
such as truncation, for a number of state-of-the-art DGNs. We also present
quantitative and qualitative results on the improvement of overall generation
quality (e.g., in terms of the Frechet Inception Distance) for a number of
state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different
conditional and unconditional image generation tasks. In particular, Polarity
Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to
FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the
AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-demo-colab",0.18630847,-0.036278695,0.021898277,A
2778,"We provide pseudocode for Polarity Sam-
linearly combines measures of quality and diversity of the      pling and an approximation scheme to control its computa-
samples, which has inspired further research into disentan-     tional complexity as desired (Sec.","‚àóequal contribution
It has been established in prior work [45] that FID non-        cobian matrix.",3).,2022-03-03 20:16:49+00:00,Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ahmed Imtiaz Humayun'), arxiv.Result.Author('Randall Balestriero'), arxiv.Result.Author('Richard Baraniuk')]","We present Polarity Sampling, a theoretically justified plug-and-play method
for controlling the generation quality and diversity of pre-trained deep
generative networks DGNs). Leveraging the fact that DGNs are, or can be
approximated by, continuous piecewise affine splines, we derive the analytical
DGN output space distribution as a function of the product of the DGN's
Jacobian singular values raised to a power $\rho$. We dub $\rho$ the
$\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on
the modes ($\rho < 0$) or anti-modes ($\rho > 0$) of the DGN output-space
distribution. We demonstrate that nonzero polarity values achieve a better
precision-recall (quality-diversity) Pareto frontier than standard methods,
such as truncation, for a number of state-of-the-art DGNs. We also present
quantitative and qualitative results on the improvement of overall generation
quality (e.g., in terms of the Frechet Inception Distance) for a number of
state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different
conditional and unconditional image generation tasks. In particular, Polarity
Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to
FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the
AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-samp",0.4140401,0.10379639,0.18249922,A
2795,"The     Performance on VG-200 As previously discussed, VG-
train/test set will be released for further research.","val split, we randomly select 10,000 images as the test set
and the rest 98,249 images are used as the training set.","200 is a more challenging benchmark that covers much more
                                                                 categories.",2022-03-04 07:56:16+00:00,Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tao Pu'), arxiv.Result.Author('Tianshui Chen'), arxiv.Result.Author('Hefeng Wu'), arxiv.Result.Author('Liang Lin')]","Training the multi-label image recognition models with partial labels, in
which merely some labels are known while others are unknown for each image, is
a considerably challenging and practical task. To address this task, current
algorithms mainly depend on pre-training classification or similarity models to
generate pseudo labels for the unknown labels. However, these algorithms depend
on sufficient multi-label annotations to train the models, leading to poor
performance especially with low known label proportion. In this work, we
propose to blend category-specific representation across different images to
transfer information of known labels to complement unknown labels, which can
get rid of pre-training models and thus does not depend on sufficient
annotations. To this end, we design a unified semantic-aware representation
blending (SARB) framework that exploits instance-level and prototype-level
semantic representation to complement unknown labels by two complementary
modules: 1) an instance-level representation blending (ILRB) module blends the
representations of the known labels in an image to the representations of the
unknown labels in another image to complement these unknown labels. 2) a
prototype-level representation blending (PLRB) module learns more stable
representation prototypes for each category and blends the representation of
unknown labels with the prototypes of corresponding labels to complement these
labels. Extensive experiments on the MS-COCO, Visual Genome, Pascal VOC 2007
datasets show that the proposed SARB framework obtains superior performance
over current leading competitors on all known label proportion settings, i.e.,
with the mAP improvement of 4.6%, 4.%, 2.2% on these three datasets when the
known label proportion is 10%. Codes are available at
https://github.com/HCPLab-SYSU/HCP-MLR-PL.",0.07349385,-0.021910705,0.14315765,A
2798,There are several issues need further study.,"We experimentally show that PWAN can effectively handle the point sets dominated
by outliers, including those containing large fraction of noise or being partially overlapped.","First, the computation time of PWAN is still relatively
high.",2022-03-04 10:23:48+00:00,Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zi-Ming Wang'), arxiv.Result.Author('Nan Xue'), arxiv.Result.Author('Ling Lei'), arxiv.Result.Author('Gui-Song Xia')]","Given two point sets, the problem of registration is to recover a
transformation that matches one set to the other. This task is challenging due
to the presence of the large number of outliers, the unknown non-rigid
deformations and the large sizes of point sets. To obtain strong robustness
against outliers, we formulate the registration problem as a partial
distribution matching (PDM) problem, where the goal is to partially match the
distributions represented by point sets in a metric space. To handle large
point sets, we propose a scalable PDM algorithm by utilizing the efficient
partial Wasserstein-1 (PW) discrepancy. Specifically, we derive the
Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient
can be explicitly computed. Based on these results, we propose a partial
Wasserstein adversarial network (PWAN), which is able to approximate the PW
discrepancy by a neural network, and minimize it by gradient descent. In
addition, it also incorporates an efficient coherence regularizer for non-rigid
transformations to avoid unrealistic deformations. We evaluate PWAN on
practical point set registration tasks, and show that the proposed PWAN is
robust, scalable and performs more favorably than the state-of-the-art methods.",0.14758246,0.29778656,0.13646448,A
2805,"8 Research Directions in Sports Vision

    Based on the investigation of available articles in sports, we were able to come out with various
research topics and identifies research directions to be taken for further research in sports.","It provides in-depth performance of any team, in terms of tracking
every player from both the team to provide comprehensive match coverage, collecting data to provide
tactical analysis of the match and highlighting the performance deviations to reduce injuries in the game.","They are
categorized based on the task specifics in sports applications (such as major sports in which
player/ball/referee detection and tracking, pose estimation, trajectory prediction is required) as shown in
figure 18 to provide promising and potential research directions for future computer vision/video
processing in various sports.",2022-03-03 07:49:21+00:00,"A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions",cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Banoth Thulasya Naik'), arxiv.Result.Author('Mohammad Farukh Hashmi'), arxiv.Result.Author('Neeraj Dhanraj Bokde'), arxiv.Result.Author('Zaher Mundher Yaseen')]","Recent developments in video analysis of sports and computer vision
techniques have achieved significant improvements to enable a variety of
critical operations. To provide enhanced information, such as detailed complex
analysis in sports like soccer, basketball, cricket, badminton, etc., studies
have focused mainly on computer vision techniques employed to carry out
different tasks. This paper presents a comprehensive review of sports video
analysis for various applications high-level analysis such as detection and
classification of players, tracking player or ball in sports and predicting the
trajectories of player or ball, recognizing the teams strategies, classifying
various events in sports. The paper further discusses published works in a
variety of application-specific tasks related to sports and the present
researchers views regarding them. Since there is a wide research scope in
sports for deploying computer vision techniques in various sports, some of the
publicly available datasets related to a particular sport have been provided.
This work reviews a detailed discussion on some of the artificial
intelligence(AI)applications in sports vision, GPU-based work stations, and
embedded platforms. Finally, this review identifies the research directions,
probable challenges, and future trends in the area of visual recognition in
sports.",-0.18636882,0.2891603,-0.26553738,B
2806,"8 Research Directions in Sports Vision

    Based on the investigation of available articles in sports, we were able to come out with various research
topics and identifies research directions to be taken for further research in sports.","It provides in-depth performance of any team, in terms of tracking every player from both
the team to provide comprehensive match coverage, collecting data to provide tactical analysis of the match
and highlighting the performance deviations to reduce injuries in the game.","They are categorized
based on the task specifics in sports applications (such as major sports in which player/ball/referee detection
and tracking, pose estimation, trajectory prediction is required) as shown in figure 18 to provide promising
and potential research directions for future computer vision/video processing in various sports.",2022-03-03 07:49:21+00:00,"A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions",cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Banoth Thulasya Naik'), arxiv.Result.Author('Mohammad Farukh Hashmi'), arxiv.Result.Author('Neeraj Dhanraj Bokde')]","Recent developments in video analysis of sports and computer vision
techniques have achieved significant improvements to enable a variety of
critical operations. To provide enhanced information, such as detailed complex
analysis in sports like soccer, basketball, cricket, badminton, etc., studies
have focused mainly on computer vision techniques employed to carry out
different tasks. This paper presents a comprehensive review of sports video
analysis for various applications high-level analysis such as detection and
classification of players, tracking player or ball in sports and predicting the
trajectories of player or ball, recognizing the teams strategies, classifying
various events in sports. The paper further discusses published works in a
variety of application-specific tasks related to sports and the present
researchers views regarding them. Since there is a wide research scope in
sports for deploying computer vision techniques in various sports, some of the
publicly available datasets related to a particular sport have been provided.
This work reviews a detailed discussion on some of the artificial
intelligence(AI)applications in sports vision, GPU-based work stations, and
embedded platforms. Finally, this review identifies the research directions,
probable challenges, and future trends in the area of visual recognition in
sports.",-0.18636882,0.2891603,-0.26553738,B
2826,"[19] K. Wang and S. Shen, ‚ÄúMVDepthNet: Real-time Multiview Depth
system as they become available, allowing our system to                            Estimation Neural Network,‚Äù in 3DV, 2018.
function as a useful platform for further research over the
longer-term.","6: Two examples (on scenes S1 and M2 of our dataset)                    [18] W. N. Greene, K. Ok, P. Lommel, and N. Roy, ‚ÄúMulti-Level Mapping:
of the rigid alignment between our reconstructions (blue) and                      Real-time Dense Monocular SLAM,‚Äù in ICRA, 2016.
the ground truth (coloured).","[20] W. N. Greene and N. Roy, ‚ÄúMultiViewStereoNet: Fast Multi-View
                                                                                   Stereo Depth Estimation using Incremental Viewpoint-Compensated
                      ACKNOWLEDGEMENTS                                             Feature Extraction,‚Äù in ICRA, 2021.",2022-03-04 17:31:26+00:00,Real-Time Hybrid Mapping of Populated Indoor Scenes using a Low-Cost Monocular UAV,cs.CV,"['cs.CV', 'cs.RO', '68T45', 'I.2.10; I.2.9']","[arxiv.Result.Author('Stuart Golodetz'), arxiv.Result.Author('Madhu Vankadari'), arxiv.Result.Author('Aluna Everitt'), arxiv.Result.Author('Sangyun Shin'), arxiv.Result.Author('Andrew Markham'), arxiv.Result.Author('Niki Trigoni')]","Unmanned aerial vehicles (UAVs) have been used for many applications in
recent years, from urban search and rescue, to agricultural surveying, to
autonomous underground mine exploration. However, deploying UAVs in tight,
indoor spaces, especially close to humans, remains a challenge. One solution,
when limited payload is required, is to use micro-UAVs, which pose less risk to
humans and typically cost less to replace after a crash. However, micro-UAVs
can only carry a limited sensor suite, e.g. a monocular camera instead of a
stereo pair or LiDAR, complicating tasks like dense mapping and markerless
multi-person 3D human pose estimation, which are needed to operate in tight
environments around people. Monocular approaches to such tasks exist, and dense
monocular mapping approaches have been successfully deployed for UAV
applications. However, despite many recent works on both marker-based and
markerless multi-UAV single-person motion capture, markerless single-camera
multi-person 3D human pose estimation remains a much earlier-stage technology,
and we are not aware of existing attempts to deploy it in an aerial context. In
this paper, we present what is thus, to our knowledge, the first system to
perform simultaneous mapping and multi-person 3D human pose estimation from a
monocular camera mounted on a single UAV. In particular, we show how to loosely
couple state-of-the-art monocular depth estimation and monocular 3D human pose
estimation approaches to reconstruct a hybrid map of a populated indoor scene
in real time. We validate our component-level design choices via extensive
experiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our
system-level performance, we also construct a new Oxford Hybrid Mapping dataset
of populated indoor scenes.",-0.29301378,0.2932666,0.13107201,B
2828,"In general though, we think that landmarks-based behavior forecasting is
still immature, and will strongly beneÔ¨Åt from further research eÔ¨Äorts.","For example, the
proper activation of the actuators of a robot may beneÔ¨Åt from any extra milliseconds of
anticipation.","Another concerning
issue related to this topic lies on the typology of the data leveraged for forecasting.",2022-03-04 18:25:30+00:00,Didn't see that coming: a survey on non-verbal social human behavior forecasting,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('German Barquero'), arxiv.Result.Author('Johnny N√∫√±ez'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Zhen Xu'), arxiv.Result.Author('Wei-Wei Tu'), arxiv.Result.Author('Isabelle Guyon'), arxiv.Result.Author('Cristina Palmero')]","Non-verbal social human behavior forecasting has increasingly attracted the
interest of the research community in recent years. Its direct applications to
human-robot interaction and socially-aware human motion generation make it a
very attractive field. In this survey, we define the behavior forecasting
problem for multiple interactive agents in a generic way that aims at unifying
the fields of social signals prediction and human motion forecasting,
traditionally separated. We hold that both problem formulations refer to the
same conceptual problem, and identify many shared fundamental challenges:
future stochasticity, context awareness, history exploitation, etc. We also
propose a taxonomy that comprises methods published in the last 5 years in a
very informative way and describes the current main concerns of the community
with regard to this problem. In order to promote further research on this
field, we also provide a summarised and friendly overview of audiovisual
datasets featuring non-acted social interactions. Finally, we describe the most
common metrics used in this task and their particular issues.",0.058735657,0.0686764,-0.27383277,B
2867,"In contrast, we con-                 1        2                    k
struct graph nodes over the Habitat-MP3D spaces, and we              rectional encoding dpi to indicate the relative orientation of
further study the idea of training agents directly in the con-
tinuous environments with high-level actions.","Each feature vpi is enhanced with a di-
invalid edges going across obstacles.","the waypoint with respect to the agent‚Äôs heading, denoted
                                                                     as f pi ‚Äú rvpi ; dpi s, which has been shown inÔ¨Çuential in later

                                                                     works [24, 26].",2022-03-05 14:56:14+00:00,Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation,cs.CV,"['cs.CV', 'cs.CL', 'cs.RO']","[arxiv.Result.Author('Yicong Hong'), arxiv.Result.Author('Zun Wang'), arxiv.Result.Author('Qi Wu'), arxiv.Result.Author('Stephen Gould')]","Most existing works in vision-and-language navigation (VLN) focus on either
discrete or continuous environments, training agents that cannot generalize
across the two. The fundamental difference between the two setups is that
discrete navigation assumes prior knowledge of the connectivity graph of the
environment, so that the agent can effectively transfer the problem of
navigation with low-level controls to jumping from node to node with high-level
actions by grounding to an image of a navigable direction. To bridge the
discrete-to-continuous gap, we propose a predictor to generate a set of
candidate waypoints during navigation, so that agents designed with high-level
actions can be transferred to and trained in continuous environments. We refine
the connectivity graph of Matterport3D to fit the continuous
Habitat-Matterport3D, and train the waypoints predictor with the refined graphs
to produce accessible waypoints at each time step. Moreover, we demonstrate
that the predicted waypoints can be augmented during training to diversify the
views and paths, and therefore enhance agent's generalization ability. Through
extensive experiments we show that agents navigating in continuous environments
with predicted waypoints perform significantly better than agents using
low-level actions, which reduces the absolute discrete-to-continuous gap by
11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent
and 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple
imitation learning objective, outperform previous methods by a large margin,
achieving new state-of-the-art results on the testing environments of the
R2R-CE and the RxR-CE datasets.",0.030289602,0.0254322,-0.083342075,B
2883,"To encourage further research, we have made publicly
available the dataset used for this work in https://bit.ly/3ImvkmC, as well as helped in organising the Parasitic Egg
Detection challenge, which contains an even richer and more challenging dataset.","By doing so, the framework would be reduced to a unique step in which the transformation of the
image would have a direct impact on the learning of the object detection.",7.,2022-03-06 11:44:35+00:00,Detection of Parasitic Eggs from Microscopy Images and the emergence of a new dataset,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Perla Mayo'), arxiv.Result.Author('Nantheera Anantrasirichai'), arxiv.Result.Author('Thanarat H. Chalidabhongse'), arxiv.Result.Author('Duangdao Palasuwan'), arxiv.Result.Author('Alin Achim')]","Automatic detection of parasitic eggs in microscopy images has the potential
to increase the efficiency of human experts whilst also providing an objective
assessment. The time saved by such a process would both help ensure a prompt
treatment to patients, and off-load excessive work from experts' shoulders.
Advances in deep learning inspired us to exploit successful architectures for
detection, adapting them to tackle a different domain. We propose a framework
that exploits two such state-of-the-art models. Specifically, we demonstrate
results produced by both a Generative Adversarial Network (GAN) and
Faster-RCNN, for image enhancement and object detection respectively, on
microscopy images of varying quality. The use of these techniques yields
encouraging results, though further improvements are still needed for certain
egg types whose detection still proves challenging. As a result, a new dataset
has been created and made publicly available, providing an even wider range of
classes and variability.",-0.2962361,-0.04854802,0.024631035,B
2884,"We have proposed a multi-step graph reasoning model
                                                                  that is enhanced by a novel dynamic memory, which it-
                 Qualitative Analysis                             eratively performs explicit and implicit reasoning over a
                                                                  key-value triplet memory and a spatial-aware image graph,
To further study the working mechanism of the DMMGR               respectively, to infer the answer in a KVQA task.","observe that the accuracy decreases by approximately 2%
when the reasoning module has no access to the knowledge                                Conclusion
triplets, which supports our proposed knowledge-guided im-
age graph reasoning module.","Our
model, we randomly select two samples from the test set that      model achieves new state-of-the-art performance on both the
are correctly answered by our model, and present the corre-       KRVQR and FVQA datasets.",2022-03-06 15:19:39+00:00,Dynamic Key-value Memory Enhanced Multi-step Graph Reasoning for Knowledge-based Visual Question Answering,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Mingxiao Li'), arxiv.Result.Author('Marie-Francine Moens')]","Knowledge-based visual question answering (VQA) is a vision-language task
that requires an agent to correctly answer image-related questions using
knowledge that is not presented in the given image. It is not only a more
challenging task than regular VQA but also a vital step towards building a
general VQA system. Most existing knowledge-based VQA systems process knowledge
and image information similarly and ignore the fact that the knowledge base
(KB) contains complete information about a triplet, while the extracted image
information might be incomplete as the relations between two objects are
missing or wrongly detected. In this paper, we propose a novel model named
dynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which
performs explicit and implicit reasoning over a key-value knowledge memory
module and a spatial-aware image graph, respectively. Specifically, the memory
module learns a dynamic knowledge representation and generates a
knowledge-aware question representation at each reasoning step. Then, this
representation is used to guide a graph attention operator over the
spatial-aware image graph. Our model achieves new state-of-the-art accuracy on
the KRVQR and FVQA datasets. We also conduct ablation experiments to prove the
effectiveness of each component of the proposed model.",-0.07548752,-0.19301239,-0.11296843,C
2885,"To further study the effective-
    4https://github.com/vmurahari3/visdial-bert        ness of the two soft constraints, we perform an abla-
Model                           MRR ‚Üë                   R@1 ‚Üë  R@5 ‚Üë  R@10 ‚Üë  Mean ‚Üì
LF (Das et al., 2017)            55.42                  40.95  72.45   82.83   5.95
HRE (Das et al., 2017)           54.16                  39.93  70.45   81.50   6.41
MN (Das et al., 2017)            55.49                  40.98  72.30   83.30   5.92
CorefNMN (Kottur et al., 2018)   61.50                  47.55  78.10   88.80   4.51
FGA (Schwartz et al., 2019)      63.70                  49.58  80.98   88.55   4.51
RVA (Niu et al., 2019)           63.03                  49.03  80.40   89.83   4.18
HACAN (Yang et al., 2019)        64.22                  50.88  80.63   89.45   4.20
Synergistic (Guo et al., 2019)   62.20                  47.90  80.43   89.95   4.17
DAN (Kang et al., 2019)          63.20                  49.63  79.75   89.35   4.30
Dual VD (Jiang et al., 2020)     63.23                  49.25  80.23   89.70   4.11
CAG (Guo et al., 2020)           63.49                  49.85  80.63   90.15   4.11
Baseline Model                   62.13                  47.38  80.40   90.17   4.09
Model + POS Embedding Only       64.20                  48.10  81.22   90.20   3.98
Model + POS Loss only            64.78                  49.88  82.40   90.85   3.86
Model + C1                       65.44                  51.20  83.38   92.03   3.64
Model + C2                       66.14                  51.97  83.63   91.55   3.60
Model + C1 + C2                  66.53                  52.63  84.13   92.50   3.40

Table 1: Results of the visual dialog models on the VisDial v1.0 test set.","and Ba, 2014) algorithm with a base learning rate
                                                          Ablation study.","C1 and C2 refer to the POS constraint
and nearest preference constraint, respectively.",2022-03-06 15:22:24+00:00,Modeling Coreference Relations in Visual Dialog,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Mingxiao Li'), arxiv.Result.Author('Marie-Francine Moens')]","Visual dialog is a vision-language task where an agent needs to answer a
series of questions grounded in an image based on the understanding of the
dialog history and the image. The occurrences of coreference relations in the
dialog makes it a more challenging task than visual question-answering. Most
previous works have focused on learning better multi-modal representations or
on exploring different ways of fusing visual and language features, while the
coreferences in the dialog are mainly ignored. In this paper, based on
linguistic knowledge and discourse features of human dialog we propose two soft
constraints that can improve the model's ability of resolving coreferences in
dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset
shows that our model, which integrates two novel and linguistically inspired
soft constraints in a deep transformer neural architecture, obtains new
state-of-the-art performance in terms of recall at 1 and other evaluation
metrics compared to current existing models and this without pretraining on
other vision-language datasets. Our qualitative results also demonstrate the
effectiveness of the method that we propose.",0.52057683,0.11036435,0.09270207,A
2886,"To further study
Figure 5: Attention score map of our best model (Model C1+C2).","We
                                                                   can also see the trend that the models‚Äô performance
                                                                   is worse when the dialog has more coreferences,
                                                                   which is reasonable and consistent with our previ-
                                                                   ous assumption that the existence of coreferences
                                                                   makes this task more difÔ¨Åcult.","(a) Two correct cases: The attention map shows
that the pronoun correctly attends to its antecedent.",2022-03-06 15:22:24+00:00,Modeling Coreference Relations in Visual Dialog,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Mingxiao Li'), arxiv.Result.Author('Marie-Francine Moens')]","Visual dialog is a vision-language task where an agent needs to answer a
series of questions grounded in an image based on the understanding of the
dialog history and the image. The occurrences of coreference relations in the
dialog makes it a more challenging task than visual question-answering. Most
previous works have focused on learning better multi-modal representations or
on exploring different ways of fusing visual and language features, while the
coreferences in the dialog are mainly ignored. In this paper, based on
linguistic knowledge and discourse features of human dialog we propose two soft
constraints that can improve the model's ability of resolving coreferences in
dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset
shows that our model, which integrates two novel and linguistically inspired
soft constraints in a deep transformer neural architecture, obtains new
state-of-the-art performance in terms of recall at 1 and other evaluation
metrics compared to current existing models and this without pretraining on
other vision-language datasets. Our qualitative results also demonstrate the
effectiveness of the method that we propose.",0.16493575,-0.2646802,-0.22036894,A
2893,"We believe that evaluating a deterministic model versus a generative model is an
open question that needs a further research and it is a limitation similar to the
KDE [12] limitation.","In the supplementary, we discuss both cases.","Now, we define the Average Mahalanobis Distance (AMD):

                       \label {eq:AMD} \text {AMD} = \frac {1}{N \times T_p} \sum _{n \in N} \sum _{t \in T_p} M_D\left (\hat {\mu }_{\text {GMM},t}^n,\hat {G}_t^n,p_t^n\right )  (3)

4 The Average Maximum Eigenvalue (AMV) Metric

A major concern of the AMD metric is that it is highly correlated with the
variance of the distribution.",2022-03-06 21:28:40+00:00,Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Abduallah Mohamed'), arxiv.Result.Author('Deyao Zhu'), arxiv.Result.Author('Warren Vu'), arxiv.Result.Author('Mohamed Elhoseiny'), arxiv.Result.Author('Christian Claudel')]","Best-of-N (BoN) Average Displacement Error (ADE)/ Final Displacement Error
(FDE) is the most used metric for evaluating trajectory prediction models. Yet,
the BoN does not quantify the whole generated samples, resulting in an
incomplete view of the model's prediction quality and performance. We propose a
new metric, Average Mahalanobis Distance (AMD) to tackle this issue. AMD is a
metric that quantifies how close the whole generated samples are to the ground
truth. We also introduce the Average Maximum Eigenvalue (AMV) metric that
quantifies the overall spread of the predictions. Our metrics are validated
empirically by showing that the ADE/FDE is not sensitive to distribution
shifts, giving a biased sense of accuracy, unlike the AMD/AMV metrics. We
introduce the usage of Implicit Maximum Likelihood Estimation (IMLE) as a
replacement for traditional generative models to train our model,
Social-Implicit. IMLE training mechanism aligns with AMD/AMV objective of
predicting trajectories that are close to the ground truth with a tight spread.
Social-Implicit is a memory efficient deep model with only 5.8K parameters that
runs in real time of about 580Hz and achieves competitive results. Interactive
demo of the problem can be seen at
https://www.abduallahmohamed.com/social-implicit-amdamv-adefde-demo . Code is
available at https://github.com/abduallahmohamed/Social-Implicit .",0.38347822,0.10226989,0.1298277,A
2897,"Although some target-less methods have been
                                       cision sensors, and further research in visual perception,             proposed, they need more strict initialization or environment
                                       automatic driving naturally becomes the foucs [1]‚Äì[3].","INTRODUCTION                                 ods are often used as rough calibration, and they can not
                                                                                                              get precise estimations due to equipment performance and
                                          With the development of computing power, high pre-                  trajectory [1].",3D              so that can not be used efÔ¨Åciently.,2022-03-07 07:36:31+00:00,CROON: Automatic Multi-LiDAR Calibration and Refinement Method in Road Scene,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pengjin Wei'), arxiv.Result.Author('Guohang Yan'), arxiv.Result.Author('Yikang Li'), arxiv.Result.Author('Kun Fang'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Xinyu Cai'), arxiv.Result.Author('Jie Yang')]","Sensor-based environmental perception is a crucial part of the autonomous
driving system. In order to get an excellent perception of the surrounding
environment, an intelligent system would configure multiple LiDARs (3D Light
Detection and Ranging) to cover the distant and near space of the car. The
precision of perception relies on the quality of sensor calibration. This
research aims at developing an accurate, automatic, and robust calibration
strategy for multiple LiDAR systems in the general road scene. We thus propose
CROON (automatiC multi-LiDAR CalibratiOn and Refinement method in rOad sceNe),
a two-stage method including rough and refinement calibration. The first stage
can calibrate the sensor from an arbitrary initial pose, and the second stage
is able to precisely calibrate the sensor iteratively. Specifically, CROON
utilize the nature characteristics of road scene so that it is independent and
easy to apply in large-scale conditions. Experimental results on real-world and
simulated data sets demonstrate the reliability and accuracy of our method. All
the related data sets and codes are open-sourced on the Github website
https://github.com/OpenCalib/LiDAR2LiDAR.",-0.09758359,0.42253488,-0.11340959,B
2898,"Motion-based meth-
                                                                                                                       ods are often used as rough calibration, and they can not
                                           With the development of computing power, high pre-                          get precise estimations due to equipment performance and
                                        cision sensors, and further research in visual perception,                     trajectory [1].",INTRODUCTION                                         can not be applied in large-scale cases.,"Although some target-less methods have been
                                        automatic driving naturally becomes the focus [1]‚Äì[3].",2022-03-07 07:36:31+00:00,CROON: Automatic Multi-LiDAR Calibration and Refinement Method in Road Scene,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pengjin Wei'), arxiv.Result.Author('Guohang Yan'), arxiv.Result.Author('Yikang Li'), arxiv.Result.Author('Kun Fang'), arxiv.Result.Author('Xinyu Cai'), arxiv.Result.Author('Jie Yang'), arxiv.Result.Author('Wei Liu')]","Sensor-based environmental perception is a crucial part of the autonomous
driving system. In order to get an excellent perception of the surrounding
environment, an intelligent system would configure multiple LiDARs (3D Light
Detection and Ranging) to cover the distant and near space of the car. The
precision of perception relies on the quality of sensor calibration. This
research aims at developing an accurate, automatic, and robust calibration
strategy for multiple LiDAR systems in the general road scene. We thus propose
CROON (automatiC multi-LiDAR CalibratiOn and Refinement method in rOad sceNe),
a two-stage method including rough and refinement calibration. The first stage
can calibrate the sensor from an arbitrary initial pose, and the second stage
is able to precisely calibrate the sensor iteratively. Specifically, CROON
utilize the nature characteristics of road scene so that it is independent and
easy to apply in large-scale conditions. Experimental results on real-world and
simulated data sets demonstrate the reliability and accuracy of our method. All
the related data sets and codes are open-sourced on the Github website
https://github.com/OpenCalib/LiDAR2LiDAR.",-0.09026642,0.39703792,-0.13684025,B
2903,"We further study the effect of using our sub-domain training vs.       Another important aspect presented in Table 3 is that using the
training on the full dataset, and using different style-transfer set-  entire DRAM training set for augmentation is not effective.",of different art concepts and styles.,"For
tings for augmentation.",2022-03-07 09:51:04+00:00,Semantic Segmentation in Art Paintings,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Nadav Cohen'), arxiv.Result.Author('Yael Newman'), arxiv.Result.Author('Ariel Shamir')]","Semantic segmentation is a difficult task even when trained in a supervised
manner on photographs. In this paper, we tackle the problem of semantic
segmentation of artistic paintings, an even more challenging task because of a
much larger diversity in colors, textures, and shapes and because there are no
ground truth annotations available for segmentation. We propose an unsupervised
method for semantic segmentation of paintings using domain adaptation. Our
approach creates a training set of pseudo-paintings in specific artistic styles
by using style-transfer on the PASCAL VOC 2012 dataset, and then applies domain
confusion between PASCAL VOC 2012 and real paintings. These two steps build on
a new dataset we gathered called DRAM (Diverse Realism in Art Movements)
composed of figurative art paintings from four movements, which are highly
diverse in pattern, color, and geometry. To segment new paintings, we present a
composite multi-domain adaptation method that trains on each sub-domain
separately and composes their solutions during inference time. Our method
provides better segmentation results not only on the specific artistic
movements of DRAM, but also on other, unseen ones. We compare our approach to
alternative methods and show applications of semantic segmentation in art
paintings. The code and models for our approach are publicly available at:
https://github.com/Nadavc220/SemanticSegmentationInArtPaintings.",0.098464146,-0.27135497,0.06073942,C
2904,"Although further research needs to be done, it seems that the higher dimensionality of the
dyads requires much more data in order to generalize in the test set.","Interestingly, the training and validation curves
showed a consistently faster convergence to the overÔ¨Åtting point for all dyad-aware methods.",3.7.,2022-03-07 09:59:30+00:00,Comparison of Spatio-Temporal Models for Human Motion and Pose Forecasting in Face-to-Face Interaction Scenarios,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('German Barquero'), arxiv.Result.Author('Johnny N√∫√±ez'), arxiv.Result.Author('Zhen Xu'), arxiv.Result.Author('Sergio Escalera'), arxiv.Result.Author('Wei-Wei Tu'), arxiv.Result.Author('Isabelle Guyon'), arxiv.Result.Author('Cristina Palmero')]","Human behavior forecasting during human-human interactions is of utmost
importance to provide robotic or virtual agents with social intelligence. This
problem is especially challenging for scenarios that are highly driven by
interpersonal dynamics. In this work, we present the first systematic
comparison of state-of-the-art approaches for behavior forecasting. To do so,
we leverage whole-body annotations (face, body, and hands) from the very
recently released UDIVA v0.5, which features face-to-face dyadic interactions.
Our best attention-based approaches achieve state-of-the-art performance in
UDIVA v0.5. We show that by autoregressively predicting the future with methods
trained for the short-term future (<400ms), we outperform the baselines even
for a considerably longer-term future (up to 2s). We also show that this
finding holds when highly noisy annotations are used, which opens new horizons
towards the use of weakly-supervised learning. Combined with large-scale
datasets, this may help boost the advances in this field.",0.29001957,-0.08115579,-0.009512782,A
2917,"The partial point cloud represents
                                       further research insights.","And this survey                   the complete point clouds are ultimately needed by recovering
                                       summarizes the comparisons among these methods to provoke                     from partial observations.","Besides, this review sums up the                   a small part of the object that is usually hard to recognize.",2022-03-07 11:47:14+00:00,Comprehensive Review of Deep Learning-Based 3D Point Clouds Completion Processing and Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ben Fei'), arxiv.Result.Author('Weidong Yang'), arxiv.Result.Author('Wenming Chen'), arxiv.Result.Author('Zhijun Li'), arxiv.Result.Author('Yikang Li'), arxiv.Result.Author('Tao Ma'), arxiv.Result.Author('Xing Hu'), arxiv.Result.Author('Lipeng Ma')]","Point cloud completion is a generation and estimation issue derived from the
partial point clouds, which plays a vital role in the applications in 3D
computer vision. The progress of deep learning (DL) has impressively improved
the capability and robustness of point cloud completion. However, the quality
of completed point clouds is still needed to be further enhanced to meet the
practical utilization. Therefore, this work aims to conduct a comprehensive
survey on various methods, including point-based, convolution-based,
graph-based, and generative model-based approaches, etc. And this survey
summarizes the comparisons among these methods to provoke further research
insights. Besides, this review sums up the commonly used datasets and
illustrates the applications of point cloud completion. Eventually, we also
discussed possible research trends in this promptly expanding field.",-0.061880074,0.2989699,-0.006113086,B
2918,"The partial point cloud represents
                                       further research insights.","And this survey                   the complete point clouds are ultimately needed by recovering
                                       summarizes the comparisons among these methods to provoke                     from partial observations.","Besides, this review sums up the                   a small part of the object that is usually hard to recognize.",2022-03-07 11:47:14+00:00,Comprehensive Review of Deep Learning-Based 3D Point Cloud Completion Processing and Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ben Fei'), arxiv.Result.Author('Weidong Yang'), arxiv.Result.Author('Wenming Chen'), arxiv.Result.Author('Zhijun Li'), arxiv.Result.Author('Yikang Li'), arxiv.Result.Author('Tao Ma'), arxiv.Result.Author('Xing Hu'), arxiv.Result.Author('Lipeng Ma')]","Point cloud completion is a generation and estimation issue derived from the
partial point clouds, which plays a vital role in the applications in 3D
computer vision. The progress of deep learning (DL) has impressively improved
the capability and robustness of point cloud completion. However, the quality
of completed point clouds is still needed to be further enhanced to meet the
practical utilization. Therefore, this work aims to conduct a comprehensive
survey on various methods, including point-based, convolution-based,
graph-based, and generative model-based approaches, etc. And this survey
summarizes the comparisons among these methods to provoke further research
insights. Besides, this review sums up the commonly used datasets and
illustrates the applications of point cloud completion. Eventually, we also
discussed possible research trends in this promptly expanding field.",-0.061880074,0.2989699,-0.006113086,B
2937,"but the performance for some classes are poor, which will
                                                                     hopefully inspire further research into more powerful and
4.9.","[58]
struggle to represent inherent specularities in shapes.",Single View Reconstruction                                      2D-to-3D reconstruction methods.,2022-03-07 18:13:59+00:00,Kubric: A scalable dataset generator,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Klaus Greff'), arxiv.Result.Author('Francois Belletti'), arxiv.Result.Author('Lucas Beyer'), arxiv.Result.Author('Carl Doersch'), arxiv.Result.Author('Yilun Du'), arxiv.Result.Author('Daniel Duckworth'), arxiv.Result.Author('David J. Fleet'), arxiv.Result.Author('Dan Gnanapragasam'), arxiv.Result.Author('Florian Golemo'), arxiv.Result.Author('Charles Herrmann'), arxiv.Result.Author('Thomas Kipf'), arxiv.Result.Author('Abhijit Kundu'), arxiv.Result.Author('Dmitry Lagun'), arxiv.Result.Author('Issam Laradji'), arxiv.Result.Author('Hsueh-Ti'), arxiv.Result.Author('Liu'), arxiv.Result.Author('Henning Meyer'), arxiv.Result.Author('Yishu Miao'), arxiv.Result.Author('Derek Nowrouzezahrai'), arxiv.Result.Author('Cengiz Oztireli'), arxiv.Result.Author('Etienne Pot'), arxiv.Result.Author('Noha Radwan'), arxiv.Result.Author('Daniel Rebain'), arxiv.Result.Author('Sara Sabour'), arxiv.Result.Author('Mehdi S. M. Sajjadi'), arxiv.Result.Author('Matan Sela'), arxiv.Result.Author('Vincent Sitzmann'), arxiv.Result.Author('Austin Stone'), arxiv.Result.Author('Deqing Sun'), arxiv.Result.Author('Suhani Vora'), arxiv.Result.Author('Ziyu Wang'), arxiv.Result.Author('Tianhao Wu'), arxiv.Result.Author('Kwang Moo Yi'), arxiv.Result.Author('Fangcheng Zhong'), arxiv.Result.Author('Andrea Tagliasacchi')]","Data is the driving force of machine learning, with the amount and quality of
training data often being more important for the performance of a system than
architecture and training details. But collecting, processing and annotating
real data at scale is difficult, expensive, and frequently raises additional
privacy, fairness and legal concerns. Synthetic data is a powerful tool with
the potential to address these shortcomings: 1) it is cheap 2) supports rich
ground-truth annotations 3) offers full control over data and 4) can circumvent
or mitigate problems regarding bias, privacy and licensing. Unfortunately,
software tools for effective data generation are less mature than those for
architecture design and training, which leads to fragmented generation efforts.
To address these problems we introduce Kubric, an open-source Python framework
that interfaces with PyBullet and Blender to generate photo-realistic scenes,
with rich annotations, and seamlessly scales to large jobs distributed over
thousands of machines, and generating TBs of data. We demonstrate the
effectiveness of Kubric by presenting a series of 13 different generated
datasets for tasks ranging from studying 3D NeRF models to optical flow
estimation. We release Kubric, the used assets, all of the generation code, as
well as the rendered datasets for reuse and modification.",-0.06365926,0.27476716,0.06778348,B
2938,"Code and trained models will be released upon publication,
                                                                  with the hope that it can spark further research in the design
Equation (2) is equivalent to equation (1) if we substitute       of binary descriptors and quantized networks for the task of
the sim(u) = 12 ||u ‚àí x||22 and constr(u) = ||u||2 = 1.           keypoint detection and description.",subject to constr(u).,"This enables the deÔ¨Ånition of optimization objectives that
can be utilized where L2 normalization does not provide           A.",2022-03-07 18:59:03+00:00,"ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization",cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Simon Maurer'), arxiv.Result.Author('Menelaos Kanakis'), arxiv.Result.Author('Matteo Spallanzani'), arxiv.Result.Author('Ajad Chhatkuli'), arxiv.Result.Author('Luc Van Gool')]","The design of more complex and powerful neural network models has
significantly advanced the state-of-the-art in local feature detection and
description. These advances can be attributed to deeper networks, improved
training methodologies through self-supervision, or the introduction of new
building blocks, such as graph neural networks for feature matching. However,
in the pursuit of increased performance, efficient architectures that generate
lightweight descriptors have received surprisingly little attention. In this
paper, we investigate the adaptations neural networks for detection and
description require in order to enable their use in embedded platforms. To that
end, we investigate and adapt network quantization techniques for use in
real-time applications. In addition, we revisit common practices in descriptor
quantization and propose the use of a binary descriptor normalization layer,
enabling the generation of distinctive length-invariant binary descriptors.
ZippyPoint, our efficient network, runs at 47.2 fps on the Apple M1 CPU. This
is up to 5x faster than other learned detection and description models, making
it the only real-time learned network. ZippyPoint consistently outperforms all
other binary detection and descriptor methods in visual localization and
homography estimation tasks. Code and trained models will be released upon
publication.",0.10392664,-0.07104076,0.12515566,A
2939,"Sun, ‚ÄúConvolutional neural networks at constrained time
                                                                                     cost,‚Äù in CVPR, 2015.
further research in this direction could in the future result in
                                                                               [32] A. Howard et al., ‚ÄúSearching for mobilenetv3,‚Äù in ICCV, 2019.
a truly powerful binary descriptor method, useful in several                   [33] X. Zhang, J. Zou, K. He, and J.",We believe that                  [31] K. He and J.,"Sun, ‚ÄúAccelerating very deep con-

real-time robotic applications.",2022-03-07 18:59:03+00:00,"ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization",cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Simon Maurer'), arxiv.Result.Author('Menelaos Kanakis'), arxiv.Result.Author('Matteo Spallanzani'), arxiv.Result.Author('Ajad Chhatkuli'), arxiv.Result.Author('Luc Van Gool')]","The design of more complex and powerful neural network models has
significantly advanced the state-of-the-art in local feature detection and
description. These advances can be attributed to deeper networks, improved
training methodologies through self-supervision, or the introduction of new
building blocks, such as graph neural networks for feature matching. However,
in the pursuit of increased performance, efficient architectures that generate
lightweight descriptors have received surprisingly little attention. In this
paper, we investigate the adaptations neural networks for detection and
description require in order to enable their use in embedded platforms. To that
end, we investigate and adapt network quantization techniques for use in
real-time applications. In addition, we revisit common practices in descriptor
quantization and propose the use of a binary descriptor normalization layer,
enabling the generation of distinctive length-invariant binary descriptors.
ZippyPoint, our efficient network, runs at 47.2 fps on the Apple M1 CPU. This
is up to 5x faster than other learned detection and description models, making
it the only real-time learned network. ZippyPoint consistently outperforms all
other binary detection and descriptor methods in visual localization and
homography estimation tasks. Code and trained models will be released upon
publication.",-0.25104237,-0.04911132,0.2031874,B
2940,"These include the Ô¨Årst encoder convolution, the re-
can both spark further research in the design of binary de-     maining encoder convolutions, spatial reduction layers, the
scriptors and quantized networks, as well as promote the        non-head decoder convolutions, and the head decoder con-
incorporation of ZippyPoint in robotic systems.",We envision our work          blocks.,"volutions, as depicted in Fig.",2022-03-07 18:59:03+00:00,"ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization",cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Menelaos Kanakis'), arxiv.Result.Author('Simon Maurer'), arxiv.Result.Author('Matteo Spallanzani'), arxiv.Result.Author('Ajad Chhatkuli'), arxiv.Result.Author('Luc Van Gool')]","Efficient detection and description of geometric regions in images is a
prerequisite in visual systems for localization and mapping. Such systems still
rely on traditional hand-crafted methods for efficient generation of
lightweight descriptors, a common limitation of the more powerful neural
network models that come with high compute and specific hardware requirements.
In this paper, we focus on the adaptations required by detection and
description neural networks to enable their use in computationally limited
platforms such as robots, mobile, and augmented reality devices. To that end,
we investigate and adapt network quantization techniques to accelerate
inference and enable its use on compute limited platforms. In addition, we
revisit common practices in descriptor quantization and propose the use of a
binary descriptor normalization layer, enabling the generation of distinctive
binary descriptors with a constant number of ones. ZippyPoint, our efficient
quantized network with binary descriptors, improves the network runtime speed,
the descriptor matching speed, and the 3D model size, by at least an order of
magnitude when compared to full-precision counterparts. These improvements come
at a minor performance degradation as evaluated on the tasks of homography
estimation, visual localization, and map-free visual relocalization. Code and
trained models will be released upon acceptance.",-0.090086475,0.12561558,0.11789172,B
2941,"lieve ZippyPoint can spark further research towards bring-
                                                                                                          ing learned binary descriptor methods to mobile platforms,
                                                                                                          as well as promote its incorporation in both new and estab-
                                                                                                          lished robotic pipelines.","We be-
their global descriptors, representing candidate locations.","Supplementary Material                                      We evaluate interest point detection and description net-
                                                                  works on the challenging Map-free Visual Relocalization
A.",2022-03-07 18:59:03+00:00,"ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization",cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Menelaos Kanakis'), arxiv.Result.Author('Simon Maurer'), arxiv.Result.Author('Matteo Spallanzani'), arxiv.Result.Author('Ajad Chhatkuli'), arxiv.Result.Author('Luc Van Gool')]","Efficient detection and description of geometric regions in images is a
prerequisite in visual systems for localization and mapping. Such systems still
rely on traditional hand-crafted methods for efficient generation of
lightweight descriptors, a common limitation of the more powerful neural
network models that come with high compute and specific hardware requirements.
In this paper, we focus on the adaptations required by detection and
description neural networks to enable their use in computationally limited
platforms such as robots, mobile, and augmented reality devices. To that end,
we investigate and adapt network quantization techniques to accelerate
inference and enable its use on compute limited platforms. In addition, we
revisit common practices in descriptor quantization and propose the use of a
binary descriptor normalization layer, enabling the generation of distinctive
binary descriptors with a constant number of ones. ZippyPoint, our efficient
quantized network with binary descriptors, improves the network runtime speed,
the descriptor matching speed, and the 3D model size, by at least an order of
magnitude when compared to full-precision counterparts. These improvements come
at a minor performance degradation as evaluated on the tasks of homography
estimation, visual localization, and map-free visual relocalization. Code and
trained models will be released upon acceptance.",-0.34390837,0.085528776,-0.11130914,B
2968,"The dataset and benchmark code implementations will be made public
with the intention to support further research progress in transparent RGB-D
visual perception.","Results from our experiments demonstrate that
there is still much room for improvement in some cases, such as heavy clutter,
transparent objects filled with liquid, or being covered by other translucent sur-
faces.","References

 1.",2022-03-08 07:29:31+00:00,ClearPose: Large-scale Transparent Object Dataset and Benchmark,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Xiaotong Chen'), arxiv.Result.Author('Huijie Zhang'), arxiv.Result.Author('Zeren Yu'), arxiv.Result.Author('Anthony Opipari'), arxiv.Result.Author('Odest Chadwicke Jenkins')]","Transparent objects are ubiquitous in household settings and pose distinct
challenges for visual sensing and perception systems. The optical properties of
transparent objects leave conventional 3D sensors alone unreliable for object
depth and pose estimation. These challenges are highlighted by the shortage of
large-scale RGB-Depth datasets focusing on transparent objects in real-world
settings. In this work, we contribute a large-scale real-world RGB-Depth
transparent object dataset named ClearPose to serve as a benchmark dataset for
segmentation, scene-level depth completion and object-centric pose estimation
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth
frames and 4M instance annotations covering 63 household objects. The dataset
includes object categories commonly used in daily life under various lighting
and occluding conditions as well as challenging test scenarios such as cases of
occlusion by opaque or translucent objects, non-planar orientations, presence
of liquids, etc. We benchmark several state-of-the-art depth completion and
object pose estimation deep neural networks on ClearPose.",-0.01895386,0.23145026,0.077232674,B
2969,"The dataset and benchmark code implementations will be made public
with the intention to support further research progress in transparent RGB-D
visual perception.","Results from our experiments demonstrate that
there is still much room for improvement in some cases, such as heavy clutter,
transparent objects Ô¨Ålled with liquid, or being covered by other translucent sur-
faces.",Acknowledgement.,2022-03-08 07:29:31+00:00,ClearPose: Large-scale Transparent Object Dataset and Benchmark,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Xiaotong Chen'), arxiv.Result.Author('Huijie Zhang'), arxiv.Result.Author('Zeren Yu'), arxiv.Result.Author('Anthony Opipari'), arxiv.Result.Author('Odest Chadwicke Jenkins')]","Transparent objects are ubiquitous in household settings and pose distinct
challenges for visual sensing and perception systems. The optical properties of
transparent objects leave conventional 3D sensors alone unreliable for object
depth and pose estimation. These challenges are highlighted by the shortage of
large-scale RGB-Depth datasets focusing on transparent objects in real-world
settings. In this work, we contribute a large-scale real-world RGB-Depth
transparent object dataset named ClearPose to serve as a benchmark dataset for
segmentation, scene-level depth completion and object-centric pose estimation
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth
frames and 5M instance annotations covering 63 household objects. The dataset
includes object categories commonly used in daily life under various lighting
and occluding conditions as well as challenging test scenarios such as cases of
occlusion by opaque or translucent objects, non-planar orientations, presence
of liquids, etc. We benchmark several state-of-the-art depth completion and
object pose estimation deep neural networks on ClearPose. The dataset and
benchmarking source code is available at https://github.com/opipari/ClearPose.",-0.023031563,0.22960256,0.07611553,B
2986,"This will allow further research and development in AI
assistants to benefit from our AQTC task and AssistQ dataset.","We intend to extend the AssistQ dataset to include more examples
with third-person videos, as it is more common for product videos/demos to be
recorded in third-person.","D Hyper-parameters in Q2A

 (a) Searching feature dimension  (b) Searching number of attention heads

Dimension R@1 ‚Üë R@3 ‚Üë MR ‚Üì MRR ‚Üë  Heads‚Äô Number R@1 ‚Üë R@3 ‚Üë MR ‚Üì MRR ‚Üë
 1 √ó 768 30.2 62.3 3.2 3.2
 2 √ó 768 20.3 56.5 4.3 3.6        1  30.2 62.3 3.2 3.2
 3 √ó 768 24.4 56.5 4.2 3.7
 4 √ó 768 21.5 56.1 4.3 3.6        2  25.2 56.1 4.0 4.0

                                  3  25.2 55.3 4.1 3.8

                                  4  24.4 59.4 4.0 3.9

Table 4.",2022-03-08 17:07:09+00:00,AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant,cs.CV,['cs.CV'],"[arxiv.Result.Author('Benita Wong'), arxiv.Result.Author('Joya Chen'), arxiv.Result.Author('You Wu'), arxiv.Result.Author('Stan Weixian Lei'), arxiv.Result.Author('Dongxing Mao'), arxiv.Result.Author('Difei Gao'), arxiv.Result.Author('Mike Zheng Shou')]","A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as ""how
can I run the microwave for 1 minute?"". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos and scripts to guide the user
step-by-step. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples derived from 100 newly filmed
first-person videos. Each question should be completed with multi-step
guidances by inferring from visual details (e.g., buttons' position) and
textural details (e.g., actions like press/turn). To address this unique task,
we developed a Question-to-Actions (Q2A) model that significantly outperforms
several baseline methods while still having large room for improvement. We
expect our task and dataset to advance Egocentric AI Assistant's development.
Our project page is available at: https://showlab.github.io/assistq",0.016357196,-0.091060534,-0.22003196,C
2987,"This will allow further research and development in AI
assistants to benefit from our AQTC task and AssistQ dataset.","We intend to extend the AssistQ dataset to include more examples
with third-person videos, as it is more common for product videos/demos to be
recorded in third-person.","D Hyper-parameters in Q2A

 (a) Searching feature dimension  (b) Searching number of attention heads

Dimension R@1 ‚Üë R@3 ‚Üë MR ‚Üì MRR ‚Üë  Heads‚Äô Number R@1 ‚Üë R@3 ‚Üë MR ‚Üì MRR ‚Üë
 1 √ó 768 30.2 62.3 3.2 3.2
 2 √ó 768 20.3 56.5 4.3 3.6        1  30.2 62.3 3.2 3.2
 3 √ó 768 24.4 56.5 4.2 3.7
 4 √ó 768 21.5 56.1 4.3 3.6        2  25.2 56.1 4.0 4.0

                                  3  25.2 55.3 4.1 3.8

                                  4  24.4 59.4 4.0 3.9

Table 4.",2022-03-08 17:07:09+00:00,AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant,cs.CV,['cs.CV'],"[arxiv.Result.Author('Benita Wong'), arxiv.Result.Author('Joya Chen'), arxiv.Result.Author('You Wu'), arxiv.Result.Author('Stan Weixian Lei'), arxiv.Result.Author('Dongxing Mao'), arxiv.Result.Author('Difei Gao'), arxiv.Result.Author('Mike Zheng Shou')]","A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as ""how
can I run the microwave for 1 minute?"". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos and scripts to guide the user
step-by-step. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples derived from 100 newly filmed
first-person videos. Each question should be completed with multi-step
guidances by inferring from visual details (e.g., buttons' position) and
textural details (e.g., actions like press/turn). To address this unique task,
we developed a Question-to-Actions (Q2A) model that significantly outperforms
several baseline methods while still having large room for improvement. We
expect our task and dataset to advance Egocentric AI Assistant's development.
Our project page is available at: https://showlab.github.io/assistq",0.016357196,-0.091060534,-0.22003196,C
2988,"This will allow further research and development in AI
assistants to benefit from our AQTC task and AssistQ dataset.","We intend to extend the AssistQ dataset to include more examples
with third-person videos, as it is more common for product videos/demos to be
recorded in third-person.","20  Affordance-centric Question-driven Task Completion

Fig.",2022-03-08 17:07:09+00:00,AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant,cs.CV,['cs.CV'],"[arxiv.Result.Author('Benita Wong'), arxiv.Result.Author('Joya Chen'), arxiv.Result.Author('You Wu'), arxiv.Result.Author('Stan Weixian Lei'), arxiv.Result.Author('Dongxing Mao'), arxiv.Result.Author('Difei Gao'), arxiv.Result.Author('Mike Zheng Shou')]","A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as ""how
can I run the microwave for 1 minute?"". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos to provide step-by-step help in the
user's view. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples from 100 newly filmed instructional
videos. We also developed a novel Question-to-Actions (Q2A) model to address
the AQTC task and validate it on the AssistQ dataset. The results show that our
model significantly outperforms several VQA-related baselines while still
having large room for improvement. We expect our task and dataset to advance
Egocentric AI Assistant's development. Our project page is available at:
https://showlab.github.io/assistq/.",-0.12504794,-0.18829443,-0.30240375,C
3017,"A.4), we will further study the selection and generation of
adversarial patches, e.g., fusing shallow texture information of targeted distribution to
guide resultant adversarial examples towards the targeted category.","Since existing algorithms are not effective yet and simply replacing our
adversarial patch with a clean targeted image cannot obtain an effective targeted attack
(as demonstrated in Sec.","Density:1  Density:2  Density:3  Density:4   Density:5   Density:6

Density:7  Density:8  Density:9  Density:10  Density:11  Density:12

Fig.",2022-03-09 09:51:00+00:00,Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qilong Zhang'), arxiv.Result.Author('Chaoning Zhang'), arxiv.Result.Author('Chaoqun Li'), arxiv.Result.Author('Jingkuan Song'), arxiv.Result.Author('Lianli Gao'), arxiv.Result.Author('Heng Tao Shen')]","In recent years, the adversarial vulnerability of deep neural networks (DNNs)
has raised increasing attention. Among all the threat models, no-box attacks
are the most practical but extremely challenging since they neither rely on any
knowledge of the target model or similar substitute model, nor access the
dataset for training a new substitute model. Although a recent method has
attempted such an attack in a loose sense, its performance is not good enough
and computational overhead of training is expensive. In this paper, we move a
step forward and show the existence of a \textbf{training-free} adversarial
perturbation under the no-box threat model, which can be successfully used to
attack different DNNs in real-time. Motivated by our observation that
high-frequency component (HFC) domains in low-level features and plays a
crucial role in classification, we attack an image mainly by manipulating its
frequency components. Specifically, the perturbation is manipulated by
suppression of the original HFC and adding of noisy HFC. We empirically and
experimentally analyze the requirements of effective noisy HFC and show that it
should be regionally homogeneous, repeating and dense. Extensive experiments on
the ImageNet dataset demonstrate the effectiveness of our proposed no-box
method. It attacks ten well-known models with a success rate of
\textbf{98.13\%} on average, which outperforms state-of-the-art no-box attacks
by \textbf{29.39\%}. Furthermore, our method is even competitive to mainstream
transfer-based black-box attacks.",-0.069951326,-0.104960166,0.21459821,C
3018,"D), we               ble adversarial training models (EAT) [58]: Inc-v3ens3,
will further study the selection and generation of adversarial             Inc-v3ens4 and IncRes-v2ens, and three feature denois-
                                                                           ing models (FD) [63]: ResNet152 Baseline (Res152B),
                                                                           ResNet152 Denoise (Res152D), ResNeXt101 DenoiseAll
                                                                           (ResNeXtDA), to choose the best tile-scheme for fooling
                                                                           defense models.","Since existing algo-
rithms are not effective yet and simply replacing our ad-                     In this section, we further consider six additional well-
versarial patch with a clean targeted image cannot obtain                  known defense models, which including three ensem-
an effective targeted attack (as demonstrated in Sec.","Generally, a smaller tile-scheme can generate a more
                                                                           perceptible perturbation.",2022-03-09 09:51:00+00:00,Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qilong Zhang'), arxiv.Result.Author('Chaoning Zhang'), arxiv.Result.Author('Chaoqun Li'), arxiv.Result.Author('Jingkuan Song'), arxiv.Result.Author('Lianli Gao')]","In recent years, the adversarial vulnerability of deep neural networks (DNNs)
has raised increasing attention. Among all the threat models, no-box attacks
are the most practical but extremely challenging since they neither rely on any
knowledge of the target model or similar substitute model, nor access the
dataset for training a new substitute model. Although a recent method has
attempted such an attack in a loose sense, its performance is not good enough
and computational overhead of training is expensive. In this paper, we move a
step forward and show the existence of a \textbf{training-free} adversarial
perturbation under the no-box threat model, which can be successfully used to
attack different DNNs in real-time. Motivated by our observation that
high-frequency component (HFC) domains in low-level features and plays a
crucial role in classification, we attack an image mainly by manipulating its
frequency components. Specifically, the perturbation is manipulated by
suppression of the original HFC and adding of noisy HFC. We empirically and
experimentally analyze the requirements of effective noisy HFC and show that it
should be regionally homogeneous, repeating and dense. Extensive experiments on
the ImageNet dataset demonstrate the effectiveness of our proposed no-box
method. It attacks ten well-known models with a success rate of
\textbf{98.13\%} on average, which outperforms state-of-the-art no-box attacks
by \textbf{29.39\%}. Furthermore, our method is even competitive to mainstream
transfer-based black-box attacks.",-0.003362812,-0.15543239,0.27552158,C
3024,"To further study the parallel performance of MGRIT training of GRU, we now consider the full
HMDB51 dataset.","In addition, both
serial and parallel inference of the Implicit GRU yield similar results, supporting Remark 3.",Fig.,2022-03-07 11:32:44+00:00,Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences,cs.CV,"['cs.CV', 'cs.DC', 'cs.LG']","[arxiv.Result.Author('Gordon Euhyun Moon'), arxiv.Result.Author('Eric C. Cyr')]","Parallelizing Gated Recurrent Unit (GRU) networks is a challenging task, as
the training procedure of GRU is inherently sequential. Prior efforts to
parallelize GRU have largely focused on conventional parallelization strategies
such as data-parallel and model-parallel training algorithms. However, when the
given sequences are very long, existing approaches are still inevitably
performance limited in terms of training time. In this paper, we present a
novel parallel training scheme (called parallel-in-time) for GRU based on a
multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into
multiple shorter sub-sequences and trains the sub-sequences on different
processors in parallel. The key to achieving speedup is a hierarchical
correction of the hidden state to accelerate end-to-end communication in both
the forward and backward propagation phases of gradient descent. Experimental
results on the HMDB51 dataset, where each video is an image sequence,
demonstrate that the new parallel training scheme achieves up to 6.5$\times$
speedup over a serial approach. As efficiency of our new parallelization
strategy is associated with the sequence length, our parallel GRU algorithm
achieves significant performance improvement as the sequence length increases.",0.23061295,-0.11201994,0.060615886,A
3033,"In our experiments, we further study with monochromatic and
RGB-Depth.","Now we elaborate the multi-modal data representations we
use for inputting to our symmetric dual-path CMX architecture.","Depth images naturally offer range, position, and          trichromatic polarization cues, coupled with RGB images in
contour information.",2022-03-09 16:12:08+00:00,CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Huayao Liu'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Xinxin Hu'), arxiv.Result.Author('Rainer Stiefelhagen')]","The performance of semantic segmentation of RGB images can be advanced by
exploiting informative features from supplementary modalities. In this work, we
propose CMX, a vision-transformer-based cross-modal fusion framework for RGB-X
semantic segmentation. To generalize to different sensing modalities
encompassing various uncertainties, we consider that comprehensive cross-modal
interactions should be provided. CMX is built with two streams to extract
features from RGB images and the complementary modality (X-modality). In each
feature extraction stage, we design a Cross-Modal Feature Rectification Module
(CM-FRM) to calibrate the feature of the current modality by combining the
feature from the other modality, in spatial- and channel-wise dimensions. With
rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix them
for the final semantic prediction. FFM is constructed with a cross-attention
mechanism, which enables exchange of long-range contexts, enhancing both
modalities' features at a global level. Extensive experiments show that CMX
generalizes to diverse multi-modal combinations, achieving state-of-the-art
performances on four RGB-Depth benchmarks, as well as RGB-Thermal and
RGB-Polarization datasets. Besides, to investigate the generalizability to
dense-sparse data fusion, we establish a RGB-Event semantic segmentation
benchmark based on the EventScape dataset, on which CMX sets the new
state-of-the-art. Code is available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation",-0.14946568,0.18156108,0.12340233,B
3034,"In Table 9, we further study with
5.4 Results on RGB-Event Dataset                                     different settings of event time bin B={1, 3, 5, 10, 15, 20, 30}
                                                                     with our CMX fusion model based on SegFormer B2.",Event data representations.,"Compared
We take a step further to assess the generalizability of our
proposed approach for dense-sparse data fusion.",2022-03-09 16:12:08+00:00,CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Huayao Liu'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Xinxin Hu'), arxiv.Result.Author('Rainer Stiefelhagen')]","The performance of semantic segmentation of RGB images can be advanced by
exploiting informative features from supplementary modalities. In this work, we
propose CMX, a vision-transformer-based cross-modal fusion framework for RGB-X
semantic segmentation. To generalize to different sensing modalities
encompassing various uncertainties, we consider that comprehensive cross-modal
interactions should be provided. CMX is built with two streams to extract
features from RGB images and the complementary modality (X-modality). In each
feature extraction stage, we design a Cross-Modal Feature Rectification Module
(CM-FRM) to calibrate the feature of the current modality by combining the
feature from the other modality, in spatial- and channel-wise dimensions. With
rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix them
for the final semantic prediction. FFM is constructed with a cross-attention
mechanism, which enables exchange of long-range contexts, enhancing both
modalities' features at a global level. Extensive experiments show that CMX
generalizes to diverse multi-modal combinations, achieving state-of-the-art
performances on four RGB-Depth benchmarks, as well as RGB-Thermal and
RGB-Polarization datasets. Besides, to investigate the generalizability to
dense-sparse data fusion, we establish a RGB-Event semantic segmentation
benchmark based on the EventScape dataset, on which CMX sets the new
state-of-the-art. Code is available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation",0.06801991,0.14521366,0.007132492,A
3035,"(11)
3.4 Multi-modal Data Representations                                 2                    S2

To investigate and reach robust RGB-X semantic segmenta-             In our experiments, we further study with monochromatic and
tion, the multi-modal data representations are critical.",AoLP = 1 arctan S1 .,"We study    trichromatic polarization cues, coupled with RGB images in
                                                                     multi-modal RGB-P semantic segmentation.",2022-03-09 16:12:08+00:00,CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Huayao Liu'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Xinxin Hu'), arxiv.Result.Author('Rainer Stiefelhagen')]","Pixel-wise semantic segmentation of RGB images can be advanced by exploiting
informative features from supplementary modalities. In this work, we propose
CMX, a vision-transformer-based cross-modal fusion framework for RGB-X semantic
segmentation. To generalize to different sensing modalities encompassing
various supplements and uncertainties, we consider that comprehensive
cross-modal interactions should be provided. CMX is built with two streams to
extract features from RGB images and the complementary modality (X-modality).
In each feature extraction stage, we design a Cross-Modal Feature Rectification
Module (CM-FRM) to calibrate the feature of the current modality by combining
the feature from the other modality, in spatial- and channel-wise dimensions.
With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix
them for the final semantic prediction. FFM is constructed with a
cross-attention mechanism, which enables exchange of long-range contexts,
enhancing both modalities' features at a global level. Extensive experiments
show that CMX generalizes to diverse multi-modal combinations, achieving
state-of-the-art performances on five RGB-Depth benchmarks, as well as
RGB-Thermal and RGB-Polarization datasets. Besides, to investigate the
generalizability to dense-sparse data fusion, we establish an RGB-Event
semantic segmentation benchmark based on the EventScape dataset, on which CMX
sets the new state-of-the-art. Code is available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation.",-0.16136439,0.024502598,-0.015858956,B
3036,"In Table 10, we further study with                                      and X-modality branches.","and FFM to rectify and merge features coming from the RGB-
Event data representations.","We take out these two modules from
different settings of event time bin B={1, 3, 5, 10, 15, 20, 30}                                    the architecture respectively, where the results are shown in
with our CMX fusion model based on SegFormer B2.",2022-03-09 16:12:08+00:00,CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Huayao Liu'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Xinxin Hu'), arxiv.Result.Author('Rainer Stiefelhagen')]","Pixel-wise semantic segmentation of RGB images can be advanced by exploiting
informative features from supplementary modalities. In this work, we propose
CMX, a vision-transformer-based cross-modal fusion framework for RGB-X semantic
segmentation. To generalize to different sensing modalities encompassing
various supplements and uncertainties, we consider that comprehensive
cross-modal interactions should be provided. CMX is built with two streams to
extract features from RGB images and the complementary modality (X-modality).
In each feature extraction stage, we design a Cross-Modal Feature Rectification
Module (CM-FRM) to calibrate the feature of the current modality by combining
the feature from the other modality, in spatial- and channel-wise dimensions.
With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix
them for the final semantic prediction. FFM is constructed with a
cross-attention mechanism, which enables exchange of long-range contexts,
enhancing both modalities' features at a global level. Extensive experiments
show that CMX generalizes to diverse multi-modal combinations, achieving
state-of-the-art performances on five RGB-Depth benchmarks, as well as
RGB-Thermal and RGB-Polarization datasets. Besides, to investigate the
generalizability to dense-sparse data fusion, we establish an RGB-Event
semantic segmentation benchmark based on the EventScape dataset, on which CMX
sets the new state-of-the-art. Code is available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation.",0.109384656,0.13891841,0.0050285836,A
3040,"While none of these
explanations can oÔ¨Äer an explanation for the observed tradeoÔ¨Ä between ImageNet accuracy and perceptual
similarity, we hope our paper opens the door for further research in this area.","We then probe a number of explanations for the
inverse-U relationship involving skip connections, Global Similarity Functions, Distortion Sensitivity, Layer-
wise Perceptual Scores, Spatial Frequency, Sensitivity, and ImageNet Class Granularity.","Broader Impact Statement Our results are based on BAPPS, which consists of exclusively low-level
distortions as opposed to high-level semantic diÔ¨Äerences.",2022-03-09 18:45:41+00:00,Do better ImageNet classifiers assess perceptual similarity better?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Manoj Kumar'), arxiv.Result.Author('Neil Houlsby'), arxiv.Result.Author('Nal Kalchbrenner'), arxiv.Result.Author('Ekin D. Cubuk')]","Perceptual distances between images, as measured in the space of pre-trained
deep features, have outperformed prior low-level, pixel-based metrics on
assessing image similarity. While the capabilities of older and less accurate
models such as AlexNet and VGG to capture perceptual similarity are well known,
modern and more accurate models are less studied. In this paper, we present a
large-scale empirical study to assess how well ImageNet classifiers perform on
perceptual similarity. First, we observe a inverse correlation between ImageNet
accuracy and Perceptual Scores of modern networks such as ResNets,
EfficientNets, and Vision Transformers: that is better classifiers achieve
worse Perceptual Scores. Then, we examine the ImageNet accuracy/Perceptual
Score relationship on varying the depth, width, number of training steps,
weight decay, label smoothing, and dropout. Higher accuracy improves Perceptual
Score up to a certain point, but we uncover a Pareto frontier between
accuracies and Perceptual Score in the mid-to-high accuracy regime. We explore
this relationship further using a number of plausible hypotheses such as
distortion invariance, spatial frequency sensitivity, and alternative
perceptual functions. Interestingly we discover shallow ResNets and ResNets
trained for less than 5 epochs only on ImageNet, whose emergent Perceptual
Score matches the prior best networks trained directly on supervised human
perceptual judgements.",-0.09428157,-0.1268826,0.19476998,C
3041,"While none of these
explanations can oÔ¨Äer an explanation for the observed tradeoÔ¨Ä between ImageNet accuracy and perceptual
similarity, we hope our paper opens the door for further research in this area.","We then probe a number of explanations for the
inverse-U relationship involving skip connections, Global Similarity Functions, Distortion Sensitivity, Layer-
wise Perceptual Scores, Spatial Frequency, Sensitivity, and ImageNet Class Granularity.","Broader Impact Statement Our results are based on BAPPS, which consists of exclusively low-level
distortions as opposed to high-level semantic diÔ¨Äerences.",2022-03-09 18:45:41+00:00,Do better ImageNet classifiers assess perceptual similarity better?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Manoj Kumar'), arxiv.Result.Author('Neil Houlsby'), arxiv.Result.Author('Nal Kalchbrenner'), arxiv.Result.Author('Ekin D. Cubuk')]","Perceptual distances between images, as measured in the space of pre-trained
deep features, have outperformed prior low-level, pixel-based metrics on
assessing perceptual similarity. While the capabilities of older and less
accurate models such as AlexNet and VGG to capture perceptual similarity are
well known, modern and more accurate models are less studied. In this paper, we
present a large-scale empirical study to assess how well ImageNet classifiers
perform on perceptual similarity. First, we observe a inverse correlation
between ImageNet accuracy and Perceptual Scores of modern networks such as
ResNets, EfficientNets, and Vision Transformers: that is better classifiers
achieve worse Perceptual Scores. Then, we examine the ImageNet
accuracy/Perceptual Score relationship on varying the depth, width, number of
training steps, weight decay, label smoothing, and dropout. Higher accuracy
improves Perceptual Score up to a certain point, but we uncover a Pareto
frontier between accuracies and Perceptual Score in the mid-to-high accuracy
regime. We explore this relationship further using a number of plausible
hypotheses such as distortion invariance, spatial frequency sensitivity, and
alternative perceptual functions. Interestingly we discover shallow ResNets and
ResNets trained for less than 5 epochs only on ImageNet, whose emergent
Perceptual Score matches the prior best networks trained directly on supervised
human perceptual judgements. The checkpoints for the models in our study are
available at
https://console.cloud.google.com/storage/browser/gresearch/perceptual_similarity.",-0.09428157,-0.1268826,0.19476998,C
3052,"Our work is the Ô¨Årst that studies this problem
ried out to further study the effectiveness of adaptive learn-  and Ô¨Ålls the gap in benchmarks and techniques for practical
ing module in our T-GNN model.",Experiments are car-        leviation.,We remove the attention-         pedestrian trajectory prediction across different domains.,2022-03-09 21:08:47+00:00,Adaptive Trajectory Prediction via Transferable GNN,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yi Xu'), arxiv.Result.Author('Lichen Wang'), arxiv.Result.Author('Yizhou Wang'), arxiv.Result.Author('Yun Fu')]","Pedestrian trajectory prediction is an essential component in a wide range of
AI applications such as autonomous driving and robotics. Existing methods
usually assume the training and testing motions follow the same pattern while
ignoring the potential distribution differences (e.g., shopping mall and
street). This issue results in inevitable performance decrease. To address this
issue, we propose a novel Transferable Graph Neural Network (T-GNN) framework,
which jointly conducts trajectory prediction as well as domain alignment in a
unified framework. Specifically, a domain invariant GNN is proposed to explore
the structural motion knowledge where the domain specific knowledge is reduced.
Moreover, an attention-based adaptive knowledge learning module is further
proposed to explore fine-grained individual-level feature representation for
knowledge transfer. By this way, disparities across different trajectory
domains will be better alleviated. More challenging while practical trajectory
prediction experiments are designed, and the experimental results verify the
superior performance of our proposed model. To the best of our knowledge, our
work is the pioneer which fills the gap in benchmarks and techniques for
practical pedestrian trajectory prediction across different domains.",-0.10007412,-0.18740731,0.010895364,C
3053,"Experiments are car-         ments prove the superiority of our T-GNN model in both
ried out to further study the effectiveness of adaptive learn-   future trajectory prediction and trajectory domain-shift al-
ing module in our T-GNN model.","Extensive experi-
Effectiveness of Adaptive Learning.",We remove the attention-          leviation.,2022-03-09 21:08:47+00:00,Adaptive Trajectory Prediction via Transferable GNN,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yi Xu'), arxiv.Result.Author('Lichen Wang'), arxiv.Result.Author('Yizhou Wang'), arxiv.Result.Author('Yun Fu')]","Pedestrian trajectory prediction is an essential component in a wide range of
AI applications such as autonomous driving and robotics. Existing methods
usually assume the training and testing motions follow the same pattern while
ignoring the potential distribution differences (e.g., shopping mall and
street). This issue results in inevitable performance decrease. To address this
issue, we propose a novel Transferable Graph Neural Network (T-GNN) framework,
which jointly conducts trajectory prediction as well as domain alignment in a
unified framework. Specifically, a domain-invariant GNN is proposed to explore
the structural motion knowledge where the domain-specific knowledge is reduced.
Moreover, an attention-based adaptive knowledge learning module is further
proposed to explore fine-grained individual-level feature representations for
knowledge transfer. By this way, disparities across different trajectory
domains will be better alleviated. More challenging while practical trajectory
prediction experiments are designed, and the experimental results verify the
superior performance of our proposed model. To the best of our knowledge, our
work is the pioneer which fills the gap in benchmarks and techniques for
practical pedestrian trajectory prediction across different domains.",-0.030233828,-0.22702187,0.036678314,C
3054,"A baseline code is released along with          signals
the dataset on GitHub to encourage further research to the
community in developing uniÔ¨Åed perception models for au-           Optical Flow
tonomous driving.","A sub-set of 10,000 images from the dataset will be made           Event camera
public on Github1.","It contains several perception tasks listed
in Fig.",2022-03-09 21:30:52+00:00,SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ahmed Rida Sekkat'), arxiv.Result.Author('Yohan Dupuis'), arxiv.Result.Author('Varun Ravi Kumar'), arxiv.Result.Author('Hazem Rashed'), arxiv.Result.Author('Senthil Yogamani'), arxiv.Result.Author('Pascal Vasseur'), arxiv.Result.Author('Paul Honeine')]","Surround-view cameras are a primary sensor for automated driving, used for
near field perception. It is one of the most commonly used sensors in
commercial vehicles. Four fisheye cameras with a 190{\deg} field of view cover
the 360{\deg} around the vehicle. Due to its high radial distortion, the
standard algorithms do not extend easily. Previously, we released the first
public fisheye surround-view dataset named WoodScape. In this work, we release
a synthetic version of the surround-view dataset, covering many of its
weaknesses and extending it. Firstly, it is not possible to obtain ground truth
for pixel-wise optical flow and depth. Secondly, WoodScape did not have all
four cameras simultaneously in order to sample diverse frames. However, this
means that multi-camera algorithms cannot be designed, which is enabled in the
new dataset. We implemented surround-view fisheye geometric projections in
CARLA Simulator matching WoodScape's configuration and created SynWoodScape. We
release 80k images from the synthetic dataset with annotations for 10+ tasks.
We also release the baseline code and supporting scripts.",-0.23152746,0.21094875,-0.090148866,B
3073,"The new dataset           Incorporating temporal information into the network
and the WaSR-T source code will be publicly released to         for improved prediction has been explored as well, with
facilitate further research of temporal features in maritime    attention-based approaches being the most prevalent method.","challenging reÔ¨Çection scenes that facilitates the training of
temporal maritime segmentation networks.",obstacle detection.,2022-03-10 12:58:14+00:00,Temporal Context for Robust Maritime Obstacle Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lojze ≈Ωust'), arxiv.Result.Author('Matej Kristan')]","Robust maritime obstacle detection is essential for fully autonomous unmanned
surface vehicles (USVs). The currently widely adopted segmentation-based
obstacle detection methods are prone to misclassification of object reflections
and sun glitter as obstacles, producing many false positive detections,
effectively rendering the methods impractical for USV navigation. However,
water-turbulence-induced temporal appearance changes on object reflections are
very distinctive from the appearance dynamics of true objects. We harness this
property to design WaSR-T, a novel maritime obstacle detection network, that
extracts the temporal context from a sequence of recent frames to reduce
ambiguity. By learning the local temporal characteristics of object reflection
on the water surface, WaSR-T substantially improves obstacle detection accuracy
in the presence of reflections and glitter. Compared with existing single-frame
methods, WaSR-T reduces the number of false positive detections by 41% overall
and by over 53% within the danger zone of the boat, while preserving a high
recall, and achieving new state-of-the-art performance on the challenging MODS
maritime obstacle detection benchmark.",-0.21635328,-0.018885618,-0.10204959,B
3074,"The new dataset           Incorporating temporal information into the network
and the WaSR-T source code will be publicly released to         for improved prediction has been explored as well, with
facilitate further research of temporal features in maritime    attention-based approaches being the most prevalent method.","challenging reÔ¨Çection scenes that facilitates the training of
temporal maritime segmentation networks.",obstacle detection.,2022-03-10 12:58:14+00:00,Temporal Context for Robust Maritime Obstacle Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lojze ≈Ωust'), arxiv.Result.Author('Matej Kristan')]","Robust maritime obstacle detection is essential for fully autonomous unmanned
surface vehicles (USVs). The currently widely adopted segmentation-based
obstacle detection methods are prone to misclassification of object reflections
and sun glitter as obstacles, producing many false positive detections,
effectively rendering the methods impractical for USV navigation. However,
water-turbulence-induced temporal appearance changes on object reflections are
very distinctive from the appearance dynamics of true objects. We harness this
property to design WaSR-T, a novel maritime obstacle detection network, that
extracts the temporal context from a sequence of recent frames to reduce
ambiguity. By learning the local temporal characteristics of object reflection
on the water surface, WaSR-T substantially improves obstacle detection accuracy
in the presence of reflections and glitter. Compared with existing single-frame
methods, WaSR-T reduces the number of false positive detections by 41% overall
and by over 53% within the danger zone of the boat, while preserving a high
recall, and achieving new state-of-the-art performance on the challenging MODS
maritime obstacle detection benchmark. The code, pretrained models and extended
datasets are available at https://github.com/lojzezust/WaSR-T",-0.21635328,-0.018885618,-0.10204959,B
3076,"In Proceedings of the IEEE/CVF Interna-
formulation can facilitate and motivate further research re-                   tional Conference on Computer Vision, pages 10323‚Äì10332,
garding scene understanding.",The dataset and the proposed scene graph                       graph alignments.,2019.,2022-03-10 14:13:35+00:00,Spatial Commonsense Graph for Object Localisation in Partial Scenes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Francesco Giuliari'), arxiv.Result.Author('Geri Skenderi'), arxiv.Result.Author('Marco Cristani'), arxiv.Result.Author('Yiming Wang'), arxiv.Result.Author('Alessio Del Bue')]","We solve object localisation in partial scenes, a new problem of estimating
the unknown position of an object (e.g. where is the bag?) given a partial 3D
scan of a scene. The proposed solution is based on a novel scene graph model,
the Spatial Commonsense Graph (SCG), where objects are the nodes and edges
define pairwise distances between them, enriched by concept nodes and
relationships from a commonsense knowledge base. This allows SCG to better
generalise its spatial inference over unknown 3D scenes. The SCG is used to
estimate the unknown position of the target object in two steps: first, we feed
the SCG into a novel Proximity Prediction Network, a graph neural network that
uses attention to perform distance prediction between the node representing the
target object and the nodes representing the observed objects in the SCG;
second, we propose a Localisation Module based on circular intersection to
estimate the object position using all the predicted pairwise distances in
order to be independent of any reference system. We create a new dataset of
partially reconstructed scenes to benchmark our method and baselines for object
localisation in partial scenes, where our proposed method achieves the best
localisation performance.",-0.18507613,0.12847675,-0.037781883,B
3077,"1
formulation can facilitate and motivate further research re-
garding scene understanding.","Sensors, 17(3):565, 2017.","[10] Keyur Faldu, Amit Sheth, Prashant Kikani, and Hemang Ak-
Acknowledgements This project has received funding from                       abari.",2022-03-10 14:13:35+00:00,Spatial Commonsense Graph for Object Localisation in Partial Scenes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Francesco Giuliari'), arxiv.Result.Author('Geri Skenderi'), arxiv.Result.Author('Marco Cristani'), arxiv.Result.Author('Yiming Wang'), arxiv.Result.Author('Alessio Del Bue')]","We solve object localisation in partial scenes, a new problem of estimating
the unknown position of an object (e.g. where is the bag?) given a partial 3D
scan of a scene. The proposed solution is based on a novel scene graph model,
the Spatial Commonsense Graph (SCG), where objects are the nodes and edges
define pairwise distances between them, enriched by concept nodes and
relationships from a commonsense knowledge base. This allows SCG to better
generalise its spatial inference over unknown 3D scenes. The SCG is used to
estimate the unknown position of the target object in two steps: first, we feed
the SCG into a novel Proximity Prediction Network, a graph neural network that
uses attention to perform distance prediction between the node representing the
target object and the nodes representing the observed objects in the SCG;
second, we propose a Localisation Module based on circular intersection to
estimate the object position using all the predicted pairwise distances in
order to be independent of any reference system. We create a new dataset of
partially reconstructed scenes to benchmark our method and baselines for object
localisation in partial scenes, where our proposed method achieves the best
localisation performance.",-0.1069995,0.20973983,-0.22518694,B
3080,"6,
experimental results demonstrate that our method achieves        we further study the effectiveness of these two operations.","Thus, in Tab.","higher mIoU than previous methods [9, 28, 63] in all three       SpeciÔ¨Åcally, based on the plain parallel convolution branches
settings.",2022-03-10 14:48:41+00:00,Representation Compensation Networks for Continual Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chang-Bin Zhang'), arxiv.Result.Author('Jia-Wen Xiao'), arxiv.Result.Author('Xialei Liu'), arxiv.Result.Author('Ying-Cong Chen'), arxiv.Result.Author('Ming-Ming Cheng')]","In this work, we study the continual semantic segmentation problem, where the
deep neural networks are required to incorporate new classes continually
without catastrophic forgetting. We propose to use a structural
re-parameterization mechanism, named representation compensation (RC) module,
to decouple the representation learning of both old and new knowledge. The RC
module consists of two dynamically evolved branches with one frozen and one
trainable. Besides, we design a pooled cube knowledge distillation strategy on
both spatial and channel dimensions to further enhance the plasticity and
stability of the model. We conduct experiments on two challenging continual
semantic segmentation scenarios, continual class segmentation and continual
domain segmentation. Without any extra computational overhead and parameters
during inference, our method outperforms state-of-the-art performance. The code
is available at \url{https://github.com/zhangchbin/RCIL}.",0.12929177,0.10192792,0.36128125,A
3104,We thus conclude with a short list of mini-discussions intended to spark further research in the field of 3D AD&S.,"Last, we point to the fact that by fusing RGB
with 3D, one can take the best from both worlds.",Image-level anomaly detection.,2022-03-10 18:57:04+00:00,An Empirical Investigation of 3D Anomaly Detection and Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eliahu Horwitz'), arxiv.Result.Author('Yedid Hoshen')]","Anomaly detection and segmentation in images has made tremendous progress in
recent years while 3D information has often been ignored. The objective of this
paper is to further understand the benefit and role of 3D as opposed to color
in image anomaly detection. Our study begins by presenting a surprising
finding: standard color-only anomaly segmentation methods, when applied to 3D
datasets, significantly outperform all current methods. On the other hand, we
observe that color-only methods are insufficient for images containing
geometric anomalies where shape cannot be unambiguously inferred from 2D. This
suggests that better 3D methods are needed. We investigate different
representations for 3D anomaly detection and discover that handcrafted
orientation-invariant representations are unreasonably effective on this task.
We uncover a simple 3D-only method that outperforms all recent approaches while
not using deep learning, external pretraining datasets, or color information.
As the 3D-only method cannot detect color and texture anomalies, we combine it
with 2D color features, granting us the best current results by a large margin
(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by
discussing future challenges for 3D anomaly detection and segmentation.",-0.18433055,0.2741719,0.063308984,B
3105,"This suggests that further research into the feature fusion
process is required.","Unfortunately,
the fused feature underperformed the RGB only result.",Datasets.,2022-03-10 18:57:04+00:00,An Empirical Investigation of 3D Anomaly Detection and Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eliahu Horwitz'), arxiv.Result.Author('Yedid Hoshen')]","Anomaly detection and segmentation in images has made tremendous progress in
recent years while 3D information has often been ignored. The objective of this
paper is to further understand the benefit and role of 3D as opposed to color
in image anomaly detection. Our study begins by presenting a surprising
finding: standard color-only anomaly segmentation methods, when applied to 3D
datasets, significantly outperform all current methods. On the other hand, we
observe that color-only methods are insufficient for images containing
geometric anomalies where shape cannot be unambiguously inferred from 2D. This
suggests that better 3D methods are needed. We investigate different
representations for 3D anomaly detection and discover that handcrafted
orientation-invariant representations are unreasonably effective on this task.
We uncover a simple 3D-only method that outperforms all recent approaches while
not using deep learning, external pretraining datasets, or color information.
As the 3D-only method cannot detect color and texture anomalies, we combine it
with 2D color features, granting us the best current results by a large margin
(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by
discussing future challenges for 3D anomaly detection and segmentation.",-0.09820287,0.14548472,0.20043403,B
3106,We thus conclude with a short list of mini-discussions intended to spark further research in the field of 3D AD&S.,"Last, we point to the fact that by fusing RGB
with 3D, one can take the best from both worlds.",Image-level anomaly detection.,2022-03-10 18:57:04+00:00,An Empirical Investigation of 3D Anomaly Detection and Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eliahu Horwitz'), arxiv.Result.Author('Yedid Hoshen')]","Anomaly detection and segmentation in images has made tremendous progress in
recent years while 3D information has often been ignored. The objective of this
paper is to further understand the benefit and role of 3D as opposed to color
in image anomaly detection. Our study begins by presenting a surprising
finding: standard color-only anomaly segmentation methods, when applied to 3D
datasets, significantly outperform all current methods. On the other hand, we
observe that color-only methods are insufficient for images containing
geometric anomalies where shape cannot be unambiguously inferred from 2D. This
suggests that better 3D methods are needed. We investigate different
representations for 3D anomaly detection and discover that handcrafted
orientation-invariant representations are unreasonably effective on this task.
We uncover a simple 3D-only method that outperforms all recent approaches while
not using deep learning, external pretraining datasets, or color information.
As the 3D-only method cannot detect color and texture anomalies, we combine it
with 2D color features, granting us the best current results by a large margin
(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by
discussing future challenges for 3D anomaly detection and segmentation.",-0.18433055,0.2741719,0.063308984,B
3107,"This suggests that further research into the feature fusion
process is required.","Unfortunately,
the fused feature underperformed the RGB only result.",Datasets.,2022-03-10 18:57:04+00:00,An Empirical Investigation of 3D Anomaly Detection and Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eliahu Horwitz'), arxiv.Result.Author('Yedid Hoshen')]","Anomaly detection and segmentation in images has made tremendous progress in
recent years while 3D information has often been ignored. The objective of this
paper is to further understand the benefit and role of 3D as opposed to color
in image anomaly detection. Our study begins by presenting a surprising
finding: standard color-only anomaly segmentation methods, when applied to 3D
datasets, significantly outperform all current methods. On the other hand, we
observe that color-only methods are insufficient for images containing
geometric anomalies where shape cannot be unambiguously inferred from 2D. This
suggests that better 3D methods are needed. We investigate different
representations for 3D anomaly detection and discover that handcrafted
orientation-invariant representations are unreasonably effective on this task.
We uncover a simple 3D-only method that outperforms all recent approaches while
not using deep learning, external pretraining datasets, or color information.
As the 3D-only method cannot detect color and texture anomalies, we combine it
with 2D color features, granting us the best current results by a large margin
(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by
discussing future challenges for 3D anomaly detection and segmentation.",-0.09820287,0.14548472,0.20043403,B
3153,"And our major contributions are as follows: 1) we build a large-scale
object detection benchmark with diverse attributes for complex urban scenes;
2) extensive experiments of existing algorithms are conducted on this bench-
mark to show their performance; 3) our benchmark can support further study
for computer vision in urban scenes.","We conduct extensive evaluations of existing algorithms on our new bench-
marks.","weather  number of box per attribute value of scene transportation                                   orientation
                                       shooting_time

600000                                  400000                            600000
500000
400000                                  300000                            500000
300000                                                                    400000
200000
100000                                  200000                            300000

     0                                  100000                            200000
                                                                          100000
500000
400000  sunny clouodyccrlauinsyiosnno_wryatfeoggyunknown 0 dawn tdrauyncadtuisokn_ranitgeht unknown 0 front  side areaoblique back
300000
200000                                  800000                            600000
100000
                                        700000                            500000
     0                                  600000                            400000
                                        500000

                                        400000                            300000

                                        300000                            200000
                                        200000                            100000
                                        100000

        0% 10%20%30%40%50%60%70%80%90%  0 0% 10%20%30%40%50%60%70%80%90%  0 small                            medium       large

Fig.",2022-03-11 14:39:48+00:00,Peng Cheng Object Detection Benchmark for Smart City,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yaowei Wang'), arxiv.Result.Author('Zhouxin Yang'), arxiv.Result.Author('Rui Liu'), arxiv.Result.Author('Deng Li'), arxiv.Result.Author('Yuandu Lai'), arxiv.Result.Author('Leyuan Fang'), arxiv.Result.Author('Yahong Han')]","Object detection is an algorithm that recognizes and locates the objects in
the image and has a wide range of applications in the visual understanding of
complex urban scenes. Existing object detection benchmarks mainly focus on a
single specific scenario and their annotation attributes are not rich enough,
these make the object detection model is not generalized for the smart city
scenes. Considering the diversity and complexity of scenes in intelligent city
governance, we build a large-scale object detection benchmark for the smart
city. Our benchmark contains about 500K images and includes three scenarios:
intelligent transportation, intelligent security, and drones. For the
complexity of the real scene in the smart city, the diversity of weather,
occlusion, and other complex environment diversity attributes of the images in
the three scenes are annotated. The characteristics of the benchmark are
analyzed and extensive experiments of the current state-of-the-art target
detection algorithm are conducted based on our benchmark to show their
performance.",-0.30235726,0.17780058,-0.017638246,B_centroid
3184,"We believe         for the Transformer to directly divide the input images into
                                        our work can promote further research in this Ô¨Åeld.","However, it is unreasonable
                                        level annotations can still focus on the crowd regions.",a series of patches as tokens.,2022-03-12 09:40:29+00:00,Joint CNN and Transformer Network via weakly supervised Learning for efficient crowd counting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fusen Wang'), arxiv.Result.Author('Kai Liu'), arxiv.Result.Author('Fei Long'), arxiv.Result.Author('Nong Sang'), arxiv.Result.Author('Xiaofeng Xia'), arxiv.Result.Author('Jun Sang')]","Currently, for crowd counting, the fully supervised methods via density map
estimation are the mainstream research directions. However, such methods need
location-level annotation of persons in an image, which is time-consuming and
laborious. Therefore, the weakly supervised method just relying upon the
count-level annotation is urgently needed. Since CNN is not suitable for
modeling the global context and the interactions between image patches, crowd
counting with weakly supervised learning via CNN generally can not show good
performance. The weakly supervised model via Transformer was sequentially
proposed to model the global context and learn contrast features. However, the
transformer directly partitions the crowd images into a series of tokens, which
may not be a good choice due to each pedestrian being an independent
individual, and the parameter number of the network is very large. Hence, we
propose a Joint CNN and Transformer Network (JCTNet) via weakly supervised
learning for crowd counting in this paper. JCTNet consists of three parts: CNN
feature extraction module (CFM), Transformer feature extraction module (TFM),
and counting regression module (CRM). In particular, the CFM extracts crowd
semantic information features, then sends their patch partitions to TRM for
modeling global context, and CRM is used to predict the number of people.
Extensive experiments and visualizations demonstrate that JCTNet can
effectively focus on the crowd regions and obtain superior weakly supervised
counting performance on five mainstream datasets. The number of parameters of
the model can be reduced by about 67%~73% compared with the pure Transformer
works. We also tried to explain the phenomenon that a model constrained only by
count-level annotations can still focus on the crowd regions. We believe our
work can promote further research in this field.",-0.13506177,0.07589578,-0.04863083,B
3185,"Some recent works introduce the vision transformer into
      could encourage further research in the weakly supervised  weakly supervised crowd counting.","In addition, we analyze why this model       regions in many vision tasks, such as image classiÔ¨Åcation [13],
      constrained only by count-level annotations can focus      [23], image segmentation [13]‚Äì[15], [24], object detection
      on crowd regions in Section 5 and hope this work           [25].",Liang et al.,2022-03-12 09:40:29+00:00,Joint CNN and Transformer Network via weakly supervised Learning for efficient crowd counting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fusen Wang'), arxiv.Result.Author('Kai Liu'), arxiv.Result.Author('Fei Long'), arxiv.Result.Author('Nong Sang'), arxiv.Result.Author('Xiaofeng Xia'), arxiv.Result.Author('Jun Sang')]","Currently, for crowd counting, the fully supervised methods via density map
estimation are the mainstream research directions. However, such methods need
location-level annotation of persons in an image, which is time-consuming and
laborious. Therefore, the weakly supervised method just relying upon the
count-level annotation is urgently needed. Since CNN is not suitable for
modeling the global context and the interactions between image patches, crowd
counting with weakly supervised learning via CNN generally can not show good
performance. The weakly supervised model via Transformer was sequentially
proposed to model the global context and learn contrast features. However, the
transformer directly partitions the crowd images into a series of tokens, which
may not be a good choice due to each pedestrian being an independent
individual, and the parameter number of the network is very large. Hence, we
propose a Joint CNN and Transformer Network (JCTNet) via weakly supervised
learning for crowd counting in this paper. JCTNet consists of three parts: CNN
feature extraction module (CFM), Transformer feature extraction module (TFM),
and counting regression module (CRM). In particular, the CFM extracts crowd
semantic information features, then sends their patch partitions to TRM for
modeling global context, and CRM is used to predict the number of people.
Extensive experiments and visualizations demonstrate that JCTNet can
effectively focus on the crowd regions and obtain superior weakly supervised
counting performance on five mainstream datasets. The number of parameters of
the model can be reduced by about 67%~73% compared with the pure Transformer
works. We also tried to explain the phenomenon that a model constrained only by
count-level annotations can still focus on the crowd regions. We believe our
work can promote further research in this field.",-0.26686245,-0.026714727,-0.12380885,B
3192,"The liter-
                                        ature review identiÔ¨Åed relevant gaps that require further research, such as the
                                        requirement of dataset-independent approaches and methods suitable for au-
                                        tonomous detection of position of parking spaces.","In this study, we surveyed and compared robust publicly available
                                        image datasets speciÔ¨Åcally crafted to test computer vision-based methods for
                                        parking lot management approaches and consequently present a systematic and
                                        comprehensive review of existing works that employ such datasets.","In addition, we have noticed
                                        that several important factors such as the presence of the same cars across con-
                                        secutive images, have been neglected in most studies, thereby rendering unrealis-
                                        tic assessment protocols.",2022-03-12 15:35:29+00:00,A Systematic Review on Computer Vision-Based Parking Lot Management Applied on Public Datasets,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Paulo Ricardo Lisboa de Almeida'), arxiv.Result.Author('Jeovane Hon√≥rio Alves'), arxiv.Result.Author('Rafael Stubs Parpinelli'), arxiv.Result.Author('Jean Paul Barddal')]","Computer vision-based parking lot management methods have been extensively
researched upon owing to their flexibility and cost-effectiveness. To evaluate
such methods authors often employ publicly available parking lot image
datasets. In this study, we surveyed and compared robust publicly available
image datasets specifically crafted to test computer vision-based methods for
parking lot management approaches and consequently present a systematic and
comprehensive review of existing works that employ such datasets. The
literature review identified relevant gaps that require further research, such
as the requirement of dataset-independent approaches and methods suitable for
autonomous detection of position of parking spaces. In addition, we have
noticed that several important factors such as the presence of the same cars
across consecutive images, have been neglected in most studies, thereby
rendering unrealistic assessment protocols. Furthermore, the analysis of the
datasets also revealed that certain features that should be present when
developing new benchmarks, such as the availability of video sequences and
images taken in more diverse conditions, including nighttime and snow, have not
been incorporated.",-0.31484002,0.12230836,-0.15067855,B
3212,"This was
                                        Face - Morphing - Attack - Detection - Development -          followed by a set of datasets that were created to train and
                                        dataset                                                       evaluate MADs [51‚Äì53] and to further study the vulnera-
                                                      bone fide:25k_BA
                                         25K

                 250K  LQ Discard

Noise z       G                     50K
                        Image
                       selection

  N (0,1)                                                                                     10K Discard

 Gaussian
Distribution

                 FIQA                                               5K   Morphing             Image filter
                                Quality scores

                                                               25K

                                                                                   25K morph                15K_MA

                                                                    20K

Figure 1.","Related work
                                        be trained to differentiate between the two classes, MAs and
                                                                                                         The creation of face MA datasets started with the initial
                                            1https://github.com/naserdamer/SMDD- Synthetic-           works that uncovered the attack possibility [26].","WorkÔ¨Çow of the SMDD dataset creation, starting from synthetic faces to the Ô¨Ånal MA and BF sets.",2022-03-13 15:55:00+00:00,Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,cs.CV,['cs.CV'],"[arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('C√©sar Augusto Fontanillo L√≥pez'), arxiv.Result.Author('Meiling Fang'), arxiv.Result.Author('No√©mie Spiller'), arxiv.Result.Author('Minh Vu Pham'), arxiv.Result.Author('Fadi Boutros')]","The main question this work aims at answering is: can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?.
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is made publicly
available for research purposes.",0.05251287,-0.025567034,0.13084881,C
3213,"This was
                                        Face - Morphing - Attack - Detection - Development -          followed by a set of datasets that were created to train and
                                        dataset                                                       evaluate MADs [50‚Äì52] and to further study the vulnera-
                                                      bone fide:25k_BA
                                         25K

                 250K  LQ Discard

Noise z       G                     50K
                        Image
                       selection

  N (0,1)                                                                                     10K Discard

 Gaussian
Distribution

                 FIQA                                               5K   Morphing             Image filter
                                Quality scores

                                                               25K

                                                                                   25K morph                15K_MA

                                                                    20K

Figure 1.","Related work
                                        be trained to differentiate between the two classes, MAs and
                                                                                                         The creation of face MA datasets started with the initial
                                            1https://github.com/naserdamer/SMDD- Synthetic-           works that uncovered the attack possibility [26].","WorkÔ¨Çow of the SMDD dataset creation, starting from synthetic faces to the Ô¨Ånal MA and BF sets.",2022-03-13 15:55:00+00:00,Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,cs.CV,['cs.CV'],"[arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('C√©sar Augusto Fontanillo L√≥pez'), arxiv.Result.Author('Meiling Fang'), arxiv.Result.Author('No√©mie Spiller'), arxiv.Result.Author('Minh Vu Pham'), arxiv.Result.Author('Fadi Boutros')]","The main question this work aims at answering is: can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?.
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is made publicly
available for research purposes.",0.055682838,-0.022646124,0.13039753,C
3214,"This was
    1https://github.com/naserdamer/SMDD- Synthetic-            followed by a set of datasets that were created to train and
Face - Morphing - Attack - Detection - Development -           evaluate MADs [53‚Äì55] and to further study the vulnera-
dataset                                                        bility of FR to MAs [28, 59].","This train-         The creation of face MA datasets started with the initial
                                                               works that uncovered the attack possibility [27].","Most of such datasets cre-
                                                      bone fide:25k_BA
                                         25K

                 250K  LQ Discard

Noise z       G                     50K
                        Image
                       selection

  N (0,1)                                                                                    10K Discard

 Gaussian
Distribution

                 FIQA                                               5K  Morphing             Image filter
                                Quality scores

                                                               25K

                                                                                  25K morph                15K_MA

                                                                                                            20K

Figure 1.",2022-03-13 15:55:00+00:00,Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,cs.CV,['cs.CV'],"[arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('C√©sar Augusto Fontanillo L√≥pez'), arxiv.Result.Author('Meiling Fang'), arxiv.Result.Author('No√©mie Spiller'), arxiv.Result.Author('Minh Vu Pham'), arxiv.Result.Author('Fadi Boutros')]","The main question this work aims at answering is: ""can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?"".
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is publicly available
for research purposes.",0.076965034,-0.01605744,0.107382536,C
3276,"This practical difference highlights the need   differentiable loss function, which we describe below, that can
for further study on real-world data, which has not been well     be used as a learning objective to train deep neural networks.","terized by a heterogeneity that originates from the different     An embedding uk is said to be cycle consistent if l = k.
hospitals, such as surgical workÔ¨Çow, demographic, and hard-       [47] reformulated this constraint as a regression task with a
ware variability.",investigated in previous works.,2022-03-14 17:44:53+00:00,Federated Cycling (FedCy): Semi-supervised Federated Learning of Surgical Phases,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.2.10']","[arxiv.Result.Author('Hasan Kassem'), arxiv.Result.Author('Deepak Alapatt'), arxiv.Result.Author('Pietro Mascagni'), arxiv.Result.Author('AI4SafeChole Consortium'), arxiv.Result.Author('Alexandros Karargyris'), arxiv.Result.Author('Nicolas Padoy')]","Recent advancements in deep learning methods bring computer-assistance a step
closer to fulfilling promises of safer surgical procedures. However, the
generalizability of such methods is often dependent on training on diverse
datasets from multiple medical institutions, which is a restrictive requirement
considering the sensitive nature of medical data. Recently proposed
collaborative learning methods such as Federated Learning (FL) allow for
training on remote datasets without the need to explicitly share data. Even so,
data annotation still represents a bottleneck, particularly in medicine and
surgery where clinical expertise is often required. With these constraints in
mind, we propose FedCy, a federated semi-supervised learning (FSSL) method that
combines FL and self-supervised learning to exploit a decentralized dataset of
both labeled and unlabeled videos, thereby improving performance on the task of
surgical phase recognition. By leveraging temporal patterns in the labeled
data, FedCy helps guide unsupervised training on unlabeled data towards
learning task-specific features for phase recognition. We demonstrate
significant performance gains over state-of-the-art FSSL methods on the task of
automatic recognition of surgical phases using a newly collected
multi-institutional dataset of laparoscopic cholecystectomy videos.
Furthermore, we demonstrate that our approach also learns more generalizable
features when tested on data from an unseen domain.",0.022940135,-0.2414341,0.17919189,C
3277,"This practical difference highlights the need      differentiable loss function, which we describe below, that can
for further study on real-world data, which has not been well
                         This article has been accepted for publication in IEEE Transactions on Medical Imaging.","terized by a heterogeneity that originates from the different        An embedding uk is said to be cycle consistent if l = k.
hospitals, such as surgical workÔ¨Çow, demographic, and hard-          [49] reformulated this constraint as a regression task with a
ware variability.","Citation information: DOI 10.1109/TMI.2022.3222126                                            4

be used as a learning objective to train deep neural networks.",2022-03-14 17:44:53+00:00,Federated Cycling (FedCy): Semi-supervised Federated Learning of Surgical Phases,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.2.10']","[arxiv.Result.Author('Hasan Kassem'), arxiv.Result.Author('Deepak Alapatt'), arxiv.Result.Author('Pietro Mascagni'), arxiv.Result.Author('AI4SafeChole Consortium'), arxiv.Result.Author('Alexandros Karargyris'), arxiv.Result.Author('Nicolas Padoy')]","Recent advancements in deep learning methods bring computer-assistance a step
closer to fulfilling promises of safer surgical procedures. However, the
generalizability of such methods is often dependent on training on diverse
datasets from multiple medical institutions, which is a restrictive requirement
considering the sensitive nature of medical data. Recently proposed
collaborative learning methods such as Federated Learning (FL) allow for
training on remote datasets without the need to explicitly share data. Even so,
data annotation still represents a bottleneck, particularly in medicine and
surgery where clinical expertise is often required. With these constraints in
mind, we propose FedCy, a federated semi-supervised learning (FSSL) method that
combines FL and self-supervised learning to exploit a decentralized dataset of
both labeled and unlabeled videos, thereby improving performance on the task of
surgical phase recognition. By leveraging temporal patterns in the labeled
data, FedCy helps guide unsupervised training on unlabeled data towards
learning task-specific features for phase recognition. We demonstrate
significant performance gains over state-of-the-art FSSL methods on the task of
automatic recognition of surgical phases using a newly collected
multi-institutional dataset of laparoscopic cholecystectomy videos.
Furthermore, we demonstrate that our approach also learns more generalizable
features when tested on data from an unseen domain.",0.018844921,-0.19898689,0.22827062,C
3305,"Code, data and pre-trained models proposed introduced herewith will be open-
sourced to further research.","Finally, we
conclude by discussing the known shortcomings of our method in Section 6.6 and
show more qualitative results over different challenging datasets in Section 6.7.","6.1 Signed Distance Regularization

To recall, we are given a template volume and target volume, denoted as [T ],
[S] respectively, which, we wish to align by learning a deformation field Dœâ(¬∑).",2022-03-15 07:22:52+00:00,Implicit field supervision for robust non-rigid shape matching,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ramana Sundararaman'), arxiv.Result.Author('Gautam Pai'), arxiv.Result.Author('Maks Ovsjanikov')]","Establishing a correspondence between two non-rigidly deforming shapes is one
of the most fundamental problems in visual computing. Existing methods often
show weak resilience when presented with challenges innate to real-world data
such as noise, outliers, self-occlusion etc. On the other hand, auto-decoders
have demonstrated strong expressive power in learning geometrically meaningful
latent embeddings. However, their use in \emph{shape analysis} and especially
in non-rigid shape correspondence has been limited. In this paper, we introduce
an approach based on auto-decoder framework, that learns a continuous
shape-wise deformation field over a fixed template. By supervising the
deformation field for points on-surface and regularising for points off-surface
through a novel \emph{Signed Distance Regularisation} (SDR), we learn an
alignment between the template and shape \emph{volumes}. Unlike classical
correspondence techniques, our method is remarkably robust in the presence of
strong artefacts and can be generalised to arbitrary shape categories. Trained
on clean water-tight meshes, \emph{without} any data-augmentation, we
demonstrate compelling performance on compromised data and real-world scans.",-0.012995632,-0.09325958,0.03092673,C
3306,"Code, data and pre-trained models proposed introduced herewith will be open-
sourced to further research.","Finally, we
conclude by discussing the known shortcomings of our method in Section 6.6 and
show more qualitative results over different challenging datasets in Section 6.7.","6.1 Signed Distance Regularization

To recall, we are given a template volume and target volume, denoted as [T ],
[S] respectively, which, we wish to align by learning a deformation field Dœâ(¬∑).",2022-03-15 07:22:52+00:00,Implicit field supervision for robust non-rigid shape matching,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ramana Sundararaman'), arxiv.Result.Author('Gautam Pai'), arxiv.Result.Author('Maks Ovsjanikov')]","Establishing a correspondence between two non-rigidly deforming shapes is one
of the most fundamental problems in visual computing. Existing methods often
show weak resilience when presented with challenges innate to real-world data
such as noise, outliers, self-occlusion etc. On the other hand, auto-decoders
have demonstrated strong expressive power in learning geometrically meaningful
latent embeddings. However, their use in shape analysis and especially in
non-rigid shape correspondence has been limited. In this paper, we introduce an
approach based on auto-decoder framework, that learns a continuous shape-wise
deformation field over a fixed template. By supervising the deformation field
for points on-surface and regularising for points off-surface through a novel
Signed Distance Regularisation (SDR), we learn an alignment between the
template and shape volumes. Unlike classical correspondence techniques, our
method is remarkably robust in the presence of strong artefacts and can be
generalised to arbitrary shape categories. Trained on clean water-tight meshes,
without any data-augmentation, we demonstrate compelling performance on
compromised data and real-world scans.",-0.012995632,-0.09325958,0.03092673,C
3310,"We expect our CODA dataset to facili-
         tate further research in reliable detection for real-world autonomous driv-
         ing.","Moreover, we experiment with the state-of-the-art open-world object de-
         tector and find that it also fails to reliably identify the novel objects in
         CODA, suggesting that a robust perception system for autonomous driv-
         ing is probably still far from reach.",Our dataset will be released at https://coda-dataset.github.io.,2022-03-15 08:32:56+00:00,CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Kaican Li'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Lanqing Hong'), arxiv.Result.Author('Chaoqiang Ye'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Yukuai Chen'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Dit-Yan Yeung'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Hang Xu')]","Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning 30+ object categories. On CODA, the
performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.",-0.3764498,0.080657765,-0.07501801,B
3311,"We hope that CODA can motivate further research
in reliable detection for real-world autonomous driving.","We further provide a thorough
comparison of different methods and shed light on potential solutions to a more
robust perception system.","CODA: A Real-World Road Corner Case Dataset for Autonomous Driving  15

References

 1.",2022-03-15 08:32:56+00:00,CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Kaican Li'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Lanqing Hong'), arxiv.Result.Author('Chaoqiang Ye'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Yukuai Chen'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Dit-Yan Yeung'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Hang Xu')]","Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning 30+ object categories. On CODA, the
performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.",-0.33834037,0.18610246,-0.1126883,B
3312,"We expect our CODA dataset to
                                                 facilitate further research in reliable detection for real-world autonomous
                                                 driving.","Moreover, we experiment with the state-of-the-art open-world object de-
                                                 tector and find that it also fails to reliably identify the novel objects
                                                 in CODA, suggesting that a robust perception system for autonomous
                                                 driving is probably still far from reach.",Our dataset is available at https://coda-dataset.github.io.,2022-03-15 08:32:56+00:00,CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Kaican Li'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Lanqing Hong'), arxiv.Result.Author('Chaoqiang Ye'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Yukuai Chen'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Dit-Yan Yeung'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Hang Xu')]","Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.",-0.39398247,0.11687997,-0.08886336,B
3313,"We hope that CODA can motivate further research
in reliable detection for real-world autonomous driving.","We further provide a thorough
comparison of different methods and shed light on potential solutions to a more
robust perception system.","CODA: A Real-World Road Corner Case Dataset for Autonomous Driving  15

References

 1.",2022-03-15 08:32:56+00:00,CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Kaican Li'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Lanqing Hong'), arxiv.Result.Author('Chaoqiang Ye'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Yukuai Chen'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Dit-Yan Yeung'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Hang Xu')]","Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.",-0.33834037,0.18610246,-0.1126883,B
3314,"We expect our CODA dataset to
                                                 facilitate further research in reliable detection for real-world autonomous
                                                 driving.","Moreover, we experiment with the state-of-the-art open-world object de-
                                                 tector and find that it also fails to reliably identify the novel objects
                                                 in CODA, suggesting that a robust perception system for autonomous
                                                 driving is probably still far from reach.",Our dataset is available at https://coda-dataset.github.io.,2022-03-15 08:32:56+00:00,CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Kaican Li'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Lanqing Hong'), arxiv.Result.Author('Chaoqiang Ye'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Yukuai Chen'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Dit-Yan Yeung'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Hang Xu')]","Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.",-0.39398247,0.11687997,-0.08886336,B
3315,"We hope that CODA can motivate further research
in reliable detection for real-world autonomous driving.","We further provide a thorough
comparison of different methods and shed light on potential solutions to a more
robust perception system.","Acknowledgement

We gratefully acknowledge the support of MindSpore, CANN (Compute Archi-
tecture for Neural Networks) and Ascend AI Processor used for this research.",2022-03-15 08:32:56+00:00,CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Kaican Li'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Lanqing Hong'), arxiv.Result.Author('Chaoqiang Ye'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Yukuai Chen'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Dit-Yan Yeung'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Hang Xu')]","Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.",-0.24343428,0.0886622,0.054213304,B
3321,"We hope our work will encourage further research in
the column labeled RGB in Fig.","As a result, the output from RGB       enable expanding the training set to learn a more robust net-
based model produces large false positive detections, see        work.","2.                                similar directions towards robust and scalable maniuplation
                                                                 detection techniques.",2022-03-15 12:26:29+00:00,SISL:Self-Supervised Image Signature Learning for Splicing Detection and Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Susmit Agrawal'), arxiv.Result.Author('Prabhat Kumar'), arxiv.Result.Author('Siddharth Seth'), arxiv.Result.Author('Toufiq Parag'), arxiv.Result.Author('Maneesh Singh'), arxiv.Result.Author('Venkatesh Babu')]","Recent algorithms for image manipulation detection almost exclusively use
deep network models. These approaches require either dense pixelwise
groundtruth masks, camera ids, or image metadata to train the networks. On one
hand, constructing a training set to represent the countless tampering
possibilities is impractical. On the other hand, social media platforms or
commercial applications are often constrained to remove camera ids as well as
metadata from images. A self-supervised algorithm for training manipulation
detection models without dense groundtruth or camera/image metadata would be
extremely useful for many forensics applications. In this paper, we propose
self-supervised approach for training splicing detection/localization models
from frequency transforms of images. To identify the spliced regions, our deep
network learns a representation to capture an image specific signature by
enforcing (image) self consistency . We experimentally demonstrate that our
proposed model can yield similar or better performances of multiple existing
methods on standard datasets without relying on labels or metadata.",-0.217805,0.07798991,0.07460567,B
3324,"Therefore,        promising avenue of research that may lead to intrinsic methods that
further research is needed for optimization-based approaches that        handle both partial and non-isometric shapes.","Moreover, such descriptors tend to be sensitive to noise, and      development of overlap estimation techniques (e.g., [APO21]) is a
may lead to poor registration results on noisy models.",can handle large deformation between noisy models.,2022-03-11 15:54:19+00:00,A Survey of Non-Rigid 3D Registration,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Bailin Deng'), arxiv.Result.Author('Yuxin Yao'), arxiv.Result.Author('Roberto M. Dyke'), arxiv.Result.Author('Juyong Zhang')]","Non-rigid registration computes an alignment between a source surface with a
target surface in a non-rigid manner. In the past decade, with the advances in
3D sensing technologies that can measure time-varying surfaces, non-rigid
registration has been applied for the acquisition of deformable shapes and has
a wide range of applications. This survey presents a comprehensive review of
non-rigid registration methods for 3D shapes, focusing on techniques related to
dynamic shape acquisition and reconstruction. In particular, we review
different approaches for representing the deformation field, and the methods
for computing the desired deformation. Both optimization-based and
learning-based methods are covered. We also review benchmarks and datasets for
evaluating non-rigid registration methods, and discuss potential future
research directions.",-0.025732229,0.27060986,-0.039395362,B
3325,"Therefore,        promising avenue of research that may lead to intrinsic methods that
further research is needed for optimization-based approaches that        handle both partial and non-isometric shapes.","Moreover, such descriptors tend to be sensitive to noise, and      development of overlap estimation techniques (e.g., [APO21]) is a
may lead to poor registration results on noisy models.",can handle large deformation between noisy models.,2022-03-11 15:54:19+00:00,A Survey of Non-Rigid 3D Registration,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Bailin Deng'), arxiv.Result.Author('Yuxin Yao'), arxiv.Result.Author('Roberto M. Dyke'), arxiv.Result.Author('Juyong Zhang')]","Non-rigid registration computes an alignment between a source surface with a
target surface in a non-rigid manner. In the past decade, with the advances in
3D sensing technologies that can measure time-varying surfaces, non-rigid
registration has been applied for the acquisition of deformable shapes and has
a wide range of applications. This survey presents a comprehensive review of
non-rigid registration methods for 3D shapes, focusing on techniques related to
dynamic shape acquisition and reconstruction. In particular, we review
different approaches for representing the deformation field, and the methods
for computing the desired deformation. Both optimization-based and
learning-based methods are covered. We also review benchmarks and datasets for
evaluating non-rigid registration methods, and discuss potential future
research directions.",-0.025732229,0.27060986,-0.039395362,B
3331,"We believe that further research
in methods of encoding image content and knowledge targets will lead to more precise control of question generation.","In the case of the bottom-right example, the generated question is indeed related to
the target knowledge, but the question is about the board itself, not the board material.","6 Conclusion

In this study, we introduce a novel VQG task that uses knowledge as the target of the question.",2022-03-15 13:38:10+00:00,K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Kohei Uehara'), arxiv.Result.Author('Tatsuya Harada')]","Visual Question Generation (VQG) is a task to generate questions from images.
When humans ask questions about an image, their goal is often to acquire some
new knowledge. However, existing studies on VQG have mainly addressed question
generation from answers or question categories, overlooking the objectives of
knowledge acquisition. To introduce a knowledge acquisition perspective into
VQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is
the first large, humanly annotated dataset in which questions regarding images
are tied to structured knowledge. We also developed a new VQG model that can
encode and use knowledge as the target for a question. The experiment results
show that our model outperforms existing models on the K-VQG dataset.",0.06946657,-0.19253084,-0.21970865,C
3332,"Thus, many popular recognition datasets [15, 16, 17] were able                                                                                2
to release panoptic segmentation benchmarks in short amount
of time, which facilitated further research.","E-mail address: josip.saric@fer.hr      tion towards the new task due to straight-forward upgrade of the
                                                                                                           annotations: panoptic segmentation labels can be acquired by
                                                                                                           combining existing semantic and instance segmentation labels.",2.1.,2022-03-15 13:47:40+00:00,Panoptic SwiftNet: Pyramidal Fusion for Real-time Panoptic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Josip ≈†ariƒá'), arxiv.Result.Author('Marin Or≈°iƒá'), arxiv.Result.Author('Sini≈°a ≈†egviƒá')]","Dense panoptic prediction is a key ingredient in many existing applications
such as autonomous driving, automated warehouses or agri-robotics. However,
most of these applications leverage the recovered dense semantics as an input
to visual closed-loop control. Hence, practical deployments require real-time
inference over large input resolutions on embedded hardware. These requirements
call for computationally efficient approaches which deliver high accuracy with
limited computational resources. We propose to achieve this goal by trading-off
backbone capacity for multi-scale feature extraction. In comparison with
contemporaneous approaches to panoptic segmentation, the main novelties of our
method are scale-equivariant feature extraction and cross-scale upsampling
through pyramidal fusion. Our best model achieves 55.9% PQ on Cityscapes val at
60 FPS on full resolution 2MPx images and RTX3090 with FP16 Tensor RT
optimization.",-0.1935667,0.015903044,-0.14086002,B
3339,"Moreover, we conduct a
                                                 thorough performance analysis of several state-of-the-art object detec-
                                                 tors on the MOBDrone data, serving as baselines for further research.","We manually
                                                 annotated more than 180K objects, of which about 113K man overboard,
                                                 precisely localizing them with bounding boxes.","Keywords: Man Overboard ¬∑ Object Detection¬∑ Unmanned Aerial Ve-
                                                 hicles ¬∑ Drone ¬∑ Benchmark

                                        1 Introduction

                                        The 2021 Annual Overview of Marine Casualties and Incidents [9] reported that
                                        22, 532 marine casualties and incidents were occurred between 2014 and 2020 in
                                        the waters of EU Member States or involving EU ships.",2022-03-15 15:02:23+00:00,MOBDrone: a Drone Video Dataset for Man OverBoard Rescue,cs.CV,['cs.CV'],"[arxiv.Result.Author('Donato Cafarelli'), arxiv.Result.Author('Luca Ciampi'), arxiv.Result.Author('Lucia Vadicamo'), arxiv.Result.Author('Claudio Gennaro'), arxiv.Result.Author('Andrea Berton'), arxiv.Result.Author('Marco Paterni'), arxiv.Result.Author('Chiara Benvenuti'), arxiv.Result.Author('Mirko Passera'), arxiv.Result.Author('Fabrizio Falchi')]","Modern Unmanned Aerial Vehicles (UAV) equipped with cameras can play an
essential role in speeding up the identification and rescue of people who have
fallen overboard, i.e., man overboard (MOB). To this end, Artificial
Intelligence techniques can be leveraged for the automatic understanding of
visual data acquired from drones. However, detecting people at sea in aerial
imagery is challenging primarily due to the lack of specialized annotated
datasets for training and testing detectors for this task. To fill this gap, we
introduce and publicly release the MOBDrone benchmark, a collection of more
than 125K drone-view images in a marine environment under several conditions,
such as different altitudes, camera shooting angles, and illumination. We
manually annotated more than 180K objects, of which about 113K man overboard,
precisely localizing them with bounding boxes. Moreover, we conduct a thorough
performance analysis of several state-of-the-art object detectors on the
MOBDrone data, serving as baselines for further research.",-0.07140938,0.21897216,-0.11234677,B
3340,"Furthermore, we report an in-depth experimental evaluation of several state-of-
the-art object detectors, serving as baselines for further research on this topic.","SpeciÔ¨Åcally, we collected more than 125K images,
and we manually annotated more than 180K objects in a marine environment
under several conditions, like diÔ¨Äerent altitudes and camera shooting angles.","Our analysis shows that detectors pre-trained on standard datasets of everyday
objects exhibit moderate performance in localizing and recognizing people at sea
in aerial images acquired at mid-high altitudes.",2022-03-15 15:02:23+00:00,MOBDrone: a Drone Video Dataset for Man OverBoard Rescue,cs.CV,['cs.CV'],"[arxiv.Result.Author('Donato Cafarelli'), arxiv.Result.Author('Luca Ciampi'), arxiv.Result.Author('Lucia Vadicamo'), arxiv.Result.Author('Claudio Gennaro'), arxiv.Result.Author('Andrea Berton'), arxiv.Result.Author('Marco Paterni'), arxiv.Result.Author('Chiara Benvenuti'), arxiv.Result.Author('Mirko Passera'), arxiv.Result.Author('Fabrizio Falchi')]","Modern Unmanned Aerial Vehicles (UAV) equipped with cameras can play an
essential role in speeding up the identification and rescue of people who have
fallen overboard, i.e., man overboard (MOB). To this end, Artificial
Intelligence techniques can be leveraged for the automatic understanding of
visual data acquired from drones. However, detecting people at sea in aerial
imagery is challenging primarily due to the lack of specialized annotated
datasets for training and testing detectors for this task. To fill this gap, we
introduce and publicly release the MOBDrone benchmark, a collection of more
than 125K drone-view images in a marine environment under several conditions,
such as different altitudes, camera shooting angles, and illumination. We
manually annotated more than 180K objects, of which about 113K man overboard,
precisely localizing them with bounding boxes. Moreover, we conduct a thorough
performance analysis of several state-of-the-art object detectors on the
MOBDrone data, serving as baselines for further research.",-0.3931719,0.058883894,-0.10850386,B
3346,"Synchronized sensor data are available
in a raw format so that various representations can be evaluated and further research can be
conducted, possibly with fusion-based approaches.","6.5 Conclusions and discussions

This chapter introduced RADIal, a new dataset containing sequences of automotive-grade
sensor signals (HD RADAR, camera and LiDAR).","The FFT-RadNet is also presented as a
novel trainable architecture to process and analyse HD RADAR signals.",2022-03-15 16:19:51+00:00,Deep learning for radar data exploitation of autonomous vehicle,cs.CV,['cs.CV'],[arxiv.Result.Author('Arthur Ouaknine')],"Autonomous driving requires a detailed understanding of complex driving
scenes. The redundancy and complementarity of the vehicle's sensors provide an
accurate and robust comprehension of the environment, thereby increasing the
level of performance and safety. This thesis focuses the on automotive RADAR,
which is a low-cost active sensor measuring properties of surrounding objects,
including their relative speed, and has the key advantage of not being impacted
by adverse weather conditions. With the rapid progress of deep learning and the
availability of public driving datasets, the perception ability of vision-based
driving systems has considerably improved. The RADAR sensor is seldom used for
scene understanding due to its poor angular resolution, the size, noise, and
complexity of RADAR raw data as well as the lack of available datasets. This
thesis proposes an extensive study of RADAR scene understanding, from the
construction of an annotated dataset to the conception of adapted deep learning
architectures. First, this thesis details approaches to tackle the current lack
of data. A simple simulation as well as generative methods for creating
annotated data will be presented. It will also describe the CARRADA dataset,
composed of synchronised camera and RADAR data with a semi-automatic annotation
method. This thesis then present a proposed set of deep learning architectures
with their associated loss functions for RADAR semantic segmentation. It also
introduces a method to open up research into the fusion of LiDAR and RADAR
sensors for scene understanding. Finally, this thesis exposes a collaborative
contribution, the RADIal dataset with synchronised High-Definition (HD) RADAR,
LiDAR and camera. A deep learning architecture is also proposed to estimate the
RADAR signal processing pipeline while performing multitask learning for object
detection and free driving space segmentation.",0.005447668,0.20729718,-0.008755083,B
3368,"We further study the inÔ¨Çuence 0.60% A4Pll 11..6982 50..5172 41..0992 60..0112
of the number of perturbed patches un-
der the same global perturbation ratio as 0.80% 24PP 00..8382 83..5679 41..3070 84..8295
shown in Tab.","4P       15.62      12.58       22.80     14.18
                                                          All      14.02      0.76        16.55      2.24

                                                          1P       16.63      26.52       32.13     25.12
                                                          2P       11.74      16.5        22.92     16.51
                                                          4P        7.57      9.46        12.14     10.10
                                                          All       6.77      0.36        9.05       0.52

InÔ¨Çuence of the number of perturbed                     2P         3.21           11.18  10.42          11.46

patches.",5.,2022-03-16 04:45:59+00:00,Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yonggan Fu'), arxiv.Result.Author('Shunyao Zhang'), arxiv.Result.Author('Shang Wu'), arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Yingyan Lin')]","Vision transformers (ViTs) have recently set off a new wave in neural
architecture design thanks to their record-breaking performance in various
vision tasks. In parallel, to fulfill the goal of deploying ViTs into
real-world vision applications, their robustness against potential malicious
attacks has gained increasing attention. In particular, recent works show that
ViTs are more robust against adversarial attacks as compared with convolutional
neural networks (CNNs), and conjecture that this is because ViTs focus more on
capturing global interactions among different input/feature patches, leading to
their improved robustness to local perturbations imposed by adversarial
attacks. In this work, we ask an intriguing question: ""Under what kinds of
perturbations do ViTs become more vulnerable learners compared to CNNs?"" Driven
by this question, we first conduct a comprehensive experiment regarding the
robustness of both ViTs and CNNs under various existing adversarial attacks to
understand the underlying reason favoring their robustness. Based on the drawn
insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that
fools the self-attention mechanism by attacking its basic component (i.e., a
single patch) with a series of attention-aware optimization techniques.
Interestingly, our Patch-Fool framework shows for the first time that ViTs are
not necessarily more robust than CNNs against adversarial perturbations. In
particular, we find that ViTs are more vulnerable learners compared with CNNs
against our Patch-Fool attack which is consistent across extensive experiments,
and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool,
indicate an intriguing insight that the perturbation density and strength on
each patch seem to be the key factors that influence the robustness ranking
between ViTs and CNNs.",0.47223586,0.19287108,0.11513882,A
3369,"We further study the inÔ¨Çuence 0.60% A4Pll 11..6982 50..5172 41..0992 60..0112
of the number of perturbed patches un-
der the same global perturbation ratio as 0.80% 24PP 00..8382 83..5679 41..3070 84..8295
shown in Tab.","4P       15.62      12.58       22.80     14.18
                                                          All      14.02      0.76        16.55      2.24

                                                          1P       16.63      26.52       32.13     25.12
                                                          2P       11.74      16.5        22.92     16.51
                                                          4P        7.57      9.46        12.14     10.10
                                                          All       6.77      0.36        9.05       0.52

InÔ¨Çuence of the number of perturbed                     2P         3.21           11.18  10.42          11.46

patches.",5.,2022-03-16 04:45:59+00:00,Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yonggan Fu'), arxiv.Result.Author('Shunyao Zhang'), arxiv.Result.Author('Shang Wu'), arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Yingyan Lin')]","Vision transformers (ViTs) have recently set off a new wave in neural
architecture design thanks to their record-breaking performance in various
vision tasks. In parallel, to fulfill the goal of deploying ViTs into
real-world vision applications, their robustness against potential malicious
attacks has gained increasing attention. In particular, recent works show that
ViTs are more robust against adversarial attacks as compared with convolutional
neural networks (CNNs), and conjecture that this is because ViTs focus more on
capturing global interactions among different input/feature patches, leading to
their improved robustness to local perturbations imposed by adversarial
attacks. In this work, we ask an intriguing question: ""Under what kinds of
perturbations do ViTs become more vulnerable learners compared to CNNs?"" Driven
by this question, we first conduct a comprehensive experiment regarding the
robustness of both ViTs and CNNs under various existing adversarial attacks to
understand the underlying reason favoring their robustness. Based on the drawn
insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that
fools the self-attention mechanism by attacking its basic component (i.e., a
single patch) with a series of attention-aware optimization techniques.
Interestingly, our Patch-Fool framework shows for the first time that ViTs are
not necessarily more robust than CNNs against adversarial perturbations. In
particular, we find that ViTs are more vulnerable learners compared with CNNs
against our Patch-Fool attack which is consistent across extensive experiments,
and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool,
indicate an intriguing insight that the perturbation density and strength on
each patch seem to be the key factors that influence the robustness ranking
between ViTs and CNNs.",0.47223586,0.19287108,0.11513882,A
3373,"We hope our new
editing perspective can inspire further researches towards
better attribute disentanglement free from pre-trained GAN
inversion methods.","Third, since the sampling process of sparse representa-
tion is based on the statistics of the whole training set, the
editing generated from AGE may lead to crashes in the im-
ages when encountering extreme cases.","G. Additional Disentangled Attribute Editing
     Directions

   We provide more visualizations of disentangled attribute
editing directions in different layers/groups learned by AGE
in Figure 18.",2022-03-16 06:54:09+00:00,Attribute Group Editing for Reliable Few-shot Image Generation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guanqi Ding'), arxiv.Result.Author('Xinzhe Han'), arxiv.Result.Author('Shuhui Wang'), arxiv.Result.Author('Shuzhe Wu'), arxiv.Result.Author('Xin Jin'), arxiv.Result.Author('Dandan Tu'), arxiv.Result.Author('Qingming Huang')]","Few-shot image generation is a challenging task even using the
state-of-the-art Generative Adversarial Networks (GANs). Due to the unstable
GAN training process and the limited training data, the generated images are
often of low quality and low diversity. In this work, we propose a new
editing-based method, i.e., Attribute Group Editing (AGE), for few-shot image
generation. The basic assumption is that any image is a collection of
attributes and the editing direction for a specific attribute is shared across
all categories. AGE examines the internal representation learned in GANs and
identifies semantically meaningful directions. Specifically, the class
embedding, i.e., the mean vector of the latent codes from a specific category,
is used to represent the category-relevant attributes, and the
category-irrelevant attributes are learned globally by Sparse Dictionary
Learning on the difference between the sample embedding and the class
embedding. Given a GAN well trained on seen categories, diverse images of
unseen categories can be synthesized through editing category-irrelevant
attributes while keeping category-relevant attributes unchanged. Without
re-training the GAN, AGE is capable of not only producing more realistic and
diverse images for downstream visual applications with limited data but
achieving controllable image editing with interpretable category-irrelevant
directions.",-0.03012057,-0.16356206,0.11579601,C
3395,"Though in some cases, the student trained by our
                                                                     method can surpass the teacher when the annotated images are
   5) Generalization Ablation: The generalization ability for        scarce, further research is needed to explore the lightweight
different networks of knowledge distillation methods is crucial      student network with the teacher level performance.",distillation.,"Secondly,
in medical image segmentation.",2022-03-16 14:56:02+00:00,Graph Flow: Cross-layer Graph Flow Distillation for Dual Efficient Medical Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wenxuan Zou'), arxiv.Result.Author('Muyi Sun')]","With the development of deep convolutional neural networks, medical image
segmentation has achieved a series of breakthroughs in recent years. However,
the high-performance convolutional neural networks always mean numerous
parameters and high computation costs, which will hinder the applications in
clinical scenarios. Meanwhile, the scarceness of large-scale annotated medical
image datasets further impedes the application of high-performance networks. To
tackle these problems, we propose Graph Flow, a comprehensive knowledge
distillation framework, for both network-efficiency and annotation-efficiency
medical image segmentation. Specifically, our core Graph Flow Distillation
transfer the essence of cross-layer variations from a well-trained cumbersome
teacher network to a non-trained compact student network. In addition, an
unsupervised Paraphraser Module is integrated to purify the knowledge of the
teacher network, which is also beneficial for the stabilization of training
procedure. Furthermore, we build a unified distillation framework by
integrating the adversarial distillation and the vanilla logits distillation,
which can further refine the final predictions of the compact network. With
different teacher networks (conventional convolutional architecture or
prevalent transformer architecture) and student networks, we conduct extensive
experiments on four medical image datasets with different modalities (Gastric
Cancer, Synapse, BUSI, and CVC-ClinicDB).We demonstrate the prominent ability
of our method which achieves competitive performance on these datasets.
Moreover, we demonstrate the effectiveness of our Graph Flow through a novel
semi-supervised paradigm for dual efficient medical image segmentation. Our
code will be available at Graph Flow.",0.003129731,-0.211551,0.017751325,C
3396,"As shown in Table A.1, the best value of Œ≤ could be posi-
tively proportional to the gap, which we suppose could be
guidance of tuning Œ≤ and a direction for further research.","Both top-1 accuracy (%) and the gap
zt ‚àí zmax (averaged over all training samples) are reported.","Based on this, the value of Œ≤ for each teacher in Table 6 and
Table 7 of the manuscript is set as follows (in Table A.2):

           teacher     zt ‚àí zmax Œ≤

           ResNet56    5.36   2.0

           ResNet110   6.73   2.0                                  T      1      2      3      4
                                                                 top-1  73.24  73.05  71.69  70.96
           WRN-40-2    7.24   6.0

           VGG13       8.25   6.0

           ResNet50    8.53   8.0                                Table A.3.",2022-03-16 15:07:47+00:00,Decoupled Knowledge Distillation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Borui Zhao'), arxiv.Result.Author('Quan Cui'), arxiv.Result.Author('Renjie Song'), arxiv.Result.Author('Yiyu Qiu'), arxiv.Result.Author('Jiajun Liang')]","State-of-the-art distillation methods are mainly based on distilling deep
features from intermediate layers, while the significance of logit distillation
is greatly overlooked. To provide a novel viewpoint to study logit
distillation, we reformulate the classical KD loss into two parts, i.e., target
class knowledge distillation (TCKD) and non-target class knowledge distillation
(NCKD). We empirically investigate and prove the effects of the two parts: TCKD
transfers knowledge concerning the ""difficulty"" of training samples, while NCKD
is the prominent reason why logit distillation works. More importantly, we
reveal that the classical KD loss is a coupled formulation, which (1)
suppresses the effectiveness of NCKD and (2) limits the flexibility to balance
these two parts. To address these issues, we present Decoupled Knowledge
Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently
and flexibly. Compared with complex feature-based methods, our DKD achieves
comparable or even better results and has better training efficiency on
CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object
detection tasks. This paper proves the great potential of logit distillation,
and we hope it will be helpful for future research. The code is available at
https://github.com/megvii-research/mdistiller.",0.48388246,-0.012256436,0.08664933,A
3397,"As shown in Table A.1, the best value of Œ≤ could be posi-
tively proportional to the gap, which we suppose could be
guidance of tuning Œ≤ and a direction for further research.","Both top-1 accuracy (%) and the gap
zt ‚àí zmax (averaged over all training samples) are reported.","Based on this, the value of Œ≤ for each teacher in Table 6 and
Table 7 of the manuscript is set as follows (in Table A.2):

           teacher     zt ‚àí zmax Œ≤

           ResNet56    5.36   2.0

           ResNet110   6.73   2.0                                  T      1      2      3      4
                                                                 top-1  73.24  73.05  71.69  70.96
           WRN-40-2    7.24   6.0

           VGG13       8.25   6.0

           ResNet50    8.53   8.0                                Table A.3.",2022-03-16 15:07:47+00:00,Decoupled Knowledge Distillation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Borui Zhao'), arxiv.Result.Author('Quan Cui'), arxiv.Result.Author('Renjie Song'), arxiv.Result.Author('Yiyu Qiu'), arxiv.Result.Author('Jiajun Liang')]","State-of-the-art distillation methods are mainly based on distilling deep
features from intermediate layers, while the significance of logit distillation
is greatly overlooked. To provide a novel viewpoint to study logit
distillation, we reformulate the classical KD loss into two parts, i.e., target
class knowledge distillation (TCKD) and non-target class knowledge distillation
(NCKD). We empirically investigate and prove the effects of the two parts: TCKD
transfers knowledge concerning the ""difficulty"" of training samples, while NCKD
is the prominent reason why logit distillation works. More importantly, we
reveal that the classical KD loss is a coupled formulation, which (1)
suppresses the effectiveness of NCKD and (2) limits the flexibility to balance
these two parts. To address these issues, we present Decoupled Knowledge
Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently
and flexibly. Compared with complex feature-based methods, our DKD achieves
comparable or even better results and has better training efficiency on
CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object
detection tasks. This paper proves the great potential of logit distillation,
and we hope it will be helpful for future research. The code is available at
https://github.com/megvii-research/mdistiller.",0.48388246,-0.012256436,0.08664933,A
3403,"Ratio Nearest Linear Quadratic Cubic-Spline DeciWatch

    20% 115.3 106.0 107.7               108.0    66.7‚Üì39.3(37.1%)
    10% 129.3 106.7 108.9               109.5    70.8‚Üì35.9(33.6%)
    5% 156.4 119.0 121.8                126.0    89.5‚Üì29.5(24.8%)

4.5 Ablation Study
As a baseline framework, we do not emphasize the novelty of network design
but provide some possible designs in each step for further research.","We use 3D pose estimator SPIN [30] as the single-frame
estimator, and its MPJPE is 108.6mm.","We have
explored the impact of different pose estimators in Table 1 and 2 in previous
sections.",2022-03-16 16:03:37+00:00,DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ailing Zeng'), arxiv.Result.Author('Xuan Ju'), arxiv.Result.Author('Lei Yang'), arxiv.Result.Author('Ruiyuan Gao'), arxiv.Result.Author('Xizhou Zhu'), arxiv.Result.Author('Bo Dai'), arxiv.Result.Author('Qiang Xu')]","This paper proposes a simple baseline framework for video-based 2D/3D human
pose estimation that can achieve 10 times efficiency improvement over existing
works without any performance degradation, named DeciWatch. Unlike current
solutions that estimate each frame in a video, DeciWatch introduces a simple
yet effective sample-denoise-recover framework that only watches sparsely
sampled frames, taking advantage of the continuity of human motions and the
lightweight pose representation. Specifically, DeciWatch uniformly samples less
than 10% video frames for detailed estimation, denoises the estimated 2D/3D
poses with an efficient Transformer architecture, and then accurately recovers
the rest of the frames using another Transformer-based network. Comprehensive
experimental results on three video-based human pose estimation and body mesh
recovery tasks with four datasets validate the efficiency and effectiveness of
DeciWatch.",0.07793617,0.37086004,-0.07604545,B
3404,"Ratio Nearest Linear Quadratic Cubic-Spline DeciWatch

           20% 106.7 104.6 105.8         106.8              67.6‚Üì39.7(37.0%)
           10% 108.3 106.3 108.2         108.9              71.3‚Üì36.0(33.6%)
           5% 123.2 120.7 119.9          121.2              90.8‚Üì16.5(15.4%)

4.5 Ablation Study

As a baseline framework, we do not emphasize the novelty of network design
but provide some possible designs in each step for further research.","We use 3D pose estimator SPIN [30] as the single-frame
estimator, and its MPJPE is 107.7mm.","We have
explored the impact of diÔ¨Äerent pose estimators in Table 1 and 2 in previous
sections.",2022-03-16 16:03:37+00:00,DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ailing Zeng'), arxiv.Result.Author('Xuan Ju'), arxiv.Result.Author('Lei Yang'), arxiv.Result.Author('Ruiyuan Gao'), arxiv.Result.Author('Xizhou Zhu'), arxiv.Result.Author('Bo Dai'), arxiv.Result.Author('Qiang Xu')]","This paper proposes a simple baseline framework for video-based 2D/3D human
pose estimation that can achieve 10 times efficiency improvement over existing
works without any performance degradation, named DeciWatch. Unlike current
solutions that estimate each frame in a video, DeciWatch introduces a simple
yet effective sample-denoise-recover framework that only watches sparsely
sampled frames, taking advantage of the continuity of human motions and the
lightweight pose representation. Specifically, DeciWatch uniformly samples less
than 10% video frames for detailed estimation, denoises the estimated 2D/3D
poses with an efficient Transformer architecture, and then accurately recovers
the rest of the frames using another Transformer-based network. Comprehensive
experimental results on three video-based human pose estimation and body mesh
recovery tasks with four datasets validate the efficiency and effectiveness of
DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.",0.10482397,0.3673793,-0.053031787,B
3443,"The code will be re-
                                                                 leased to facilitate further research.",a deeper understanding of the images.,"Acknowledgements                               Li, Y.; Vishwamitra, N.; Knijnenburg, B. P.; Hu, H.; and
                                                                Caine, K. 2017b.",2022-03-17 06:56:29+00:00,DRAG: Dynamic Region-Aware GCN for Privacy-Leaking Image Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guang Yang'), arxiv.Result.Author('Juan Cao'), arxiv.Result.Author('Qiang Sheng'), arxiv.Result.Author('Peng Qi'), arxiv.Result.Author('Xirong Li'), arxiv.Result.Author('Jintao Li')]","The daily practice of sharing images on social media raises a severe issue
about privacy leakage. To address the issue, privacy-leaking image detection is
studied recently, with the goal to automatically identify images that may leak
privacy. Recent advance on this task benefits from focusing on crucial objects
via pretrained object detectors and modeling their correlation. However, these
methods have two limitations: 1) they neglect other important elements like
scenes, textures, and objects beyond the capacity of pretrained object
detectors; 2) the correlation among objects is fixed, but a fixed correlation
is not appropriate for all the images. To overcome the limitations, we propose
the Dynamic Region-Aware Graph Convolutional Network (DRAG) that dynamically
finds out crucial regions including objects and other important elements, and
models their correlation adaptively for each input image. To find out crucial
regions, we cluster spatially-correlated feature channels into several
region-aware feature maps. Further, we dynamically model the correlation with
the self-attention mechanism and explore the interaction among the regions with
a graph convolutional network. The DRAG achieved an accuracy of 87% on the
largest dataset for privacy-leaking image detection, which is 10 percentage
points higher than the state of the art. The further case study demonstrates
that it found out crucial regions containing not only objects but other
important elements like textures.",0.021778151,0.09353623,-0.02421876,C
3444,"need further research in accumulating more comprehensive
4.6.",reason about the answer in an end-to-end mode.,Limitation Analysis                                                       knowledge and evaluating the triplet extraction quality.,2022-03-17 07:42:14+00:00,MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Yang Ding'), arxiv.Result.Author('Jing Yu'), arxiv.Result.Author('Bang Liu'), arxiv.Result.Author('Yue Hu'), arxiv.Result.Author('Mingxin Cui'), arxiv.Result.Author('Qi Wu')]","Knowledge-based visual question answering requires the ability of associating
external knowledge for open-ended cross-modal scene understanding. One
limitation of existing solutions is that they capture relevant knowledge from
text-only knowledge bases, which merely contain facts expressed by first-order
predicates or language descriptions while lacking complex but indispensable
multimodal knowledge for visual understanding. How to construct vision-relevant
and explainable multimodal knowledge for the VQA scenario has been less
studied. In this paper, we propose MuKEA to represent multimodal knowledge by
an explicit triplet to correlate visual objects and fact answers with implicit
relations. To bridge the heterogeneous gap, we propose three objective losses
to learn the triplet representations from complementary views: embedding
structure, topological relation and semantic space. By adopting a pre-training
and fine-tuning learning strategy, both basic and domain-specific multimodal
knowledge are progressively accumulated for answer prediction. We outperform
the state-of-the-art by 3.35% and 6.08% respectively on two challenging
knowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the
complementary benefits of the multimodal knowledge with existing knowledge
bases and the advantages of our end-to-end framework over the existing pipeline
methods. The code is available at https://github.com/AndersonStra/MuKEA.",0.3606388,0.06704213,-0.052379273,A
3448,"Human self-localization methods

   To facilitate further research and down-stream applica-        Human self-localization aims at estimating the 6-DoF
tions, we propose a dataset containing three large scenes      of the human subject with carrying devices.",2.2.,"The re-
(1k-5k m2) with accurate dynamic human motions and lo-         ceived signal strength (RSS) fingerprinting-based method-
cations.",2022-03-17 10:05:55+00:00,HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yudi Dai'), arxiv.Result.Author('Yitai Lin'), arxiv.Result.Author('Chenglu Wen'), arxiv.Result.Author('Siqi Shen'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Jingyi Yu'), arxiv.Result.Author('Yuexin Ma'), arxiv.Result.Author('Cheng Wang')]","We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code is available at https://github.com/climbingdaily/HSC4D.",-0.1723266,0.18425378,-0.15426654,B
3449,"Human self-localization methods
   To facilitate further research and down-stream applica-
tions, we propose a dataset containing three large scenes         Human self-localization aims at estimating the 6-DoF
(1k-5k m2) with accurate dynamic human motions and lo-         of the human subject with carrying devices.",2.2.,"The re-
cations.",2022-03-17 10:05:55+00:00,HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yudi Dai'), arxiv.Result.Author('Yitai Lin'), arxiv.Result.Author('Chenglu Wen'), arxiv.Result.Author('Siqi Shen'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Jingyi Yu'), arxiv.Result.Author('Yuexin Ma'), arxiv.Result.Author('Cheng Wang')]","We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code are available at http://www.lidarhumanmotion.net/hsc4d/.",-0.22744393,0.17380784,-0.16534492,B
3450,"Human self-localization methods
   To facilitate further research and down-stream applica-
tions, we propose a dataset containing three large scenes         Human self-localization aims at estimating the 6-DoF
(1k-5k m2) with accurate dynamic human motions and lo-         of the human subject with carrying devices.",2.2.,"The re-
cations.",2022-03-17 10:05:55+00:00,HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yudi Dai'), arxiv.Result.Author('Yitai Lin'), arxiv.Result.Author('Chenglu Wen'), arxiv.Result.Author('Siqi Shen'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Jingyi Yu'), arxiv.Result.Author('Yuexin Ma'), arxiv.Result.Author('Cheng Wang')]","We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code are available at http://www.lidarhumanmotion.net/hsc4d/.",-0.22744393,0.17380784,-0.16534492,B
3456,"We hope that the                          [21] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine,
                                                                                      ‚ÄúStochastic variational video prediction,‚Äù in International Conference
contributions of our work, such as the hierarchical recurrent                         on Learning Representations (ICLR), 2018.

predictor or our open-source video prediction framework,                        [22] I. Pre¬¥mont-Schwarz, A. Ilin, T. Hao, A. Rasmus, R. Boney, and
                                                                                      H. Valpola, ‚ÄúRecurrent ladder networks,‚Äù Advances in Neural Infor-
enable further research on hierarchical video prediction.","11 474‚Äì11 484.

locations, over longer time horizons.","We                          mation Processing Systems (NeurIPS), vol.",2022-03-17 13:08:28+00:00,Video Prediction at Multiple Scales with Hierarchical Recurrent Networks,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Ani Karapetyan'), arxiv.Result.Author('Angel Villar-Corrales'), arxiv.Result.Author('Andreas Boltres'), arxiv.Result.Author('Sven Behnke')]","Autonomous systems not only need to understand their current environment, but
should also be able to predict future actions conditioned on past states, for
instance based on captured camera frames. For certain tasks, detailed
predictions such as future video frames are required in the near future,
whereas for others it is beneficial to also predict more abstract
representations for longer time horizons. However, existing video prediction
models mainly focus on forecasting detailed possible outcomes for short
time-horizons, hence being of limited use for robot perception and spatial
reasoning. We propose Multi-Scale Hierarchical Prediction (MSPred), a novel
video prediction model able to forecast future possible outcomes of different
levels of granularity at different time-scales simultaneously. By combining
spatial and temporal downsampling, MSPred is able to efficiently predict
abstract representations such as human poses or object locations over long time
horizons, while still maintaining a competitive performance for video frame
prediction. In our experiments, we demonstrate that our proposed model
accurately predicts future video frames as well as other representations (e.g.
keypoints or positions) on various scenarios, including bin-picking scenes or
action recognition datasets, consistently outperforming popular approaches for
video frame prediction. Furthermore, we conduct an ablation study to
investigate the importance of the different modules and design choices in
MSPred. In the spirit of reproducible research, we open-source VP-Suite, a
general framework for deep-learning-based video prediction, as well as
pretrained models to reproduce our results.",-0.13472551,-0.18068667,0.017610505,C
3473,"Thus, we
believe that this work demonstrates the promise of this type of approaches, and we hope that it will
encourage further research in this underexplored space.","VDVAE-SR is part of the
scarce family of VAE-based models for image super-resolution and, to the best of our knowledge, it
is the Ô¨Årst super-resolution model that employs a very deep hierarchy of latent variables.","References

Agustsson, Eirikur and Radu Timofte (July 2017).",2022-03-17 17:05:14+00:00,Image Super-Resolution With Deep Variational Autoencoders,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Darius Chira'), arxiv.Result.Author('Ilian Haralampiev'), arxiv.Result.Author('Ole Winther'), arxiv.Result.Author('Andrea Dittadi'), arxiv.Result.Author('Valentin Li√©vin')]","Image super-resolution (SR) techniques are used to generate a high-resolution
image from a low-resolution image. Until now, deep generative models such as
autoregressive models and Generative Adversarial Networks (GANs) have proven to
be effective at modelling high-resolution images. Models based on Variational
Autoencoders (VAEs) have often been criticized for their feeble generative
performance, but with new advancements such as VDVAE (very deep VAE), there is
now strong evidence that deep VAEs have the potential to outperform current
state-of-the-art models for high-resolution image generation. In this paper, we
introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE
methodologies to improve upon image super-resolution using transfer learning on
pretrained VDVAEs. Through qualitative and quantitative evaluations, we show
that the proposed model is competitive with other state-of-the-art methods.",-0.0042705154,0.13097444,0.23221825,C
3484,"Future work will further study uniÔ¨Åed approaches for such
layout problems.","But if we can obtain text columns as layout entities, semantic
paragraphs will be available to provide better document structures for down-
stream applications.",Acknowledgements.,2022-03-17 22:27:12+00:00,Unified Line and Paragraph Detection by Graph Convolutional Networks,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Shuang Liu'), arxiv.Result.Author('Renshen Wang'), arxiv.Result.Author('Michalis Raptis'), arxiv.Result.Author('Yasuhisa Fujii')]","We formulate the task of detecting lines and paragraphs in a document into a
unified two-level clustering problem. Given a set of text detection boxes that
roughly correspond to words, a text line is a cluster of boxes and a paragraph
is a cluster of lines. These clusters form a two-level tree that represents a
major part of the layout of a document. We use a graph convolutional network to
predict the relations between text detection boxes and then build both levels
of clusters from these predictions. Experimentally, we demonstrate that the
unified approach can be highly efficient while still achieving state-of-the-art
quality for detecting paragraphs in public benchmarks and real-world images.",0.1817452,-0.0026116706,-0.27791965,A
3495,"We further study the influence of
Quantitative Results.",11).,"In Table 7, we report the final results      different decoding modes on performance.",2022-03-18 07:35:26+00:00,Local-Global Context Aware Transformer for Language-Guided Video Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chen Liang'), arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('Tianfei Zhou'), arxiv.Result.Author('Jiaxu Miao'), arxiv.Result.Author('Yawei Luo'), arxiv.Result.Author('Yi Yang')]","We explore the task of language-guided video segmentation (LVS). Previous
algorithms mostly adopt 3D CNNs to learn video representation, struggling to
capture long-term context and easily suffering from visual-linguistic
misalignment. In light of this, we present Locater (local-global context aware
Transformer), which augments the Transformer architecture with a finite memory
so as to query the entire video with the language expression in an efficient
manner. The memory is designed to involve two components -- one for
persistently preserving global video content, and one for dynamically gathering
local temporal context and segmentation history. Based on the memorized
local-global context and the particular content of each frame, Locater
holistically and flexibly comprehends the expression as an adaptive query
vector for each frame. The vector is used to query the corresponding frame for
mask generation. The memory also allows Locater to process videos with linear
time complexity and constant size memory, while Transformer-style
self-attention computation scales quadratically with sequence length. To
thoroughly examine the visual grounding capability of LVS models, we contribute
a new LVS dataset, A2D-S+, which is built upon A2D-S dataset but poses
increased challenges in disambiguating among similar objects. Experiments on
three LVS datasets and our A2D-S+ show that Locater outperforms previous
state-of-the-arts. Further, our Locater based solution achieved the 1st place
in the Referring Video Object Segmentation Track of the 3rd Large-scale Video
Object Segmentation Challenge. Our code and dataset are available at:
https://github.com/leonnnop/Locater",0.4155175,-0.048025467,0.07958378,A
3496,"The pre-established
Although suitable as baselines for further research, they       approach to mitigate the eÔ¨Äects of image post-processing
are inherently inscalable, rapidly obsolescent, and uncer-      (e.g.",uated on randomly cropped images.,"blurring, resizing, cropping, rotation, relighting,
tain in reality where plausible sources are not bounded to      and additive noise) is to retrain the classiÔ¨Åers on samples
the arbitrarily learned set of GMs.",2022-03-18 07:43:03+00:00,Transferable Class-Modelling for Decentralized Source Attribution of GAN-Generated Images,cs.CV,"['cs.CV', 'cs.LG', 'I.2.10; I.5.4; K.6.5']","[arxiv.Result.Author('Brandon B. G. Khoo'), arxiv.Result.Author('Chern Hong Lim'), arxiv.Result.Author('Raphael C. -W. Phan')]","GAN-generated deepfakes as a genre of digital images are gaining ground as
both catalysts of artistic expression and malicious forms of deception,
therefore demanding systems to enforce and accredit their ethical use. Existing
techniques for the source attribution of synthetic images identify subtle
intrinsic fingerprints using multiclass classification neural nets limited in
functionality and scalability. Hence, we redefine the deepfake detection and
source attribution problems as a series of related binary classification tasks.
We leverage transfer learning to rapidly adapt forgery detection networks for
multiple independent attribution problems, by proposing a semi-decentralized
modular design to solve them simultaneously and efficiently. Class activation
mapping is also demonstrated as an effective means of feature localization for
model interpretation. Our models are determined via experimentation to be
competitive with current benchmarks, and capable of decent performance on human
portraits in ideal conditions. Decentralized fingerprint-based attribution is
found to retain validity in the presence of novel sources, but is more
susceptible to type II errors that intensify with image perturbations and
attributive uncertainty. We describe both our conceptual framework and model
prototypes for further enhancement when investigating the technical limits of
reactive deepfake attribution.",-0.08487289,-0.087841995,0.21412987,C
3497,"4.4 Evaluation on clean images

We further study the performance of RConv-MK on clean images.","In contrast, our RConv-MK improves RConv-UK under
all attacks as the channel operations are conducted in a well-structured space.","We evaluate
it on tasks of image recognition, object detection and instance segmentation.",2022-03-18 08:13:56+00:00,Towards Robust 2D Convolution for Reliable Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lida Li'), arxiv.Result.Author('Shuai Li'), arxiv.Result.Author('Kun Wang'), arxiv.Result.Author('Xiangchu Feng'), arxiv.Result.Author('Lei Zhang')]","2D convolution (Conv2d), which is responsible for extracting features from
the input image, is one of the key modules of a convolutional neural network
(CNN). However, Conv2d is vulnerable to image corruptions and adversarial
samples. It is an important yet rarely investigated problem that whether we can
design a more robust alternative of Conv2d for more reliable feature
extraction. In this paper, inspired by the recently developed learnable sparse
transform that learns to convert the CNN features into a compact and sparse
latent space, we design a novel building block, denoted by RConv-MK, to
strengthen the robustness of extracted convolutional features. Our method
leverages a set of learnable kernels of different sizes to extract features at
different frequencies and employs a normalized soft thresholding operator to
adaptively remove noises and trivial features at different corruption levels.
Extensive experiments on clean images, corrupted images as well as adversarial
samples validate the effectiveness of the proposed robust module for reliable
visual recognition. The source codes are enclosed in the submission.",-0.17724347,0.078048326,0.16322297,B
3525,"Several mathematical properties of style-transfer operations were
also analysed and the obtained results could give extremely interesting hints to
further study the Forensic Ballistics task on Deepfake images with techniques
similar to double quantization detection [8].","In our experi-
ments, only the Style-Transfer category as deepfake manipulation type was taken
into account.","The remainder of this paper is organized as follows: Section 2 presents some
state-of-the-art methods of Deepfakes creation in order to understand the process
of creating synthetic digital content.",2022-03-18 13:11:54+00:00,Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Luca Guarnera'), arxiv.Result.Author('Oliver Giudice'), arxiv.Result.Author('Sebastiano Battiato')]","Most recent style-transfer techniques based on generative architectures are
able to obtain synthetic multimedia contents, or commonly called deepfakes,
with almost no artifacts. Researchers already demonstrated that synthetic
images contain patterns that can determine not only if it is a deepfake but
also the generative architecture employed to create the image data itself.
These traces can be exploited to study problems that have never been addressed
in the context of deepfakes. To this aim, in this paper a first approach to
investigate the image ballistics on deepfake images subject to style-transfer
manipulations is proposed. Specifically, this paper describes a study on
detecting how many times a digital image has been processed by a generative
architecture for style transfer. Moreover, in order to address and study
accurately forensic ballistics on deepfake images, some mathematical properties
of style-transfer operations were investigated.",-0.08695533,0.022976836,0.13579668,C
3535,"Additionally, to
         spur further research in event-based semantic segmentation, we intro-
         duce DSEC-Semantic, the first large-scale event-based dataset with fine-
         grained labels.","For this reason, our method neither requires video
         data nor per-pixel alignment between images and events and, crucially,
         does not need to hallucinate motion from still images.","We show that using image labels alone, ESS outperforms
         existing UDA approaches, and when combined with event labels, it even
         outperforms state-of-the-art supervised approaches on both DDD17 and
         DSEC-Semantic.",2022-03-18 15:30:01+00:00,ESS: Learning Event-based Semantic Segmentation from Still Images,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhaoning Sun'), arxiv.Result.Author('Nico Messikommer'), arxiv.Result.Author('Daniel Gehrig'), arxiv.Result.Author('Davide Scaramuzza')]","Retrieving accurate semantic information in challenging high dynamic range
(HDR) and high-speed conditions remains an open challenge for image-based
algorithms due to severe image degradations. Event cameras promise to address
these challenges since they feature a much higher dynamic range and are
resilient to motion blur. Nonetheless, semantic segmentation with event cameras
is still in its infancy which is chiefly due to the novelty of the sensor, and
the lack of high-quality, labeled datasets. In this work, we introduce ESS,
which tackles this problem by directly transferring the semantic segmentation
task from existing labeled image datasets to unlabeled events via unsupervised
domain adaptation (UDA). Compared to existing UDA methods, our approach aligns
recurrent, motion-invariant event embeddings with image embeddings. For this
reason, our method neither requires video data nor per-pixel alignment between
images and events and, crucially, does not need to hallucinate motion from
still images. Additionally, to spur further research in event-based semantic
segmentation, we introduce DSEC-Semantic, the first large-scale event-based
dataset with fine-grained labels. We show that using image labels alone, ESS
outperforms existing UDA approaches, and when combined with event labels, it
even outperforms state-of-the-art supervised approaches on both DDD17 and
DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount
of existing labeled image datasets and paves the way for new and exciting
research directions in new fields previously inaccessible for event cameras.",-0.33872247,0.01562239,-0.123558775,B
3536,"DSEC-Semantic is a large-scale
event-based dataset for semantic segmentation, with high-quality, fine-grained
semantic labels, which will spur further research in event-based semantic scene
understanding.","We thoroughly
evaluated our method, both on the existing DDD17 benchmark, and the newly
generated DSEC-Semantic benchmark, where we outperform existing state-of-
the-art methods in UDA and supervised settings.","While only evaluated for semantic segmentation, we believe that
these performance gains can readily be transferred to other tasks.",2022-03-18 15:30:01+00:00,ESS: Learning Event-based Semantic Segmentation from Still Images,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhaoning Sun'), arxiv.Result.Author('Nico Messikommer'), arxiv.Result.Author('Daniel Gehrig'), arxiv.Result.Author('Davide Scaramuzza')]","Retrieving accurate semantic information in challenging high dynamic range
(HDR) and high-speed conditions remains an open challenge for image-based
algorithms due to severe image degradations. Event cameras promise to address
these challenges since they feature a much higher dynamic range and are
resilient to motion blur. Nonetheless, semantic segmentation with event cameras
is still in its infancy which is chiefly due to the novelty of the sensor, and
the lack of high-quality, labeled datasets. In this work, we introduce ESS,
which tackles this problem by directly transferring the semantic segmentation
task from existing labeled image datasets to unlabeled events via unsupervised
domain adaptation (UDA). Compared to existing UDA methods, our approach aligns
recurrent, motion-invariant event embeddings with image embeddings. For this
reason, our method neither requires video data nor per-pixel alignment between
images and events and, crucially, does not need to hallucinate motion from
still images. Additionally, to spur further research in event-based semantic
segmentation, we introduce DSEC-Semantic, the first large-scale event-based
dataset with fine-grained labels. We show that using image labels alone, ESS
outperforms existing UDA approaches, and when combined with event labels, it
even outperforms state-of-the-art supervised approaches on both DDD17 and
DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount
of existing labeled image datasets and paves the way for new and exciting
research directions in new fields previously inaccessible for event cameras.",-0.3032816,-0.044548146,-0.07818618,B
3537,"DSEC-Semantic is a
large-scale event-based dataset for semantic segmentation, with high-quality,
fine-grained semantic labels, which will spur further research in event-based
semantic scene understanding.","We
thoroughly evaluated our method, both on the existing DDD17 benchmark, and
the newly generated DSEC-Semantic benchmark, where we outperform existing
state-of-the-art methods in UDA and supervised settings.","While only evaluated for semantic segmentation,
we believe that these performance gains can be transferred to other tasks.",2022-03-18 15:30:01+00:00,ESS: Learning Event-based Semantic Segmentation from Still Images,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhaoning Sun'), arxiv.Result.Author('Nico Messikommer'), arxiv.Result.Author('Daniel Gehrig'), arxiv.Result.Author('Davide Scaramuzza')]","Retrieving accurate semantic information in challenging high dynamic range
(HDR) and high-speed conditions remains an open challenge for image-based
algorithms due to severe image degradations. Event cameras promise to address
these challenges since they feature a much higher dynamic range and are
resilient to motion blur. Nonetheless, semantic segmentation with event cameras
is still in its infancy which is chiefly due to the lack of high-quality,
labeled datasets. In this work, we introduce ESS (Event-based Semantic
Segmentation), which tackles this problem by directly transferring the semantic
segmentation task from existing labeled image datasets to unlabeled events via
unsupervised domain adaptation (UDA). Compared to existing UDA methods, our
approach aligns recurrent, motion-invariant event embeddings with image
embeddings. For this reason, our method neither requires video data nor
per-pixel alignment between images and events and, crucially, does not need to
hallucinate motion from still images. Additionally, we introduce DSEC-Semantic,
the first large-scale event-based dataset with fine-grained labels. We show
that using image labels alone, ESS outperforms existing UDA approaches, and
when combined with event labels, it even outperforms state-of-the-art
supervised approaches on both DDD17 and DSEC-Semantic. Finally, ESS is
general-purpose, which unlocks the vast amount of existing labeled image
datasets and paves the way for new and exciting research directions in new
fields previously inaccessible for event cameras.",-0.30452597,-0.040974937,-0.07798278,B
3538,"Different Segmentation Architectures: We further study
whether USRN can work well with different seman-
tic segmentation architectures.",Ô¨Åcial Intelligence Lab for Enterprises (SCALE@NTU).,"We studied three widely
References                                                                 samples.",2022-03-18 15:53:18+00:00,Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dayan Guan'), arxiv.Result.Author('Jiaxing Huang'), arxiv.Result.Author('Aoran Xiao'), arxiv.Result.Author('Shijian Lu')]","Semi-supervised semantic segmentation learns from small amounts of labelled
images and large amounts of unlabelled images, which has witnessed impressive
progress with the recent advance of deep neural networks. However, it often
suffers from severe class-bias problem while exploring the unlabelled images,
largely due to the clear pixel-wise class imbalance in the labelled images.
This paper presents an unbiased subclass regularization network (USRN) that
alleviates the class imbalance issue by learning class-unbiased segmentation
from balanced subclass distributions. We build the balanced subclass
distributions by clustering pixels of each original class into multiple
subclasses of similar sizes, which provide class-balanced pseudo supervision to
regularize the class-biased segmentation. In addition, we design an
entropy-based gate mechanism to coordinate learning between the original
classes and the clustered subclasses which facilitates subclass regularization
effectively by suppressing unconfident subclass predictions. Extensive
experiments over multiple public benchmarks show that USRN achieves superior
performance as compared with the state-of-the-art.",0.0817177,-0.061095256,-0.046161942,A
3539,"Different Segmentation Architectures: We further study
whether USRN can work well with different seman-
tic segmentation architectures.",Ô¨Åcial Intelligence Lab for Enterprises (SCALE@NTU).,"We studied three widely
References                                                                 samples.",2022-03-18 15:53:18+00:00,Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dayan Guan'), arxiv.Result.Author('Jiaxing Huang'), arxiv.Result.Author('Aoran Xiao'), arxiv.Result.Author('Shijian Lu')]","Semi-supervised semantic segmentation learns from small amounts of labelled
images and large amounts of unlabelled images, which has witnessed impressive
progress with the recent advance of deep neural networks. However, it often
suffers from severe class-bias problem while exploring the unlabelled images,
largely due to the clear pixel-wise class imbalance in the labelled images.
This paper presents an unbiased subclass regularization network (USRN) that
alleviates the class imbalance issue by learning class-unbiased segmentation
from balanced subclass distributions. We build the balanced subclass
distributions by clustering pixels of each original class into multiple
subclasses of similar sizes, which provide class-balanced pseudo supervision to
regularize the class-biased segmentation. In addition, we design an
entropy-based gate mechanism to coordinate learning between the original
classes and the clustered subclasses which facilitates subclass regularization
effectively by suppressing unconfident subclass predictions. Extensive
experiments over multiple public benchmarks show that USRN achieves superior
performance as compared with the state-of-the-art.",0.0817177,-0.061095256,-0.046161942,A
3559,"We follow the command
line in [44] and use FFmpeg with default mode to implement                 We further study the black-box attack scenario, i.e., when
H.264 and H.265.",the performance of legacy methods.,"the attacker does not know the architecture or weights of the
                                                                       DNN used for video compression.",2022-03-18 22:42:20+00:00,RoVISQ: Reduction of Video Service Quality via Adversarial Attacks on Deep Learning-based Video Compression,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Jung-Woo Chang'), arxiv.Result.Author('Mojan Javaheripi'), arxiv.Result.Author('Seira Hidano'), arxiv.Result.Author('Farinaz Koushanfar')]","Video compression plays a crucial role in video streaming and classification
systems by maximizing the end-user quality of experience (QoE) at a given
bandwidth budget. In this paper, we conduct the first systematic study for
adversarial attacks on deep learning-based video compression and downstream
classification systems. Our attack framework, dubbed RoVISQ, manipulates the
Rate-Distortion ($\textit{R}$-$\textit{D}$) relationship of a video compression
model to achieve one or both of the following goals: (1) increasing the network
bandwidth, (2) degrading the video quality for end-users. We further devise new
objectives for targeted and untargeted attacks to a downstream video
classification service. Finally, we design an input-invariant perturbation that
universally disrupts video compression and classification systems in real time.
Unlike previously proposed attacks on video classification, our adversarial
perturbations are the first to withstand compression. We empirically show the
resilience of RoVISQ attacks against various defenses, i.e., adversarial
training, video denoising, and JPEG compression. Our extensive experimental
results on various video datasets show RoVISQ attacks deteriorate peak
signal-to-noise ratio by up to 5.6dB and the bit-rate by up to $\sim$
2.4$\times$ while achieving over 90$\%$ attack success rate on a downstream
classifier. Our user study further demonstrates the effect of RoVISQ attacks on
users' QoE.",0.019564603,-0.0036028153,0.23083726,C
3570,"Contact was estimated by Ô¨Ånding intersection
the test set, we used K-fold cross validation to further study     between the hand and sensor meshes.",Due to the small number of participants in      ground truth.,"IoUvol                  =                                             Video Baseline Motion cues can be useful for machine
                                                                   perception of human activities [52].",2022-03-19 19:54:56+00:00,PressureVision: Estimating Hand Pressure from a Single RGB Image,cs.CV,['cs.CV'],"[arxiv.Result.Author('Patrick Grady'), arxiv.Result.Author('Chengcheng Tang'), arxiv.Result.Author('Samarth Brahmbhatt'), arxiv.Result.Author('Christopher D. Twigg'), arxiv.Result.Author('Chengde Wan'), arxiv.Result.Author('James Hays'), arxiv.Result.Author('Charles C. Kemp')]","People often interact with their surroundings by applying pressure with their
hands. Machine perception of hand pressure has been limited by the challenges
of placing sensors between the hand and the contact surface. We explore the
possibility of using a conventional RGB camera to infer hand pressure. The
central insight is that the application of pressure by a hand results in
informative appearance changes. Hands share biomechanical properties that
result in similar observable phenomena, such as soft-tissue deformation, blood
distribution, hand pose, and cast shadows. We collected videos of 36
participants with diverse skin tone applying pressure to an instrumented planar
surface. We then trained a deep model (PressureVisionNet) to infer a pressure
image from a single RGB image. Our model infers pressure for participants
outside of the training data and outperforms baselines. We also show that the
output of our model depends on the appearance of the hand and cast shadows near
contact regions. Overall, our results suggest the appearance of a previously
unobserved human hand can be used to accurately infer applied pressure.",-0.09007113,0.23634711,-0.3071826,B
3572,We leave it to our further research.,It can be seen that the Cityscapes-trained model yields  process.,"Besides, the
fragmented segmentation that disqualiÔ¨Åes its application in     current experiments are conducted based on the referred
self-driving scenarios, as it poses great threats in potential  datasets, thus there are still data biases when the model is
accident scenes.",2022-03-19 21:18:54+00:00,Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning,cs.CV,"['cs.CV', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Xinyu Luo'), arxiv.Result.Author('Jiaming Zhang'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Alina Roitberg'), arxiv.Result.Author('Kunyu Peng'), arxiv.Result.Author('Rainer Stiefelhagen')]","Autonomous vehicles utilize urban scene segmentation to understand the real
world like a human and react accordingly. Semantic segmentation of normal
scenes has experienced a remarkable rise in accuracy on conventional
benchmarks. However, a significant portion of real-life accidents features
abnormal scenes, such as those with object deformations, overturns, and
unexpected traffic behaviors. Since even small mis-segmentation of driving
scenes can lead to serious threats to human lives, the robustness of such
models in accident scenarios is an extremely important factor in ensuring
safety of intelligent transportation systems.
  In this paper, we propose a Multi-source Meta-learning Unsupervised Domain
Adaptation (MMUDA) framework, to improve the generalization of segmentation
transformers to extreme accident scenes. In MMUDA, we make use of Multi-Domain
Mixed Sampling to augment the images of multiple-source domains (normal scenes)
with the target data appearances (abnormal scenes). To train our model, we
intertwine and study a meta-learning strategy in the multi-source setting for
robustifying the segmentation results. We further enhance the segmentation
backbone (SegFormer) with a HybridASPP decoder design, featuring large window
attention spatial pyramid pooling and strip pooling, to efficiently aggregate
long-range contextual dependencies. Our approach achieves a mIoU score of
46.97% on the DADA-seg benchmark, surpassing the previous state-of-the-art
model by more than 7.50%. Code will be made publicly available at
https://github.com/xinyu-laura/MMUDA.",-0.18876979,-0.03690734,-0.032786403,B
3578,"We further study if using patches
for clustering is better than using the whole image, and how
           polar tiger         zebra hamster               Seen      4.4.",Ablation over patches.,"Human Evaluation
rabbit bear                                               Unseen
                                                                        To evaluate if our VGSE conveys consistent visual and
                           antelope bobcat         rat               semantic properties, we randomly pick 50 clusters, each
                                                     ox deer         equipped with 30 images from the cluster center, and ask 5
   fox rat                                                           postgraduate students without prior knowledge of zero-shot
killer humpback                                                      learning to examine the clusters and answer the following
whale whale                                                          three questions.",2022-03-20 03:49:02+00:00,VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wenjia Xu'), arxiv.Result.Author('Yongqin Xian'), arxiv.Result.Author('Jiuniu Wang'), arxiv.Result.Author('Bernt Schiele'), arxiv.Result.Author('Zeynep Akata')]","Human-annotated attributes serve as powerful semantic embeddings in zero-shot
learning. However, their annotation process is labor-intensive and needs expert
supervision. Current unsupervised semantic embeddings, i.e., word embeddings,
enable knowledge transfer between classes. However, word embeddings do not
always reflect visual similarities and result in inferior zero-shot
performance. We propose to discover semantic embeddings containing
discriminative visual properties for zero-shot learning, without requiring any
human annotation. Our model visually divides a set of images from seen classes
into clusters of local image regions according to their visual similarity, and
further imposes their class discrimination and semantic relatedness. To
associate these clusters with previously unseen classes, we use external
knowledge, e.g., word embeddings and propose a novel class relation discovery
module. Through quantitative and qualitative evaluation, we demonstrate that
our model discovers semantic embeddings that model the visual properties of
both seen and unseen classes. Furthermore, we demonstrate on three benchmarks
that our visually-grounded semantic embeddings further improve performance over
word embeddings across various ZSL models by a large margin.",-0.10461685,-0.053321168,-0.12777385,C
3580,"Consequently, proposing new methods to effectively make use of spatio-
temporal information in video is worth further study.","Current methods, like 3D convolution and non-local
modules are computationally inefÔ¨Åcient, and the quality of optical Ô¨Çow cannot be
guaranteed.","6.2.4 More Reasonable Evaluation Metrics

Evaluation metric is the most fundamental component in each computer vision task,
which greatly inÔ¨Çuences the task‚Äôs progress.",2022-03-20 06:04:56+00:00,Optical Flow for Video Super-Resolution: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhigang Tu'), arxiv.Result.Author('Hongyan Li'), arxiv.Result.Author('Wei Xie'), arxiv.Result.Author('Yuanzhong Liu'), arxiv.Result.Author('Shifu Zhang'), arxiv.Result.Author('Baoxin Li'), arxiv.Result.Author('Junsong Yuan')]","Video super-resolution is currently one of the most active research topics in
computer vision as it plays an important role in many visual applications.
Generally, video super-resolution contains a significant component, i.e.,
motion compensation, which is used to estimate the displacement between
successive video frames for temporal alignment. Optical flow, which can supply
dense and sub-pixel motion between consecutive frames, is among the most common
ways for this task. To obtain a good understanding of the effect that optical
flow acts in video super-resolution, in this work, we conduct a comprehensive
review on this subject for the first time. This investigation covers the
following major topics: the function of super-resolution (i.e., why we require
super-resolution); the concept of video super-resolution (i.e., what is video
super-resolution); the description of evaluation metrics (i.e., how (video)
superresolution performs); the introduction of optical flow based video
super-resolution; the investigation of using optical flow to capture temporal
dependency for video super-resolution. Prominently, we give an in-depth study
of the deep learning based video super-resolution method, where some
representative algorithms are analyzed and compared. Additionally, we highlight
some promising research directions and open issues that should be further
addressed.",-0.25130168,0.27702212,-0.0446137,B
3589,"Here the
   We further study the performance under a semi-
supervision manner.","Semi-Supervision Evaluation                                             that the terms ‚Äústyle‚Äù and ‚Äúcontent‚Äù are somewhat differ-
                                                                             ent from those of font style transfer tasks [56].","Since it can make the best use of abun-
dant unlabeled data, it has important practical significance.",2022-03-20 08:43:10+00:00,SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Canjie Luo'), arxiv.Result.Author('Lianwen Jin'), arxiv.Result.Author('Jingdong Chen')]","Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications.",0.1822952,-0.28452677,-0.17399949,C
3590,"Here the
   We further study the performance under a semi-
supervision manner.","Semi-Supervision Evaluation                                             that the terms ‚Äústyle‚Äù and ‚Äúcontent‚Äù are somewhat differ-
                                                                             ent from those of font style transfer tasks [56].","Since it can make the best use of abun-
dant unlabeled data, it has important practical significance.",2022-03-20 08:43:10+00:00,SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Canjie Luo'), arxiv.Result.Author('Lianwen Jin'), arxiv.Result.Author('Jingdong Chen')]","Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications.",0.1822952,-0.28452677,-0.17399949,C
3591,"The dataset will be          of PatchMatch Stereo [2], and build a thin cost volume to
open for further research.","[8] adopt the idea
fectiveness of the proposed method.",speed up the prediction process.,2022-03-20 08:46:37+00:00,Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuhua Xu'), arxiv.Result.Author('Xiaoli Yang'), arxiv.Result.Author('Yushan Yu'), arxiv.Result.Author('Wei Jia'), arxiv.Result.Author('Zhaobi Chu'), arxiv.Result.Author('Yulan Guo')]","It is well known that the passive stereo system cannot adapt well to weak
texture objects, e.g., white walls. However, these weak texture targets are
very common in indoor environments. In this paper, we present a novel stereo
system, which consists of two cameras (an RGB camera and an IR camera) and an
IR speckle projector. The RGB camera is used both for depth estimation and
texture acquisition. The IR camera and the speckle projector can form a
monocular structured-light (MSL) subsystem, while the two cameras can form a
binocular stereo subsystem. The depth map generated by the MSL subsystem can
provide external guidance for the stereo matching networks, which can improve
the matching accuracy significantly. In order to verify the effectiveness of
the proposed system, we build a prototype and collect a test dataset in indoor
scenes. The evaluation results show that the Bad 2.0 error of the proposed
system is 28.2% of the passive stereo system when the network RAFT is used. The
dataset and trained models are available at
https://github.com/YuhuaXu/MonoStereoFusion.",-0.014665071,0.24602869,0.09073786,B
3592,"Although the Encoder-Decoder archi-
tecture has achieved satisfying performance, further research is needed for more
Ô¨Çexible and lightweight approaches.",1(c).,Fig.,2022-03-20 12:51:14+00:00,Document Dewarping with Control Points,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guo-Wang Xie'), arxiv.Result.Author('Fei Yin'), arxiv.Result.Author('Xu-Yao Zhang'), arxiv.Result.Author('Cheng-Lin Liu')]","Document images are now widely captured by handheld devices such as mobile
phones. The OCR performance on these images are largely affected due to
geometric distortion of the document paper, diverse camera positions and
complex backgrounds. In this paper, we propose a simple yet effective approach
to rectify distorted document image by estimating control points and reference
points. After that, we use interpolation method between control points and
reference points to convert sparse mappings to backward mapping, and remap the
original distorted document image to the rectified image. Furthermore, control
points are controllable to facilitate interaction or subsequent adjustment. We
can flexibly select post-processing methods and the number of vertices
according to different application scenarios. Experiments show that our
approach can rectify document images with various distortion types, and yield
state-of-the-art performance on real-world dataset. This paper also provides a
training dataset based on control points for document dewarping. Both the code
and the dataset are released at
https://github.com/gwxie/Document-Dewarping-with-Control-Points.",0.108755454,0.140899,0.16798954,A
3593,"Although our approach has better tradeoÔ¨Ä between computational com-
plexity and rectiÔ¨Åcation performance, further research is needed to explore more
lightweight and eÔ¨Écient models.","The control
points can also be used as a preprocessing step to realize semi-automatic dis-
torted document image annotation with some further control point editing by
humans.","Acknowledgements

This work has been supported by the National Key Research and Development
Program Grant 2020AAA0109702, the National Natural Science Foundation of
China (NSFC) grants 61733007, 61721004.",2022-03-20 12:51:14+00:00,Document Dewarping with Control Points,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guo-Wang Xie'), arxiv.Result.Author('Fei Yin'), arxiv.Result.Author('Xu-Yao Zhang'), arxiv.Result.Author('Cheng-Lin Liu')]","Document images are now widely captured by handheld devices such as mobile
phones. The OCR performance on these images are largely affected due to
geometric distortion of the document paper, diverse camera positions and
complex backgrounds. In this paper, we propose a simple yet effective approach
to rectify distorted document image by estimating control points and reference
points. After that, we use interpolation method between control points and
reference points to convert sparse mappings to backward mapping, and remap the
original distorted document image to the rectified image. Furthermore, control
points are controllable to facilitate interaction or subsequent adjustment. We
can flexibly select post-processing methods and the number of vertices
according to different application scenarios. Experiments show that our
approach can rectify document images with various distortion types, and yield
state-of-the-art performance on real-world dataset. This paper also provides a
training dataset based on control points for document dewarping. Both the code
and the dataset are released at
https://github.com/gwxie/Document-Dewarping-with-Control-Points.",-0.035166495,0.008518834,-0.1177122,C
3601,"A current limitation of our
work is lack of understanding with respect to classes that have not been seen
during training (open-set or heavily under-represented), however, we believe that
our work can set a precedent for further research in the area.","Our approach achieves comparable performance to full supervised data with a
significant reduction in the amount of labels used.","Semantic Segmentation with Active Semi-Supervised Learning  15

Acknowledgements

This work was supported by the Dynamic Data Driven Applications Systems
Program, Air Force Office of Scientific Research under Grant FA9550-19-1-0021.",2022-03-21 04:16:25+00:00,Semantic Segmentation with Active Semi-Supervised Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Aneesh Rangnekar'), arxiv.Result.Author('Christopher Kanan'), arxiv.Result.Author('Matthew Hoffman')]","Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 19% of the training data,
whereas the previous state of the art required 40% of the training data.",-0.18812415,-0.19566175,-0.17047642,B
3602,"We believe the higher variance here as
opposed to our other results is caused by using only 3.5% data initially labeled
data (vs. 10% in the other experiments), and further research could help reduce
this variance with improvements in SSL [2,47,17,51].","Compared to EquAL‚Äôs 12% usage with an mIoU of 67.4, we obtained 66.7 ¬± 1.5
using only 10% of the total labeled data.","S3 Visual Results

Fig.",2022-03-21 04:16:25+00:00,Semantic Segmentation with Active Semi-Supervised Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Aneesh Rangnekar'), arxiv.Result.Author('Christopher Kanan'), arxiv.Result.Author('Matthew Hoffman')]","Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 19% of the training data,
whereas the previous state of the art required 40% of the training data.",0.29823723,0.01498496,-0.06436995,A
3603,"A current limitation of our
work is lack of understanding with respect to classes that have not been seen
during training (open-set or heavily under-represented), however, we believe that
our work can set a precedent for further research in the area.","Our approach achieves comparable performance to full supervised data with a
significant reduction in the amount of labels used.","Semantic Segmentation with Active Semi-Supervised Learning  15

Acknowledgements

This work was supported by the Dynamic Data Driven Applications Systems
Program, Air Force Office of Scientific Research under Grant FA9550-19-1-0021.",2022-03-21 04:16:25+00:00,Semantic Segmentation with Active Semi-Supervised Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Aneesh Rangnekar'), arxiv.Result.Author('Christopher Kanan'), arxiv.Result.Author('Matthew Hoffman')]","Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 17% of the training data,
whereas the previous state of the art required 40% of the training data.",-0.18812415,-0.19566175,-0.17047642,B
3604,"We believe the higher variance here as
opposed to our other results is caused by using only 3.5% data initially labeled
data (vs. 10% in the other experiments), and further research could help reduce
this variance with improvements in SSL [2,47,17,51].","Compared to EquAL‚Äôs 12% usage with an mIoU of 67.4, we obtained 66.7 ¬± 1.5
using only 10% of the total labeled data.","S3 Visual Results

Fig.",2022-03-21 04:16:25+00:00,Semantic Segmentation with Active Semi-Supervised Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Aneesh Rangnekar'), arxiv.Result.Author('Christopher Kanan'), arxiv.Result.Author('Matthew Hoffman')]","Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 17% of the training data,
whereas the previous state of the art required 40% of the training data.",0.29823723,0.01498496,-0.06436995,A
3605,"We          under-represented), however, we believe that our work can
believe stronger regularizers can help prevent pseudo label       set a precedent for further research in the area.","A current limitation
                                                                  of our work is lack of understanding with respect to classes
call across tail distribution classes except for the ‚ÄòTrain‚Äô      that have not been seen during training (open-set or heavily
class for CityScapes, which gets confused with ‚ÄòBus‚Äô.",confusion and mitigate this performance gap.,2022-03-21 04:16:25+00:00,Semantic Segmentation with Active Semi-Supervised Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Aneesh Rangnekar'), arxiv.Result.Author('Christopher Kanan'), arxiv.Result.Author('Matthew Hoffman')]","Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 17% of the training data,
whereas the previous state of the art required 40% of the training data.",0.08009131,-0.21736532,-0.06641562,A
3630,"contrastive learning on pyramid features but only one fea-    ii) Loss weight of PixSim: We further study the loss weight
ture scale is used when Ô¨Åne-tuning Faster R-CNN on PAS-       Œª1 of Ldense used for per-pixel similarity learning.","The hypothesis also explains        that PixSim only brings 0.1% ‚àº 0.3% extra memory cost in
the inferior performance of DetCo [49]: DetCo conducts        comparison with SimSiam.","The
CAL VOC dataset.",2022-03-21 15:55:23+00:00,Dense Siamese Network,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Wenwei Zhang'), arxiv.Result.Author('Jiangmiao Pang'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Chen Change Loy')]","This paper presents Dense Siamese Network (DenseSiam), a simple unsupervised
learning framework for dense prediction tasks. It learns visual representations
by maximizing the similarity between two views of one image with two types of
consistency, i.e., pixel consistency and region consistency. Concretely,
DenseSiam first maximizes the pixel level spatial consistency according to the
exact location correspondence in the overlapped area. It also extracts a batch
of region embeddings that correspond to some sub-regions in the overlapped area
to be contrasted for region consistency. In contrast to previous methods that
require negative pixel pairs, momentum encoders, or heuristic masks, DenseSiam
benefits from the simple Siamese network and optimizes the consistency of
different granularities. It also proves that the simple location correspondence
and interacted region embeddings are effective enough to learn the similarity.
We apply DenseSiam on ImageNet and obtain competitive improvements on various
downstream tasks. We also show that only with some extra task-specific losses,
the simple framework can directly conduct dense prediction tasks. On an
existing unsupervised semantic segmentation benchmark, it surpasses
state-of-the-art segmentation methods by 2.1 mIoU with 28% training costs.",0.004808861,-0.18232876,0.24799553,C
3631,"54.9 80.8 60.9                                            w/o PixSim 52.8 79.2 58.0

ii) Loss weight of PixSim: We further study the loss weight Œª1 of Ldense used
for per-pixel similarity learning.",+ grid.,"The comparative results in Table 4b show that
with Œª1 = 1 we achieve the best performance, which is equal to the loss weight
of the image-level similarity learning.",2022-03-21 15:55:23+00:00,Dense Siamese Network for Dense Unsupervised Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Wenwei Zhang'), arxiv.Result.Author('Jiangmiao Pang'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Chen Change Loy')]","This paper presents Dense Siamese Network (DenseSiam), a simple unsupervised
learning framework for dense prediction tasks. It learns visual representations
by maximizing the similarity between two views of one image with two types of
consistency, i.e., pixel consistency and region consistency. Concretely,
DenseSiam first maximizes the pixel level spatial consistency according to the
exact location correspondence in the overlapped area. It also extracts a batch
of region embeddings that correspond to some sub-regions in the overlapped area
to be contrasted for region consistency. In contrast to previous methods that
require negative pixel pairs, momentum encoders or heuristic masks, DenseSiam
benefits from the simple Siamese network and optimizes the consistency of
different granularities. It also proves that the simple location correspondence
and interacted region embeddings are effective enough to learn the similarity.
We apply DenseSiam on ImageNet and obtain competitive improvements on various
downstream tasks. We also show that only with some extra task-specific losses,
the simple framework can directly conduct dense prediction tasks. On an
existing unsupervised semantic segmentation benchmark, it surpasses
state-of-the-art segmentation methods by 2.1 mIoU with 28% training costs. Code
and models are released at https://github.com/ZwwWayne/DenseSiam.",0.05873989,-0.026501399,0.18470156,C
3645,"We further study the              lowest class awareness only causes negligible changes, and
contribution of the latent code, z, to each channel.","and structure of castle), while masking channels with the
Latent-oriented Channel Awareness.","Similarly,      randomly selected channels have the in-between effect.",2022-03-21 17:53:22+00:00,Interpreting Class Conditional GANs with Channel Awareness,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yingqing He'), arxiv.Result.Author('Zhiyi Zhang'), arxiv.Result.Author('Jiapeng Zhu'), arxiv.Result.Author('Yujun Shen'), arxiv.Result.Author('Qifeng Chen')]","Understanding the mechanism of generative adversarial networks (GANs) helps
us better use GANs for downstream applications. Existing efforts mainly target
interpreting unconditional models, leaving it less explored how a conditional
GAN learns to render images regarding various categories. This work fills in
this gap by investigating how a class conditional generator unifies the
synthesis of multiple classes. For this purpose, we dive into the widely used
class-conditional batch normalization (CCBN), and observe that each feature
channel is activated at varying degrees given different categorical embeddings.
To describe such a phenomenon, we propose channel awareness, which
quantitatively characterizes how a single channel contributes to the final
synthesis. Extensive evaluations and analyses on the BigGAN model pre-trained
on ImageNet reveal that only a subset of channels is primarily responsible for
the generation of a particular category, similar categories (e.g., cat and dog)
usually get related to some same channels, and some channels turn out to share
information across all classes. For good measure, our algorithm enables several
novel applications with conditional GANs. Concretely, we achieve (1) versatile
image editing via simply altering a single channel and manage to (2)
harmoniously hybridize two different classes. We further verify that the
proposed channel awareness shows promising potential in (3) segmenting the
synthesized image and (4) evaluating the category-wise synthesis performance.",0.2989272,-0.0069680493,-0.051531002,A
3658,"Section 6 concludes
this paper and discusses some interesting research topics for further research.","Numerical experiments and some performance comparisons are demonstrated in
Section 5 to show the superiority of the propose algorithm.",2.,2022-03-22 00:05:19+00:00,Multiple Convex Objects Image Segmentation via Proximal Alternating Direction Method of Multipliers,cs.CV,"['cs.CV', 'math.OC']","[arxiv.Result.Author('Shousheng Luo'), arxiv.Result.Author('Jinfeng Chen'), arxiv.Result.Author('Yunhai Xiao'), arxiv.Result.Author('Xue-Cheng Tai')]","This paper focuses on the issue of image segmentation with convex shape
prior. Firstly, we use binary function to represent convex object(s). The
convex shape prior turns out to be a simple quadratic inequality constraint on
the binary indicator function associated with each object. An image
segmentation model incorporating convex shape prior into a probability-based
method is proposed. Secondly, a new algorithm is designed to solve involved
optimization problem, which is a challenging task because of the quadratic
inequality constraint. To tackle this difficulty, we relax and linearize the
quadratic inequality constraint to reduce it to solve a sequence of convex
minimization problems. For each convex problem, an efficient proximal
alternating direction method of multipliers is developed to solve it. The
convergence of the algorithm follows some existing results in the optimization
literature. Moreover, an interactive procedure is introduced to improve the
accuracy of segmentation gradually. Numerical experiments on natural and
medical images demonstrate that the proposed method is superior to some
existing methods in terms of segmentation accuracy and computational time.",0.3364908,0.2957468,0.030317212,A
3659,Section 6 concludes this paper and discusses some interesting topics for further research.,Numerical experiments and some performance comparisons are demonstrated in Section 5.,"3
2.",2022-03-22 00:05:19+00:00,A Binary Characterization Method for Shape Convexity and Applications,cs.CV,"['cs.CV', 'math.OC']","[arxiv.Result.Author('Shousheng Luo'), arxiv.Result.Author('Jinfeng Chen'), arxiv.Result.Author('Yunhai Xiao'), arxiv.Result.Author('Xue-Cheng Tai')]","Convexity prior is one of the main cue for human vision and shape completion
with important applications in image processing, computer vision. This paper
focuses on characterization methods for convex objects and applications in
image processing. We present a new method for convex objects representations
using binary functions, that is, the convexity of a region is equivalent to a
simple quadratic inequality constraint on its indicator function. Models are
proposed firstly by incorporating this result for image segmentation with
convexity prior and convex hull computation of a given set with and without
noises. Then, these models are summarized to a general optimization problem on
binary function(s) with the quadratic inequality. Numerical algorithm is
proposed based on linearization technique, where the linearized problem is
solved by a proximal alternating direction method of multipliers with
guaranteed convergent. Numerical experiments demonstrate the efficiency and
effectiveness of the proposed methods for image segmentation and convex hull
computation in accuracy and computing time.",0.43893468,0.27570793,0.10113968,A
3671,"Ablation Studies
                                                                 RIE                                                                           RIE
                                                                 Videopose                 50.0                                                Videopose           In this experiment, we further study the effectiveness of
                                                                                           47.5                                                Ray3D            intrinsic decoupling, camera coordinate normalization and
       55 Ray3D                                                                            45.0                                                Ray3D_w/o_CE     camera embedding using cross-dataset setting.","Extrinsic generalization To evaluate the impact of the

                                                                                                                       7
MPJPE  60 (a) PoseFormer                                                    MPJPE          (b) 52.5                                            PoseFormer       4.5.","We use RIE
       50 Ray3D_w/o_CE                                                                     42.5                                                                 network as baseline model and add our modules gradually.",2022-03-22 05:42:31+00:00,Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Zhan'), arxiv.Result.Author('Fenghai Li'), arxiv.Result.Author('Renliang Weng'), arxiv.Result.Author('Wongun Choi')]","In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .",-0.0073886355,0.2595139,0.11836426,B
3672,"Ablation Studies
                                                                 RIE                                                                           RIE
                                                                 Videopose                 50.0                                                Videopose           In this experiment, we further study the effectiveness of
                                                                                           47.5                                                Ray3D            intrinsic decoupling, camera coordinate normalization and
       55 Ray3D                                                                            45.0                                                Ray3D_w/o_CE     camera embedding using cross-dataset setting.","Extrinsic generalization To evaluate the impact of the

                                                                                                                       7
MPJPE  60 (a) PoseFormer                                                    MPJPE          (b) 52.5                                            PoseFormer       4.5.","We use RIE
       50 Ray3D_w/o_CE                                                                     42.5                                                                 network as baseline model and add our modules gradually.",2022-03-22 05:42:31+00:00,Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Zhan'), arxiv.Result.Author('Fenghai Li'), arxiv.Result.Author('Renliang Weng'), arxiv.Result.Author('Wongun Choi')]","In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .",-0.0073886355,0.2595139,0.11836426,B
3673,"Ablation Studies
                                                                 RIE                                                                           RIE
                                                                 Videopose                 50.0                                                Videopose           In this experiment, we further study the effectiveness of
                                                                                           47.5                                                Ray3D            intrinsic decoupling, camera coordinate normalization and
       55 Ray3D                                                                            45.0                                                Ray3D_w/o_CE     camera embedding using cross-dataset setting.","Extrinsic generalization To evaluate the impact of the

                                                                                                                       7
MPJPE  60 (a) PoseFormer                                                    MPJPE          (b) 52.5                                            PoseFormer       4.5.","We use RIE
       50 Ray3D_w/o_CE                                                                     42.5                                                                 network as baseline model and add our modules gradually.",2022-03-22 05:42:31+00:00,Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Zhan'), arxiv.Result.Author('Fenghai Li'), arxiv.Result.Author('Renliang Weng'), arxiv.Result.Author('Wongun Choi')]","In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .",-0.0073886355,0.2595139,0.11836426,B
3674,"We hope that further research in this
amount of epochs T .","3 we expect that the best accuracy will be ob-      cies than fully private training without compromising the
tained using a large œÉ and training for a respectively larger     privacy of the data.","In the Appendix, we plot the test error      Ô¨Åeld may help the adoption of private models in more areas
as a function of œÉ and we see that indeed, a larger œÉ give        of computer vision.",2022-03-22 06:15:43+00:00,Mixed Differential Privacy in Computer Vision,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Aditya Golatkar'), arxiv.Result.Author('Alessandro Achille'), arxiv.Result.Author('Yu-Xiang Wang'), arxiv.Result.Author('Aaron Roth'), arxiv.Result.Author('Michael Kearns'), arxiv.Result.Author('Stefano Soatto')]","We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.",0.09384102,-0.08718016,0.061314784,A
3675,"We hope that further research in this
amount of epochs T .","3 we expect that the best accuracy will be ob-        cies than fully private training without compromising the
tained using a large œÉ and training for a respectively larger       privacy of the data.","In the Appendix, we plot the test error        Ô¨Åeld may help the adoption of private models in more areas
as a function of œÉ and we see that indeed, a larger œÉ give          of computer vision.",2022-03-22 06:15:43+00:00,Mixed Differential Privacy in Computer Vision,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Aditya Golatkar'), arxiv.Result.Author('Alessandro Achille'), arxiv.Result.Author('Yu-Xiang Wang'), arxiv.Result.Author('Aaron Roth'), arxiv.Result.Author('Michael Kearns'), arxiv.Result.Author('Stefano Soatto')]","We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.",0.09384102,-0.08718016,0.061314784,A
3678,"We further study the
both the image and rain layers.","Third, we employ the discriminator as the feature encoder for  The Strategies of Location Contrast.","The discriminator encoder      strategies of location contrast in Table 5, which shows the
has achieved the best result, which veriÔ¨Åes the discriminator  improvement from reverse non-local sampling.",2022-03-22 07:37:08+00:00,Unsupervised Deraining: Where Contrastive Learning Meets Self-similarity,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuntong Ye'), arxiv.Result.Author('Changfeng Yu'), arxiv.Result.Author('Yi Chang'), arxiv.Result.Author('Lin Zhu'), arxiv.Result.Author('Xile Zhao'), arxiv.Result.Author('Luxin Yan'), arxiv.Result.Author('Yonghong Tian')]","Image deraining is a typical low-level image restoration task, which aims at
decomposing the rainy image into two distinguishable layers: the clean image
layer and the rain layer. Most of the existing learning-based deraining methods
are supervisedly trained on synthetic rainy-clean pairs. The domain gap between
the synthetic and real rains makes them less generalized to different real
rainy scenes. Moreover, the existing methods mainly utilize the property of the
two layers independently, while few of them have considered the mutually
exclusive relationship between the two layers. In this work, we propose a novel
non-local contrastive learning (NLCL) method for unsupervised image deraining.
Consequently, we not only utilize the intrinsic self-similarity property within
samples but also the mutually exclusive property between the two layers, so as
to better differ the rain layer from the clean image. Specifically, the
non-local self-similarity image layer patches as the positives are pulled
together and similar rain layer patches as the negatives are pushed away. Thus
the similar positive/negative samples that are close in the original space
benefit us to enrich more discriminative representation. Apart from the
self-similarity sampling strategy, we analyze how to choose an appropriate
feature encoder in NLCL. Extensive experiments on different real rainy datasets
demonstrate that the proposed method obtains state-of-the-art performance in
real deraining.",-0.18089941,0.14670411,0.09789024,B
3707,"Inspired by these observations, we further study the eÔ¨Äect of backbones and
pre-training datasets on domain transfer evaluation benchmarks.","In addition to network architectures, it
is shown that larger pre-training datasets such as ImageNet-22K, JFT-300M,
or image-text pairs can further improve the transferability [11,24,32,35,42,66].",Self-supervised learning.,2022-03-22 15:38:36+00:00,A Broad Study of Pre-training for Domain Generalization and Adaptation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Donghyun Kim'), arxiv.Result.Author('Kaihong Wang'), arxiv.Result.Author('Stan Sclaroff'), arxiv.Result.Author('Kate Saenko')]","Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.",-0.07187505,-0.36582035,0.166231,C
3708,"Inspired by these observations, we further study the eÔ¨Äect of backbones and
pre-training datasets on domain transfer evaluation benchmarks.","In addition to network architectures, it
is shown that larger pre-training datasets such as ImageNet-22K, JFT-300M,
or image-text pairs can further improve the transferability [11,24,32,35,42,66].",Self-supervised learning.,2022-03-22 15:38:36+00:00,A Broad Study of Pre-training for Domain Generalization and Adaptation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Donghyun Kim'), arxiv.Result.Author('Kaihong Wang'), arxiv.Result.Author('Stan Sclaroff'), arxiv.Result.Author('Kate Saenko')]","Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.",-0.07187505,-0.36582035,0.166231,C
3709,"Inspired by these observations, we further study the eÔ¨Äect of backbones and
pre-training datasets on domain transfer evaluation benchmarks.","In addition to network architectures, it
is shown that larger pre-training datasets such as ImageNet-22K, JFT-300M,
or image-text pairs can further improve the transferability [11,24,32,35,42,66].",Self-supervised learning.,2022-03-22 15:38:36+00:00,A Broad Study of Pre-training for Domain Generalization and Adaptation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Donghyun Kim'), arxiv.Result.Author('Kaihong Wang'), arxiv.Result.Author('Stan Sclaroff'), arxiv.Result.Author('Kate Saenko')]","Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.",-0.07187505,-0.36582035,0.166231,C
3713,"We will address these shortcomings in
further research.","Our method is not robust
to the unrelated out-of-distribution text queries.","B Discussion of Object Proposals

In previous work ViLD [13], class-agnostic object proposals are leveraged to transfer the
knowledge from CLIP image encoder to the detector.",2022-03-22 16:54:52+00:00,Open-Vocabulary DETR with Conditional Matching,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuhang Zang'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Kaiyang Zhou'), arxiv.Result.Author('Chen Huang'), arxiv.Result.Author('Chen Change Loy')]","Open-vocabulary object detection, which is concerned with the problem of
detecting novel objects guided by natural language, has gained increasing
attention from the community. Ideally, we would like to extend an
open-vocabulary detector such that it can produce bounding box predictions
based on user inputs in form of either natural language or exemplar image. This
offers great flexibility and user experience for human-computer interaction. To
this end, we propose a novel open-vocabulary detector based on DETR -- hence
the name OV-DETR -- which, once trained, can detect any object given its class
name or an exemplar image. The biggest challenge of turning DETR into an
open-vocabulary detector is that it is impossible to calculate the
classification cost matrix of novel classes without access to their labeled
images. To overcome this challenge, we formulate the learning objective as a
binary matching one between input queries (class name or exemplar image) and
the corresponding objects, which learns useful correspondence to generalize to
unseen queries during testing. For training, we choose to condition the
Transformer decoder on the input embeddings obtained from a pre-trained
vision-language model like CLIP, in order to enable matching for both text and
image queries. With extensive experiments on LVIS and COCO datasets, we
demonstrate that our OV-DETR -- the first end-to-end Transformer-based
open-vocabulary detector -- achieves non-trivial improvements over current
state of the arts.",-0.30306798,-0.16443728,-0.066868216,C
3714,"We will
address these shortcomings in further research.","Our
method is not robust to the unrelated out-of-distribution text queries.","B Discussion of Object Proposals

In previous work ViLD [13], class-agnostic object proposals are leveraged to
transfer the knowledge from CLIP image encoder to the detector.",2022-03-22 16:54:52+00:00,Open-Vocabulary DETR with Conditional Matching,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuhang Zang'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Kaiyang Zhou'), arxiv.Result.Author('Chen Huang'), arxiv.Result.Author('Chen Change Loy')]","Open-vocabulary object detection, which is concerned with the problem of
detecting novel objects guided by natural language, has gained increasing
attention from the community. Ideally, we would like to extend an
open-vocabulary detector such that it can produce bounding box predictions
based on user inputs in form of either natural language or exemplar image. This
offers great flexibility and user experience for human-computer interaction. To
this end, we propose a novel open-vocabulary detector based on DETR -- hence
the name OV-DETR -- which, once trained, can detect any object given its class
name or an exemplar image. The biggest challenge of turning DETR into an
open-vocabulary detector is that it is impossible to calculate the
classification cost matrix of novel classes without access to their labeled
images. To overcome this challenge, we formulate the learning objective as a
binary matching one between input queries (class name or exemplar image) and
the corresponding objects, which learns useful correspondence to generalize to
unseen queries during testing. For training, we choose to condition the
Transformer decoder on the input embeddings obtained from a pre-trained
vision-language model like CLIP, in order to enable matching for both text and
image queries. With extensive experiments on LVIS and COCO datasets, we
demonstrate that our OV-DETR -- the first end-to-end Transformer-based
open-vocabulary detector -- achieves non-trivial improvements over current
state of the arts.",-0.30306798,-0.16443728,-0.066868216,C
3715,"Their success has motivated further research, focused on
two speciÔ¨Åc fronts [1]:
EÔ¨Éciency.","Originally focused on NLP applications, Transformers
have been successfully deployed in several vision-based tasks, such as image classiÔ¨Åcation,

                                                          3
detection and segmentation [1].","Extend the length of the sequences they can process, circumventing the complexity
of the self-attention module.",2022-03-22 16:56:05+00:00,Under the Hood of Transformer Networks for Trajectory Forecasting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Luca Franco'), arxiv.Result.Author('Leonardo Placidi'), arxiv.Result.Author('Francesco Giuliari'), arxiv.Result.Author('Irtiza Hasan'), arxiv.Result.Author('Marco Cristani'), arxiv.Result.Author('Fabio Galasso')]","Transformer Networks have established themselves as the de-facto
state-of-the-art for trajectory forecasting but there is currently no
systematic study on their capability to model the motion patterns of people,
without interactions with other individuals nor the social context. This paper
proposes the first in-depth study of Transformer Networks (TF) and
Bidirectional Transformers (BERT) for the forecasting of the individual motion
of people, without bells and whistles. We conduct an exhaustive evaluation of
input/output representations, problem formulations and sequence modeling,
including a novel analysis of their capability to predict multi-modal futures.
Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are
top performers in predicting individual motions, definitely overcoming RNNs and
LSTMs. Furthermore, they remain within a narrow margin wrt more complex
techniques, which include both social interactions and scene contexts. Source
code will be released for all conducted experiments.",-0.27214646,-0.071473114,-0.13803938,C
3728,"We also
conduct ablation experiments to further study the proposed
methods.","histopathology WSI, and qualitatively validate the sound-
ness of the derivation of instance probability.","More experimental results are presented in the
supplementary.",2022-03-22 22:33:42+00:00,DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Hongrun Zhang'), arxiv.Result.Author('Yanda Meng'), arxiv.Result.Author('Yitian Zhao'), arxiv.Result.Author('Yihong Qiao'), arxiv.Result.Author('Xiaoyun Yang'), arxiv.Result.Author('Sarah E. Coupland'), arxiv.Result.Author('Yalin Zheng')]","Multiple instance learning (MIL) has been increasingly used in the
classification of histopathology whole slide images (WSIs). However, MIL
approaches for this specific classification problem still face unique
challenges, particularly those related to small sample cohorts. In these, there
are limited number of WSI slides (bags), while the resolution of a single WSI
is huge, which leads to a large number of patches (instances) cropped from this
slide. To address this issue, we propose to virtually enlarge the number of
bags by introducing the concept of pseudo-bags, on which a double-tier MIL
framework is built to effectively use the intrinsic features. Besides, we also
contribute to deriving the instance probability under the framework of
attention-based MIL, and utilize the derivation to help construct and analyze
the proposed framework. The proposed method outperforms other latest methods on
the CAMELYON-16 by substantially large margins, and is also better in
performance on the TCGA lung cancer dataset. The proposed framework is ready to
be extended for wider MIL applications. The code is available at:
https://github.com/hrzhang1123/DTFD-MIL",0.41147107,0.06912431,-0.14213818,A
3729,"However, further study needs to be done on whether this database can produce statistically
significant results.","This implies a total number of 40√ó15 genuine tests
             plus 40√ó20 impostor tests (skilled) and 40√ó39√ó5 impostor tests (random).","In (Guyon et al., 1998), the minimum size of the test data set N, which
guarantees statistical significance in a pattern recognition task, is derived.",2022-03-23 00:04:27+00:00,Fast on-line signature recognition based on VQ with time modeling,cs.CV,"['cs.CV', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Juan-Manuel Pascual-Gaspar'), arxiv.Result.Author('Marcos Faundez-Zanuy'), arxiv.Result.Author('Carlos Vivaracho')]","This paper proposes a multi-section vector quantization approach for on-line
signature recognition. We have used the MCYT database, which consists of 330
users and 25 skilled forgeries per person performed by 5 different impostors.
This database is larger than those typically used in the literature.
Nevertheless, we also provide results from the SVC database.
  Our proposed system outperforms the winner of SVC with a reduced
computational requirement, which is around 47 times lower than DTW. In
addition, our system improves the database storage requirements due to vector
compression, and is more privacy-friendly as it is not possible to recover the
original signature using the codebooks. Experimental results with MCYT provide
a 99.76% identification rate and 2.46% EER (skilled forgeries and individual
threshold). Experimental results with SVC are 100% of identification rate and
0% (individual threshold) and 0.31% (general threshold) when using a
two-section VQ approach.",0.23996924,-0.07906115,-0.18946494,A
3756,"However, the SVM
accuracy needs further research to confirm.","We also consider studying further SVM
accuracy compared to RF because SVM showed higher efficiency than RF.","We also consider fine-tuning the RF parameters to
achieve higher efficiency at utmost accuracy, 1.00.",2022-03-23 15:05:23+00:00,3D Adapted Random Forest Vision (3DARFV) for Untangling Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency at the Utmost Accuracy,cs.CV,"['cs.CV', 'astro-ph.IM', 'cs.LG', 'eess.IV', 'physics.geo-ph']","[arxiv.Result.Author('Omar Alfarisi'), arxiv.Result.Author('Zeyar Aung'), arxiv.Result.Author('Qingfeng Huang'), arxiv.Result.Author('Ashraf Al-Khateeb'), arxiv.Result.Author('Hamed Alhashmi'), arxiv.Result.Author('Mohamed Abdelsalam'), arxiv.Result.Author('Salem Alzaabi'), arxiv.Result.Author('Haifa Alyazeedi'), arxiv.Result.Author('Anthony Tzes')]","Planetary exploration depends heavily on 3D image data to characterize the
static and dynamic properties of the rock and environment. Analyzing 3D images
requires many computations, causing efficiency to suffer lengthy processing
time alongside large energy consumption. High-Performance Computing (HPC)
provides apparent efficiency at the expense of energy consumption. However, for
remote explorations, the conveyed surveillance and the robotized sensing need
faster data analysis with ultimate accuracy to make real-time decisions. In
such environments, access to HPC and energy is limited. Therefore, we realize
that reducing the number of computations to optimal and maintaining the desired
accuracy leads to higher efficiency. This paper demonstrates the semantic
segmentation capability of a probabilistic decision tree algorithm, 3D Adapted
Random Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at
the utmost accuracy.",0.26552764,-0.0026786495,0.047821827,A
3783,"We also propose initial
solutions to these problems, with the goal of encouraging further research along
these directions.","For each challenge, we
construct a curated data stream that simulates this challenge and quantitatively
demonstrate the shortcomings of existing SSL methods.","We explore the idea of Buffered SSL, which involves augmenting
existing approaches with a replay buffer to improve training efficiency.",2022-03-23 20:05:06+00:00,The Challenges of Continuous Self-Supervised Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Senthil Purushwalkam'), arxiv.Result.Author('Pedro Morgado'), arxiv.Result.Author('Abhinav Gupta')]","Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.",0.2110638,-0.1461332,-0.04763053,A
3784,"We believe however that further research is needed to develop
deployable systems that deliver on the promise of self-supervised learning, and
hope future efforts in SSL research focus on these challenges.","To this end, we identified three broad challenges of deployable SSL - training
efficiency, correlated data, and lifelong learning, - and proposed potential solutions
to address them.","Bibliography

 [1] Achille, A., Eccles, T., Matthey, L., Burgess, C., Watters, N., Lerchner, A.,
      Higgins, I.: Life-long disentangled representation learning with cross-domain
      latent homologies.",2022-03-23 20:05:06+00:00,The Challenges of Continuous Self-Supervised Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Senthil Purushwalkam'), arxiv.Result.Author('Pedro Morgado'), arxiv.Result.Author('Abhinav Gupta')]","Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.",0.122148514,-0.3349538,-0.07270655,C
3785,"We also propose
initial solutions to these problems, with the goal of encouraging further research
along these directions.","For each challenge,
we construct a curated data stream that simulates this challenge and quantita-
tively demonstrate the shortcomings of existing SSL methods.","We explore the idea of Buffered SSL, which involves
augmenting existing approaches with a replay buffer to improve training effi-
ciency.",2022-03-23 20:05:06+00:00,The Challenges of Continuous Self-Supervised Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Senthil Purushwalkam'), arxiv.Result.Author('Pedro Morgado'), arxiv.Result.Author('Abhinav Gupta')]","Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.",0.19649142,-0.16678384,-0.060100503,A
3786,"We believe however that further research is needed
to develop deployable systems that deliver on the promise of self-supervised
learning, and hope future efforts in SSL research focus on these challenges.","To this end, we identified three broad challenges of deployable SSL - training
efficiency, correlated data, and lifelong learning, - and proposed potential so-
lutions to address them.","References

 [1] Achille, A., Eccles, T., Matthey, L., Burgess, C., Watters, N., Lerchner, A.,
      Higgins, I.: Life-long disentangled representation learning with cross-
      domain latent homologies.",2022-03-23 20:05:06+00:00,The Challenges of Continuous Self-Supervised Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Senthil Purushwalkam'), arxiv.Result.Author('Pedro Morgado'), arxiv.Result.Author('Abhinav Gupta')]","Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.",0.14487743,-0.3333078,-0.07044201,C
3791,"We further study the relationship between vacuity and
the prediction accuracy for query set predictions to access
the reliability of the uncertainty.","Additional illustrative examples and
comparisons demonstrating our model‚Äôs potential in open-
set/OOD detection, and the model training process is pro-
vided in the Appendix.","Figure 5 visualizes how the
prediction accuracy varies with vacuity using 5-way 1-shot
and 5-way 5-shot tasks from CifarFS.",2022-03-23 23:37:16+00:00,Multidimensional Belief Quantification for Label-Efficient Meta-Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Deep Pandey'), arxiv.Result.Author('Qi Yu')]","Optimization-based meta-learning offers a promising direction for few-shot
learning that is essential for many real-world computer vision applications.
However, learning from few samples introduces uncertainty, and quantifying
model confidence for few-shot predictions is essential for many critical
domains. Furthermore, few-shot tasks used in meta training are usually sampled
randomly from a task distribution for an iterative model update, leading to
high labeling costs and computational overhead in meta-training. We propose a
novel uncertainty-aware task selection model for label efficient meta-learning.
The proposed model formulates a multidimensional belief measure, which can
quantify the known uncertainty and lower bound the unknown uncertainty of any
given task. Our theoretical result establishes an important relationship
between the conflicting belief and the incorrect belief. The theoretical result
allows us to estimate the total uncertainty of a task, which provides a
principled criterion for task selection. A novel multi-query task formulation
is further developed to improve both the computational and labeling efficiency
of meta-learning. Experiments conducted over multiple real-world few-shot image
classification tasks demonstrate the effectiveness of the proposed model.",0.028101478,-0.13132852,-0.1266732,A
3795,"To accelerate further research in this domain, we           Modern visual pose estimation algorithms in 2D
                                        also open-source MAPdat, a new multi-modal dataset            [9, 40] and 3D [24, 29] have paved a promising path
                                        of 3D violin playing motion with music.","from sparse input pose sequences but continuous au-
                                        dio.","We perform a          for using smartphones and webcams to estimate human
                                        comparison of different standard machine learning mod-        pose.",2022-03-24 03:16:42+00:00,AIMusicGuru: Music Assisted Human Pose Correction,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Snehesh Shrestha'), arxiv.Result.Author('Cornelia Ferm√ºller'), arxiv.Result.Author('Tianyu Huang'), arxiv.Result.Author('Pyone Thant Win'), arxiv.Result.Author('Adam Zukerman'), arxiv.Result.Author('Chethan M. Parameshwara'), arxiv.Result.Author('Yiannis Aloimonos')]","Pose Estimation techniques rely on visual cues available through observations
represented in the form of pixels. But the performance is bounded by the frame
rate of the video and struggles from motion blur, occlusions, and temporal
coherence. This issue is magnified when people are interacting with objects and
instruments, for example playing the violin. Standard approaches for
postprocessing use interpolation and smoothing functions to filter noise and
fill gaps, but they cannot model highly non-linear motion. We present a method
that leverages our understanding of the high degree of a causal relationship
between the sound produced and the motion that produces them. We use the audio
signature to refine and predict accurate human body pose motion models. We
propose MAPnet (Music Assisted Pose network) for generating a fine grain motion
model from sparse input pose sequences but continuous audio. To accelerate
further research in this domain, we also open-source MAPdat, a new multi-modal
dataset of 3D violin playing motion with music. We perform a comparison of
different standard machine learning models and perform analysis on input
modalities, sampling techniques, and audio and motion features. Experiments on
MAPdat suggest multi-modal approaches like ours as a promising direction for
tasks previously approached with visual methods only. Our results show both
qualitatively and quantitatively how audio can be combined with visual
observation to help improve any pose estimation methods.",-0.26487258,0.116566114,-0.096320376,B
3796,"To accelerate further research in this
ble 4), which shows as much as 17.39% difference in               domain, we also open-source MAPdata, a new multi-
the MPJPE results.","We propose MAPnet (Music As-
                                                                  sisted Pose network) for generating a Ô¨Åne-grained mo-
   Early, balanced, or late fusion: We also con-                  tion model from sparse input pose sequences and con-
ducted early, balanced, and late fusion ablation (see ta-         tinuous audio.","In this paper, our experiments sug-            modal dataset of 3D violin playing.",2022-03-24 03:16:42+00:00,AIMusicGuru: Music Assisted Human Pose Correction,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Snehesh Shrestha'), arxiv.Result.Author('Cornelia Ferm√ºller'), arxiv.Result.Author('Tianyu Huang'), arxiv.Result.Author('Pyone Thant Win'), arxiv.Result.Author('Adam Zukerman'), arxiv.Result.Author('Chethan M. Parameshwara'), arxiv.Result.Author('Yiannis Aloimonos')]","Pose Estimation techniques rely on visual cues available through observations
represented in the form of pixels. But the performance is bounded by the frame
rate of the video and struggles from motion blur, occlusions, and temporal
coherence. This issue is magnified when people are interacting with objects and
instruments, for example playing the violin. Standard approaches for
postprocessing use interpolation and smoothing functions to filter noise and
fill gaps, but they cannot model highly non-linear motion. We present a method
that leverages our understanding of the high degree of a causal relationship
between the sound produced and the motion that produces them. We use the audio
signature to refine and predict accurate human body pose motion models. We
propose MAPnet (Music Assisted Pose network) for generating a fine grain motion
model from sparse input pose sequences but continuous audio. To accelerate
further research in this domain, we also open-source MAPdat, a new multi-modal
dataset of 3D violin playing motion with music. We perform a comparison of
different standard machine learning models and perform analysis on input
modalities, sampling techniques, and audio and motion features. Experiments on
MAPdat suggest multi-modal approaches like ours as a promising direction for
tasks previously approached with visual methods only. Our results show both
qualitatively and quantitatively how audio can be combined with visual
observation to help improve any pose estimation methods.",-0.05222847,0.009117729,-0.007511413,C
3799,"Their attention are paid on
provide researchers with further research ideas.","It will       attention, temporal attention, and branch attention.","what, where, when, and which to attention respectively.",2022-03-24 09:09:00+00:00,Transformers Meet Visual Learning Understanding: A Comprehensive Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuting Yang'), arxiv.Result.Author('Licheng Jiao'), arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Fang Liu'), arxiv.Result.Author('Shuyuan Yang'), arxiv.Result.Author('Zhixi Feng'), arxiv.Result.Author('Xu Tang')]","Dynamic attention mechanism and global modeling ability make Transformer show
strong feature learning ability. In recent years, Transformer has become
comparable to CNNs methods in computer vision. This review mainly investigates
the current research progress of Transformer in image and video applications,
which makes a comprehensive overview of Transformer in visual learning
understanding. First, the attention mechanism is reviewed, which plays an
essential part in Transformer. And then, the visual Transformer model and the
principle of each module are introduced. Thirdly, the existing
Transformer-based models are investigated, and their performance is compared in
visual learning understanding applications. Three image tasks and two video
tasks of computer vision are investigated. The former mainly includes image
classification, object detection, and image segmentation. The latter contains
object tracking and video classification. It is significant for comparing
different models' performance in various tasks on several public benchmark data
sets. Finally, ten general problems are summarized, and the developing
prospects of the visual Transformer are given in this review.",0.13437176,-0.07786687,-0.32376316,A
3803,"In the future, We will study
                else                      minimizing input data time without performance decline,
                     loss = Losssitu      and further study how to improve performance through the
                end if                    application of task discriminator.","We tried to
                if i is MT L then         extract task independent features by adding a Gradient reversal
                     loss = LMS T L       layer and a task discriminator.","Œ∏s ‚Üê Œ∏s ‚àí lr ¬∑ ‚àÇ‚àÇloŒ∏sss
            end for                                                  REFERENCES

       end while                           [1] Geesung Oh, Euiseok Jeong, and Sejoon Lim.",2022-03-24 13:50:48+00:00,Multitask Emotion Recognition Model with Knowledge Distillation and Task Discriminator,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Euiseok Jeong'), arxiv.Result.Author('Geesung Oh'), arxiv.Result.Author('Sejoon Lim')]","Due to the collection of big data and the development of deep learning,
research to predict human emotions in the wild is being actively conducted. We
designed a multi-task model using ABAW dataset to predict valence-arousal,
expression, and action unit through audio data and face images at in real
world. We trained model from the incomplete label by applying the knowledge
distillation technique. The teacher model was trained as a supervised learning
method, and the student model was trained by using the output of the teacher
model as a soft label. As a result we achieved 2.40 in Multi Task Learning task
validation dataset.",0.10434781,-0.23907334,0.071805045,C
3806,"Our results demonstrate that this new
task is worthy of further study by researchers in robotics,       Basically, cognitive robots that need to interact with hu-
vision, and learning communities.",their effectiveness.,"mans should be able to expect target locations of human
                                                               actions at an early stage, allowing robots to compute appro-
1.",2022-03-24 15:16:05+00:00,Egocentric Prediction of Action Target in 3D,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yiming Li'), arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Andrew Liang'), arxiv.Result.Author('Benjamin Liang'), arxiv.Result.Author('Luoyao Chen'), arxiv.Result.Author('Hang Zhao'), arxiv.Result.Author('Chen Feng')]","We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities.",-0.16382673,0.112466596,-0.36605027,B
3810,"Promising results were shown so far, thus encouraging further research.","Recently, several classical problems were
reformulated to enable quantum optimization, including recognition [47, 15] and match-
ing tasks [55, 10].","Among the two existing paradigms for quantum computing, i.e., gate-based and adiabatic

Ground truth Our method  Mode [5]  Synch [4]  Xu et al.",2022-03-24 17:02:43+00:00,Quantum Motion Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Federica Arrigoni'), arxiv.Result.Author('Willi Menapace'), arxiv.Result.Author('Marcel Seelbach Benkner'), arxiv.Result.Author('Elisa Ricci'), arxiv.Result.Author('Vladislav Golyanik')]","Motion segmentation is a challenging problem that seeks to identify
independent motions in two or several input images. This paper introduces the
first algorithm for motion segmentation that relies on adiabatic quantum
optimization of the objective function. The proposed method achieves on-par
performance with the state of the art on problem instances which can be mapped
to modern quantum annealers.",0.20443669,-0.02370793,0.15261331,A
3821,"To resolve this, Plotz and             While the later two factors are easy to improve, how to im-
Roth [41] propose a fully end-to-end trainable neural near-        prove the network architecture remains further study.","2
ing in Ô¨Åxed feature spaces.","est neighbor block to leverage the principle of non-local
self-similarity.",2022-03-24 18:11:31+00:00,Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis,cs.CV,"['cs.CV', 'cs.GR', 'eess.IV']","[arxiv.Result.Author('Kai Zhang'), arxiv.Result.Author('Yawei Li'), arxiv.Result.Author('Jingyun Liang'), arxiv.Result.Author('Jiezhang Cao'), arxiv.Result.Author('Yulun Zhang'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Radu Timofte'), arxiv.Result.Author('Luc Van Gool')]","While recent years have witnessed a dramatic upsurge of exploiting deep
neural networks toward solving image denoising, existing methods mostly rely on
simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG
compression noise and camera sensor noise, and a general-purpose blind
denoising method for real images remains unsolved. In this paper, we attempt to
solve this problem from the perspective of network architecture design and
training data synthesis. Specifically, for the network architecture design, we
propose a swin-conv block to incorporate the local modeling ability of residual
convolutional layer and non-local modeling ability of swin transformer block,
and then plug it as the main building block into the widely-used image-to-image
translation UNet architecture. For the training data synthesis, we design a
practical noise degradation model which takes into consideration different
kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and
processed camera sensor noises) and resizing, and also involves a random
shuffle strategy and a double degradation strategy. Extensive experiments on
AGWN removal and real image denoising demonstrate that the new network
architecture design achieves state-of-the-art performance and the new
degradation model can help to significantly improve the practicability. We
believe our work can provide useful insights into current denoising research.",-0.03269799,-0.24890628,0.17945376,C
3822,"To resolve this, Plotz and             While the later two factors are easy to improve, how to im-
Roth [41] propose a fully end-to-end trainable neural near-        prove the network architecture remains further study.","2
ing in Ô¨Åxed feature spaces.","est neighbor block to leverage the principle of non-local
self-similarity.",2022-03-24 18:11:31+00:00,Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis,cs.CV,"['cs.CV', 'cs.GR', 'eess.IV']","[arxiv.Result.Author('Kai Zhang'), arxiv.Result.Author('Yawei Li'), arxiv.Result.Author('Jingyun Liang'), arxiv.Result.Author('Jiezhang Cao'), arxiv.Result.Author('Yulun Zhang'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Radu Timofte'), arxiv.Result.Author('Luc Van Gool')]","While recent years have witnessed a dramatic upsurge of exploiting deep
neural networks toward solving image denoising, existing methods mostly rely on
simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG
compression noise and camera sensor noise, and a general-purpose blind
denoising method for real images remains unsolved. In this paper, we attempt to
solve this problem from the perspective of network architecture design and
training data synthesis. Specifically, for the network architecture design, we
propose a swin-conv block to incorporate the local modeling ability of residual
convolutional layer and non-local modeling ability of swin transformer block,
and then plug it as the main building block into the widely-used image-to-image
translation UNet architecture. For the training data synthesis, we design a
practical noise degradation model which takes into consideration different
kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and
processed camera sensor noises) and resizing, and also involves a random
shuffle strategy and a double degradation strategy. Extensive experiments on
AGWN removal and real image denoising demonstrate that the new network
architecture design achieves state-of-the-art performance and the new
degradation model can help to significantly improve the practicability. We
believe our work can provide useful insights into current denoising research.",-0.03269799,-0.24890628,0.17945376,C
3829,"However, the well-known ‚Äòblack-box‚Äô charac-
                                        teristic of CNN has limited its further research and application.CNN was initially inspired by the
                                        concept of receptive Ô¨Åeld in the biological visual system [19].","1 Introduction

                                        Convolutional neural networks (CNN), the mainstream deep learning model architecture, has achieved
                                        milestone breakthrough in the computer vision area.","The biological visual information
                                        processing system consists of visual senses, visual pathways and multi-level visual centers that enable
                                        the generation, transmission and processing of visual information [28].",2022-03-25 05:27:28+00:00,CNN LEGO: Disassembling and Assembling Convolutional Neural Network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiacong Hu'), arxiv.Result.Author('Jing Gao'), arxiv.Result.Author('Zunlei Feng'), arxiv.Result.Author('Lechao Cheng'), arxiv.Result.Author('Jie Lei'), arxiv.Result.Author('Hujun Bao'), arxiv.Result.Author('Mingli Song')]","Convolutional Neural Network (CNN), which mimics human visual perception
mechanism, has been successfully used in many computer vision areas. Some
psychophysical studies show that the visual perception mechanism synchronously
processes the form, color, movement, depth, etc., in the initial stage [7,20]
and then integrates all information for final recognition [38]. What's more,
the human visual system [20] contains different subdivisions or different
tasks. Inspired by the above visual perception mechanism, we investigate a new
task, termed as Model Disassembling and Assembling (MDA-Task), which can
disassemble the deep models into independent parts and assemble those parts
into a new deep model without performance cost like playing LEGO toys. To this
end, we propose a feature route attribution technique (FRAT) for disassembling
CNN classifiers in this paper. In FRAT, the positive derivatives of predicted
class probability w.r.t. the feature maps are adopted to locate the critical
features in each layer. Then, relevance analysis between the critical features
and preceding/subsequent parameter layers is adopted to bridge the route
between two adjacent parameter layers. In the assembling phase, class-wise
components of each layer are assembled into a new deep model for a specific
task. Extensive experiments demonstrate that the assembled CNN classifier can
achieve close accuracy with the original classifier without any fine-tune, and
excess original performance with one-epoch fine-tune. What's more, we also
conduct massive experiments to verify the broad application of MDA-Task on
model decision route visualization, model compression, knowledge distillation,
transfer learning, incremental learning, and so on.",-0.28718555,-0.09806737,0.16207862,C
3837,"The attempts for adapting the concurrent
                                     1.30 / 18.82 9.43 / 25.20                    vehicle-view 3D detection approaches need further study.","Note
              90-120                                                              that all the sensitive information including license plates,
                                     19.34 / 33.45 30.65 / 42.82                  human faces, names of bus stops, roads, and buildings are
                                                                                  totally masked.","24.39 / 37.81 11.22 / 27.54

                                     24.07 / 37.18 16.77 / 31.80

                                     25.22 / 38.84 14.38 / 30.36

                                     39.17 / 50.42 5.80 / 23.02

                                     6.55 / 24.06 4.23 / 21.48

Table 4.",2022-03-25 12:13:23+00:00,Rope3D: TheRoadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaoqing Ye'), arxiv.Result.Author('Mao Shu'), arxiv.Result.Author('Hanyu Li'), arxiv.Result.Author('Yifeng Shi'), arxiv.Result.Author('Yingying Li'), arxiv.Result.Author('Guangjie Wang'), arxiv.Result.Author('Xiao Tan'), arxiv.Result.Author('Errui Ding')]","Concurrent perception datasets for autonomous driving are mainly limited to
frontal view with sensors mounted on the vehicle. None of them is designed for
the overlooked roadside perception tasks. On the other hand, the data captured
from roadside cameras have strengths over frontal-view data, which is believed
to facilitate a safer and more intelligent autonomous driving system. To
accelerate the progress of roadside perception, we present the first
high-diversity challenging Roadside Perception 3D dataset- Rope3D from a novel
view. The dataset consists of 50k images and over 1.5M 3D objects in various
scenes, which are captured under different settings including various cameras
with ambiguous mounting positions, camera specifications, viewpoints, and
different environmental conditions. We conduct strict 2D-3D joint annotation
and comprehensive data analysis, as well as set up a new 3D roadside perception
benchmark with metrics and evaluation devkit. Furthermore, we tailor the
existing frontal-view monocular 3D object detection approaches and propose to
leverage the geometry constraint to solve the inherent ambiguities caused by
various sensors, viewpoints. Our dataset is available on
https://thudair.baai.ac.cn/rope.",-0.2364937,0.33491743,-0.18907653,B
3842,"more robust than CNNs under different white- and black-
We further study necessary conditions (refer to Sec.","While
that corresponds to the adversarial patch, i.e., to increase the     some works [2,5,28,28,34] have hypothesized that ViTs are
attention weights of the queries to the targeted key token.","4.2) for        box transfer attack settings (including universal adversarial
a successful attack that misdirects attention weights: both          patches), others [7, 27, 29, 43] have claimed that ViTs are
(a) having projection matrices with large singular values            at least as vulnerable as CNNs.",2022-03-25 13:26:27+00:00,Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness,cs.CV,"['cs.CV', '68T07', 'I.4']","[arxiv.Result.Author('Giulio Lovisotto'), arxiv.Result.Author('Nicole Finnie'), arxiv.Result.Author('Mauricio Munoz'), arxiv.Result.Author('Chaithanya Kumar Mummadi'), arxiv.Result.Author('Jan Hendrik Metzen')]","Neural architectures based on attention such as vision transformers are
revolutionizing image recognition. Their main benefit is that attention allows
reasoning about all parts of a scene jointly. In this paper, we show how the
global reasoning of (scaled) dot-product attention can be the source of a major
vulnerability when confronted with adversarial patch attacks. We provide a
theoretical understanding of this vulnerability and relate it to an adversary's
ability to misdirect the attention of all queries to a single key token under
the control of the adversarial patch. We propose novel adversarial objectives
for crafting adversarial patches which target this vulnerability explicitly. We
show the effectiveness of the proposed patch attacks on popular image
classification (ViTs and DeiTs) and object detection models (DETR). We find
that adversarial patches occupying 0.5% of the input can lead to robust
accuracies as low as 0% for ViT on ImageNet, and reduce the mAP of DETR on MS
COCO to less than 3%.",0.050940767,-0.21497989,0.17648391,C
3845,"In recent years, researchers using MD simulations have helped make the Ô¨Åeld of MD more FAIR (Ô¨Åndability, accessibility,
interoperability, reproducibility of data [7]) by depositing their data in general-purpose scientiÔ¨Åc data repositories,
making them accessible for further study and research [8].","MD simulations can provide
unprecedented insight into temporal and spatial details by calculating the dynamic behavior of proteins in their natural
cellular environment in silico.","Even if access to MD trajectories is granted, some degree of
expert knowledge is required to even load and visually examine a simulation.",2022-03-25 14:08:24+00:00,MDsrv -- visual sharing and analysis of molecular dynamics simulations,cs.CV,"['cs.CV', 'cs.HC']","[arxiv.Result.Author('Michelle Kampfrath'), arxiv.Result.Author('Ren√© Staritzbichler'), arxiv.Result.Author('Guillermo P√©rez Hern√°ndez'), arxiv.Result.Author('Alexander S. Rose'), arxiv.Result.Author('Johanna K. S. Tiemann'), arxiv.Result.Author('Gerik Scheuermann'), arxiv.Result.Author('Daniel Wiegreffe'), arxiv.Result.Author('Peter W. Hildebrand')]","Molecular dynamics simulation is a proven technique for computing and
visualizing the time-resolved motion of macromolecules at atomic resolution.
The MDsrv is a tool that streams MD trajectories and displays them
interactively in web browsers without requiring advanced skills, facilitating
interactive exploration and collaborative visual analysis. We have now enhanced
the MDsrv to further simplify the upload and sharing of MD trajectories and
improve their online viewing and analysis. With the new instance, the MDsrv
simplifies the creation of sessions, which allows the exchange of MD
trajectories with preset representations and perspectives. An important
innovation is that the MDsrv can now access and visualize trajectories from
remote datasets, which greatly expands its applicability and use, as the data
no longer needs to be accessible on a local server. In addition, initial
analyses such as sequence or structure alignments, distance measurements, or
RMSD calculations have been implemented, which optionally support visual
analysis. Finally, the MDsrv now offers a faster and more efficient
visualization of even large trajectories.",0.23797154,0.095369205,-0.045217507,A
3846,"In recent years, researchers using MD simulations have helped make the Ô¨Åeld of MD more FAIR (Ô¨Åndability, accessibility,
interoperability, reproducibility of data [7]) by depositing their data in general-purpose scientiÔ¨Åc data repositories,
making them accessible for further study and research [8].","MD simulations can provide
unprecedented insight into temporal and spatial details by calculating the dynamic behavior of proteins in their natural
cellular environment in silico.","Even if access to MD trajectories is granted, some degree

                                                                      2
MDsrv

of expert knowledge is required to even load and visually examine a simulation.",2022-03-25 14:08:24+00:00,MDsrv -- visual sharing and analysis of molecular dynamics simulations,cs.CV,"['cs.CV', 'cs.HC']","[arxiv.Result.Author('Michelle Kampfrath'), arxiv.Result.Author('Ren√© Staritzbichler'), arxiv.Result.Author('Guillermo P√©rez Hern√°ndez'), arxiv.Result.Author('Alexander S. Rose'), arxiv.Result.Author('Johanna K. S. Tiemann'), arxiv.Result.Author('Gerik Scheuermann'), arxiv.Result.Author('Daniel Wiegreffe'), arxiv.Result.Author('Peter W. Hildebrand')]","Molecular dynamics simulation is a proven technique for computing and
visualizing the time-resolved motion of macromolecules at atomic resolution.
The MDsrv is a tool that streams MD trajectories and displays them
interactively in web browsers without requiring advanced skills, facilitating
interactive exploration and collaborative visual analysis. We have now enhanced
the MDsrv to further simplify the upload and sharing of MD trajectories and
improve their online viewing and analysis. With the new instance, the MDsrv
simplifies the creation of sessions, which allows the exchange of MD
trajectories with preset representations and perspectives. An important
innovation is that the MDsrv can now access and visualize trajectories from
remote datasets, which greatly expands its applicability and use, as the data
no longer needs to be accessible on a local server. In addition, initial
analyses such as sequence or structure alignments, distance measurements, or
RMSD calculations have been implemented, which optionally support visual
analysis. Finally, the MDsrv now offers a faster and more efficient
visualization of even large trajectories.",0.23191054,0.08038444,-0.04533105,A
3879,"Further Research Issues

    We turn to discussing some further research issues which have recently drawn
much attentions in robust adversarial training, hoping to oÔ¨Äer more inspirations
and outlooks in this Ô¨Åeld.",5.,"We will review from three aspects: Interpretability
of adversarial examples, robust generalization, and robustness evaluation.",2022-03-26 11:00:25+00:00,"A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies",cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhuang Qian'), arxiv.Result.Author('Kaizhu Huang'), arxiv.Result.Author('Qiu-Feng Wang'), arxiv.Result.Author('Xu-Yao Zhang')]","In the last a few decades, deep neural networks have achieved remarkable
success in machine learning, computer vision, and pattern recognition. Recent
studies however show that neural networks (both shallow and deep) may be easily
fooled by certain imperceptibly perturbed input samples called adversarial
examples. Such security vulnerability has resulted in a large body of research
in recent years because real-world threats could be introduced due to vast
applications of neural networks. To address the robustness issue to adversarial
examples particularly in pattern recognition, robust adversarial training has
become one mainstream. Various ideas, methods, and applications have boomed in
the field. Yet, a deep understanding of adversarial training including
characteristics, interpretations, theories, and connections among different
models has still remained elusive. In this paper, we present a comprehensive
survey trying to offer a systematic and structured investigation on robust
adversarial training in pattern recognition. We start with fundamentals
including definition, notations, and properties of adversarial examples. We
then introduce a unified theoretical framework for defending against
adversarial samples - robust adversarial training with visualizations and
interpretations on why adversarial training can lead to model robustness.
Connections will be also established between adversarial training and other
traditional learning theories. After that, we summarize, review, and discuss
various methodologies with adversarial attack and defense/training algorithms
in a structured way. Finally, we present analysis, outlook, and remarks of
adversarial training.",-0.05873326,-0.24415319,0.0071602874,C
3880,"E. Examples

                                                                     To further study different input modalities and validate
                                                                  the effectiveness of the proposed model and compare to
                                                                  recent QA methods, we visualize some QA examples and
                                                                  have following findings:
Q1: Is the guzheng in the video always playing?","Q+V*                                        41.70

                                                                  In such a situation, our             Q+V+A* 41.45

                                                                  method still shows better            Q+V‚Ä†                                        42.01

                                                                  robustness with less                 Q+V+A‚Ä† 41.95

                                                                  performance drop.",Q1: Which violin makes the sound first ?,2022-03-26 13:03:42+00:00,Learning to Answer Questions in Dynamic Audio-Visual Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangyao Li'), arxiv.Result.Author('Yake Wei'), arxiv.Result.Author('Yapeng Tian'), arxiv.Result.Author('Chenliang Xu'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Di Hu')]","In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,
which aims to answer questions regarding different visual objects, sounds, and
their associations in videos. The problem requires comprehensive multimodal
understanding and spatio-temporal reasoning over audio-visual scenes. To
benchmark this task and facilitate our study, we introduce a large-scale
MUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering
33 different question templates spanning over different modalities and question
types. We develop several baselines and introduce a spatio-temporal grounded
audio-visual network for the AVQA problem. Our results demonstrate that AVQA
benefits from multisensory perception and our model outperforms recent A-, V-,
and AVQA approaches. We believe that our built dataset has the potential to
serve as testbed for evaluating and promoting progress in audio-visual scene
understanding and spatio-temporal reasoning. Code and dataset:
http://gewu-lab.github.io/MUSIC-AVQA/",0.31408846,0.010626116,-0.14770913,A
3881,"E. Examples

                                                                     To further study different input modalities and validate
                                                                  the effectiveness of the proposed model and compare to
                                                                  recent QA methods, we visualize some QA examples and
                                                                  have following findings:
Q1: Is the guzheng in the video always playing?","Q+V*                                        41.70

                                                                  In such a situation, our             Q+V+A* 41.45

                                                                  method still shows better            Q+V‚Ä†                                        42.01

                                                                  robustness with less                 Q+V+A‚Ä† 41.95

                                                                  performance drop.",Q1: Which violin makes the sound first ?,2022-03-26 13:03:42+00:00,Learning to Answer Questions in Dynamic Audio-Visual Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangyao Li'), arxiv.Result.Author('Yake Wei'), arxiv.Result.Author('Yapeng Tian'), arxiv.Result.Author('Chenliang Xu'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Di Hu')]","In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,
which aims to answer questions regarding different visual objects, sounds, and
their associations in videos. The problem requires comprehensive multimodal
understanding and spatio-temporal reasoning over audio-visual scenes. To
benchmark this task and facilitate our study, we introduce a large-scale
MUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering
33 different question templates spanning over different modalities and question
types. We develop several baselines and introduce a spatio-temporal grounded
audio-visual network for the AVQA problem. Our results demonstrate that AVQA
benefits from multisensory perception and our model outperforms recent A-, V-,
and AVQA approaches. We believe that our built dataset has the potential to
serve as testbed for evaluating and promoting progress in audio-visual scene
understanding and spatio-temporal reasoning. Code and dataset:
http://gewu-lab.github.io/MUSIC-AVQA/",0.31408846,0.010626116,-0.14770913,A
3903,"Finally, discussions on                      IEEE/CVF International Conference on Computer Vision, 2015.
dataset construction, evaluation protocols, and model design
are given for further research.","He, ‚ÄúThe
performance, depth data quality, long-term settings, attribute-                      visual object tracking vot2015 challenge results,‚Äù in Proceedings of the
based tests, and robustness evaluation.","[16] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, and Z.",2022-03-26 18:53:51+00:00,RGBD Object Tracking: An In-depth Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinyu Yang'), arxiv.Result.Author('Zhe Li'), arxiv.Result.Author('Song Yan'), arxiv.Result.Author('Feng Zheng'), arxiv.Result.Author('Ale≈° Leonardis'), arxiv.Result.Author('Joni-Kristian K√§m√§r√§inen'), arxiv.Result.Author('Ling Shao')]","RGBD object tracking is gaining momentum in computer vision research thanks
to the development of depth sensors. Although numerous RGBD trackers have been
proposed with promising performance, an in-depth review for comprehensive
understanding of this area is lacking. In this paper, we firstly review RGBD
object trackers from different perspectives, including RGBD fusion, depth
usage, and tracking framework. Then, we summarize the existing datasets and the
evaluation metrics. We benchmark a representative set of RGBD trackers, and
give detailed analyses based on their performances. Particularly, we are the
first to provide depth quality evaluation and analysis of tracking results in
depth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD
tracking videos, we give an analysis of trackers' performance on handling
target disappearance. To enable better understanding of RGBD trackers, we
propose robustness evaluation against input perturbations. Finally, we
summarize the challenges and provide open directions for this community. All
resources are publicly available at
https://github.com/memoryunreal/RGBD-tracking-review.",-0.25448236,0.08164844,-0.028592093,B
3904,"We further study the sensitivity of CDD-RED to
the choice of Œª, which is set by 0, 0.0125, and 0.05.","d(x, xRED)                    Œª=0                  Œª=0.0125           Œª=0.05
(‚Üì is better)            8.92 (‚Üì 4.12)          12.79 (‚Üì 0.25)    14.85 (‚Üë 2.13)
                      47.61% (‚Üì38.10%)        81.00% (‚Üì4.71%)   85.56% (‚Üì 0.15%)
 PAbenign             73.37% (‚Üì7.06%)         78.25% (‚Üì 2.18%)  79.94% (‚Üì 0.49%)
(‚Üë is better)

   PAadv
(‚Üë is better)

The overall training objective of CDD-RED is (4), which is the weighted sum of the reconstruction
error and PA with a regularization parameter Œª.","Table A7 shows the RED performance of using
different Œª values, compared with the default setting Œª = 0.025.",2022-03-26 19:52:40+00:00,Reverse Engineering of Imperceptible Adversarial Image Perturbations,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yifan Gong'), arxiv.Result.Author('Yuguang Yao'), arxiv.Result.Author('Yize Li'), arxiv.Result.Author('Yimeng Zhang'), arxiv.Result.Author('Xiaoming Liu'), arxiv.Result.Author('Xue Lin'), arxiv.Result.Author('Sijia Liu')]","It has been well recognized that neural network based image classifiers are
easily fooled by images with tiny perturbations crafted by an adversary. There
has been a vast volume of research to generate and defend such adversarial
attacks. However, the following problem is left unexplored: How to
reverse-engineer adversarial perturbations from an adversarial image? This
leads to a new adversarial learning paradigm--Reverse Engineering of Deceptions
(RED). If successful, RED allows us to estimate adversarial perturbations and
recover the original images. However, carefully crafted, tiny adversarial
perturbations are difficult to recover by optimizing a unilateral RED
objective. For example, the pure image denoising method may overfit to
minimizing the reconstruction error but hardly preserve the classification
properties of the true adversarial perturbations. To tackle this challenge, we
formalize the RED problem and identify a set of principles crucial to the RED
approach design. Particularly, we find that prediction alignment and proper
data augmentation (in terms of spatial transformations) are two criteria to
achieve a generalizable RED approach. By integrating these RED principles with
image denoising, we propose a new Class-Discriminative Denoising based RED
framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness
of CDD-RED under different evaluation metrics (ranging from the pixel-level,
prediction-level to the attribution-level alignment) and a variety of attack
generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).",0.376828,0.1249741,0.15730514,A
3905,"We further study the sensitivity of CDD-RED to
the choice of Œª, which is set by 0, 0.0125, and 0.05.","d(x, xRED)                    Œª=0                  Œª=0.0125           Œª=0.05
(‚Üì is better)            8.92 (‚Üì 4.12)          12.79 (‚Üì 0.25)    14.85 (‚Üë 2.13)
                      47.61% (‚Üì38.10%)        81.00% (‚Üì4.71%)   85.56% (‚Üì 0.15%)
 PAbenign             73.37% (‚Üì7.06%)         78.25% (‚Üì 2.18%)  79.94% (‚Üì 0.49%)
(‚Üë is better)

   PAadv
(‚Üë is better)

The overall training objective of CDD-RED is (4), which is the weighted sum of the reconstruction
error and PA with a regularization parameter Œª.","Table A7 shows the RED performance of using
different Œª values, compared with the default setting Œª = 0.025.",2022-03-26 19:52:40+00:00,Reverse Engineering of Imperceptible Adversarial Image Perturbations,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yifan Gong'), arxiv.Result.Author('Yuguang Yao'), arxiv.Result.Author('Yize Li'), arxiv.Result.Author('Yimeng Zhang'), arxiv.Result.Author('Xiaoming Liu'), arxiv.Result.Author('Xue Lin'), arxiv.Result.Author('Sijia Liu')]","It has been well recognized that neural network based image classifiers are
easily fooled by images with tiny perturbations crafted by an adversary. There
has been a vast volume of research to generate and defend such adversarial
attacks. However, the following problem is left unexplored: How to
reverse-engineer adversarial perturbations from an adversarial image? This
leads to a new adversarial learning paradigm--Reverse Engineering of Deceptions
(RED). If successful, RED allows us to estimate adversarial perturbations and
recover the original images. However, carefully crafted, tiny adversarial
perturbations are difficult to recover by optimizing a unilateral RED
objective. For example, the pure image denoising method may overfit to
minimizing the reconstruction error but hardly preserve the classification
properties of the true adversarial perturbations. To tackle this challenge, we
formalize the RED problem and identify a set of principles crucial to the RED
approach design. Particularly, we find that prediction alignment and proper
data augmentation (in terms of spatial transformations) are two criteria to
achieve a generalizable RED approach. By integrating these RED principles with
image denoising, we propose a new Class-Discriminative Denoising based RED
framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness
of CDD-RED under different evaluation metrics (ranging from the pixel-level,
prediction-level to the attribution-level alignment) and a variety of attack
generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).",0.376828,0.1249741,0.15730514,A
3911,"To further study this phenomenon, we                        [7] L. Yue, H. Shen, J. Li, Q. Yuan, H. Zhang, and L. Zhang, ‚ÄúImage super-
looked into the score distributions of each comparator, ob-                           resolution: The techniques, applications, and future,‚Äù Signal Processing,
serving that genuine scores shift towards the impostor ones                           vol.","2 is better than in scenario 1 for any comparator, specially
at low resolutions.","128, pp.",2022-03-27 03:58:10+00:00,A Survey of Super-Resolution in Iris Biometrics with Evaluation of Dictionary-Learning,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('F. Alonso-Fernandez'), arxiv.Result.Author('R. A. Farrugia'), arxiv.Result.Author('J. Bigun'), arxiv.Result.Author('J. Fierrez'), arxiv.Result.Author('E. Gonzalez-Sosa')]","The lack of resolution has a negative impact on the performance of
image-based biometrics. While many generic super-resolution methods have been
proposed to restore low-resolution images, they usually aim to enhance their
visual appearance. However, a visual enhancement of biometric images does not
necessarily correlate with a better recognition performance. Reconstruction
approaches need thus to incorporate specific information from the target
biometric modality to effectively improve recognition. This paper presents a
comprehensive survey of iris super-resolution approaches proposed in the
literature. We have also adapted an Eigen-patches reconstruction method based
on PCA Eigen-transformation of local image patches. The structure of the iris
is exploited by building a patch-position dependent dictionary. In addition,
image patches are restored separately, having their own reconstruction weights.
This allows the solution to be locally optimized, helping to preserve local
information. To evaluate the algorithm, we degraded high-resolution images from
the CASIA Interval V3 database. Different restorations were considered, with
15x15 pixels being the smallest resolution. To the best of our knowledge, this
is among the smallest resolutions employed in the literature. The framework is
complemented with six public iris comparators, which were used to carry out
biometric verification and identification experiments. Experimental results
show that the proposed method significantly outperforms both bilinear and
bicubic interpolation at very low-resolution. The performance of a number of
comparators attains an impressive Equal Error Rate as low as 5%, and a Top-1
accuracy of 77-84% when considering iris images of only 15x15 pixels. These
results clearly demonstrate the benefit of using trained super-resolution
techniques to improve the quality of iris images prior to matching.",0.07960688,0.15500996,0.21230493,A
3934,"We further study the inÔ¨Çuence of our
                                                                           position shifting strategy on performance.",5                                     72.1 (+6.8) 41.2 (+6.0)           Position Shifting.,"As shown in
Table 4.",2022-03-27 15:46:42+00:00,Locality-Aware Inter-and Intra-Video Reconstruction for Self-Supervised Correspondence Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Liulei Li'), arxiv.Result.Author('Tianfei Zhou'), arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('Lu Yang'), arxiv.Result.Author('Jianwu Li'), arxiv.Result.Author('Yi Yang')]","Our target is to learn visual correspondence from unlabeled videos. We
develop LIIR, a locality-aware inter-and intra-video reconstruction framework
that fills in three missing pieces, i.e., instance discrimination, location
awareness, and spatial compactness, of self-supervised correspondence learning
puzzle. First, instead of most existing efforts focusing on intra-video
self-supervision only, we exploit cross video affinities as extra negative
samples within a unified, inter-and intra-video reconstruction scheme. This
enables instance discriminative representation learning by contrasting desired
intra-video pixel association against negative inter-video correspondence.
Second, we merge position information into correspondence matching, and design
a position shifting strategy to remove the side-effect of position encoding
during inter-video affinity computation, making our LIIR location-sensitive.
Third, to make full use of the spatial continuity nature of video data, we
impose a compactness-based constraint on correspondence matching, yielding more
sparse and reliable solutions. The learned representation surpasses
self-supervised state-of-the-arts on label propagation tasks including objects,
semantic parts, and keypoints.",0.32525247,0.12069027,-0.0007670745,A
3935,"We further study the inÔ¨Çuence of our
                                                                           position shifting strategy on performance.",5                                     72.1 (+6.8) 41.2 (+6.0)           Position Shifting.,"As shown in
Table 4.",2022-03-27 15:46:42+00:00,Locality-Aware Inter-and Intra-Video Reconstruction for Self-Supervised Correspondence Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Liulei Li'), arxiv.Result.Author('Tianfei Zhou'), arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('Lu Yang'), arxiv.Result.Author('Jianwu Li'), arxiv.Result.Author('Yi Yang')]","Our target is to learn visual correspondence from unlabeled videos. We
develop LIIR, a locality-aware inter-and intra-video reconstruction framework
that fills in three missing pieces, i.e., instance discrimination, location
awareness, and spatial compactness, of self-supervised correspondence learning
puzzle. First, instead of most existing efforts focusing on intra-video
self-supervision only, we exploit cross video affinities as extra negative
samples within a unified, inter-and intra-video reconstruction scheme. This
enables instance discriminative representation learning by contrasting desired
intra-video pixel association against negative inter-video correspondence.
Second, we merge position information into correspondence matching, and design
a position shifting strategy to remove the side-effect of position encoding
during inter-video affinity computation, making our LIIR location-sensitive.
Third, to make full use of the spatial continuity nature of video data, we
impose a compactness-based constraint on correspondence matching, yielding more
sparse and reliable solutions. The learned representation surpasses
self-supervised state-of-the-arts on label propagation tasks including objects,
semantic parts, and keypoints.",0.32525247,0.12069027,-0.0007670745,A
3942,"However, the category-aware edge is more              ing details will be released to encourage further research
informative than binary edge, thus it is more beneficial for        towards face parsing.","Train-
face parsing.",face parsing.,2022-03-28 02:12:30+00:00,Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qingping Zheng'), arxiv.Result.Author('Jiankang Deng'), arxiv.Result.Author('Zheng Zhu'), arxiv.Result.Author('Ying Li'), arxiv.Result.Author('Stefanos Zafeiriou')]","This paper probes intrinsic factors behind typical failure cases (e.g.
spatial inconsistency and boundary confusion) produced by the existing
state-of-the-art method in face parsing. To tackle these problems, we propose a
novel Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR) for
face parsing. Specifically, DML-CSR designs a multi-task model which comprises
face parsing, binary edge, and category edge detection. These tasks only share
low-level encoder weights without high-level interactions between each other,
enabling to decouple auxiliary modules from the whole network at the inference
stage. To address spatial inconsistency, we develop a dynamic dual graph
convolutional network to capture global contextual information without using
any extra pooling operation. To handle boundary confusion in both single and
multiple face scenarios, we exploit binary and category edge detection to
jointly obtain generic geometric structure and fine-grained semantic clues of
human faces. Besides, to prevent noisy labels from degrading model
generalization during training, cyclical self-regulation is proposed to
self-ensemble several model instances to get a new model and the resulting
model then is used to self-distill subsequent models, through alternating
iterations. Experiments show that our method achieves the new state-of-the-art
performance on the Helen, CelebAMask-HQ, and Lapa datasets. The source code is
available at
https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr.",-0.052934494,-0.1158876,-0.08964736,C
3943,"To further research in these directions, we
system for collecting videos.","The taxonomy has been curated manually to       only on the consumer side, but user profiling is also help-
capture purely visual categories, and the dataset has been     ful for helping content creators on such social media plat-
machine annotated using the YouTube Video Annotation           forms [5, 27].","Similarly, Holistic Video Un-    provide masked user identifications.",2022-03-28 02:47:01+00:00,"3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos",cs.CV,"['cs.CV', 'cs.AI', 'cs.MM']","[arxiv.Result.Author('Vikram Gupta'), arxiv.Result.Author('Trisha Mittal'), arxiv.Result.Author('Puneet Mathur'), arxiv.Result.Author('Vaibhav Mishra'), arxiv.Result.Author('Mayank Maheshwari'), arxiv.Result.Author('Aniket Bera'), arxiv.Result.Author('Debdoot Mukherjee'), arxiv.Result.Author('Dinesh Manocha')]","We present 3MASSIV, a multilingual, multimodal and multi-aspect,
expertly-annotated dataset of diverse short videos extracted from short-video
social media platform - Moj. 3MASSIV comprises of 50k short videos (20 seconds
average duration) and 100K unlabeled videos in 11 different languages and
captures popular short video trends like pranks, fails, romance, comedy
expressed via unique audio-visual formats like self-shot videos, reaction
videos, lip-synching, self-sung songs, etc. 3MASSIV presents an opportunity for
multimodal and multilingual semantic understanding on these unique videos by
annotating them for concepts, affective states, media types, and audio
language. We present a thorough analysis of 3MASSIV and highlight the variety
and unique aspects of our dataset compared to other contemporary popular
datasets with strong baselines. We also show how the social media content in
3MASSIV is dynamic and temporal in nature, which can be used for semantic
understanding tasks and cross-lingual analysis.",-0.09376849,-0.119570464,-0.28910047,C
3946,We further study the effect of weight re-              geNet and transfer learning.,"different ViT models with lossless performance on Ima-
Weight Recycling.","Ablation studies have proved
cycling by training VOLO-D1 using AutoProg.",2022-03-28 05:37:08+00:00,Automated Progressive Learning for Efficient Training of Vision Transformers,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Changlin Li'), arxiv.Result.Author('Bohan Zhuang'), arxiv.Result.Author('Guangrun Wang'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Xiaojun Chang'), arxiv.Result.Author('Yi Yang')]","Recent advances in vision Transformers (ViTs) have come with a voracious
appetite for computing power, high-lighting the urgent need to develop
efficient training methods for ViTs. Progressive learning, a training scheme
where the model capacity grows progressively during training, has started
showing its ability in efficient training. In this paper, we take a practical
step towards efficient training of ViTs by customizing and automating
progressive learning. First, we develop a strong manual baseline for
progressive learning of ViTs, by introducing momentum growth (MoGrow) to bridge
the gap brought by model growth. Then, we propose automated progressive
learning (AutoProg), an efficient training scheme that aims to achieve lossless
acceleration by automatically increasing the training overload on-the-fly; this
is achieved by adaptively deciding whether, where and how much should the model
grow during progressive learning. Specifically, we first relax the optimization
of the growth schedule to sub-network architecture optimization problem, then
propose one-shot estimation of the sub-network performance via an elastic
supernet. The searching overhead is reduced to minimal by recycling the
parameters of the supernet. Extensive experiments of efficient training on
ImageNet with two representative ViT models, DeiT and VOLO, demonstrate that
AutoProg can accelerate ViTs training by up to 85.1% with no performance drop.
Code: https://github.com/changlin31/AutoProg",0.26525357,-0.1317873,0.067015074,A
3968,"The recent learning-
dataset is challenging and worthy of further research.",Ablation experiments demonstrate that our        ods are more practical and attractive.,"Fi-   based techniques have enable robust human motion cap-
nally, the experiments on the KITTI Dataset and the Waymo    ture from a single RGB stream, using pre-scanned human
Open Dataset show that our method can be generalized to      templates [16, 17, 19, 62, 64] or parametric human mod-
                                                             els [5, 27, 31, 32, 36, 37, 39].",2022-03-28 12:52:45+00:00,LiDARCap: Long-range Marker-less 3D Human Motion Capture with LiDAR Point Clouds,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jialian Li'), arxiv.Result.Author('Jingyi Zhang'), arxiv.Result.Author('Zhiyong Wang'), arxiv.Result.Author('Siqi Shen'), arxiv.Result.Author('Chenglu Wen'), arxiv.Result.Author('Yuexin Ma'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Jingyi Yu'), arxiv.Result.Author('Cheng Wang')]","Existing motion capture datasets are largely short-range and cannot yet fit
the need of long-range applications. We propose LiDARHuman26M, a new human
motion capture dataset captured by LiDAR at a much longer range to overcome
this limitation. Our dataset also includes the ground truth human motions
acquired by the IMU system and the synchronous RGB images. We further present a
strong baseline method, LiDARCap, for LiDAR point cloud human motion capture.
Specifically, we first utilize PointNet++ to encode features of points and then
employ the inverse kinematics solver and SMPL optimizer to regress the pose
through aggregating the temporally encoded features hierarchically.
Quantitative and qualitative experiments show that our method outperforms the
techniques based only on RGB images. Ablation experiments demonstrate that our
dataset is challenging and worthy of further research. Finally, the experiments
on the KITTI Dataset and the Waymo Open Dataset show that our method can be
generalized to different LiDAR sensor settings.",-0.085422635,0.15289074,-0.08686969,B
3976,"Ablation Studies
                                                                       We further study the design choices of example sam-
   In this section, we conduct ablation experiments on              pling strategies.",4.3.,"For a mini-batch, we consider the IoU
VOC-COCO-20 to analyze the effect of our main compo-                threshold Tb; for the memory bank, we consider the IoU
nents and core design choices.",2022-03-28 17:11:09+00:00,Expanding Low-Density Latent Regions for Open-Set Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiaming Han'), arxiv.Result.Author('Yuqiang Ren'), arxiv.Result.Author('Jian Ding'), arxiv.Result.Author('Xingjia Pan'), arxiv.Result.Author('Ke Yan'), arxiv.Result.Author('Gui-Song Xia')]","Modern object detectors have achieved impressive progress under the close-set
setup. However, open-set object detection (OSOD) remains challenging since
objects of unknown categories are often misclassified to existing known
classes. In this work, we propose to identify unknown objects by separating
high/low-density regions in the latent space, based on the consensus that
unknown objects are usually distributed in low-density latent regions. As
traditional threshold-based methods only maintain limited low-density regions,
which cannot cover all unknown objects, we present a novel Open-set Detector
(OpenDet) with expanded low-density regions. To this aim, we equip OpenDet with
two learners, Contrastive Feature Learner (CFL) and Unknown Probability Learner
(UPL). CFL performs instance-level contrastive learning to encourage compact
features of known classes, leaving more low-density regions for unknown
classes; UPL optimizes unknown probability based on the uncertainty of
predictions, which further divides more low-density regions around the cluster
of known classes. Thus, unknown objects in low-density regions can be easily
identified with the learned unknown probability. Extensive experiments
demonstrate that our method can significantly improve the OSOD performance,
e.g., OpenDet reduces the Absolute Open-Set Errors by 25%-35% on six OSOD
benchmarks. Code is available at: https://github.com/csuhan/opendet2.",0.5049161,0.052902788,0.09190805,A
3977,"Ablation Studies
                                                                       We further study the design choices of example sam-
   In this section, we conduct ablation experiments on              pling strategies.",4.3.,"For a mini-batch, we consider the IoU
VOC-COCO-20 to analyze the effect of our main compo-                threshold Tb; for the memory bank, we consider the IoU
nents and core design choices.",2022-03-28 17:11:09+00:00,Expanding Low-Density Latent Regions for Open-Set Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiaming Han'), arxiv.Result.Author('Yuqiang Ren'), arxiv.Result.Author('Jian Ding'), arxiv.Result.Author('Xingjia Pan'), arxiv.Result.Author('Ke Yan'), arxiv.Result.Author('Gui-Song Xia')]","Modern object detectors have achieved impressive progress under the close-set
setup. However, open-set object detection (OSOD) remains challenging since
objects of unknown categories are often misclassified to existing known
classes. In this work, we propose to identify unknown objects by separating
high/low-density regions in the latent space, based on the consensus that
unknown objects are usually distributed in low-density latent regions. As
traditional threshold-based methods only maintain limited low-density regions,
which cannot cover all unknown objects, we present a novel Open-set Detector
(OpenDet) with expanded low-density regions. To this aim, we equip OpenDet with
two learners, Contrastive Feature Learner (CFL) and Unknown Probability Learner
(UPL). CFL performs instance-level contrastive learning to encourage compact
features of known classes, leaving more low-density regions for unknown
classes; UPL optimizes unknown probability based on the uncertainty of
predictions, which further divides more low-density regions around the cluster
of known classes. Thus, unknown objects in low-density regions can be easily
identified with the learned unknown probability. Extensive experiments
demonstrate that our method can significantly improve the OSOD performance,
e.g., OpenDet reduces the Absolute Open-Set Errors by 25%-35% on six OSOD
benchmarks. Code is available at: https://github.com/csuhan/opendet2.",0.5049161,0.052902788,0.09190805,A
3983,"It is often required to combine knowledge from multiple studies
to inform a holistic perspective and guide further research.","Although several methods for fusing
DEMs have been developed, the absence of a well-rounded review has limited their proliferation
among researchers and end-users.","In response, this paper provides a
systematic review of DEM fusion: the pre-processing workflow, methods and applications,
enhanced with a meta-analysis.",2022-03-28 18:39:14+00:00,"A systematic review and meta-analysis of Digital Elevation Model (DEM) fusion: pre-processing, methods and applications",cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Chukwuma Okolie'), arxiv.Result.Author('Julian Smit')]","The remote sensing community has identified data fusion as one of the key
challenging topics of the 21st century. The subject of image fusion in
two-dimensional (2D) space has been covered in several published reviews.
However, the special case of 2.5D/3D Digital Elevation Model (DEM) fusion has
not been addressed till date. DEM fusion is a key application of data fusion in
remote sensing. It takes advantage of the complementary characteristics of
multi-source DEMs to deliver a more complete, accurate and reliable elevation
dataset. Although several methods for fusing DEMs have been developed, the
absence of a well-rounded review has limited their proliferation among
researchers and end-users. It is often required to combine knowledge from
multiple studies to inform a holistic perspective and guide further research.
In response, this paper provides a systematic review of DEM fusion: the
pre-processing workflow, methods and applications, enhanced with a
meta-analysis. Through the discussion and comparative analysis, unresolved
challenges and open issues were identified, and future directions for research
were proposed. This review is a timely solution and an invaluable source of
information for researchers within the fields of remote sensing and spatial
information science, and the data fusion community at large.",0.15184839,0.15686545,-0.06009231,A
3984,"Combining knowledge from multiple studies is often required
to inform a holistic perspective and guide further research.","Although several methods for fusing
DEMs have been developed, the absence of a well-rounded review has limited their proliferation
among researchers and end-users.","In response, this paper provides a
systematic review of DEM fusion: the pre-processing workflow, methods and applications,
enhanced with a meta-analysis.",2022-03-28 18:39:14+00:00,"A systematic review and meta-analysis of Digital Elevation Model (DEM) fusion: pre-processing, methods and applications",cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Chukwuma Okolie'), arxiv.Result.Author('Julian Smit')]","The remote sensing community has identified data fusion as one of the key
challenging topics of the 21st century. The subject of image fusion in
two-dimensional (2D) space has been covered in several published reviews.
However, the special case of 2.5D/3D Digital Elevation Model (DEM) fusion has
not been addressed till date. DEM fusion is a key application of data fusion in
remote sensing. It takes advantage of the complementary characteristics of
multi-source DEMs to deliver a more complete, accurate and reliable elevation
dataset. Although several methods for fusing DEMs have been developed, the
absence of a well-rounded review has limited their proliferation among
researchers and end-users. It is often required to combine knowledge from
multiple studies to inform a holistic perspective and guide further research.
In response, this paper provides a systematic review of DEM fusion: the
pre-processing workflow, methods and applications, enhanced with a
meta-analysis. Through the discussion and comparative analysis, unresolved
challenges and open issues were identified, and future directions for research
were proposed. This review is a timely solution and an invaluable source of
information for researchers within the fields of remote sensing and spatial
information science, and the data fusion community at large.",0.18041545,0.10502495,-0.0983894,A
3991,"the non-differentiability in ‚ÄòStandard‚Äô, we further study an
Training Objective.","To handle
[120] as the backbone and train models for 40K iterations.","We Ô¨Årst investigate our overall train-         approximated Huber-like function [52] (‚ÄòHuberized‚Äô), i.e.,
ing objective (cf.",2022-03-28 21:15:32+00:00,Rethinking Semantic Segmentation: A Prototype View,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tianfei Zhou'), arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('Ender Konukoglu'), arxiv.Result.Author('Luc Van Gool')]","Prevalent semantic segmentation solutions, despite their different network
designs (FCN based or attention based) and mask decoding strategies (parametric
softmax based or pixel-query based), can be placed in one category, by
considering the softmax weights or query vectors as learnable class prototypes.
In light of this prototype view, this study uncovers several limitations of
such parametric segmentation regime, and proposes a nonparametric alternative
based on non-learnable prototypes. Instead of prior methods learning a single
weight/query vector for each class in a fully parametric manner, our model
represents each class as a set of non-learnable prototypes, relying solely on
the mean features of several training pixels within that class. The dense
prediction is thus achieved by nonparametric nearest prototype retrieving. This
allows our model to directly shape the pixel embedding space, by optimizing the
arrangement between embedded pixels and anchored prototypes. It is able to
handle arbitrary number of classes with a constant amount of learnable
parameters. We empirically show that, with FCN based and attention based
segmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet,
HRNet, Swin, MiT), our nonparametric framework yields compelling results over
several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in
the large-vocabulary situation. We expect this work will provoke a rethink of
the current de facto semantic segmentation model design.",0.23853937,-0.07366122,0.15814215,A
3992,"the non-differentiability in ‚ÄòStandard‚Äô, we further study an
Training Objective.","To handle
[120] as the backbone and train models for 40K iterations.","We Ô¨Årst investigate our overall train-         approximated Huber-like function [52] (‚ÄòHuberized‚Äô), i.e.,
ing objective (cf.",2022-03-28 21:15:32+00:00,Rethinking Semantic Segmentation: A Prototype View,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tianfei Zhou'), arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('Ender Konukoglu'), arxiv.Result.Author('Luc Van Gool')]","Prevalent semantic segmentation solutions, despite their different network
designs (FCN based or attention based) and mask decoding strategies (parametric
softmax based or pixel-query based), can be placed in one category, by
considering the softmax weights or query vectors as learnable class prototypes.
In light of this prototype view, this study uncovers several limitations of
such parametric segmentation regime, and proposes a nonparametric alternative
based on non-learnable prototypes. Instead of prior methods learning a single
weight/query vector for each class in a fully parametric manner, our model
represents each class as a set of non-learnable prototypes, relying solely on
the mean features of several training pixels within that class. The dense
prediction is thus achieved by nonparametric nearest prototype retrieving. This
allows our model to directly shape the pixel embedding space, by optimizing the
arrangement between embedded pixels and anchored prototypes. It is able to
handle arbitrary number of classes with a constant amount of learnable
parameters. We empirically show that, with FCN based and attention based
segmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet,
HRNet, Swin, MiT), our nonparametric framework yields compelling results over
several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in
the large-vocabulary situation. We expect this work will provoke a rethink of
the current de facto semantic segmentation model design.",0.23853937,-0.07366122,0.15814215,A
3999,"In
this study, convolutional neural networks and image processing techniques are applied to classify SETI
signals, which will be helpful for further research on data mining of astronomy.","Then an
image classification network will be used to realize the accurate multi classification of SETI signals.","2 Related Work

      SETI intends to use advanced equipment such as radio telescopes to receive electromagnetic waves
from the universe, including background radiation, radio waves from stars and other noise in the galaxy,
to analyze regular radio signals in the hope of discovering alien civilizations.",2022-03-29 04:31:48+00:00,Edge Detection and Deep Learning Based SETI Signal Classification Method,cs.CV,"['cs.CV', 'eess.IV', 'eess.SP']","[arxiv.Result.Author('Zhewei Chen'), arxiv.Result.Author('Sami Ahmed Haider')]","Scientists at the Berkeley SETI Research Center are Searching for
Extraterrestrial Intelligence (SETI) by a new signal detection method that
converts radio signals into spectrograms through Fourier transforms and
classifies signals represented by two-dimensional time-frequency spectrums,
which successfully converts a signal classification problem into an image
classification task. In view of the negative impact of background noises on the
accuracy of spectrograms classification, a new method is introduced in this
paper. After Gaussian convolution smoothing the signals, edge detection
functions are applied to detect the edge of the signals and enhance the outline
of the signals, then the processed spectrograms are used to train the deep
neural network to compare the classification accuracy of various image
classification networks. The results show that the proposed method can
effectively improve the classification accuracy of SETI spectrums.",-0.03641591,0.032302935,0.08751871,C
4006,"Transla-
   To train our large-scale SLP approach, we set a new                tion performance is considerably lower on both meineDGS-
translation protocol on the Meine DGS (mDGS) cor-                     Variants (mDGS-V) and mDGS due to the larger domain,
pus3 [21], a large German Sign Language - Deutsche                    showing that further research is required to scale the task to
Geba¬®rdensprache (DGS) linguistic resource capturing free-            larger vocabularies.","Experimental Setup                                               on PHOENIX14T, outperforming [45] (20.23) but falling
                                                                      short of [39] (23.17) who use larger training data.","form signing from 330 deaf participants, with a vocabulary
of 10,042 glosses.",2022-03-29 08:51:38+00:00,Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ben Saunders'), arxiv.Result.Author('Necati Cihan Camgoz'), arxiv.Result.Author('Richard Bowden')]","Sign languages are visual languages, with vocabularies as rich as their
spoken language counterparts. However, current deep-learning based Sign
Language Production (SLP) models produce under-articulated skeleton pose
sequences from constrained vocabularies and this limits applicability. To be
understandable and accepted by the deaf, an automatic SLP system must be able
to generate co-articulated photo-realistic signing sequences for large domains
of discourse.
  In this work, we tackle large-scale SLP by learning to co-articulate between
dictionary signs, a method capable of producing smooth signing while scaling to
unconstrained domains of discourse. To learn sign co-articulation, we propose a
novel Frame Selection Network (FS-Net) that improves the temporal alignment of
interpolated dictionary signs to continuous signing sequences. Additionally, we
propose SignGAN, a pose-conditioned human synthesis model that produces
photo-realistic sign language videos direct from skeleton pose. We propose a
novel keypoint-based loss function which improves the quality of synthesized
hand images.
  We evaluate our SLP model on the large-scale meineDGS (mDGS) corpus,
conducting extensive user evaluation showing our FS-Net approach improves
co-articulation of interpolated dictionary signs. Additionally, we show that
SignGAN significantly outperforms all baseline methods for quantitative
metrics, human perceptual studies and native deaf signer comprehension.",0.21131667,-0.21255556,-0.14697593,A
4010,"We further study whether it makes a difference
if the window token is initialized with a fixed zero vector or a learnable vector.",Window Token Embedding.,"In contrast to the fixed zero initialization scheme, the learnable window token
helps our SepViT-T to boost the performance to 82.5%, as shown in the last row
of Table 6.",2022-03-29 09:20:01+00:00,SepViT: Separable Vision Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Shiping Wen')]","Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.",0.19853038,-0.16039884,0.16154666,A
4011,"Method         Param(M) FLOPs(G) Top-1 Acc(\% )

    MobileNet-V2 [29] 3.4              0.3  71.8
                                            69.8
                  ResNet18 [14]  11.1  1.8  70.5
                                            72.3
    PVTv2-B0 [34]                3.4   0.6

                  SepViT-Lite    3.7   0.6

    Moreover, to verify the effectiveness of learning the global representation
of each window with our window token embedding scheme (Win Tokens), we
further study some other methods that directly get the global representations
from the output feature maps of DWA, such as average pooling (Avg Pooling)
and depthwise convolution (DW Conv).",Comparison of lite models on ImageNet-1K classification.,"As the results illustrated in Table 7,
our window token embedding scheme achieves the best performance among
these approaches.",2022-03-29 09:20:01+00:00,SepViT: Separable Vision Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Shiping Wen')]","Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.",-0.029165385,-0.15843827,0.30265722,C
4012,"We further study whether it makes a difference
if the window token is initialized with a fixed zero vector or a learnable vector.",Window Token Embedding.,"In contrast to the fixed zero initialization scheme, the learnable window token
helps our SepViT-T to boost the performance to 82.5%, as shown in the last row
of Table 6.",2022-03-29 09:20:01+00:00,SepViT: Separable Vision Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Shiping Wen')]","Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.",0.19853038,-0.16039884,0.16154666,A
4013,"Method         Param(M) FLOPs(G) Top-1 Acc(\% )

    MobileNet-V2 [29] 3.4              0.3  71.8
                                            69.8
                  ResNet18 [14]  11.1  1.8  70.5
                                            72.3
    PVTv2-B0 [34]                3.4   0.6

                  SepViT-Lite    3.7   0.6

    Moreover, to verify the effectiveness of learning the global representation
of each window with our window token embedding scheme (Win Tokens), we
further study some other methods that directly get the global representations
from the output feature maps of DWA, such as average pooling (Avg Pooling)
and depthwise convolution (DW Conv).",Comparison of lite models on ImageNet-1K classification.,"As the results illustrated in Table 7,
our window token embedding scheme achieves the best performance among
these approaches.",2022-03-29 09:20:01+00:00,SepViT: Separable Vision Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Shiping Wen')]","Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.",-0.029165385,-0.15843827,0.30265722,C
4014,"We further study whether it makes a difference
if the window token is initialized with a fixed zero vector or a learnable vector.",Window Token Embedding.,"In contrast to the fixed zero initialization scheme, the learnable window token
helps our SepViT-T to boost the performance to 82.5%, as shown in the last row
of Table 6.",2022-03-29 09:20:01+00:00,SepViT: Separable Vision Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Shiping Wen')]","Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.",0.19853038,-0.16039884,0.16154666,A
4015,"Method         Param(M) FLOPs(G) Top-1 Acc(\% )

    MobileNet-V2 [29] 3.4              0.3  71.8
                                            69.8
                  ResNet18 [14]  11.1  1.8  70.5
                                            72.3
    PVTv2-B0 [34]                3.4   0.6

                  SepViT-Lite    3.7   0.6

    Moreover, to verify the effectiveness of learning the global representation
of each window with our window token embedding scheme (Win Tokens), we
further study some other methods that directly get the global representations
from the output feature maps of DWA, such as average pooling (Avg Pooling)
and depthwise convolution (DW Conv).",Comparison of lite models on ImageNet-1K classification.,"As the results illustrated in Table 7,
our window token embedding scheme achieves the best performance among
these approaches.",2022-03-29 09:20:01+00:00,SepViT: Separable Vision Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Shiping Wen')]","Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.",-0.029165385,-0.15843827,0.30265722,C
4017,"Com-
vide further research opportunities.","Despite the ad-                       5618‚Äì5627, 2017.
vantages of the novel architecture, certain aspects of it pro-           [14] E. Oyallon, E. Belilovsky, S. Zagoruyko, and M. Valko.","For instance, currently,                  pressing the input for cnns with the Ô¨Årst-order scattering trans-
each Hybrid Fusion Block requires recalculation of scattering                  form.",2022-03-29 09:33:59+00:00,Efficient Hybrid Network: Inducting Scattering Features,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dmitry Minskiy'), arxiv.Result.Author('Miroslaw Bober')]","Recent work showed that hybrid networks, which combine predefined and learnt
filters within a single architecture, are more amenable to theoretical analysis
and less prone to overfitting in data-limited scenarios. However, their
performance has yet to prove competitive against the conventional counterparts
when sufficient amounts of training data are available. In an attempt to
address this core limitation of current hybrid networks, we introduce an
Efficient Hybrid Network (E-HybridNet). We show that it is the first scattering
based approach that consistently outperforms its conventional counterparts on a
diverse range of datasets. It is achieved with a novel inductive architecture
that embeds scattering features into the network flow using Hybrid Fusion
Blocks. We also demonstrate that the proposed design inherits the key property
of prior hybrid networks -- an effective generalisation in data-limited
scenarios. Our approach successfully combines the best of the two worlds:
flexibility and power of learnt features and stability and predictability of
scattering representations.",0.0466542,0.055362534,0.2402367,A
4028,"We further study the effectiveness of spatial attention and

                                                                7
       Models       Backbone                  RefCOCO                     RefCOCO+             RefCOCOg
                                         val testA testB             val testA testB      val-g val-u test-u
Two-stage            VGG16
                   ResNet-101              -    73.33         67.44    -    58.40  53.18  62.30    -      -
      VC [65]      ResNet-101            76.65  81.14         69.99  65.33  71.62  56.02    -    66.58  67.27
   MAttNet [61]                          78.82  82.71         73.94  66.95  71.29  58.40    -    68.89  68.67
   Ref-NMS [7]       VGG16                      76.60         66.40         64.00  53.40
  LGRANs [52]      ResNet-101              -    78.61         69.85    -    67.45  56.66  61.78    -      -
  RvG-Tree [22]    ResNet-101            75.06  83.14         71.32  63.51  73.65  58.03    -    66.95  66.51
CM-Att-Erase [31]  ResNet-101            78.35  81.21         70.09  68.09  72.02  57.52
   NMTree [30]                           76.41                       66.46                68.67    -      -
                   DarkNet-53                                                             64.62  65.87  66.44
One-stage            DLA-34
                                         72.54  74.35         68.50  56.81  60.23  49.60  56.12  61.33  60.36
    FAOA [58]      DarkNet-53              -    81.06         71.85    -    70.35  56.32    -      -    65.73
    RCCF [28]      DarkNet-53                   80.45         72.30         68.36  56.81                67.20
 ReSC-Large [57]   ResNet-101            77.63  81.09         76.55  63.59  68.46  58.43  63.12  67.30  68.91
     SAFF [59]     ResNet-101            79.26  83.12         75.51  64.43  72.53  59.09    -    68.94  69.08
    HFRN [40]      DarkNet-53            79.76  74.27         68.10  66.80  71.05  58.25    -    69.71  70.05
     ISRL [45]     ResNet-101                   82.91         74.15         73.38  59.49    -
  LBYL-Net [24]                            -    82.72         78.35    -    70.70  56.94           -      -
   TransVG [11]      Swin-S              79.67  84.01         79.83  68.64  70.19  56.47  62.70    -    67.73
 TransVG (Swin)                          81.02                       64.82                67.02  68.67  68.99
                     Swin-S              82.33  85.85         82.34  64.94  76.17  63.81  67.81  69.34
   QRNet (ours)                                                                                         72.52
                                         84.01                       72.94                71.89  73.03

Table 2.","We also notice that, when compared
with completely disabling QD-ATT, only enabling QD-ATT
in multiscale fusion gains little improvement because the
features from the transformer are still noise and query-
inconsistent.","The performance comparisons (Acc@0.5) on ReferCOCO, ReferCOCO+, and ReferCOCOg.",2022-03-29 11:17:23+00:00,Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Jiabo Ye'), arxiv.Result.Author('Junfeng Tian'), arxiv.Result.Author('Ming Yan'), arxiv.Result.Author('Xiaoshan Yang'), arxiv.Result.Author('Xuwu Wang'), arxiv.Result.Author('Ji Zhang'), arxiv.Result.Author('Liang He'), arxiv.Result.Author('Xin Lin')]","Visual grounding focuses on establishing fine-grained alignment between
vision and natural language, which has essential applications in multimodal
reasoning systems. Existing methods use pre-trained query-agnostic visual
backbones to extract visual feature maps independently without considering the
query information. We argue that the visual features extracted from the visual
backbones and the features really needed for multimodal reasoning are
inconsistent. One reason is that there are differences between pre-training
tasks and visual grounding. Moreover, since the backbones are query-agnostic,
it is difficult to completely avoid the inconsistency issue by training the
visual backbone end-to-end in the visual grounding framework. In this paper, we
propose a Query-modulated Refinement Network (QRNet) to address the
inconsistent issue by adjusting intermediate features in the visual backbone
with a novel Query-aware Dynamic Attention (QD-ATT) mechanism and query-aware
multiscale fusion. The QD-ATT can dynamically compute query-dependent visual
attention at the spatial and channel levels of the feature maps produced by the
visual backbone. We apply the QRNet to an end-to-end visual grounding
framework. Extensive experiments show that the proposed method outperforms
state-of-the-art methods on five widely used datasets.",0.22630054,0.056886278,-0.0029758606,A
4052,"We still need further research
                                        spatial-temporal separation attention mechanism.","                                        VPTR: EfÔ¨Åcient Transformers for Video Prediction

                                                            Xi Ye                                               Guillaume-Alexandre Bilodeau
                                        LITIV Laboratory, Polytechnique Montre¬¥al                          LITIV Laboratory, Polytechnique Montre¬¥al

                                                       Montre¬¥al, Canada                                                  Montre¬¥al, Canada
                                                   Email: xi.ye@polymtl.ca                                        Email: gabilodeau@polymtl.ca

arXiv:2203.15836v1 [cs.CV] 29 Mar 2022     Abstract‚ÄîIn this paper, we propose a new Transformer block      is computational expensive to apply Transformer to high
                                        for video future frames prediction based on an efÔ¨Åcient local      dimensional visual features.","Based on this     about more efÔ¨Åcient visual Transformers, especially for videos.",2022-03-29 18:09:09+00:00,VPTR: Efficient Transformers for Video Prediction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xi Ye'), arxiv.Result.Author('Guillaume-Alexandre Bilodeau')]","In this paper, we propose a new Transformer block for video future frames
prediction based on an efficient local spatial-temporal separation attention
mechanism. Based on this new Transformer block, a fully autoregressive video
future frames prediction Transformer is proposed. In addition, a
non-autoregressive video prediction Transformer is also proposed to increase
the inference speed and reduce the accumulated inference errors of its
autoregressive counterpart. In order to avoid the prediction of very similar
future frames, a contrastive feature loss is applied to maximize the mutual
information between predicted and ground-truth future frame features. This work
is the first that makes a formal comparison of the two types of attention-based
video future frames prediction models over different scenarios. The proposed
models reach a performance competitive with more complex state-of-the-art
models. The source code is available at \emph{https://github.com/XiYe20/VPTR}.",-0.19963652,0.073127925,0.062360052,B
4072,This can be an avenue for further research.,"Thanks to our disen-            disentanglement between different categories of appearance
tangled attributes transfer, our Ô¨Ånal swapped faces present             attributes.","the successful semantic transfer from the targets, with the
source identity well preserved.",2022-03-30 00:33:08+00:00,High-resolution Face Swapping via Latent Semantics Disentanglement,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yangyang Xu'), arxiv.Result.Author('Bailin Deng'), arxiv.Result.Author('Junle Wang'), arxiv.Result.Author('Yanqing Jing'), arxiv.Result.Author('Jia Pan'), arxiv.Result.Author('Shengfeng He')]","We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes.",-0.05821592,-0.081325084,-0.106813,C
4076,Copying once along the                         incomplete observations is worthy of further study.,formance gain than copying once.,"spatial dimension, the channel dimension and the tempo-
ral dimension are all better than not copying, while copying                    5.",2022-03-30 04:35:53+00:00,Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tiezheng Ma'), arxiv.Result.Author('Yongwei Nie'), arxiv.Result.Author('Chengjiang Long'), arxiv.Result.Author('Qing Zhang'), arxiv.Result.Author('Guiqing Li')]","This paper presents a high-quality human motion prediction method that
accurately predicts future human poses given observed ones. Our method is based
on the observation that a good initial guess of the future poses is very
helpful in improving the forecasting accuracy. This motivates us to propose a
novel two-stage prediction framework, including an init-prediction network that
just computes the good guess and then a formal-prediction network that predicts
the target future poses based on the guess. More importantly, we extend this
idea further and design a multi-stage prediction framework where each stage
predicts initial guess for the next stage, which brings more performance gain.
To fulfill the prediction task at each stage, we propose a network comprising
Spatial Dense Graph Convolutional Networks (S-DGCN) and Temporal Dense Graph
Convolutional Networks (T-DGCN). Alternatively executing the two networks helps
extract spatiotemporal features over the global receptive field of the whole
pose sequence. All the above design choices cooperating together make our
method outperform previous approaches by large margins: 6%-7% on Human3.6M,
5%-10% on CMU-MoCap, and 13%-16% on 3DPW.",0.3401049,0.05571369,0.044642728,A
4122,"We believe
attempted to learn PIAA from various perspectives, such                                                    the aforementioned dimensions can bring further research
as multi-modal collaborative learning [17], meta-learning                                                  opportunities in understanding correlation between PIAA
[21], multi-task learning [7], deep reinforcement learning                                                 and psychological feelings.","Later, research work                                                 judgment, 3) emotion, 4) willingness to share.","In addition, we also provide
[10] etc.",2022-03-31 02:23:46+00:00,Personalized Image Aesthetics Assessment with Rich Attributes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuzhe Yang'), arxiv.Result.Author('Liwu Xu'), arxiv.Result.Author('Leida Li'), arxiv.Result.Author('Nan Qie'), arxiv.Result.Author('Yaqian Li'), arxiv.Result.Author('Peng Zhang'), arxiv.Result.Author('Yandong Guo')]","Personalized image aesthetics assessment (PIAA) is challenging due to its
highly subjective nature. People's aesthetic tastes depend on diversified
factors, including image characteristics and subject characters. The existing
PIAA databases are limited in terms of annotation diversity, especially the
subject aspect, which can no longer meet the increasing demands of PIAA
research. To solve the dilemma, we conduct so far, the most comprehensive
subjective study of personalized image aesthetics and introduce a new
Personalized image Aesthetics database with Rich Attributes (PARA), which
consists of 31,220 images with annotations by 438 subjects. PARA features
wealthy annotations, including 9 image-oriented objective attributes and 4
human-oriented subjective attributes. In addition, desensitized subject
information, such as personality traits, is also provided to support study of
PIAA and user portraits. A comprehensive analysis of the annotation data is
provided and statistic study indicates that the aesthetic preferences can be
mirrored by proposed subjective attributes. We also propose a conditional PIAA
model by utilizing subject information as conditional prior. Experimental
results indicate that the conditional PIAA model can outperform the control
group, which is also the first attempt to demonstrate how image aesthetics and
subject characters interact to produce the intricate personalized tastes on
image aesthetics. We believe the database and the associated analysis would be
useful for conducting next-generation PIAA study. The project page of PARA can
be found at: https://cv-datasets.institutecv.com/#/data-sets.",0.1681734,-0.19001263,-0.21089518,A
4123,"We
                                                                                     further study the correlation among personality traits, emo-
                                                                                     tion, annotation difficulty and aesthetics attribute preference
                                                                                     to discover characteristics inside.","It can be observed that the correlation
9DULDQFH                                                                             such as personality traits and photographic experience.","For intuitive observation,
                                                                                     we show the personality traits of different subjects together
                                                                                     with their aesthetics judgements on sample image in Figure
                                                                                     7.",2022-03-31 02:23:46+00:00,Personalized Image Aesthetics Assessment with Rich Attributes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuzhe Yang'), arxiv.Result.Author('Liwu Xu'), arxiv.Result.Author('Leida Li'), arxiv.Result.Author('Nan Qie'), arxiv.Result.Author('Yaqian Li'), arxiv.Result.Author('Peng Zhang'), arxiv.Result.Author('Yandong Guo')]","Personalized image aesthetics assessment (PIAA) is challenging due to its
highly subjective nature. People's aesthetic tastes depend on diversified
factors, including image characteristics and subject characters. The existing
PIAA databases are limited in terms of annotation diversity, especially the
subject aspect, which can no longer meet the increasing demands of PIAA
research. To solve the dilemma, we conduct so far, the most comprehensive
subjective study of personalized image aesthetics and introduce a new
Personalized image Aesthetics database with Rich Attributes (PARA), which
consists of 31,220 images with annotations by 438 subjects. PARA features
wealthy annotations, including 9 image-oriented objective attributes and 4
human-oriented subjective attributes. In addition, desensitized subject
information, such as personality traits, is also provided to support study of
PIAA and user portraits. A comprehensive analysis of the annotation data is
provided and statistic study indicates that the aesthetic preferences can be
mirrored by proposed subjective attributes. We also propose a conditional PIAA
model by utilizing subject information as conditional prior. Experimental
results indicate that the conditional PIAA model can outperform the control
group, which is also the first attempt to demonstrate how image aesthetics and
subject characters interact to produce the intricate personalized tastes on
image aesthetics. We believe the database and the associated analysis would be
useful for conducting next-generation PIAA study. The project page of PARA can
be found at: https://cv-datasets.institutecv.com/#/data-sets.",0.076228976,-0.06288414,-0.22989476,C
4142,"For instance, this can facilitate
       further research on the provision of Smart City services that can automatically detect whether a
       street light is ON or OFF, alert a maintenance team, and reduce the person-hours required for
       on-site monitoring.","‚Ä¢ By building upon the provided images, experts from the Ô¨Åelds of Computer Vision, Internet of
       Things (IoT), and Smart Cities can develop and train ML models and heuristic tools that assess the
       status of the street and emergency lights in real-time (e.g., as in [5]).","‚Ä¢ Deep neural network training and the regularisation of the models produced can beneÔ¨Åt from
       pre-training models [6].",2022-03-31 09:36:07+00:00,A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ioannis Mavromatis'), arxiv.Result.Author('Aleksandar Stanoev'), arxiv.Result.Author('Pietro Carnelli'), arxiv.Result.Author('Yichao Jin'), arxiv.Result.Author('Mahesh Sooriyabandara'), arxiv.Result.Author('Aftab Khan')]","A dataset of street light images is presented. Our dataset consists of
$\sim350\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the
South Gloucestershire region in the UK. Each UMBRELLA node is installed on the
pole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing
upwards towards the sky and lamppost light bulb. Each node collects an image at
hourly intervals for 24h every day. The data collection spans for a period of
six months.",-0.20508309,-0.1168533,0.0102848895,B
4143,"For instance, this can facilitate
       further research on the provision of Smart City services that can automatically detect whether a
       street light is ON or OFF, alert a maintenance team, and reduce the person-hours required for
       on-site monitoring.","‚Ä¢ By building upon the provided images, experts from the Ô¨Åelds of Computer Vision, Internet of
       Things (IoT), and Smart Cities can develop and train ML models and heuristic tools that assess the
       status of the street and emergency lights in real-time (e.g., as in [6]).","‚Ä¢ Deep neural network training and the regularisation of the models produced can beneÔ¨Åt from
       pre-training models [7].",2022-03-31 09:36:07+00:00,A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ioannis Mavromatis'), arxiv.Result.Author('Aleksandar Stanoev'), arxiv.Result.Author('Pietro Carnelli'), arxiv.Result.Author('Yichao Jin'), arxiv.Result.Author('Mahesh Sooriyabandara'), arxiv.Result.Author('Aftab Khan')]","A dataset of street light images is presented. Our dataset consists of
$\sim350\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the
South Gloucestershire region in the UK. Each UMBRELLA node is installed on the
pole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing
upwards towards the sky and lamppost light bulb. Each node collects an image at
hourly intervals for 24h every day. The data collection spans for a period of
six months.
  Each image taken is logged as a single entry in the dataset along with the
Global Positioning System (GPS) coordinates of the lamppost. All entries in the
dataset have been post-processed and labelled based on the operation of the
lamppost, i.e., whether the lamppost is switched ON or OFF. The dataset can be
used to train deep neural networks and generate pre-trained models providing
feature representations for smart city CCTV applications, smart weather
detection algorithms, or street infrastructure monitoring. The dataset can be
found at \url{https://doi.org/10.5281/zenodo.6046758}.",-0.20570841,-0.11629793,0.009147637,B
4144,"For instance, this can facilitate
       further research on the provision of Smart City services that can automatically detect whether a
       street light is ON or OFF, alert a maintenance team, and reduce the person-hours required for
       on-site monitoring.","‚Ä¢ By building upon the provided images, experts from the Ô¨Åelds of Computer Vision, Internet of
       Things (IoT), and Smart Cities can develop and train ML models and heuristic tools that assess the
       status of the street and emergency lights in real-time (e.g., as in [7]).","‚Ä¢ Deep neural network training and the regularisation of the models produced can beneÔ¨Åt from
       pre-training models [8].",2022-03-31 09:36:07+00:00,A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ioannis Mavromatis'), arxiv.Result.Author('Aleksandar Stanoev'), arxiv.Result.Author('Pietro Carnelli'), arxiv.Result.Author('Yichao Jin'), arxiv.Result.Author('Mahesh Sooriyabandara'), arxiv.Result.Author('Aftab Khan')]","A dataset of street light images is presented. Our dataset consists of
$\sim350\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the
South Gloucestershire region in the UK. Each UMBRELLA node is installed on the
pole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing
upwards towards the sky and lamppost light bulb. Each node collects an image at
hourly intervals for 24h every day. The data collection spans for a period of
six months.
  Each image taken is logged as a single entry in the dataset along with the
Global Positioning System (GPS) coordinates of the lamppost. All entries in the
dataset have been post-processed and labelled based on the operation of the
lamppost, i.e., whether the lamppost is switched ON or OFF. The dataset can be
used to train deep neural networks and generate pre-trained models providing
feature representations for smart city CCTV applications, smart weather
detection algorithms, or street infrastructure monitoring. The dataset can be
found at \url{https://doi.org/10.5281/zenodo.6046758}.",-0.20768833,-0.118351184,0.012487177,B
4159,"The spatial alignment operation and the learning
                                                                                                                      targets should be carefully designed to cooperate with the
available for further research1.",The source code is publicly                                                                 formance.,"elegant framework so that the velocity prediction task can
                                                                                                                      be simpliÔ¨Åed and superior generalization performance can
1.",2022-03-31 14:21:19+00:00,BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junjie Huang'), arxiv.Result.Author('Guan Huang')]","Single frame data contains finite information which limits the performance of
the existing vision-based multi-camera 3D object detection paradigms. For
fundamentally pushing the performance boundary in this area, a novel paradigm
dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the
spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive
BEVDet framework with a few modifications just for fusing the feature from the
previous frame with the corresponding one in the current frame. In this way,
with negligible additional computing budget, we enable BEVDet4D to access the
temporal cues by querying and comparing the two candidate features. Beyond
this, we simplify the task of velocity prediction by removing the factors of
ego-motion and time in the learning target. As a result, BEVDet4D with robust
generalization performance reduces the velocity error by up to -62.9%. This
makes the vision-based methods, for the first time, become comparable with
those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes,
we report a new record of 54.5% NDS with the high-performance configuration
dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base
by +7.3% NDS. The source code is publicly available for further research at
https://github.com/HuangJunJie2017/BEVDet .",-0.005864905,0.10017362,-0.05466469,B
4180,"We hope that our unique ndings will spur further research into: 1) better
understanding pixel space perturbations, and why they are so e ective at steering deep networks, and 2)
developing better visual prompts that further add to our repertoire of mechanisms for creating exible
and adaptable vision systems.","Through various experiments across pre-trained
models of di erent architectures and modalities, we have demonstrated that prompting in pixel space
indeed works successfully.","Acknowledgements

We would like to thank Lucy Chai, Caroline Chan, Joanna Materzynska, Xavier Puig Fernandez, Miny-
oung Huh, Tongzhou Wang, and Yen-Chen Lin for proofreading the paper.",2022-03-31 17:59:30+00:00,Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hyojin Bahng'), arxiv.Result.Author('Ali Jahanian'), arxiv.Result.Author('Swami Sankaranarayanan'), arxiv.Result.Author('Phillip Isola')]","Prompting has recently become a popular paradigm for adapting language models
to downstream tasks. Rather than fine-tuning model parameters or adding
task-specific heads, this approach steers a model to perform a new task simply
by adding a text prompt to the model's inputs. In this paper, we explore the
question: can we create prompts with pixels instead? In other words, can
pre-trained vision models be adapted to a new task solely by adding pixels to
their inputs? We introduce visual prompting, which learns a task-specific image
perturbation such that a frozen pre-trained model prompted with this
perturbation performs a new task. We discover that changing only a few pixels
is enough to adapt models to new tasks and datasets, and performs on par with
linear probing, the current de facto approach to lightweight adaptation. The
surprising effectiveness of visual prompting provides a new perspective on how
to adapt pre-trained models in vision, and opens up the possibility of adapting
models solely through their inputs, which, unlike model parameters or outputs,
are typically under an end-user's control. Code is available at
http://hjbahng.github.io/visual_prompting .",-0.19903699,-0.106771186,0.15252341,C
4203,"This requires further study on topics such as: attention mechanism
on text regions, end-to-end framework for text spotting based product recognition, and one-shot
learning based sequence matching.","Some products in the dataset do not contain texts (2%), and many words
are partially or fully illegible.","6 CONCLUSIONS

In this work, we introduce the United Retail Datasets (Unitail), a large-scale benchmark aims at sup-
porting well-aligned textually enhanced scene product recognition.",2022-04-01 09:06:48+00:00,"Unitail: Detecting, Reading, and Matching in Retail Scene",cs.CV,['cs.CV'],"[arxiv.Result.Author('Fangyi Chen'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Zaiwang Li'), arxiv.Result.Author('Jiachen Dou'), arxiv.Result.Author('Shentong Mo'), arxiv.Result.Author('Hao Chen'), arxiv.Result.Author('Yongxin Zhang'), arxiv.Result.Author('Uzair Ahmed'), arxiv.Result.Author('Chenchen Zhu'), arxiv.Result.Author('Marios Savvides')]","To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.",-0.18676755,-0.23704761,-0.15920234,C
4204,"This requires further study on topics such as: attention mechanism
on text regions, end-to-end framework for text spotting based product recognition, and one-shot
learning based sequence matching.","Some products in the dataset do not contain texts (2%), and many words
are partially or fully illegible.","6 CONCLUSIONS

In this work, we introduce the United Retail Datasets (Unitail), a large-scale benchmark aims at sup-
porting well-aligned textually enhanced scene product recognition.",2022-04-01 09:06:48+00:00,"Unitail: Detecting, Reading, and Matching in Retail Scene",cs.CV,['cs.CV'],"[arxiv.Result.Author('Fangyi Chen'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Zaiwang Li'), arxiv.Result.Author('Jiachen Dou'), arxiv.Result.Author('Shentong Mo'), arxiv.Result.Author('Hao Chen'), arxiv.Result.Author('Yongxin Zhang'), arxiv.Result.Author('Uzair Ahmed'), arxiv.Result.Author('Chenchen Zhu'), arxiv.Result.Author('Marios Savvides')]","To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.",-0.18676755,-0.23704761,-0.15920234,C
4205,"This requires further study on topics such
as: attention mechanism on text regions, end-to-end framework for text spotting
based product recognition, and one-shot learning based sequence matching.","Some products in the dataset do not contain texts (2%), and many
words are partially or fully illegible.","6 Conclusions

In this work, we introduce the United Retail Datasets (Unitail), a large-scale
benchmark aims at supporting well-aligned textually enhanced scene product
Unitail: Detecting, Reading, and Matching in Retail Scene  15

recognition.",2022-04-01 09:06:48+00:00,"Unitail: Detecting, Reading, and Matching in Retail Scene",cs.CV,['cs.CV'],"[arxiv.Result.Author('Fangyi Chen'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Zaiwang Li'), arxiv.Result.Author('Jiachen Dou'), arxiv.Result.Author('Shentong Mo'), arxiv.Result.Author('Hao Chen'), arxiv.Result.Author('Yongxin Zhang'), arxiv.Result.Author('Uzair Ahmed'), arxiv.Result.Author('Chenchen Zhu'), arxiv.Result.Author('Marios Savvides')]","To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.",-0.19871444,-0.22588113,-0.1840904,C
4206,"This requires further study on topics such
as: attention mechanism on text regions, end-to-end framework for text spotting
based product recognition, and one-shot learning based sequence matching.","Some products in the dataset do not contain texts (2%), and many
words are partially or fully illegible.","6 Conclusions

In this work, we introduce the United Retail Datasets (Unitail), a large-scale
benchmark aims at supporting well-aligned textually enhanced scene product
recognition.",2022-04-01 09:06:48+00:00,"Unitail: Detecting, Reading, and Matching in Retail Scene",cs.CV,['cs.CV'],"[arxiv.Result.Author('Fangyi Chen'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Zaiwang Li'), arxiv.Result.Author('Jiachen Dou'), arxiv.Result.Author('Shentong Mo'), arxiv.Result.Author('Hao Chen'), arxiv.Result.Author('Yongxin Zhang'), arxiv.Result.Author('Uzair Ahmed'), arxiv.Result.Author('Chenchen Zhu'), arxiv.Result.Author('Marios Savvides')]","To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.",-0.1991054,-0.23361218,-0.17344442,C
4223,"However, with our current work
on newer, improved YOLO architectures as well as the recent development on pruning methods, this aspect is an
interesting topic for further research to further increase inference performance w.r.t.","Due
to time constraints during training, we have not investigated model pruning yet.",time on edge devices.,2022-04-01 17:38:50+00:00,Fast and Automatic Object Registration for Human-Robot Collaboration in Industrial Manufacturing,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Manuela Gei√ü'), arxiv.Result.Author('Martin Baresch'), arxiv.Result.Author('Georgios Chasparis'), arxiv.Result.Author('Edwin Schweiger'), arxiv.Result.Author('Nico Teringl'), arxiv.Result.Author('Michael Zwick')]","We present an end-to-end framework for fast retraining of object detection
models in human-robot-collaboration. Our Faster R-CNN based setup covers the
whole workflow of automatic image generation and labeling, model retraining
on-site as well as inference on a FPGA edge device. The intervention of a human
operator reduces to providing the new object together with its label and
starting the training process. Moreover, we present a new loss, the
intraspread-objectosphere loss, to tackle the problem of open world
recognition. Though it fails to completely solve the problem, it significantly
reduces the number of false positive detections of unknown objects.",0.08619887,-0.08940738,0.26022977,A
4263,"Finally,          dataset will encourage further research in this area and provide
scenario 3 seems to be the middle ground for OAD, which              the required momentum for the development of more models
was unexpected.","We believe that the proposed
core dataset would provide a boost in the metrics.","This may be attributed to the ‚Äúbidirectional‚Äù        and approaches, especially as far as panoptic segmentation is
element of the LSTM cells which results in better performance        concerned where research studies focused on Remote Sensing
since it enables them to learn in both directions.",2022-04-02 23:14:46+00:00,"A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning",cs.CV,['cs.CV'],"[arxiv.Result.Author('Dimitrios Sykas'), arxiv.Result.Author('Maria Sdraka'), arxiv.Result.Author('Dimitrios Zografakis'), arxiv.Result.Author('Ioannis Papoutsis')]","In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi
country benchmark dataset, tailored for agricultural monitoring applications
with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer
declarations collected via the Land Parcel Identification System (LPIS) for
harmonizing country wide labels. These declarations have only recently been
made available as open data, allowing for the first time the labeling of
satellite imagery from ground truth data. We proceed to propose and standardise
a new crop type taxonomy across Europe that address Common Agriculture Policy
(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative
Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year
dataset that includes all spectral information. It is constructed to cover the
period 2016-2020 for Catalonia and France, while it can be extended to include
additional countries. Currently, it contains 42.5 million parcels, which makes
it significantly larger than other available archives. We extract two
sub-datasets to highlight its value for diverse Deep Learning applications; the
Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD
capitalizes zonal statistics of each parcel, thus creating a powerful
label-to-features instance for classification algorithms. On the other hand,
PAD structure generalizes the classification problem to parcel extraction and
semantic segmentation and labeling. The PAD and OAD are examined under three
different scenarios to showcase and model the effects of spatial and temporal
variability across different years and different countries.",-0.07570266,-0.03424278,0.009601693,B
4264,"Finally,          dataset will encourage further research in this area and provide
scenario 3 seems to be the middle ground for OAD, which              the required momentum for the development of more models
was unexpected.","We believe that the proposed
core dataset would provide a boost in the metrics.","This may be attributed to the ‚Äúbidirectional‚Äù        and approaches, especially as far as panoptic segmentation is
element of the LSTM cells which results in better performance        concerned where research studies focused on Remote Sensing
since it enables them to learn in both directions.",2022-04-02 23:14:46+00:00,"A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning",cs.CV,['cs.CV'],"[arxiv.Result.Author('Dimitrios Sykas'), arxiv.Result.Author('Maria Sdraka'), arxiv.Result.Author('Dimitrios Zografakis'), arxiv.Result.Author('Ioannis Papoutsis')]","In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi
country benchmark dataset, tailored for agricultural monitoring applications
with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer
declarations collected via the Land Parcel Identification System (LPIS) for
harmonizing country wide labels. These declarations have only recently been
made available as open data, allowing for the first time the labeling of
satellite imagery from ground truth data. We proceed to propose and standardise
a new crop type taxonomy across Europe that address Common Agriculture Policy
(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative
Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year
dataset that includes all spectral information. It is constructed to cover the
period 2016-2020 for Catalonia and France, while it can be extended to include
additional countries. Currently, it contains 42.5 million parcels, which makes
it significantly larger than other available archives. We extract two
sub-datasets to highlight its value for diverse Deep Learning applications; the
Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD
capitalizes zonal statistics of each parcel, thus creating a powerful
label-to-features instance for classification algorithms. On the other hand,
PAD structure generalizes the classification problem to parcel extraction and
semantic segmentation and labeling. The PAD and OAD are examined under three
different scenarios to showcase and model the effects of spatial and temporal
variability across different years and different countries.",-0.07570266,-0.03424278,0.009601693,B
4270,"We further study the distillation method introduced in [54], i.e., transferring
the learned knowledge in a ViT model using a CNN teacher.","It may be
because cascading self-attention blocks in ViT models is equivalent to repeatedly
applying a low-pass filter, corresponding to the theoretical justification in [58],
while CNN models utilizing convolution operations behave like a series of high-
pass filters [44] to catch more high-frequency components [57].","The results in
Figure 1 show that the improvement of KD (from 82.0% to 83.6%) is primarily
attributed to the stronger ability to exploit the high-frequency components of
images.",2022-04-03 05:16:51+00:00,Improving Vision Transformers by Revisiting High-frequency Components,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiawang Bai'), arxiv.Result.Author('Li Yuan'), arxiv.Result.Author('Shu-Tao Xia'), arxiv.Result.Author('Shuicheng Yan'), arxiv.Result.Author('Zhifeng Li'), arxiv.Result.Author('Wei Liu')]","The transformer models have shown promising effectiveness in dealing with
various vision tasks. However, compared with training Convolutional Neural
Network (CNN) models, training Vision Transformer (ViT) models is more
difficult and relies on the large-scale training set. To explain this
observation we make a hypothesis that ViT models are less effective in
capturing the high-frequency components of images than CNN models, and verify
it by a frequency analysis. Inspired by this finding, we first investigate the
effects of existing techniques for improving ViT models from a new frequency
perspective, and find that the success of some techniques (e.g., RandAugment)
can be attributed to the better usage of the high-frequency components. Then,
to compensate for this insufficient ability of ViT models, we propose HAT,
which directly augments high-frequency components of images via adversarial
training. We show that HAT can consistently boost the performance of various
ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance
the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the
superiority can also be maintained on out-of-distribution data and transferred
to downstream tasks.",-0.019481238,-0.26553273,0.24409017,C
4271,"We further study the distillation method introduced in [54], i.e., transferring
the learned knowledge in a ViT model using a CNN teacher.","It may be
because cascading self-attention blocks in ViT models is equivalent to repeatedly
applying a low-pass filter, corresponding to the theoretical justification in [58],
while CNN models utilizing convolution operations behave like a series of high-
pass filters [44] to catch more high-frequency components [57].","We use a RegNetY-
16GF model [46] as a teacher with the hard-label distillation, and adopt all
settings in [54].",2022-04-03 05:16:51+00:00,Improving Vision Transformers by Revisiting High-frequency Components,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiawang Bai'), arxiv.Result.Author('Li Yuan'), arxiv.Result.Author('Shu-Tao Xia'), arxiv.Result.Author('Shuicheng Yan'), arxiv.Result.Author('Zhifeng Li'), arxiv.Result.Author('Wei Liu')]","The transformer models have shown promising effectiveness in dealing with
various vision tasks. However, compared with training Convolutional Neural
Network (CNN) models, training Vision Transformer (ViT) models is more
difficult and relies on the large-scale training set. To explain this
observation we make a hypothesis that \textit{ViT models are less effective in
capturing the high-frequency components of images than CNN models}, and verify
it by a frequency analysis. Inspired by this finding, we first investigate the
effects of existing techniques for improving ViT models from a new frequency
perspective, and find that the success of some techniques (e.g., RandAugment)
can be attributed to the better usage of the high-frequency components. Then,
to compensate for this insufficient ability of ViT models, we propose HAT,
which directly augments high-frequency components of images via adversarial
training. We show that HAT can consistently boost the performance of various
ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance
the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the
superiority can also be maintained on out-of-distribution data and transferred
to downstream tasks. The code is available at:
https://github.com/jiawangbai/HAT.",0.016445935,-0.35940325,0.2316993,C
4272,"We further study the distillation method introduced in [62], i.e., transferring
the learned knowledge in a ViT model using a CNN teacher.","It may be
because cascading self-attention blocks in ViT models is equivalent to repeatedly
applying a low-pass filter, corresponding to the theoretical justification in [67],
while CNN models utilizing convolution operations behave like a series of high-
pass filters [50] to catch more high-frequency components [66].","We use a RegNetY-
16GF model [54] as a teacher with the hard-label distillation, and adopt all
settings in [62].",2022-04-03 05:16:51+00:00,Improving Vision Transformers by Revisiting High-frequency Components,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiawang Bai'), arxiv.Result.Author('Li Yuan'), arxiv.Result.Author('Shu-Tao Xia'), arxiv.Result.Author('Shuicheng Yan'), arxiv.Result.Author('Zhifeng Li'), arxiv.Result.Author('Wei Liu')]","The transformer models have shown promising effectiveness in dealing with
various vision tasks. However, compared with training Convolutional Neural
Network (CNN) models, training Vision Transformer (ViT) models is more
difficult and relies on the large-scale training set. To explain this
observation we make a hypothesis that \textit{ViT models are less effective in
capturing the high-frequency components of images than CNN models}, and verify
it by a frequency analysis. Inspired by this finding, we first investigate the
effects of existing techniques for improving ViT models from a new frequency
perspective, and find that the success of some techniques (e.g., RandAugment)
can be attributed to the better usage of the high-frequency components. Then,
to compensate for this insufficient ability of ViT models, we propose HAT,
which directly augments high-frequency components of images via adversarial
training. We show that HAT can consistently boost the performance of various
ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance
the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the
superiority can also be maintained on out-of-distribution data and transferred
to downstream tasks. The code is available at:
https://github.com/jiawangbai/HAT.",0.018405482,-0.3583508,0.23055199,C
4276,"However, these datasets all     rics for most of the tasks to facilitate further research.","We provide baselines and met-
datasets [4, 8, 19, 24, 33, 39, 58].","focus on the traffic scenes, where most objects on the roads         3) We propose a novel method to enhance LiDAR-based
are vehicles and pedestrians are distributed sparsely, which         pedestrian perception in crowded scenes and achieve state-
limits the exploration and evaluation of learning-based per-         of-the-art performance on the STCrowd benchmark.",2022-04-03 08:26:07+00:00,STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Peishan Cong'), arxiv.Result.Author('Xinge Zhu'), arxiv.Result.Author('Feng Qiao'), arxiv.Result.Author('Yiming Ren'), arxiv.Result.Author('Xidong Peng'), arxiv.Result.Author('Yuenan Hou'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Ruigang Yang'), arxiv.Result.Author('Dinesh Manocha'), arxiv.Result.Author('Yuexin Ma')]","Accurately detecting and tracking pedestrians in 3D space is challenging due
to large variations in rotations, poses and scales. The situation becomes even
worse for dense crowds with severe occlusions. However, existing benchmarks
either only provide 2D annotations, or have limited 3D annotations with
low-density pedestrian distribution, making it difficult to build a reliable
pedestrian perception system especially in crowded scenes. To better evaluate
pedestrian perception algorithms in crowded scenarios, we introduce a
large-scale multimodal dataset,STCrowd. Specifically, in STCrowd, there are a
total of 219 K pedestrian instances and 20 persons per frame on average, with
various levels of occlusion. We provide synchronized LiDAR point clouds and
camera images as well as their corresponding 3D labels and joint IDs. STCrowd
can be used for various tasks, including LiDAR-only, image-only, and
sensor-fusion based pedestrian detection and tracking. We provide baselines for
most of the tasks. In addition, considering the property of sparse global
distribution and density-varying local distribution of pedestrians, we further
propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA),
to enhance pedestrian perception in crowded scenes. Extensive experiments show
that our new method achieves state-of-the-art performance for pedestrian
detection on various datasets.",-0.2869978,0.06956541,-0.06306809,B
4277,"Apolloscape Detec-
paper to facilitate further research.",We also provide baselines for most of the tasks in this        scale datasets with 3D annotations.,"tion/Tracking [30] is a challenging urban traffic scenes
                                                                     dataset.",2022-04-03 08:26:07+00:00,STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Peishan Cong'), arxiv.Result.Author('Xinge Zhu'), arxiv.Result.Author('Feng Qiao'), arxiv.Result.Author('Yiming Ren'), arxiv.Result.Author('Xidong Peng'), arxiv.Result.Author('Yuenan Hou'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Ruigang Yang'), arxiv.Result.Author('Dinesh Manocha'), arxiv.Result.Author('Yuexin Ma')]","Accurately detecting and tracking pedestrians in 3D space is challenging due
to large variations in rotations, poses and scales. The situation becomes even
worse for dense crowds with severe occlusions. However, existing benchmarks
either only provide 2D annotations, or have limited 3D annotations with
low-density pedestrian distribution, making it difficult to build a reliable
pedestrian perception system especially in crowded scenes. To better evaluate
pedestrian perception algorithms in crowded scenarios, we introduce a
large-scale multimodal dataset,STCrowd. Specifically, in STCrowd, there are a
total of 219 K pedestrian instances and 20 persons per frame on average, with
various levels of occlusion. We provide synchronized LiDAR point clouds and
camera images as well as their corresponding 3D labels and joint IDs. STCrowd
can be used for various tasks, including LiDAR-only, image-only, and
sensor-fusion based pedestrian detection and tracking. We provide baselines for
most of the tasks. In addition, considering the property of sparse global
distribution and density-varying local distribution of pedestrians, we further
propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA),
to enhance pedestrian perception in crowded scenes. Extensive experiments show
that our new method achieves state-of-the-art performance for pedestrian
detection on various datasets.",-0.27289313,0.17590202,-0.03095884,B
4292,"Adding 3D image data may decrease the accuracy, we believe this happens when the rendered
   images are less representees of the real image distributions, but this requires further study.",5 .,"Tools
   tha t a na lyze the rea l da ta a nd suggest the pa ra meters of the 3D ima ges is a clea r next step in this
   work.",2022-04-03 16:35:54+00:00,Adjusting for Bias with Procedural Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shesh Narayan Gupta'), arxiv.Result.Author('Nicholas Bear Brown')]","3D softwares are now capable of producing highly realistic images that look
nearly indistinguishable from the real images. This raises the question: can
real datasets be enhanced with 3D rendered data? We investigate this question.
In this paper we demonstrate the use of 3D rendered data, procedural, data for
the adjustment of bias in image datasets. We perform error analysis of images
of animals which shows that the misclassification of some animal breeds is
largely a data issue. We then create procedural images of the poorly classified
breeds and that model further trained on procedural data can better classify
poorly performing breeds on real data. We believe that this approach can be
used for the enhancement of visual data for any underrepresented group,
including rare diseases, or any data bias potentially improving the accuracy
and fairness of models. We find that the resulting representations rival or
even out-perform those learned directly from real data, but that good
performance requires care in the 3D rendered procedural data generation. 3D
image dataset can be viewed as a compressed and organized copy of a real
dataset, and we envision a future where more and more procedural data
proliferate while datasets become increasingly unwieldy, missing, or private.
This paper suggests several techniques for dealing with visual representation
learning in such a future.",-0.050097268,0.34372342,0.1552275,B
4293,"Adding 3D image data may decrease the accuracy, we believe this happens when the rendered
   images are less representees of the real image distributions, but this requires further study.",5 .,"Tools
   tha t a na lyze the rea l da ta a nd suggest the pa ra meters of the 3D ima ges is a clea r next step in this
   work.",2022-04-03 16:35:54+00:00,Adjusting for Bias with Procedural Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shesh Narayan Gupta'), arxiv.Result.Author('Nicholas Bear Brown')]","3D softwares are now capable of producing highly realistic images that look
nearly indistinguishable from the real images. This raises the question: can
real datasets be enhanced with 3D rendered data? We investigate this question.
In this paper we demonstrate the use of 3D rendered data, procedural, data for
the adjustment of bias in image datasets. We perform error analysis of images
of animals which shows that the misclassification of some animal breeds is
largely a data issue. We then create procedural images of the poorly classified
breeds and that model further trained on procedural data can better classify
poorly performing breeds on real data. We believe that this approach can be
used for the enhancement of visual data for any underrepresented group,
including rare diseases, or any data bias potentially improving the accuracy
and fairness of models. We find that the resulting representations rival or
even out-perform those learned directly from real data, but that good
performance requires care in the 3D rendered procedural data generation. 3D
image dataset can be viewed as a compressed and organized copy of a real
dataset, and we envision a future where more and more procedural data
proliferate while datasets become increasingly unwieldy, missing, or private.
This paper suggests several techniques for dealing with visual representation
learning in such a future.",-0.050097268,0.34372342,0.1552275,B
4320,"This leads us to further study a domain-incremental (DI) scenario;
where the model must adapt to new environments, without forgetting how to perform well in those previously seen.","The
VOC and COCO X+Y benchmarks are specifically designed to test continual detection methods, but they fall some-
what short of representing a real-world setting.","We test the fine-tuning baseline, Faster-ILOD and our extension on a recent industrial-scale object detection dataset for
autonomous driving; SODA 10M [11].",2022-04-04 11:50:54+00:00,Re-examining Distillation For Continual Object Detection,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Eli Verwimp'), arxiv.Result.Author('Kuo Yang'), arxiv.Result.Author('Sarah Parisot'), arxiv.Result.Author('Hong Lanqing'), arxiv.Result.Author('Steven McDonagh'), arxiv.Result.Author('Eduardo P√©rez-Pellitero'), arxiv.Result.Author('Matthias De Lange'), arxiv.Result.Author('Tinne Tuytelaars')]","Training models continually to detect and classify objects, from new classes
and new domains, remains an open problem. In this work, we conduct a thorough
analysis of why and how object detection models forget catastrophically. We
focus on distillation-based approaches in two-stage networks; the most-common
strategy employed in contemporary continual object detection work.Distillation
aims to transfer the knowledge of a model trained on previous tasks -- the
teacher -- to a new model -- the student -- while it learns the new task. We
show that this works well for the region proposal network, but that wrong, yet
overly confident teacher predictions prevent student models from effective
learning of the classification head. Our analysis provides a foundation that
allows us to propose improvements for existing techniques by detecting
incorrect teacher predictions, based on current ground-truth labels, and by
employing an adaptive Huber loss as opposed to the mean squared error for the
distillation loss in the classification heads. We evidence that our strategy
works not only in a class incremental setting, but also in domain incremental
settings, which constitute a realistic context, likely to be the setting of
representative real-world problems.",-0.26727736,-0.057801284,0.0733234,B
4334,"Therefore, based on the results reported in this study, it can be argued that Capsule
Neural Networks are a promising approach for drowsiness detection and are worth
further research and development.","On the other hand, since the most challenging step in the development of Deep
Learning applications is the training process, which requires high computational re-
sources (characteristics of the used machine for training are described in Section 4), af-
ter this step, the already trained model can be easily imported into a portable computer
or mobile phone, since the trained model does not require any speciÔ¨Åc overwhelming
requirements to operate, allowing the implementation of a wearable drowsiness identi-
Ô¨Åcation system.","27
8.",2022-04-04 17:23:53+00:00,A Novel Capsule Neural Network Based Model for Drowsiness Detection Using Electroencephalography Signals,cs.CV,['cs.CV'],"[arxiv.Result.Author('Luis Guarda'), arxiv.Result.Author('Juan Tapia'), arxiv.Result.Author('Enrique Lopez Droguett'), arxiv.Result.Author('Marcelo Ramos')]","The early detection of drowsiness has become vital to ensure the correct and
safe development of several industries' tasks. Due to the transient mental
state of a human subject between alertness and drowsiness, automated drowsiness
detection is a complex problem to tackle. The electroencephalography signals
allow us to record variations in an individual's brain's electrical potential,
where each of them gives specific information about a subject's mental state.
However, due to this type of signal's nature, its acquisition, in general, is
complex, so it is hard to have a large volume of data to apply techniques of
Deep Learning for processing and classification optimally. Nevertheless,
Capsule Neural Networks are a brand-new Deep Learning algorithm proposed for
work with reduced amounts of data. It is a robust algorithm to handle the
data's hierarchical relationships, which is an essential characteristic for
work with biomedical signals. Therefore, this paper presents a Deep
Learning-based method for drowsiness detection with CapsNet by using a
concatenation of spectrogram images of the electroencephalography signals
channels. The proposed CapsNet model is compared with a Convolutional Neural
Network, which is outperformed by the proposed model, which obtains an average
accuracy of 86,44% and 87,57% of sensitivity against an average accuracy of
75,86% and 79,47% sensitivity for the CNN, showing that CapsNet is more
suitable for this kind of datasets and tasks.",-0.022670114,-0.16304019,0.073167585,C
4380,"While our
method produces promising results, it is merely the Ô¨Årst step in this direction
and further research eÔ¨Äorts are required to improve the image quality to enable
practical applications.","Improved mask prediction
[38,24] and context awareness [9], both of which are recent improvements of the
generator, may as well improve the image quality for our application.","5 Conclusion

In this paper, we introduced dense text-to-image (DT2I) synthesis as a new task
and proposed DTC-GAN, a novel method that generates images from multi-
ple free-form region descriptions.",2022-04-05 07:57:11+00:00,DT2I: Dense Text-to-Image Generation from Region Descriptions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('Prateek Bansal'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Andreas Dengel')]","Despite astonishing progress, generating realistic images of complex scenes
remains a challenging problem. Recently, layout-to-image synthesis approaches
have attracted much interest by conditioning the generator on a list of
bounding boxes and corresponding class labels. However, previous approaches are
very restrictive because the set of labels is fixed a priori. Meanwhile,
text-to-image synthesis methods have substantially improved and provide a
flexible way for conditional image generation. In this work, we introduce dense
text-to-image (DT2I) synthesis as a new task to pave the way toward more
intuitive image generation. Furthermore, we propose DTC-GAN, a novel method to
generate images from semantically rich region descriptions, and a multi-modal
region feature matching loss to encourage semantic image-text matching. Our
results demonstrate the capability of our approach to generate plausible images
of complex scenes using region captions.",-0.14731169,-0.03510339,0.15961158,C
4383,"ing, however, the design of projectors requires further study
(iii) We create FVgNET, the largest publicly available dataset   to identify their optimal number and response.","In hyperspectral imag-
state-of-the-art, reporting the highest performance to date.","Human eyes
of 317 samples of labeled hyperspectral images for semantic      are not the best imaging apparatus for every possible real-
segmentation and classiÔ¨Åcation.",2022-04-05 09:52:51+00:00,Real-time Hyperspectral Imaging in Hardware via Trained Metasurface Encoders,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Maksim Makarenko'), arxiv.Result.Author('Arturo Burguete-Lopez'), arxiv.Result.Author('Qizhou Wang'), arxiv.Result.Author('Fedor Getman'), arxiv.Result.Author('Silvio Giancola'), arxiv.Result.Author('Bernard Ghanem'), arxiv.Result.Author('Andrea Fratalocchi')]","Hyperspectral imaging has attracted significant attention to identify
spectral signatures for image classification and automated pattern recognition
in computer vision. State-of-the-art implementations of snapshot hyperspectral
imaging rely on bulky, non-integrated, and expensive optical elements,
including lenses, spectrometers, and filters. These macroscopic components do
not allow fast data processing for, e.g real-time and high-resolution videos.
This work introduces Hyplex, a new integrated architecture addressing the
limitations discussed above. Hyplex is a CMOS-compatible, fast hyperspectral
camera that replaces bulk optics with nanoscale metasurfaces inversely designed
through artificial intelligence. Hyplex does not require spectrometers but
makes use of conventional monochrome cameras, opening up the possibility for
real-time and high-resolution hyperspectral imaging at inexpensive costs.
Hyplex exploits a model-driven optimization, which connects the physical
metasurfaces layer with modern visual computing approaches based on end-to-end
training. We design and implement a prototype version of Hyplex and compare its
performance against the state-of-the-art for typical imaging tasks such as
spectral reconstruction and semantic segmentation. In all benchmarks, Hyplex
reports the smallest reconstruction error. We additionally present what is, to
the best of our knowledge, the largest publicly available labeled hyperspectral
dataset for semantic segmentation.",-0.10125908,0.16539654,0.070437476,B
4407,"To further study the
context reliance, we change the object attributes while keep-                         We can further use SwapMix to improve the robustness
ing the object class unchanged.",Swapping the context attributes.,The object feature is                              of the model.,2022-04-05 15:32:25+00:00,SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering,cs.CV,"['cs.CV', 'cs.CL', 'cs.LG']","[arxiv.Result.Author('Vipul Gupta'), arxiv.Result.Author('Zhuowan Li'), arxiv.Result.Author('Adam Kortylewski'), arxiv.Result.Author('Chenyu Zhang'), arxiv.Result.Author('Yingwei Li'), arxiv.Result.Author('Alan Yuille')]","While Visual Question Answering (VQA) has progressed rapidly, previous works
raise concerns about robustness of current VQA models. In this work, we study
the robustness of VQA models from a novel perspective: visual context. We
suggest that the models over-rely on the visual context, i.e., irrelevant
objects in the image, to make predictions. To diagnose the model's reliance on
visual context and measure their robustness, we propose a simple yet effective
perturbation technique, SwapMix. SwapMix perturbs the visual context by
swapping features of irrelevant context objects with features from other
objects in the dataset. Using SwapMix we are able to change answers to more
than 45 % of the questions for a representative VQA model. Additionally, we
train the models with perfect sight and find that the context over-reliance
highly depends on the quality of visual representations. In addition to
diagnosing, SwapMix can also be applied as a data augmentation strategy during
training in order to regularize the context over-reliance. By swapping the
context object features, the model reliance on context can be suppressed
effectively. Two representative VQA models are studied using SwapMix: a
co-attention model MCAN and a large-scale pretrained model LXMERT. Our
experiments on the popular GQA dataset show the effectiveness of SwapMix for
both diagnosing model robustness and regularizing the over-reliance on visual
context. The code for our method is available at
https://github.com/vipulgupta1011/swapmix",0.07019341,-0.13452555,-0.09232409,A
4413,"Hence, it is clear that fast and accurate small vehicle detection remains nowadays a debated issue that
encourages further research in this area.","SpeciÔ¨Åcally, authors demonstrate that a joint-learning of the two networks allows to
obtain more meaningful targets and a higher perceptual quality in the super-resolved image, which in turn
lead to an enhanced accuracy in the detection task and performances on low-resolution aerial imagery close
to the existing state-of-the-art methods fed with the corresponding high-resolution aerial images.","In particular, as far as detection of tiny targets from aerial images
is concerned, Tiny-YOLOv3 does not guarantee an adequate accuracy [3].",2022-04-05 16:29:49+00:00,A lightweight and accurate YOLO-like network for small target detection in Aerial Imagery,cs.CV,"['cs.CV', 'cs.LG', 'I.2.6']",[arxiv.Result.Author('Alessandro Betti')],"Despite the breakthrough deep learning performances achieved for automatic
object detection, small target detection is still a challenging problem,
especially when looking at fast and accurate solutions suitable for mobile or
edge applications. In this work we present YOLO-S, a simple, fast and efficient
network for small target detection. The architecture exploits a small feature
extractor based on Darknet20, as well as skip connection, via both bypass and
concatenation, and reshape-passthrough layer to alleviate the vanishing
gradient problem, promote feature reuse across network and combine low-level
positional information with more meaningful high-level information. To verify
the performances of YOLO-S, we build ""AIRES"", a novel dataset for cAr detectIon
fRom hElicopter imageS acquired in Europe, and set up experiments on both AIRES
and VEDAI datasets, benchmarking this architecture with four baseline
detectors. Furthermore, in order to handle efficiently the issue of data
insufficiency and domain gap when dealing with a transfer learning strategy, we
introduce a transitional learning task over a combined dataset based on DOTAv2
and VEDAI and demonstrate that can enhance the overall accuracy with respect to
more general features transferred from COCO data. YOLO-S is from 25% to 50%
faster than YOLOv3 and only 15-25% slower than Tiny-YOLOv3, outperforming also
YOLOv3 in terms of accuracy in a wide range of experiments. Further simulations
performed on SARD dataset demonstrate also its applicability to different
scenarios such as for search and rescue operations. Besides, YOLO-S has an 87%
decrease of parameter size and almost one half FLOPs of YOLOv3, making
practical the deployment for low-power industrial applications.",-0.30471447,0.002228422,0.18838474,B
4419,"One of our Ô¨Åndings is, that explanations for counting problems
are worse than for other answer types, suggesting that further research into this direction
is needed.","We observed that the generated explanations were of higher quality for easier answer
and question categories.","Additionally, we Ô¨Ånd that the four NLG metrics used to evaluate the quality
of the generated explanations exhibit different convergence patterns depending on the
number of available ground-truth references.",2022-04-05 17:38:04+00:00,CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Leonard Salewski'), arxiv.Result.Author('A. Sophia Koepke'), arxiv.Result.Author('Hendrik P. A. Lensch'), arxiv.Result.Author('Zeynep Akata')]","Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.",0.29095978,-0.2283728,-0.22884333,A
4420,"We hope that our proposed CLEVR-X benchmark will facilitate
further research to improve the generation of natural language explanations in the context
of vision-language tasks.","of additional recent frameworks for textual explanations in the context of VQA on the
CLEVR-X dataset.","6 Acknowledgements

The authors thank the Amazon Mechanical Turk workers that participated in the user
study.",2022-04-05 17:38:04+00:00,CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Leonard Salewski'), arxiv.Result.Author('A. Sophia Koepke'), arxiv.Result.Author('Hendrik P. A. Lensch'), arxiv.Result.Author('Zeynep Akata')]","Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.",-0.092903815,-0.2628352,-0.3106687,C
4428,"To promote further research in this emerging field of single image 3D human-
object interaction capture, we will release our pre-trained models and code, so
that other researchers can use CHORE as a strong baseline.","We
also conducted extensive ablations, which reveal the effectiveness of the different
components of CHORE, and the proposed depth-aware scaling for pixel-aligned
learning on real data.","CHORE  15

Acknowledgements.",2022-04-05 18:38:06+00:00,"CHORE: Contact, Human and Object REconstruction from a single RGB image",cs.CV,['cs.CV'],"[arxiv.Result.Author('Xianghui Xie'), arxiv.Result.Author('Bharat Lal Bhatnagar'), arxiv.Result.Author('Gerard Pons-Moll')]","While most works in computer vision and learning have focused on perceiving
3D humans from single images in isolation, in this work we focus on capturing
3D humans interacting with objects. The problem is extremely challenging due to
heavy occlusions between human and object, diverse interaction types and depth
ambiguity. In this paper, we introduce CHORE, a novel method that learns to
jointly reconstruct human and object from a single image. CHORE takes
inspiration from recent advances in implicit surface learning and classical
model-based fitting. We compute a neural reconstruction of human and object
represented implicitly with two unsigned distance fields, and additionally
predict a correspondence field to a parametric body as well as an object pose
field. This allows us to robustly fit a parametric body model and a 3D object
template, while reasoning about interactions. Furthermore, prior pixel-aligned
implicit learning methods use synthetic data and make assumptions that are not
met in real data. We propose a simple yet effective depth-aware scaling that
allows more efficient shape learning on real data. Our experiments show that
our joint reconstruction learned with the proposed strategy significantly
outperforms the SOTA. Our code and models will be released to foster future
research in this direction.",-0.3159515,0.019408068,-0.0012423424,B
4429,"Our code and models are released to promote further research in this
emerging field of single image 3D human-object interaction capture.","We also conducted
extensive ablations, which reveal the effectiveness of the different components
of CHORE, and the proposed depth-aware scaling for pixel-aligned learning on
real data.",Acknowledgements.,2022-04-05 18:38:06+00:00,"CHORE: Contact, Human and Object REconstruction from a single RGB image",cs.CV,['cs.CV'],"[arxiv.Result.Author('Xianghui Xie'), arxiv.Result.Author('Bharat Lal Bhatnagar'), arxiv.Result.Author('Gerard Pons-Moll')]","Most prior works in perceiving 3D humans from images reason human in
isolation without their surroundings. However, humans are constantly
interacting with the surrounding objects, thus calling for models that can
reason about not only the human but also the object and their interaction. The
problem is extremely challenging due to heavy occlusions between humans and
objects, diverse interaction types and depth ambiguity. In this paper, we
introduce CHORE, a novel method that learns to jointly reconstruct the human
and the object from a single RGB image. CHORE takes inspiration from recent
advances in implicit surface learning and classical model-based fitting. We
compute a neural reconstruction of human and object represented implicitly with
two unsigned distance fields, a correspondence field to a parametric body and
an object pose field. This allows us to robustly fit a parametric body model
and a 3D object template, while reasoning about interactions. Furthermore,
prior pixel-aligned implicit learning methods use synthetic data and make
assumptions that are not met in the real data. We propose a elegant depth-aware
scaling that allows more efficient shape learning on real data. Experiments
show that our joint reconstruction learned with the proposed strategy
significantly outperforms the SOTA. Our code and models are available at
https://virtualhumans.mpi-inf.mpg.de/chore",-0.31658113,0.1237395,-0.0338896,B
4440,It clearly paves the way for further research.,"FHOI [38]                             14.4 33.7
                                                                   ImagineRNN [65]     -     26.1        15.4 34.3       5.6   15.7
   Certainly, apples-to-apples comparisons verify the con-         ActionBanks [46]                      14.7 35.0
tribution of DCR in training effective anticipation model at       Ego-OMG [10]        15.3 35.3         16.7 36.1       8.2   21.1
lower expense.","AVT [20]                              16.0 34.5
                                                                   DCR                 10.4 25.5         16.8 36.5       8.6   22.9
4.5.",2022-04-06 05:24:28+00:00,Learning to Anticipate Future with Dynamic Context Removal,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xinyu Xu'), arxiv.Result.Author('Yong-Lu Li'), arxiv.Result.Author('Cewu Lu')]","Anticipating future events is an essential feature for intelligent systems
and embodied AI. However, compared to the traditional recognition task, the
uncertainty of future and reasoning ability requirement make the anticipation
task very challenging and far beyond solved. In this filed, previous methods
usually care more about the model architecture design or but few attention has
been put on how to train an anticipation model with a proper learning policy.
To this end, in this work, we propose a novel training scheme called Dynamic
Context Removal (DCR), which dynamically schedules the visibility of observed
future in the learning procedure. It follows the human-like curriculum learning
process, i.e., gradually removing the event context to increase the
anticipation difficulty till satisfying the final anticipation target. Our
learning scheme is plug-and-play and easy to integrate any reasoning model
including transformer and LSTM, with advantages in both effectiveness and
efficiency. In extensive experiments, the proposed method achieves
state-of-the-art on four widely-used benchmarks. Our code and models are
publicly released at https://github.com/AllenXuuu/DCR.",0.29855213,-0.08798795,-0.10355638,A
4441,It clearly paves the way for further research.,"ActionBanks [46]                      14.7 35.0
                                                                   Ego-OMG [10]        15.3 35.3         16.7 36.1       8.2   21.1
   Certainly, apples-to-apples comparisons verify the con-         AVT [20]                              16.0 34.5
tribution of DCR in training effective anticipation model at       DCR                 10.4 25.5         16.8 36.5       8.6   22.9
lower expense.","17.7 38.5
                                                                                       -     35.6                        9.3   22.2
4.5.",2022-04-06 05:24:28+00:00,Learning to Anticipate Future with Dynamic Context Removal,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xinyu Xu'), arxiv.Result.Author('Yong-Lu Li'), arxiv.Result.Author('Cewu Lu')]","Anticipating future events is an essential feature for intelligent systems
and embodied AI. However, compared to the traditional recognition task, the
uncertainty of future and reasoning ability requirement make the anticipation
task very challenging and far beyond solved. In this filed, previous methods
usually care more about the model architecture design or but few attention has
been put on how to train an anticipation model with a proper learning policy.
To this end, in this work, we propose a novel training scheme called Dynamic
Context Removal (DCR), which dynamically schedules the visibility of observed
future in the learning procedure. It follows the human-like curriculum learning
process, i.e., gradually removing the event context to increase the
anticipation difficulty till satisfying the final anticipation target. Our
learning scheme is plug-and-play and easy to integrate any reasoning model
including transformer and LSTM, with advantages in both effectiveness and
efficiency. In extensive experiments, the proposed method achieves
state-of-the-art on four widely-used benchmarks. Our code and models are
publicly released at https://github.com/AllenXuuu/DCR.",0.31054923,-0.12624067,-0.12978205,A
4442,"Ablation Study
                                                                                                                         6% 19%
   To deeply investigate our FGPL, we further study differ-
ent ablation variants of CDL and EDL on PredCls task.","68%
                                                                                      14%                                6%
4.6.","16%                                              24%
Entity Discriminating Loss: To validate the superiority for
                                                                                                                                       87%

                                                                   Figure 5.",2022-04-06 06:20:09+00:00,Fine-Grained Predicates Learning for Scene Graph Generation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xinyu Lyu'), arxiv.Result.Author('Lianli Gao'), arxiv.Result.Author('Yuyu Guo'), arxiv.Result.Author('Zhou Zhao'), arxiv.Result.Author('Hao Huang'), arxiv.Result.Author('Heng Tao Shen'), arxiv.Result.Author('Jingkuan Song')]","The performance of current Scene Graph Generation models is severely hampered
by some hard-to-distinguish predicates, e.g., ""woman-on/standing on/walking
on-beach"" or ""woman-near/looking at/in front of-child"". While general SGG
models are prone to predict head predicates and existing re-balancing
strategies prefer tail categories, none of them can appropriately handle these
hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained
image classification, which focuses on differentiating among
hard-to-distinguish object classes, we propose a method named Fine-Grained
Predicates Learning (FGPL) which aims at differentiating among
hard-to-distinguish predicates for Scene Graph Generation task. Specifically,
we first introduce a Predicate Lattice that helps SGG models to figure out
fine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose
a Category Discriminating Loss and an Entity Discriminating Loss, which both
contribute to distinguishing fine-grained predicates while maintaining learned
discriminatory power over recognizable ones. The proposed model-agnostic
strategy significantly boosts the performances of three benchmark models
(Transformer, VCTree, and Motif) by 22.8\%, 24.1\% and 21.7\% of Mean Recall
(mR@100) on the Predicate Classification sub-task, respectively. Our model also
outperforms state-of-the-art methods by a large margin (i.e., 6.1\%, 4.6\%, and
3.2\% of Mean Recall (mR@100)) on the Visual Genome dataset.",0.42245743,-0.059897054,-0.1468265,A
4443,"Ablation Study
                                                                                                                         6% 19%
   To deeply investigate our FGPL, we further study differ-
ent ablation variants of CDL and EDL on PredCls task.","68%
                                                                                      14%                                6%
4.6.","16%                                              24%
Entity Discriminating Loss: To validate the superiority for
                                                                                                                                       87%

                                                                   Figure 5.",2022-04-06 06:20:09+00:00,Fine-Grained Predicates Learning for Scene Graph Generation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xinyu Lyu'), arxiv.Result.Author('Lianli Gao'), arxiv.Result.Author('Yuyu Guo'), arxiv.Result.Author('Zhou Zhao'), arxiv.Result.Author('Hao Huang'), arxiv.Result.Author('Heng Tao Shen'), arxiv.Result.Author('Jingkuan Song')]","The performance of current Scene Graph Generation models is severely hampered
by some hard-to-distinguish predicates, e.g., ""woman-on/standing on/walking
on-beach"" or ""woman-near/looking at/in front of-child"". While general SGG
models are prone to predict head predicates and existing re-balancing
strategies prefer tail categories, none of them can appropriately handle these
hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained
image classification, which focuses on differentiating among
hard-to-distinguish object classes, we propose a method named Fine-Grained
Predicates Learning (FGPL) which aims at differentiating among
hard-to-distinguish predicates for Scene Graph Generation task. Specifically,
we first introduce a Predicate Lattice that helps SGG models to figure out
fine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose
a Category Discriminating Loss and an Entity Discriminating Loss, which both
contribute to distinguishing fine-grained predicates while maintaining learned
discriminatory power over recognizable ones. The proposed model-agnostic
strategy significantly boosts the performances of three benchmark models
(Transformer, VCTree, and Motif) by 22.8\%, 24.1\% and 21.7\% of Mean Recall
(mR@100) on the Predicate Classification sub-task, respectively. Our model also
outperforms state-of-the-art methods by a large margin (i.e., 6.1\%, 4.6\%, and
3.2\% of Mean Recall (mR@100)) on the Visual Genome dataset.",0.42245743,-0.059897054,-0.1468265,A
4459,"These Ô¨Åndings               of the scene-irrelevant regions and the complicated spatial
                                       call for further research efforts on both large-scale pretrain-              distribution of land objects.","The obtained aerial
                                       the task discrepancy, where downstream tasks require different               scene is usually difÔ¨Åcult to be interpreted since the interference
                                       representations from the scene recognition task.","SpeciÔ¨Åcally, it causes the issue of
                                       ing datasets and effective pretraining methods.",2022-04-06 13:38:11+00:00,An Empirical Study of Remote Sensing Pretraining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Di Wang'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Gui-Song Xia'), arxiv.Result.Author('Dacheng Tao')]","Deep learning has largely reshaped remote sensing research for aerial image
understanding. Nevertheless, most of existing deep models are initialized with
ImageNet pretrained weights, where the natural images inevitably presents a
large domain gap relative to the aerial images, probably limiting the
finetuning performance on downstream aerial scene tasks. This issue motivates
us to conduct an empirical study of remote sensing pretraining (RSP). To this
end, we train different networks from scratch with the help of the largest
remote sensing scene recognition dataset up to now-MillionAID, to obtain the
remote sensing pretrained backbones, including both convolutional neural
networks (CNN) and vision transformers such as Swin and ViTAE, which have shown
promising performance on computer vision tasks. Then, we investigate the impact
of ImageNet pretraining (IMP) and RSP on a series of downstream tasks including
scene recognition, semantic segmentation, object detection, and change
detection using the CNN and vision transformers backbones. We have some
empirical findings as follows. First, vision transformers generally outperforms
CNN backbones, where ViTAE achieves the best performance, owing to its strong
representation capacity by introducing intrinsic inductive bias from
convolutions to transformers. Second, both IMP and RSP help deliver better
performance, where IMP enjoys a versatility by learning more universal
representations from diverse images belonging to much more categories while RSP
is distinctive in perceiving remote sensing related semantics. Third, RSP
mitigates the data discrepancy of IMP for remote sensing but may still suffer
from the task discrepancy, where downstream tasks require different
representations from the scene recognition task. These findings call for
further research efforts on both large-scale pretraining datasets and effective
pretraining methods.",-0.18767169,-0.040118575,-0.027885197,B
4460,"The obtained aerial
                                        call for further research efforts on both large-scale pretrain-              scene is usually difÔ¨Åcult to be interpreted since the interference
                                        ing datasets and effective pretraining methods.",These Ô¨Åndings                  large scope of land uses and land covers.,"The codes and                of the scene-irrelevant regions and the complicated spatial
                                        pretrained models will be released at https://github.com/ViTAE-              distribution of land objects.",2022-04-06 13:38:11+00:00,An Empirical Study of Remote Sensing Pretraining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Di Wang'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Gui-Song Xia'), arxiv.Result.Author('Dacheng Tao')]","Deep learning has largely reshaped remote sensing (RS) research for aerial
image understanding and made a great success. Nevertheless, most of the
existing deep models are initialized with the ImageNet pretrained weights.
Since natural images inevitably present a large domain gap relative to aerial
images, probably limiting the finetuning performance on downstream aerial scene
tasks. This issue motivates us to conduct an empirical study of remote sensing
pretraining (RSP) on aerial images. To this end, we train different networks
from scratch with the help of the largest RS scene recognition dataset up to
now -- MillionAID, to obtain a series of RS pretrained backbones, including
both convolutional neural networks (CNN) and vision transformers such as Swin
and ViTAE, which have shown promising performance on computer vision tasks.
Then, we investigate the impact of RSP on representative downstream tasks
including scene recognition, semantic segmentation, object detection, and
change detection using these CNN and vision transformer backbones. Empirical
study shows that RSP can help deliver distinctive performances in scene
recognition tasks and in perceiving RS related semantics such as ""Bridge"" and
""Airplane"". We also find that, although RSP mitigates the data discrepancies of
traditional ImageNet pretraining on RS images, it may still suffer from task
discrepancies, where downstream tasks require different representations from
scene recognition tasks. These findings call for further research efforts on
both large-scale pretraining datasets and effective pretraining methods. The
codes and pretrained models will be released at
https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.",-0.11836504,0.06359761,0.05679182,B
4497,"On the other side, OOS oper-               The competition allowed to move the state-of-the-art forward
                                                                             and provided useful insights for further research.",tinuous and safe access to space.,"Three out the
ations could result in a cost containment with respect to full               four top-scoring works exploited Convolutional Neural Net-
                                                                             works (CNNs) to detect the target on the image, and to regress
replacement.",2022-04-07 08:53:18+00:00,Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU,cs.CV,['cs.CV'],"[arxiv.Result.Author('Alessandro Lotti'), arxiv.Result.Author('Dario Modenini'), arxiv.Result.Author('Paolo Tortora'), arxiv.Result.Author('Massimiliano Saponara'), arxiv.Result.Author('Maria A. Perino')]","Pose estimation of an uncooperative space resident object is a key asset
towards autonomy in close proximity operations. In this context monocular
cameras are a valuable solution because of their low system requirements.
However, the associated image processing algorithms are either too
computationally expensive for real time on-board implementation, or not enough
accurate. In this paper we propose a pose estimation software exploiting neural
network architectures which can be scaled to different accuracy-latency
trade-offs. We designed our pipeline to be compatible with Edge Tensor
Processing Units to show how low power machine learning accelerators could
enable Artificial Intelligence exploitation in space. The neural networks were
tested both on the benchmark Spacecraft Pose Estimation Dataset, and on the
purposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed
satellite in a variety of random poses and steerable solar panels orientations.
The lightest version of our architecture achieves state-of-the-art accuracy on
both datasets but at a fraction of networks complexity, running at 7.7 frames
per second on a Coral Dev Board Mini consuming just 2.2W.",-0.17560935,-0.051549762,0.12177325,C
4514,"Thus, this is one of our further research directions by exploring a
stronger aggregation mechanism.","Since clips and phrases are learned from datasets, clip-phrase matching results are
worse than frame-word matching.","4.4 Comparison with State-of-the-Art Methods

In this part, we compare our HCMI with state-of-the-art methods on MSR-VTT, MSVD, LSMDC,
DiDeMo, and ActivityNet benchmarks.",2022-04-07 11:59:36+00:00,Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jie Jiang'), arxiv.Result.Author('Shaobo Min'), arxiv.Result.Author('Weijie Kong'), arxiv.Result.Author('Hongfa Wang'), arxiv.Result.Author('Zhifeng Li'), arxiv.Result.Author('Wei Liu')]","Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while overlooking fine-grained cross-modal relationships, e.g.,
clip-phrase or frame-word. In this paper, we propose a novel method, named
Hierarchical Cross-Modal Interaction (HCMI), to explore multi-level cross-modal
relationships among video-sentence, clip-phrase, and frame-word for text-video
retrieval. Considering intrinsic semantic frame relations, HCMI performs
self-attention to explore frame-level correlations and adaptively cluster
correlated frames into clip-level and video-level representations. In this way,
HCMI constructs multi-level video representations for frame-clip-video
granularities to capture fine-grained video content, and multi-level text
representations at word-phrase-sentence granularities for the text modality.
With multi-level representations for video and text, hierarchical contrastive
learning is designed to explore fine-grained cross-modal relationships, i.e.,
frame-word, clip-phrase, and video-sentence, which enables HCMI to achieve a
comprehensive semantic comparison between video and text modalities. Further
boosted by adaptive label denoising and marginal sample enhancement, HCMI
achieves new state-of-the-art results on various benchmarks, e.g., Rank@1 of
55.0%, 58.2%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC, DiDemo, and
ActivityNet, respectively.",-0.0921174,-0.17408119,-0.1853108,C
4515,"Thus, this is one of our further research directions by exploring a
stronger aggregation mechanism.","Since clips and phrases are learned from datasets, clip-phrase matching results are
worse than frame-word matching.","4.4 Comparison with State-of-the-Art Methods

In this part, we compare our HCMI with state-of-the-art methods on MSR-VTT, MSVD, LSMDC,
DiDeMo, and ActivityNet benchmarks.",2022-04-07 11:59:36+00:00,Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jie Jiang'), arxiv.Result.Author('Shaobo Min'), arxiv.Result.Author('Weijie Kong'), arxiv.Result.Author('Dihong Gong'), arxiv.Result.Author('Hongfa Wang'), arxiv.Result.Author('Zhifeng Li'), arxiv.Result.Author('Wei Liu')]","Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while overlooking fine-grained cross-modal relationships, e.g.,
clip-phrase or frame-word. In this paper, we propose a novel method, named
Hierarchical Cross-Modal Interaction (HCMI), to explore multi-level cross-modal
relationships among video-sentence, clip-phrase, and frame-word for text-video
retrieval. Considering intrinsic semantic frame relations, HCMI performs
self-attention to explore frame-level correlations and adaptively cluster
correlated frames into clip-level and video-level representations. In this way,
HCMI constructs multi-level video representations for frame-clip-video
granularities to capture fine-grained video content, and multi-level text
representations at word-phrase-sentence granularities for the text modality.
With multi-level representations for video and text, hierarchical contrastive
learning is designed to explore fine-grained cross-modal relationships, i.e.,
frame-word, clip-phrase, and video-sentence, which enables HCMI to achieve a
comprehensive semantic comparison between video and text modalities. Further
boosted by adaptive label denoising and marginal sample enhancement, HCMI
achieves new state-of-the-art results on various benchmarks, e.g., Rank@1 of
55.0%, 58.2%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC, DiDemo, and
ActivityNet, respectively.",-0.0921174,-0.17408119,-0.1853108,C
4517,"We conduct a further study about the contribution of example prototypes and category
prototypes for our method.",Settings.,"In detail, we employ different numbers of example prototypes (including
number zero which means there is no example prototype used) under the PHI and PNI setting for
both 10 and 50 phases on CIFAR-100.",2022-04-07 12:49:14+00:00,Incremental Prototype Tuning for Class Incremental Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jieren Deng'), arxiv.Result.Author('Jianhua Hu'), arxiv.Result.Author('Haojian Zhang'), arxiv.Result.Author('Yunkuan Wang')]","Class incremental learning(CIL) has attracted much attention, but most
existing related works focus on fine-tuning the entire representation model,
which inevitably results in much catastrophic forgetting. In the contrast, with
a semantic-rich pre-trained representation model, parameter-additional-tuning
(PAT) only changes very few parameters to learn new visual concepts. Recent
studies have proved that PAT-based CIL can naturally avoid fighting against
forgetting by replaying or distilling like most of the existing methods.
However, we find that PAT-based CIL still faces serious semantic drift, the
high-level forgetting problem caused by classifier learning bias at different
learning phases, which significantly reduces the performance of PAT-based CIL.
To address this problem, we propose Incremental Prototype Tuning (IPT), a
simple but effective method that tunes category prototypes for classification
and learning example prototypes to compensate for semantic drift. Extensive
experiments demonstrate that our method can effectively compensate for semantic
drift. Combined with well-pre-trained Vit backbones and other PAT methods, IPT
surpasses the state-of-the-art baselines on mainstream incremental learning
benchmarks.",0.23945133,-0.020569291,-0.08567158,A
4532,"We share our implementation details and design decisions with the
intent to aid and assist further research pursuits in this Ô¨Åeld.","A novel contribution of our system is a quick and accurate user-in-
the-loop facial annotation system that can guarantee accurate bounding boxes quickly, allowing face swaps in frames
where automatic face detection algorithms fail.","When designing the system, we considered privacy risks
created by tele-health, remote training for clinicians, and dataset sharing for computational research.",2022-04-07 16:34:15+00:00,Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Ethan Wilson'), arxiv.Result.Author('Frederick Shic'), arxiv.Result.Author('Jenny Skytta'), arxiv.Result.Author('Eakta Jain')]","With rapid advancements in image generation technology, face swapping for
privacy protection has emerged as an active area of research. The ultimate
benefit is improved access to video datasets, e.g. in healthcare settings.
Recent literature has proposed deep network-based architectures to perform
facial swaps and reported the associated reduction in facial recognition
accuracy. However, there is not much reporting on how well these methods
preserve the types of semantic information needed for the privatized videos to
remain useful for their intended application. Our main contribution is a novel
end-to-end face swapping pipeline for recorded videos of standardized
assessments of autism symptoms in children. Through this design, we are the
first to provide a methodology for assessing the privacy-utility trade-offs for
the face swapping approach to patient privacy protection. Our methodology can
show, for example, that current deep network based face swapping is
bottle-necked by face detection in real world videos, and the extent to which
gaze and expression information is preserved by face swaps relative to baseline
privatization methods such as blurring.",-0.11552259,-0.06527898,-0.17380571,C
4533,"We share our implementation details and design decisions with the
intent to aid and assist further research pursuits in this Ô¨Åeld.","A novel contribution of our system is a quick and accurate user-in-
the-loop facial annotation system that can guarantee accurate bounding boxes quickly, allowing face swaps in frames
where automatic face detection algorithms fail.","When designing the system, we considered privacy risks
created by tele-health, remote training for clinicians, and dataset sharing for computational research.",2022-04-07 16:34:15+00:00,Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Ethan Wilson'), arxiv.Result.Author('Frederick Shic'), arxiv.Result.Author('Jenny Skytta'), arxiv.Result.Author('Eakta Jain')]","With rapid advancements in image generation technology, face swapping for
privacy protection has emerged as an active area of research. The ultimate
benefit is improved access to video datasets, e.g. in healthcare settings.
Recent literature has proposed deep network-based architectures to perform
facial swaps and reported the associated reduction in facial recognition
accuracy. However, there is not much reporting on how well these methods
preserve the types of semantic information needed for the privatized videos to
remain useful for their intended application. Our main contribution is a novel
end-to-end face swapping pipeline for recorded videos of standardized
assessments of autism symptoms in children. Through this design, we are the
first to provide a methodology for assessing the privacy-utility trade-offs for
the face swapping approach to patient privacy protection. Our methodology can
show, for example, that current deep network based face swapping is
bottle-necked by face detection in real world videos, and the extent to which
gaze and expression information is preserved by face swaps relative to baseline
privatization methods such as blurring.",-0.11552259,-0.06527898,-0.17380571,C
4536,"It thus points out that                         Table 5, we observe that both loss terms make substantial
category-aware generalization, like our method, should be                           contributions to the performance gain for all datasets by
encouraged importantly to further research in this area.","From
a relatively high-performance gain.",operating complementary each other.,2022-04-07 17:34:01+00:00,Pin the Memory: Learning to Generalize Semantic Segmentation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jin Kim'), arxiv.Result.Author('Jiyoung Lee'), arxiv.Result.Author('Jungin Park'), arxiv.Result.Author('Dongbo Min'), arxiv.Result.Author('Kwanghoon Sohn')]","The rise of deep neural networks has led to several breakthroughs for
semantic segmentation. In spite of this, a model trained on source domain often
fails to work properly in new challenging domains, that is directly concerned
with the generalization capability of the model. In this paper, we present a
novel memory-guided domain generalization method for semantic segmentation
based on meta-learning framework. Especially, our method abstracts the
conceptual knowledge of semantic classes into categorical memory which is
constant beyond the domains. Upon the meta-learning concept, we repeatedly
train memory-guided networks and simulate virtual test to 1) learn how to
memorize a domain-agnostic and distinct information of classes and 2) offer an
externally settled memory as a class-guidance to reduce the ambiguity of
representation in the test data of arbitrary unseen domain. To this end, we
also propose memory divergence and feature cohesion losses, which encourage to
learn memory reading and update processes for category-aware domain
generalization. Extensive experiments for semantic segmentation demonstrate the
superior generalization capability of our method over state-of-the-art works on
various benchmarks.",0.24636506,-0.26898026,-0.0012500919,A
4537,"It thus points out that                         Table 5, we observe that both loss terms make substantial
category-aware generalization, like our method, should be                           contributions to the performance gain for all datasets by
encouraged importantly to further research in this area.","From
a relatively high-performance gain.",operating complementary each other.,2022-04-07 17:34:01+00:00,Pin the Memory: Learning to Generalize Semantic Segmentation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jin Kim'), arxiv.Result.Author('Jiyoung Lee'), arxiv.Result.Author('Jungin Park'), arxiv.Result.Author('Dongbo Min'), arxiv.Result.Author('Kwanghoon Sohn')]","The rise of deep neural networks has led to several breakthroughs for
semantic segmentation. In spite of this, a model trained on source domain often
fails to work properly in new challenging domains, that is directly concerned
with the generalization capability of the model. In this paper, we present a
novel memory-guided domain generalization method for semantic segmentation
based on meta-learning framework. Especially, our method abstracts the
conceptual knowledge of semantic classes into categorical memory which is
constant beyond the domains. Upon the meta-learning concept, we repeatedly
train memory-guided networks and simulate virtual test to 1) learn how to
memorize a domain-agnostic and distinct information of classes and 2) offer an
externally settled memory as a class-guidance to reduce the ambiguity of
representation in the test data of arbitrary unseen domain. To this end, we
also propose memory divergence and feature cohesion losses, which encourage to
learn memory reading and update processes for category-aware domain
generalization. Extensive experiments for semantic segmentation demonstrate the
superior generalization capability of our method over state-of-the-art works on
various benchmarks.",0.24636506,-0.26898026,-0.0012500919,A
4539,"5.3 Qualitative Results

Beyond the quantitative results, we further study the quality of the extracted score map.","This shows that our method, which is not Ô¨Åne-tuned
on any textual grounding dataset, is less prone to bias in object categories.",In Fig.,2022-04-07 17:59:38+00:00,Adapting CLIP For Phrase Localization Without Further Training,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jiahao Li'), arxiv.Result.Author('Greg Shakhnarovich'), arxiv.Result.Author('Raymond A. Yeh')]","Supervised or weakly supervised methods for phrase localization (textual
grounding) either rely on human annotations or some other supervised models,
e.g., object detectors. Obtaining these annotations is labor-intensive and may
be difficult to scale in practice. We propose to leverage recent advances in
contrastive language-vision models, CLIP, pre-trained on image and caption
pairs collected from the internet. In its original form, CLIP only outputs an
image-level embedding without any spatial resolution. We adapt CLIP to generate
high-resolution spatial feature maps. Importantly, we can extract feature maps
from both ViT and ResNet CLIP model while maintaining the semantic properties
of an image embedding. This provides a natural framework for phrase
localization. Our method for phrase localization requires no human annotations
or additional training. Extensive experiments show that our method outperforms
existing no-training methods in zero-shot phrase localization, and in some
cases, it even outperforms supervised methods. Code is available at
https://github.com/pals-ttic/adapting-CLIP .",0.0035631396,-0.15070572,-0.24767864,C
4553,"Our further research    T.; and Chen, Q.","Extensive experiments    Li, D.; Hu, J.; Wang, C.; Li, X.; She, Q.; Zhu, L.; Zhang,
verify the effectiveness of our model.",2021.,2022-04-08 05:11:04+00:00,From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weiguang Zhao'), arxiv.Result.Author('Chaolong Yang'), arxiv.Result.Author('Jianan Ye'), arxiv.Result.Author('Yuyao Yan'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Kaizhu Huang')]","We consider the problem of Multi-view 3D Face Reconstruction (MVR) with
weakly supervised learning that leverages a limited number of 2D face images
(e.g. 3) to generate a high-quality 3D face model with very light annotation.
Despite their encouraging performance, present MVR methods simply concatenate
multi-view image features and pay less attention to critical areas (e.g. eye,
brow, nose and mouth). To this end, we propose a novel model called Deep Fusion
MVR (DF-MVR) and design a multi-view encoding to a single decoding framework
with skip connections, able to extract, integrate, and compensate deep features
with attention from multi-view images. In addition, we develop a multi-view
face parse network to learn, identify, and emphasize the critical common face
area. Finally, though our model is trained with a few 2D images, it can
reconstruct an accurate 3D model even if one single 2D image is input. We
conduct extensive experiments to evaluate various multi-view 3D face
reconstruction methods. Our proposed model attains superior performance,
leading to 11.4% RMSE improvement over the existing best weakly supervised
MVRs. Source codes are available in the supplementary materials.",0.48398417,0.05337564,0.021413177,A
4560,"age with notably different expressions every time like hu-
First, we further study the Long-Tailed Weight strategy that      mans [36], or use rich and diverse wording [18] to generate
mainly works on word-level and supplements the sentence-          captions; (2) discriminability: describe an image by referring
level CIDErBtwStrategy from [22].","In summary, they propose
    The preliminary version of this work was published            three aspects to consider: (1) diversity: describe one im-
in [22], and has been extended in 4 aspects in this paper.","Second, we investigate          to the important and detailed aspects of the image, which
using contrastive learning, based on word level negative          is accurate, and informative [9], [12], [15], [16], [37]; (3)
samples from the similar image set, to promote the dis-           distinctiveness: describe the important and speciÔ¨Åc aspects of
tinctiveness of generated captions.",2022-04-08 08:59:23+00:00,On Distinctive Image Captioning via Comparing and Reweighting,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jiuniu Wang'), arxiv.Result.Author('Wenjia Xu'), arxiv.Result.Author('Qingzhong Wang'), arxiv.Result.Author('Antoni B. Chan')]","Recent image captioning models are achieving impressive results based on
popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most
popular metrics that only consider the overlap between the generated captions
and human annotation could result in using common words and phrases, which
lacks distinctiveness, i.e., many similar images have the same caption. In this
paper, we aim to improve the distinctiveness of image captions via comparing
and reweighting with a set of similar images. First, we propose a
distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the
distinctiveness of a caption with respect to those of similar images. Our
metric reveals that the human annotations of each image in the MSCOCO dataset
are not equivalent based on distinctiveness; however, previous works normally
treat the human annotations equally during training, which could be a reason
for generating less distinctive captions. In contrast, we reweight each
ground-truth caption according to its distinctiveness during training. We
further integrate a long-tailed weight strategy to highlight the rare words
that contain more information, and captions from the similar image set are
sampled as negative examples to encourage the generated sentence to be unique.
Finally, extensive experiments are conducted, showing that our proposed
approach significantly improves both distinctiveness (as measured by CIDErBtw
and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide
variety of image captioning baselines. These results are further confirmed
through a user study.",-0.058776837,-0.2979174,-0.21552964,C
4564,"As it will be discussed on Section 7, further research is needed
in terms of optimizing this CNNs.","The experiments ran on Tables
12 were performed with an Inception v3 pre-trained on ImageNet Ô¨Åne-tuning all
the parameters.","Using patches instead of the whole image as
shown in [7] should make it perform better.",2022-04-08 12:30:39+00:00,A Generic Image Retrieval Method for Date Estimation of Historical Document Collections,cs.CV,"['cs.CV', 'cs.DL', 'cs.IR']","[arxiv.Result.Author('Adri√† Molina'), arxiv.Result.Author('Lluis Gomez'), arxiv.Result.Author('Oriol Ramos Terrades'), arxiv.Result.Author('Josep Llad√≥s')]","Date estimation of historical document images is a challenging problem, with
several contributions in the literature that lack of the ability to generalize
from one dataset to others. This paper presents a robust date estimation system
based in a retrieval approach that generalizes well in front of heterogeneous
collections. we use a ranking loss function named smooth-nDCG to train a
Convolutional Neural Network that learns an ordination of documents for each
problem. One of the main usages of the presented approach is as a tool for
historical contextual retrieval. It means that scholars could perform
comparative analysis of historical images from big datasets in terms of the
period where they were produced. We provide experimental evaluation on
different types of documents from real datasets of manuscript and newspaper
images.",-0.15445408,-0.16517733,0.34582022,C
4565,"Despite further research is needed in terms of this particular
CNN optimization (as Hamid et al.","Mean Absolute Error (MAE) comparison of our model with existing methods
on the test set of the MPS dataset

Baseline                                MAE mAP

Fraglet and Hinge Features [13]         35.4 -
Hinge Features [3]                      12.20 -
Quill Features [2]                      12.10 -
Polar Stroke Descriptor (PSD) [12]      20.90 -
PSD + Temporal Pattern Codebook [10]    7.80 -
Textural Features [8]                   20.13 -
InceptionResnetv2 [7]                   3.01 -

Smooth-nDCG Manuscript Retrieval (MPS)  23.8 0.43

7 Conclusions

As exposed in Sections 1, 6; the retrieval task is a powerful tool for archivists and
social scientists.","[7] proved, Inception can outperform our
2 mAP approximated from training bacthes
Image Retrieval Method For Document Date Estimation                   13

Table 2.",2022-04-08 12:30:39+00:00,A Generic Image Retrieval Method for Date Estimation of Historical Document Collections,cs.CV,"['cs.CV', 'cs.DL', 'cs.IR']","[arxiv.Result.Author('Adri√† Molina'), arxiv.Result.Author('Lluis Gomez'), arxiv.Result.Author('Oriol Ramos Terrades'), arxiv.Result.Author('Josep Llad√≥s')]","Date estimation of historical document images is a challenging problem, with
several contributions in the literature that lack of the ability to generalize
from one dataset to others. This paper presents a robust date estimation system
based in a retrieval approach that generalizes well in front of heterogeneous
collections. we use a ranking loss function named smooth-nDCG to train a
Convolutional Neural Network that learns an ordination of documents for each
problem. One of the main usages of the presented approach is as a tool for
historical contextual retrieval. It means that scholars could perform
comparative analysis of historical images from big datasets in terms of the
period where they were produced. We provide experimental evaluation on
different types of documents from real datasets of manuscript and newspaper
images.",-0.11243966,-0.22004786,0.04956225,C
4573,"These arguments are             transferability of adversarial example (AE) in the real world
beyond the scope of this paper and require further study in         and propose customized metrics to address these difÔ¨Åculties.","For example, we do not validate the hypothesis
that the number of the local optimal increases exponentially           In this paper, we identify the difÔ¨Åculties of evaluating
when the task complexity increases.",the future.,2022-04-07 12:16:24+00:00,Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings,cs.CV,"['cs.CV', 'cs.AI', 'cs.CR', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Yuhao Mao'), arxiv.Result.Author('Chong Fu'), arxiv.Result.Author('Saizhuo Wang'), arxiv.Result.Author('Shouling Ji'), arxiv.Result.Author('Xuhong Zhang'), arxiv.Result.Author('Zhenguang Liu'), arxiv.Result.Author('Jun Zhou'), arxiv.Result.Author('Alex X. Liu'), arxiv.Result.Author('Raheem Beyah'), arxiv.Result.Author('Ting Wang')]","One intriguing property of adversarial attacks is their ""transferability"" --
an adversarial example crafted with respect to one deep neural network (DNN)
model is often found effective against other DNNs as well. Intensive research
has been conducted on this phenomenon under simplistic controlled conditions.
Yet, thus far, there is still a lack of comprehensive understanding about
transferability-based attacks (""transfer attacks"") in real-world environments.
  To bridge this critical gap, we conduct the first large-scale systematic
empirical study of transfer attacks against major cloud-based MLaaS platforms,
taking the components of a real transfer attack into account. The study leads
to a number of interesting findings which are inconsistent to the existing
ones, including: (1) Simple surrogates do not necessarily improve real transfer
attacks. (2) No dominant surrogate architecture is found in real transfer
attacks. (3) It is the gap between posterior (output of the softmax layer)
rather than the gap between logit (so-called $\kappa$ value) that increases
transferability. Moreover, by comparing with prior works, we demonstrate that
transfer attacks possess many previously unknown properties in real-world
environments, such as (1) Model similarity is not a well-defined concept. (2)
$L_2$ norm of perturbation can generate high transferability without usage of
gradient and is a more powerful source than $L_\infty$ norm. We believe this
work sheds light on the vulnerabilities of popular MLaaS platforms and points
to a few promising research directions.",0.11620788,-0.22451001,0.07492085,C
4579,"So for the video anomaly detection task, the           aly detection datasets demonstrate our methods‚Äô effective-
semantics of appearance and motion features extracted from frame              ness, and all code will be released for further research con-
sequences and optical flow without background information should              venience to the community.","surveillance video, so the model‚Äôs attention should focus on the
moving foreground part, rather than the background which is less           ‚Ä¢ Extensive experiments on three standard public video anom-
relevant for behavior.","be consistent since the two modalities all represent the foreground
behavior properties in the surveillance video.",2022-04-08 15:59:57+00:00,A Video Anomaly Detection Framework based on Appearance-Motion Semantics Representation Consistency,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiangyu Huang'), arxiv.Result.Author('Caidan Zhao'), arxiv.Result.Author('Yilin Wang'), arxiv.Result.Author('Zhiqiang Wu')]","Video anomaly detection refers to the identification of events that deviate
from the expected behavior. Due to the lack of anomalous samples in training,
video anomaly detection becomes a very challenging task. Existing methods
almost follow a reconstruction or future frame prediction mode. However, these
methods ignore the consistency between appearance and motion information of
samples, which limits their anomaly detection performance. Anomalies only occur
in the moving foreground of surveillance videos, so the semantics expressed by
video frame sequences and optical flow without background information in
anomaly detection should be highly consistent and significant for anomaly
detection. Based on this idea, we propose Appearance-Motion Semantics
Representation Consistency (AMSRC), a framework that uses normal data's
appearance and motion semantic representation consistency to handle anomaly
detection. Firstly, we design a two-stream encoder to encode the appearance and
motion information representations of normal samples and introduce constraints
to further enhance the consistency of the feature semantics between appearance
and motion information of normal samples so that abnormal samples with low
consistency appearance and motion feature representation can be identified.
Moreover, the lower consistency of appearance and motion features of anomalous
samples can be used to generate predicted frames with larger reconstruction
error, which makes anomalies easier to spot. Experimental results demonstrate
the effectiveness of the proposed method.",-0.25364465,0.04036388,-0.20337558,B
4618,"The dataset       lenging, however, to identify these biases from a black-box
and code are made public to facilitate further research.1         model with distributed knowledge.","It is very chal-
ferent subsets without using additional labels.",1.,2022-04-10 04:57:56+00:00,Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic Filter Attention,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yu Yang'), arxiv.Result.Author('Seungbae Kim'), arxiv.Result.Author('Jungseock Joo')]","Interpretability is an important property for visual models as it helps
researchers and users understand the internal mechanism of a complex model.
However, generating semantic explanations about the learned representation is
challenging without direct supervision to produce such explanations. We propose
a general framework, Latent Visual Semantic Explainer (LaViSE), to teach any
existing convolutional neural network to generate text descriptions about its
own latent representations at the filter level. Our method constructs a mapping
between the visual and semantic spaces using generic image datasets, using
images and category names. It then transfers the mapping to the target domain
which does not have semantic labels. The proposed framework employs a modular
structure and enables to analyze any trained network whether or not its
original training data is available. We show that our method can generate novel
descriptions for learned filters beyond the set of categories defined in the
training dataset and perform an extensive evaluation on multiple datasets. We
also demonstrate a novel application of our method for unsupervised dataset
bias analysis which allows us to automatically discover hidden biases in
datasets or compare different subsets without using additional labels. The
dataset and code are made public to facilitate further research.",0.21530399,-0.23999935,-0.1436548,A
4624,"A
ments of GANs for image synthesis, given successful ap-          further study was conducted on generating lesion images
plications in medical diagnosis (Yi et al., 2019), has in-       with a speciÔ¨Åc shape by feeding binary images into the
spired signiÔ¨Åcant eÔ¨Äorts made to utilize GANs to aug-            GAN generator (Sun et al., 2020), which improved the
ment image data for enhancing the detection of plant dis-        recognition of leaf lesion from 95.5% on the original data
eases and other health conditions.",The develop-              ease and obtained a classiÔ¨Åcation accuracy of 90.1%.,to 97.8% for synthetic images.,2022-04-10 15:33:05+00:00,Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Ebenezer Olaniyi'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Yuzhen Lu'), arxiv.Result.Author('Yanbo Huang')]","In agricultural image analysis, optimal model performance is keenly pursued
for better fulfilling visual recognition tasks (e.g., image classification,
segmentation, object detection and localization), in the presence of challenges
with biological variability and unstructured environments. Large-scale,
balanced and ground-truthed image datasets, however, are often difficult to
obtain to fuel the development of advanced, high-performance models. As
artificial intelligence through deep learning is impacting analysis and
modeling of agricultural images, data augmentation plays a crucial role in
boosting model performance while reducing manual efforts for data preparation,
by algorithmically expanding training datasets. Beyond traditional data
augmentation techniques, generative adversarial network (GAN) invented in 2014
in the computer vision community, provides a suite of novel approaches that can
learn good data representations and generate highly realistic samples. Since
2017, there has been a growth of research into GANs for image augmentation or
synthesis in agriculture for improved model performance. This paper presents an
overview of the evolution of GAN architectures followed by a systematic review
of their application to agriculture
(https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks
for plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping
as well as postharvest detection of fruit defects. Challenges and opportunities
of GANs are discussed for future research.",-0.09731554,0.046735574,0.15025961,C
4625,"A
ments of GANs for image synthesis, given successful ap-          further study was conducted on generating lesion images
plications in medical diagnosis (Yi et al., 2019), has in-       with a speciÔ¨Åc shape by feeding binary images into the
spired signiÔ¨Åcant eÔ¨Äorts made to utilize GANs to aug-            GAN generator (Sun et al., 2020), which improved the
ment image data for enhancing the detection of plant dis-        recognition of leaf lesion from 95.5% on the original data
eases and other health conditions.",The develop-              ease and obtained a classiÔ¨Åcation accuracy of 90.1%.,to 97.8% for synthetic images.,2022-04-10 15:33:05+00:00,Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Ebenezer Olaniyi'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Yuzhen Lu'), arxiv.Result.Author('Yanbo Huang')]","In agricultural image analysis, optimal model performance is keenly pursued
for better fulfilling visual recognition tasks (e.g., image classification,
segmentation, object detection and localization), in the presence of challenges
with biological variability and unstructured environments. Large-scale,
balanced and ground-truthed image datasets, however, are often difficult to
obtain to fuel the development of advanced, high-performance models. As
artificial intelligence through deep learning is impacting analysis and
modeling of agricultural images, data augmentation plays a crucial role in
boosting model performance while reducing manual efforts for data preparation,
by algorithmically expanding training datasets. Beyond traditional data
augmentation techniques, generative adversarial network (GAN) invented in 2014
in the computer vision community, provides a suite of novel approaches that can
learn good data representations and generate highly realistic samples. Since
2017, there has been a growth of research into GANs for image augmentation or
synthesis in agriculture for improved model performance. This paper presents an
overview of the evolution of GAN architectures followed by a systematic review
of their application to agriculture
(https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks
for plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping
as well as postharvest detection of fruit defects. Challenges and opportunities
of GANs are discussed for future research.",-0.09731554,0.046735574,0.15025961,C
4671,"We can         systems and encourages further research on tackling domain
draw 3 conclusions from the results.","The proposed DR approach      eÔ¨Åcial to numerous practical real-world robotic autonomous
is only used in the highlighted rows in Table IV.","First, the existence of     shifts when learning novel objects in unseen domains.",2022-04-11 13:16:41+00:00,Few-Shot Object Detection in Unseen Domains,cs.CV,['cs.CV'],"[arxiv.Result.Author('Karim Guirguis'), arxiv.Result.Author('George Eskandar'), arxiv.Result.Author('Matthias Kayser'), arxiv.Result.Author('Bin Yang'), arxiv.Result.Author('Juergen Beyerer')]","Few-shot object detection (FSOD) has thrived in recent years to learn novel
object classes with limited data by transfering knowledge gained on abundant
base classes. FSOD approaches commonly assume that both the scarcely provided
examples of novel classes and test-time data belong to the same domain.
However, this assumption does not hold in various industrial and robotics
applications (e.g., object grasping and manipulation), where a model can learn
novel classes from a source domain while inferring on classes from a different
target domain. In this work, we address the task of zero-shot domain
adaptation, also known as domain generalization, for FSOD. Specifically, we
assume that neither images nor labels of the novel classes in the target domain
are available during training. Our approach for solving the domain gap is
two-fold. First, we leverage a meta-training paradigm, where we learn
domain-invariant features on the base classes. Second, we propose various data
augmentations techniques on the few shots of novel classes to account for all
possible domain-specific information. To further constraint the network into
encoding domain-agnostic class-specific representations only, a contrastive
loss is proposed to maximize the mutual information between foreground
proposals and class prototypes, and to reduce the network's bias to the
background information. Our experiments on the T-LESS dataset show that the
proposed approach succeeds in alleviating the domain gap considerably without
utilizing labels or images of novel categories from the target domain.",-0.15663853,-0.010238474,-0.18435085,B
4720,"Therefore, we release the DAIR-V2X
there are still some tough problems that remain to solve          dataset to boost further study in this field.","This may cause a huge gap between
While 3D object detection has made great progress recently,       theory and practice.",such as blind spots and weak long-distance perception.,2022-04-12 07:13:33+00:00,DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Haibao Yu'), arxiv.Result.Author('Yizhen Luo'), arxiv.Result.Author('Mao Shu'), arxiv.Result.Author('Yiyi Huo'), arxiv.Result.Author('Zebang Yang'), arxiv.Result.Author('Yifeng Shi'), arxiv.Result.Author('Zhenglong Guo'), arxiv.Result.Author('Hanyu Li'), arxiv.Result.Author('Xing Hu'), arxiv.Result.Author('Jirui Yuan'), arxiv.Result.Author('Zaiqing Nie')]","Autonomous driving faces great safety challenges for a lack of global
perspective and the limitation of long-range perception capabilities. It has
been widely agreed that vehicle-infrastructure cooperation is required to
achieve Level 5 autonomy. However, there is still NO dataset from real
scenarios available for computer vision researchers to work on
vehicle-infrastructure cooperation-related problems. To accelerate computer
vision research and innovation for Vehicle-Infrastructure Cooperative
Autonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first
large-scale, multi-modality, multi-view dataset from real scenarios for VICAD.
DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames
are captured from real scenes with 3D annotations. The Vehicle-Infrastructure
Cooperative 3D Object Detection problem (VIC3D) is introduced, formulating the
problem of collaboratively locating and identifying 3D objects using sensory
inputs from both vehicle and infrastructure. In addition to solving traditional
3D object detection problems, the solution of VIC3D needs to consider the
temporal asynchrony problem between vehicle and infrastructure sensors and the
data transmission cost between them. Furthermore, we propose Time Compensation
Late Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark
based on DAIR-V2X. Find data, code, and more up-to-date information at
https://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X.",-0.2777077,0.17646824,-0.0062271506,B
4728,"Moreover, further research should be conducted regarding the development of AI-enabled
glaucoma detection frameworks.","Specifically,
investigations need to be made between OCT and fundus imaging to comprehensively compare both
modalities such that we can determine which is most suitable for AI-enabled glaucoma detection
frameworks.","For example, an area to be studied is the quantification of
uncertainty of the outputs provided by the AI frameworkD .Also, the inclusion of other data sources
needs to be investigated, e.g.",2022-04-12 07:47:13+00:00,Automatic detection of glaucoma via fundus imaging and artificial intelligence: A review,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Lauren Coan'), arxiv.Result.Author('Bryan Williams'), arxiv.Result.Author('Krishna Adithya Venkatesh'), arxiv.Result.Author('Swati Upadhyaya'), arxiv.Result.Author('Silvester Czanner'), arxiv.Result.Author('Rengaraj Venkatesh'), arxiv.Result.Author('Colin E. Willoughby'), arxiv.Result.Author('Srinivasan Kavitha'), arxiv.Result.Author('Gabriela Czanner')]","Glaucoma is a leading cause of irreversible vision impairment globally and
cases are continuously rising worldwide. Early detection is crucial, allowing
timely intervention which can prevent further visual field loss. To detect
glaucoma, examination of the optic nerve head via fundus imaging can be
performed, at the centre of which is the assessment of the optic cup and disc
boundaries. Fundus imaging is non-invasive and low-cost; however, the image
examination relies on subjective, time-consuming, and costly expert
assessments. A timely question to ask is can artificial intelligence mimic
glaucoma assessments made by experts. Namely, can artificial intelligence
automatically find the boundaries of the optic cup and disc (providing a
so-called segmented fundus image) and then use the segmented image to identify
glaucoma with high accuracy. We conducted a comprehensive review on artificial
intelligence-enabled glaucoma detection frameworks that produce and use
segmented fundus images. We found 28 papers and identified two main approaches:
1) logical rule-based frameworks, based on a set of simplistic decision rules;
and 2) machine learning/statistical modelling based frameworks. We summarise
the state-of-art of the two approaches and highlight the key hurdles to
overcome for artificial intelligence-enabled glaucoma detection frameworks to
be translated into clinical practice.",0.0013983846,0.21352683,-0.06866941,B
4729,"We hope that our work will        proved the practicality of object detection by enabling de-
                                        promote further research towards a more robust real-world       tection of instances of unknown classes, there is still the
                                        detection system.","Although OSOD has im-
                                        thorough evaluation protocol.","issue that all identiÔ¨Åed objects of an unknown class share
                                                                                                        the same category as ‚Äúunknown‚Äù (see Fig.",2022-04-12 08:07:01+00:00,Towards Open-Set Object Detection and Discovery,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiyang Zheng'), arxiv.Result.Author('Weihao Li'), arxiv.Result.Author('Jie Hong'), arxiv.Result.Author('Lars Petersson'), arxiv.Result.Author('Nick Barnes')]","With the human pursuit of knowledge, open-set object detection (OSOD) has
been designed to identify unknown objects in a dynamic world. However, an issue
with the current setting is that all the predicted unknown objects share the
same category as ""unknown"", which require incremental learning via a
human-in-the-loop approach to label novel classes. In order to address this
problem, we present a new task, namely Open-Set Object Detection and Discovery
(OSODD). This new task aims to extend the ability of open-set object detectors
to further discover the categories of unknown objects based on their visual
appearance without human effort. We propose a two-stage method that first uses
an open-set object detector to predict both known and unknown objects. Then, we
study the representation of predicted objects in an unsupervised manner and
discover new categories from the set of unknown objects. With this method, a
detector is able to detect objects belonging to known classes and define novel
categories for objects of unknown classes with minimal supervision. We show the
performance of our model on the MS-COCO dataset under a thorough evaluation
protocol. We hope that our work will promote further research towards a more
robust real-world detection system.",-0.2450738,0.027995989,-0.12523592,B
4732,This phenomenon supports further research on                                                   even small magnitudes can break the DNNs‚Äô performance.,"Experimentally, we in-
mance drops dramatically as the perturbation‚Äôs magnitude                                                  deed observe that DNNs are rather brittle against this noise:
increases.","increasing the robustness of point cloud DNNs against sim-
ModelNet40       z‚àíRot.",2022-04-12 10:24:31+00:00,3DeformRS: Certifying Spatial Deformations on Point Clouds,cs.CV,['cs.CV'],"[arxiv.Result.Author('Gabriel P√©rez S.'), arxiv.Result.Author('Juan C. P√©rez'), arxiv.Result.Author('Motasem Alfarra'), arxiv.Result.Author('Silvio Giancola'), arxiv.Result.Author('Bernard Ghanem')]","3D computer vision models are commonly used in security-critical applications
such as autonomous driving and surgical robotics. Emerging concerns over the
robustness of these models against real-world deformations must be addressed
practically and reliably. In this work, we propose 3DeformRS, a method to
certify the robustness of point cloud Deep Neural Networks (DNNs) against
real-world deformations. We developed 3DeformRS by building upon recent work
that generalized Randomized Smoothing (RS) from pixel-intensity perturbations
to vector-field deformations. In particular, we specialized RS to certify DNNs
against parameterized deformations (e.g. rotation, twisting), while enjoying
practical computational costs. We leverage the virtues of 3DeformRS to conduct
a comprehensive empirical study on the certified robustness of four
representative point cloud DNNs on two datasets and against seven different
deformations. Compared to previous approaches for certifying point cloud DNNs,
3DeformRS is fast, scales well with point cloud size, and provides
comparable-to-better certificates. For instance, when certifying a plain
PointNet against a 3{\deg} z-rotation on 1024-point clouds, 3DeformRS grants a
certificate 3x larger and 20x faster than previous work.",0.04535751,-0.020275235,0.3308123,A
4733,"We at-
tations it enjoyed, and so we further study this phenomenon      tributed this superiority to augmentations PointNet enjoyed
in the next subsection.","We thus           Previously, in Figure 3, we observed PointNet‚Äôs domi-
attribute PointNet‚Äôs robustness to the training-time augmen-     nant performance on z‚àíaxis rotation and twisting.",during training.,2022-04-12 10:24:31+00:00,3DeformRS: Certifying Spatial Deformations on Point Clouds,cs.CV,['cs.CV'],"[arxiv.Result.Author('Gabriel P√©rez S.'), arxiv.Result.Author('Juan C. P√©rez'), arxiv.Result.Author('Motasem Alfarra'), arxiv.Result.Author('Silvio Giancola'), arxiv.Result.Author('Bernard Ghanem')]","3D computer vision models are commonly used in security-critical applications
such as autonomous driving and surgical robotics. Emerging concerns over the
robustness of these models against real-world deformations must be addressed
practically and reliably. In this work, we propose 3DeformRS, a method to
certify the robustness of point cloud Deep Neural Networks (DNNs) against
real-world deformations. We developed 3DeformRS by building upon recent work
that generalized Randomized Smoothing (RS) from pixel-intensity perturbations
to vector-field deformations. In particular, we specialized RS to certify DNNs
against parameterized deformations (e.g. rotation, twisting), while enjoying
practical computational costs. We leverage the virtues of 3DeformRS to conduct
a comprehensive empirical study on the certified robustness of four
representative point cloud DNNs on two datasets and against seven different
deformations. Compared to previous approaches for certifying point cloud DNNs,
3DeformRS is fast, scales well with point cloud size, and provides
comparable-to-better certificates. For instance, when certifying a plain
PointNet against a 3{\deg} z-rotation on 1024-point clouds, 3DeformRS grants a
certificate 3x larger and 20x faster than previous work.",0.020334814,0.013464939,0.10431681,A
4753,"To spur further research in this direction,
                                                                                [19] A.","994‚Äì1001, 2018.
mission scenario.","Z. Zhu, N. Atanasov, and K. Daniilidis, ‚ÄúEvent-based visual inertial
we provide an open-source implementation of this work and                             odometry,‚Äù in IEEE Conf.",2022-04-12 15:19:50+00:00,Exploring Event Camera-based Odometry for Planetary Robots,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Florian Mahlknecht'), arxiv.Result.Author('Daniel Gehrig'), arxiv.Result.Author('Jeremy Nash'), arxiv.Result.Author('Friedrich M. Rockenbauer'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Jeff Delaune'), arxiv.Result.Author('Davide Scaramuzza')]","Due to their resilience to motion blur and high robustness in low-light and
high dynamic range conditions, event cameras are poised to become enabling
sensors for vision-based exploration on future Mars helicopter missions.
However, existing event-based visual-inertial odometry (VIO) algorithms either
suffer from high tracking errors or are brittle, since they cannot cope with
significant depth uncertainties caused by an unforeseen loss of tracking or
other effects. In this work, we introduce EKLT-VIO, which addresses both
limitations by combining a state-of-the-art event-based frontend with a
filter-based backend. This makes it both accurate and robust to uncertainties,
outperforming event- and frame-based VIO algorithms on challenging benchmarks
by 32%. In addition, we demonstrate accurate performance in hover-like
conditions (outperforming existing event-based methods) as well as high
robustness in newly collected Mars-like and high-dynamic-range sequences, where
existing frame-based methods fail. In doing so, we show that event-based VIO is
the way forward for vision-based exploration on Mars.",-0.0735175,0.4322025,-0.19677575,B
4754,"To spur
                                                                         further research in this direction, we open-source the imple-
                                                                         mentation of this work and release our Mars-like sequences.","Finally, we demonstrate our method‚Äôs robustness in visually
                                                                         challenging conditions recorded in the JPL Mars Yard and
                                                                         in the Wells Cave, replicating our mission scenario.",8  IEEE ROBOTICS AND AUTOMATION LETTERS.,2022-04-12 15:19:50+00:00,Exploring Event Camera-based Odometry for Planetary Robots,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Florian Mahlknecht'), arxiv.Result.Author('Daniel Gehrig'), arxiv.Result.Author('Jeremy Nash'), arxiv.Result.Author('Friedrich M. Rockenbauer'), arxiv.Result.Author('Benjamin Morrell'), arxiv.Result.Author('Jeff Delaune'), arxiv.Result.Author('Davide Scaramuzza')]","Due to their resilience to motion blur and high robustness in low-light and
high dynamic range conditions, event cameras are poised to become enabling
sensors for vision-based exploration on future Mars helicopter missions.
However, existing event-based visual-inertial odometry (VIO) algorithms either
suffer from high tracking errors or are brittle, since they cannot cope with
significant depth uncertainties caused by an unforeseen loss of tracking or
other effects. In this work, we introduce EKLT-VIO, which addresses both
limitations by combining a state-of-the-art event-based frontend with a
filter-based backend. This makes it both accurate and robust to uncertainties,
outperforming event- and frame-based VIO algorithms on challenging benchmarks
by 32%. In addition, we demonstrate accurate performance in hover-like
conditions (outperforming existing event-based methods) as well as high
robustness in newly collected Mars-like and high-dynamic-range sequences, where
existing frame-based methods fail. In doing so, we show that event-based VIO is
the way forward for vision-based exploration on Mars.",-0.14455903,0.31457973,-0.099992305,B
4759,"Ô¨Åelds, e.g., instance segmentation, object tracking and 3D
                                                                        object detection, warrant further research.",‚Äô2√ó‚Äô: multi-scale training 24 epochs.,"Besides, since our
Method       TS AP AP50 AP75 APS APM APL                                LD shares the equivalent optimization effect to classiÔ¨Åcation
                                                                        KD, some improved KD methods may also bring gain to LD,
             ResNet-50 backbone on val2017                              e.g., Relational KD [23], Self-KD [88], [89], Teacher Assistant
                                                                        KD [24], and Decoupled KD [90], etc.",2022-04-12 17:14:34+00:00,Localization Distillation for Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhaohui Zheng'), arxiv.Result.Author('Rongguang Ye'), arxiv.Result.Author('Qibin Hou'), arxiv.Result.Author('Dongwei Ren'), arxiv.Result.Author('Ping Wang'), arxiv.Result.Author('Wangmeng Zuo'), arxiv.Result.Author('Ming-Ming Cheng')]","Previous knowledge distillation (KD) methods for object detection mostly
focus on feature imitation instead of mimicking the prediction logits due to
its inefficiency in distilling the localization information. In this paper, we
investigate whether logit mimicking always lags behind feature imitation.
Towards this goal, we first present a novel localization distillation (LD)
method which can efficiently transfer the localization knowledge from the
teacher to the student. Second, we introduce the concept of valuable
localization region that can aid to selectively distill the classification and
localization knowledge for a certain region. Combining these two new
components, for the first time, we show that logit mimicking can outperform
feature imitation and the absence of localization distillation is a critical
reason for why logit mimicking underperforms for years. The thorough studies
exhibit the great potential of logit mimicking that can significantly alleviate
the localization ambiguity, learn robust feature representation, and ease the
training difficulty in the early stage. We also provide the theoretical
connection between the proposed LD and the classification KD, that they share
the equivalent optimization effect. Our distillation scheme is simple as well
as effective and can be easily applied to both dense horizontal object
detectors and rotated object detectors. Extensive experiments on the MS COCO,
PASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve
considerable AP improvement without any sacrifice on the inference speed. Our
source code and pretrained models are publicly available at
https://github.com/HikariTJU/LD.",0.007074548,-0.010050276,0.06546365,B
4784,"We further study the
impact of the size of the bounding boxes Bk on model per-
formance.",Ablation of the bounding box size.,"We compute the uniform IoU in Table B.2 for a
varying number of up-sampling steps for the generalization
experiment on the PosePrior dataset (Tab.",2022-04-13 06:02:20+00:00,COAP: Compositional Articulated Occupancy of People,cs.CV,['cs.CV'],"[arxiv.Result.Author('Marko Mihajlovic'), arxiv.Result.Author('Shunsuke Saito'), arxiv.Result.Author('Aayush Bansal'), arxiv.Result.Author('Michael Zollhoefer'), arxiv.Result.Author('Siyu Tang')]","We present a novel neural implicit representation for articulated human
bodies. Compared to explicit template meshes, neural implicit body
representations provide an efficient mechanism for modeling interactions with
the environment, which is essential for human motion reconstruction and
synthesis in 3D scenes. However, existing neural implicit bodies suffer from
either poor generalization on highly articulated poses or slow inference time.
In this work, we observe that prior knowledge about the human body's shape and
kinematic structure can be leveraged to improve generalization and efficiency.
We decompose the full-body geometry into local body parts and employ a
part-aware encoder-decoder architecture to learn neural articulated occupancy
that models complex deformations locally. Our local shape encoder represents
the body deformation of not only the corresponding body part but also the
neighboring body parts. The decoder incorporates the geometric constraints of
local body shape which significantly improves pose generalization. We
demonstrate that our model is suitable for resolving self-intersections and
collisions with 3D environments. Quantitative and qualitative experiments show
that our method largely outperforms existing solutions in terms of both
efficiency and accuracy. The code and models are available at
https://neuralbodies.github.io/COAP/index.html",0.2730646,0.18877894,0.06732422,A
4786,"Therefore, in this paper, we describe the process of creating    tion from observations of artifacts such as different eye colors,
such large-scale dataset that will enable further research in this      unnatural blink and lip-sync issues in deepfake videos.","Deepfake detection methods draw inspira-
tions.","These bi-
important direction.",2022-04-13 08:02:11+00:00,Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhixi Cai'), arxiv.Result.Author('Kalin Stefanov'), arxiv.Result.Author('Abhinav Dhall'), arxiv.Result.Author('Munawar Hayat')]","Due to its high societal impact, deepfake detection is getting active
attention in the computer vision community. Most deepfake detection methods
rely on identity, facial attribute and adversarial perturbation based
spatio-temporal modifications at the whole video or random locations, while
keeping the meaning of the content intact. However, a sophisticated deepfake
may contain only a small segment of video/audio manipulation, through which the
meaning of the content can be, for example, completely inverted from sentiment
perspective. To address this gap, we introduce a content driven audio-visual
deepfake dataset, termed as Localized Audio Visual DeepFake (LAV-DF),
explicitly designed for the task of learning temporal forgery localization.
Specifically, the content driven audio-visual manipulations are performed at
strategic locations in order to change the sentiment polarity of the whole
video. Our baseline method for benchmarking the proposed dataset is a 3DCNN
model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is
guided via contrastive, boundary matching and frame classification loss
functions. Our extensive quantitative analysis demonstrates the strong
performance of the proposed method for both task of temporal forgery
localization and deepfake detection.",-0.13826028,0.0028464189,0.090952545,C
4810,"For future work, we intend to further study the deep-learning methods on
engagement detection and work on various other datasets to further benchmark
our proposed model.","The
limitations of our research is that our techniques cannot diÔ¨Äerentiate between
a human that is partially engaged and fully engaged because there are no clear
and hard boundaries that we can be deÔ¨Åned which clearly separates the two
classes.","Especially, we will focus on combining advanced deep-
learning models‚Äô block such as visual transformer and RepVGG.",2022-04-13 15:24:38+00:00,DMCNet: Diversified Model Combination Network for Understanding Engagement from Video Screengrabs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sarthak Batra'), arxiv.Result.Author('Hewei Wang'), arxiv.Result.Author('Avishek Nag'), arxiv.Result.Author('Philippe Brodeur'), arxiv.Result.Author('Marianne Checkley'), arxiv.Result.Author('Annette Klinkert'), arxiv.Result.Author('Soumyabrata Dev')]","Engagement is an essential indicator of the Quality-of-Learning Experience
(QoLE) and plays a major role in developing intelligent educational interfaces.
The number of people learning through Massively Open Online Courses (MOOCs) and
other online resources has been increasing rapidly because they provide us with
the flexibility to learn from anywhere at any time. This provides a good
learning experience for the students. However, such learning interface requires
the ability to recognize the level of engagement of the students for a holistic
learning experience. This is useful for both students and educators alike.
However, understanding engagement is a challenging task, because of its
subjectivity and ability to collect data. In this paper, we propose a variety
of models that have been trained on an open-source dataset of video
screengrabs. Our non-deep learning models are based on the combination of
popular algorithms such as Histogram of Oriented Gradient (HOG), Support Vector
Machine (SVM), Scale Invariant Feature Transform (SIFT) and Speeded Up Robust
Features (SURF). The deep learning methods include Densely Connected
Convolutional Networks (DenseNet-121), Residual Network (ResNet-18) and
MobileNetV1. We show the performance of each models using a variety of metrics
such as the Gini Index, Adjusted F-Measure (AGF), and Area Under receiver
operating characteristic Curve (AUC). We use various dimensionality reduction
techniques such as Principal Component Analysis (PCA) and t-Distributed
Stochastic Neighbor Embedding (t-SNE) to understand the distribution of data in
the feature sub-space. Our work will thereby assist the educators and students
in obtaining a fruitful and efficient online learning experience.",-0.19840947,-0.20935632,-0.09741901,C
4852,data for further research in this direction.,"We
   ‚Ä¢ Since there exists no publicly available code and            address a more challenging problem of jointly tracking
      datasets to accurately track human-object interactions      human-object interactions in dynamic environments where
      in natural environments, we will release our code and       objects are manipulated.","Dynamic human object interactions Recently, there has
2.",2022-04-14 13:21:19+00:00,BEHAVE: Dataset and Method for Tracking Human Object Interactions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bharat Lal Bhatnagar'), arxiv.Result.Author('Xianghui Xie'), arxiv.Result.Author('Ilya A. Petrov'), arxiv.Result.Author('Cristian Sminchisescu'), arxiv.Result.Author('Christian Theobalt'), arxiv.Result.Author('Gerard Pons-Moll')]","Modelling interactions between humans and objects in natural environments is
central to many applications including gaming, virtual and mixed reality, as
well as human behavior analysis and human-robot collaboration. This challenging
operation scenario requires generalization to vast number of objects, scenes,
and human actions. Unfortunately, there exist no such dataset. Moreover, this
data needs to be acquired in diverse natural environments, which rules out 4D
scanners and marker based capture systems. We present BEHAVE dataset, the first
full body human- object interaction dataset with multi-view RGBD frames and
corresponding 3D SMPL and object fits along with the annotated contacts between
them. We record around 15k frames at 5 locations with 8 subjects performing a
wide range of interactions with 20 common objects. We use this data to learn a
model that can jointly track humans and objects in natural environments with an
easy-to-use portable multi-camera setup. Our key insight is to predict
correspondences from the human and the object to a statistical body model to
obtain human-object contacts during interactions. Our approach can record and
track not just the humans and objects but also their interactions, modeled as
surface contacts, in 3D. Our code and data can be found at:
http://virtualhumans.mpi-inf.mpg.de/behave",-0.2768274,0.07686296,-0.30951202,B
4853,"We hope this work spurs further research into nu-
                                                                    anced applications of computer vision for satellite imagery.","We note that region          the machine learning tools and the task to which they are
                                                                    applied.","References                                                               Marchand, and Victor Lempitsky.",2022-04-14 15:41:39+00:00,Activation Regression for Continuous Domain Generalization with Applications to Crop Classification,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Samar Khanna'), arxiv.Result.Author('Bram Wallace'), arxiv.Result.Author('Kavita Bala'), arxiv.Result.Author('Bharath Hariharan')]","Geographic variance in satellite imagery impacts the ability of machine
learning models to generalise to new regions. In this paper, we model
geographic generalisation in medium resolution Landsat-8 satellite imagery as a
continuous domain adaptation problem, demonstrating how models generalise
better with appropriate domain knowledge. We develop a dataset spatially
distributed across the entire continental United States, providing macroscopic
insight into the effects of geography on crop classification in multi-spectral
and temporally distributed satellite imagery. Our method demonstrates improved
generalisability from 1) passing geographically correlated climate variables
along with the satellite data to a Transformer model and 2) regressing on the
model features to reconstruct these domain variables. Combined, we provide a
novel perspective on geographic generalisation in satellite imagery and a
simple-yet-effective approach to leverage domain knowledge. Code is available
at: \url{https://github.com/samar-khanna/cropmap}",-0.23977908,0.042143576,0.05712937,B
4859,"We hope that our work may spark further research into interactive segmentation
of 3D scenes.","The take-home message we hope to convey with this paper is that current 3D
deep learning technology, in combination with a user in the loop, makes it possible
to efficiently generate high-accuracy segmentations with limited training data.","2 Related Work

Fully-Supervised 3D Instance Segmentation.",2022-04-14 18:31:59+00:00,Interactive Object Segmentation in 3D Point Clouds,cs.CV,['cs.CV'],"[arxiv.Result.Author('Theodora Kontogianni'), arxiv.Result.Author('Ekin Celikkan'), arxiv.Result.Author('Siyu Tang'), arxiv.Result.Author('Konrad Schindler')]","Deep learning depends on large amounts of labeled training data. Manual
labeling is expensive and represents a bottleneck, especially for tasks such as
segmentation, where labels must be assigned down to the level of individual
points. That challenge is even more daunting for 3D data: 3D point clouds
contain millions of points per scene, and their accurate annotation is markedly
more time-consuming. The situation is further aggravated by the added
complexity of user interfaces for 3D point clouds, which slows down annotation
even more. For the case of 2D image segmentation, interactive techniques have
become common, where user feedback in the form of a few clicks guides a
segmentation algorithm -- nowadays usually a neural network -- to achieve an
accurate labeling with minimal effort. Surprisingly, interactive segmentation
of 3D scenes has not been explored much. Previous work has attempted to obtain
accurate 3D segmentation masks using human feedback from the 2D domain, which
is only possible if correctly aligned images are available together with the 3D
point cloud, and it involves switching between the 2D and 3D domains. Here, we
present an interactive 3D object segmentation method in which the user
interacts directly with the 3D point cloud. Importantly, our model does not
require training data from the target domain: when trained on ScanNet, it
performs well on several other datasets with different data characteristics as
well as different object classes. Moreover, our method is orthogonal to
supervised (instance) segmentation methods and can be combined with them to
refine automatic segmentations with minimal human effort.",-0.39858806,0.04087495,0.009200258,B
4873,"Within                         Another area of further research for improvement could
certain k ranges, both Sintel and KITTI samples produce                     be an analysis on the frame rate.",We also experiment with a range of k values.,"Beyond our methods of
noticeable improvements.",2022-04-14 22:58:30+00:00,Imposing Consistency for Optical Flow Estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jisoo Jeong'), arxiv.Result.Author('Jamie Menjay Lin'), arxiv.Result.Author('Fatih Porikli'), arxiv.Result.Author('Nojun Kwak')]","Imposing consistency through proxy tasks has been shown to enhance
data-driven learning and enable self-supervision in various tasks. This paper
introduces novel and effective consistency strategies for optical flow
estimation, a problem where labels from real-world data are very challenging to
derive. More specifically, we propose occlusion consistency and zero forcing in
the forms of self-supervised learning and transformation consistency in the
form of semi-supervised learning. We apply these consistency techniques in a
way that the network model learns to describe pixel-level motions better while
requiring no additional annotations. We demonstrate that our consistency
strategies applied to a strong baseline network model using the original
datasets and labels provide further improvements, attaining the
state-of-the-art results on the KITTI-2015 scene flow benchmark in the
non-stereo category. Our method achieves the best foreground accuracy (4.33% in
Fl-all) over both the stereo and non-stereo categories, even though using only
monocular image inputs.",0.28639883,0.29798442,0.042145487,A
4874,"Within                         Another area of further research for improvement could
certain k ranges, both Sintel and KITTI samples produce                     be an analysis on the frame rate.",We also experiment with a range of k values.,"Beyond our methods of
noticeable improvements.",2022-04-14 22:58:30+00:00,Imposing Consistency for Optical Flow Estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jisoo Jeong'), arxiv.Result.Author('Jamie Menjay Lin'), arxiv.Result.Author('Fatih Porikli'), arxiv.Result.Author('Nojun Kwak')]","Imposing consistency through proxy tasks has been shown to enhance
data-driven learning and enable self-supervision in various tasks. This paper
introduces novel and effective consistency strategies for optical flow
estimation, a problem where labels from real-world data are very challenging to
derive. More specifically, we propose occlusion consistency and zero forcing in
the forms of self-supervised learning and transformation consistency in the
form of semi-supervised learning. We apply these consistency techniques in a
way that the network model learns to describe pixel-level motions better while
requiring no additional annotations. We demonstrate that our consistency
strategies applied to a strong baseline network model using the original
datasets and labels provide further improvements, attaining the
state-of-the-art results on the KITTI-2015 scene flow benchmark in the
non-stereo category. Our method achieves the best foreground accuracy (4.33% in
Fl-all) over both the stereo and non-stereo categories, even though using only
monocular image inputs.",0.28639883,0.29798442,0.042145487,A
4919,Conclusions and directions of further research are addressed in Section 6.,"Experiments showing the very good performance of our method are given in
Section 5.","Our software and video results are available at https://github.com/phflot/dsd_
momag.",2022-04-15 20:24:11+00:00,Lagrangian Motion Magnification with Double Sparse Optical Flow Decomposition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Philipp Flotho'), arxiv.Result.Author('Cosmas Heiss'), arxiv.Result.Author('Gabriele Steidl'), arxiv.Result.Author('Daniel J. Strauss')]","Motion magnification techniques aim at amplifying and hence revealing subtle
motion in videos. There are basically two main approaches to reach this goal,
namely via Eulerian or Lagrangian techniques. While the first one magnifies
motion implicitly by operating directly on image pixels, the Lagrangian
approach uses optical flow techniques to extract and amplify pixel
trajectories. Microexpressions are fast and spatially small facial expressions
that are difficult to detect. In this paper, we propose a novel approach for
local Lagrangian motion magnification of facial micromovements. Our
contribution is three-fold: first, we fine-tune the recurrent all-pairs field
transforms for optical flows (RAFT) deep learning approach for faces by adding
ground truth obtained from the variational dense inverse search (DIS) for
optical flow algorithm applied to the CASME II video set of faces. This enables
us to produce optical flows of facial videos in an efficient and sufficiently
accurate way. Second, since facial micromovements are both local in space and
time, we propose to approximate the optical flow field by sparse components
both in space and time leading to a double sparse decomposition. Third, we use
this decomposition to magnify micro-motions in specific areas of the face,
where we introduce a new forward warping strategy using a triangular splitting
of the image grid and barycentric interpolation of the RGB vectors at the
corners of the transformed triangles. We demonstrate the very good performance
of our approach by various examples.",0.3788013,0.27660513,0.07744864,A_centroid
4934,"However, further research should be            novel robotic device for shoulder rehabilitation,‚Äù 07 2018.
directed towards more augmentation schemes for
skeletal data.","In this paper, we have explored the technique of
masking and pace manipulation as a way of data          [6] S. Guguloth, S. Balasubramanian, and S. Srinivasan, ‚ÄúA
augmentation.","Better augmentation schemes can aid      [7] L. Gauthier, C. Kane, A. Borstad, N. Strahl, G. Uswatte,
the application of newer techniques such as self-            E. Taub, D. Morris, A.",2022-04-16 16:37:30+00:00,A Robust and Scalable Attention Guided Deep Learning Framework for Movement Quality Assessment,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Aditya Kanade'), arxiv.Result.Author('Mansi Sharma'), arxiv.Result.Author('Manivannan Muniyandi')]","Physical rehabilitation programs frequently begin with a brief stay in the
hospital and continue with home-based rehabilitation. Lack of feedback on
exercise correctness is a significant issue in home-based rehabilitation.
Automated movement quality assessment (MQA) using skeletal movement data
(hereafter referred to as skeletal data) collected via depth imaging devices
can assist with home-based rehabilitation by providing the necessary
quantitative feedback. This paper aims to use recent advances in deep learning
to address the problem of MQA. Movement quality score generation is an
essential component of MQA. We propose three novel skeletal data augmentation
schemes. We show that using the proposed augmentations for generating movement
quality scores result in significant performance boosts over existing methods.
Finally, we propose a novel transformer based architecture for MQA. Four novel
feature extractors are proposed and studied that allow the transformer network
to operate on skeletal data. We show that adding the attention mechanism in the
design of the proposed feature extractor allows the transformer network to pay
attention to specific body parts that make a significant contribution towards
executing a movement. We report an improvement in movement quality score
prediction of 12% on UI-PRMD dataset and 21% on KIMORE dataset compared to the
existing methods.",0.056686856,0.14658809,-0.19848675,A
4935,"In this
subsection, we further study the system performance under more challenging scenarios.","5.4 Impact of Environmental Change

We have demonstrated that our system is able to achieve environment-independent where the performance
stays the same when the location of the furniture was changed in the overall performance subsection.","In particular, we use the
data collected in one environment (e.g., living room or dining room) to train our system, and then evaluate the
performance when the system is operating in a different environment (e.g., bedroom).",2022-04-16 21:58:24+00:00,3D Human Pose Estimation for Free-from and Moving Activities Using WiFi,cs.CV,"['cs.CV', 'cs.HC']","[arxiv.Result.Author('Yili Ren'), arxiv.Result.Author('Jie Yang')]","This paper presents GoPose, a 3D skeleton-based human pose estimation system
that uses WiFi devices at home. Our system leverages the WiFi signals reflected
off the human body for 3D pose estimation. In contrast to prior systems that
need specialized hardware or dedicated sensors, our system does not require a
user to wear or carry any sensors and can reuse the WiFi devices that already
exist in a home environment for mass adoption. To realize such a system, we
leverage the 2D AoA spectrum of the signals reflected from the human body and
the deep learning techniques. In particular, the 2D AoA spectrum is proposed to
locate different parts of the human body as well as to enable
environment-independent pose estimation. Deep learning is incorporated to model
the complex relationship between the 2D AoA spectrums and the 3D skeletons of
the human body for pose tracking. Our evaluation results show GoPose achieves
around 4.7cm of accuracy under various scenarios including tracking unseen
activities and under NLoS scenarios.",0.1529487,0.112173975,-0.22523057,A
4936,"We further study the impact of different packet
rateson the performance ofour system.","5.6 Impact of Packet Rate

For our evaluation, the default packet rate is set at 1000pkts/s.","Weset the packet rate at 250pkts/s, 500pkts/s and 1000pkts/s, respectively.",2022-04-16 21:58:24+00:00,3D Human Pose Estimation for Free-from and Moving Activities Using WiFi,cs.CV,"['cs.CV', 'cs.HC']","[arxiv.Result.Author('Yili Ren'), arxiv.Result.Author('Jie Yang')]","This paper presents GoPose, a 3D skeleton-based human pose estimation system
that uses WiFi devices at home. Our system leverages the WiFi signals reflected
off the human body for 3D pose estimation. In contrast to prior systems that
need specialized hardware or dedicated sensors, our system does not require a
user to wear or carry any sensors and can reuse the WiFi devices that already
exist in a home environment for mass adoption. To realize such a system, we
leverage the 2D AoA spectrum of the signals reflected from the human body and
the deep learning techniques. In particular, the 2D AoA spectrum is proposed to
locate different parts of the human body as well as to enable
environment-independent pose estimation. Deep learning is incorporated to model
the complex relationship between the 2D AoA spectrums and the 3D skeletons of
the human body for pose tracking. Our evaluation results show GoPose achieves
around 4.7cm of accuracy under various scenarios including tracking unseen
activities and under NLoS scenarios.",0.46654317,0.16193017,0.032446228,A
4937,Deep convolutional neural networks (CNNs)               line and toolbox for further research in this topic.,"Codes and pre-trained models of these
the development of deep learning, SR has witnessed signifi-            methods are made publicly available to serve as a base-
cant progress.","have been applied to learn the end-to-end mapping function
from RGB images to HSI cubes.",2022-04-17 02:39:32+00:00,MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuanhao Cai'), arxiv.Result.Author('Jing Lin'), arxiv.Result.Author('Zudi Lin'), arxiv.Result.Author('Haoqian Wang'), arxiv.Result.Author('Yulun Zhang'), arxiv.Result.Author('Hanspeter Pfister'), arxiv.Result.Author('Radu Timofte'), arxiv.Result.Author('Luc Van Gool')]","Existing leading methods for spectral reconstruction (SR) focus on designing
deeper or wider convolutional neural networks (CNNs) to learn the end-to-end
mapping from the RGB image to its hyperspectral image (HSI). These CNN-based
methods achieve impressive restoration performance while showing limitations in
capturing the long-range dependencies and self-similarity prior. To cope with
this problem, we propose a novel Transformer-based method, Multi-stage
Spectral-wise Transformer (MST++), for efficient spectral reconstruction. In
particular, we employ Spectral-wise Multi-head Self-attention (S-MSA) that is
based on the HSI spatially sparse while spectrally self-similar nature to
compose the basic unit, Spectral-wise Attention Block (SAB). Then SABs build up
Single-stage Spectral-wise Transformer (SST) that exploits a U-shaped structure
to extract multi-resolution contextual information. Finally, our MST++,
cascaded by several SSTs, progressively improves the reconstruction quality
from coarse to fine. Comprehensive experiments show that our MST++
significantly outperforms other state-of-the-art methods. In the NTIRE 2022
Spectral Reconstruction Challenge, our approach won the First place. Code and
pre-trained models are publicly available at
https://github.com/caiyuanhao1998/MST-plus-plus.",-0.2765978,0.01523002,0.17349741,B
4962,"This means all species are contained in both           highlight the challenges and reinforce the compelling need
the train and test sets, for evaluating the efficacy of models      for further research in the areas of video grounding, ani-
when estimating pose of all animals.","The variations
                                                                    within and between classes of animals for both action recog-
   Protocol 1: The whole dataset with all animal species is         nition and pose estimation illustrate the advantage of our
split to the training set (80% of samples) and test set (20%        dataset being diverse in nature, and yet at the same time
of samples).",mal action recognition and pose estimation.,2022-04-18 02:05:15+00:00,Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xun Long Ng'), arxiv.Result.Author('Kian Eng Ong'), arxiv.Result.Author('Qichen Zheng'), arxiv.Result.Author('Yun Ni'), arxiv.Result.Author('Si Yong Yeo'), arxiv.Result.Author('Jun Liu')]","Understanding animals' behaviors is significant for a wide range of
applications. However, existing animal behavior datasets have limitations in
multiple aspects, including limited numbers of animal classes, data samples and
provided tasks, and also limited variations in environmental conditions and
viewpoints. To address these limitations, we create a large and diverse
dataset, Animal Kingdom, that provides multiple annotated tasks to enable a
more thorough understanding of natural animal behaviors. The wild animal
footages used in our dataset record different times of the day in extensive
range of environments containing variations in backgrounds, viewpoints,
illumination and weather conditions. More specifically, our dataset contains 50
hours of annotated videos to localize relevant animal behavior segments in long
videos for the video grounding task, 30K video sequences for the fine-grained
multi-label action recognition task, and 33K frames for the pose estimation
task, which correspond to a diverse range of animals with 850 species across 6
major animal classes. Such a challenging and comprehensive dataset shall be
able to facilitate the community to develop, adapt, and evaluate various types
of advanced methods for animal behavior analysis. Moreover, we propose a
Collaborative Action Recognition (CARe) model that learns general and specific
features for action recognition with unseen new animals. This method achieves
promising performance in our experiments. Our dataset can be found at
https://sutdcv.github.io/Animal-Kingdom.",-0.21519566,0.0067959763,-0.22569749,B
4963,"This means all species are contained in both           highlight the challenges and reinforce the compelling need
the train and test sets, for evaluating the efficacy of models      for further research in the areas of video grounding, ani-
when estimating pose of all animals.","The variations
                                                                    within and between classes of animals for both action recog-
   Protocol 1: The whole dataset with all animal species is         nition and pose estimation illustrate the advantage of our
split to the training set (80% of samples) and test set (20%        dataset being diverse in nature, and yet at the same time
of samples).",mal action recognition and pose estimation.,2022-04-18 02:05:15+00:00,Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xun Long Ng'), arxiv.Result.Author('Kian Eng Ong'), arxiv.Result.Author('Qichen Zheng'), arxiv.Result.Author('Yun Ni'), arxiv.Result.Author('Si Yong Yeo'), arxiv.Result.Author('Jun Liu')]","Understanding animals' behaviors is significant for a wide range of
applications. However, existing animal behavior datasets have limitations in
multiple aspects, including limited numbers of animal classes, data samples and
provided tasks, and also limited variations in environmental conditions and
viewpoints. To address these limitations, we create a large and diverse
dataset, Animal Kingdom, that provides multiple annotated tasks to enable a
more thorough understanding of natural animal behaviors. The wild animal
footages used in our dataset record different times of the day in extensive
range of environments containing variations in backgrounds, viewpoints,
illumination and weather conditions. More specifically, our dataset contains 50
hours of annotated videos to localize relevant animal behavior segments in long
videos for the video grounding task, 30K video sequences for the fine-grained
multi-label action recognition task, and 33K frames for the pose estimation
task, which correspond to a diverse range of animals with 850 species across 6
major animal classes. Such a challenging and comprehensive dataset shall be
able to facilitate the community to develop, adapt, and evaluate various types
of advanced methods for animal behavior analysis. Moreover, we propose a
Collaborative Action Recognition (CARe) model that learns general and specific
features for action recognition with unseen new animals. This method achieves
promising performance in our experiments. Our dataset can be found at
https://sutdcv.github.io/Animal-Kingdom.",-0.21519566,0.0067959763,-0.22569749,B
4969,"To further study the representation structure             Qualitative Analysis From qualitative aspect, in
learned in our Ge2-AE, we plot the Centered Kernel Align-          Fig.","tribution of intermediate features of representation and its
robustness.","5, we visualize the predicted results of baseline
ment (CKA) similarities [23] between all pairs of layers           MAE [18] and our Ge2-AE adopting ViT-B as encoder
across MAE and our Ge2-AE after pre-trained or Ô¨Åne-tuned           and taking masked images with 75% ratio.",2022-04-18 09:22:55+00:00,The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Xinghua Jiang'), arxiv.Result.Author('Xin Li'), arxiv.Result.Author('Antai Guo'), arxiv.Result.Author('Deqiang Jiang'), arxiv.Result.Author('Bo Ren')]","The self-supervised Masked Image Modeling (MIM) schema, following
""mask-and-reconstruct"" pipeline of recovering contents from masked image, has
recently captured the increasing interest in the multimedia community, owing to
the excellent ability of learning visual representation from unlabeled data.
Aiming at learning representations with high semantics abstracted, a group of
works attempts to reconstruct non-semantic pixels with large-ratio masking
strategy, which may suffer from ""over-smoothing"" problem, while others directly
infuse semantics into targets in off-line way requiring extra data. Different
from them, we shift the perspective to the Fourier domain which naturally has
global perspective and present a new Masked Image Modeling (MIM), termed
Geminated Gestalt Autoencoder (Ge$^2$-AE) for visual pre-training.
Specifically, we equip our model with geminated decoders in charge of
reconstructing image contents from both pixel and frequency space, where each
other serves as not only the complementation but also the reciprocal
constraints. Through this way, more robust representations can be learned in
the pre-trained encoders, of which the effectiveness is confirmed by the
juxtaposing experimental results on downstream recognition tasks. We also
conduct several quantitative and qualitative experiments to investigate the
learning behavior of our method. To our best knowledge, this is the first MIM
work to solve the visual pre-training through the lens of frequency domain.",0.014550401,-0.18856657,-0.03596763,C
4974,"with the unlabeled         lenge prompts further research in the field of low supervi-
ground-level images).","This chal-
level data (labeled only or semi-sup.","The performance proposed by our            sion computer vision for herbage biomass prediction from
low supervision algorithm is close to be on par with human       drone images.",2022-04-18 12:11:15+00:00,Unsupervised domain adaptation and super resolution on drone images for autonomous dry herbage biomass estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Paul Albert'), arxiv.Result.Author('Mohamed Saadeldin'), arxiv.Result.Author('Badri Narayanan'), arxiv.Result.Author('Jaime Fernandez'), arxiv.Result.Author('Brian Mac Namee'), arxiv.Result.Author('Deirdre Hennessey'), arxiv.Result.Author(""Noel E. O'Connor""), arxiv.Result.Author('Kevin McGuinness')]","Herbage mass yield and composition estimation is an important tool for dairy
farmers to ensure an adequate supply of high quality herbage for grazing and
subsequently milk production. By accurately estimating herbage mass and
composition, targeted nitrogen fertiliser application strategies can be
deployed to improve localised regions in a herbage field, effectively reducing
the negative impacts of over-fertilization on biodiversity and the environment.
In this context, deep learning algorithms offer a tempting alternative to the
usual means of sward composition estimation, which involves the destructive
process of cutting a sample from the herbage field and sorting by hand all
plant species in the herbage. The process is labour intensive and time
consuming and so not utilised by farmers. Deep learning has been successfully
applied in this context on images collected by high-resolution cameras on the
ground. Moving the deep learning solution to drone imaging, however, has the
potential to further improve the herbage mass yield and composition estimation
task by extending the ground-level estimation to the large surfaces occupied by
fields/paddocks. Drone images come at the cost of lower resolution views of the
fields taken from a high altitude and requires further herbage ground-truth
collection from the large surfaces covered by drone images. This paper proposes
to transfer knowledge learned on ground-level images to raw drone images in an
unsupervised manner. To do so, we use unpaired image style translation to
enhance the resolution of drone images by a factor of eight and modify them to
appear closer to their ground-level counterparts. We then ...
~\url{www.github.com/PaulAlbert31/Clover_SSL}.",-0.15268323,0.06328641,0.056390814,B
4985,"Formally, Edata(I, Itrain)
      open issues inspiring further research in DE-GANs.","Finally, we point out                and limited training dataset Itrain.","and Eemp(H, Pinit, Itrain) are deÔ¨Åned as:

2 Problem DeÔ¨Ånition                                                     Edata(I, Itrain) = M (Pexp(I), Pemp(Itrain)),

In this section, we provide a formal description of DE-GANs             Eemp(H, Pinit, Itrain) = M (Pemp(Itrain), Pgen(H, Pinit, Itrain)).",2022-04-18 14:14:09+00:00,A Comprehensive Survey on Data-Efficient GANs in Image Generation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Ziqiang Li'), arxiv.Result.Author('Xintian Wu'), arxiv.Result.Author('Beihao Xia'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Chaoyue Wang'), arxiv.Result.Author('Bin Li')]","Generative Adversarial Networks (GANs) have achieved remarkable achievements
in image synthesis. These successes of GANs rely on large scale datasets,
requiring too much cost. With limited training data, how to stable the training
process of GANs and generate realistic images have attracted more attention.
The challenges of Data-Efficient GANs (DE-GANs) mainly arise from three
aspects: (i) Mismatch Between Training and Target Distributions, (ii)
Overfitting of the Discriminator, and (iii) Imbalance Between Latent and Data
Spaces. Although many augmentation and pre-training strategies have been
proposed to alleviate these issues, there lacks a systematic survey to
summarize the properties, challenges, and solutions of DE-GANs. In this paper,
we revisit and define DE-GANs from the perspective of distribution
optimization. We conclude and analyze the challenges of DE-GANs. Meanwhile, we
propose a taxonomy, which classifies the existing methods into three
categories: Data Selection, GANs Optimization, and Knowledge Sharing. Last but
not the least, we attempt to highlight the current problems and the future
directions.",0.14865302,-0.15984981,0.045429975,C
4986,"Formally, Edata(I, Itrain)
      open issues inspiring further research in DE-GANs.","Finally, we point out                and limited training dataset Itrain.","and Eemp(H, Pinit, Itrain) are deÔ¨Åned as:

2 Problem DeÔ¨Ånition                                                     Edata(I, Itrain) = M (Pexp(I), Pemp(Itrain)),

In this section, we provide a formal description of DE-GANs             Eemp(H, Pinit, Itrain) = M (Pemp(Itrain), Pgen(H, Pinit, Itrain)).",2022-04-18 14:14:09+00:00,A Comprehensive Survey on Data-Efficient GANs in Image Generation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Ziqiang Li'), arxiv.Result.Author('Beihao Xia'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Chaoyue Wang'), arxiv.Result.Author('Bin Li')]","Generative Adversarial Networks (GANs) have achieved remarkable achievements
in image synthesis. These successes of GANs rely on large scale datasets,
requiring too much cost. With limited training data, how to stable the training
process of GANs and generate realistic images have attracted more attention.
The challenges of Data-Efficient GANs (DE-GANs) mainly arise from three
aspects: (i) Mismatch Between Training and Target Distributions, (ii)
Overfitting of the Discriminator, and (iii) Imbalance Between Latent and Data
Spaces. Although many augmentation and pre-training strategies have been
proposed to alleviate these issues, there lacks a systematic survey to
summarize the properties, challenges, and solutions of DE-GANs. In this paper,
we revisit and define DE-GANs from the perspective of distribution
optimization. We conclude and analyze the challenges of DE-GANs. Meanwhile, we
propose a taxonomy, which classifies the existing methods into three
categories: Data Selection, GANs Optimization, and Knowledge Sharing. Last but
not the least, we attempt to highlight the current problems and the future
directions.",0.14865302,-0.15984981,0.045429975,C
4995,"This way, we        further research on this new proposed setting for Embodied
can evaluate the whole exploration trend, and not only its Ô¨Ånal    AI.","and IoU    We believe that the results presented in this paper motivate
over different time-steps during the episodes.",point.,2022-04-18 18:30:56+00:00,Spot the Difference: A Novel Task for Embodied Agents in Changing Environments,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Federico Landi'), arxiv.Result.Author('Roberto Bigazzi'), arxiv.Result.Author('Marcella Cornia'), arxiv.Result.Author('Silvia Cascianelli'), arxiv.Result.Author('Lorenzo Baraldi'), arxiv.Result.Author('Rita Cucchiara')]","Embodied AI is a recent research area that aims at creating intelligent
agents that can move and operate inside an environment. Existing approaches in
this field demand the agents to act in completely new and unexplored scenes.
However, this setting is far from realistic use cases that instead require
executing multiple tasks in the same environment. Even if the environment
changes over time, the agent could still count on its global knowledge about
the scene while trying to adapt its internal representation to the current
state of the environment. To make a step towards this setting, we propose Spot
the Difference: a novel task for Embodied AI where the agent has access to an
outdated map of the environment and needs to recover the correct layout in a
fixed time budget. To this end, we collect a new dataset of occupancy maps
starting from existing datasets of 3D spaces and generating a number of
possible layouts for a single environment. This dataset can be employed in the
popular Habitat simulator and is fully compliant with existing methods that
employ reconstructed occupancy maps during navigation. Furthermore, we propose
an exploration policy that can take advantage of previous knowledge of the
environment and identify changes in the scene faster and more effectively than
existing agents. Experimental results show that the proposed architecture
outperforms existing state-of-the-art models for exploration on this new
setting.",0.10816042,-0.036911167,-0.3357584,A
5003,"Here we discuss opportunities      Comparing Datasets and Subsets Thereof Such com-
for further research on visualizing CV datasets.","The techniques presented focus almost ex-
clusively on image datasets.","parisons have various applications, e.g., to assessing do-
                                                                main suitability in transfer learning and analyzing distribu-
Video, Point Cloud, 3D, and Keypoint Datasets Gener-            tion shift [57].",2022-04-19 01:04:28+00:00,A Tour of Visualization Techniques for Computer Vision Datasets,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Bilal Alsallakh'), arxiv.Result.Author('Pamela Bhattacharya'), arxiv.Result.Author('Vanessa Feng'), arxiv.Result.Author('Narine Kokhlikyan'), arxiv.Result.Author('Orion Reblitz-Richardson'), arxiv.Result.Author('Rahul Rajan'), arxiv.Result.Author('David Yan')]","We survey a number of data visualization techniques for analyzing Computer
Vision (CV) datasets. These techniques help us understand properties and latent
patterns in such data, by applying dataset-level analysis. We present various
examples of how such analysis helps predict the potential impact of the dataset
properties on CV models and informs appropriate mitigation of their
shortcomings. Finally, we explore avenues for further visualization techniques
of different modalities of CV datasets as well as ones that are tailored to
support specific CV tasks and analysis needs.",-0.24060246,-0.011543915,-0.004475318,B
5004,"alizations can hence assign visual primacy in their designs     We Ô¨Ånally explored a few avenues of how visualization can
to different facets, such as the class hierarchy in Figure 12   support further research on dataset understanding, towards
and the spatial distribution in Figure 5.                       robust and transparent CV.",The corresponding visu-       understand the impact of dataset properties on CV models.,"Visually Linking Model Results with Dataset Properties          References
Such an analysis can be useful to explain model behav-
ior and Ô¨Ånd a suitable mitigation for observed shortcom-         [1] B. Alsallakh, N. Kokhlikyan, V. Miglani, S. Muttepawar, E.
                                                                      Wang, S. Zhang, D. Adkins, and O. Reblitz-Richardson.",2022-04-19 01:04:28+00:00,A Tour of Visualization Techniques for Computer Vision Datasets,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Bilal Alsallakh'), arxiv.Result.Author('Pamela Bhattacharya'), arxiv.Result.Author('Vanessa Feng'), arxiv.Result.Author('Narine Kokhlikyan'), arxiv.Result.Author('Orion Reblitz-Richardson'), arxiv.Result.Author('Rahul Rajan'), arxiv.Result.Author('David Yan')]","We survey a number of data visualization techniques for analyzing Computer
Vision (CV) datasets. These techniques help us understand properties and latent
patterns in such data, by applying dataset-level analysis. We present various
examples of how such analysis helps predict the potential impact of the dataset
properties on CV models and informs appropriate mitigation of their
shortcomings. Finally, we explore avenues for further visualization techniques
of different modalities of CV datasets as well as ones that are tailored to
support specific CV tasks and analysis needs.",-0.038079694,-0.046359695,-0.06087247,C
5019,"We further study the effects         liers mentioned above, we conduct the experiment to an-
of hyper-parameter p on PIPAL [19] and KADID-10k [35],            alyze the positive unlabeled data and outliers selected by
because the distortion types of these two datasets are very       the classiÔ¨Åer.","As shown in Table H, region size p = 8 is the best choice
on PIPAL, while original sliced Wasserstein (Global) yields          To verify that the binary classiÔ¨Åer can eliminate the out-
signiÔ¨Åcant performance drop.","Take our FR-IQA as an example, the PIPAL
different.",2022-04-19 09:10:06+00:00,Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Yue Cao'), arxiv.Result.Author('Zhaolin Wan'), arxiv.Result.Author('Dongwei Ren'), arxiv.Result.Author('Zifei Yan'), arxiv.Result.Author('Wangmeng Zuo')]","Full-reference (FR) image quality assessment (IQA) evaluates the visual
quality of a distorted image by measuring its perceptual difference with
pristine-quality reference, and has been widely used in low-level vision tasks.
Pairwise labeled data with mean opinion score (MOS) are required in training
FR-IQA model, but is time-consuming and cumbersome to collect. In contrast,
unlabeled data can be easily collected from an image degradation or restoration
process, making it encouraging to exploit unlabeled training data to boost
FR-IQA performance. Moreover, due to the distribution inconsistency between
labeled and unlabeled data, outliers may occur in unlabeled data, further
increasing the training difficulty. In this paper, we suggest to incorporate
semi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled
data while mitigating the adverse effect of outliers. Particularly, by treating
all labeled data as positive samples, PU learning is leveraged to identify
negative samples (i.e., outliers) from unlabeled data. Semi-supervised learning
(SSL) is further deployed to exploit positive unlabeled data by dynamically
generating pseudo-MOS. We adopt a dual-branch network including reference and
distortion branches. Furthermore, spatial attention is introduced in the
reference branch to concentrate more on the informative regions, and sliced
Wasserstein distance is used for robust difference map computation to address
the misalignment issues caused by images recovered by GAN models. Extensive
experiments show that our method performs favorably against state-of-the-arts
on the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ.",0.38310993,0.094486535,0.13494557,A
5020,"This is left

for further research.","Could we

Ô¨Ånd other unbiased methods toward a particular operation or Sel ?","Binarization Once the training is done, we can binarize each element indepen-
dently to binarize the whole BiMoNN.",2022-04-19 09:26:11+00:00,Binary Multi Channel Morphological Neural Network,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Theodore Aouad'), arxiv.Result.Author('Hugues Talbot')]","Neural networks and particularly Deep learning have been comparatively little
studied from the theoretical point of view. Conversely, Mathematical Morphology
is a discipline with solid theoretical foundations. We combine these domains to
propose a new type of neural architecture that is theoretically more
explainable. We introduce a Binary Morphological Neural Network (BiMoNN) built
upon the convolutional neural network. We design it for learning morphological
networks with binary inputs and outputs. We demonstrate an equivalence between
BiMoNNs and morphological operators that we can use to binarize entire
networks. These can learn classical morphological operators and show promising
results on a medical imaging application.",0.2083798,-0.05763644,-0.016440643,A
5022,"We will
                                                                        conduct further research to solve the ‚Äúdouble-edged sword‚Äù issue
                                                                        of transformer backbones for saliency detection.","However, for small salient object, it
                                                                        can also lead to signiÔ¨Åcant amount of false positives.","Uncertainty modeling difference: The deterministic performance
                                                                        in Table 3 and Table 4 shows advantage of Swin transformer for
                                                                        saliency detection.",2022-04-19 10:51:00+00:00,An Energy-Based Prior for Generative Saliency,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Jianwen Xie'), arxiv.Result.Author('Nick Barnes'), arxiv.Result.Author('Ping Li')]","We propose a novel energy-based prior for generative saliency prediction,
where the latent variables follow an informative energy-based prior. Both the
saliency generator and the energy-based prior are jointly trained via Markov
chain Monte Carlo-based maximum likelihood estimation, in which the sampling
from the intractable posterior and prior distributions of the latent variables
are performed by Langevin dynamics. With the generative saliency model, we can
obtain a pixel-wise uncertainty map from an image, indicating model confidence
in the saliency prediction. Different from existing generative models, which
define the prior distribution of the latent variable as a simple isotropic
Gaussian distribution, our model uses an energy-based informative prior which
can be more expressive in capturing the latent space of the data. With the
informative energy-based prior, we extend the Gaussian distribution assumption
of generative models to achieve a more representative distribution of the
latent space, leading to more reliable uncertainty estimation. We apply the
proposed frameworks to both RGB and RGB-D salient object detection tasks with
both transformer and convolutional neural network backbones. Experimental
results show that our generative saliency model with an energy-based prior can
achieve not only accurate saliency predictions but also reliable uncertainty
maps that are consistent with human perception.",-0.09759688,0.15791576,0.011014893,B
5036,"It is mainly because the Gc is trained with limited
   To further study the impact of unlabeled datasets with        exemplars (the input of Gc are exemplars and the generated
different scales, we evaluate our method using different sub-    samples produced by itself), and a deeper Gc will easily
sets of ImageNet-ec.","When
lower than ‚ÄòImageNet-full‚Äô in Table 3.                           the feature generator G gets deeper, the performance drops
                                                                 slightly.","Specifically, we select the first 100,      overfit these inputs and degenerate its performance.",2022-04-19 15:15:18+00:00,Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu-Ming Tang'), arxiv.Result.Author('Yi-Xing Peng'), arxiv.Result.Author('Wei-Shi Zheng')]","Deep neural network (DNN) suffers from catastrophic forgetting when learning
incrementally, which greatly limits its applications. Although maintaining a
handful of samples (called `exemplars`) of each task could alleviate forgetting
to some extent, existing methods are still limited by the small number of
exemplars since these exemplars are too few to carry enough task-specific
knowledge, and therefore the forgetting remains. To overcome this problem, we
propose to `imagine` diverse counterparts of given exemplars referring to the
abundant semantic-irrelevant information from unlabeled data. Specifically, we
develop a learnable feature generator to diversify exemplars by adaptively
generating diverse counterparts of exemplars based on semantic information from
exemplars and semantically-irrelevant information from unlabeled data. We
introduce semantic contrastive learning to enforce the generated samples to be
semantic consistent with exemplars and perform semanticdecoupling contrastive
learning to encourage diversity of generated samples. The diverse generated
samples could effectively prevent DNN from forgetting when learning new tasks.
Our method does not bring any extra inference cost and outperforms
state-of-the-art methods on two benchmarks CIFAR-100 and ImageNet-Subset by a
clear margin.",-0.0019794768,-0.22271185,0.25876358,C
5079,"Our results support further research                                [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.","data annotation paradigm for VMR, and ViGA is a feasible method
for glance annotated VMR.",2016.,2022-04-20 11:59:17+00:00,Video Moment Retrieval from Text Queries via Single Frame Annotation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ran Cui'), arxiv.Result.Author('Tianwen Qian'), arxiv.Result.Author('Pai Peng'), arxiv.Result.Author('Elena Daskalaki'), arxiv.Result.Author('Jingjing Chen'), arxiv.Result.Author('Xiaowei Guo'), arxiv.Result.Author('Huyang Sun'), arxiv.Result.Author('Yu-Gang Jiang')]","Video moment retrieval aims at finding the start and end timestamps of a
moment (part of a video) described by a given natural language query. Fully
supervised methods need complete temporal boundary annotations to achieve
promising results, which is costly since the annotator needs to watch the whole
moment. Weakly supervised methods only rely on the paired video and query, but
the performance is relatively poor. In this paper, we look closer into the
annotation process and propose a new paradigm called ""glance annotation"". This
paradigm requires the timestamp of only one single random frame, which we refer
to as a ""glance"", within the temporal boundary of the fully supervised
counterpart. We argue this is beneficial because comparing to weak supervision,
trivial cost is added yet more potential in performance is provided. Under the
glance annotation setting, we propose a method named as Video moment retrieval
via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input
video into clips and contrasts between clips and queries, in which glance
guided Gaussian distributed weights are assigned to all clips. Our extensive
experiments indicate that ViGA achieves better results than the
state-of-the-art weakly supervised methods by a large margin, even comparable
to fully supervised methods in some cases.",0.037347067,-0.001792442,-0.21017434,A
5080,"Our results support further research                                [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.","data annotation paradigm for VMR, and ViGA is a feasible method
for glance annotated VMR.",2016.,2022-04-20 11:59:17+00:00,Video Moment Retrieval from Text Queries via Single Frame Annotation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ran Cui'), arxiv.Result.Author('Tianwen Qian'), arxiv.Result.Author('Pai Peng'), arxiv.Result.Author('Elena Daskalaki'), arxiv.Result.Author('Jingjing Chen'), arxiv.Result.Author('De Wei'), arxiv.Result.Author('Huyang Sun'), arxiv.Result.Author('Yu-Gang Jiang')]","Video moment retrieval aims at finding the start and end timestamps of a
moment (part of a video) described by a given natural language query. Fully
supervised methods need complete temporal boundary annotations to achieve
promising results, which is costly since the annotator needs to watch the whole
moment. Weakly supervised methods only rely on the paired video and query, but
the performance is relatively poor. In this paper, we look closer into the
annotation process and propose a new paradigm called ""glance annotation"". This
paradigm requires the timestamp of only one single random frame, which we refer
to as a ""glance"", within the temporal boundary of the fully supervised
counterpart. We argue this is beneficial because comparing to weak supervision,
trivial cost is added yet more potential in performance is provided. Under the
glance annotation setting, we propose a method named as Video moment retrieval
via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input
video into clips and contrasts between clips and queries, in which glance
guided Gaussian distributed weights are assigned to all clips. Our extensive
experiments indicate that ViGA achieves better results than the
state-of-the-art weakly supervised methods by a large margin, even comparable
to fully supervised methods in some cases.",0.037347067,-0.001792442,-0.21017434,A
5081,"Our results support further research                                [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.","data annotation paradigm for VMR, and ViGA is a feasible method
for glance annotated VMR.",2016.,2022-04-20 11:59:17+00:00,Video Moment Retrieval from Text Queries via Single Frame Annotation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ran Cui'), arxiv.Result.Author('Tianwen Qian'), arxiv.Result.Author('Pai Peng'), arxiv.Result.Author('Elena Daskalaki'), arxiv.Result.Author('Jingjing Chen'), arxiv.Result.Author('Xiaowei Guo'), arxiv.Result.Author('Huyang Sun'), arxiv.Result.Author('Yu-Gang Jiang')]","Video moment retrieval aims at finding the start and end timestamps of a
moment (part of a video) described by a given natural language query. Fully
supervised methods need complete temporal boundary annotations to achieve
promising results, which is costly since the annotator needs to watch the whole
moment. Weakly supervised methods only rely on the paired video and query, but
the performance is relatively poor. In this paper, we look closer into the
annotation process and propose a new paradigm called ""glance annotation"". This
paradigm requires the timestamp of only one single random frame, which we refer
to as a ""glance"", within the temporal boundary of the fully supervised
counterpart. We argue this is beneficial because comparing to weak supervision,
trivial cost is added yet more potential in performance is provided. Under the
glance annotation setting, we propose a method named as Video moment retrieval
via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input
video into clips and contrasts between clips and queries, in which glance
guided Gaussian distributed weights are assigned to all clips. Our extensive
experiments indicate that ViGA achieves better results than the
state-of-the-art weakly supervised methods by a large margin, even comparable
to fully supervised methods in some cases.",0.037347067,-0.001792442,-0.21017434,A
5107,"The results motivate us to             The aforementioned observations reveal the visual be-
further study the correlation between attention and task          haviors of humans and machines when deriving different
performance, and how different attention patterns lead to         answers.","between the attention, the task outcome, and the interme-
diate decision-making process.","More importantly, it shows that, unlike humans,
diverse answers.",2022-04-20 20:32:31+00:00,"Attention in Reasoning: Dataset, Analysis, and Modeling",cs.CV,['cs.CV'],"[arxiv.Result.Author('Shi Chen'), arxiv.Result.Author('Ming Jiang'), arxiv.Result.Author('Jinhui Yang'), arxiv.Result.Author('Qi Zhao')]","While attention has been an increasingly popular component in deep neural
networks to both interpret and boost the performance of models, little work has
examined how attention progresses to accomplish a task and whether it is
reasonable. In this work, we propose an Attention with Reasoning capability
(AiR) framework that uses attention to understand and improve the process
leading to task outcomes. We first define an evaluation metric based on a
sequence of atomic reasoning operations, enabling a quantitative measurement of
attention that considers the reasoning process. We then collect human
eye-tracking and answer correctness data, and analyze various machine and human
attention mechanisms on their reasoning capability and how they impact task
performance. To improve the attention and reasoning ability of visual question
answering models, we propose to supervise the learning of attention
progressively along the reasoning process and to differentiate the correct and
incorrect attention patterns. We demonstrate the effectiveness of the proposed
framework in analyzing and modeling attention with better reasoning capability
and task performance. The code and data are available at
https://github.com/szzexpoi/AiR",0.12396094,-0.16560009,-0.36123174,A
5108,"NTHU [48] R 1     - 13 13 13 - - - 12800
   The dataset is freely available for download1, thus supporting
open science and stimulating further research in the Ô¨Åeld of                     Nuscenes [35] R 6  1   1 23 23 - - - - 40000
autonomous driving.","IDD [34]   R 1    1 - 34 25 19 - - - 10003
   We validate the accuracy and realism of our dataset starting
from a set of baseline experiments, and show that DL models                      KITTI [28] R 2 4‚Ä† 1    8 8 8 - - - N/A
for semantic understanding trained on our dataset outperform
the same models trained on competing synthetic datasets when                     Mapillary [32] R 1    1 - 66 66 19 * * - 25000
tested on a real (i.e., non-simulated) domain.","RainCouver [49] R 1     - 3 3 - 1 3 - N/A
   The remainder of this paper is organized as follows.",2022-04-20 21:22:56+00:00,"SELMA: SEmantic Large-scale Multimodal Acquisitions in Variable Weather, Daytime and Viewpoints",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Paolo Testolina'), arxiv.Result.Author('Francesco Barbato'), arxiv.Result.Author('Umberto Michieli'), arxiv.Result.Author('Marco Giordani'), arxiv.Result.Author('Pietro Zanuttigh'), arxiv.Result.Author('Michele Zorzi')]","Accurate scene understanding from multiple sensors mounted on cars is a key
requirement for autonomous driving systems. Nowadays, this task is mainly
performed through data-hungry deep learning techniques that need very large
amounts of data to be trained. Due to the high cost of performing segmentation
labeling, many synthetic datasets have been proposed. However, most of them
miss the multi-sensor nature of the data, and do not capture the significant
changes introduced by the variation of daytime and weather conditions. To fill
these gaps, we introduce SELMA, a novel synthetic dataset for semantic
segmentation that contains more than 30K unique waypoints acquired from 24
different sensors including RGB, depth, semantic cameras and LiDARs, in 27
different atmospheric and daytime conditions, for a total of more than 20M
samples. SELMA is based on CARLA, an open-source simulator for generating
synthetic data in autonomous driving scenarios, that we modified to increase
the variability and the diversity in the scenes and class sets, and to align it
with other benchmark datasets. As shown by the experimental evaluation, SELMA
allows the efficient training of standard and multi-modal deep learning
architectures, and achieves remarkable results on real-world data. SELMA is
free and publicly available, thus supporting open science and research.",-0.06907179,-0.09888407,-0.0052986946,B
5110,"A.2, we show and describe some failure cases of BioViL on the MS-CXR dataset to
motivate any further research on this topic.","Additionally, in Fig.","In particular, the models show limitations in grounding the
descriptions relating to smaller structures (e.g., rib fracture, pneumothorax), and in a few cases the location
modifier is not disassociated from the entities corresponding to abnormalities, see (a) in Fig.",2022-04-21 00:04:35+00:00,Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Benedikt Boecking'), arxiv.Result.Author('Naoto Usuyama'), arxiv.Result.Author('Shruthi Bannur'), arxiv.Result.Author('Daniel C. Castro'), arxiv.Result.Author('Anton Schwaighofer'), arxiv.Result.Author('Stephanie Hyland'), arxiv.Result.Author('Maria Wetscherek'), arxiv.Result.Author('Tristan Naumann'), arxiv.Result.Author('Aditya Nori'), arxiv.Result.Author('Javier Alvarez-Valle'), arxiv.Result.Author('Hoifung Poon'), arxiv.Result.Author('Ozan Oktay')]","Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.",0.22657664,0.11051844,-0.1812308,A
5111,"A.2, we show and describe some failure cases of BioViL on the MS-CXR dataset to
motivate any further research on this topic.","Additionally, in Fig.","In particular, the models show limitations in grounding the
descriptions relating to smaller structures (e.g., rib fracture, pneumothorax), and in a few cases the location
modifier is not disassociated from the entities corresponding to abnormalities, see (a) in Fig.",2022-04-21 00:04:35+00:00,Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Benedikt Boecking'), arxiv.Result.Author('Naoto Usuyama'), arxiv.Result.Author('Shruthi Bannur'), arxiv.Result.Author('Daniel C. Castro'), arxiv.Result.Author('Anton Schwaighofer'), arxiv.Result.Author('Stephanie Hyland'), arxiv.Result.Author('Maria Wetscherek'), arxiv.Result.Author('Tristan Naumann'), arxiv.Result.Author('Aditya Nori'), arxiv.Result.Author('Javier Alvarez-Valle'), arxiv.Result.Author('Hoifung Poon'), arxiv.Result.Author('Ozan Oktay')]","Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.",0.22657664,0.11051844,-0.1812308,A
5112,"A.2, we show and describe some failure cases of BioViL
on the MS-CXR dataset to motivate any further research on this topic.","Additionally, in Fig.","In particu-
lar, the models show limitations in grounding the descriptions relating to smaller
structures (e.g., rib fracture, pneumothorax), and in a few cases the location
            Making the Most of Text Semantics to Improve Biomedical VLP                   23

Table A.2: An extension of Table 6 to include Sensitivity and Specificity for the RSNA
Pneumonia zero-shot and fine-tuned classification.",2022-04-21 00:04:35+00:00,Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Benedikt Boecking'), arxiv.Result.Author('Naoto Usuyama'), arxiv.Result.Author('Shruthi Bannur'), arxiv.Result.Author('Daniel C. Castro'), arxiv.Result.Author('Anton Schwaighofer'), arxiv.Result.Author('Stephanie Hyland'), arxiv.Result.Author('Maria Wetscherek'), arxiv.Result.Author('Tristan Naumann'), arxiv.Result.Author('Aditya Nori'), arxiv.Result.Author('Javier Alvarez-Valle'), arxiv.Result.Author('Hoifung Poon'), arxiv.Result.Author('Ozan Oktay')]","Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.",0.16617441,-0.1495174,-0.18414116,A
5113,"A.2, we show and describe some failure cases of BioViL
on the MS-CXR dataset to motivate any further research on this topic.","Additionally, in Fig.","In particu-
lar, the models show limitations in grounding the descriptions relating to smaller
structures (e.g., rib fracture, pneumothorax), and in a few cases the location
            Making the Most of Text Semantics to Improve Biomedical VLP                   23

Table A.2: An extension of Table 6 to include Sensitivity and Specificity for the RSNA
Pneumonia zero-shot and fine-tuned classification.",2022-04-21 00:04:35+00:00,Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Benedikt Boecking'), arxiv.Result.Author('Naoto Usuyama'), arxiv.Result.Author('Shruthi Bannur'), arxiv.Result.Author('Daniel C. Castro'), arxiv.Result.Author('Anton Schwaighofer'), arxiv.Result.Author('Stephanie Hyland'), arxiv.Result.Author('Maria Wetscherek'), arxiv.Result.Author('Tristan Naumann'), arxiv.Result.Author('Aditya Nori'), arxiv.Result.Author('Javier Alvarez-Valle'), arxiv.Result.Author('Hoifung Poon'), arxiv.Result.Author('Ozan Oktay')]","Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.",0.16617441,-0.1495174,-0.18414116,A
5117,"On a high level, how to integrating the               object reconstruction from a single image,‚Äù in Proceedings of the IEEE
similar idea in emerging new representations, such as part based             Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
model with shape basis and learned function [20] are interesting
for further study.","For future             IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
work, combining with efÔ¨Åcient shape retrieval for initialization,            2017.
integrating with multi-view stereo models for explicit photometric
consistency, and extending to scene scales are some of the practical   [9] H. Fan, H. Su, and L. J. Guibas, ‚ÄúA point set generation network for 3d
directions to explore.","[10] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, ‚ÄúFrustum pointnets
                                                                             for 3d object detection from RGB-D data,‚Äù in Proceedings of the IEEE
ACKNOWLEDGMENTS                                                              Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",2022-04-21 03:42:31+00:00,Pixel2Mesh++: 3D Mesh Generation and Refinement from Multi-View Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Wen'), arxiv.Result.Author('Yinda Zhang'), arxiv.Result.Author('Chenjie Cao'), arxiv.Result.Author('Zhuwen Li'), arxiv.Result.Author('Xiangyang Xue'), arxiv.Result.Author('Yanwei Fu')]","We study the problem of shape generation in 3D mesh representation from a
small number of color images with or without camera poses. While many previous
works learn to hallucinate the shape directly from priors, we adopt to further
improve the shape quality by leveraging cross-view information with a graph
convolution network. Instead of building a direct mapping function from images
to 3D shape, our model learns to predict series of deformations to improve a
coarse shape iteratively. Inspired by traditional multiple view geometry
methods, our network samples nearby area around the initial mesh's vertex
locations and reasons an optimal deformation using perceptual feature
statistics built from multiple input images. Extensive experiments show that
our model produces accurate 3D shapes that are not only visually plausible from
the input perspectives, but also well aligned to arbitrary viewpoints. With the
help of physically driven architecture, our model also exhibits generalization
capability across different semantic categories, and the number of input
images. Model analysis experiments show that our model is robust to the quality
of the initial mesh and the error of camera pose, and can be combined with a
differentiable renderer for test-time optimization.",-0.36596394,0.22168647,0.08304297,B
5169,"However, this requires further study in the
2.0        Waymo                                              4                    Waymo                                 future, e.g., with respect to larger network capacity.","As our proposed self-training
                                                                                                                         pipeline facilitates scalable learning from large amounts of
                                                nS-Singapore                                               nS-Singapore  data, the model may continue to improve with additional
                                                                                                                         YouTube data.","Argoverse                                                               Argoverse
                                                                                                                         5.",2022-04-21 17:58:36+00:00,SelfD: Self-Learning Large-Scale Driving Policies From the Web,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Jimuyang Zhang'), arxiv.Result.Author('Ruizhao Zhu'), arxiv.Result.Author('Eshed Ohn-Bar')]","Effectively utilizing the vast amounts of ego-centric navigation data that is
freely available on the internet can advance generalized intelligent systems,
i.e., to robustly scale across perspectives, platforms, environmental
conditions, scenarios, and geographical locations. However, it is difficult to
directly leverage such large amounts of unlabeled and highly diverse data for
complex 3D reasoning and planning tasks. Consequently, researchers have
primarily focused on its use for various auxiliary pixel- and image-level
computer vision tasks that do not consider an ultimate navigational objective.
In this work, we introduce SelfD, a framework for learning scalable driving by
utilizing large amounts of online monocular images. Our key idea is to leverage
iterative semi-supervised training when learning imitative agents from
unlabeled data. To handle unconstrained viewpoints, scenes, and camera
parameters, we train an image-based model that directly learns to plan in the
Bird's Eye View (BEV) space. Next, we use unlabeled data to augment the
decision-making knowledge and robustness of an initially trained model via
self-training. In particular, we propose a pseudo-labeling step which enables
making full use of highly diverse demonstration data through ""hypothetical""
planning-based data augmentation. We employ a large dataset of publicly
available YouTube videos to train SelfD and comprehensively analyze its
generalization benefits across challenging navigation scenarios. Without
requiring any additional data collection or annotation efforts, SelfD
demonstrates consistent improvements (by up to 24%) in driving performance
evaluation on nuScenes, Argoverse, Waymo, and CARLA.",0.04158477,-0.23430766,0.068242654,C
5170,"Nonetheless, given           ditional branches results in a signiÔ¨Åcant increase for the size
the need for larger and more diverse evaluation sets across          of the underlying BEV planner model and thus its scalabil-
all settings in Table 8, this requires further study by future       ity, we focus on extensively analyzing the three-branch ar-
                                                                     chitecture.","As adding con-
without additional pseudo-labeled data.","Nonetheless, while such architecture may learn
Speed: 1.75 Speed: 5.26 Speed: 1.00  Speed: 1.46 Speed: 1.71 Speed: 3.27
Score: 0.51 Score: 0.57 Score: 0.42  Score: 0.54 Score: 0.52 Score: 0.41
Cmd: 1  Cmd: 2  Cmd: 3               Cmd: 1  Cmd: 2                                                                           Cmd: 3

Speed: 0.32 Speed: 0.89 Speed: 2.35  Speed: 1.39 Speed: 2.88 Speed: 0.33
Score: 0.57 Score: 0.52 Score: 0.42  Score: 0.55 Score: 0.52 Score: 0.94
Cmd: 1  Cmd: 2  Cmd: 3               Cmd: 1  Cmd: 2                                                                           Cmd: 3

Speed: 9.27 Speed: 8.93 Speed: 1.45  Speed: 0.33 Speed: 3.98 Speed: 6.00
Score: 0.45 Score: 0.63 Score: 0.42  Score: 0.60 Score: 0.55 Score: 0.34
Cmd: 1  Cmd: 2  Cmd: 3               Cmd: 1  Cmd: 2                                                                           Cmd: 3

Speed: 1.50 Speed: 0.34 Speed: 0.89  Speed: 0.01 Speed: 1.2 Speed: 0.88
Score: 0.53 Score: 0.58 Score: 0.43  Score: 0.60 Score: 0.52 Score: 0.43
Cmd: 1  Cmd: 2  Cmd: 3               Cmd: 1  Cmd: 2                                                                           Cmd: 3

             Figure 6.",2022-04-21 17:58:36+00:00,SelfD: Self-Learning Large-Scale Driving Policies From the Web,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Jimuyang Zhang'), arxiv.Result.Author('Ruizhao Zhu'), arxiv.Result.Author('Eshed Ohn-Bar')]","Effectively utilizing the vast amounts of ego-centric navigation data that is
freely available on the internet can advance generalized intelligent systems,
i.e., to robustly scale across perspectives, platforms, environmental
conditions, scenarios, and geographical locations. However, it is difficult to
directly leverage such large amounts of unlabeled and highly diverse data for
complex 3D reasoning and planning tasks. Consequently, researchers have
primarily focused on its use for various auxiliary pixel- and image-level
computer vision tasks that do not consider an ultimate navigational objective.
In this work, we introduce SelfD, a framework for learning scalable driving by
utilizing large amounts of online monocular images. Our key idea is to leverage
iterative semi-supervised training when learning imitative agents from
unlabeled data. To handle unconstrained viewpoints, scenes, and camera
parameters, we train an image-based model that directly learns to plan in the
Bird's Eye View (BEV) space. Next, we use unlabeled data to augment the
decision-making knowledge and robustness of an initially trained model via
self-training. In particular, we propose a pseudo-labeling step which enables
making full use of highly diverse demonstration data through ""hypothetical""
planning-based data augmentation. We employ a large dataset of publicly
available YouTube videos to train SelfD and comprehensively analyze its
generalization benefits across challenging navigation scenarios. Without
requiring any additional data collection or annotation efforts, SelfD
demonstrates consistent improvements (by up to 24%) in driving performance
evaluation on nuScenes, Argoverse, Waymo, and CARLA.",0.29504347,0.04235002,-0.04979223,A
5188,"We hope that our insights pave
                                                                        the way for further research on the interpretability of deep
                                                                        neural networks.","For the image clas-
                                                                        siÔ¨Åcation task, experts focused on distinct repeating class
                                                                        groups, whereas for object detection, they specialized in
                                                                        objects of distinct sizes.","8
Acknowledgement                                                              Chen.",2022-04-22 09:40:23+00:00,Sparsely-gated MoE Layers for CNN Interpretability,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Svetlana Pavlitskaya'), arxiv.Result.Author('Christian Hubschneider'), arxiv.Result.Author('Lukas Struppek'), arxiv.Result.Author('J. Marius Z√∂llner')]","Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully
applied for scaling large transformers, especially for language modeling tasks.
An intriguing side effect of sparse MoE layers is that they convey inherent
interpretability to a model via natural expert specialization. In this work, we
apply sparse MoE layers to CNNs for computer vision tasks and analyze the
resulting effect on model interpretability. To stabilize MoE training, we
present both soft and hard constraint-based approaches. With hard constraints,
the weights of certain experts are allowed to become zero, while soft
constraints balance the contribution of experts with an additional auxiliary
loss. As a result, soft constraints handle expert utilization better and
support the expert specialization process, while hard constraints maintain more
generalized experts and increase overall model performance. Our findings
demonstrate that experts can implicitly focus on individual sub-domains of the
input space. For example, experts trained for CIFAR-100 image classification
specialize in recognizing different domains such as flowers or animals without
previous data clustering. Experiments with RetinaNet and the COCO dataset
further indicate that object detection experts can also specialize in detecting
objects of distinct sizes.",-0.24283403,-0.19878809,0.043712813,C
5203,"Selecting       network [20]‚Äì[23] for feature extraction in the Ô¨Åeld of cross-
different feature extractors in the ‚ÄúModel List‚Äù will replace the  view matching due to its clever design structure and excel-
backbone network part in the corresponding network structure,      lent performance.With the further research on ResNet and
and the user can also customize its network structure.","The network is divided into a           developed, and ResNet [27] is widely used as the backbone
backbone network and a classiÔ¨Åcation network part.","The         the emergence of attention mechanism, some scholars have
details of the network structure of deep neural networks will      further improved ResNet, such as SE-ResNet [33],ResNeSt
be illustrated in the next section.",2022-04-22 13:49:52+00:00,SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across Drone and Satellite,cs.CV,"['cs.CV', 'eess.IV']",[arxiv.Result.Author('Runzhe Zhu')],"The purpose of cross-view image matching is to match images acquired from the
different platforms of the same target scene and then help positioning system
to infer the location of the target scene. With the rapid development of drone
technology, how to help Drone positioning or navigation through cross-view
matching technology has become a challenging research topic. However, the
accuracy of current cross-view matching models is still low, mainly because the
existing public datasets do not include the differences in images obtained by
drones at different heights, and the types of scenes are relatively
homogeneous, which makes the models unable to adapt to complex and changing
scenes. We propose a new cross-view dataset, SUES-200, to address these
issues.SUES-200 contains images acquired by the drone at four flight heights
and the corresponding satellite view images under the same target scene. To our
knowledge, SUES-200 is the first dataset that considers the differences
generated by aerial photography of drones at different flight heights. In
addition, we build a pipeline for efficient training testing and evaluation of
cross-view matching models. Then, we comprehensively evaluate the performance
of feature extractors with different CNN architectures on SUES-200 through an
evaluation system for cross-view matching models and propose a robust baseline
model. The experimental results show that SUES-200 can help the model learn
features with high discrimination at different heights. Evaluating indicators
of the matching system improves as the drone flight height gets higher because
the drone camera pose and the surrounding environment have less influence on
aerial photography.",-0.15647113,-0.28411978,0.13672417,C
5244,"The overall goal is to establish
      els of missing visions leads to severe performance drop,     a model to generate a scene graph G from the given image
      which brings insights for further research directions.","In        For the SGG task, let Im to be the image input with various
      the meanwhile, we empirically prove that not all lev-        levels of data missingness.",input Im.,2022-04-23 21:46:17+00:00,Supplementing Missing Visions via Dialog for Scene Graph Generations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ye Zhu'), arxiv.Result.Author('Xiaoguang Zhu'), arxiv.Result.Author('Yuzhang Shang'), arxiv.Result.Author('Zhenghao Zhao'), arxiv.Result.Author('Yan Yan')]","Most current AI systems rely on the premise that the input visual data are
sufficient to achieve competitive performance in various computer vision tasks.
However, the classic task setup rarely considers the challenging, yet common
practical situations where the complete visual data may be inaccessible due to
various reasons (e.g., restricted view range and occlusions). To this end, we
investigate a computer vision task setting with incomplete visual input data.
Specifically, we exploit the Scene Graph Generation (SGG) task with various
levels of visual data missingness as input. While insufficient visual input
intuitively leads to performance drop, we propose to supplement the missing
visions via the natural language dialog interactions to better accomplish the
task objective. We design a model-agnostic Supplementary Interactive Dialog
(SI-Dial) framework that can be jointly learned with most existing models,
endowing the current AI systems with the ability of question-answer
interactions in natural language. We demonstrate the feasibility of such a task
setting with missing visual input and the effectiveness of our proposed dialog
module as the supplementary information source through extensive experiments
and analysis, by achieving promising performance improvement over multiple
baselines.",-0.17662477,-0.030338988,0.046933725,C
5273,"simple, yet powerful, and easily adaptable for further research
                                        and applications.",The network architecture is relatively          orientation estimation on cropped full body input data.,"2) An approach to 3D human pose estimation in which
                                                                                                                   3D human joint positions are encoded in the skeletal
                                                                 I.",2022-04-25 10:47:01+00:00,PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dennis Burgermeister'), arxiv.Result.Author('Crist√≥bal Curio')]","We present a multitask network that supports various deep neural network
based pedestrian detection functions. Besides 2D and 3D human pose, it also
supports body and head orientation estimation based on full body bounding box
input. This eliminates the need for explicit face recognition. We show that the
performance of 3D human pose estimation and orientation estimation is
comparable to the state-of-the-art. Since very few data sets exist for 3D human
pose and in particular body and head orientation estimation based on full body
data, we further show the benefit of particular simulation data to train the
network. The network architecture is relatively simple, yet powerful, and
easily adaptable for further research and applications.",-0.24059522,0.19530475,-0.065761834,B
5274,"Thus, the introduced model is       [6] D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, M. Elgharib, P. Fua,
also well suited as a baseline for further research.","all these tasks in a simple and extensible architecture which
is straight forward to train.","We have           H.-P. Seidel, H. Rhodin, G. Pons-Moll, and C. Theobalt, ‚ÄúXNect: Real-
further shown that we can train the orientation estimation             time multi-person 3D motion capture with a single RGB camera,‚Äù
purely with simulation data and achieve high accuracy on               ACM Transactions on Graphics, vol.",2022-04-25 10:47:01+00:00,PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dennis Burgermeister'), arxiv.Result.Author('Crist√≥bal Curio')]","We present a multitask network that supports various deep neural network
based pedestrian detection functions. Besides 2D and 3D human pose, it also
supports body and head orientation estimation based on full body bounding box
input. This eliminates the need for explicit face recognition. We show that the
performance of 3D human pose estimation and orientation estimation is
comparable to the state-of-the-art. Since very few data sets exist for 3D human
pose and in particular body and head orientation estimation based on full body
data, we further show the benefit of particular simulation data to train the
network. The network architecture is relatively simple, yet powerful, and
easily adaptable for further research and applications.",-0.22509131,0.2798018,-0.05788704,B
5293,"Both
have signiÔ¨Åcant inÔ¨Çuence on the performance of the RDM/RARM and further research is needed to
determine the best choices here.","Furthermore, our approach depends on the image
representation chosen to encode images from the retrieval database D and the retrieval model.",Our work demonstrates the beneÔ¨Åts of adding an external database in general.,2022-04-25 17:55:26+00:00,Semi-Parametric Neural Image Synthesis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Andreas Blattmann'), arxiv.Result.Author('Robin Rombach'), arxiv.Result.Author('Kaan Oktay'), arxiv.Result.Author('Jonas M√ºller'), arxiv.Result.Author('Bj√∂rn Ommer')]","Novel architectures have recently improved generative image synthesis leading
to excellent visual quality in various tasks. Much of this success is due to
the scalability of these architectures and hence caused by a dramatic increase
in model complexity and in the computational resources invested in training
these models. Our work questions the underlying paradigm of compressing large
training data into ever growing parametric representations. We rather present
an orthogonal, semi-parametric approach. We complement comparably small
diffusion or autoregressive models with a separate image database and a
retrieval strategy. During training we retrieve a set of nearest neighbors from
this external database for each training instance and condition the generative
model on these informative samples. While the retrieval approach is providing
the (local) content, the model is focusing on learning the composition of
scenes based on this content. As demonstrated by our experiments, simply
swapping the database for one with different contents transfers a trained model
post-hoc to a novel domain. The evaluation shows competitive performance on
tasks which the generative model has not been trained on, such as
class-conditional synthesis, zero-shot stylization or text-to-image synthesis
without requiring paired text-image data. With negligible memory and
computational overhead for the external database and retrieval we can
significantly reduce the parameter count of the generative model and still
outperform the state-of-the-art.",-0.021576863,-0.061303023,0.010150335,C
5294,"A further study is required to apply the proposed
                                                                           evaluation metrics in such a setup.","Such a setup will require to evaluate inter-
                                                                           pretability between ensemble of teachers and ensemble of
                                                                           students.",Acknowledgements.,2022-04-25 17:59:30+00:00,"Proto2Proto: Can you recognize the car, the way I do?",cs.CV,['cs.CV'],"[arxiv.Result.Author('Monish Keswani'), arxiv.Result.Author('Sriranjani Ramakrishnan'), arxiv.Result.Author('Nishant Reddy'), arxiv.Result.Author('Vineeth N Balasubramanian')]","Prototypical methods have recently gained a lot of attention due to their
intrinsic interpretable nature, which is obtained through the prototypes. With
growing use cases of model reuse and distillation, there is a need to also
study transfer of interpretability from one model to another. We present
Proto2Proto, a novel method to transfer interpretability of one prototypical
part network to another via knowledge distillation. Our approach aims to add
interpretability to the ""dark"" knowledge transferred from the teacher to the
shallower student model. We propose two novel losses: ""Global Explanation"" loss
and ""Patch-Prototype Correspondence"" loss to facilitate such a transfer. Global
Explanation loss forces the student prototypes to be close to teacher
prototypes, and Patch-Prototype Correspondence loss enforces the local
representations of the student to be similar to that of the teacher. Further,
we propose three novel metrics to evaluate the student's proximity to the
teacher as measures of interpretability transfer in our settings. We
qualitatively and quantitatively demonstrate the effectiveness of our method on
CUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed
method indeed achieves interpretability transfer from teacher to student while
simultaneously exhibiting competitive performance.",0.3388703,-0.032253593,-0.2647147,A
5295,"A further study is required to apply the proposed
                                                                           evaluation metrics in such a setup.","Such a setup will require to evaluate inter-
                                                                           pretability between ensemble of teachers and ensemble of
                                                                           students.",Acknowledgements.,2022-04-25 17:59:30+00:00,"Proto2Proto: Can you recognize the car, the way I do?",cs.CV,['cs.CV'],"[arxiv.Result.Author('Monish Keswani'), arxiv.Result.Author('Sriranjani Ramakrishnan'), arxiv.Result.Author('Nishant Reddy'), arxiv.Result.Author('Vineeth N Balasubramanian')]","Prototypical methods have recently gained a lot of attention due to their
intrinsic interpretable nature, which is obtained through the prototypes. With
growing use cases of model reuse and distillation, there is a need to also
study transfer of interpretability from one model to another. We present
Proto2Proto, a novel method to transfer interpretability of one prototypical
part network to another via knowledge distillation. Our approach aims to add
interpretability to the ""dark"" knowledge transferred from the teacher to the
shallower student model. We propose two novel losses: ""Global Explanation"" loss
and ""Patch-Prototype Correspondence"" loss to facilitate such a transfer. Global
Explanation loss forces the student prototypes to be close to teacher
prototypes, and Patch-Prototype Correspondence loss enforces the local
representations of the student to be similar to that of the teacher. Further,
we propose three novel metrics to evaluate the student's proximity to the
teacher as measures of interpretability transfer in our settings. We
qualitatively and quantitatively demonstrate the effectiveness of our method on
CUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed
method indeed achieves interpretability transfer from teacher to student while
simultaneously exhibiting competitive performance.",0.3388703,-0.032253593,-0.2647147,A
5304,"With this, we further study some
                                                important questions for action recognition that lead to interesting Ô¨Ånd-
                                                ings.","We then conduct comprehen-
                                                sive experiments and in-depth analysis to provide a better understanding
                                                of how temporal modeling is aÔ¨Äected by various factors such as dataset,
                                                network architecture, and input frames.","Our analysis shows that there is no strong correlation between
                                                temporal relevance and model performance; and action models tend to
                                                capture local temporal information, but less long-range dependencies.",2022-04-25 19:06:48+00:00,Temporal Relevance Analysis for Video Action Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Quanfu Fan'), arxiv.Result.Author('Donghyun Kim'), arxiv.Result.Author('Chun-Fu'), arxiv.Result.Author('Chen'), arxiv.Result.Author('Stan Sclaroff'), arxiv.Result.Author('Kate Saenko'), arxiv.Result.Author('Sarah Adel Bargal')]","In this paper, we provide a deep analysis of temporal modeling for action
recognition, an important but underexplored problem in the literature. We first
propose a new approach to quantify the temporal relationships between frames
captured by CNN-based action models based on layer-wise relevance propagation.
We then conduct comprehensive experiments and in-depth analysis to provide a
better understanding of how temporal modeling is affected by various factors
such as dataset, network architecture, and input frames. With this, we further
study some important questions for action recognition that lead to interesting
findings. Our analysis shows that there is no strong correlation between
temporal relevance and model performance; and action models tend to capture
local temporal information, but less long-range dependencies. Our codes and
models will be publicly available.",-0.122104675,-0.20374127,-0.20240577,C
5310,"Non-causal visual
general feature learning, it gives a good theoretical  understanding methods are easily aÔ¨Äected by con-
basis for the further research of causal reason-       founders in visual content.","Since section 3 has       linguistic features into a robust and discrim-
reviewed the recent causal reasoning methods for       inative representation space.","Illumination, position,
ing with speciÔ¨Åc visual representation learning        backgrounds, co-occurrence of objects, and other
tasks.",2022-04-26 02:22:28+00:00,Causal Reasoning Meets Visual Representation Learning: A Prospective Study,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Yushen Wei'), arxiv.Result.Author('Hong Yan'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Liang Lin')]","Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks an unified guidance and analysis about
why modern visual representation learning methods are easily collapse into data
bias and have limited generalization and cognitive abilities. Inspired by the
strong inference ability of human-level agents, recent years have therefore
witnessed great effort in developing causal reasoning paradigms to realize
robust representation and model learning with good cognitive ability. In this
paper, we conduct a comprehensive review of existing causal reasoning methods
for visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.",-0.1324974,-0.19571412,-0.27232903,C
5311,"Since section 3 has       models can be mitigated by introducing causal-
reviewed the recent causal reasoning methods for       ity that integrates external knowledge, visual and
general feature learning, it gives a good theoretical  linguistic features into a robust and discrim-
basis for the further research of causal reason-       inative representation space.","Generally, the super-
why, and to make decisions through intervention        Ô¨Åcial correlations captured by the existing VCR
or counterfactual reasoning.","Non-causal visual
ing with speciÔ¨Åc visual representation learning        understanding methods are easily aÔ¨Äected by con-
tasks.",2022-04-26 02:22:28+00:00,Causal Reasoning Meets Visual Representation Learning: A Prospective Study,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Yushen Wei'), arxiv.Result.Author('Hong Yan'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Liang Lin')]","Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks an unified guidance and analysis about
why modern visual representation learning methods are easily collapse into data
bias and have limited generalization and cognitive abilities. Inspired by the
strong inference ability of human-level agents, recent years have therefore
witnessed great effort in developing causal reasoning paradigms to realize
robust representation and model learning with good cognitive ability. In this
paper, we conduct a comprehensive review of existing causal reasoning methods
for visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.",-0.093395114,-0.23524645,-0.28119716,C
5312,"Since Section 3      models can be mitigated by introducing causal-
has reviewed the recent causal reasoning meth-         ity that integrates external knowledge and visual
ods for general feature learning, it provides a good   and linguistic features into a robust and discrim-
theoretical basis for further research on causal       inative representation space.","Generally, the super-
why and to make decisions through interven-            Ô¨Åcial correlations captured by the existing VCR
tion or counterfactual reasoning.","Non-causal visual
                                                       understanding methods are easily aÔ¨Äected by con-
                                                       founders in visual content.",2022-04-26 02:22:28+00:00,Causal Reasoning Meets Visual Representation Learning: A Prospective Study,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Yushen Wei'), arxiv.Result.Author('Hong Yan'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Liang Lin')]","Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks unified guidance and analysis about why
modern visual representation learning methods easily collapse into data bias
and have limited generalization and cognitive abilities. Inspired by the strong
inference ability of human-level agents, recent years have therefore witnessed
great effort in developing causal reasoning paradigms to realize robust
representation and model learning with good cognitive ability. In this paper,
we conduct a comprehensive review of existing causal reasoning methods for
visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.",-0.10715679,-0.23344712,-0.2273183,C
5321,"We will conduct further research
                                                                   for unsupervised deblurring in the future.","Meanwhile, it ensures the quantitative score on
there always exists relative sharp patches within a blurry         the GoPro paired dataset.","Sharp                                                                                                                                                                                           12

ùëù(ùëÜ) Blurry                           ùëù(ùëÜ)  Relative                                                                [4] A. Levin, Y. Weiss, F. Durand, and W. T. Freeman, ‚ÄúEfÔ¨Åcient
                                            motion                                                                        marginal likelihood optimization in blind deconvolution,‚Äù in Proc.",2022-04-26 08:09:47+00:00,Neural Maximum A Posteriori Estimation on Unpaired Data for Motion Deblurring,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Youjian Zhang'), arxiv.Result.Author('Chaoyue Wang'), arxiv.Result.Author('Dacheng Tao')]","Real-world dynamic scene deblurring has long been a challenging task since
paired blurry-sharp training data is unavailable. Conventional Maximum A
Posteriori estimation and deep learning-based deblurring methods are restricted
by handcrafted priors and synthetic blurry-sharp training pairs respectively,
thereby failing to generalize to real dynamic blurriness. To this end, we
propose a Neural Maximum A Posteriori (NeurMAP) estimation framework for
training neural networks to recover blind motion information and sharp content
from unpaired data. The proposed NeruMAP consists of a motion estimation
network and a deblurring network which are trained jointly to model the
(re)blurring process (i.e. likelihood function). Meanwhile, the motion
estimation network is trained to explore the motion information in images by
applying implicit dynamic motion prior, and in return enforces the deblurring
network training (i.e. providing sharp image prior). The proposed NeurMAP is an
orthogonal approach to existing deblurring neural networks, and is the first
framework that enables training image deblurring networks on unpaired datasets.
Experiments demonstrate our superiority on both quantitative metrics and visual
quality over state-of-the-art methods. Codes are available on
https://github.com/yjzhang96/NeurMAP-deblur.",-0.07715269,0.18498689,0.14187908,B
5327,"With
                                        Convolutional Neural Networks (CNNs) have become the de        further research, researchers found that convolution and at-
                                        facto standard in the Ô¨Åeld of computer vision.","1 Introduction                                                 of these models has caught up with CNNs and is challenging
                                                                                                       the position of CNNs in the Ô¨Åeld of computer vision.","Deep Neural     tention mechanisms are not unique to good performance, and
                                        Networks (DNNs) based on CNNs continue to improve clas-        only using MultiLayer Perceptrons (MLPs) can also achieve
                                        siÔ¨Åcation performance in computer vision, such as Densenet     good performance, so MLP-Mixer [Tolstikhin et al., 2021] is
                                        [Huang et al., 2017], MobileNet [Sandler et al., 2018], Ef-    proposed.",2022-04-26 10:18:59+00:00,Boosting Adversarial Transferability of MLP-Mixer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haoran Lyu'), arxiv.Result.Author('Yajie Wang'), arxiv.Result.Author('Yu-an Tan'), arxiv.Result.Author('Huipeng Zhou'), arxiv.Result.Author('Yuhang Zhao'), arxiv.Result.Author('Quanxin Zhang')]","The security of models based on new architectures such as MLP-Mixer and ViTs
needs to be studied urgently. However, most of the current researches are
mainly aimed at the adversarial attack against ViTs, and there is still
relatively little adversarial work on MLP-mixer. We propose an adversarial
attack method against MLP-Mixer called Maxwell's demon Attack (MA). MA breaks
the channel-mixing and token-mixing mechanism of MLP-Mixer by controlling the
part input of MLP-Mixer's each Mixer layer, and disturbs MLP-Mixer to obtain
the main information of images. Our method can mask the part input of the Mixer
layer, avoid overfitting of the adversarial examples to the source model, and
improve the transferability of cross-architecture. Extensive experimental
evaluation demonstrates the effectiveness and superior performance of the
proposed MA. Our method can be easily combined with existing methods and can
improve the transferability by up to 38.0% on MLP-based ResMLP. Adversarial
examples produced by our method on MLP-Mixer are able to exceed the
transferability of adversarial examples produced using DenseNet against CNNs.
To the best of our knowledge, we are the first work to study adversarial
transferability of MLP-Mixer.",-0.22971365,-0.10488826,0.27593672,C
5335,"interpolation could improve performance through data augmentation, which we plan to
study in further research.","The ability to abstract from discrete classes
to continuous features opens up a variety of machine learning problems where label
20  Mertes et al.","ACKNOWLEDGEMENTS

This work has been funded by the European Union Horizon 2020 research and innova-
tion programme, grant agreement 856879.",2022-04-26 11:36:32+00:00,Intercategorical Label Interpolation for Emotional Face Generation with Conditional Generative Adversarial Networks,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Silvan Mertes'), arxiv.Result.Author('Dominik Schiller'), arxiv.Result.Author('Florian Lingenfelser'), arxiv.Result.Author('Thomas Kiderle'), arxiv.Result.Author('Valentin Kroner'), arxiv.Result.Author('Lama Diab'), arxiv.Result.Author('Elisabeth Andr√©')]","Generative adversarial networks offer the possibility to generate deceptively
real images that are almost indistinguishable from actual photographs. Such
systems however rely on the presence of large datasets to realistically
replicate the corresponding domain. This is especially a problem if not only
random new images are to be generated, but specific (continuous) features are
to be co-modeled. A particularly important use case in \emph{Human-Computer
Interaction} (HCI) research is the generation of emotional images of human
faces, which can be used for various use cases, such as the automatic
generation of avatars. The problem hereby lies in the availability of training
data. Most suitable datasets for this task rely on categorical emotion models
and therefore feature only discrete annotation labels. This greatly hinders the
learning and modeling of smooth transitions between displayed affective states.
To overcome this challenge, we explore the potential of label interpolation to
enhance networks trained on categorical datasets with the ability to generate
images conditioned on continuous features.",-0.0025285184,-0.14901708,0.0731615,C
5344,"Moreover, we provide
                                        the visible light cameras exhibit inferior efficacy without sufficient   a new and challenging dataset encouraging further research for
                                        illumination.","Extensive ex-
                                                                                                                 periments and evaluations for specific applications indicate ROMA
                                        Infrared cameras are often utilized to enhance the night vision since    outperforms the state-of-the-art methods.","However, infrared data possesses inadequate color          unpaired nighttime infrared and daytime visible video translation,
                                        contrast and representation ability attributed to its intrinsic heat-    named InfraredCity.",2022-04-26 15:08:15+00:00,ROMA: Cross-Domain Region Similarity Matching for Unpaired Nighttime Infrared to Daytime Visible Video Translation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenjie Yu'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Bingfeng Han'), arxiv.Result.Author('Chi Harold Liu'), arxiv.Result.Author('Shuigen Wang')]","Infrared cameras are often utilized to enhance the night vision since the
visible light cameras exhibit inferior efficacy without sufficient
illumination. However, infrared data possesses inadequate color contrast and
representation ability attributed to its intrinsic heat-related imaging
principle. This makes it arduous to capture and analyze information for human
beings, meanwhile hindering its application. Although, the domain gaps between
unpaired nighttime infrared and daytime visible videos are even huger than
paired ones that captured at the same time, establishing an effective
translation mapping will greatly contribute to various fields. In this case,
the structural knowledge within nighttime infrared videos and semantic
information contained in the translated daytime visible pairs could be utilized
simultaneously. To this end, we propose a tailored framework ROMA that couples
with our introduced cRoss-domain regiOn siMilarity mAtching technique for
bridging the huge gaps. To be specific, ROMA could efficiently translate the
unpaired nighttime infrared videos into fine-grained daytime visible ones,
meanwhile maintain the spatiotemporal consistency via matching the cross-domain
region similarity. Furthermore, we design a multiscale region-wise
discriminator to distinguish the details from synthesized visible results and
real references. Extensive experiments and evaluations for specific
applications indicate ROMA outperforms the state-of-the-art methods. Moreover,
we provide a new and challenging dataset encouraging further research for
unpaired nighttime infrared and daytime visible video translation, named
InfraredCity. In particular, it consists of 9 long video clips including City,
Highway and Monitor scenarios. All clips could be split into 603,142 frames in
total, which are 20 times larger than the recently released daytime
infrared-to-visible dataset IRVI.",-0.10817464,0.19695258,0.026551832,B
5345,"‚Ä¢ We provide new datasets for nighttime infrared to daytime vis-               ùë•                    ùë¶                     ùïã         ‚Ä¶‚Ä¶‚Ä¶‚Ä¶ ùïã  ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
   ible video translation, i.e., InfraredCity and InfraredCity-Lite,
   encouraging further research on this area.","The promising results validate the effectiveness of ROMA for                                                                     Feature Extractor
   night vision scenarios.",Eq.,2022-04-26 15:08:15+00:00,ROMA: Cross-Domain Region Similarity Matching for Unpaired Nighttime Infrared to Daytime Visible Video Translation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenjie Yu'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Bingfeng Han'), arxiv.Result.Author('Chi Harold Liu'), arxiv.Result.Author('Shuigen Wang')]","Infrared cameras are often utilized to enhance the night vision since the
visible light cameras exhibit inferior efficacy without sufficient
illumination. However, infrared data possesses inadequate color contrast and
representation ability attributed to its intrinsic heat-related imaging
principle. This makes it arduous to capture and analyze information for human
beings, meanwhile hindering its application. Although, the domain gaps between
unpaired nighttime infrared and daytime visible videos are even huger than
paired ones that captured at the same time, establishing an effective
translation mapping will greatly contribute to various fields. In this case,
the structural knowledge within nighttime infrared videos and semantic
information contained in the translated daytime visible pairs could be utilized
simultaneously. To this end, we propose a tailored framework ROMA that couples
with our introduced cRoss-domain regiOn siMilarity mAtching technique for
bridging the huge gaps. To be specific, ROMA could efficiently translate the
unpaired nighttime infrared videos into fine-grained daytime visible ones,
meanwhile maintain the spatiotemporal consistency via matching the cross-domain
region similarity. Furthermore, we design a multiscale region-wise
discriminator to distinguish the details from synthesized visible results and
real references. Extensive experiments and evaluations for specific
applications indicate ROMA outperforms the state-of-the-art methods. Moreover,
we provide a new and challenging dataset encouraging further research for
unpaired nighttime infrared and daytime visible video translation, named
InfraredCity. In particular, it consists of 9 long video clips including City,
Highway and Monitor scenarios. All clips could be split into 603,142 frames in
total, which are 20 times larger than the recently released daytime
infrared-to-visible dataset IRVI.",-0.2007075,0.1624061,0.00028793514,B
5374,"These Good latent codes can
be utilized for further research including data augmentation         B.",codes by adopting a linear SVM.,"Latent Space Manipulation
and latent-space direction discovery [8], [9].",2022-04-27 03:12:55+00:00,Optimized latent-code selection for explainable conditional text-to-image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenxing Zhang'), arxiv.Result.Author('Lambert Schomaker')]","The task of text-to-image generation has achieved remarkable progress due to
the advances in the conditional generative adversarial networks (GANs).
However, existing conditional text-to-image GANs approaches mostly concentrate
on improving both image quality and semantic relevance but ignore the
explainability of the model which plays a vital role in real-world
applications. In this paper, we present a variety of techniques to take a deep
look into the latent space and semantic space of the conditional text-to-image
GANs model. We introduce pairwise linear interpolation of latent codes and
`linguistic' linear interpolation to study what the model has learned within
the latent space and `linguistic' embeddings. Subsequently, we extend linear
interpolation to triangular interpolation conditioned on three corners to
further analyze the model. After that, we build a Good/Bad data set containing
unsuccessfully and successfully synthetic samples and corresponding latent
codes for the image-quality research. Based on this data set, we propose a
framework for finding good latent codes by utilizing a linear SVM. Experimental
results on the recent DiverGAN generator trained on two benchmark data sets
qualitatively prove the effectiveness of our presented techniques, with a
better than 94\% accuracy in predicting ${Good}$/${Bad}$ classes for latent
vectors. The Good/Bad data set is publicly available at
https://zenodo.org/record/5850224#.YeGMwP7MKUk.",0.031920727,-0.12167004,-0.07772825,C
5375,"Afterwards, triangular interpolation conditioned
image generation methods have achieved tremendous progress           on three corners is introduced to further study the model.","THE PROPOSED APPROACH

A. Text-to-Image Generation                                             In this section, we discuss pairwise linear interpolation
                                                                     of latent codes and ‚Äòlinguistic‚Äô linear interpolation for an
   Thanks to recent unprecedented advances in generative             improved explainability of the conditional text-to-image GANs
approaches especially GANs and cGANs, existing text-to-              frameworks.","Sub-
in both image quality and semantic consistency.",2022-04-27 03:12:55+00:00,Optimized latent-code selection for explainable conditional text-to-image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenxing Zhang'), arxiv.Result.Author('Lambert Schomaker')]","The task of text-to-image generation has achieved remarkable progress due to
the advances in the conditional generative adversarial networks (GANs).
However, existing conditional text-to-image GANs approaches mostly concentrate
on improving both image quality and semantic relevance but ignore the
explainability of the model which plays a vital role in real-world
applications. In this paper, we present a variety of techniques to take a deep
look into the latent space and semantic space of the conditional text-to-image
GANs model. We introduce pairwise linear interpolation of latent codes and
`linguistic' linear interpolation to study what the model has learned within
the latent space and `linguistic' embeddings. Subsequently, we extend linear
interpolation to triangular interpolation conditioned on three corners to
further analyze the model. After that, we build a Good/Bad data set containing
unsuccessfully and successfully synthetic samples and corresponding latent
codes for the image-quality research. Based on this data set, we propose a
framework for finding good latent codes by utilizing a linear SVM. Experimental
results on the recent DiverGAN generator trained on two benchmark data sets
qualitatively prove the effectiveness of our presented techniques, with a
better than 94\% accuracy in predicting ${Good}$/${Bad}$ classes for latent
vectors. The Good/Bad data set is publicly available at
https://zenodo.org/record/5850224#.YeGMwP7MKUk.",-0.10377727,-0.07849636,0.14236456,C
5376,"importantly, when given a single natural-language description,
the quality of generated images depends on injected random         C. Triangular Interpolation
noise but should change smoothly along with the latent vec-
tors, which we detail in Section III-D.                               We extend linear interpolation between two points to an
                                                                   interpolation between three points, i.e., in the 2-simplex,
                                                                   for further studying G(z, (w, s)) and better performing data
                                                                   augmentation.",More     ground) regions are determined by the linguistic terms.,"Since this kind of interpolation forms a tri-
                                                                   angular plane, we name it triangular interpolation.",2022-04-27 03:12:55+00:00,Optimized latent-code selection for explainable conditional text-to-image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenxing Zhang'), arxiv.Result.Author('Lambert Schomaker')]","The task of text-to-image generation has achieved remarkable progress due to
the advances in the conditional generative adversarial networks (GANs).
However, existing conditional text-to-image GANs approaches mostly concentrate
on improving both image quality and semantic relevance but ignore the
explainability of the model which plays a vital role in real-world
applications. In this paper, we present a variety of techniques to take a deep
look into the latent space and semantic space of the conditional text-to-image
GANs model. We introduce pairwise linear interpolation of latent codes and
`linguistic' linear interpolation to study what the model has learned within
the latent space and `linguistic' embeddings. Subsequently, we extend linear
interpolation to triangular interpolation conditioned on three corners to
further analyze the model. After that, we build a Good/Bad data set containing
unsuccessfully and successfully synthetic samples and corresponding latent
codes for the image-quality research. Based on this data set, we propose a
framework for finding good latent codes by utilizing a linear SVM. Experimental
results on the recent DiverGAN generator trained on two benchmark data sets
qualitatively prove the effectiveness of our presented techniques, with a
better than 94\% accuracy in predicting ${Good}$/${Bad}$ classes for latent
vectors. The Good/Bad data set is publicly available at
https://zenodo.org/record/5850224#.YeGMwP7MKUk.",0.09426206,0.15981233,-0.019903267,A
5377,"In the long run, our work will
with CNNs during iterative attacks, thus improving the suc-         support further research to build more robust deep learning
                                                                    models to defend against our proposed attacks, thus eliminat-
                                                                    ing short-term risks.","We demonstrate that the possibility
tantly, we can help ViTs keep the region of interest consistent     of this threat arises in ViTs.","References                                                      [Naseer et al., 2021] Muzammal Naseer, Kanchana Ranas-
                                                                   inghe, Salman Hameed Khan, Fahad Shahbaz Khan, and
[Ding et al., 2021] Xiaohan Ding, X. Zhang, Ningning Ma,           Fatih Murat Porikli.",2022-04-27 03:22:55+00:00,Improving the Transferability of Adversarial Examples with Restructure Embedded Patches,cs.CV,['cs.CV'],"[arxiv.Result.Author('Huipeng Zhou'), arxiv.Result.Author('Yu-an Tan'), arxiv.Result.Author('Yajie Wang'), arxiv.Result.Author('Haoran Lyu'), arxiv.Result.Author('Shangbo Wu'), arxiv.Result.Author('Yuanzhang Li')]","Vision transformers (ViTs) have demonstrated impressive performance in
various computer vision tasks. However, the adversarial examples generated by
ViTs are challenging to transfer to other networks with different structures.
Recent attack methods do not consider the specificity of ViTs architecture and
self-attention mechanism, which leads to poor transferability of the generated
adversarial samples by ViTs. We attack the unique self-attention mechanism in
ViTs by restructuring the embedded patches of the input. The restructured
embedded patches enable the self-attention mechanism to obtain more diverse
patches connections and help ViTs keep regions of interest on the object.
Therefore, we propose an attack method against the unique self-attention
mechanism in ViTs, called Self-Attention Patches Restructure (SAPR). Our method
is simple to implement yet efficient and applicable to any self-attention based
network and gradient transferability-based attack methods. We evaluate attack
transferability on black-box models with different structures. The result show
that our method generates adversarial examples on white-box ViTs with higher
transferability and higher image quality. Our research advances the development
of black-box transfer attacks on ViTs and demonstrates the feasibility of using
white-box ViTs to attack other black-box models.",-0.0076258695,-0.33336574,0.20433408,C
5379,"To leverage the powerful image
inversions techniques along with disentanglement properties in latent space, this work focuses on the
W+ latent space, where we further study the existence of locally low-rank micromotion subspace.","[2021] are mainly conducted in W+ space,
an augmented latent space from W with more degree of freedom.","3
Strength of Micromotion  StyleGAN
                         Latent V

                         ‚Ä¶                     ‚Ä¶

                                   Identities     Latent space

                                                  Low-dimension
                                                  Micromotion space

Figure 2: A tensor illustration of our low-rank micromotion subspace hypothesis.",2022-04-27 04:38:39+00:00,Grasping the Arrow of Time from the Singularity: Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qiucheng Wu'), arxiv.Result.Author('Yifan Jiang'), arxiv.Result.Author('Junru Wu'), arxiv.Result.Author('Kai Wang'), arxiv.Result.Author('Gong Zhang'), arxiv.Result.Author('Humphrey Shi'), arxiv.Result.Author('Zhangyang Wang'), arxiv.Result.Author('Shiyu Chang')]","The disentanglement of StyleGAN latent space has paved the way for realistic
and controllable image editing, but does StyleGAN know anything about temporal
motion, as it was only trained on static images? To study the motion features
in the latent space of StyleGAN, in this paper, we hypothesize and demonstrate
that a series of meaningful, natural, and versatile small, local movements
(referred to as ""micromotion"", such as expression, head movement, and aging
effect) can be represented in low-rank spaces extracted from the latent space
of a conventionally pre-trained StyleGAN-v2 model for face generation, with the
guidance of proper ""anchors"" in the form of either short text or video clips.
Starting from one target face image, with the editing direction decoded from
the low-rank space, its micromotion features can be represented as simple as an
affine transformation over its latent feature. Perhaps more surprisingly, such
micromotion subspace, even learned from just single target face, can be
painlessly transferred to other unseen face images, even those from vastly
different domains (such as oil painting, cartoon, and sculpture faces). It
demonstrates that the local feature geometry corresponding to one type of
micromotion is aligned across different face subjects, and hence that
StyleGAN-v2 is indeed ""secretly"" aware of the subject-disentangled feature
variations caused by that micromotion. We present various successful examples
of applying our low-dimensional micromotion subspace technique to directly and
effortlessly manipulate faces, showing high robustness, low computational
overhead, and impressive domain transferability. Our codes are available at
https://github.com/wuqiuche/micromotion-StyleGAN.",0.008752454,0.0040791705,0.13267106,C
5386,‚Äö We release our solver as open source to facilitate further research.,"‚Äö We incorporate our solver into a recently proposed distributed BA framework
     and show a signiÔ¨Åcant improvement in terms of speed and accuracy.","2 Related Work

As we propose a new way to solve large-scale bundle adjustment problems we
review works on bundle adjustment and on traditional solving methods, that
is, direct and iterative methods.",2022-04-27 10:38:33+00:00,Power Bundle Adjustment for Large-Scale 3D Reconstruction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Simon Weber'), arxiv.Result.Author('Nikolaus Demmel'), arxiv.Result.Author('Daniel Cremers')]","We present the design and the implementation of a new expansion type
algorithm to solve large-scale bundle adjustment problems. Our approach --
called Power Bundle Adjustment -- is based on the power series expansion of the
inverse Schur complement. This initiates a new family of solvers that we call
inverse expansion methods. We show with the real-world BAL dataset that the
proposed solver challenges the traditional direct and iterative methods. The
solution of the normal equation is significantly accelerated, even for reaching
a very high accuracy. Last but not least, our solver can also complement a
recently presented distributed bundle adjustment framework. We demonstrate that
employing the proposed Power Bundle Adjustment as a sub-problem solver greatly
improves speed and accuracy of the distributed optimization.",0.062544666,0.30106372,-0.027803376,B
5387,‚Äö We release our solver as open source to facilitate further research.,"‚Äö We incorporate our solver into a recently proposed distributed BA framework
     and show a signiÔ¨Åcant improvement in terms of speed and accuracy.","2 Related Work

As we propose a new way to solve large-scale bundle adjustment problems we
review works on bundle adjustment and on traditional solving methods, that
is, direct and iterative methods.",2022-04-27 10:38:33+00:00,Power Bundle Adjustment for Large-Scale 3D Reconstruction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Simon Weber'), arxiv.Result.Author('Nikolaus Demmel'), arxiv.Result.Author('Tin Chon Chan'), arxiv.Result.Author('Daniel Cremers')]","We present the design and the implementation of a new expansion type
algorithm to solve large-scale bundle adjustment problems. Our approach --
called Power Bundle Adjustment -- is based on the power series expansion of the
inverse Schur complement. This initiates a new family of solvers that we call
inverse expansion methods. We show with the real-world BAL dataset that the
proposed solver challenges the traditional direct and iterative methods. The
solution of the normal equation is significantly accelerated, even for reaching
a very high accuracy. Last but not least, our solver can also complement a
recently presented distributed bundle adjustment framework. We demonstrate that
employing the proposed Power Bundle Adjustment as a sub-problem solver greatly
improves speed and accuracy of the distributed optimization.",0.062544666,0.30106372,-0.027803376,B
5423,"Size ad mIoU

    1‚Äì        ‚Äì      1 (HR) 0.5  65.1 ¬±1.9
                     1 (HR) 0.5  65.9 ¬±1.2
    24        0.5    1 (HR) 0.5  68.5 ¬±0.6
                     1 (HR) 0.5  67.5 ¬±0.7
    3 2 (LR)  0.5

    4 1.33    0.5

D Influence of Context Scale

We further study the influence of the downscale factor of the context crop sc
in Tab.",Size ac Detail sd Detail Rel.,S1.,2022-04-27 18:00:26+00:00,HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lukas Hoyer'), arxiv.Result.Author('Dengxin Dai'), arxiv.Result.Author('Luc Van Gool')]","Unsupervised domain adaptation (UDA) aims to adapt a model trained on the
source domain (e.g. synthetic data) to the target domain (e.g. real-world data)
without requiring further annotations on the target domain. This work focuses
on UDA for semantic segmentation as real-world pixel-wise annotations are
particularly expensive to acquire. As UDA methods for semantic segmentation are
usually GPU memory intensive, most previous methods operate only on downscaled
images. We question this design as low-resolution predictions often fail to
preserve fine details. The alternative of training with random crops of
high-resolution images alleviates this problem but falls short in capturing
long-range, domain-robust context information. Therefore, we propose HRDA, a
multi-resolution training approach for UDA, that combines the strengths of
small high-resolution crops to preserve fine segmentation details and large
low-resolution crops to capture long-range context dependencies with a learned
scale attention, while maintaining a manageable GPU memory footprint. HRDA
enables adapting small objects and preserving fine segmentation details. It
significantly improves the state-of-the-art performance by 5.5 mIoU for
GTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in
unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available
at https://github.com/lhoyer/HRDA.",0.15331672,0.16983399,0.03805311,A
5424,"Size ad mIoU

    1‚Äì        ‚Äì      1 (HR) 0.5  65.1 ¬±1.9
                     1 (HR) 0.5  65.9 ¬±1.2
    24        0.5    1 (HR) 0.5  68.5 ¬±0.6
                     1 (HR) 0.5  67.5 ¬±0.7
    3 2 (LR)  0.5

    4 1.33    0.5

D Influence of Context Scale

We further study the influence of the downscale factor of the context crop sc
in Tab.",Size ac Detail sd Detail Rel.,S1.,2022-04-27 18:00:26+00:00,HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lukas Hoyer'), arxiv.Result.Author('Dengxin Dai'), arxiv.Result.Author('Luc Van Gool')]","Unsupervised domain adaptation (UDA) aims to adapt a model trained on the
source domain (e.g. synthetic data) to the target domain (e.g. real-world data)
without requiring further annotations on the target domain. This work focuses
on UDA for semantic segmentation as real-world pixel-wise annotations are
particularly expensive to acquire. As UDA methods for semantic segmentation are
usually GPU memory intensive, most previous methods operate only on downscaled
images. We question this design as low-resolution predictions often fail to
preserve fine details. The alternative of training with random crops of
high-resolution images alleviates this problem but falls short in capturing
long-range, domain-robust context information. Therefore, we propose HRDA, a
multi-resolution training approach for UDA, that combines the strengths of
small high-resolution crops to preserve fine segmentation details and large
low-resolution crops to capture long-range context dependencies with a learned
scale attention, while maintaining a manageable GPU memory footprint. HRDA
enables adapting small objects and preserving fine segmentation details. It
significantly improves the state-of-the-art performance by 5.5 mIoU for
GTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in
unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available
at https://github.com/lhoyer/HRDA.",0.15331672,0.16983399,0.03805311,A
5448,"Therefore, under the VeRi-776 dataset, the
OVG model is not very helpful for improving the performance of vehicle Re-ID,
which is worthy of further study.","For multi-view recognition and inference, our OVG
method is still slightly insuÔ¨Écient.","5 Discussion

Fig.",2022-04-28 07:46:03+00:00,Discriminative-Region Attention and Orthogonal-View Generation Model for Vehicle Re-Identification,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Huadong Li'), arxiv.Result.Author('Yuefeng Wang'), arxiv.Result.Author('Ying Wei'), arxiv.Result.Author('Lin Wang'), arxiv.Result.Author('Li Ge')]","Vehicle re-identification (Re-ID) is urgently demanded to alleviate
thepressure caused by the increasingly onerous task of urban traffic
management. Multiple challenges hamper the applications of vision-based vehicle
Re-ID methods: (1) The appearances of different vehicles of the same
brand/model are often similar; However, (2) the appearances of the same vehicle
differ significantly from different viewpoints. Previous methods mainly use
manually annotated multi-attribute datasets to assist the network in getting
detailed cues and in inferencing multi-view to improve the vehicle Re-ID
performance. However, finely labeled vehicle datasets are usually unattainable
in real application scenarios. Hence, we propose a Discriminative-Region
Attention and Orthogonal-View Generation (DRA-OVG) model, which only requires
identity (ID) labels to conquer the multiple challenges of vehicle Re-ID.The
proposed DRA model can automatically extract the discriminative region
features, which can distinguish similar vehicles. And the OVG model can
generate multi-view features based on the input view features to reduce the
impact of viewpoint mismatches. Finally, the distance between vehicle
appearances is presented by the discriminative region features and multi-view
features together. Therefore, the significance of pairwise distance measure
between vehicles is enhanced in acomplete feature space. Extensive experiments
substantiate the effectiveness of each proposed ingredient, and experimental
results indicate that our approach achieves remarkable improvements over the
state- of-the-art vehicle Re-ID methods on VehicleID and VeRi-776 datasets.",-0.13675892,0.073117465,-0.04647016,B
5475,"To better handle the BFE problem in off-nadir
images and reduce the failure cases, further study may                     Hager, and M. Z.","The right one is most
challenging as both the shape and the location are difÔ¨Åcult                [13] G. Christie, R. R. R. M. Abujder, K. Foster, S. Hagstrom, G. D.
to predict.","Brown, ‚ÄúLearning geocentric object pose in
consider facade segmentation as an extra task in a multi-
task learning scheme.",2022-04-28 16:56:06+00:00,Learning to Extract Building Footprints from Off-Nadir Aerial Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinwang Wang'), arxiv.Result.Author('Lingxuan Meng'), arxiv.Result.Author('Weijia Li'), arxiv.Result.Author('Wen Yang'), arxiv.Result.Author('Lei Yu'), arxiv.Result.Author('Gui-Song Xia')]","Extracting building footprints from aerial images is essential for precise
urban mapping with photogrammetric computer vision technologies. Existing
approaches mainly assume that the roof and footprint of a building are well
overlapped, which may not hold in off-nadir aerial images as there is often a
big offset between them. In this paper, we propose an offset vector learning
scheme, which turns the building footprint extraction problem in off-nadir
images into an instance-level joint prediction problem of the building roof and
its corresponding ""roof to footprint"" offset vector. Thus the footprint can be
estimated by translating the predicted roof mask according to the predicted
offset vector. We further propose a simple but effective feature-level offset
augmentation module, which can significantly refine the offset vector
prediction by introducing little extra cost. Moreover, a new dataset, Buildings
in Off-Nadir Aerial Images (BONAI), is created and released in this paper. It
contains 268,958 building instances across 3,300 aerial images with fully
annotated instance-level roof, footprint, and corresponding offset vector for
each building. Experiments on the BONAI dataset demonstrate that our method
achieves the state-of-the-art, outperforming other competitors by 3.37 to 7.39
points in F1-score. The codes, datasets, and trained models are available at
https://github.com/jwwangchn/BONAI.git.",-0.25754994,0.13271084,-0.048618793,B
5480,"Future work should expand the number and complexity of objects to
further study the problems of depth ambiguity and hand-object occlusion in this setting.","Our work is the first step towards hand interaction with articulated objects
from RGB images.","Another limitation is that we use optical marker-based capture to provide accurate hand
and object poses, thus potentially introducing label noise.",2022-04-28 17:23:59+00:00,Articulated Objects in Free-form Hand Interaction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zicong Fan'), arxiv.Result.Author('Omid Taheri'), arxiv.Result.Author('Dimitrios Tzionas'), arxiv.Result.Author('Muhammed Kocabas'), arxiv.Result.Author('Manuel Kaufmann'), arxiv.Result.Author('Michael J. Black'), arxiv.Result.Author('Otmar Hilliges')]","We use our hands to interact with and to manipulate objects. Articulated
objects are especially interesting since they often require the full dexterity
of human hands to manipulate them. To understand, model, and synthesize such
interactions, automatic and robust methods that reconstruct hands and
articulated objects in 3D from a color image are needed. Existing methods for
estimating 3D hand and object pose from images focus on rigid objects. In part,
because such methods rely on training data and no dataset of articulated object
manipulation exists. Consequently, we introduce ARCTIC - the first dataset of
free-form interactions of hands and articulated objects. ARCTIC has 1.2M images
paired with accurate 3D meshes for both hands and for objects that move and
deform over time. The dataset also provides hand-object contact information. To
show the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D
reconstruction of two hands and an articulated object in interaction; (2) an
estimation of dense hand-object relative distances, which we call interaction
field estimation. For the first task, we present ArcticNet, a baseline method
for the task of jointly reconstructing two hands and an articulated object from
an RGB image. For interaction field estimation, we predict the relative
distances from each hand vertex to the object surface, and vice versa. We
introduce InterField, the first method that estimates such distances from a
single RGB image. We provide qualitative and quantitative experiments for both
tasks, and provide detailed analysis on the data. Code and data will be
available at https://arctic.is.tue.mpg.de.",-0.2623052,0.3169831,-0.20447594,B
5482,"Finally, extending from the single-agent set-
ting, we further study multi-agent behavior generation where multiple humans interact with
each other in complex social scenes.","To address the uncertainty in future human behavior, we develop two deep generative
models that can generate diverse future human motions using determinantal point processes
(DPPs) and latent normalizing Ô¨Çows respectively.","We develop a stochastic agent-aware trajectory generation
framework that can forecast diverse and socially-aware human trajectories.",2022-04-28 17:40:44+00:00,"Unified Simulation, Perception, and Generation of Human Behavior",cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']",[arxiv.Result.Author('Ye Yuan')],"Understanding and modeling human behavior is fundamental to almost any
computer vision and robotics applications that involve humans. In this thesis,
we take a holistic approach to human behavior modeling and tackle its three
essential aspects -- simulation, perception, and generation. Throughout the
thesis, we show how the three aspects are deeply connected and how utilizing
and improving one aspect can greatly benefit the other aspects. We also discuss
the lessons learned and our vision for what is next for human behavior
modeling.",-0.102496564,-0.026331097,-0.18221332,B
5483,"To further study the eÔ¨Äect of JSMLPs, we also show the design with and without JSMLPs for 3D

                                                             32
Locomotion and Swimmer in Figure 3.5.","For Swimmer, the variant without JSMLPs has a very large performance variance.","We can observe that the designs without JSMLPs are
overly symmetric and uniform while the designs with JSMLPs contain joint-specialized features
that help the agent achieve better performance.",2022-04-28 17:40:44+00:00,"Unified Simulation, Perception, and Generation of Human Behavior",cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']",[arxiv.Result.Author('Ye Yuan')],"Understanding and modeling human behavior is fundamental to almost any
computer vision and robotics applications that involve humans. In this thesis,
we take a holistic approach to human behavior modeling and tackle its three
essential aspects -- simulation, perception, and generation. Throughout the
thesis, we show how the three aspects are deeply connected and how utilizing
and improving one aspect can greatly benefit the other aspects. We also discuss
the lessons learned and our vision for what is next for human behavior
modeling.",0.12335782,0.23085147,-0.12506212,A
5503,"We further study the ef-
                                                                   fect of the number of negative samples.","In addition, to further           Number of Negative Samples.","As shown in Fig-
                                                                   ure 9, adding more negative samples achieves better per-
                                                                   formance, because the more negative samples, the more
                                                                   powerful constraints can be performed.",2022-04-29 09:22:01+00:00,SCS-Co: Self-Consistent Style Contrastive Learning for Image Harmonization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yucheng Hang'), arxiv.Result.Author('Bin Xia'), arxiv.Result.Author('Wenming Yang'), arxiv.Result.Author('Qingmin Liao')]","Image harmonization aims to achieve visual consistency in composite images by
adapting a foreground to make it compatible with a background. However,
existing methods always only use the real image as the positive sample to guide
the training, and at most introduce the corresponding composite image as a
single negative sample for an auxiliary constraint, which leads to limited
distortion knowledge, and further causes a too large solution space, making the
generated harmonized image distorted. Besides, none of them jointly constrain
from the foreground self-style and foreground-background style consistency,
which exacerbates this problem. Moreover, recent region-aware adaptive instance
normalization achieves great success but only considers the global background
feature distribution, making the aligned foreground feature distribution
biased. To address these issues, we propose a self-consistent style contrastive
learning scheme (SCS-Co). By dynamically generating multiple negative samples,
our SCS-Co can learn more distortion knowledge and well regularize the
generated harmonized image in the style representation space from two aspects
of the foreground self-style and foreground-background style consistency,
leading to a more photorealistic visual result. In addition, we propose a
background-attentional adaptive instance normalization (BAIN) to achieve an
attention-weighted background feature distribution according to the
foreground-background feature similarity. Experiments demonstrate the
superiority of our method over other state-of-the-art methods in both
quantitative comparison and visual analysis.",0.32355863,0.092899986,0.08780363,A
5507,"We plan to use them in further research of
                                                                                   high-level semantics of entities beyond the roles.","Moreover, the whole
                                                                                   origin image can be used in more fields, i.e., detection and
392126                                                                    0003     recognition.","All im-
        Figure 4.",2022-04-29 12:09:42+00:00,A Challenging Benchmark of Anime Style Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haotang Li'), arxiv.Result.Author('Shengtao Guo'), arxiv.Result.Author('Kailin Lyu'), arxiv.Result.Author('Xiao Yang'), arxiv.Result.Author('Tianchen Chen'), arxiv.Result.Author('Jianqing Zhu'), arxiv.Result.Author('Huanqiang Zeng')]","Given two images of different anime roles, anime style recognition (ASR) aims
to learn abstract painting style to determine whether the two images are from
the same work, which is an interesting but challenging problem. Unlike
biometric recognition, such as face recognition, iris recognition, and person
re-identification, ASR suffers from a much larger semantic gap but receives
less attention. In this paper, we propose a challenging ASR benchmark. Firstly,
we collect a large-scale ASR dataset (LSASRD), which contains 20,937 images of
190 anime works and each work at least has ten different roles. In addition to
the large-scale, LSASRD contains a list of challenging factors, such as complex
illuminations, various poses, theatrical colors and exaggerated compositions.
Secondly, we design a cross-role protocol to evaluate ASR performance, in which
query and gallery images must come from different roles to validate an ASR
model is to learn abstract painting style rather than learn discriminative
features of roles. Finally, we apply two powerful person re-identification
methods, namely, AGW and TransReID, to construct the baseline performance on
LSASRD. Surprisingly, the recent transformer model (i.e., TransReID) only
acquires a 42.24% mAP on LSASRD. Therefore, we believe that the ASR task of a
huge semantic gap deserves deep and long-term research. We will open our
dataset and code at https://github.com/nkjcqvcpi/ASR.",-0.20348325,-0.07143243,-0.20019758,C
5513,"Ablation of Semantically Mismatched Alignment On the basis of PyramidCLIP paradigm, we
further study on the additional effect of LGT and LLS, termed as semantically mismatched alignment,
shown in Figure 7(d).","And we attribute this to the inherent precise
alignment between the ROI feature sequence R and the object-attribute description TOA, since the
feature and category with attributes of each salient object in the image are extracted in pairs by the
pre-trained powerful object-attribute detector.","The ablation results are listed in Table 15, which reveal that adding semantically
mismatched alignment cannot bring stable beneÔ¨Åts, even degrades the performance in most cases.",2022-04-29 13:38:42+00:00,PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuting Gao'), arxiv.Result.Author('Jinfeng Liu'), arxiv.Result.Author('Zihan Xu'), arxiv.Result.Author('Jun Zhang'), arxiv.Result.Author('Ke Li'), arxiv.Result.Author('Rongrong Ji'), arxiv.Result.Author('Chunhua Shen')]","Large-scale vision-language pre-training has achieved promising results on
downstream tasks. Existing methods highly rely on the assumption that the
image-text pairs crawled from the Internet are in perfect one-to-one
correspondence. However, in real scenarios, this assumption can be difficult to
hold: the text description, obtained by crawling the affiliated metadata of the
image, often suffers from the semantic mismatch and the mutual compatibility.
To address these issues, we introduce PyramidCLIP, which constructs an input
pyramid with different semantic levels for each modality, and aligns visual
elements and linguistic elements in the form of hierarchy via peer-level
semantics alignment and cross-level relation alignment. Furthermore, we soften
the loss of negative samples (unpaired samples) so as to weaken the strict
constraint during the pre-training stage, thus mitigating the risk of forcing
the model to distinguish compatible negative pairs. Experiments on five
downstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In
particular, with the same amount of 15 million pre-training image-text pairs,
PyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by
10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder
respectively. When scaling to larger datasets, PyramidCLIP achieves the
state-of-the-art results on several downstream tasks. In particular, the
results of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that
of CLIP using 400M data on ImageNet zero-shot classification task,
significantly improving the data efficiency of CLIP.",-0.104155645,-0.10389654,-0.06791408,C
5515,"Despite building upon simple structures,
                                        To enable further research on this task, the dataset is publicly     highly complex graphs can become too difÔ¨Åcult to interpret for
                                        available at https://bit.ly/3jN1pJJ.",types of arrows.,handcrafted methods.,2022-04-29 14:44:52+00:00,Towards Automatic Parsing of Structured Visual Content through the Use of Synthetic Data,cs.CV,"['cs.CV', 'I.4.9; I.7.5']","[arxiv.Result.Author('Lukas Scholch'), arxiv.Result.Author('Jonas Steinhauser'), arxiv.Result.Author('Maximilian Beichter'), arxiv.Result.Author('Constantin Seibold'), arxiv.Result.Author('Kailun Yang'), arxiv.Result.Author('Merlin Kn√§ble'), arxiv.Result.Author('Thorsten Schwarz'), arxiv.Result.Author('Alexander M√§dche'), arxiv.Result.Author('Rainer Stiefelhagen')]","Structured Visual Content (SVC) such as graphs, flow charts, or the like are
used by authors to illustrate various concepts. While such depictions allow the
average reader to better understand the contents, images containing SVCs are
typically not machine-readable. This, in turn, not only hinders automated
knowledge aggregation, but also the perception of displayed in-formation for
visually impaired people. In this work, we propose a synthetic dataset,
containing SVCs in the form of images as well as ground truths. We show the
usage of this dataset by an application that automatically extracts a graph
representation from an SVC image. This is done by training a model via common
supervised learning methods. As there currently exist no large-scale public
datasets for the detailed analysis of SVC, we propose the Synthetic SVC (SSVC)
dataset comprising 12,000 images with respective bounding box annotations and
detailed graph representations. Our dataset enables the development of strong
models for the interpretation of SVCs while skipping the time-consuming dense
data annotation. We evaluate our model on both synthetic and manually annotated
data and show the transferability of synthetic to real via various metrics,
given the presented application. Here, we evaluate that this proof of concept
is possible to some extend and lay down a solid baseline for this task. We
discuss the limitations of our approach for further improvements. Our utilized
metrics can be used as a tool for future comparisons in this domain. To enable
further research on this task, the dataset is publicly available at
https://bit.ly/3jN1pJJ",0.08865601,-0.026694823,-0.16847952,A
5525,"This assumption doesn‚Äôt hold in general, and further research is
needed to develop techniques to address these issues in the few-shot setting.","They also show that such issues
can be mitigated with calibration techniques, provided one can assume a certain prior distribution
(e.g., uniform) over the label space.","More generally, seeking
objectives, architectures, or evaluation procedures that could bridge the gap between these two
classes of models is a promising research direction.",2022-04-29 16:29:01+00:00,Flamingo: a Visual Language Model for Few-Shot Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jean-Baptiste Alayrac'), arxiv.Result.Author('Jeff Donahue'), arxiv.Result.Author('Pauline Luc'), arxiv.Result.Author('Antoine Miech'), arxiv.Result.Author('Iain Barr'), arxiv.Result.Author('Yana Hasson'), arxiv.Result.Author('Karel Lenc'), arxiv.Result.Author('Arthur Mensch'), arxiv.Result.Author('Katie Millican'), arxiv.Result.Author('Malcolm Reynolds'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('Eliza Rutherford'), arxiv.Result.Author('Serkan Cabi'), arxiv.Result.Author('Tengda Han'), arxiv.Result.Author('Zhitao Gong'), arxiv.Result.Author('Sina Samangooei'), arxiv.Result.Author('Marianne Monteiro'), arxiv.Result.Author('Jacob Menick'), arxiv.Result.Author('Sebastian Borgeaud'), arxiv.Result.Author('Andrew Brock'), arxiv.Result.Author('Aida Nematzadeh'), arxiv.Result.Author('Sahand Sharifzadeh'), arxiv.Result.Author('Mikolaj Binkowski'), arxiv.Result.Author('Ricardo Barreira'), arxiv.Result.Author('Oriol Vinyals'), arxiv.Result.Author('Andrew Zisserman'), arxiv.Result.Author('Karen Simonyan')]","Building models that can be rapidly adapted to numerous tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. Flamingo models include key architectural innovations
to: (i) bridge powerful pretrained vision-only and language-only models, (ii)
handle sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
the proposed Flamingo models, exploring and measuring their ability to rapidly
adapt to a variety of image and video understanding benchmarks. These include
open-ended tasks such as visual question-answering, where the model is prompted
with a question which it has to answer, captioning tasks, which evaluate the
ability to describe a scene or an event, and close-ended tasks such as multiple
choice visual question-answering. For tasks lying anywhere on this spectrum, we
demonstrate that a single Flamingo model can achieve a new state of the art for
few-shot learning, simply by prompting the model with task-specific examples.
On many of these benchmarks, Flamingo actually surpasses the performance of
models that are fine-tuned on thousands of times more task-specific data.",0.08851062,-0.0069770413,-0.13157314,A
5526,"This
study is preliminary and we foresee that further research eÔ¨Äorts should be undertaken to better assess
those risks.","Risks and mitigation strategies

This section provides some early investigations of the potential risks of models like Flamingo.",We also discuss potential mitigation strategies towards safely deploying these models.,2022-04-29 16:29:01+00:00,Flamingo: a Visual Language Model for Few-Shot Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jean-Baptiste Alayrac'), arxiv.Result.Author('Jeff Donahue'), arxiv.Result.Author('Pauline Luc'), arxiv.Result.Author('Antoine Miech'), arxiv.Result.Author('Iain Barr'), arxiv.Result.Author('Yana Hasson'), arxiv.Result.Author('Karel Lenc'), arxiv.Result.Author('Arthur Mensch'), arxiv.Result.Author('Katie Millican'), arxiv.Result.Author('Malcolm Reynolds'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('Eliza Rutherford'), arxiv.Result.Author('Serkan Cabi'), arxiv.Result.Author('Tengda Han'), arxiv.Result.Author('Zhitao Gong'), arxiv.Result.Author('Sina Samangooei'), arxiv.Result.Author('Marianne Monteiro'), arxiv.Result.Author('Jacob Menick'), arxiv.Result.Author('Sebastian Borgeaud'), arxiv.Result.Author('Andrew Brock'), arxiv.Result.Author('Aida Nematzadeh'), arxiv.Result.Author('Sahand Sharifzadeh'), arxiv.Result.Author('Mikolaj Binkowski'), arxiv.Result.Author('Ricardo Barreira'), arxiv.Result.Author('Oriol Vinyals'), arxiv.Result.Author('Andrew Zisserman'), arxiv.Result.Author('Karen Simonyan')]","Building models that can be rapidly adapted to numerous tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. Flamingo models include key architectural innovations
to: (i) bridge powerful pretrained vision-only and language-only models, (ii)
handle sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
the proposed Flamingo models, exploring and measuring their ability to rapidly
adapt to a variety of image and video understanding benchmarks. These include
open-ended tasks such as visual question-answering, where the model is prompted
with a question which it has to answer, captioning tasks, which evaluate the
ability to describe a scene or an event, and close-ended tasks such as multiple
choice visual question-answering. For tasks lying anywhere on this spectrum, we
demonstrate that a single Flamingo model can achieve a new state of the art for
few-shot learning, simply by prompting the model with task-specific examples.
On many of these benchmarks, Flamingo actually surpasses the performance of
models that are fine-tuned on thousands of times more task-specific data.",0.3153352,0.05465384,-0.15029214,A
5527,"This assumption doesn‚Äôt hold in general, and further research is
needed to develop techniques to address these issues in the few-shot setting.","They also show that such issues
can be mitigated with calibration techniques, provided one can assume a certain prior distribution
(e.g., uniform) over the label space.","More generally, seeking
objectives, architectures, or evaluation procedures that could bridge the gap between these two classes
of models is a promising research direction.",2022-04-29 16:29:01+00:00,Flamingo: a Visual Language Model for Few-Shot Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jean-Baptiste Alayrac'), arxiv.Result.Author('Jeff Donahue'), arxiv.Result.Author('Pauline Luc'), arxiv.Result.Author('Antoine Miech'), arxiv.Result.Author('Iain Barr'), arxiv.Result.Author('Yana Hasson'), arxiv.Result.Author('Karel Lenc'), arxiv.Result.Author('Arthur Mensch'), arxiv.Result.Author('Katie Millican'), arxiv.Result.Author('Malcolm Reynolds'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('Eliza Rutherford'), arxiv.Result.Author('Serkan Cabi'), arxiv.Result.Author('Tengda Han'), arxiv.Result.Author('Zhitao Gong'), arxiv.Result.Author('Sina Samangooei'), arxiv.Result.Author('Marianne Monteiro'), arxiv.Result.Author('Jacob Menick'), arxiv.Result.Author('Sebastian Borgeaud'), arxiv.Result.Author('Andrew Brock'), arxiv.Result.Author('Aida Nematzadeh'), arxiv.Result.Author('Sahand Sharifzadeh'), arxiv.Result.Author('Mikolaj Binkowski'), arxiv.Result.Author('Ricardo Barreira'), arxiv.Result.Author('Oriol Vinyals'), arxiv.Result.Author('Andrew Zisserman'), arxiv.Result.Author('Karen Simonyan')]","Building models that can be rapidly adapted to novel tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. We propose key architectural innovations to: (i)
bridge powerful pretrained vision-only and language-only models, (ii) handle
sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
our models, exploring and measuring their ability to rapidly adapt to a variety
of image and video tasks. These include open-ended tasks such as visual
question-answering, where the model is prompted with a question which it has to
answer; captioning tasks, which evaluate the ability to describe a scene or an
event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve
a new state of the art with few-shot learning, simply by prompting the model
with task-specific examples. On numerous benchmarks, Flamingo outperforms
models fine-tuned on thousands of times more task-specific data.",0.08851062,-0.0069770413,-0.13157314,A
5528,"We hope such
results may inspire further research into how existing models can be repurposed efÔ¨Åciently rather
than trained from scratch.",[105] for language models.,"D.2.2 Risks and mitigation strategies

This section provides some early investigations of the potential risks of models like Flamingo.",2022-04-29 16:29:01+00:00,Flamingo: a Visual Language Model for Few-Shot Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jean-Baptiste Alayrac'), arxiv.Result.Author('Jeff Donahue'), arxiv.Result.Author('Pauline Luc'), arxiv.Result.Author('Antoine Miech'), arxiv.Result.Author('Iain Barr'), arxiv.Result.Author('Yana Hasson'), arxiv.Result.Author('Karel Lenc'), arxiv.Result.Author('Arthur Mensch'), arxiv.Result.Author('Katie Millican'), arxiv.Result.Author('Malcolm Reynolds'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('Eliza Rutherford'), arxiv.Result.Author('Serkan Cabi'), arxiv.Result.Author('Tengda Han'), arxiv.Result.Author('Zhitao Gong'), arxiv.Result.Author('Sina Samangooei'), arxiv.Result.Author('Marianne Monteiro'), arxiv.Result.Author('Jacob Menick'), arxiv.Result.Author('Sebastian Borgeaud'), arxiv.Result.Author('Andrew Brock'), arxiv.Result.Author('Aida Nematzadeh'), arxiv.Result.Author('Sahand Sharifzadeh'), arxiv.Result.Author('Mikolaj Binkowski'), arxiv.Result.Author('Ricardo Barreira'), arxiv.Result.Author('Oriol Vinyals'), arxiv.Result.Author('Andrew Zisserman'), arxiv.Result.Author('Karen Simonyan')]","Building models that can be rapidly adapted to novel tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. We propose key architectural innovations to: (i)
bridge powerful pretrained vision-only and language-only models, (ii) handle
sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
our models, exploring and measuring their ability to rapidly adapt to a variety
of image and video tasks. These include open-ended tasks such as visual
question-answering, where the model is prompted with a question which it has to
answer; captioning tasks, which evaluate the ability to describe a scene or an
event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve
a new state of the art with few-shot learning, simply by prompting the model
with task-specific examples. On numerous benchmarks, Flamingo outperforms
models fine-tuned on thousands of times more task-specific data.",0.24019611,-0.31757748,-0.17315045,A
5529,"This
study is preliminary and we foresee that further research efforts should be undertaken to better
assess those risks.","D.2.2 Risks and mitigation strategies

This section provides some early investigations of the potential risks of models like Flamingo.","We also discuss potential mitigation strategies towards safely deploying these
models.",2022-04-29 16:29:01+00:00,Flamingo: a Visual Language Model for Few-Shot Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jean-Baptiste Alayrac'), arxiv.Result.Author('Jeff Donahue'), arxiv.Result.Author('Pauline Luc'), arxiv.Result.Author('Antoine Miech'), arxiv.Result.Author('Iain Barr'), arxiv.Result.Author('Yana Hasson'), arxiv.Result.Author('Karel Lenc'), arxiv.Result.Author('Arthur Mensch'), arxiv.Result.Author('Katie Millican'), arxiv.Result.Author('Malcolm Reynolds'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('Eliza Rutherford'), arxiv.Result.Author('Serkan Cabi'), arxiv.Result.Author('Tengda Han'), arxiv.Result.Author('Zhitao Gong'), arxiv.Result.Author('Sina Samangooei'), arxiv.Result.Author('Marianne Monteiro'), arxiv.Result.Author('Jacob Menick'), arxiv.Result.Author('Sebastian Borgeaud'), arxiv.Result.Author('Andrew Brock'), arxiv.Result.Author('Aida Nematzadeh'), arxiv.Result.Author('Sahand Sharifzadeh'), arxiv.Result.Author('Mikolaj Binkowski'), arxiv.Result.Author('Ricardo Barreira'), arxiv.Result.Author('Oriol Vinyals'), arxiv.Result.Author('Andrew Zisserman'), arxiv.Result.Author('Karen Simonyan')]","Building models that can be rapidly adapted to novel tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. We propose key architectural innovations to: (i)
bridge powerful pretrained vision-only and language-only models, (ii) handle
sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
our models, exploring and measuring their ability to rapidly adapt to a variety
of image and video tasks. These include open-ended tasks such as visual
question-answering, where the model is prompted with a question which it has to
answer; captioning tasks, which evaluate the ability to describe a scene or an
event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve
a new state of the art with few-shot learning, simply by prompting the model
with task-specific examples. On numerous benchmarks, Flamingo outperforms
models fine-tuned on thousands of times more task-specific data.",0.32141006,0.055494286,-0.1559103,A
5557,While the third line      further research in scene text recognition.,"We hope that SVTR will foster
ture by viewing a character as a whole.","exhibits three maps simultaneously activate multiple charac-
ters.",2022-04-30 04:37:01+00:00,SVTR: Scene Text Recognition with a Single Visual Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yongkun Du'), arxiv.Result.Author('Zhineng Chen'), arxiv.Result.Author('Caiyan Jia'), arxiv.Result.Author('Xiaoting Yin'), arxiv.Result.Author('Tianlun Zheng'), arxiv.Result.Author('Chenxia Li'), arxiv.Result.Author('Yuning Du'), arxiv.Result.Author('Yu-Gang Jiang')]","Dominant scene text recognition models commonly contain two building blocks,
a visual model for feature extraction and a sequence model for text
transcription. This hybrid architecture, although accurate, is complex and less
efficient. In this study, we propose a Single Visual model for Scene Text
recognition within the patch-wise image tokenization framework, which dispenses
with the sequential modeling entirely. The method, termed SVTR, firstly
decomposes an image text into small patches named character components.
Afterward, hierarchical stages are recurrently carried out by component-level
mixing, merging and/or combining. Global and local mixing blocks are devised to
perceive the inter-character and intra-character patterns, leading to a
multi-grained character component perception. Thus, characters are recognized
by a simple linear prediction. Experimental results on both English and Chinese
scene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L
(Large) achieves highly competitive accuracy in English and outperforms
existing methods by a large margin in Chinese, while running faster. In
addition, SVTR-T (Tiny) is an effective and much smaller model, which shows
appealing speed at inference. The code is publicly available at
https://github.com/PaddlePaddle/PaddleOCR.",-0.043422084,-0.04508833,-0.212322,C
5558,While the third line      further research in scene text recognition.,"We hope that SVTR will foster
ture by viewing a character as a whole.","exhibits three maps simultaneously activate multiple charac-
ters.",2022-04-30 04:37:01+00:00,SVTR: Scene Text Recognition with a Single Visual Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yongkun Du'), arxiv.Result.Author('Zhineng Chen'), arxiv.Result.Author('Caiyan Jia'), arxiv.Result.Author('Xiaoting Yin'), arxiv.Result.Author('Tianlun Zheng'), arxiv.Result.Author('Chenxia Li'), arxiv.Result.Author('Yuning Du'), arxiv.Result.Author('Yu-Gang Jiang')]","Dominant scene text recognition models commonly contain two building blocks,
a visual model for feature extraction and a sequence model for text
transcription. This hybrid architecture, although accurate, is complex and less
efficient. In this study, we propose a Single Visual model for Scene Text
recognition within the patch-wise image tokenization framework, which dispenses
with the sequential modeling entirely. The method, termed SVTR, firstly
decomposes an image text into small patches named character components.
Afterward, hierarchical stages are recurrently carried out by component-level
mixing, merging and/or combining. Global and local mixing blocks are devised to
perceive the inter-character and intra-character patterns, leading to a
multi-grained character component perception. Thus, characters are recognized
by a simple linear prediction. Experimental results on both English and Chinese
scene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L
(Large) achieves highly competitive accuracy in English and outperforms
existing methods by a large margin in Chinese, while running faster. In
addition, SVTR-T (Tiny) is an effective and much smaller model, which shows
appealing speed at inference. The code is publicly available at
https://github.com/PaddlePaddle/PaddleOCR.",-0.043422084,-0.04508833,-0.212322,C
5572,"Moreover, the shortage of large-scale open-source datasets also
                                        retards further research in this Ô¨Åeld.",different modalities.,"In this paper, we propose             To address this issue, Li [10] uses a supervised method to
                                        an unsupervised visible-light image guided cross-spectrum (i.e.,          train the dual-spectral depth estimation network with fusion
                                        thermal and visible-light, TIR-VIS in short) depth estimation             TIR-VIS images.",2022-04-30 12:58:35+00:00,Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yubin Guo'), arxiv.Result.Author('Haobo Jiang'), arxiv.Result.Author('Xinlei Qi'), arxiv.Result.Author('Jin Xie'), arxiv.Result.Author('Cheng-Zhong Xu'), arxiv.Result.Author('Hui Kong')]","Cross-spectrum depth estimation aims to provide a depth map in all
illumination conditions with a pair of dual-spectrum images. It is valuable for
autonomous vehicle applications when the vehicle is equipped with two cameras
of different modalities. However, images captured by different-modality cameras
can be photometrically quite different. Therefore, cross-spectrum depth
estimation is a very challenging problem. Moreover, the shortage of large-scale
open-source datasets also retards further research in this field. In this
paper, we propose an unsupervised visible-light image guided cross-spectrum
(i.e., thermal and visible-light, TIR-VIS in short) depth estimation framework
given a pair of RGB and thermal images captured from a visible-light camera and
a thermal one. We first adopt a base depth estimation network using RGB-image
pairs. Then we propose a multi-scale feature transfer network to transfer
features from the TIR-VIS domain to the VIS domain at the feature level to fit
the trained depth estimation network. At last, we propose a cross-spectrum
depth cycle consistency to improve the depth result of dual-spectrum image
pairs. Meanwhile, we release a large dual-spectrum depth estimation dataset
with visible-light and far-infrared stereo images captured in different scenes
to the society. The experiment result shows that our method achieves better
performance than the compared existing methods. Our datasets is available at
https://github.com/whitecrow1027/VIS-TIR-Datasets.",-0.19921367,0.12114334,0.12886187,B
5573,Compared to the vision system with                   further research in this direction.,"(3) There are
                                        are related to object thermal radiation and not affected                  barely open-source large-scale dual-spectrum datasets for the
                                        by illumination conditions as much as images by visible-                  depth estimation problems, and the lack of such data retards
                                        light (VIS) cameras.","only visible-light cameras, Dual-spectrum visual system with
                                        additional TIR cameras is more robust to different illumina-                 In this work, we propose an unsupervised visible-light
                                        tion conditions such as dark or overexposed scenes.",2022-04-30 12:58:35+00:00,Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yubin Guo'), arxiv.Result.Author('Haobo Jiang'), arxiv.Result.Author('Xinlei Qi'), arxiv.Result.Author('Jin Xie'), arxiv.Result.Author('Cheng-Zhong Xu'), arxiv.Result.Author('Hui Kong')]","Cross-spectrum depth estimation aims to provide a depth map in all
illumination conditions with a pair of dual-spectrum images. It is valuable for
autonomous vehicle applications when the vehicle is equipped with two cameras
of different modalities. However, images captured by different-modality cameras
can be photometrically quite different. Therefore, cross-spectrum depth
estimation is a very challenging problem. Moreover, the shortage of large-scale
open-source datasets also retards further research in this field. In this
paper, we propose an unsupervised visible-light image guided cross-spectrum
(i.e., thermal and visible-light, TIR-VIS in short) depth estimation framework
given a pair of RGB and thermal images captured from a visible-light camera and
a thermal one. We first adopt a base depth estimation network using RGB-image
pairs. Then we propose a multi-scale feature transfer network to transfer
features from the TIR-VIS domain to the VIS domain at the feature level to fit
the trained depth estimation network. At last, we propose a cross-spectrum
depth cycle consistency to improve the depth result of dual-spectrum image
pairs. Meanwhile, we release a large dual-spectrum depth estimation dataset
with visible-light and far-infrared stereo images captured in different scenes
to the society. The experiment result shows that our method achieves better
performance than the compared existing methods. Our datasets is available at
https://github.com/whitecrow1027/VIS-TIR-Datasets.",-0.22225913,0.24578702,0.041407574,B
5585,"signiÔ¨Åcantly reduced computational cost, and facial landmarks
                                       can signiÔ¨Åcantly contribute to MER and are worth further study        As we know, the graph-based model can process the fa-
                                       for efÔ¨Åcient ME analysis.","this paper, we make the Ô¨Årst step to study the discriminability
                                       The experimental results demonstrate that the proposed method      of landmarks and employ only landmarks as input to recognize
                                       can achieve competitive or even superior performance with a        ME, for exploring its advantages over RGB input.",cial landmarks lying in the non-Euclidean space.,2022-05-01 02:20:43+00:00,Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jinsheng Wei'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Guanming Lu'), arxiv.Result.Author('Yante Li'), arxiv.Result.Author('Jingjie Yan'), arxiv.Result.Author('Guoying Zhao')]","Micro-expression recognition (MER) is valuable because the involuntary nature
of micro-expressions (MEs) can reveal genuine emotions. Most works recognize
MEs by taking RGB videos or images as input. In fact, the activated facial
regions in ME images are very small and the subtle motion can be easily
submerged in the unrelated information. Facial landmarks are a low-dimensional
and compact modality, which leads to much lower computational cost and can
potentially concentrate more on ME-related features. However, the
discriminability of landmarks for MER is not clear. Thus, this paper explores
the contribution of facial landmarks and constructs a new framework to
efficiently recognize MEs with sole facial landmark information. Specially, we
design a separate structure module to separately aggregate the spatial and
temporal information in the geometric movement graph based on facial landmarks,
and a Geometric Two-Stream Graph Network is constructed to aggregate the
low-order geometric information and high-order semantic information of facial
landmarks. Furthermore, two core components are proposed to enhance features.
Specifically, a semantic adjacency matrix can automatically model the
relationship between nodes even long-distance nodes in a self-learning fashion;
and an Adaptive Action Unit loss is introduced to guide the learning process
such that the learned features are forced to have a synchronized pattern with
facial action units. Notably, this work tackles MER only utilizing geometric
features, processed based on a graph model, which provides a new idea with much
higher efficiency to promote MER. The experimental results demonstrate that the
proposed method can achieve competitive or even superior performance with a
significantly reduced computational cost, and facial landmarks can
significantly contribute to MER and are worth further study for efficient ME
analysis.",-0.13062961,0.14809003,-0.1090228,B
5586,"Therefore, how to
GTS-GN+Type B     72.91  0.710  73.05  0.693                      better combine the appearance features with the geometric
                  73.31  0.717  75.89  0.741                      features is also worthy of further study.","In addition, this paper
GCN+Type A                                                        demonstrates the discriminability and efÔ¨Åciency of landmarks,
SS-GN+Type A      70.12  0.690  70.21  0.661                      yet the geometric features from landmarks and the appearance
SS-GN+Type B      72.91  0.716  75.17  0.732                      features from ME images do not conÔ¨Çict.",(a) Graph models with different node features.,2022-05-01 02:20:43+00:00,Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jinsheng Wei'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Guanming Lu'), arxiv.Result.Author('Yante Li'), arxiv.Result.Author('Jingjie Yan'), arxiv.Result.Author('Guoying Zhao')]","Micro-expression recognition (MER) is valuable because the involuntary nature
of micro-expressions (MEs) can reveal genuine emotions. Most works recognize
MEs by taking RGB videos or images as input. In fact, the activated facial
regions in ME images are very small and the subtle motion can be easily
submerged in the unrelated information. Facial landmarks are a low-dimensional
and compact modality, which leads to much lower computational cost and can
potentially concentrate more on ME-related features. However, the
discriminability of landmarks for MER is not clear. Thus, this paper explores
the contribution of facial landmarks and constructs a new framework to
efficiently recognize MEs with sole facial landmark information. Specially, we
design a separate structure module to separately aggregate the spatial and
temporal information in the geometric movement graph based on facial landmarks,
and a Geometric Two-Stream Graph Network is constructed to aggregate the
low-order geometric information and high-order semantic information of facial
landmarks. Furthermore, two core components are proposed to enhance features.
Specifically, a semantic adjacency matrix can automatically model the
relationship between nodes even long-distance nodes in a self-learning fashion;
and an Adaptive Action Unit loss is introduced to guide the learning process
such that the learned features are forced to have a synchronized pattern with
facial action units. Notably, this work tackles MER only utilizing geometric
features, processed based on a graph model, which provides a new idea with much
higher efficiency to promote MER. The experimental results demonstrate that the
proposed method can achieve competitive or even superior performance with a
significantly reduced computational cost, and facial landmarks can
significantly contribute to MER and are worth further study for efficient ME
analysis.",-0.043242276,0.10890239,-0.044813108,B
5606,"‚Äì To stimulate further research in controllable synthesis of human motion, we
     will release the COUCH model and dataset.","The dataset features multiple subject, real
     chair geometry, accurately annotated hand contacts, and RGB-D images.","2 Related Work

Scene agnostic human motion prediction.",2022-05-01 19:14:22+00:00,COUCH: Towards Controllable Human-Chair Interactions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaohan Zhang'), arxiv.Result.Author('Bharat Lal Bhatnagar'), arxiv.Result.Author('Vladimir Guzov'), arxiv.Result.Author('Sebastian Starke'), arxiv.Result.Author('Gerard Pons-Moll')]","Humans interact with an object in many different ways by making contact at
different locations, creating a highly complex motion space that can be
difficult to learn, particularly when synthesizing such human interactions in a
controllable manner. Existing works on synthesizing human scene interaction
focus on the high-level control of action but do not consider the fine-grained
control of motion. In this work, we study the problem of synthesizing scene
interactions conditioned on different contact positions on the object. As a
testbed to investigate this new problem, we focus on human-chair interaction as
one of the most common actions which exhibit large variability in terms of
contacts. We propose a novel synthesis framework COUCH that plans ahead the
motion by predicting contact-aware control signals of the hands, which are then
used to synthesize contact-conditioned interactions. Furthermore, we contribute
a large human-chair interaction dataset with clean annotations, the COUCH
Dataset. Our method shows significant quantitative and qualitative improvements
over existing methods for human-object interactions. More importantly, our
method enables control of the motion through user-specified or automatically
predicted contacts.",-0.21538964,0.097931206,-0.1972456,B
5637,"We evaluate our method on three contactless Ô¨Ångerprint            Several researchers [29] [35] [30] further study the appli-
databases: PolyU Contactless 3D Fingerprint Database [18],        cation of 3D Ô¨Ångerprint feature extraction and recognition
UNSW 2D/3D Fingerprint Database [19], and PolyU Con-              after 3D reconstruction.",prints.,"The feature extraction and matching
tactless 2D to Contact-based 2D Fingerprint Database [20]         are conducted entirely in 3D space and don‚Äôt consider the
for evaluating 3D reconstruction performance and the con-         perspective distortion of Ô¨Ångerprint images.",2022-05-02 15:09:05+00:00,Monocular 3D Fingerprint Reconstruction and Unwarping,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhe Cui'), arxiv.Result.Author('Jianjiang Feng'), arxiv.Result.Author('Jie Zhou')]","Compared with contact-based fingerprint acquisition techniques, contactless
acquisition has the advantages of less skin distortion, larger fingerprint
area, and hygienic acquisition. However, perspective distortion is a challenge
in contactless fingerprint recognition, which changes ridge orientation,
frequency, and minutiae location, and thus causes degraded recognition
accuracy. We propose a learning based shape from texture algorithm to
reconstruct a 3D finger shape from a single image and unwarp the raw image to
suppress perspective distortion. Experimental results on contactless
fingerprint databases show that the proposed method has high 3D reconstruction
accuracy. Matching experiments on contactless-contact and
contactless-contactless matching prove that the proposed method improves
matching accuracy.",-0.036040813,0.34291685,-0.036830455,B
5638,"Gradtrue, and Deptrue are the ground-truth orientation, pe-           To further study the accuracy of the proposed recon-
riod, gradient, and depth value.",estimate shapes and have larger errors.,"1/|ùëÄ | means averaging           struction algorithm, we conduct a numerical experiment.",2022-05-02 15:09:05+00:00,Monocular 3D Fingerprint Reconstruction and Unwarping,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhe Cui'), arxiv.Result.Author('Jianjiang Feng'), arxiv.Result.Author('Jie Zhou')]","Compared with contact-based fingerprint acquisition techniques, contactless
acquisition has the advantages of less skin distortion, larger fingerprint
area, and hygienic acquisition. However, perspective distortion is a challenge
in contactless fingerprint recognition, which changes ridge orientation,
frequency, and minutiae location, and thus causes degraded recognition
accuracy. We propose a learning based shape from texture algorithm to
reconstruct a 3D finger shape from a single image and unwarp the raw image to
suppress perspective distortion. Experimental results on contactless
fingerprint databases show that the proposed method has high 3D reconstruction
accuracy. Matching experiments on contactless-contact and
contactless-contactless matching prove that the proposed method improves
matching accuracy.",0.108170375,0.37129003,-0.0013176072,B
5670,"From the aforementioned tutorials along
with their citations, readers can have a basic knowledge of relevant backgrounds in preparation for
the further research.","Following that, we recommend Ô¨Åve classic references in RS & DL, which aims to
present some successful applications in RS achieved by DL.",4.2.,2022-05-03 09:08:16+00:00,Deep Learning in Multimodal Remote Sensing Data Fusion: A Comprehensive Review,cs.CV,"['cs.CV', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Jiaxin Li'), arxiv.Result.Author('Danfeng Hong'), arxiv.Result.Author('Lianru Gao'), arxiv.Result.Author('Jing Yao'), arxiv.Result.Author('Ke Zheng'), arxiv.Result.Author('Bing Zhang'), arxiv.Result.Author('Jocelyn Chanussot')]","With the extremely rapid advances in remote sensing (RS) technology, a great
quantity of Earth observation (EO) data featuring considerable and complicated
heterogeneity is readily available nowadays, which renders researchers an
opportunity to tackle current geoscience applications in a fresh way. With the
joint utilization of EO data, much research on multimodal RS data fusion has
made tremendous progress in recent years, yet these developed traditional
algorithms inevitably meet the performance bottleneck due to the lack of the
ability to comprehensively analyse and interpret these strongly heterogeneous
data. Hence, this non-negligible limitation further arouses an intense demand
for an alternative tool with powerful processing competence. Deep learning
(DL), as a cutting-edge technology, has witnessed remarkable breakthroughs in
numerous computer vision tasks owing to its impressive ability in data
representation and reconstruction. Naturally, it has been successfully applied
to the field of multimodal RS data fusion, yielding great improvement compared
with traditional methods. This survey aims to present a systematic overview in
DL-based multimodal RS data fusion. More specifically, some essential knowledge
about this topic is first given. Subsequently, a literature survey is conducted
to analyse the trends of this field. Some prevalent sub-fields in the
multimodal RS data fusion are then reviewed in terms of the to-be-fused data
modalities, i.e., spatiospectral, spatiotemporal, light detection and
ranging-optical, synthetic aperture radar-optical, and RS-Geospatial Big Data
fusion. Furthermore, We collect and summarize some valuable resources for the
sake of the development in multimodal RS data fusion. Finally, the remaining
challenges and potential future directions are highlighted.",0.31647676,0.068677485,-0.16413449,A
5688,"[27] does
representation even in deep network layers, we can proÔ¨Åt        further research with established state of the art top-down
from a symbiosis of the individual tasks [18].",Guesdon et al.,"Network speed    techniques for single-person driver pose estimation without
can also be increased and computing costs reduced.",2022-05-03 14:11:18+00:00,"Multitask Network for Joint Object Detection, Semantic Segmentation and Human Pose Estimation in Vehicle Occupancy Monitoring",cs.CV,['cs.CV'],"[arxiv.Result.Author('Nikolas Ebert'), arxiv.Result.Author('Patrick Mangat'), arxiv.Result.Author('Oliver Wasenm√ºller')]","In order to ensure safe autonomous driving, precise information about the
conditions in and around the vehicle must be available. Accordingly, the
monitoring of occupants and objects inside the vehicle is crucial. In the
state-of-the-art, single or multiple deep neural networks are used for either
object recognition, semantic segmentation, or human pose estimation. In
contrast, we propose our Multitask Detection, Segmentation and Pose Estimation
Network (MDSP) -- the first multitask network solving all these three tasks
jointly in the area of occupancy monitoring. Due to the shared architecture,
memory and computing costs can be saved while achieving higher accuracy.
Furthermore, our architecture allows a flexible combination of the three
mentioned tasks during a simple end-to-end training. We perform comprehensive
evaluations on the public datasets SVIRO and TiCaM in order to demonstrate the
superior performance.",-0.24065566,-0.007551592,0.014180429,B
5707,"While the probabilistic segmentation
models capture knowledge in terms of a single probability distribution and
can not distinguish between aleatoric and epistemic uncertainty, which lim-
its the further study of uncertainty and reliability.","The popularity of deep
segmentation models has revived the research on model uncertainty esti-
mation and has given rise to speciÔ¨Åc methods such as variational dropout
[21, 22], and model ensembles [18, 23].","In this paper, we focus
on a diÔ¨Äerent uncertainty quantiÔ¨Åcation approach based on belief function
theory (BFT) [24, 25, 26] with the speciÔ¨Åc focus on medical image segmenta-
tion.",2022-05-03 19:06:45+00:00,Application of belief functions to medical image segmentation: A review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ling Huang'), arxiv.Result.Author('Su Ruan')]","The investigation of uncertainty is of major importance in risk-critical
applications, such as medical image segmentation. Belief function theory, a
formal framework for uncertainty analysis and multiple evidence fusion, has
made significant contributions in the medical domain, especially since the
development of deep learning. Medical image segmentation with belief function
theory has shown significant benefits in clinical diagnosis and medical image
research. In this paper, we provide a review of medical image segmentation
methods using belief function theory. We classify the methods according to the
fusion step and explain how information with uncertainty or imprecision is
modeled and fused with belief function theory. In addition, we discuss the
challenges and limitations of present belief function-based medical image
segmentation and propose orientations for future research. Future research
could investigate both belief function theory and deep learning to achieve more
promising and reliable segmentation results.",-0.05174063,-0.026701663,0.047224917,C
5708,"Even experiments showed about 10% improvements in segmentation
accuracy compared with single-channel results, further study could be done
on this work to generate uncertainty from probability functions to get achieve
performance.","In this
paper, the author takes the probability functions as mass functions and cal-
culates the orthogonal sum of the probability functions for the three-channel
images.","In general, the BFT-based RGB medical image segmentation approaches
are used to generate mass functions by possibility distributions, e.g., Gaus-
sian distribution and Possibility C-means distribution, and fuse them by

                                               32
Dempster‚Äôs rule.",2022-05-03 19:06:45+00:00,Application of belief functions to medical image segmentation: A review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ling Huang'), arxiv.Result.Author('Su Ruan')]","The investigation of uncertainty is of major importance in risk-critical
applications, such as medical image segmentation. Belief function theory, a
formal framework for uncertainty analysis and multiple evidence fusion, has
made significant contributions in the medical domain, especially since the
development of deep learning. Medical image segmentation with belief function
theory has shown significant benefits in clinical diagnosis and medical image
research. In this paper, we provide a review of medical image segmentation
methods using belief function theory. We classify the methods according to the
fusion step and explain how information with uncertainty or imprecision is
modeled and fused with belief function theory. In addition, we discuss the
challenges and limitations of present belief function-based medical image
segmentation and propose orientations for future research. Future research
could investigate both belief function theory and deep learning to achieve more
promising and reliable segmentation results.",-0.06344347,0.22861809,0.1541703,B
5709,"Some directions
for further research are discussed below.","Future work
    Despite the advantages of BFT for medical image segmentation, existing

methods still have limitations that need to be addressed.","First, most of the existing BFT-based medical image segmentation meth-
ods still use low-level features and do not fully exploit the advantages of deep
learning.",2022-05-03 19:06:45+00:00,Application of belief functions to medical image segmentation: A review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ling Huang'), arxiv.Result.Author('Su Ruan'), arxiv.Result.Author('Thierry Denoeux')]","The investigation of uncertainty is of major importance in risk-critical
applications, such as medical image segmentation. Belief function theory, a
formal framework for uncertainty analysis and multiple evidence fusion, has
made significant contributions to medical image segmentation, especially since
the development of deep learning. In this paper, we provide an introduction to
the topic of medical image segmentation methods using belief function theory.
We classify the methods according to the fusion step and explain how
information with uncertainty or imprecision is modeled and fused with belief
function theory. In addition, we discuss the challenges and limitations of
present belief function-based medical image segmentation and propose
orientations for future research. Future research could investigate both belief
function theory and deep learning to achieve more promising and reliable
segmentation results.",-0.18924683,-0.01925946,0.1711254,C
5710,"Some directions
for further research are discussed below.","Future work
    Despite the advantages of BFT for medical image segmentation, existing

methods still have limitations that need to be addressed.","First, most of the existing BFT-based medical image segmentation meth-
ods still use low-level features and do not fully exploit the advantages of deep
learning.",2022-05-03 19:06:45+00:00,Application of belief functions to medical image segmentation: A review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ling Huang'), arxiv.Result.Author('Su Ruan'), arxiv.Result.Author('Thierry Denoeux')]","The investigation of uncertainty is of major importance in risk-critical
applications, such as medical image segmentation. Belief function theory, a
formal framework for uncertainty analysis and multiple evidence fusion, has
made significant contributions to medical image segmentation, especially since
the development of deep learning. In this paper, we provide an introduction to
the topic of medical image segmentation methods using belief function theory.
We classify the methods according to the fusion step and explain how
information with uncertainty or imprecision is modeled and fused with belief
function theory. In addition, we discuss the challenges and limitations of
present belief function-based medical image segmentation and propose
orientations for future research. Future research could investigate both belief
function theory and deep learning to achieve more promising and reliable
segmentation results.",-0.18924683,-0.01925946,0.1711254,C
5729,"To further study the effectiveness of UCL-
SOTAs, our UCL-Dehaze produces the most natural haze-                Dehaze, we implement extensive ablation studies to analyze
free images with perceptually pleasing and consistent qual-          the effectiveness of its components.",Compared with these          pared to SOTAs.,ity.,2022-05-04 03:25:13+00:00,UCL-Dehaze: Towards Real-world Image Dehazing via Unsupervised Contrastive Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yongzhen Wang'), arxiv.Result.Author('Xuefeng Yan'), arxiv.Result.Author('Fu Lee Wang'), arxiv.Result.Author('Haoran Xie'), arxiv.Result.Author('Wenhan Yang'), arxiv.Result.Author('Mingqiang Wei'), arxiv.Result.Author('Jing Qin')]","While the wisdom of training an image dehazing model on synthetic hazy data
can alleviate the difficulty of collecting real-world hazy/clean image pairs,
it brings the well-known domain shift problem. From a different yet new
perspective, this paper explores contrastive learning with an adversarial
training effort to leverage unpaired real-world hazy and clean images, thus
bridging the gap between synthetic and real-world haze is avoided. We propose
an effective unsupervised contrastive learning paradigm for image dehazing,
dubbed UCL-Dehaze. Unpaired real-world clean and hazy images are easily
captured, and will serve as the important positive and negative samples
respectively when training our UCL-Dehaze network. To train the network more
effectively, we formulate a new self-contrastive perceptual loss function,
which encourages the restored images to approach the positive samples and keep
away from the negative samples in the embedding space. Besides the overall
network architecture of UCL-Dehaze, adversarial training is utilized to align
the distributions between the positive samples and the dehazed images. Compared
with recent image dehazing works, UCL-Dehaze does not require paired data
during training and utilizes unpaired positive/negative data to better enhance
the dehazing performance. We conduct comprehensive experiments to evaluate our
UCL-Dehaze and demonstrate its superiority over the state-of-the-arts, even
only 1,800 unpaired real-world images are used to train our network. Source
code has been available at https://github.com/yz-wang/UCL-Dehaze.",0.041297078,0.17794149,-0.0107490495,C
5733,"Accurate image super-
tate further research on deep learning-based old photo restoration                                resolution using very deep convolutional networks.",2016.,In Proc.,2022-05-04 05:46:43+00:00,Pik-Fix: Restoring and Colorizing Old Photo,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Runsheng Xu'), arxiv.Result.Author('Zhengzhong Tu'), arxiv.Result.Author('Yuanqi Du'), arxiv.Result.Author('Xiaoyu Dong'), arxiv.Result.Author('Jinlong Li'), arxiv.Result.Author('Zibo Meng'), arxiv.Result.Author('Jiaqi Ma'), arxiv.Result.Author('Alan Bovik'), arxiv.Result.Author('Hongkai Yu')]","Restoring and inpainting the visual memories that are present, but often
impaired, in old photos remains an intriguing but unsolved research topic.
Decades-old photos often suffer from severe and commingled degradation such as
cracks, defocus, and color-fading, which are difficult to treat individually
and harder to repair when they interact. Deep learning presents a plausible
avenue, but the lack of large-scale datasets of old photos makes addressing
this restoration task very challenging. Here we present a novel reference-based
end-to-end learning framework that is able to both repair and colorize old and
degraded pictures. Our proposed framework consists of three modules: a
restoration sub-network that conducts restoration from degradations, a
similarity sub-network that performs color histogram matching and color
transfer, and a colorization subnet that learns to predict the chroma elements
of images that have been conditioned on chromatic reference signals. The
overall system makes uses of color histogram priors from reference images,
which greatly reduces the need for large-scale training data. We have also
created a first-of-a-kind public dataset of real old photos that are paired
with ground truth ""pristine"" photos that have been that have been manually
restored by PhotoShop experts. We conducted extensive experiments on this
dataset and synthetic datasets, and found that our method significantly
outperforms previous state-of-the-art models using both qualitative comparisons
and quantitative measurements.",-0.13131222,0.056362018,0.38562882,C
5734,"Accurate image super-
tate further research on deep learning-based old photo restoration                                resolution using very deep convolutional networks.",2016.,In Proc.,2022-05-04 05:46:43+00:00,Pik-Fix: Restoring and Colorizing Old Photos,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Runsheng Xu'), arxiv.Result.Author('Zhengzhong Tu'), arxiv.Result.Author('Yuanqi Du'), arxiv.Result.Author('Xiaoyu Dong'), arxiv.Result.Author('Jinlong Li'), arxiv.Result.Author('Zibo Meng'), arxiv.Result.Author('Jiaqi Ma'), arxiv.Result.Author('Alan Bovik'), arxiv.Result.Author('Hongkai Yu')]","Restoring and inpainting the visual memories that are present, but often
impaired, in old photos remains an intriguing but unsolved research topic.
Decades-old photos often suffer from severe and commingled degradation such as
cracks, defocus, and color-fading, which are difficult to treat individually
and harder to repair when they interact. Deep learning presents a plausible
avenue, but the lack of large-scale datasets of old photos makes addressing
this restoration task very challenging. Here we present a novel reference-based
end-to-end learning framework that is able to both repair and colorize old and
degraded pictures. Our proposed framework consists of three modules: a
restoration sub-network that conducts restoration from degradations, a
similarity sub-network that performs color histogram matching and color
transfer, and a colorization subnet that learns to predict the chroma elements
of images that have been conditioned on chromatic reference signals. The
overall system makes use of color histogram priors from reference images, which
greatly reduces the need for large-scale training data. We have also created a
first-of-a-kind public dataset of real old photos that are paired with ground
truth ""pristine"" photos that have been that have been manually restored by
PhotoShop experts. We conducted extensive experiments on this dataset and
synthetic datasets, and found that our method significantly outperforms
previous state-of-the-art models using both qualitative comparisons and
quantitative measurements.",-0.13131222,0.056362018,0.38562882,C
5735,"Moreover, we created the Ô¨Årst publicly avail-
                                                                 able real-world old photo dataset repaired by Photoshop
                                                                 experts, which we hope will facilitate further research on
                                                                 deep learning-based old photo restoration problems.","Ex-
                                                                 tensive experimental results show that Pik-Fix attains excel-
                                                                 lent performance both visually and numerically on synthetic
                                                                 and real old photo datasets, as compared with state-of-the-
                                                                 art models.","References                                                           [17] Mark Everingham, S. Eslami, Luc Van Gool, Christopher
                                                                           Williams, John Winn, and Andrew Zisserman.",2022-05-04 05:46:43+00:00,Pik-Fix: Restoring and Colorizing Old Photos,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Runsheng Xu'), arxiv.Result.Author('Zhengzhong Tu'), arxiv.Result.Author('Yuanqi Du'), arxiv.Result.Author('Xiaoyu Dong'), arxiv.Result.Author('Jinlong Li'), arxiv.Result.Author('Zibo Meng'), arxiv.Result.Author('Jiaqi Ma'), arxiv.Result.Author('Alan Bovik'), arxiv.Result.Author('Hongkai Yu')]","Restoring and inpainting the visual memories that are present, but often
impaired, in old photos remains an intriguing but unsolved research topic.
Decades-old photos often suffer from severe and commingled degradation such as
cracks, defocus, and color-fading, which are difficult to treat individually
and harder to repair when they interact. Deep learning presents a plausible
avenue, but the lack of large-scale datasets of old photos makes addressing
this restoration task very challenging. Here we present a novel reference-based
end-to-end learning framework that is able to both repair and colorize old,
degraded pictures. Our proposed framework consists of three modules: a
restoration sub-network that conducts restoration from degradations, a
similarity network that performs color histogram matching and color transfer,
and a colorization subnet that learns to predict the chroma elements of images
conditioned on chromatic reference signals. The overall system makes uses of
color histogram priors from reference images, which greatly reduces the need
for large-scale training data. We have also created a first-of-a-kind public
dataset of real old photos that are paired with ground truth ''pristine''
photos that have been manually restored by PhotoShop experts. We conducted
extensive experiments on this dataset and synthetic datasets, and found that
our method significantly outperforms previous state-of-the-art models using
both qualitative comparisons and quantitative measurements. The code is
available at https://github.com/DerrickXuNu/Pik-Fix.",-0.1981476,-0.046474203,0.31967166,C
5738,"Annotations are however crucial to properly train supervised models or
to further study the signiÔ¨Åcance of certain histomorphologies, e.g.","While such approaches can lead to accurate models, obtaining rigorous clinical annotations is time
consuming and requires expertise.",Johannet et al.,2022-05-04 08:06:55+00:00,Self-supervised learning in non-small cell lung cancer discovers novel morphological clusters linked to patient outcome and molecular phenotypes,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Adalberto Claudio Quiros'), arxiv.Result.Author('Nicolas Coudray'), arxiv.Result.Author('Anna Yeaton'), arxiv.Result.Author('Xinyu Yang'), arxiv.Result.Author('Luis Chiriboga'), arxiv.Result.Author('Afreen Karimkhan'), arxiv.Result.Author('Navneet Narula'), arxiv.Result.Author('Harvey Pass'), arxiv.Result.Author('Andre L. Moreira'), arxiv.Result.Author('John Le Quesne'), arxiv.Result.Author('Aristotelis Tsirigos'), arxiv.Result.Author('Ke Yuan')]","Histopathological images provide the definitive source of cancer diagnosis,
containing information used by pathologists to identify and subclassify
malignant disease, and to guide therapeutic choices. These images contain vast
amounts of information, much of which is currently unavailable to human
interpretation. Supervised deep learning approaches have been powerful for
classification tasks, but they are inherently limited by the cost and quality
of annotations. Therefore, we developed Histomorphological Phenotype Learning,
an unsupervised methodology, which requires no annotations and operates via the
self-discovery of discriminatory image features in small image tiles. Tiles are
grouped into morphologically similar clusters which appear to represent
recurrent modes of tumor growth emerging under natural selection. These
clusters have distinct features which can be identified using orthogonal
methods. Applied to lung cancer tissues, we show that they align closely with
patient outcomes, with histopathologically recognised tumor types and growth
patterns, and with transcriptomic measures of immunophenotype.",0.07891833,-0.20164037,-0.28275704,C
5745,"Furthermore, we expect that our results trigger further study beyond the domain or research area.","We
hope this will lead to a better understanding of the role of various inductive biases in computer vision.","Especially, it would be a very interesting open question to see if such a design works with time-series
data in vision such as video or in a multi-modal problem setting combined with another modality
such as video with audio.",2022-05-04 09:47:46+00:00,Sequencer: Deep LSTM for Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuki Tatsunami'), arxiv.Result.Author('Masato Taki')]","In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6\% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.",-0.15037313,0.11119698,-0.08721158,B
5746,"Thus, our study would be
an impetus for further research on its application to various computer vision tasks.","It is exciting to see if
this design beneÔ¨Åts computer vision tasks other than image classiÔ¨Åcation.","On the other side, our architecture may increase the carbon dioxide footprint: the study of new
architectures for vision, such as Sequencer, requires iterative training of models for long periods
to optimize the model‚Äôs design.",2022-05-04 09:47:46+00:00,Sequencer: Deep LSTM for Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuki Tatsunami'), arxiv.Result.Author('Masato Taki')]","In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6\% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.",-0.24135232,0.04090133,0.05153046,B
5747,"Furthermore, we expect that our results trigger further study beyond the domain or research area.","We
hope this will lead to a better understanding of the role of various inductive biases in computer vision.","Especially, it would be a very interesting open question to see if such a design works with time-series
data in vision such as video or in a multi-modal problem setting combined with another modality
such as video with audio.",2022-05-04 09:47:46+00:00,Sequencer: Deep LSTM for Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuki Tatsunami'), arxiv.Result.Author('Masato Taki')]","In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.",-0.15037313,0.11119698,-0.08721158,B
5748,"Thus, our study would be
an impetus for further research on its application to various computer vision tasks.","It is exciting to see if
this design beneÔ¨Åts computer vision tasks other than image classiÔ¨Åcation.","On the other side, our architecture may increase the carbon dioxide footprint: the study of new
architectures for vision, such as Sequencer, requires iterative training of models for long periods
to optimize the model‚Äôs design.",2022-05-04 09:47:46+00:00,Sequencer: Deep LSTM for Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuki Tatsunami'), arxiv.Result.Author('Masato Taki')]","In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.",-0.24135232,0.04090133,0.05153046,B
5749,"Furthermore, we expect that our results
trigger further study beyond the domain or research area.","We hope this will lead to a better understanding
of the role of various inductive biases in computer vision.","Especially, it would be a very interesting
open question to see if such a design works with time-series data in vision such as video or in a
multi-modal problem setting combined with another modality such as video with audio.",2022-05-04 09:47:46+00:00,Sequencer: Deep LSTM for Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuki Tatsunami'), arxiv.Result.Author('Masato Taki')]","In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.",-0.15037313,0.11119698,-0.08721158,B
5750,"Thus, our study would be
an impetus for further research on its application to various computer vision tasks.","It is exciting to see if
this design beneÔ¨Åts computer vision tasks other than image classiÔ¨Åcation.","On the other side, our architecture may increase the carbon dioxide footprint: the study of new
architectures for vision, such as Sequencer, requires iterative training of models for long periods
to optimize the model‚Äôs design.",2022-05-04 09:47:46+00:00,Sequencer: Deep LSTM for Image Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuki Tatsunami'), arxiv.Result.Author('Masato Taki')]","In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.",-0.24135232,0.04090133,0.05153046,B
5765,"However, SeCo outperformed the
with further research on self-supervised learning.","Unlike in the classiÔ¨Åcation task, the segmentation re-
RS based model for RGB images, the difference in results was                         sults for RSDnet-3 and RSDnet-3 RGB-S1S2 outperformed the
not statistically signiÔ¨Åcant and could potentially be reduced                        MS COCO-based models.","weights of RSDnet-3 RGB-S1S2 and MS COCO, yet RSDnet-
                                                                                     3 showed better results than SeCo.",2022-05-04 13:16:48+00:00,Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pallavi Jain'), arxiv.Result.Author('Bianca Schoen-Phelan'), arxiv.Result.Author('Robert Ross')]","Self-Supervised learning (SSL) has become the new state-of-art in several
domain classification and segmentation tasks. Of these, one popular category in
SSL is distillation networks such as BYOL. This work proposes RSDnet, which
applies the distillation network (BYOL) in the remote sensing (RS) domain where
data is non-trivially different from natural RGB images. Since Multi-spectral
(MS) and synthetic aperture radar (SAR) sensors provide varied spectral and
spatial resolution information, we utilised them as an implicit augmentation to
learn invariant feature embeddings. In order to learn RS based invariant
features with SSL, we trained RSDnet in two ways, i.e., single channel feature
learning and three channel feature learning. This work explores the usefulness
of single channel feature learning from random MS and SAR bands compared to the
common notion of using three or more bands. In our linear evaluation, these
single channel features reached a 0.92 F1 score on the EuroSAT classification
task and 59.6 mIoU on the DFC segmentation task for certain single bands. We
also compared our results with ImageNet weights and showed that the RS based
SSL model outperforms the supervised ImageNet based model. We further explored
the usefulness of multi-modal data compared to single modality data, and it is
shown that utilising MS and SAR data learn better invariant representations
than utilising only MS data.",-0.1976904,-0.13029791,0.1025638,C
5766,"This may open the path for further research work in
very precise for the highway class for ImageNet and RSDnet-         satellite data aiming to achieve better generalisation across the
3.","with only 32 batch size unlike other networks trained on 256
Whereas, in contrast it should be noted that the same model is      or more.","While these are illustrative examples, we believe such           domain.",2022-05-04 13:16:48+00:00,Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pallavi Jain'), arxiv.Result.Author('Bianca Schoen-Phelan'), arxiv.Result.Author('Robert Ross')]","Self-Supervised learning (SSL) has become the new state-of-art in several
domain classification and segmentation tasks. Of these, one popular category in
SSL is distillation networks such as BYOL. This work proposes RSDnet, which
applies the distillation network (BYOL) in the remote sensing (RS) domain where
data is non-trivially different from natural RGB images. Since Multi-spectral
(MS) and synthetic aperture radar (SAR) sensors provide varied spectral and
spatial resolution information, we utilised them as an implicit augmentation to
learn invariant feature embeddings. In order to learn RS based invariant
features with SSL, we trained RSDnet in two ways, i.e., single channel feature
learning and three channel feature learning. This work explores the usefulness
of single channel feature learning from random MS and SAR bands compared to the
common notion of using three or more bands. In our linear evaluation, these
single channel features reached a 0.92 F1 score on the EuroSAT classification
task and 59.6 mIoU on the DFC segmentation task for certain single bands. We
also compared our results with ImageNet weights and showed that the RS based
SSL model outperforms the supervised ImageNet based model. We further explored
the usefulness of multi-modal data compared to single modality data, and it is
shown that utilising MS and SAR data learn better invariant representations
than utilising only MS data.",-0.065420054,-0.13411869,0.23711602,C
5767,"In fact, it is notable that while ImageNet based
three datasets, we evaluated RS-BYOL-3‚Äôs on red, green, and                               models outperformed the RS based model for RGB images, the
blue (RGB) bands, and for non-RGB bands, we utilised Red                                  difference in results was not statistically signiÔ¨Åcant and could
Edge bands, i.e., RE5-RE6-RE7 and SAR (VV-VH-VV) bands                                    potentially be reduced with further research on self-supervised
in the Sen12MS dataset.",For all                                  SAR images.,learning.,2022-05-04 13:16:48+00:00,Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pallavi Jain'), arxiv.Result.Author('Bianca Schoen-Phelan'), arxiv.Result.Author('Robert Ross')]","Self-Supervised learning (SSL) has become the new state-of-art in several
domain classification and segmentation tasks. Of these, one popular category in
SSL is distillation networks such as BYOL. This work proposes RSDnet, which
applies the distillation network (BYOL) in the remote sensing (RS) domain where
data is non-trivially different from natural RGB images. Since Multi-spectral
(MS) and synthetic aperture radar (SAR) sensors provide varied spectral and
spatial resolution information, we utilised them as an implicit augmentation to
learn invariant feature embeddings. In order to learn RS based invariant
features with SSL, we trained RSDnet in two ways, i.e., single channel feature
learning and three channel feature learning. This work explores the usefulness
of single channel feature learning from random MS and SAR bands compared to the
common notion of using three or more bands. In our linear evaluation, these
single channel features reached a 0.92 F1 score on the EuroSAT classification
task and 59.6 mIoU on the DFC segmentation task for certain single bands. We
also compared our results with ImageNet weights and showed that the RS based
SSL model outperforms the supervised ImageNet based model. We further explored
the usefulness of multi-modal data compared to single modality data, and it is
shown that utilising MS and SAR data learn better invariant representations
than utilising only MS data.",-0.062171854,-0.004909669,0.11517729,C
5768,"This may open the path for further research work in
                                                                   satellite data, aiming at achieving better generalisation across
   In this work, we applied the distillation network concept to    the domain.",CONCLUSION                           or more.,"build and analyse single channel and three channel features
                                                                                11

   In continuation of our work, we will further explore other                   [15] Y. Wei, X. Luo, L. Hu, Y. Peng, and J. Feng, ‚ÄúAn improved unsupervised
computationally efÔ¨Åcient SSL networks and other RS-based                              representation learning generative adversarial network for remote sens-
pre-trained networks.",2022-05-04 13:16:48+00:00,Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pallavi Jain'), arxiv.Result.Author('Bianca Schoen-Phelan'), arxiv.Result.Author('Robert Ross')]","Self-Supervised learning (SSL) has become the new state-of-art in several
domain classification and segmentation tasks. Of these, one popular category in
SSL is distillation networks such as BYOL. This work proposes RSDnet, which
applies the distillation network (BYOL) in the remote sensing (RS) domain where
data is non-trivially different from natural RGB images. Since Multi-spectral
(MS) and synthetic aperture radar (SAR) sensors provide varied spectral and
spatial resolution information, we utilised them as an implicit augmentation to
learn invariant feature embeddings. In order to learn RS based invariant
features with SSL, we trained RSDnet in two ways, i.e., single channel feature
learning and three channel feature learning. This work explores the usefulness
of single channel feature learning from random MS and SAR bands compared to the
common notion of using three or more bands. In our linear evaluation, these
single channel features reached a 0.92 F1 score on the EuroSAT classification
task and 59.6 mIoU on the DFC segmentation task for certain single bands. We
also compared our results with ImageNet weights and showed that the RS based
SSL model outperforms the supervised ImageNet based model. We further explored
the usefulness of multi-modal data compared to single modality data, and it is
shown that utilising MS and SAR data learn better invariant representations
than utilising only MS data.",0.004414346,-0.26482692,0.16343468,C
5773,"This demonstrates the need for further research on the topic to
be able to deal with real inpainting scenarios.","This is probably because the models
have diÔ¨Éculties learning the underlying multimodal distribution of these complex
An Analysis of Generative Methods for Multiple Image Inpainting  35

and diverse datasets.",Computational Time.,2022-05-04 15:54:08+00:00,An Analysis of Generative Methods for Multiple Image Inpainting,cs.CV,"['cs.CV', 'I.4.4; I.4.10']","[arxiv.Result.Author('Coloma Ballester'), arxiv.Result.Author('Aurelie Bugeau'), arxiv.Result.Author('Samuel Hurault'), arxiv.Result.Author('Simone Parisotto'), arxiv.Result.Author('Patricia Vitoria')]","Image inpainting refers to the restoration of an image with missing regions
in a way that is not detectable by the observer. The inpainting regions can be
of any size and shape. This is an ill-posed inverse problem that does not have
a unique solution. In this work, we focus on learning-based image completion
methods for multiple and diverse inpainting which goal is to provide a set of
distinct solutions for a given damaged image. These methods capitalize on the
probabilistic nature of certain generative models to sample various solutions
that coherently restore the missing content. Along the chapter, we will analyze
the underlying theory and analyze the recent proposals for multiple inpainting.
To investigate the pros and cons of each method, we present quantitative and
qualitative comparisons, on common datasets, regarding both the quality and the
diversity of the set of inpainted solutions. Our analysis allows us to identify
the most successful generative strategies in both inpainting quality and
inpainting diversity. This task is closely related to the learning of an
accurate probability distribution of images. Depending on the dataset in use,
the challenges that entail the training of such a model will be discussed
through the analysis.",-0.14076933,0.0029958896,0.1259805,C
5820,"We further study and adopt stronger data augmenta-
                                                             tions for training and multi-view ensemble methods in
    Anchor-based Method.",plementation details.,Anchor-based methods                the testing phase.,2022-05-05 15:42:56+00:00,BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Min Yang'), arxiv.Result.Author('Guo Chen'), arxiv.Result.Author('Yin-Dong Zheng'), arxiv.Result.Author('Tong Lu'), arxiv.Result.Author('Limin Wang')]","Temporal action detection (TAD) is extensively studied in the video
understanding community by generally following the object detection pipeline in
images. However, complex designs are not uncommon in TAD, such as two-stream
feature extraction, multi-stage training, complex temporal modeling, and global
context fusion. In this paper, we do not aim to introduce any novel technique
for TAD. Instead, we study a simple, straightforward, yet must-known baseline
given the current status of complex design and low detection efficiency in TAD.
In our simple baseline (termed BasicTAD), we decompose the TAD pipeline into
several essential components: data sampling, backbone design, neck
construction, and detection head. We extensively investigate the existing
techniques in each component for this baseline, and more importantly, perform
end-to-end training over the entire pipeline thanks to the simplicity of
design. As a result, this simple BasicTAD yields an astounding and real-time
RGB-Only baseline very close to the state-of-the-art methods with two-stream
inputs. In addition, we further improve the BasicTAD by preserving more
temporal and spatial information in network representation (termed as PlusTAD).
Empirical results demonstrate that our PlusTAD is very efficient and
significantly outperforms the previous methods on the datasets of THUMOS14 and
FineAction. Meanwhile, we also perform in-depth visualization and error
analysis on our proposed method and try to provide more insights on the TAD
problem. Our approach can serve as a strong baseline for future TAD research.
The code and model will be released at https://github.com/MCG-NJU/BasicTAD.",0.11972597,-0.04165136,-0.07448826,A
5821,"Inserting downsampling layers at diÔ¨Äer-
                                                           ent locations can lead to diÔ¨Äerences in model perfor-
Following the previous section, we further study down-     mance due to diÔ¨Äerent temporal receptive Ô¨Åelds and
sampling locations in the SlowOnly backbone.","downsample the spatiotemporal features before res-2,
                                                           res-3, res-4, and res-5 (res-x denotes the x-th res-stage
4.3.2 Study on Downsampling Locations in backbone          in SlowOnly).",To con-       aggregations.,2022-05-05 15:42:56+00:00,BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Min Yang'), arxiv.Result.Author('Guo Chen'), arxiv.Result.Author('Yin-Dong Zheng'), arxiv.Result.Author('Tong Lu'), arxiv.Result.Author('Limin Wang')]","Temporal action detection (TAD) is extensively studied in the video
understanding community by generally following the object detection pipeline in
images. However, complex designs are not uncommon in TAD, such as two-stream
feature extraction, multi-stage training, complex temporal modeling, and global
context fusion. In this paper, we do not aim to introduce any novel technique
for TAD. Instead, we study a simple, straightforward, yet must-known baseline
given the current status of complex design and low detection efficiency in TAD.
In our simple baseline (termed BasicTAD), we decompose the TAD pipeline into
several essential components: data sampling, backbone design, neck
construction, and detection head. We extensively investigate the existing
techniques in each component for this baseline, and more importantly, perform
end-to-end training over the entire pipeline thanks to the simplicity of
design. As a result, this simple BasicTAD yields an astounding and real-time
RGB-Only baseline very close to the state-of-the-art methods with two-stream
inputs. In addition, we further improve the BasicTAD by preserving more
temporal and spatial information in network representation (termed as PlusTAD).
Empirical results demonstrate that our PlusTAD is very efficient and
significantly outperforms the previous methods on the datasets of THUMOS14 and
FineAction. Meanwhile, we also perform in-depth visualization and error
analysis on our proposed method and try to provide more insights on the TAD
problem. Our approach can serve as a strong baseline for future TAD research.
The code and model will be released at https://github.com/MCG-NJU/BasicTAD.",-0.00051357225,0.080939196,0.13014975,B
5838,"We hope these results will accelerate
further research works in this direction.","Our multimodal fusion approach achieves 0.5698
micro f1-score on the test data, which is a considerable result.","References

S. Abd Kadir, A. Lokman, and T. Tsuchiya.",2022-05-03 18:33:27+00:00,Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","[arxiv.Result.Author('Sunil Gundapu'), arxiv.Result.Author('Radhika Mamidi')]","The exponential rise of social media networks has allowed the production,
distribution, and consumption of data at a phenomenal rate. Moreover, the
social media revolution has brought a unique phenomenon to social media
platforms called Internet memes. Internet memes are one of the most popular
contents used on social media, and they can be in the form of images with a
witty, catchy, or satirical text description. In this paper, we are dealing
with propaganda that is often seen in Internet memes in recent times.
Propaganda is communication, which frequently includes psychological and
rhetorical techniques to manipulate or influence an audience to act or respond
as the propagandist wants. To detect propaganda in Internet memes, we propose a
multimodal deep learning fusion system that fuses the text and image feature
representations and outperforms individual models based solely on either text
or image modalities.",0.19952428,0.11776196,-0.016993504,A
5852,"This work paves the way for RSVQA by providing              CGA utilizes language features as the guidance to generate
datasets and a baseline method for further research.",task.,"However,     global attention maps on the whole image.",2022-05-06 11:37:00+00:00,From Easy to Hard: Learning Language-guided Curriculum for Visual Question Answering on Remote Sensing Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenghang Yuan'), arxiv.Result.Author('Lichao Mou'), arxiv.Result.Author('Qi Wang'), arxiv.Result.Author('Xiao Xiang Zhu')]","Visual question answering (VQA) for remote sensing scene has great potential
in intelligent human-computer interaction system. Although VQA in computer
vision has been widely researched, VQA for remote sensing data (RSVQA) is still
in its infancy. There are two characteristics that need to be specially
considered for the RSVQA task. 1) No object annotations are available in RSVQA
datasets, which makes it difficult for models to exploit informative region
representation; 2) There are questions with clearly different difficulty levels
for each image in the RSVQA task. Directly training a model with questions in a
random order may confuse the model and limit the performance. To address these
two problems, in this paper, a multi-level visual feature learning method is
proposed to jointly extract language-guided holistic and regional image
features. Besides, a self-paced curriculum learning (SPCL)-based VQA model is
developed to train networks with samples in an easy-to-hard way. To be more
specific, a language-guided SPCL method with a soft weighting strategy is
explored in this work. The proposed model is evaluated on three public
datasets, and extensive experimental results show that the proposed RSVQA
framework can achieve promising performance.",-0.1477566,-0.08047272,-0.12557086,C
5853,"Image features           To further study the effect of different numbers of training
and question representations are Ô¨Årst learned by the repre-      samples on model performance, we have conducted exper-
sentation module.","MAIN consists of two modules: a
representation module and a fusion module.","Then, mutual attention and bilinear fusion     iments by training the proposed SPCL+MLL model with
are utilized to fuse image and question representations in an    different proportions of the training data.",2022-05-06 11:37:00+00:00,From Easy to Hard: Learning Language-guided Curriculum for Visual Question Answering on Remote Sensing Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenghang Yuan'), arxiv.Result.Author('Lichao Mou'), arxiv.Result.Author('Qi Wang'), arxiv.Result.Author('Xiao Xiang Zhu')]","Visual question answering (VQA) for remote sensing scene has great potential
in intelligent human-computer interaction system. Although VQA in computer
vision has been widely researched, VQA for remote sensing data (RSVQA) is still
in its infancy. There are two characteristics that need to be specially
considered for the RSVQA task. 1) No object annotations are available in RSVQA
datasets, which makes it difficult for models to exploit informative region
representation; 2) There are questions with clearly different difficulty levels
for each image in the RSVQA task. Directly training a model with questions in a
random order may confuse the model and limit the performance. To address these
two problems, in this paper, a multi-level visual feature learning method is
proposed to jointly extract language-guided holistic and regional image
features. Besides, a self-paced curriculum learning (SPCL)-based VQA model is
developed to train networks with samples in an easy-to-hard way. To be more
specific, a language-guided SPCL method with a soft weighting strategy is
explored in this work. The proposed model is evaluated on three public
datasets, and extensive experimental results show that the proposed RSVQA
framework can achieve promising performance.",-0.03730376,-0.2664528,-0.06641026,C
5857,"We                     Main contributions of this work are:
                                       publicly release our occupancy grid dataset based on nuScenes
                                       to support further research.","temporal networks to understand scene dynamics without the
                                       need for HD-Maps and explicit modeling dynamic objects.","‚Ä¢ A Spatio-Temporal Network Pipeline for long-term
                                                                                                                         future occupancy grid prediction.",2022-05-06 13:45:32+00:00,Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Khushdeep Singh Mann'), arxiv.Result.Author('Abhishek Tomy'), arxiv.Result.Author('Anshul Paigwar'), arxiv.Result.Author('Alessandro Renzaglia'), arxiv.Result.Author('Christian Laugier')]","Reliably predicting future occupancy of highly dynamic urban environments is
an important precursor for safe autonomous navigation. Common challenges in the
prediction include forecasting the relative position of other vehicles,
modelling the dynamics of vehicles subjected to different traffic conditions,
and vanishing surrounding objects. To tackle these challenges, we propose a
spatio-temporal prediction network pipeline that takes the past information
from the environment and semantic labels separately for generating future
occupancy predictions. Compared to the current SOTA, our approach predicts
occupancy for a longer horizon of 3 seconds and in a relatively complex
environment from the nuScenes dataset. Our experimental results demonstrate the
ability of spatio-temporal networks to understand scene dynamics without the
need for HD-Maps and explicit modeling dynamic objects. We publicly release our
occupancy grid dataset based on nuScenes to support further research.",-0.25597367,0.14071502,-0.006827567,B
5882,"The Ô¨Årst three kinds of information are
                                                                   analyzed in further research.",quality of this image.,"In the UIUD database, only the
    Distortion Types      Number Numbering                         subjective scores are used.",2022-05-07 06:52:36+00:00,Utility-Oriented Underwater Image Quality Assessment Based on Transfer Learning,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Weiling Chen'), arxiv.Result.Author('Rongfu Lin'), arxiv.Result.Author('Honggang Liao'), arxiv.Result.Author('Tiesong Zhao'), arxiv.Result.Author('Ke Gu'), arxiv.Result.Author('Patrick Le Callet')]","The widespread image applications have greatly promoted the vision-based
tasks, in which the Image Quality Assessment (IQA) technique has become an
increasingly significant issue. For user enjoyment in multimedia systems, the
IQA exploits image fidelity and aesthetics to characterize user experience;
while for other tasks such as popular object recognition, there exists a low
correlation between utilities and perceptions. In such cases, the
fidelity-based and aesthetics-based IQA methods cannot be directly applied. To
address this issue, this paper proposes a utility-oriented IQA in object
recognition. In particular, we initialize our research in the scenario of
underwater fish detection, which is a critical task that has not yet been
perfectly addressed. Based on this task, we build an Underwater Image Utility
Database (UIUD) and a learning-based Underwater Image Utility Measure (UIUM).
Inspired by the top-down design of fidelity-based IQA, we exploit the deep
models of object recognition and transfer their features to our UIUM.
Experiments validate that the proposed transfer-learning-based UIUM achieves
promising performance in the recognition task. We envision our research
provides insights to bridge the researches of IQA and computer vision.",0.252946,0.06617002,-0.24019629,A
5895,"Overall, it is a new excellent
  STMTrack [Fu et al., 2021]      0.647          0.719              baseline for further researches.","Besides, the training time of our
  DiMP-50 [Bhat et al., 2019]     0.654          0.684              method is only 25% of TransT.","Table 7: The performance of our method and other excellent ones     References
on UAV123 and OTB2015.",2022-05-08 04:00:28+00:00,SparseTT: Visual Tracking with Sparse Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhihong Fu'), arxiv.Result.Author('Zehua Fu'), arxiv.Result.Author('Qingjie Liu'), arxiv.Result.Author('Wenrui Cai'), arxiv.Result.Author('Yunhong Wang')]","Transformers have been successfully applied to the visual tracking task and
significantly promote tracking performance. The self-attention mechanism
designed to model long-range dependencies is the key to the success of
Transformers. However, self-attention lacks focusing on the most relevant
information in the search regions, making it easy to be distracted by
background. In this paper, we relieve this issue with a sparse attention
mechanism by focusing the most relevant information in the search regions,
which enables a much accurate tracking. Furthermore, we introduce a double-head
predictor to boost the accuracy of foreground-background classification and
regression of target bounding boxes, which further improve the tracking
performance. Extensive experiments show that, without bells and whistles, our
method significantly outperforms the state-of-the-art approaches on LaSOT,
GOT-10k, TrackingNet, and UAV123, while running at 40 FPS. Notably, the
training time of our method is reduced by 75% compared to that of TransT. The
source code and models are available at https://github.com/fzh0917/SparseTT.",0.28598058,0.08756593,0.13877693,A
5938,"We hope that our work will
                                                                      stimulate further research in this direction and elucidate
                                                                      the underlying reasons why speciÔ¨Åc model components are
                                                                      better suited for DP training, which may lead to Ô¨Åndings of
                                                                      independent theoretical interest.","In conclusion, our work
                                                                      is an empirical investigation into the factors enabling neu-
                                                                      ral network training on sensitive datasets with high utility
                                                                      and strong privacy guarantees.",Figure 2.,2022-05-09 07:51:54+00:00,SmoothNets: Optimizing CNN architecture design for differentially private deep learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Nicolas W. Remerscheid'), arxiv.Result.Author('Alexander Ziller'), arxiv.Result.Author('Daniel Rueckert'), arxiv.Result.Author('Georgios Kaissis')]","The arguably most widely employed algorithm to train deep neural networks
with Differential Privacy is DPSGD, which requires clipping and noising of
per-sample gradients. This introduces a reduction in model utility compared to
non-private training. Empirically, it can be observed that this accuracy
degradation is strongly dependent on the model architecture. We investigated
this phenomenon and, by combining components which exhibit good individual
performance, distilled a new model architecture termed SmoothNet, which is
characterised by increased robustness to the challenges of DP-SGD training.
Experimentally, we benchmark SmoothNet against standard architectures on two
benchmark datasets and observe that our architecture outperforms others,
reaching an accuracy of 73.5\% on CIFAR-10 at $\varepsilon=7.0$ and 69.2\% at
$\varepsilon=7.0$ on ImageNette, a state-of-the-art result compared to prior
architectural modifications for DP.",0.19046226,-0.27118224,0.07785459,A
5954,"speed and performance against UAV tracking chal-
                                                                              lenges, the feasible solutions to deal with adverse
    To enlighten further researches on Siamese UAV track-                     environments.","Hence, accurately dis-                directions of tracker design and optimization are
         tinguishing the foreground and background in low-                    clariÔ¨Åed, e.g., the trade-off between the processing
         illumination settings is critical to the trackers.","ing, in this work, the fundamental idea of Siamese networks
is expounded Ô¨Årst.",2022-05-09 13:53:34+00:00,Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Changhong Fu'), arxiv.Result.Author('Kunhan Lu'), arxiv.Result.Author('Guangze Zheng'), arxiv.Result.Author('Junjie Ye'), arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Bowen Li')]","Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide
range of applications and attracted increasing attention in the field of remote
sensing because of its versatility and effectiveness. As a new force in the
revolutionary trend of deep learning, Siamese networks shine in visual object
tracking with their promising balance of accuracy, robustness, and speed.
Thanks to the development of embedded processors and the gradual optimization
of deep neural networks, Siamese trackers receive extensive research and
realize preliminary combinations with UAVs. However, due to the UAV's limited
onboard computational resources and the complex real-world circumstances,
aerial tracking with Siamese networks still faces severe obstacles in many
aspects. To further explore the deployment of Siamese networks in UAV tracking,
this work presents a comprehensive review of leading-edge Siamese trackers,
along with an exhaustive UAV-specific analysis based on the evaluation using a
typical UAV onboard processor. Then, the onboard tests are conducted to
validate the feasibility and efficacy of representative Siamese trackers in
real-world UAV deployment. Furthermore, to better promote the development of
the tracking community, this work analyzes the limitations of existing Siamese
trackers and conducts additional experiments represented by low-illumination
evaluations. In the end, prospects for the development of Siamese UAV tracking
in the remote sensing field are discussed. The unified framework of
leading-edge Siamese trackers, i.e., code library, and the results of their
experimental evaluations are available at
https://github.com/vision4robotics/SiameseTracking4UAV .",-0.09426713,0.21316704,-0.055487618,B
5955,"Furthermore,
this work further explores the performance of SOTA track-               [10] S. M. Marvasti-Zadeh, L. Cheng, H. Ghanei-Yakhdan, and
ers in low-illumination UAV tracking conditions to arose                        S. Kasaei, ‚ÄúDeep Learning for Visual Tracking: A Comprehensive
further research.","1‚Äì13,
Siamese trackers and UAV-based tracking confront accord-                        2021.
ing to the current difÔ¨Åculties and challenges.","Finally, this work presents a perspective,                    Survey,‚Äù IEEE Transactions on Intelligent Transportation Systems,
discussing the direction of future development based on the                     vol.",2022-05-09 13:53:34+00:00,Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Changhong Fu'), arxiv.Result.Author('Kunhan Lu'), arxiv.Result.Author('Guangze Zheng'), arxiv.Result.Author('Junjie Ye'), arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Bowen Li')]","Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide
range of applications and attracted increasing attention in the field of remote
sensing because of its versatility and effectiveness. As a new force in the
revolutionary trend of deep learning, Siamese networks shine in visual object
tracking with their promising balance of accuracy, robustness, and speed.
Thanks to the development of embedded processors and the gradual optimization
of deep neural networks, Siamese trackers receive extensive research and
realize preliminary combinations with UAVs. However, due to the UAV's limited
onboard computational resources and the complex real-world circumstances,
aerial tracking with Siamese networks still faces severe obstacles in many
aspects. To further explore the deployment of Siamese networks in UAV tracking,
this work presents a comprehensive review of leading-edge Siamese trackers,
along with an exhaustive UAV-specific analysis based on the evaluation using a
typical UAV onboard processor. Then, the onboard tests are conducted to
validate the feasibility and efficacy of representative Siamese trackers in
real-world UAV deployment. Furthermore, to better promote the development of
the tracking community, this work analyzes the limitations of existing Siamese
trackers and conducts additional experiments represented by low-illumination
evaluations. In the end, prospects for the development of Siamese UAV tracking
in the remote sensing field are discussed. The unified framework of
leading-edge Siamese trackers, i.e., code library, and the results of their
experimental evaluations are available at
https://github.com/vision4robotics/SiameseTracking4UAV .",-0.25827345,0.04251654,-0.03303244,B
5956,"To enlighten further researches on Siamese UAV track-
ing, in this work, the fundamental idea of Siamese networks         Remark 1: To the best of our knowledge, this work is the
is expounded Ô¨Årst.","lenges, the feasible solutions to deal with adverse
                                                                              environments.","Besides, the core framework of represen-         Ô¨Årst to extensively compare the performance of Siamese
tative Siamese trackers are presented and summarized in             trackers on the typical UAV onboard processor, aiming to
detail according to their respective innovations and advan-         achieve comprehensive Siamese object tracking analysis for
tages.",2022-05-09 13:53:34+00:00,Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Changhong Fu'), arxiv.Result.Author('Kunhan Lu'), arxiv.Result.Author('Guangze Zheng'), arxiv.Result.Author('Junjie Ye'), arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Bowen Li'), arxiv.Result.Author('Geng Lu')]","Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide
range of applications and attracted increasing attention in the field of
intelligent transportation systems because of its versatility and
effectiveness. As an emerging force in the revolutionary trend of deep
learning, Siamese networks shine in UAV-based object tracking with their
promising balance of accuracy, robustness, and speed. Thanks to the development
of embedded processors and the gradual optimization of deep neural networks,
Siamese trackers receive extensive research and realize preliminary
combinations with UAVs. However, due to the UAV's limited onboard computational
resources and the complex real-world circumstances, aerial tracking with
Siamese networks still faces severe obstacles in many aspects. To further
explore the deployment of Siamese networks in UAV-based tracking, this work
presents a comprehensive review of leading-edge Siamese trackers, along with an
exhaustive UAV-specific analysis based on the evaluation using a typical UAV
onboard processor. Then, the onboard tests are conducted to validate the
feasibility and efficacy of representative Siamese trackers in real-world UAV
deployment. Furthermore, to better promote the development of the tracking
community, this work analyzes the limitations of existing Siamese trackers and
conducts additional experiments represented by low-illumination evaluations. In
the end, prospects for the development of Siamese tracking for UAV-based
intelligent transportation systems are deeply discussed. The unified framework
of leading-edge Siamese trackers, i.e., code library, and the results of their
experimental evaluations are available at
https://github.com/vision4robotics/SiameseTracking4UAV .",-0.13350815,0.090988174,-0.15459985,B
5957,tracking conditions to arose further research.,3086‚Äì3092.,"Finally, this
work presents a perspective, discussing the direction of          [10] Z. Cao, Z. Huang, L. Pan, S. Zhang, Z. Liu, and C. Fu, ‚ÄúTCTrack:
future development based on the existing challenges of UAV                Temporal Contexts for Aerial Tracking,‚Äù in Proceedings of the
tracking and Siamese networks.",2022-05-09 13:53:34+00:00,Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Changhong Fu'), arxiv.Result.Author('Kunhan Lu'), arxiv.Result.Author('Guangze Zheng'), arxiv.Result.Author('Junjie Ye'), arxiv.Result.Author('Ziang Cao'), arxiv.Result.Author('Bowen Li'), arxiv.Result.Author('Geng Lu')]","Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide
range of applications and attracted increasing attention in the field of
intelligent transportation systems because of its versatility and
effectiveness. As an emerging force in the revolutionary trend of deep
learning, Siamese networks shine in UAV-based object tracking with their
promising balance of accuracy, robustness, and speed. Thanks to the development
of embedded processors and the gradual optimization of deep neural networks,
Siamese trackers receive extensive research and realize preliminary
combinations with UAVs. However, due to the UAV's limited onboard computational
resources and the complex real-world circumstances, aerial tracking with
Siamese networks still faces severe obstacles in many aspects. To further
explore the deployment of Siamese networks in UAV-based tracking, this work
presents a comprehensive review of leading-edge Siamese trackers, along with an
exhaustive UAV-specific analysis based on the evaluation using a typical UAV
onboard processor. Then, the onboard tests are conducted to validate the
feasibility and efficacy of representative Siamese trackers in real-world UAV
deployment. Furthermore, to better promote the development of the tracking
community, this work analyzes the limitations of existing Siamese trackers and
conducts additional experiments represented by low-illumination evaluations. In
the end, prospects for the development of Siamese tracking for UAV-based
intelligent transportation systems are deeply discussed. The unified framework
of leading-edge Siamese trackers, i.e., code library, and the results of their
experimental evaluations are available at
https://github.com/vision4robotics/SiameseTracking4UAV .",-0.10053453,0.14856407,-0.0936096,B
6016,"Although there is room for improvement, this level of accuracy is still suÔ¨Écient to identify potential areas for
further research and stewardship.","Using this threshold, more than half of true shrubland pixels
were correctly classiÔ¨Åed and 22% of positive predictions reÔ¨Çect true shrubland (as deÔ¨Åned in the LiDAR
analysis), approximately 10 times better than simply guessing using the shrubland occurrence rate of 2.5%.","This work should be of interest to both those producing LULC models and products and those interested
in the research and management of shrubland at a landscape level.",2022-05-09 14:54:41+00:00,Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast,cs.CV,"['cs.CV', 'cs.LG', 'stat.AP']","[arxiv.Result.Author('Michael J Mahoney'), arxiv.Result.Author('Lucas K Johnson'), arxiv.Result.Author('Colin M Beier')]","Context: Novel plant communities reshape landscapes and pose challenges for
land cover classification and mapping that can constrain research and
stewardship efforts. In the US Northeast, emergence of low-statured woody
vegetation, or 'shrublands', instead of secondary forests in post-agricultural
landscapes is well-documented by field studies, but poorly understood from a
landscape perspective, which limits the ability to systematically study and
manage these lands. Objectives: To address gaps in classification/mapping of
low-statured cover types where they have been historically rare, we developed
models to predict 'shrubland' distributions at 30m resolution across New York
State (NYS), using machine learning and model ensembling techniques to
integrate remote sensing of structural (airborne LIDAR) and optical (satellite
imagery) properties of vegetation cover. We first classified a 1m canopy height
model (CHM), derived from a ""patchwork"" of available LIDAR coverages, to define
shrubland presence/absence. Next, these non-contiguous maps were used to train
a model ensemble based on temporally-segmented imagery to predict 'shrubland'
probability for the entire study landscape (NYS). Results: Approximately 2.5%
of the CHM coverage area was classified as shrubland. Models using Landsat
predictors trained on the classified CHM were effective at identifying
shrubland (test set AUC=0.893, real-world AUC=0.904), in discriminating between
shrub/young forest and other cover classes, and produced qualitatively sensible
maps, even when extending beyond the original training data. Conclusions: After
ground-truthing, we expect these shrubland maps and models will have many
research and stewardship applications including wildlife conservation, invasive
species mitigation and natural climate solutions.",0.056135476,0.21601449,0.015610475,B
6018,"Our work makes the first attempt towards interactive modulation in RWSR, and
there is still a long way with challenges, which deserves further research.","The challenge is that real samples usually do not contain a pair of images with the same content but
with different degradations.","References

 1.",2022-05-10 17:46:59+00:00,Metric Learning based Interactive Modulation for Real-World Super-Resolution,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Chong Mou'), arxiv.Result.Author('Yanze Wu'), arxiv.Result.Author('Xintao Wang'), arxiv.Result.Author('Chao Dong'), arxiv.Result.Author('Jian Zhang'), arxiv.Result.Author('Ying Shan')]","Interactive image restoration aims to restore images by adjusting several
controlling coefficients, which determine the restoration strength. Existing
methods are restricted in learning the controllable functions under the
supervision of known degradation types and levels. They usually suffer from a
severe performance drop when the real degradation is different from their
assumptions. Such a limitation is due to the complexity of real-world
degradations, which can not provide explicit supervision to the interactive
modulation during training. However, how to realize the interactive modulation
in real-world super-resolution has not yet been studied. In this work, we
present a Metric Learning based Interactive Modulation for Real-World
Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised
degradation estimation strategy to estimate the degradation level in real-world
scenarios. Instead of using known degradation levels as explicit supervision to
the interactive mechanism, we propose a metric learning strategy to map the
unquantifiable degradation levels in real-world scenarios to a metric space,
which is trained in an unsupervised manner. Moreover, we introduce an anchor
point strategy in the metric learning process to normalize the distribution of
metric space. Extensive experiments demonstrate that the proposed MM-RealSR
achieves excellent modulation and restoration performance in real-world
super-resolution. Codes are available at
https://github.com/TencentARC/MM-RealSR.",0.08272375,0.15085253,0.1842938,A
6021,"It also offers a taxonomy allowing to further study

    8We use the convention P(i) = [P(i, j)]|jC=t1est|.",cations.,"Also, occ represents
the whole vector, while occ(i) represents the element at index i.",2022-05-10 20:25:43+00:00,Few-Shot Image Classification Benchmarks are Too Far From Reality: Build Back Better with Semantic Task Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Etienne Bennequin'), arxiv.Result.Author('Myriam Tami'), arxiv.Result.Author('Antoine Toubhans'), arxiv.Result.Author('Celine Hudelot')]","Every day, a new method is published to tackle Few-Shot Image Classification,
showing better and better performances on academic benchmarks. Nevertheless, we
observe that these current benchmarks do not accurately represent the real
industrial use cases that we encountered. In this work, through both
qualitative and quantitative studies, we expose that the widely used benchmark
tieredImageNet is strongly biased towards tasks composed of very semantically
dissimilar classes e.g. bathtub, cabbage, pizza, schipperke, and cardoon. This
makes tieredImageNet (and similar benchmarks) irrelevant to evaluate the
ability of a model to solve real-life use cases usually involving more
fine-grained classification. We mitigate this bias using semantic information
about the classes of tieredImageNet and generate an improved, balanced
benchmark. Going further, we also introduce a new benchmark for Few-Shot Image
Classification using the Danish Fungi 2020 dataset. This benchmark proposes a
wide variety of evaluation tasks with various fine-graininess. Moreover, this
benchmark includes many-way tasks (e.g. composed of 100 classes), which is a
challenging setting yet very common in industrial applications. Our experiments
bring out the correlation between the difficulty of a task and the semantic
similarity between its classes, as well as a heavy performance drop of
state-of-the-art methods on many-way few-shot classification, raising questions
about the scaling abilities of these methods. We hope that our work will
encourage the community to further question the quality of standard evaluation
processes and their relevance to real-life applications.",0.32862774,0.042252343,-0.05281996,A
6040,"To improve the accuracy of the segmentation masks generated there
are many methods could be applied to this workflow that may warrant possible further study in this
space.","This
work shows that it is possible to develop a deep learning based segmentation process for corrosion,
using no per-pixel training data.","Improved Diverse Data Set

A key limiting factor identified for this method was the training dataset used.",2022-05-11 11:48:02+00:00,RustSEG -- Automated segmentation of corrosion using deep learning,cs.CV,"['cs.CV', 'cond-mat.mtrl-sci']","[arxiv.Result.Author('B. Burton'), arxiv.Result.Author('W. T. Nash'), arxiv.Result.Author('N. Birbilis')]","The inspection of infrastructure for corrosion remains a task that is
typically performed manually by qualified engineers or inspectors. This task of
inspection is laborious, slow, and often requires complex access. Recently,
deep learning based algorithms have revealed promise and performance in the
automatic detection of corrosion. However, to date, research regarding the
segmentation of images for automated corrosion detection has been limited, due
to the lack of availability of per-pixel labelled data sets which are required
for model training. Herein, a novel deep learning approach (termed RustSEG) is
presented, that can accurately segment images for automated corrosion
detection, without the requirement of per-pixel labelled data sets for
training. The RustSEG method will first, using deep learning techniques,
determine if corrosion is present in an image (i.e. a classification task), and
then if corrosion is present, the model will examine what pixels in the
original image contributed to that classification decision. Finally, the method
can refine its predictions into a pixel-level segmentation mask. In ideal
cases, the method is able to generate precise masks of corrosion in images,
demonstrating that the automated segmentation of corrosion without per-pixel
training data is possible, addressing a significant hurdle in automated
infrastructure inspection.",-0.21689086,-0.09396026,0.18592507,C
6063,"Since a threshold T = 0.02 is set, the
difference between the target overall pruning ratio and the             To further study the status of channel pruning, we incor-
actual overall pruning ratio is within 2%.","Benchmarking channel pruning methods
ric is relatively fixed.",During the ran-           porate the results of more methods in Table 3.,2022-05-11 17:59:04+00:00,Revisiting Random Channel Pruning for Neural Network Compression,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yawei Li'), arxiv.Result.Author('Kamil Adamczewski'), arxiv.Result.Author('Wen Li'), arxiv.Result.Author('Shuhang Gu'), arxiv.Result.Author('Radu Timofte'), arxiv.Result.Author('Luc Van Gool')]","Channel (or 3D filter) pruning serves as an effective way to accelerate the
inference of neural networks. There has been a flurry of algorithms that try to
solve this practical problem, each being claimed effective in some ways. Yet, a
benchmark to compare those algorithms directly is lacking, mainly due to the
complexity of the algorithms and some custom settings such as the particular
network configuration or training procedure. A fair benchmark is important for
the further development of channel pruning.
  Meanwhile, recent investigations reveal that the channel configurations
discovered by pruning algorithms are at least as important as the pre-trained
weights. This gives channel pruning a new role, namely searching the optimal
channel configuration. In this paper, we try to determine the channel
configuration of the pruned models by random search. The proposed approach
provides a new way to compare different methods, namely how well they behave
compared with random pruning. We show that this simple strategy works quite
well compared with other channel pruning methods. We also show that under this
setting, there are surprisingly no clear winners among different channel
importance evaluation methods, which then may tilt the research efforts into
advanced channel configuration searching methods.",0.4010926,0.039326485,0.15422323,A
6113,"We hope that our model will
serve as a strong starting point for further research on open-world detection.","In
our analyses we disentangle the determinants of successful transfer of image-level
representations to detection, and show that pre-training simple, scalable archi-
tectures on more data leads to strong zero-shot detection performance, mirroring
previous observations for image classification tasks.","References

 1.",2022-05-12 17:20:36+00:00,Simple Open-Vocabulary Object Detection with Vision Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Matthias Minderer'), arxiv.Result.Author('Alexey Gritsenko'), arxiv.Result.Author('Austin Stone'), arxiv.Result.Author('Maxim Neumann'), arxiv.Result.Author('Dirk Weissenborn'), arxiv.Result.Author('Alexey Dosovitskiy'), arxiv.Result.Author('Aravindh Mahendran'), arxiv.Result.Author('Anurag Arnab'), arxiv.Result.Author('Mostafa Dehghani'), arxiv.Result.Author('Zhuoran Shen'), arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Xiaohua Zhai'), arxiv.Result.Author('Thomas Kipf'), arxiv.Result.Author('Neil Houlsby')]","Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.",-0.34804854,-0.095086746,0.052280605,B
6114,"We hope that our model will
serve as a strong starting point for further research on open-world detection.","In
our analyses we disentangle the determinants of successful transfer of image-level
representations to detection, and show that pre-training simple, scalable archi-
tectures on more data leads to strong zero-shot detection performance, mirroring
previous observations for image classification tasks.",Acknowledgements.,2022-05-12 17:20:36+00:00,Simple Open-Vocabulary Object Detection with Vision Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Matthias Minderer'), arxiv.Result.Author('Alexey Gritsenko'), arxiv.Result.Author('Austin Stone'), arxiv.Result.Author('Maxim Neumann'), arxiv.Result.Author('Dirk Weissenborn'), arxiv.Result.Author('Alexey Dosovitskiy'), arxiv.Result.Author('Aravindh Mahendran'), arxiv.Result.Author('Anurag Arnab'), arxiv.Result.Author('Mostafa Dehghani'), arxiv.Result.Author('Zhuoran Shen'), arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Xiaohua Zhai'), arxiv.Result.Author('Thomas Kipf'), arxiv.Result.Author('Neil Houlsby')]","Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.",-0.35010606,-0.100017525,0.05243647,B
6117,"5.2 Shape expressiveness and convergence analysis

We further study the ability of all methods to represent diÔ¨Äerent body shapes.",Mat.).,"For this, we obtain the SMPL shape for our model and pose estimation baselines
in Tab.",2022-05-12 17:55:51+00:00,Learned Vertex Descent: A New Direction for 3D Human Model Fitting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Enric Corona'), arxiv.Result.Author('Gerard Pons-Moll'), arxiv.Result.Author('Guillem Aleny√†'), arxiv.Result.Author('Francesc Moreno-Noguer')]","We propose a novel optimization-based paradigm for 3D human model fitting on
images and scans. In contrast to existing approaches that directly regress the
parameters of a low-dimensional statistical body model (e.g. SMPL) from input
images, we train an ensemble of per-vertex neural fields network. The network
predicts, in a distributed manner, the vertex descent direction towards the
ground truth, based on neural features extracted at the current vertex
projection. At inference, we employ this network, dubbed LVD, within a
gradient-descent optimization pipeline until its convergence, which typically
occurs in a fraction of a second even when initializing all vertices into a
single point. An exhaustive evaluation demonstrates that our approach is able
to capture the underlying body of clothed people with very different body
shapes, achieving a significant improvement compared to state-of-the-art. LVD
is also applicable to 3D model fitting of humans and hands, for which we show a
significant improvement to the SOTA with a much simpler and faster method.",-0.05640989,0.26524824,-0.142837,B
6118,"5.2 Shape expressiveness and convergence analysis

We further study the ability of all methods to represent diÔ¨Äerent body shapes.",Mat.).,"For this, we obtain the SMPL shape for our model and pose estimation baselines
in Tab.",2022-05-12 17:55:51+00:00,Learned Vertex Descent: A New Direction for 3D Human Model Fitting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Enric Corona'), arxiv.Result.Author('Gerard Pons-Moll'), arxiv.Result.Author('Guillem Aleny√†'), arxiv.Result.Author('Francesc Moreno-Noguer')]","We propose a novel optimization-based paradigm for 3D human model fitting on
images and scans. In contrast to existing approaches that directly regress the
parameters of a low-dimensional statistical body model (e.g. SMPL) from input
images, we train an ensemble of per-vertex neural fields network. The network
predicts, in a distributed manner, the vertex descent direction towards the
ground truth, based on neural features extracted at the current vertex
projection. At inference, we employ this network, dubbed LVD, within a
gradient-descent optimization pipeline until its convergence, which typically
occurs in a fraction of a second even when initializing all vertices into a
single point. An exhaustive evaluation demonstrates that our approach is able
to capture the underlying body of clothed people with very different body
shapes, achieving a significant improvement compared to state-of-the-art. LVD
is also applicable to 3D model fitting of humans and hands, for which we show a
significant improvement to the SOTA with a much simpler and faster method.",-0.05640989,0.26524824,-0.142837,B
6140,"However,             examples are performed for validating and evaluating the-
these traditional LR models reshape each spectral band as             oretical methods, followed by a discussion of remaining
a vector, leading to the destruction of the inherent spatial-         challenges and further research directions.","The experimental
tolerable increment of computational complexity.",spectral completeness of HS images.,2022-05-13 00:39:23+00:00,Tensor Decompositions for Hyperspectral Data Processing in Remote Sensing: A Comprehensive Review,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Minghua Wang'), arxiv.Result.Author('Danfeng Hong'), arxiv.Result.Author('Zhu Han'), arxiv.Result.Author('Jiaxin Li'), arxiv.Result.Author('Jing Yao'), arxiv.Result.Author('Lianru Gao'), arxiv.Result.Author('Bing Zhang'), arxiv.Result.Author('Jocelyn Chanussot')]","Owing to the rapid development of sensor technology, hyperspectral (HS)
remote sensing (RS) imaging has provided a significant amount of spatial and
spectral information for the observation and analysis of the Earth's surface at
a distance of data acquisition devices, such as aircraft, spacecraft, and
satellite. The recent advancement and even revolution of the HS RS technique
offer opportunities to realize the full potential of various applications,
while confronting new challenges for efficiently processing and analyzing the
enormous HS acquisition data. Due to the maintenance of the 3-D HS inherent
structure, tensor decomposition has aroused widespread concern and research in
HS data processing tasks over the past decades. In this article, we aim at
presenting a comprehensive overview of tensor decomposition, specifically
contextualizing the five broad topics in HS data processing, and they are HS
restoration, compressed sensing, anomaly detection, super-resolution, and
spectral unmixing. For each topic, we elaborate on the remarkable achievements
of tensor decomposition models for HS RS with a pivotal description of the
existing methodologies and a representative exhibition on the experimental
results. As a result, the remaining challenges of the follow-up research
directions are outlined and discussed from the perspective of the real HS RS
practices and tensor decomposition merged with advanced priors and even with
deep neural networks. This article summarizes different tensor
decomposition-based HS data processing methods and categorizes them into
different classes from simple adoptions to complex combinations with other
priors for the algorithm beginners. We also expect this survey can provide new
investigations and development trends for the experienced researchers who
understand tensor decomposition and HS RS to some extent.",0.16509788,0.25454962,0.18251061,A
6141,"and design blind estimation algorithms deserves further study
in following research.",How best to predict the degradation positions    similarity.,"A. Tensor decomposition-based HS CS reconstruction meth-
                                                                  ods
   Due to some HS images containing hundreds of spectral
bands, the high dimensions of an HS tensor cause a time-             Tucker decomposition-based methods have aroused wide at-
consuming problem.",2022-05-13 00:39:23+00:00,Tensor Decompositions for Hyperspectral Data Processing in Remote Sensing: A Comprehensive Review,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Minghua Wang'), arxiv.Result.Author('Danfeng Hong'), arxiv.Result.Author('Zhu Han'), arxiv.Result.Author('Jiaxin Li'), arxiv.Result.Author('Jing Yao'), arxiv.Result.Author('Lianru Gao'), arxiv.Result.Author('Bing Zhang'), arxiv.Result.Author('Jocelyn Chanussot')]","Owing to the rapid development of sensor technology, hyperspectral (HS)
remote sensing (RS) imaging has provided a significant amount of spatial and
spectral information for the observation and analysis of the Earth's surface at
a distance of data acquisition devices, such as aircraft, spacecraft, and
satellite. The recent advancement and even revolution of the HS RS technique
offer opportunities to realize the full potential of various applications,
while confronting new challenges for efficiently processing and analyzing the
enormous HS acquisition data. Due to the maintenance of the 3-D HS inherent
structure, tensor decomposition has aroused widespread concern and research in
HS data processing tasks over the past decades. In this article, we aim at
presenting a comprehensive overview of tensor decomposition, specifically
contextualizing the five broad topics in HS data processing, and they are HS
restoration, compressed sensing, anomaly detection, super-resolution, and
spectral unmixing. For each topic, we elaborate on the remarkable achievements
of tensor decomposition models for HS RS with a pivotal description of the
existing methodologies and a representative exhibition on the experimental
results. As a result, the remaining challenges of the follow-up research
directions are outlined and discussed from the perspective of the real HS RS
practices and tensor decomposition merged with advanced priors and even with
deep neural networks. This article summarizes different tensor
decomposition-based HS data processing methods and categorizes them into
different classes from simple adoptions to complex combinations with other
priors for the algorithm beginners. We also expect this survey can provide new
investigations and development trends for the experienced researchers who
understand tensor decomposition and HS RS to some extent.",0.12161291,0.20690575,0.1708847,A
6158,"Hence, further research efforts on data and algorithms are
                                        to different groups.","Moreover, we
                                        [10], [11], FR system seems discriminative based on classes like             demonstrated that this bias comes from both data and algorithm
                                        race, demonstrating signiÔ¨Åcantly different accuracy when applied             aspects.",Such bias can result in mistreatment of                 requested to eliminate this bias.,2022-05-13 10:25:44+00:00,Meta Balanced Network for Fair Face Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mei Wang'), arxiv.Result.Author('Yaobin Zhang'), arxiv.Result.Author('Weihong Deng')]","Although deep face recognition has achieved impressive progress in recent
years, controversy has arisen regarding discrimination based on skin tone,
questioning their deployment into real-world scenarios. In this paper, we aim
to systematically and scientifically study this bias from both data and
algorithm aspects. First, using the dermatologist approved Fitzpatrick Skin
Type classification system and Individual Typology Angle, we contribute a
benchmark called Identity Shades (IDS) database, which effectively quantifies
the degree of the bias with respect to skin tone in existing face recognition
algorithms and commercial APIs. Further, we provide two skin-tone aware
training datasets, called BUPT-Globalface dataset and BUPT-Balancedface
dataset, to remove bias in training data. Finally, to mitigate the algorithmic
bias, we propose a novel meta-learning algorithm, called Meta Balanced Network
(MBN), which learns adaptive margins in large margin loss such that the model
optimized by this loss can perform fairly across people with different skin
tones. To determine the margins, our method optimizes a meta skewness loss on a
clean and unbiased meta set and utilizes backward-on-backward automatic
differentiation to perform a second order gradient descent step on the current
margins. Extensive experiments show that MBN successfully mitigates bias and
learns more balanced performance for people with different skin tones in face
recognition. The proposed datasets are available at
http://www.whdeng.cn/RFW/index.html.",0.22905287,-0.14074025,-0.13802591,A
6160,"To further study the impact of the weights, we perform
can steadily improve the anomaly detection performance           a sensitivity analysis for each of the above hyperparameters
(from 96.2% to 96.8% in AUC).","i.e., all individual loss weights Œªi (i = {1, ¬∑ ¬∑ ¬∑ , 4}) are set
Table IV shows that the mask attention module (MAM)              to 1.","Then, the progressive mask         (Ô¨Åxing the other hyperparameters to 1).",2022-05-13 11:42:06+00:00,Self-Supervised Masking for Unsupervised Anomaly Detection and Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chaoqin Huang'), arxiv.Result.Author('Qinwei Xu'), arxiv.Result.Author('Yanfeng Wang'), arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Ya Zhang')]","Recently, anomaly detection and localization in multimedia data have received
significant attention among the machine learning community. In real-world
applications such as medical diagnosis and industrial defect detection,
anomalies only present in a fraction of the images. To extend the
reconstruction-based anomaly detection architecture to the localized anomalies,
we propose a self-supervised learning approach through random masking and then
restoring, named Self-Supervised Masking (SSM) for unsupervised anomaly
detection and localization. SSM not only enhances the training of the
inpainting network but also leads to great improvement in the efficiency of
mask prediction at inference. Through random masking, each image is augmented
into a diverse set of training triplets, thus enabling the autoencoder to learn
to reconstruct with masks of various sizes and shapes during training. To
improve the efficiency and effectiveness of anomaly detection and localization
at inference, we propose a novel progressive mask refinement approach that
progressively uncovers the normal regions and finally locates the anomalous
regions. The proposed SSM method outperforms several state-of-the-arts for both
anomaly detection and anomaly localization, achieving 98.3% AUC on Retinal-OCT
and 93.9% AUC on MVTec AD, respectively.",0.19166216,-0.07475671,0.10195701,A
6164,"offs between these types of detection techniques and make
                                                                                                             suggestions on where further research in the area should focus.","We show clear trade-
                                        purposes.","Image editing techniques have been available to the gen-
                                        eral public for many years.",2022-05-13 14:39:25+00:00,The Effectiveness of Temporal Dependency in Deepfake Video Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Will Rowan'), arxiv.Result.Author('Nick Pears')]","Deepfakes are a form of synthetic image generation used to generate fake
videos of individuals for malicious purposes. The resulting videos may be used
to spread misinformation, reduce trust in media, or as a form of blackmail.
These threats necessitate automated methods of deepfake video detection. This
paper investigates whether temporal information can improve the deepfake
detection performance of deep learning models.
  To investigate this, we propose a framework that classifies new and existing
approaches by their defining characteristics. These are the types of feature
extraction: automatic or manual, and the temporal relationship between frames:
dependent or independent. We apply this framework to investigate the effect of
temporal dependency on a model's deepfake detection performance.
  We find that temporal dependency produces a statistically significant (p <
0.05) increase in performance in classifying real images for the model using
automatic feature selection, demonstrating that spatio-temporal information can
increase the performance of deepfake video detection models.",-0.19884062,0.17207983,-0.045188148,B
6184,"We have reviewed the relevant work, and here we high-
                                           Image recognition represents the backbone for all vi-        light the key challenges where further research is needed.","Introduction                                                    There are three domains that are relevant to the stated is-
                                                                                                        sue.","sion tasks in which its accuracy and efÔ¨Åciency are sem-
                                        inal for tasks such as classiÔ¨Åcation, object detection, or         Lightweight convolution models: Training fast, efÔ¨Å-
                                        semantic segmentation.",2022-05-13 23:48:32+00:00,ImageSig: A signature transform for ultra-lightweight image recognition,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Mohamed R. Ibrahim'), arxiv.Result.Author('Terry Lyons')]","This paper introduces a new lightweight method for image recognition.
ImageSig is based on computing signatures and does not require a convolutional
structure or an attention-based encoder. It is striking to the authors that it
achieves: a) an accuracy for 64 X 64 RGB images that exceeds many of the
state-of-the-art methods and simultaneously b) requires orders of magnitude
less FLOPS, power and memory footprint. The pretrained model can be as small as
44.2 KB in size. ImageSig shows unprecedented performance on hardware such as
Raspberry Pi and Jetson-nano. ImageSig treats images as streams with multiple
channels. These streams are parameterized by spatial directions. We contribute
to the functionality of signature and rough path theory to stream-like data and
vision tasks on static images beyond temporal streams. With very few parameters
and small size models, the key advantage is that one could have many of these
""detectors"" assembled on the same chip; moreover, the feature acquisition can
be performed once and shared between different models of different tasks -
further accelerating the process. This contributes to energy efficiency and the
advancements of embedded AI at the edge.",-0.26305142,-0.036384508,0.09102045,B
6185,"object detection and semantic segmentation), in       Programme under the Lloyd‚Äôs Register Foundation grant
which further research is required to consider both local and     G0095, in part by The Alan Turing Institute‚Äôs Defence and
global signatures.","Model size: Before and after quantization                number EP/S026347/1], in part by The Alan Turing In-
                                                                  stitute under the EPSRC grant EP/N510129/1, in part
ture research, we aim to extend ImageSig to other vision          by The Alan Turing Institute‚Äôs Data Centric Engineering
tasks (e.g.","Security Programme, funded by the UK Government, in
                                                                  part by The Alan Turing Institute‚Äôs OfÔ¨Åce of National Statis-
                                                                  tics Programme, funded by the UK Government and in part
                                                                  by the Hong Kong Innovation and Technology Commission
                                                                  (InnoHK Project CIMDA).",2022-05-13 23:48:32+00:00,ImageSig: A signature transform for ultra-lightweight image recognition,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Mohamed R. Ibrahim'), arxiv.Result.Author('Terry Lyons')]","This paper introduces a new lightweight method for image recognition.
ImageSig is based on computing signatures and does not require a convolutional
structure or an attention-based encoder. It is striking to the authors that it
achieves: a) an accuracy for 64 X 64 RGB images that exceeds many of the
state-of-the-art methods and simultaneously b) requires orders of magnitude
less FLOPS, power and memory footprint. The pretrained model can be as small as
44.2 KB in size. ImageSig shows unprecedented performance on hardware such as
Raspberry Pi and Jetson-nano. ImageSig treats images as streams with multiple
channels. These streams are parameterized by spatial directions. We contribute
to the functionality of signature and rough path theory to stream-like data and
vision tasks on static images beyond temporal streams. With very few parameters
and small size models, the key advantage is that one could have many of these
""detectors"" assembled on the same chip; moreover, the feature acquisition can
be performed once and shared between different models of different tasks -
further accelerating the process. This contributes to energy efficiency and the
advancements of embedded AI at the edge.",-0.16754735,-0.06840441,0.014921537,C
6193,"Sampling Strategy                                                 Our experiment shows that random sampling, grid sam-
                                                                    pling, and DPP-based sampling all work well in perfor-
   We further study the possible sampling strategies in both        mance.",3.5.,"Using a rather small sampling rate, e.g.",2022-05-14 21:16:21+00:00,ETAD: Training Action Detection End to End on a Laptop,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shuming Liu'), arxiv.Result.Author('Mengmeng Xu'), arxiv.Result.Author('Chen Zhao'), arxiv.Result.Author('Xu Zhao'), arxiv.Result.Author('Bernard Ghanem')]","Temporal action detection (TAD) with end-to-end training often suffers from
the pain of huge demand for computing resources due to long video duration. In
this work, we propose an efficient temporal action detector (ETAD) that can
train directly from video frames with extremely low GPU memory consumption. Our
main idea is to minimize and balance the heavy computation among features and
gradients in each training iteration. We propose to sequentially forward the
snippet frame through the video encoder, and backward only a small necessary
portion of gradients to update the encoder. To further alleviate the
computational redundancy in training, we propose to dynamically sample only a
small subset of proposals during training. Moreover, various sampling
strategies and ratios are studied for both the encoder and detector. ETAD
achieves state-of-the-art performance on TAD benchmarks with remarkable
efficiency. On ActivityNet-1.3, training ETAD in 18 hours can reach 38.25%
average mAP with only 1.3 GB memory consumption per video under end-to-end
training. Our code will be publicly released.",0.29928145,0.20365776,-0.06701407,A
6199,"The coexistence of variance control and improved effective learning rate
requires further research.","Because the Ô¨Årst stage is relatively shallow compared to other
stages, the variance from Œ≥0 would have a minor effect, and improvement of the effective learning
rate may become dominant.","See the Appendix for more results on other datasets, including NABirds,
Food-101, and CIFAR-10.",2022-05-15 11:22:25+00:00,Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bum Jun Kim'), arxiv.Result.Author('Hyeyeon Choi'), arxiv.Result.Author('Hyeonah Jang'), arxiv.Result.Author('Dong Gu Lee'), arxiv.Result.Author('Wonseok Jeong'), arxiv.Result.Author('Sang Woo Kim')]","L2 regularization for weights in neural networks is widely used as a standard
training trick. However, L2 regularization for gamma, a trainable parameter of
batch normalization, remains an undiscussed mystery and is applied in different
ways depending on the library and practitioner. In this paper, we study whether
L2 regularization for gamma is valid. To explore this issue, we consider two
approaches: 1) variance control to make the residual network behave like
identity mapping and 2) stable optimization through the improvement of
effective learning rate. Through two analyses, we specify the desirable and
undesirable gamma to apply L2 regularization and propose four guidelines for
managing them. In several experiments, we observed the increase and decrease in
performance caused by applying L2 regularization to gamma of four categories,
which is consistent with our four guidelines. Our proposed guidelines were
validated through various tasks and architectures, including variants of
residual networks and transformers.",0.37263203,-0.06032881,0.06834197,A
6210,"Our PillarNet significantly advances
pillar-based 3D detectors and sheds new light on further research on point cloud
object detection.","1, our PillarNet with variant configurations, i.e., PillarNet-
vgg/18/34, offer the scalability and flexibility for point cloud-based 3D object
detection by using merely 2D convolutions.","Despite its simplicity, the proposed PillarNet achieves the state-
of-the-art performance on two large-scale autonomous driving benchmarks [2,38]
and runs in real-time (see Sec.",2022-05-16 00:14:50+00:00,PillarNet: High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
merely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from an orientation-decoupled IoU regression loss along with the IoU-aware
prediction branch. Extensive experimental results on the large-scale nuScenes
Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs
well over the state-of-the-art 3D detectors in terms of effectiveness and
efficiency.",-0.43254507,0.27024257,0.15975505,B
6211,"We expect that our findings will stimulate further research into
pillar-based point cloud representation learning.","Extensive experiments on two large-scale au-
tonomous driving benchmarks demonstrate that our PillarNet achieves the new
state-of-the-art performance with merely 2D convolutions while running at real-
time speed.","16  G. Shi, R. Li, and C. Ma

References

 1.",2022-05-16 00:14:50+00:00,PillarNet: High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
merely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from an orientation-decoupled IoU regression loss along with the IoU-aware
prediction branch. Extensive experimental results on the large-scale nuScenes
Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs
well over the state-of-the-art 3D detectors in terms of effectiveness and
efficiency.",-0.2731812,0.090268776,0.22073698,B
6212,"Our PillarNet significantly advances
pillar-based 3D detectors and sheds new light on further research on point cloud
object detection.","1, our PillarNet with variant configurations, i.e., PillarNet-
vgg/18/34, offer the scalability and flexibility for point cloud-based 3D object
detection by using merely 2D convolutions.","Despite its simplicity, the proposed PillarNet achieves the state-
of-the-art performance on two large-scale autonomous driving benchmarks [2,39]
and runs in real-time (see Sec.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. While recent researches focus on
point-based or 3D voxel-based convolutions for higher performance, these
methods fail to meet latency and power efficiency requirements especially for
deployment on embedded devices. In contrast, pillar-based methods use merely 2D
convolutions, which consume less computation resources, but they lag far behind
their voxel-based counterparts in detection accuracy. However, the superiority
of such 3D voxel-based methods over pillar-based methods is still broadly
attributed to the effectiveness of 3D convolution neural network (CNN). In this
paper, by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. Code will be made publicly available.",-0.43218723,0.2696801,0.16022684,B
6213,"We expect that our findings will stimulate further research into
pillar-based point cloud representation learning.","Extensive experiments on two large-scale au-
tonomous driving benchmarks demonstrate that our PillarNet achieves the new
state-of-the-art performance with merely 2D convolutions while running at real-
time speed.","16  G. Shi, R. Li, and C. Ma

References

 1.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. While recent researches focus on
point-based or 3D voxel-based convolutions for higher performance, these
methods fail to meet latency and power efficiency requirements especially for
deployment on embedded devices. In contrast, pillar-based methods use merely 2D
convolutions, which consume less computation resources, but they lag far behind
their voxel-based counterparts in detection accuracy. However, the superiority
of such 3D voxel-based methods over pillar-based methods is still broadly
attributed to the effectiveness of 3D convolution neural network (CNN). In this
paper, by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. Code will be made publicly available.",-0.2731812,0.090268776,0.22073698,B
6214,"Our PillarNet significantly advances
pillar-based 3D detectors and sheds new light on further research on point cloud
object detection.","1, our PillarNet with variant configurations, i.e., PillarNet-
vgg/18/34, offer the scalability and flexibility for point cloud-based 3D object
detection by using merely 2D convolutions.","Despite its simplicity, the proposed PillarNet achieves the state-
of-the-art performance on two large-scale autonomous driving benchmarks [2,39]
and runs in real-time (see Sec.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. The source code is available at
https://github.com/agent-sgs/PillarNet.git.",-0.43218723,0.2696801,0.16022684,B
6215,"We expect that our findings will stimulate further research into
pillar-based point cloud representation learning.","Extensive experiments on two large-scale au-
tonomous driving benchmarks demonstrate that our PillarNet achieves the new
state-of-the-art performance with merely 2D convolutions while running at real-
time speed.","PillarNet: High-Performance Pillar-based 3D Object Detection  17

References

 1.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. The source code is available at
https://github.com/agent-sgs/PillarNet.git.",-0.4157167,0.13753988,0.17042409,B
6216,"Our PillarNet significantly advances
pillar-based 3D detectors and sheds new light on further research on point cloud
object detection.","1, our PillarNet with variant configurations, i.e., PillarNet-
vgg/18/34, offer the scalability and flexibility for point cloud-based 3D object
detection by using merely 2D convolutions.","Despite its simplicity, the proposed PillarNet achieves the state-
of-the-art performance on two large-scale autonomous driving benchmarks [2,39]
and runs in real-time (see Sec.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. The source code is available at
https://github.com/agent-sgs/PillarNet.git.",-0.43218723,0.2696801,0.16022684,B
6217,"We expect that our findings will stimulate further research into
pillar-based point cloud representation learning.","Extensive experiments on two large-scale au-
tonomous driving benchmarks demonstrate that our PillarNet achieves the new
state-of-the-art performance with merely 2D convolutions while running at real-
time speed.","PillarNet: High-Performance Pillar-based 3D Object Detection  17

References

 1.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. The source code is available at
https://github.com/agent-sgs/PillarNet.git.",-0.4157167,0.13753988,0.17042409,B
6218,"Our PillarNet significantly advances
pillar-based 3D detectors and sheds new light on further research on point cloud
object detection.","1, our PillarNet with variant configurations, i.e., PillarNet-
vgg/18/34, offer the scalability and flexibility for point cloud-based 3D object
detection by using merely 2D convolutions.","Despite its simplicity, the proposed PillarNet achieves the state-
of-the-art performance on two large-scale autonomous driving benchmarks [2,37]
and runs in real-time (see Sec.",2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet.The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on the large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over state-of-the-art 3D detectors in terms of effectiveness and
efficiency. Code is available at \url{https://github.com/agent-sgs/PillarNet}.",-0.43240535,0.2700622,0.15995136,B
6219,"We expect that our findings will stimulate further research
into pillar-based point cloud representation learning.","From the perspective of ‚Äúencoder-neck-head‚Äù architecture design, Pil-
larNet achieves the scalability and flexibility for the hard-balanced pillar size and
model complexities.",Acknowledgements.,2022-05-16 00:14:50+00:00,PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Guangsheng Shi'), arxiv.Result.Author('Ruifeng Li'), arxiv.Result.Author('Chao Ma')]","Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet.The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on the large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over state-of-the-art 3D detectors in terms of effectiveness and
efficiency. Code is available at \url{https://github.com/agent-sgs/PillarNet}.",-0.07295965,0.078462414,0.038196795,B
6228,"Therefore, we advocate for efficiency metrics and hope this work
motivates further research to develop efficient and lightweight models suited for
at-scale applications.","SSBVER obtains performance
on par to state-of-the-art approaches in spite of being computationally far less
demanding.","References

 1.",2022-05-16 12:14:42+00:00,Scalable Vehicle Re-Identification via Self-Supervision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pirazh Khorramshahi'), arxiv.Result.Author('Vineet Shenoy'), arxiv.Result.Author('Rama Chellappa')]","As Computer Vision technologies become more mature for intelligent
transportation applications, it is time to ask how efficient and scalable they
are for large-scale and real-time deployment. Among these technologies is
Vehicle Re-Identification which is one of the key elements in city-scale
vehicle analytics systems. Many state-of-the-art solutions for vehicle re-id
mostly focus on improving the accuracy on existing re-id benchmarks and often
ignore computational complexity. To balance the demands of accuracy and
computational efficiency, in this work we propose a simple yet effective hybrid
solution empowered by self-supervised training which only uses a single network
during inference time and is free of intricate and computation-demanding add-on
modules often seen in state-of-the-art approaches. Through extensive
experiments, we show our approach, termed Self-Supervised and Boosted VEhicle
Re-Identification (SSBVER), is on par with state-of-the-art alternatives in
terms of accuracy without introducing any additional overhead during
deployment. Additionally we show that our approach, generalizes to different
backbone architectures which facilitates various resource constraints and
consistently results in a significant accuracy boost.",0.32077545,0.048545808,0.13065638,A
6317,"In this work, we further study
      models [42, 54].","found notable differences between transfer task
   ‚Ä¢ We show that our model generalizes and adapts to new          affinity and multi-task affinity and showed the benefits of
      domains with a lower average error on the different          leveraging structural similarities between tasks at all lev-
      vision tasks than the existing multitask convolutional       els for multitask learning.","the task inter-dependencies, but by designing a multitask
                                                                   transformer model instead of a CNN one.",2022-05-17 13:03:18+00:00,MulT: An End-to-End Multitask Learning Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Deblina Bhattacharjee'), arxiv.Result.Author('Tong Zhang'), arxiv.Result.Author('Sabine S√ºsstrunk'), arxiv.Result.Author('Mathieu Salzmann')]","We propose an end-to-end Multitask Learning Transformer framework, named
MulT, to simultaneously learn multiple high-level vision tasks, including depth
estimation, semantic segmentation, reshading, surface normal estimation, 2D
keypoint detection, and edge detection. Based on the Swin transformer model,
our framework encodes the input image into a shared representation and makes
predictions for each vision task using task-specific transformer-based decoder
heads. At the heart of our approach is a shared attention mechanism modeling
the dependencies across the tasks. We evaluate our model on several multitask
benchmarks, showing that our MulT framework outperforms both the state-of-the
art multitask convolutional neural network models and all the respective single
task transformer models. Our experiments further highlight the benefits of
sharing attention across all the tasks, and demonstrate that our MulT model is
robust and generalizes well to new domains. Our project website is at
https://ivrl.github.io/MulT/.",-0.14155143,-0.3182745,0.024176866,C
6340,"We
                                                                       hope that this encouraging result inspires further research
                                                                       into general-purpose object reconstruction.","At the
                                                                       same time, our method is the first that not only handles revo-
                                                                       lute, but also prismatic joints and combinations thereof.","Acknowledgement We would like to thank Michael Goesele,
                                                                       Eddy Ilg, Zhaoyang Lv, Jisan Mahmud, Tanner Schmidt, and
                                                                       Anh Thai for helpful discussions.",2022-05-17 17:50:47+00:00,Self-supervised Neural Articulated Shape and Appearance Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fangyin Wei'), arxiv.Result.Author('Rohan Chabra'), arxiv.Result.Author('Lingni Ma'), arxiv.Result.Author('Christoph Lassner'), arxiv.Result.Author('Michael Zollh√∂fer'), arxiv.Result.Author('Szymon Rusinkiewicz'), arxiv.Result.Author('Chris Sweeney'), arxiv.Result.Author('Richard Newcombe'), arxiv.Result.Author('Mira Slavcheva')]","Learning geometry, motion, and appearance priors of object classes is
important for the solution of a large variety of computer vision problems.
While the majority of approaches has focused on static objects, dynamic
objects, especially with controllable articulation, are less explored. We
propose a novel approach for learning a representation of the geometry,
appearance, and motion of a class of articulated objects given only a set of
color images as input. In a self-supervised manner, our novel representation
learns shape, appearance, and articulation codes that enable independent
control of these semantic dimensions. Our model is trained end-to-end without
requiring any articulation annotations. Experiments show that our approach
performs well for different joint types, such as revolute and prismatic joints,
as well as different combinations of these joints. Compared to state of the art
that uses direct 3D supervision and does not output appearance, we recover more
faithful geometry and appearance from 2D observations only. In addition, our
representation enables a large variety of applications, such as few-shot
reconstruction, the generation of novel articulations, and novel
view-synthesis.",-0.038412273,0.2729085,0.013267051,B
6383,We further study MAE pre-training on real-world Instagram videos.,Real-world data.,"We study two
sets: (i) Instagram videos curated (IG-curated) [24] with hashtags similar to K400 classes, and (ii)
random, uncrated Instagram videos (IG-uncurated).",2022-05-18 17:59:59+00:00,Masked Autoencoders As Spatiotemporal Learners,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Christoph Feichtenhofer'), arxiv.Result.Author('Haoqi Fan'), arxiv.Result.Author('Yanghao Li'), arxiv.Result.Author('Kaiming He')]","This paper studies a conceptually simple extension of Masked Autoencoders
(MAE) to spatiotemporal representation learning from videos. We randomly mask
out spacetime patches in videos and learn an autoencoder to reconstruct them in
pixels. Interestingly, we show that our MAE method can learn strong
representations with almost no inductive bias on spacetime (only except for
patch and positional embeddings), and spacetime-agnostic random masking
performs the best. We observe that the optimal masking ratio is as high as 90%
(vs. 75% on images), supporting the hypothesis that this ratio is related to
information redundancy of the data. A high masking ratio leads to a large
speedup, e.g., > 4x in wall-clock time or even more. We report competitive
results on several challenging video datasets using vanilla Vision
Transformers. We observe that MAE can outperform supervised pre-training by
large margins. We further report encouraging results of training on real-world,
uncurated Instagram data. Our study suggests that the general framework of
masked autoencoding (BERT, MAE, etc.) can be a unified methodology for
representation learning with minimal domain knowledge.",-0.06884868,-0.22982618,-0.1349998,C
6384,We further study MAE pre-training on real-world Instagram videos.,Real-world data.,"We study two
sets: (i) Instagram videos curated (IG-curated) [24] with hashtags similar to K400 classes, and (ii)
random, uncrated Instagram videos (IG-uncurated).",2022-05-18 17:59:59+00:00,Masked Autoencoders As Spatiotemporal Learners,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Christoph Feichtenhofer'), arxiv.Result.Author('Haoqi Fan'), arxiv.Result.Author('Yanghao Li'), arxiv.Result.Author('Kaiming He')]","This paper studies a conceptually simple extension of Masked Autoencoders
(MAE) to spatiotemporal representation learning from videos. We randomly mask
out spacetime patches in videos and learn an autoencoder to reconstruct them in
pixels. Interestingly, we show that our MAE method can learn strong
representations with almost no inductive bias on spacetime (only except for
patch and positional embeddings), and spacetime-agnostic random masking
performs the best. We observe that the optimal masking ratio is as high as 90%
(vs. 75% on images), supporting the hypothesis that this ratio is related to
information redundancy of the data. A high masking ratio leads to a large
speedup, e.g., > 4x in wall-clock time or even more. We report competitive
results on several challenging video datasets using vanilla Vision
Transformers. We observe that MAE can outperform supervised pre-training by
large margins. We further report encouraging results of training on real-world,
uncurated Instagram data. Our study suggests that the general framework of
masked autoencoding (BERT, MAE, etc.) can be a unified methodology for
representation learning with minimal domain knowledge.",-0.06884868,-0.22982618,-0.1349998,C
6400,"This phenomenon indicates how to select                   None    23.67 18.15 15.41 31.59 24.57 21.45
accurate depths deserves further study in the future work.","Notably, as presented in the last row of Table 4, if we de-     Strategies         Val, AP3D70 (%)     Val, APBEV 70 (%)
velop a perfect strategy that always selects the most accurate                    Easy Moderate Hard   Easy Moderate Hard
one from the 20 depths, the AP3D70 on the Moderate level
arrives 38.73%.","3D IOU     22.67  18.54  16.06  30.30  24.14  21.17
5.4.",2022-05-19 08:12:55+00:00,Diversity Matters: Fully Exploiting Depth Clues for Reliable Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhuoling Li'), arxiv.Result.Author('Zhan Qu'), arxiv.Result.Author('Yang Zhou'), arxiv.Result.Author('Jianzhuang Liu'), arxiv.Result.Author('Haoqian Wang'), arxiv.Result.Author('Lihui Jiang')]","As an inherently ill-posed problem, depth estimation from single images is
the most challenging part of monocular 3D object detection (M3OD). Many
existing methods rely on preconceived assumptions to bridge the missing spatial
information in monocular images, and predict a sole depth value for every
object of interest. However, these assumptions do not always hold in practical
applications. To tackle this problem, we propose a depth solving system that
fully explores the visual clues from the subtasks in M3OD and generates
multiple estimations for the depth of each target. Since the depth estimations
rely on different assumptions in essence, they present diverse distributions.
Even if some assumptions collapse, the estimations established on the remaining
assumptions are still reliable. In addition, we develop a depth selection and
combination strategy. This strategy is able to remove abnormal estimations
caused by collapsed assumptions, and adaptively combine the remaining
estimations into a single one. In this way, our depth solving system becomes
more precise and robust. Exploiting the clues from multiple subtasks of M3OD
and without introducing any extra information, our method surpasses the current
best method by more than 20% relatively on the Moderate level of test split in
the KITTI 3D object detection benchmark, while still maintaining real-time
efficiency.",0.32992858,0.1574457,0.092188925,A
6425,"We hope our work could revitalize contrastive learning and look forward to
further research along this direction.","This attempt unleashes the great potential of contrastive learning in self-supervised pre-training of
vision Transformers [10].",Limitations.,2022-05-19 15:22:29+00:00,Masked Image Modeling with Denoising Contrast,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kun Yi'), arxiv.Result.Author('Yixiao Ge'), arxiv.Result.Author('Xiaotong Li'), arxiv.Result.Author('Shusheng Yang'), arxiv.Result.Author('Dian Li'), arxiv.Result.Author('Jianping Wu'), arxiv.Result.Author('Ying Shan'), arxiv.Result.Author('Xiaohu Qie')]","Since the development of self-supervised visual representation learning from
contrastive learning to masked image modeling, there is no significant
difference in essence, that is, how to design proper pretext tasks for vision
dictionary look-up. Masked image modeling recently dominates this line of
research with state-of-the-art performance on vision Transformers, where the
core is to enhance the patch-level visual context capturing of the network via
denoising auto-encoding mechanism. Rather than tailoring image tokenizers with
extra training stages as in previous works, we unleash the great potential of
contrastive learning on denoising auto-encoding and introduce a new
pre-training method, ConMIM, to produce simple intra-image inter-patch
contrastive constraints as the learning objectives for masked patch prediction.
We further strengthen the denoising mechanism with asymmetric designs,
including image perturbations and model progress rates, to improve the network
pre-training. ConMIM-pretrained vision Transformers with various scales achieve
promising results on downstream image classification, semantic segmentation,
object detection, and instance segmentation tasks.",-0.18974602,-0.19124162,0.01280795,C
6454,"In
the future, how to design NMF methods for unmixing deserves          [1] D. Landgrebe, ‚ÄúHyperspectral image data analysis,‚Äù IEEE Signal
further research.","To this end, it is                                   REFERENCES
quite challenging to obtain excellent unmixing performance.",We provide some considerations as follows.,2022-05-20 02:48:43+00:00,Hyperspectral Unmixing Based on Nonnegative Matrix Factorization: A Comprehensive Review,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Xin-Ru Feng'), arxiv.Result.Author('Heng-Chao Li'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Qian Du'), arxiv.Result.Author('Xiuping Jia'), arxiv.Result.Author('Antonio Plaza')]","Hyperspectral unmixing has been an important technique that estimates a set
of endmembers and their corresponding abundances from a hyperspectral image
(HSI). Nonnegative matrix factorization (NMF) plays an increasingly significant
role in solving this problem. In this article, we present a comprehensive
survey of the NMF-based methods proposed for hyperspectral unmixing. Taking the
NMF model as a baseline, we show how to improve NMF by utilizing the main
properties of HSIs (e.g., spectral, spatial, and structural information). We
categorize three important development directions including constrained NMF,
structured NMF, and generalized NMF. Furthermore, several experiments are
conducted to illustrate the effectiveness of associated algorithms. Finally, we
conclude the article with possible future directions with the purposes of
providing guidelines and inspiration to promote the development of
hyperspectral unmixing.",0.102615,0.17292139,0.28374895,A
6457,"Although a few datasets [8][9] have
task deserving further research and exploration.","An appropriate dataset is critical to the geometry diagram
Due to the complex layout and between-primitive relationship,       parsing research and facilitate explorations of various high-
plane geometry diagram parsing (PGDP) is still a challenging        performance algorithms.","An appropriate     been proposed recently, they have small number of samples,
dataset is critical for the research of PGDP.",2022-05-20 03:41:41+00:00,PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yihan Hao'), arxiv.Result.Author('Mingliang Zhang'), arxiv.Result.Author('Fei Yin'), arxiv.Result.Author('Linlin Huang')]","Diagram parsing is an important foundation for geometry problem solving,
attracting increasing attention in the field of intelligent education and
document image understanding. Due to the complex layout and between-primitive
relationship, plane geometry diagram parsing (PGDP) is still a challenging task
deserving further research and exploration. An appropriate dataset is critical
for the research of PGDP. Although some datasets with rough annotations have
been proposed to solve geometric problems, they are either small in scale or
not publicly available. The rough annotations also make them not very useful.
Thus, we propose a new large-scale geometry diagram dataset named PGDP5K and a
novel annotation method. Our dataset consists of 5000 diagram samples composed
of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text
types. Different from previous datasets, our PGDP5K dataset is labeled with
more fine-grained annotations at primitive level, including primitive classes,
locations and relationships. What is more, combined with above annotations and
geometric prior knowledge, it can generate intelligible geometric propositions
automatically and uniquely. We performed experiments on PGDP5K and
IMP-Geometry3K datasets reveal that the state-of-the-art (SOTA) method achieves
only 66.07% F1 value. This shows that PGDP5K presents a challenge for future
research. Our dataset is available at
http://www.nlpr.ia.ac.cn/databases/CASIA-PGDP5K/.",-0.02059239,0.0964466,-0.17974344,B
6459,"To further study the effectiveness
      the inter-channel and inter-spatial relationships.","CBAM [66] sequentially calculates
      the channel attention and the spatial attention to exploit                Computational costs.","Similarly,          of the structured attention composition module, Table VIII
      we can Ô¨Årst estimate the modality attention and then                   reports parameters and FLOPs based on four different methods
      learn the frame attention, to model the frame-modality                 [20], [10], [7], [11].",2022-05-20 04:32:09+00:00,Structured Attention Composition for Temporal Action Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Le Yang'), arxiv.Result.Author('Junwei Han'), arxiv.Result.Author('Tao Zhao'), arxiv.Result.Author('Nian Liu'), arxiv.Result.Author('Dingwen Zhang')]","Temporal action localization aims at localizing action instances from
untrimmed videos. Existing works have designed various effective modules to
precisely localize action instances based on appearance and motion features.
However, by treating these two kinds of features with equal importance,
previous works cannot take full advantage of each modality feature, making the
learned model still sub-optimal. To tackle this issue, we make an early effort
to study temporal action localization from the perspective of multi-modality
feature learning, based on the observation that different actions exhibit
specific preferences to appearance or motion modality. Specifically, we build a
novel structured attention composition module. Unlike conventional attention,
the proposed module would not infer frame attention and modality attention
independently. Instead, by casting the relationship between the modality
attention and the frame attention as an attention assignment process, the
structured attention composition module learns to encode the frame-modality
structure and uses it to regularize the inferred frame attention and modality
attention, respectively, upon the optimal transport theory. The final
frame-modality attention is obtained by the composition of the two individual
attentions. The proposed structured attention composition module can be
deployed as a plug-and-play module into existing action localization
frameworks. Extensive experiments on two widely used benchmarks show that the
proposed structured attention composition consistently improves four
state-of-the-art temporal action localization methods and builds new
state-of-the-art performance on THUMOS14. Code is availabel at
https://github.com/VividLe/Online-Action-Detection.",0.0071620457,-0.06650708,-0.10896152,C
6460,"To further study the effectiveness
      the inter-channel and inter-spatial relationships.","CBAM [66] sequentially calculates
      the channel attention and the spatial attention to exploit                Computational costs.","Similarly,          of the structured attention composition module, Table VIII
      we can Ô¨Årst estimate the modality attention and then                   reports parameters and FLOPs based on four different methods
      learn the frame attention, to model the frame-modality                 [20], [10], [7], [11].",2022-05-20 04:32:09+00:00,Structured Attention Composition for Temporal Action Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Le Yang'), arxiv.Result.Author('Junwei Han'), arxiv.Result.Author('Tao Zhao'), arxiv.Result.Author('Nian Liu'), arxiv.Result.Author('Dingwen Zhang')]","Temporal action localization aims at localizing action instances from
untrimmed videos. Existing works have designed various effective modules to
precisely localize action instances based on appearance and motion features.
However, by treating these two kinds of features with equal importance,
previous works cannot take full advantage of each modality feature, making the
learned model still sub-optimal. To tackle this issue, we make an early effort
to study temporal action localization from the perspective of multi-modality
feature learning, based on the observation that different actions exhibit
specific preferences to appearance or motion modality. Specifically, we build a
novel structured attention composition module. Unlike conventional attention,
the proposed module would not infer frame attention and modality attention
independently. Instead, by casting the relationship between the modality
attention and the frame attention as an attention assignment process, the
structured attention composition module learns to encode the frame-modality
structure and uses it to regularize the inferred frame attention and modality
attention, respectively, upon the optimal transport theory. The final
frame-modality attention is obtained by the composition of the two individual
attentions. The proposed structured attention composition module can be
deployed as a plug-and-play module into existing action localization
frameworks. Extensive experiments on two widely used benchmarks show that the
proposed structured attention composition consistently improves four
state-of-the-art temporal action localization methods and builds new
state-of-the-art performance on THUMOS14. Code is availabel at
https://github.com/VividLe/Structured-Attention-Composition.",0.0071620457,-0.06650708,-0.10896152,C
6468,"These issues warrant further research
and consideration when using the pre-trained models.","1, 3

A Broader Impacts

The proposed method may generate inexistent images and introduce biases from the learned training
dataset, which would contain possible negative societal impacts.","B Implementation Details

B.1 Architectures

Vanilla ViT Architecture.",2022-05-20 10:16:30+00:00,Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiang Li'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Lingfeng Yang'), arxiv.Result.Author('Jian Yang')]","Masked AutoEncoder (MAE) has recently led the trends of visual
self-supervision area by an elegant asymmetric encoder-decoder design, which
significantly optimizes both the pre-training efficiency and fine-tuning
accuracy. Notably, the success of the asymmetric structure relies on the
""global"" property of Vanilla Vision Transformer (ViT), whose self-attention
mechanism reasons over arbitrary subset of discrete image patches. However, it
is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be
adopted in MAE pre-training as they commonly introduce operators within ""local""
windows, making it difficult to handle the random sequence of partial vision
tokens. In this paper, we propose Uniform Masking (UM), successfully enabling
MAE pre-training for Pyramid-based ViTs with locality (termed ""UM-MAE"" for
short). Specifically, UM includes a Uniform Sampling (US) that strictly samples
$1$ random patch from each $2 \times 2$ grid, and a Secondary Masking (SM)
which randomly masks a portion of (usually $25\%$) the already sampled regions
as learnable tokens. US preserves equivalent elements across multiple
non-overlapped local windows, resulting in the smooth support for popular
Pyramid-based ViTs; whilst SM is designed for better transferable visual
representations since US reduces the difficulty of pixel recovery pre-task that
hinders the semantic learning. We demonstrate that UM-MAE significantly
improves the pre-training efficiency (e.g., it speeds up and reduces the GPU
memory by $\sim 2\times$) of Pyramid-based ViTs, but maintains the competitive
fine-tuning performance across downstream tasks. For example using HTC++
detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only
in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The
codes are available at https://github.com/implus/UM-MAE.",0.0339493,-0.24856591,-0.034334674,C
6470,"In this paper, we introduce a further study of
                                        PoseTReID framework in order to give a more complete compre-       modules with some other improvements we made.","analysis, etc.","The details
                                        hension of the framework.",2022-05-20 11:06:58+00:00,People Tracking and Re-Identifying in Distributed Contexts: Extension of PoseTReID,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ratha Siv'), arxiv.Result.Author('Matei Mancas'), arxiv.Result.Author('Bernard Gosselin'), arxiv.Result.Author('Dona Valy'), arxiv.Result.Author('Sokchenda Sreng')]","In our previous paper, we introduced PoseTReID which is a generic framework
for real-time 2D multi-person tracking in distributed interaction spaces where
long-term people's identities are important for other studies such as behavior
analysis, etc. In this paper, we introduce a further study of PoseTReID
framework in order to give a more complete comprehension of the framework. We
use a well-known bounding box detector YOLO (v4) for the detection to compare
to OpenPose which was used in our last paper, and we use SORT and DeepSORT to
compare to centroid which was also used previously, and most importantly for
the re-identification, we use a bunch of deep leaning methods such as MLFN,
OSNet, and OSNet-AIN with our custom classification layer to compare to FaceNet
which was also used earlier in our last paper. By evaluating on our PoseTReID
datasets, even though those deep learning re-identification methods are
designed for only short-term re-identification across multiple cameras or
videos, it is worth showing that they give impressive results which boost the
overall tracking performance of PoseTReID framework regardless the type of
tracking method. At the same time, we also introduce our research-friendly and
open source Python toolbox pyppbox, which is pure written in Python and
contains all sub-modules which are used this study along with real-time online
and offline evaluations for our PoseTReID datasets. This pyppbox is available
on GitHub https://github.com/rathaumons/pyppbox .",0.1335254,0.20047037,-0.12860681,A
6471,"In this paper, we introduce a further study of
                                        PoseTReID framework in order to give a more complete compre-       and show the potential of the overall tracking performance
                                        hension of the framework.","analysis, etc.","We use a well-known bounding box         with a selection of different detector, tracker, and re-identiÔ¨Åer
                                        detector YOLO (v4) for the detection to compare to OpenPose        modules with some other improvements we made.",2022-05-20 11:06:58+00:00,People Tracking and Re-Identifying in Distributed Contexts: Extension Study of PoseTReID,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ratha Siv'), arxiv.Result.Author('Matei Mancas'), arxiv.Result.Author('Bernard Gosselin'), arxiv.Result.Author('Dona Valy'), arxiv.Result.Author('Sokchenda Sreng')]","In our previous paper, we introduced PoseTReID which is a generic framework
for real-time 2D multi-person tracking in distributed interaction spaces where
long-term people's identities are important for other studies such as behavior
analysis, etc. In this paper, we introduce a further study of PoseTReID
framework in order to give a more complete comprehension of the framework. We
use a well-known bounding box detector YOLO (v4) for the detection to compare
to OpenPose which was used in our last paper, and we use SORT and DeepSORT to
compare to centroid which was also used previously, and most importantly for
the re-identification, we use a bunch of deep leaning methods such as MLFN,
OSNet, and OSNet-AIN with our custom classification layer to compare to FaceNet
which was also used earlier in our last paper. By evaluating on our PoseTReID
datasets, even though those deep learning re-identification methods are
designed for only short-term re-identification across multiple cameras or
videos, it is worth showing that they give impressive results which boost the
overall tracking performance of PoseTReID framework regardless the type of
tracking method. At the same time, we also introduce our research-friendly and
open source Python toolbox pyppbox, which is purely written in Python and
contains all sub-modules which are used in this study along with real-time
online and offline evaluations for our PoseTReID datasets. This pyppbox is
available on GitHub https://github.com/rathaumons/pyppbox .",-0.15228131,0.2418674,-0.02350637,B
6508,We further study the effect of smoothing in Tabs.,"For example, IntGrad at the input layer compared with Grad-CAM at the Ô¨Ånal layer results in a
correlation coefÔ¨Åcient of 0.34, while S-IntGrad results in a correlation coefÔ¨Åcient of 0.80.",1 and 2.,2022-05-20 20:50:17+00:00,Towards Better Understanding Attribution Methods,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sukrut Rao'), arxiv.Result.Author('Moritz B√∂hle'), arxiv.Result.Author('Bernt Schiele')]","Deep neural networks are very successful on many vision tasks, but hard to
interpret due to their black box nature. To overcome this, various post-hoc
attribution methods have been proposed to identify image regions most
influential to the models' decisions. Evaluating such methods is challenging
since no ground truth attributions exist. We thus propose three novel
evaluation schemes to more reliably measure the faithfulness of those methods,
to make comparisons between them more fair, and to make visual inspection more
systematic. To address faithfulness, we propose a novel evaluation setting
(DiFull) in which we carefully control which parts of the input can influence
the output in order to distinguish possible from impossible attributions. To
address fairness, we note that different methods are applied at different
layers, which skews any comparison, and so evaluate all methods on the same
layers (ML-Att) and discuss how this impacts their performance on quantitative
metrics. For more systematic visualizations, we propose a scheme (AggAtt) to
qualitatively evaluate the methods on complete datasets. We use these
evaluation schemes to study strengths and shortcomings of some widely used
attribution methods. Finally, we propose a post-processing smoothing step that
significantly improves the performance of some attribution methods, and discuss
its applicability.",0.2925343,0.069569774,0.12150393,A
6514,"Our source code and trained models      multiple camera viewpoint problem in the Ô¨Åne-grained clas-
      will available for further study.","[31] lever-
      different Ô¨Åne-grained classiÔ¨Åers to achieve new state-      aged Canonical 3D Object Representation to cope up with
      of-the-art results.",siÔ¨Åcation task.,2022-05-21 07:41:27+00:00,Fine-Grained Visual Classification using Self Assessment Classifier,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tuong Do'), arxiv.Result.Author('Huy Tran'), arxiv.Result.Author('Erman Tjiputra'), arxiv.Result.Author('Quang D. Tran'), arxiv.Result.Author('Anh Nguyen')]","Extracting discriminative features plays a crucial role in the fine-grained
visual classification task. Most of the existing methods focus on developing
attention or augmentation mechanisms to achieve this goal. However, addressing
the ambiguity in the top-k prediction classes is not fully investigated. In
this paper, we introduce a Self Assessment Classifier, which simultaneously
leverages the representation of the image and top-k prediction classes to
reassess the classification results. Our method is inspired by continual
learning with coarse-grained and fine-grained classifiers to increase the
discrimination of features in the backbone and produce attention maps of
informative areas on the image. In practice, our method works as an auxiliary
branch and can be easily integrated into different architectures. We show that
by effectively addressing the ambiguity in the top-k prediction classes, our
method achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and
FGVC Aircraft datasets. Furthermore, our method also consistently improves the
accuracy of different existing fine-grained classifiers with a unified setup.",-0.31806743,0.21873468,-0.036394052,B
6521,"Through further research in the          coder middle layer while keeping the original MIM de-
Ô¨Åeld of computer vision about [8] we found that the spatial         coder hyperparameters.","We replace the de-
prove encoder performance.","Deep convolution contributes
location information is beneÔ¨Åcial to improve the local mod-         to the MIM decoder‚Äôs ability to improve channel and
eling capability of the model.",2022-05-21 09:45:50+00:00,Improvements to Self-Supervised Representation Learning for Masked Image Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiawei Mao'), arxiv.Result.Author('Xuesong Yin'), arxiv.Result.Author('Yuanqi Chang'), arxiv.Result.Author('Honggu Zhou')]","This paper explores improvements to the masked image modeling (MIM) paradigm.
The MIM paradigm enables the model to learn the main object features of the
image by masking the input image and predicting the masked part by the unmasked
part. We found the following three main directions for MIM to be improved.
First, since both encoders and decoders contribute to representation learning,
MIM uses only encoders for downstream tasks, which ignores the impact of
decoders on representation learning. Although the MIM paradigm already employs
small decoders with asymmetric structures, we believe that continued reduction
of decoder parameters is beneficial to improve the representational learning
capability of the encoder . Second, MIM solves the image prediction task by
training the encoder and decoder together , and does not design a separate task
for the encoder . To further enhance the performance of the encoder when
performing downstream tasks, we designed the encoder for the tasks of
comparative learning and token position prediction. Third, since the input
image may contain background and other objects, and the proportion of each
object in the image varies, reconstructing the tokens related to the background
or to other objects is not meaningful for MIM to understand the main object
representations. Therefore we use ContrastiveCrop to crop the input image so
that the input image contains as much as possible only the main objects. Based
on the above three improvements to MIM, we propose a new model, Contrastive
Masked AutoEncoders (CMAE). We achieved a Top-1 accuracy of 65.84% on
tinyimagenet using the ViT-B backbone, which is +2.89 outperforming the MAE of
competing methods when all conditions are equal. Code will be made available.",-0.15361448,0.03960246,0.18024075,B
6563,"See Appendix C
for further study, which demonstrates that the vectors learned by       [9] Rainer Dahlhaus.",feature vectors with rich semantic information.,2000.,2022-05-23 07:51:15+00:00,Supporting Vision-Language Model Inference with Causality-pruning Knowledge Prompt,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiangmeng Li'), arxiv.Result.Author('Wenyi Mo'), arxiv.Result.Author('Wenwen Qiang'), arxiv.Result.Author('Bing Su'), arxiv.Result.Author('Changwen Zheng')]","Vision-language models are pre-trained by aligning image-text pairs in a
common space so that the models can deal with open-set visual concepts by
learning semantic information from textual labels. To boost the transferability
of these models on downstream tasks in a zero-shot manner, recent works explore
generating fixed or learnable prompts, i.e., classification weights are
synthesized from natural language describing task-relevant categories, to
reduce the gap between tasks in the training and test phases. However, how and
what prompts can improve inference performance remains unclear. In this paper,
we explicitly provide exploration and clarify the importance of including
semantic information in prompts, while existing prompt methods generate prompts
without exploring the semantic information of textual labels. A challenging
issue is that manually constructing prompts, with rich semantic information,
requires domain expertise and is extremely time-consuming. To this end, we
propose Causality-pruning Knowledge Prompt (CapKP) for adapting pre-trained
vision-language models to downstream image recognition. CapKP retrieves an
ontological knowledge graph by treating the textual label as a query to explore
task-relevant semantic information. To further refine the derived semantic
information, CapKP introduces causality-pruning by following the first
principle of Granger causality. Empirically, we conduct extensive evaluations
to demonstrate the effectiveness of CapKP, e.g., with 8 shots, CapKP
outperforms the manual-prompt method by 12.51% and the learnable-prompt method
by 1.39% on average, respectively. Experimental analyses prove the superiority
of CapKP in domain generalization compared to benchmark approaches.",-0.06163086,-0.3104489,-0.13511309,C
6567,We encourage further study to understand       2020.,"be is the compressed model vulnerable to adversarial attacks
because of the introduction of prunining and quantization      He, Y.; Ding, Y.; Liu, P.; Zhu, L.; Zhang, H.; and Yang, Y.
into the model?","Learning Filter Pruning Criteria for Deep Convo-
and mitigate the biases and risks potentially brought by net-  lutional Neural Networks Acceleration.",2022-05-23 09:05:25+00:00,OPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Peng Hu'), arxiv.Result.Author('Xi Peng'), arxiv.Result.Author('Hongyuan Zhu'), arxiv.Result.Author('Mohamed M. Sabry Aly'), arxiv.Result.Author('Jie Lin')]","As Deep Neural Networks (DNNs) usually are overparameterized and have
millions of weight parameters, it is challenging to deploy these large DNN
models on resource-constrained hardware platforms, e.g., smartphones. Numerous
network compression methods such as pruning and quantization are proposed to
reduce the model size significantly, of which the key is to find suitable
compression allocation (e.g., pruning sparsity and quantization codebook) of
each layer. Existing solutions obtain the compression allocation in an
iterative/manual fashion while finetuning the compressed model, thus suffering
from the efficiency issue. Different from the prior art, we propose a novel
One-shot Pruning-Quantization (OPQ) in this paper, which analytically solves
the compression allocation with pre-trained weight parameters only. During
finetuning, the compression module is fixed and only weight parameters are
updated. To our knowledge, OPQ is the first work that reveals pre-trained model
is sufficient for solving pruning and quantization simultaneously, without any
complex iterative/manual optimization at the finetuning stage. Furthermore, we
propose a unified channel-wise quantization method that enforces all channels
of each layer to share a common codebook, which leads to low bit-rate
allocation without introducing extra overhead brought by traditional
channel-wise quantization. Comprehensive experiments on ImageNet with
AlexNet/MobileNet-V1/ResNet-50 show that our method improves accuracy and
training efficiency while obtains significantly higher compression rates
compared to the state-of-the-art.",0.026651185,-0.3150937,0.3111153,C
6574,"These theories have given rise to a further study of musical per-
formance through the analysis of movement data captured during performance
(God√∏y & Leman, 2010).","Theories of embodied music cognition posit that music is a multimodal
medium experienced not only through sound, but also through visual and kine-
matic cues.","Research outcomes indicate that body motion during
musical performances is not incidental, but instead contributes to the meaning
of performed music (MacRitchie et al., 2013; Gorton & O¬® stersj¬®o, 2019).",2022-05-05 09:04:04+00:00,Deep Neural Network approaches for Analysing Videos of Music Performances,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Foteini Simistira Liwicki'), arxiv.Result.Author('Richa Upadhya'), arxiv.Result.Author('Prakash Chandra Chhipa'), arxiv.Result.Author('Killian Murphy'), arxiv.Result.Author('Federico Visi'), arxiv.Result.Author('Stefan √ñstersj√∂'), arxiv.Result.Author('Marcus Liwicki')]","This paper presents a framework to automate the labelling process for
gestures in musical performance videos with a 3D Convolutional Neural Network
(CNN). While this idea was proposed in a previous study, this paper introduces
several novelties: (i) Presents a novel method to overcome the class imbalance
challenge and make learning possible for co-existent gestures by batch
balancing approach and spatial-temporal representations of gestures. (ii)
Performs a detailed study on 7 and 18 categories of gestures generated during
the performance (guitar play) of musical pieces that have been video-recorded.
(iii) Investigates the possibility to use audio features. (iv) Extends the
analysis to multiple videos. The novel methods significantly improve the
performance of gesture identification by 12 %, when compared to the previous
work (51 % in this study over 39 % in previous work). We successfully validate
the proposed methods on 7 super classes (72 %), an ensemble of the 18
gestures/classes, and additional videos (75 %).",0.11325765,0.09818639,-0.3705931,A
6575,"These theories have given rise to a further study of musical per-
formance through the analysis of movement data captured during performance
(God√∏y & Leman, 2010).","Theories of embodied music cognition posit that music is a multimodal
medium experienced not only through sound, but also through visual and kine-
matic cues.","Research outcomes indicate that body motion during
musical performances is not incidental, but instead contributes to the meaning
of performed music (MacRitchie et al., 2013; Gorton & O¬® stersj¬®o, 2019).",2022-05-05 09:04:04+00:00,Deep Neural Network approaches for Analysing Videos of Music Performances,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Foteini Simistira Liwicki'), arxiv.Result.Author('Richa Upadhyay'), arxiv.Result.Author('Prakash Chandra Chhipa'), arxiv.Result.Author('Killian Murphy'), arxiv.Result.Author('Federico Visi'), arxiv.Result.Author('Stefan √ñstersj√∂'), arxiv.Result.Author('Marcus Liwicki')]","This paper presents a framework to automate the labelling process for
gestures in musical performance videos with a 3D Convolutional Neural Network
(CNN). While this idea was proposed in a previous study, this paper introduces
several novelties: (i) Presents a novel method to overcome the class imbalance
challenge and make learning possible for co-existent gestures by batch
balancing approach and spatial-temporal representations of gestures. (ii)
Performs a detailed study on 7 and 18 categories of gestures generated during
the performance (guitar play) of musical pieces that have been video-recorded.
(iii) Investigates the possibility to use audio features. (iv) Extends the
analysis to multiple videos. The novel methods significantly improve the
performance of gesture identification by 12 %, when compared to the previous
work (51 % in this study over 39 % in previous work). We successfully validate
the proposed methods on 7 super classes (72 %), an ensemble of the 18
gestures/classes, and additional videos (75 %).",0.11325765,0.09818639,-0.3705931,A
6577,"The
experiment where we completely omit the cross-entropy loss also helps gain insights on the working of HCL, and
provides a new dimension for further research.","The HCL experiments in
Section 4.2.2 and the ABF experiments in Section 4.2.3 further strengthen the claims mentioned in the paper.",We would like to thank our institution for providing us access to the GPU.,2022-05-18 07:01:05+00:00,[Re] Distilling Knowledge via Knowledge Review,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Apoorva Verma'), arxiv.Result.Author('Pranjal Gulati'), arxiv.Result.Author('Sarthak Gupta')]","This effort aims to reproduce the results of experiments and analyze the
robustness of the review framework for knowledge distillation introduced in the
CVPR '21 paper 'Distilling Knowledge via Knowledge Review' by Chen et al.
Previous works in knowledge distillation only studied connections paths between
the same levels of the student and the teacher, and cross-level connection
paths had not been considered. Chen et al. propose a new residual learning
framework to train a single student layer using multiple teacher layers. They
also design a novel fusion module to condense feature maps across levels and a
loss function to compare feature information stored across different levels to
improve performance. In this work, we consistently verify the improvements in
test accuracy across student models as reported in the original paper and study
the effectiveness of the novel modules introduced by conducting ablation
studies and new experiments.",0.38089305,0.08318044,0.10661134,A
6583,"The cause of the catastrophic forgetting
                                        agement Engineering Antonio Ruberti (DIAG), Sapienza University of Rome,  problem is that different tasks or datasets are independent
                                        00185, Rome, Italy, mail: valerio.marsocci@uniroma1.it                    but not identically distributed in the feature domain, as it is

                                           Simone Scardapane is with the Department of Information Engineering,
                                        Electronics and Telecommunication (DIET), Sapienza University of Rome,
                                        00184, Rome, Italy, mail: simone.scardapane@uniroma1.it
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING                   2

known that the distribution of various RS datasets vary greatly      ImageNet), and we expect this to become a useful benchmark
[2], due to different resolutions, acquisitions, textures, and       scenario for further research in SSL and CL in remote sensing.","Therefore, there is a
                                        (SSL) to deal with the lack of labeling information, satisfying           need to ensure that the newly-developed models have the abil-
                                                                                                                  ity to learn new tasks while retaining satisfying performance
                                           Valerio Marsocci is with the Department of Computer, Control and Man-  on previous ones.",captured scenes.,2022-05-23 14:02:12+00:00,Continual Barlow Twins: continual self-supervised learning for remote sensing semantic segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Valerio Marsocci'), arxiv.Result.Author('Simone Scardapane')]","In the field of Earth Observation (EO), Continual Learning (CL) algorithms
have been proposed to deal with large datasets by decomposing them into several
subsets and processing them incrementally. The majority of these algorithms
assume that data is (a) coming from a single source, and (b) fully labeled.
Real-world EO datasets are instead characterized by a large heterogeneity
(e.g., coming from aerial, satellite, or drone scenarios), and for the most
part they are unlabeled, meaning they can be fully exploited only through the
emerging Self-Supervised Learning (SSL) paradigm. For these reasons, in this
paper we propose a new algorithm for merging SSL and CL for remote sensing
applications, that we call Continual Barlow Twins (CBT). It combines the
advantages of one of the simplest self-supervision techniques, i.e., Barlow
Twins, with the Elastic Weight Consolidation method to avoid catastrophic
forgetting. In addition, for the first time we evaluate SSL methods on a highly
heterogeneous EO dataset, showing the effectiveness of these strategies on a
novel combination of three almost non-overlapping domains datasets (airborne
Potsdam dataset, satellite US3D dataset, and drone UAVid dataset), on a crucial
downstream task in EO, i.e., semantic segmentation. Encouraging results show
the superiority of SSL in this setting, and the effectiveness of creating an
incremental effective pretrained feature extractor, based on ResNet50, without
the need of relying on the complete availability of all the data, with a
valuable saving of time and resources.",0.0073217843,-0.11221072,3.3657998e-05,C
6597,"VCR dataset with 290K image-caption pairs per-
forms similarly to UNITER-L that requires over       5.3 Effect of the multimodal semantic graph

                                                     To further study the behavior of modules in the
                                                     multimodal semantic graph, and quantitatively
                                                     evaluate pretrained models used in this work
       Model         # Image-caption  Parameters          knowledge source                         Test Acc.","In contrast, VQA-GNN trained with       reasoning (¬ß5.4).","(%)

     ViLBERT          in pretraining      221M          # image-level  # concept-level  Q‚ÜíA QA‚ÜíR Q‚ÜíAR
    VLBERT-L                              383M
 UNITER-(B/L)              3.3M       154M/378M           -                 -              73.3       74.6          54.8
ERNIE-ViL-(B/L)            3.3M       212M/533M                                            75.8       78.4          59.7
     MERLOT                9.5M           223M            -                 -           75.0/77.3  77.2/80.8     58.2/62.8
RESERVE-(B/L)              3.8M       200M/644M                                         77.0/79.2  80.3/83.5     62.1/66.3
                           180M                           -                 -              80.6       80.4          65.1
                             1B                                                         79.3/84.0  78.7/84.9     62.6/72.0
                                                          -                 -

                                                          -                 -

                                                          -                 -

VQA-GNN-(B/L)        290k/1B          372M/1B     Visual scene graph,  ConceptNet       77.9/85.2 80.0/86.6 62.8/74.0
                                                    VisualGenome

Table 2: Accuracy scores on VCR.",2022-05-23 17:55:34+00:00,VQA-GNN: Reasoning with Multimodal Semantic Graph for Visual Question Answering,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Yanan Wang'), arxiv.Result.Author('Michihiro Yasunaga'), arxiv.Result.Author('Hongyu Ren'), arxiv.Result.Author('Shinya Wada'), arxiv.Result.Author('Jure Leskovec')]","Visual understanding requires seamless integration between recognition and
reasoning: beyond image-level recognition (e.g., detecting objects), systems
must perform concept-level reasoning (e.g., inferring the context of objects
and intents of people). However, existing methods only model the image-level
features, and do not ground them and reason with background concepts such as
knowledge graphs (KGs). In this work, we propose a novel visual question
answering method, VQA-GNN, which unifies the image-level information and
conceptual knowledge to perform joint reasoning of the scene. Specifically,
given a question-image pair, we build a scene graph from the image, retrieve a
relevant linguistic subgraph from ConceptNet and visual subgraph from
VisualGenome, and unify these three graphs and the question into one joint
graph, multimodal semantic graph. Our VQA-GNN then learns to aggregate messages
and reason across different modalities captured by the multimodal semantic
graph. In the evaluation on the VCR task, our method outperforms the previous
scene graph-based Trans-VL models by over 4%, and VQA-GNN-Large, our model that
fuses a Trans-VL further improves the state of the art by 2%, attaining the top
of the VCR leaderboard at the time of submission. This result suggests the
efficacy of our model in performing conceptual reasoning beyond image-level
recognition for visual understanding. Finally, we demonstrate that our model is
the first work to provide interpretability across visual and textual knowledge
domains for the VQA task.",0.055765115,-0.27532443,-0.033482723,C
6609,"Extensive experimental
results on three datasets demonstrate the effectiveness of DGMono3D, which can serve as a solid
baseline for industrial applications and further research on domain adaptation for Mono3D.","To alleviate this issue, we propose the 2D-3D geometry-consistent object scaling strategy
to scale objects based on statistical information during the training stage.","References

 [1] Garrick Brazil and Xiaoming Liu.",2022-05-23 23:05:07+00:00,Towards Model Generalization for Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Li'), arxiv.Result.Author('Zehui Chen'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Liangji Fang'), arxiv.Result.Author('Qinhong Jiang'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Junjun Jiang')]","Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.",-0.17302823,0.041436773,0.1199926,B
6610,How to reduce this dependence on statistical information worths further research.,"Moreover, for more challenging conditions where there are signiÔ¨Åcant discrepancies
in distributions of object sizes on the target domain (i.e., Nus‚ÜíKITTI), the statistical information for
GCOS is necessary.","16
Figure 9: Images augmented by the 2D-3D GCOS.",2022-05-23 23:05:07+00:00,Towards Model Generalization for Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Li'), arxiv.Result.Author('Zehui Chen'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Liangji Fang'), arxiv.Result.Author('Qinhong Jiang'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Junjun Jiang')]","Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.",-0.04913215,0.16031474,0.042123426,B
6611,"Extensive experimental
results on three datasets demonstrate the effectiveness of DGMono3D, which can serve as a solid
baseline for industrial applications and further research on domain adaptation for Mono3D.","To alleviate this issue, we propose the 2D-3D geometry-consistent object scaling strategy
to scale objects based on statistical information during the training stage.","References

 [1] Garrick Brazil and Xiaoming Liu.",2022-05-23 23:05:07+00:00,Towards Model Generalization for Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Li'), arxiv.Result.Author('Zehui Chen'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Liangji Fang'), arxiv.Result.Author('Qinhong Jiang'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Junjun Jiang')]","Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.",-0.17302823,0.041436773,0.1199926,B
6612,How to reduce this dependence on statistical information worths further research.,"Moreover, for more challenging conditions where there are signiÔ¨Åcant discrepancies
in distributions of object sizes on the target domain (i.e., Nus‚ÜíKITTI), the statistical information for
GCOS is necessary.","16
Figure 9: Images augmented by the 2D-3D GCOS.",2022-05-23 23:05:07+00:00,Towards Model Generalization for Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Li'), arxiv.Result.Author('Zehui Chen'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Liangji Fang'), arxiv.Result.Author('Qinhong Jiang'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Junjun Jiang')]","Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.",-0.04913215,0.16031474,0.042123426,B
6613,"Extensive experimental
results on three datasets demonstrate the effectiveness of DGMono3D, which can serve as a solid
baseline for industrial applications and further research on domain adaptation for Mono3D.","To alleviate this issue, we propose the 2D-3D geometry-consistent object scaling strategy
to scale objects based on statistical information during the training stage.","References

 [1] Garrick Brazil and Xiaoming Liu.",2022-05-23 23:05:07+00:00,Towards Model Generalization for Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Li'), arxiv.Result.Author('Zehui Chen'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Liangji Fang'), arxiv.Result.Author('Qinhong Jiang'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Junjun Jiang')]","Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.",-0.17302823,0.041436773,0.1199926,B
6614,How to reduce this dependence on statistical information worths further research.,"Moreover, for more challenging conditions where there are signiÔ¨Åcant discrepancies
in distributions of object sizes on the target domain (i.e., Nus‚ÜíKITTI), the statistical information for
GCOS is necessary.","16
Figure 9: Images augmented by the 2D-3D GCOS.",2022-05-23 23:05:07+00:00,Towards Model Generalization for Monocular 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Li'), arxiv.Result.Author('Zehui Chen'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Liangji Fang'), arxiv.Result.Author('Qinhong Jiang'), arxiv.Result.Author('Xianming Liu'), arxiv.Result.Author('Junjun Jiang')]","Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.",-0.04913215,0.16031474,0.042123426,B
6647,"On the one hand, we will carry on further research on the integration of a
third layout modality in our transformer-based multimodal model.",We will push forward two research lines for the future.,"As prior
related works rely mainly on the three vision, language, and layout modal-
ities to extract better cross-modality relations, we would like to propose a
better solution for layout integration in our vision-language model.",2022-05-24 12:28:12+00:00,VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Souhail Bakkali'), arxiv.Result.Author('Zuheng Ming'), arxiv.Result.Author('Mickael Coustaty'), arxiv.Result.Author('Mar√ßal Rusi√±ol'), arxiv.Result.Author('Oriol Ramos Terrades')]","Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream approach. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a common representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the common feature representation space}. Extensive
experiments on public document classification datasets demonstrate the
effectiveness and the generalization capacity of our model on both low-scale
and large-scale datasets.",-0.080461316,0.051418018,-0.065387726,C
6662,"The U.S. National Academies have
identiÔ¨Åed the Uranus Orbiter and Probe (UOP) as ‚Äúthe highest-priority new Flagship mission for initiation in the decade
2023-2032‚Äù [100], which makes Uranus a prime candidate for further study.","Both the gas
giants (Jupiter and Saturn) and the ice giants (Uranus and Neptune) have a large number of moons, and it is possible
to use LOS measurements to these moons to triangulate a spacecraft‚Äôs location.","A simulated scenario is created where
a spacecraft observes the moons Titania and Oberon, with a diÔ¨Äerent camera pointing in the direction of each of the
moons.",2022-05-24 16:54:07+00:00,Absolute Triangulation Algorithms for Space Exploration,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sebastien Henry'), arxiv.Result.Author('John A. Christian')]","Images are an important source of information for spacecraft navigation and
for three-dimensional reconstruction of observed space objects. Both of these
applications take the form of a triangulation problem when the camera has a
known attitude and the measurements extracted from the image are line of sight
(LOS) directions. This work provides a comprehensive review of the history and
theoretical foundations of triangulation. A variety of classical triangulation
algorithms are reviewed, including a number of suboptimal linear methods (many
LOS measurements) and the optimal method of Hartley and Sturm (only two LOS
measurements). Two new optimal non-iterative triangulation algorithms are
introduced that provide the same solution as Hartley and Sturm. The optimal
two-measurement case can be solved as a quadratic equation in many common
situations. The optimal many-measurement case may be solved without iteration
as a linear system using the new Linear Optimal Sine Triangulation (LOST)
method. The various triangulation algorithms are assessed with a few numerical
examples, including planetary terrain relative navigation, angles-only optical
navigation at Uranus, 3-D reconstruction of Notre-Dame de Paris, and
angles-only relative navigation.",0.047545146,0.26760444,-0.0818265,B
6663,"The U.S. National Academies have

identiÔ¨Åed the Uranus Orbiter and Probe (UOP) as ‚Äúthe highest-priority new Flagship mission for initiation in the decade

2023-2032‚Äù [109], which makes Uranus a prime candidate for further study.","Both the gas

giants (Jupiter and Saturn) and the ice giants (Uranus and Neptune) have a large number of moons, and it is possible

to use LOS measurements to these moons to triangulate a spacecraft‚Äôs location.","A simulated scenario is created where

a spacecraft observes the moons Titania and Oberon, with a diÔ¨Äerent camera pointing in the direction of each of the

moons.",2022-05-24 16:54:07+00:00,Absolute Triangulation Algorithms for Space Exploration,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sebastien Henry'), arxiv.Result.Author('John A. Christian')]","Images are an important source of information for spacecraft navigation and
for three-dimensional reconstruction of observed space objects. Both of these
applications take the form of a triangulation problem when the camera has a
known attitude and the measurements extracted from the image are line of sight
(LOS) directions. This work provides a comprehensive review of the history and
theoretical foundations of triangulation. A variety of classical triangulation
algorithms are reviewed, including a number of suboptimal linear methods (many
LOS measurements) and the optimal method of Hartley and Sturm (only two LOS
measurements). It is shown that the optimal many-measurement case may be solved
without iteration as a linear system using the new Linear Optimal Sine
Triangulation (LOST) method. Both LOST and the polynomial method of Hartley and
Sturm provide the same result in the case of only two measurements. The various
triangulation algorithms are assessed with a few numerical examples, including
planetary terrain relative navigation, angles-only optical navigation at
Uranus, 3-D reconstruction of Notre-Dame de Paris, and angles-only relative
navigation.",0.05210538,0.26468745,-0.08125756,B
6695,"14: return DID, DOOD
                                                               D. Ablation Study
   Though the main purpose of this work is to improve the
accuracy on the original test set when training with DA, we       The only difference between our method and LSR+RA
also collect the DAOOD dataset for potential further research  baseline in training is the label smoothing parameter.","We can see that ReSmooth can be
                                                               easily extended to more augmentations and improve on them.","In
like OOD generalization [46].",2022-05-25 09:29:27+00:00,ReSmooth: Detecting and Utilizing OOD Samples when Training with Data Augmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chenyang Wang'), arxiv.Result.Author('Junjun Jiang'), arxiv.Result.Author('Xiong Zhou'), arxiv.Result.Author('Xianming Liu')]","Data augmentation (DA) is a widely used technique for enhancing the training
of deep neural networks. Recent DA techniques which achieve state-of-the-art
performance always meet the need for diversity in augmented training samples.
However, an augmentation strategy that has a high diversity usually introduces
out-of-distribution (OOD) augmented samples and these samples consequently
impair the performance. To alleviate this issue, we propose ReSmooth, a
framework that firstly detects OOD samples in augmented samples and then
leverages them. To be specific, we first use a Gaussian mixture model to fit
the loss distribution of both the original and augmented samples and
accordingly split these samples into in-distribution (ID) samples and OOD
samples. Then we start a new training where ID and OOD samples are incorporated
with different smooth labels. By treating ID samples and OOD samples unequally,
we can make better use of the diverse augmented data. Further, we incorporate
our ReSmooth framework with negative data augmentation strategies. By properly
handling their intentionally created ODD samples, the classification
performance of negative data augmentations is largely ameliorated. Experiments
on several classification benchmarks show that ReSmooth can be easily extended
to existing augmentation strategies (such as RandAugment, rotate, and jigsaw)
and improve on them.",0.29258296,-0.04571142,0.0064043924,A
6696,"Though the main purpose of this work is to improve the
accuracy on the original test set when training with DA, we      C. ClassiÔ¨Åcation Performance
also collect the DAOOD dataset for potential further research
like OOD generalization [47].","What is more, even if the
B. DAOOD Dataset                                                 Algorithm 2 can select DAOOD samples more accurately than
                                                                 Algorithm 1, the collected dataset is still noisy.","DAOOD samples are semantic-           1) Diverse DA: From Table II, we can see that our method
invariant under the meaning of recognition and should be         consistently improves on the basic DA strategy RA w or
generalized in robust model.",2022-05-25 09:29:27+00:00,ReSmooth: Detecting and Utilizing OOD Samples when Training with Data Augmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chenyang Wang'), arxiv.Result.Author('Junjun Jiang'), arxiv.Result.Author('Xiong Zhou'), arxiv.Result.Author('Xianming Liu')]","Data augmentation (DA) is a widely used technique for enhancing the training
of deep neural networks. Recent DA techniques which achieve state-of-the-art
performance always meet the need for diversity in augmented training samples.
However, an augmentation strategy that has a high diversity usually introduces
out-of-distribution (OOD) augmented samples and these samples consequently
impair the performance. To alleviate this issue, we propose ReSmooth, a
framework that firstly detects OOD samples in augmented samples and then
leverages them. To be specific, we first use a Gaussian mixture model to fit
the loss distribution of both the original and augmented samples and
accordingly split these samples into in-distribution (ID) samples and OOD
samples. Then we start a new training where ID and OOD samples are incorporated
with different smooth labels. By treating ID samples and OOD samples unequally,
we can make better use of the diverse augmented data. Further, we incorporate
our ReSmooth framework with negative data augmentation strategies. By properly
handling their intentionally created OOD samples, the classification
performance of negative data augmentations is largely ameliorated. Experiments
on several classification benchmarks show that ReSmooth can be easily extended
to existing augmentation strategies (such as RandAugment, rotate, and jigsaw)
and improve on them. Our code is available at
https://github.com/Chenyang4/ReSmooth.",0.122091725,-0.12789409,-0.038454987,A
6743,"The train/test                         ASL         36.1      25.7
set will be released for further research.","Moreover, since there is no train/val                         P-GCN         24.9      23.3
split, we randomly select 10,000 images as the test set and the                                     23.6      10.9
rest 98,249 images are used as the training set.","CL         45.1      37.7
                                                                                    Partial BCE
   Since all the datasets have complete labels, we follow the                          SARB         87.7      84.5
setting of previous works [8], [9] to randomly drop a certain                                       87.3      84.6
proportion of positive and negative labels to create partially   Pascal  VOC  2007    SSGRL         86.5      84.7
annotated datasets.",2022-05-26 00:33:44+00:00,Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tao Pu'), arxiv.Result.Author('Tianshui Chen'), arxiv.Result.Author('Hefeng Wu'), arxiv.Result.Author('Yongyi Lu'), arxiv.Result.Author('Liang Lin')]","Despite achieving impressive progress, current multi-label image recognition
(MLR) algorithms heavily depend on large-scale datasets with complete labels,
making collecting large-scale datasets extremely time-consuming and
labor-intensive. Training the multi-label image recognition models with partial
labels (MLR-PL) is an alternative way to address this issue, in which merely
some labels are known while others are unknown for each image (see Figure 1).
However, current MLP-PL algorithms mainly rely on the pre-trained image
classification or similarity models to generate pseudo labels for the unknown
labels. Thus, they depend on a certain amount of data annotations and
inevitably suffer from obvious performance drops, especially when the known
label proportion is low. To address this dilemma, we propose a unified
semantic-aware representation blending (SARB) that consists of two crucial
modules to blend multi-granularity category-specific semantic representation
across different images to transfer information of known labels to complement
unknown labels. Extensive experiments on the MS-COCO, Visual Genome, and Pascal
VOC 2007 datasets show that the proposed SARB consistently outperforms current
state-of-the-art algorithms on all known label proportion settings. Concretely,
it obtain the average mAP improvement of 1.9%, 4.5%, 1.0% on the three
benchmark datasets compared with the second-best algorithm.",0.16723686,-0.11841068,0.095075905,A
6746,"F N s and F P s. Differently, our method based on game
theory optimizes false positive pixels and false negative pixels         We further study the spatiotemporal complexity of CNN-
separately.",The efÔ¨Åcient of MIM is fully proved.,The pixelGame is conducive to Ô¨Çexible selection of        based methods.,2022-05-26 03:13:27+00:00,PixelGame: Infrared small target segmentation as a Nash equilibrium,cs.CV,['cs.CV'],"[arxiv.Result.Author('Heng Zhou'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Zhenxi Zhang'), arxiv.Result.Author('Chengyang Li'), arxiv.Result.Author('Yongqiang Xie'), arxiv.Result.Author('Zhongbo Li')]","A key challenge of infrared small target segmentation (ISTS) is to balance
false negative pixels (FNs) and false positive pixels (FPs). Traditional
methods combine FNs and FPs into a single objective by weighted sum, and the
optimization process is decided by one actor. Minimizing FNs and FPs with the
same strategy leads to antagonistic decisions. To address this problem, we
propose a competitive game framework (pixelGame) from a novel perspective for
ISTS. In pixelGame, FNs and FPs are controlled by different player whose goal
is to minimize their own utility function. FNs-player and FPs-player are
designed with different strategies: One is to minimize FNs and the other is to
minimize FPs. The utility function drives the evolution of the two participants
in competition. We consider the Nash equilibrium of pixelGame as the optimal
solution. In addition, we propose maximum information modulation (MIM) to
highlight the tar-get information. MIM effectively focuses on the salient
region including small targets. Extensive experiments on two standard public
datasets prove the effectiveness of our method. Compared with other
state-of-the-art methods, our method achieves better performance in terms of
F1-measure (F1) and the intersection of union (IoU).",-0.030605247,-0.03083469,0.22626656,C
6757,"[20]    mantic segmentation of thermal images is very promising,
attempts to segment vehicles from UAV-based thermal im-        and further research can be advanced in several directions,
ages, so a Boltzmann machine is employed for geometry ex-      such as creating synthetic data, data augmentation, and fu-
traction from vehicles up view to increase the segmentation    sion strategies.","The se-
criminate important pixels while calculating the loss.",accuracy.,2022-05-26 11:32:15+00:00,Semantic Segmentation for Thermal Images: A Comparative Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Z√ºlfiye K√ºt√ºk'), arxiv.Result.Author('G√∂rkem Algan')]","Semantic segmentation is a challenging task since it requires excessively
more low-level spatial information of the image compared to other computer
vision problems. The accuracy of pixel-level classification can be affected by
many factors, such as imaging limitations and the ambiguity of object
boundaries in an image. Conventional methods exploit three-channel RGB images
captured in the visible spectrum with deep neural networks (DNN). Thermal
images can significantly contribute during the segmentation since thermal
imaging cameras are capable of capturing details despite the weather and
illumination conditions. Using infrared spectrum in semantic segmentation has
many real-world use cases, such as autonomous driving, medical imaging,
agriculture, defense industry, etc. Due to this wide range of use cases,
designing accurate semantic segmentation algorithms with the help of infrared
spectrum is an important challenge. One approach is to use both visible and
infrared spectrum images as inputs. These methods can accomplish higher
accuracy due to enriched input information, with the cost of extra effort for
the alignment and processing of multiple inputs. Another approach is to use
only thermal images, enabling less hardware cost for smaller use cases. Even
though there are multiple surveys on semantic segmentation methods, the
literature lacks a comprehensive survey centered explicitly around semantic
segmentation using infrared spectrum. This work aims to fill this gap by
presenting algorithms in the literature and categorizing them by their input
images.",-0.1554856,0.17376524,0.13031045,B
6758,"Despite the analyses
from the perspective of visual explainability, we think this                                                     iSQRT-COV [8]  84.7  89.6  91.4
problem is worth further research in our future work.","To validate this                             Backbone          Method         Birds [1] Aircrafts [22] Cars [21]
guess, we also apply our proposed explainability methods                             Architecture Pooling Layer
on the ResNet-50 of MPN-COV [7] trained on ImageNet
to evaluate the behavior of eigenvalues, which will be                               GCP ResNet-50 2nd-order GCP iMSQPRNT--CCOOVV[[78]] 8877..32 8990..55 9912..78
illustrated in the following sections.","ResNet-50 [63] 1st-order GAP MPN-COV [7]
                                                                                                                                84.3  89.9  91.7
TABLE 9: Comparison on validation accuracy with other
GCP methods on ImageNet.",2022-05-26 11:41:36+00:00,On the Eigenvalues of Global Covariance Pooling for Fine-grained Visual Recognition,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Yue Song'), arxiv.Result.Author('Nicu Sebe'), arxiv.Result.Author('Wei Wang')]","The Fine-Grained Visual Categorization (FGVC) is challenging because the
subtle inter-class variations are difficult to be captured. One notable
research line uses the Global Covariance Pooling (GCP) layer to learn powerful
representations with second-order statistics, which can effectively model
inter-class differences. In our previous conference paper, we show that
truncating small eigenvalues of the GCP covariance can attain smoother gradient
and improve the performance on large-scale benchmarks. However, on fine-grained
datasets, truncating the small eigenvalues would make the model fail to
converge. This observation contradicts the common assumption that the small
eigenvalues merely correspond to the noisy and unimportant information.
Consequently, ignoring them should have little influence on the performance. To
diagnose this peculiar behavior, we propose two attribution methods whose
visualizations demonstrate that the seemingly unimportant small eigenvalues are
crucial as they are in charge of extracting the discriminative class-specific
features. Inspired by this observation, we propose a network branch dedicated
to magnifying the importance of small eigenvalues. Without introducing any
additional parameters, this branch simply amplifies the small eigenvalues and
achieves state-of-the-art performances of GCP methods on three fine-grained
benchmarks. Furthermore, the performance is also competitive against other FGVC
approaches on larger datasets. Code is available at
\href{https://github.com/KingJamesSong/DifferentiableSVD}{https://github.com/KingJamesSong/DifferentiableSVD}.",0.07550597,-0.006367266,0.13037172,A
6761,"To further study the impact of diverse pos-     more diversity, which are often meaningful and compen-
itive pairs to self-supervised contrastive learning, we com-    sate to those generated by 3D CNN in contrastive learn-
pute average similarities of the positive pairs on UCF101       ing.",Positive pairs.,"Therefore, our cross-architecture can boost the per-
test split 1, as reported in Table 3.",2022-05-26 12:41:19+00:00,Cross-Architecture Self-supervised Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sheng Guo'), arxiv.Result.Author('Zihua Xiong'), arxiv.Result.Author('Yujie Zhong'), arxiv.Result.Author('Limin Wang'), arxiv.Result.Author('Xiaobo Guo'), arxiv.Result.Author('Bing Han'), arxiv.Result.Author('Weilin Huang')]","In this paper, we present a new cross-architecture contrastive learning
(CACL) framework for self-supervised video representation learning. CACL
consists of a 3D CNN and a video transformer which are used in parallel to
generate diverse positive pairs for contrastive learning. This allows the model
to learn strong representations from such diverse yet meaningful pairs.
Furthermore, we introduce a temporal self-supervised learning module able to
predict an Edit distance explicitly between two video sequences in the temporal
order. This enables the model to learn a rich temporal representation that
compensates strongly to the video-level representation learned by the CACL. We
evaluate our method on the tasks of video retrieval and action recognition on
UCF101 and HMDB51 datasets, where our method achieves excellent performance,
surpassing the state-of-the-art methods such as VideoMoCo and MoCo+BE by a
large margin. The code is made available at https://github.com/guoshengcv/CACL.",-0.02866575,-0.2358545,0.11776027,C
6772,"We further study the optimization procedures of VPT by monitoring the test accuracy of the training
stage.","For example, the accuracy of VPT keeps going up when the number of tunable parameters
increases up to 300K on SSv2, whereas it begins to drop when the number of tunable parameters
exceeds 50K on HMDB-51.","As shown in Figure 5, we gradually increase the number of tokens in VPT and plot the Top-1
accuracy of each epoch.",2022-05-26 17:56:15+00:00,AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shoufa Chen'), arxiv.Result.Author('Chongjian Ge'), arxiv.Result.Author('Zhan Tong'), arxiv.Result.Author('Jiangliu Wang'), arxiv.Result.Author('Yibing Song'), arxiv.Result.Author('Jue Wang'), arxiv.Result.Author('Ping Luo')]","Although the pre-trained Vision Transformers (ViTs) achieved great success in
computer vision, adapting a ViT to various image and video tasks is challenging
because of its heavy computation and storage burdens, where each model needs to
be independently and comprehensively fine-tuned to different tasks, limiting
its transferability in different domains. To address this challenge, we propose
an effective adaptation approach for Transformer, namely AdaptFormer, which can
adapt the pre-trained ViTs into many different image and video tasks
efficiently. It possesses several benefits more appealing than prior arts.
Firstly, AdaptFormer introduces lightweight modules that only add less than 2%
extra parameters to a ViT, while it is able to increase the ViT's
transferability without updating its original pre-trained parameters,
significantly outperforming the existing 100% fully fine-tuned models on action
recognition benchmarks. Secondly, it can be plug-and-play in different
Transformers and scalable to many visual tasks. Thirdly, extensive experiments
on five image and video datasets show that AdaptFormer largely improves ViTs in
the target domains. For example, when updating just 1.5% extra parameters, it
achieves about 10% and 19% relative improvement compared to the fully
fine-tuned models on Something-Something~v2 and HMDB51, respectively. Project
page: http://www.shoufachen.com/adaptformer-page.",0.31083113,-0.086226285,0.1737418,A
6773,"We further study the optimization procedures of VPT by monitoring the test accuracy of the training
stage.","For example, the accuracy of VPT keeps going up when the number of tunable parameters
increases up to 300K on SSv2, whereas it begins to drop when the number of tunable parameters
exceeds 50K on HMDB-51.","As shown in Figure 5, we gradually increase the number of tokens in VPT and plot the Top-1
accuracy of each epoch.",2022-05-26 17:56:15+00:00,AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shoufa Chen'), arxiv.Result.Author('Chongjian Ge'), arxiv.Result.Author('Zhan Tong'), arxiv.Result.Author('Jiangliu Wang'), arxiv.Result.Author('Yibing Song'), arxiv.Result.Author('Jue Wang'), arxiv.Result.Author('Ping Luo')]","Pretraining Vision Transformers (ViTs) has achieved great success in visual
recognition. A following scenario is to adapt a ViT to various image and video
recognition tasks. The adaptation is challenging because of heavy computation
and memory storage. Each model needs an independent and complete finetuning
process to adapt to different tasks, which limits its transferability to
different visual domains. To address this challenge, we propose an effective
adaptation approach for Transformer, namely AdaptFormer, which can adapt the
pre-trained ViTs into many different image and video tasks efficiently. It
possesses several benefits more appealing than prior arts. Firstly, AdaptFormer
introduces lightweight modules that only add less than 2% extra parameters to a
ViT, while it is able to increase the ViT's transferability without updating
its original pre-trained parameters, significantly outperforming the existing
100\% fully fine-tuned models on action recognition benchmarks. Secondly, it
can be plug-and-play in different Transformers and scalable to many visual
tasks. Thirdly, extensive experiments on five image and video datasets show
that AdaptFormer largely improves ViTs in the target domains. For example, when
updating just 1.5% extra parameters, it achieves about 10% and 19% relative
improvement compared to the fully fine-tuned models on Something-Something~v2
and HMDB51, respectively. Code is available at
https://github.com/ShoufaChen/AdaptFormer.",0.31083113,-0.086226285,0.1737418,A
6774,"We further study the optimization procedures of VPT by monitoring the test accuracy of the training
stage.","For example, the accuracy of VPT keeps going up when the number of tunable parameters
increases up to 300K on SSv2, whereas it begins to drop when the number of tunable parameters
exceeds 50K on HMDB-51.","As shown in Figure 5, we gradually increase the number of tokens in VPT and plot the Top-1
accuracy of each epoch.",2022-05-26 17:56:15+00:00,AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shoufa Chen'), arxiv.Result.Author('Chongjian Ge'), arxiv.Result.Author('Zhan Tong'), arxiv.Result.Author('Jiangliu Wang'), arxiv.Result.Author('Yibing Song'), arxiv.Result.Author('Jue Wang'), arxiv.Result.Author('Ping Luo')]","Pretraining Vision Transformers (ViTs) has achieved great success in visual
recognition. A following scenario is to adapt a ViT to various image and video
recognition tasks. The adaptation is challenging because of heavy computation
and memory storage. Each model needs an independent and complete finetuning
process to adapt to different tasks, which limits its transferability to
different visual domains. To address this challenge, we propose an effective
adaptation approach for Transformer, namely AdaptFormer, which can adapt the
pre-trained ViTs into many different image and video tasks efficiently. It
possesses several benefits more appealing than prior arts. Firstly, AdaptFormer
introduces lightweight modules that only add less than 2% extra parameters to a
ViT, while it is able to increase the ViT's transferability without updating
its original pre-trained parameters, significantly outperforming the existing
100\% fully fine-tuned models on action recognition benchmarks. Secondly, it
can be plug-and-play in different Transformers and scalable to many visual
tasks. Thirdly, extensive experiments on five image and video datasets show
that AdaptFormer largely improves ViTs in the target domains. For example, when
updating just 1.5% extra parameters, it achieves about 10% and 19% relative
improvement compared to the fully fine-tuned models on Something-Something~v2
and HMDB51, respectively. Code is available at
https://github.com/ShoufaChen/AdaptFormer.",0.31083113,-0.086226285,0.1737418,A
6794,"tailed distributed data challenging in a number of ways:

 ‚Äì Beyond the existing research scope, we further study the          From the perspective of model learning.","‚Äì We have studied four quantitative metrics to evaluate
    imbalance, deeply compared their characteristics, and            The long-tailed phenomenon is inherently present in
    proposed to use Gini coefÔ¨Åcient to evaluate the long-        large vocabulary scenarios, making model learning with long-
    tailedness of a dataset.","First, since the
    long-tailed phenomenon of 20 widely-used and large-          data in the tail classes is usually insufÔ¨Åcient to represent
    scale visual datasets proposed in the last decade, and       its true distribution, this poses a signiÔ¨Åcant challenge for
    reveal that this problem has not been fully studied in       classiÔ¨Åers: a good classiÔ¨Åer aims to provide a good deci-
    some Ô¨Åelds.",2022-05-27 06:22:55+00:00,A Survey on Long-Tailed Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lu Yang'), arxiv.Result.Author('He Jiang'), arxiv.Result.Author('Qing Song'), arxiv.Result.Author('Jun Guo')]","The heavy reliance on data is one of the major reasons that currently limit
the development of deep learning. Data quality directly dominates the effect of
deep learning models, and the long-tailed distribution is one of the factors
affecting data quality. The long-tailed phenomenon is prevalent due to the
prevalence of power law in nature. In this case, the performance of deep
learning models is often dominated by the head classes while the learning of
the tail classes is severely underdeveloped. In order to learn adequately for
all classes, many researchers have studied and preliminarily addressed the
long-tailed problem. In this survey, we focus on the problems caused by
long-tailed data distribution, sort out the representative long-tailed visual
recognition datasets and summarize some mainstream long-tailed studies.
Specifically, we summarize these studies into ten categories from the
perspective of representation learning, and outline the highlights and
limitations of each category. Besides, we have studied four quantitative
metrics for evaluating the imbalance, and suggest using the Gini coefficient to
evaluate the long-tailedness of a dataset. Based on the Gini coefficient, we
quantitatively study 20 widely-used and large-scale visual datasets proposed in
the last decade, and find that the long-tailed phenomenon is widespread and has
not been fully studied. Finally, we provide several future directions for the
development of long-tailed learning to provide more ideas for readers.",0.047304906,-0.31781393,-0.09415981,C
6795,"6, we further study the        item distributions [164, 189].",In Sec.,"Using conventional methods
long-tailed phenomenon of mainstream large-scale visual          (e.g., Cross-Entropy loss, or simple Ô¨Åne-tuning, etc.)",2022-05-27 06:22:55+00:00,A Survey on Long-Tailed Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lu Yang'), arxiv.Result.Author('He Jiang'), arxiv.Result.Author('Qing Song'), arxiv.Result.Author('Jun Guo')]","The heavy reliance on data is one of the major reasons that currently limit
the development of deep learning. Data quality directly dominates the effect of
deep learning models, and the long-tailed distribution is one of the factors
affecting data quality. The long-tailed phenomenon is prevalent due to the
prevalence of power law in nature. In this case, the performance of deep
learning models is often dominated by the head classes while the learning of
the tail classes is severely underdeveloped. In order to learn adequately for
all classes, many researchers have studied and preliminarily addressed the
long-tailed problem. In this survey, we focus on the problems caused by
long-tailed data distribution, sort out the representative long-tailed visual
recognition datasets and summarize some mainstream long-tailed studies.
Specifically, we summarize these studies into ten categories from the
perspective of representation learning, and outline the highlights and
limitations of each category. Besides, we have studied four quantitative
metrics for evaluating the imbalance, and suggest using the Gini coefficient to
evaluate the long-tailedness of a dataset. Based on the Gini coefficient, we
quantitatively study 20 widely-used and large-scale visual datasets proposed in
the last decade, and find that the long-tailed phenomenon is widespread and has
not been fully studied. Finally, we provide several future directions for the
development of long-tailed learning to provide more ideas for readers.",0.1665622,-0.007891364,-0.04304558,A
6847,"A large percentage         To further study the role of intra-object semantics on pre-
of patches are trivial, and we could fast pre-train our model     training models, we introduce a new selection strategy to
by simply removing them.","unpredictable, such as the background.","select the source region from one object and select the target
                                                                  region from another object as shown in Figure 6.",2022-05-28 05:13:45+00:00,Object-wise Masked Autoencoders for Fast Pre-training,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jiantao Wu'), arxiv.Result.Author('Shentong Mo')]","Self-supervised pre-training for images without labels has recently achieved
promising performance in image classification. The success of transformer-based
methods, ViT and MAE, draws the community's attention to the design of backbone
architecture and self-supervised task. In this work, we show that current
masked image encoding models learn the underlying relationship between all
objects in the whole scene, instead of a single object representation.
Therefore, those methods bring a lot of compute time for self-supervised
pre-training. To solve this issue, we introduce a novel object selection and
division strategy to drop non-object patches for learning object-wise
representations by selective reconstruction with interested region masks. We
refer to this method ObjMAE. Extensive experiments on four commonly-used
datasets demonstrate the effectiveness of our model in reducing the compute
cost by 72% while achieving competitive performance. Furthermore, we
investigate the inter-object and intra-object relationship and find that the
latter is crucial for self-supervised pre-training.",-0.123711,-0.150188,-0.026223734,C
6872,"However, given the efÔ¨Åciency and the various applications of dilated convolution, we believe that
how to more properly apply it to optical Ô¨Çow networks is still a question worth further studying.",(2) Irrelevant information across large distances due to the sparse sample of input.,Runtime comparison.,2022-05-29 10:36:39+00:00,SKFlow: Learning Optical Flow with Super Kernels,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Shangkun Sun'), arxiv.Result.Author('Yuanqi Chen'), arxiv.Result.Author('Yu Zhu'), arxiv.Result.Author('Guodong Guo'), arxiv.Result.Author('Ge Li')]","Optical flow estimation is a classical yet challenging task in computer
vision. One of the essential factors in accurately predicting optical flow is
to alleviate occlusions between frames. However, it is still a thorny problem
for current top-performing optical flow estimation methods due to insufficient
local evidence to model occluded areas. In this paper, we propose the Super
Kernel Flow Network (SKFlow), a CNN architecture to ameliorate the impacts of
occlusions on optical flow estimation. SKFlow benefits from the super kernels
which bring enlarged receptive fields to complement the absent matching
information and recover the occluded motions. We present efficient super kernel
designs by utilizing conical connections and hybrid depth-wise convolutions.
Extensive experiments demonstrate the effectiveness of SKFlow on multiple
benchmarks, especially in the occluded areas. Without pre-trained backbones on
ImageNet and with a modest increase in computation, SKFlow achieves compelling
performance and ranks $\textbf{1st}$ among currently published methods on the
Sintel benchmark. On the challenging Sintel clean and final passes (test),
SKFlow surpasses the best-published result in the unmatched areas ($7.96$ and
$12.50$) by $9.09\%$ and $7.92\%$. The code is available at
\href{https://github.com/littlespray/SKFlow}{https://github.com/littlespray/SKFlow}.",0.0063654557,0.12614715,0.36883497,B
6875,"[15] P. Follmann, R. Ko¬®nig, P. Ha¬®rtinger, M. Klostermann, and T. Bo¬®ttger,
Finally, we have made the code and models publicly available                          ‚ÄúLearning to see the invisible: End-to-end trainable amodal instance
to accelerate further research in this area.","11 612‚Äì
detailed ablation studies and qualitative evaluations highlighting                    11 621.
the improvements that we make to various core network
modules of our amodal panoptic segmentation architectures.","segmentation,‚Äù in IEEE Winter Conference on Applications of Computer
                                                                                      Vision, 2019, pp.",2022-05-29 12:05:07+00:00,Perceiving the Invisible: Proposal-Free Amodal Panoptic Segmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Rohit Mohan'), arxiv.Result.Author('Abhinav Valada')]","Amodal panoptic segmentation aims to connect the perception of the world to
its cognitive understanding. It entails simultaneously predicting the semantic
labels of visible scene regions and the entire shape of traffic participant
instances, including regions that may be occluded. In this work, we formulate a
proposal-free framework that tackles this task as a multi-label and multi-class
problem by first assigning the amodal masks to different layers according to
their relative occlusion order and then employing amodal instance regression on
each layer independently while learning background semantics. We propose the
\net architecture that incorporates a shared backbone and an asymmetrical
dual-decoder consisting of several modules to facilitate within-scale and
cross-scale feature aggregations, bilateral feature propagation between
decoders, and integration of global instance-level and local pixel-level
occlusion reasoning. Further, we propose the amodal mask refiner that resolves
the ambiguity in complex occlusion scenarios by explicitly leveraging the
embedding of unoccluded instance masks. Extensive evaluation on the BDD100K-APS
and KITTI-360-APS datasets demonstrate that our approach set the new
state-of-the-art on both benchmarks.",-0.1320389,0.078093536,0.003093388,B
6898,"Driven by this problem, more
                                        conduct a comprehensive comparison of existing methods              practical benchmarks has been proposed, the most inÔ¨Çuen-
                                        to inspire further research.",It is the time to        ject class distinctions [4].,"This paper extensively com-            tial one is the MVTec AD dataset [4], which provides sev-
                                        pares 13 papers in terms of the performance in unsuper-             eral Ô¨Åne-grained anomalies with corresponding pixel-level
                                        vised anomaly detection and localization tasks, and adds a          anomaly segmentation annotation.",2022-05-30 04:57:25+00:00,Benchmarking Unsupervised Anomaly Detection and Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ye Zheng'), arxiv.Result.Author('Xiang Wang'), arxiv.Result.Author('Yu Qi'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Liwei Wu')]","Unsupervised anomaly detection and localization, as of one the most practical
and challenging problems in computer vision, has received great attention in
recent years. From the time the MVTec AD dataset was proposed to the present,
new research methods that are constantly being proposed push its precision to
saturation. It is the time to conduct a comprehensive comparison of existing
methods to inspire further research. This paper extensively compares 13 papers
in terms of the performance in unsupervised anomaly detection and localization
tasks, and adds a comparison of inference efficiency previously ignored by the
community. Meanwhile, analysis of the MVTec AD dataset are also given,
especially the label ambiguity that affects the model fails to achieve full
marks. Moreover, considering the proposal of the new MVTec 3D-AD dataset, this
paper also conducts experiments using the existing state-of-the-art 2D methods
on this new dataset, and reports the corresponding results with analysis.",-0.062167093,-0.008778396,0.052653052,B
6948,"limited because the constructed facial atlas can only be
                                        We provide a link to a PyPi program at the end of this          applied to T1-Weighted MRI images, which encompasses
                                        manuscript to encourage further research into the applica-      a very limited range of voxel intensities.","However, these techniques are
                                        put versus the ground truth images produced by Pydeface.","Pydeface, on the
                                        tion of deep learning to MRI anonymization.",2022-05-31 04:51:00+00:00,DeepDefacer: Automatic Removal of Facial Features via U-Net Image Segmentation,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Anish Khazane'), arxiv.Result.Author('Julien Hoachuck'), arxiv.Result.Author('Krzysztof J. Gorgolewski'), arxiv.Result.Author('Russell A. Poldrack')]","Recent advancements in the field of magnetic resonance imaging (MRI) have
enabled large-scale collaboration among clinicians and researchers for
neuroimaging tasks. However, researchers are often forced to use outdated and
slow software to anonymize MRI images for publication. These programs
specifically perform expensive mathematical operations over 3D images that
rapidly slow down anonymization speed as an image's volume increases in size.
In this paper, we introduce DeepDefacer, an application of deep learning to MRI
anonymization that uses a streamlined 3D U-Net network to mask facial regions
in MRI images with a significant increase in speed over traditional
de-identification software. We train DeepDefacer on MRI images from the Brain
Development Organization (IXI) and International Consortium for Brain Mapping
(ICBM) and quantitatively evaluate our model against a baseline 3D U-Net model
with regards to Dice, recall, and precision scores. We also evaluate
DeepDefacer against Pydeface, a traditional defacing application, with regards
to speed on a range of CPU and GPU devices and qualitatively evaluate our
model's defaced output versus the ground truth images produced by Pydeface. We
provide a link to a PyPi program at the end of this manuscript to encourage
further research into the application of deep learning to MRI anonymization.",-0.074911326,-0.042058837,0.11258483,C
6954,"We further study the im-               ate smoother surface with fewer sampling points at training
pact of different sampling strategies on the performance of           time.","Compared to 3-
                                                                      way classiÔ¨Åcation, such a method has the potential to gener-
Different sampling strategies.","However, it is not as robust as the 3-way counterpart
                                                                      as it requires the results of the two branches align well in or-
                                                                      der to prevent holes and artifacts.",2022-05-31 07:24:04+00:00,3PSDF: Three-Pole Signed Distance Function for Learning Surfaces with Arbitrary Topologies,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Weikai Chen'), arxiv.Result.Author('Cheng Lin'), arxiv.Result.Author('Weiyang Li'), arxiv.Result.Author('Bo Yang')]","Recent advances in learning 3D shapes using neural implicit functions have
achieved impressive results by breaking the previous barrier of resolution and
diversity for varying topologies. However, most of such approaches are limited
to closed surfaces as they require the space to be divided into inside and
outside. More recent works based on unsigned distance function have been
proposed to handle complex geometry containing both the open and closed
surfaces. Nonetheless, as their direct outputs are point clouds, robustly
obtaining high-quality meshing results from discrete points remains an open
question. We present a novel learnable implicit representation, called the
three-pole signed distance function (3PSDF), that can represent non-watertight
3D shapes with arbitrary topologies while supporting easy field-to-mesh
conversion using the classic Marching Cubes algorithm. The key to our method is
the introduction of a new sign, the NULL sign, in addition to the conventional
in and out labels. The existence of the null sign could stop the formation of a
closed isosurface derived from the bisector of the in/out regions. Further, we
propose a dedicated learning framework to effectively learn 3PSDF without
worrying about the vanishing gradient due to the null labels. Experimental
results show that our approach outperforms the previous state-of-the-art
methods in a wide range of benchmarks both quantitatively and qualitatively.",0.16360407,0.25712645,0.12845823,A
6964,We further study the convergence speed of            Figure 6: Convergence speed.,Convergence speed.,"our proposed method, Geo-Neus, and baseline, NeuS.",2022-05-31 14:52:07+00:00,Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Qiancheng Fu'), arxiv.Result.Author('Qingshan Xu'), arxiv.Result.Author('Yew-Soon Ong'), arxiv.Result.Author('Wenbing Tao')]","Recently, neural implicit surfaces learning by volume rendering has become
popular for multi-view reconstruction. However, one key challenge remains:
existing approaches lack explicit multi-view geometry constraints, hence
usually fail to generate geometry consistent surface reconstruction. To address
this challenge, we propose geometry-consistent neural implicit surfaces
learning for multi-view reconstruction. We theoretically analyze that there
exists a gap between the volume rendering integral and point-based signed
distance function (SDF) modeling. To bridge this gap, we directly locate the
zero-level set of SDF networks and explicitly perform multi-view geometry
optimization by leveraging the sparse geometry from structure from motion (SFM)
and photometric consistency in multi-view stereo. This makes our SDF
optimization unbiased and allows the multi-view geometry constraints to focus
on the true surface optimization. Extensive experiments show that our proposed
method achieves high-quality surface reconstruction in both complex thin
structures and large smooth regions, thus outperforming the state-of-the-arts
by a large margin.",0.19172846,0.23749709,0.041074995,A
6973,"A two-stage network brings more computational cost compared with beneÔ¨Åts, a better way of
cascading need further research.","There is still room for further experimental optimizations of the optimal EXIF information combina-
tion.","9
6 Appendix

6.1 EXIF in CSRNet

In LCCNet, EXIF has been proven to work in AIR, and naturally, it can also be added to other auto
retouching networks to make the output more in line with human aesthetics.",2022-05-31 17:58:28+00:00,Cascade Luminance and Chrominance for Image Retouching: More Like Artist,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hailong Ma'), arxiv.Result.Author('Sibo Feng'), arxiv.Result.Author('Xi Xiao'), arxiv.Result.Author('Chenyu Dong'), arxiv.Result.Author('Xingyue Cheng')]","Photo retouching aims to adjust the luminance, contrast, and saturation of
the image to make it more human aesthetically desirable. However, artists'
actions in photo retouching are difficult to quantitatively analyze. By
investigating their retouching behaviors, we propose a two-stage network that
brightens images first and then enriches them in the chrominance plane. Six
pieces of useful information from image EXIF are picked as the network's
condition input. Additionally, hue palette loss is added to make the image more
vibrant. Based on the above three aspects, Luminance-Chrominance Cascading
Net(LCCNet) makes the machine learning problem of mimicking artists in photo
retouching more reasonable. Experiments show that our method is effective on
the benchmark MIT-Adobe FiveK dataset, and achieves state-of-the-art
performance for both quantitative and qualitative evaluation.",-0.0045954315,0.09832146,0.12266455,C
6980,"We hope that the introduced FHIST dataset, along with the
evaluations of state-of-the-art methods, inspires further research in this direction, and help in building
more realistic evaluations and fairer comparisons of few-shot learning methods.","Our Ô¨Ånding is in line with recent observations in the context of few-shot classiÔ¨Åcation of
natural images with domains shifts [11].","In a future work, we
intend to investigate the degree to which different tissue types correlate with each other, within and
across datasets.",2022-05-31 20:03:40+00:00,FHIST: A Benchmark for Few-shot Classification of Histological Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fereshteh Shakeri'), arxiv.Result.Author('Malik Boudiaf'), arxiv.Result.Author('Sina Mohammadi'), arxiv.Result.Author('Ivaxi Sheth'), arxiv.Result.Author('Mohammad Havaei'), arxiv.Result.Author('Ismail Ben Ayed'), arxiv.Result.Author('Samira Ebrahimi Kahou')]","Few-shot learning has recently attracted wide interest in image
classification, but almost all the current public benchmarks are focused on
natural images. The few-shot paradigm is highly relevant in medical-imaging
applications due to the scarcity of labeled data, as annotations are expensive
and require specialized expertise. However, in medical imaging, few-shot
learning research is sparse, limited to private data sets and is at its early
stage. In particular, the few-shot setting is of high interest in histology due
to the diversity and fine granularity of cancer related tissue classification
tasks, and the variety of data-preparation techniques. This paper introduces a
highly diversified public benchmark, gathered from various public datasets, for
few-shot histology data classification. We build few-shot tasks and
base-training data with various tissue types, different levels of domain shifts
stemming from various cancer sites, and different class-granularity levels,
thereby reflecting realistic scenarios. We evaluate the performances of
state-of-the-art few-shot learning methods on our benchmark, and observe that
simple fine-tuning and regularization methods achieve better results than the
popular meta-learning and episodic-training paradigm. Furthermore, we introduce
three scenarios based on the domain shifts between the source and target
histology data: near-domain, middle-domain and out-domain. Our experiments
display the potential of few-shot learning in histology classification, with
state-of-art few shot learning methods approaching the supervised-learning
baselines in the near-domain setting. In our out-domain setting, for 5-way
5-shot, the best performing method reaches 60% accuracy. We believe that our
work could help in building realistic evaluations and fair comparisons of
few-shot learning methods and will further encourage research in the few-shot
paradigm.",-0.12509009,-0.14692259,-0.059580695,C
6981,"Figure 4 compares METEOR score
   We further study the robustness of VALHALLA frame-                 of text-only baseline and VALHALLA as a function of context
work for machine translation under limited textual context            length k. On both EN‚ÜíDE and EN‚ÜíFR tasks, VALHALLA
by degrading the input language modality during training              consistently outperforms the baseline under all settings.","Translation Under Limited Textual Context
                                                                      Progressive Masking.","The
and inference in two ways [5]: (1) Progressive masking that           gap between both methods widens as context size is reduced,
replaces all but the Ô¨Årst k words of source sentences with a          with VALHALLA performing ‚àº 3 METEOR points better.",2022-05-31 20:25:15+00:00,VALHALLA: Visual Hallucination for Machine Translation,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Yi Li'), arxiv.Result.Author('Rameswar Panda'), arxiv.Result.Author('Yoon Kim'), arxiv.Result.Author('Chun-Fu Chen'), arxiv.Result.Author('Rogerio Feris'), arxiv.Result.Author('David Cox'), arxiv.Result.Author('Nuno Vasconcelos')]","Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: http://www.svcl.ucsd.edu/projects/valhalla.",0.030939648,-0.19828355,-0.117284715,C
6999,"A.2 Additional Ablation Study

In addition to the ablation studies provided in the main paper, we further study the effectiveness of
each component in our proposed EfÔ¨Åcient-CLS in the following sections.","We
concatenate the videos as one long video stream, and use the same ordering as OAK to construct the
train and test data splits.","A.2.1 Effect of EMA and Pseudo-labeling

In the main text, we studied the effect of EMA and pseudo-labeling at 12.5% annotation costs
(Section 5.3).",2022-06-01 08:22:34+00:00,Label-Efficient Online Continual Object Detection in Streaming Video,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jay Zhangjie Wu'), arxiv.Result.Author('David Junhao Zhang'), arxiv.Result.Author('Wynne Hsu'), arxiv.Result.Author('Mengmi Zhang'), arxiv.Result.Author('Mike Zheng Shou')]","To thrive in evolving environments, humans are capable of continual
acquisition and transfer of new knowledge, from a continuous video stream, with
minimal supervisions, while retaining previously learnt experiences. In
contrast to human learning, most standard continual learning benchmarks focus
on learning from static iid images in fully supervised settings. Here, we
examine a more realistic and challenging
problem$\unicode{x2014}$Label-Efficient Online Continual Object Detection
(LEOCOD) in video streams. By addressing this problem, it would greatly benefit
many real-world applications with reduced annotation costs and retraining time.
To tackle this problem, we seek inspirations from complementary learning
systems (CLS) in human brains and propose a computational model, dubbed as
Efficient-CLS. Functionally correlated with the hippocampus and the neocortex
in CLS, Efficient-CLS posits a memory encoding mechanism involving
bidirectional interaction between fast and slow learners via synaptic weight
transfers and pattern replays. We test Efficient-CLS and competitive baselines
in two challenging real-world video stream datasets. Like humans, Efficient-CLS
learns to detect new object classes incrementally from a continuous temporal
stream of non-repeating video with minimal forgetting. Remarkably, with only
25% annotated video frames, our Efficient-CLS still leads among all comparative
models, which are trained with 100% annotations on all video frames. The data
and source code will be publicly available at
https://github.com/showlab/Efficient-CLS.",0.2577697,-0.070780195,-0.31719905,A
7017,"Such a synthetic amodal dataset can be obtained
tate further research on amodal segmentation in automotive       via copy-paste of instances into the target images.","Particularly,      This can be adapted to many datasets to simulate amodal
we provide code2 for automatic dataset generation to facili-     labeling.","This has
perception tasks.",2022-06-01 14:38:33+00:00,"Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline",cs.CV,['cs.CV'],"[arxiv.Result.Author('Jasmin Breitenstein'), arxiv.Result.Author('Tim Fingscheidt')]","Amodal perception terms the ability of humans to imagine the entire shapes of
occluded objects. This gives humans an advantage to keep track of everything
that is going on, especially in crowded situations. Typical perception
functions, however, lack amodal perception abilities and are therefore at a
disadvantage in situations with occlusions. Complex urban driving scenarios
often experience many different types of occlusions and, therefore, amodal
perception for automated vehicles is an important task to investigate. In this
paper, we consider the task of amodal semantic segmentation and propose a
generic way to generate datasets to train amodal semantic segmentation methods.
We use this approach to generate an amodal Cityscapes dataset. Moreover, we
propose and evaluate a method as baseline on Amodal Cityscapes, showing its
applicability for amodal semantic segmentation in automotive environment
perception. We provide the means to re-generate this dataset on github.",-0.24374855,-0.021619752,-0.09052171,B
7020,"This work has been supported by the Academy of
   To further study the relationships between the       Finland in projects 317388, 329268 and 345791.
retrieval-based adaptation and the captioning accu-     We also acknowledge the computational resources
racy, we Ô¨Åne-tune the models from the adaptation        provided by both the Aalto Science-IT project and
stage on the captioning task with the frozen image      CSC ‚Äì IT Center for Science, Finland.","We owe it to the fact that the global infor-
mation represented by two separate image embed-         Acknowledgments
dings fails to localize the changes between them.",encoder.,2022-06-01 17:02:08+00:00,CLIP4IDC: CLIP for Image Difference Captioning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zixin Guo'), arxiv.Result.Author('Tzu-Jui Julius Wang'), arxiv.Result.Author('Jorma Laaksonen')]","Image Difference Captioning (IDC) aims at generating sentences to describe
differences between two similar-looking images. Conventional approaches learn
an IDC model with a pre-trained and usually frozen visual feature extractor.
Accordingly, two major issues may arise: (1) a large domain gap usually exists
between the pre-training datasets used for training such a visual encoder and
that of the downstream IDC task, and (2) the visual feature extractor, when
separately encoding two images, often does not effectively encode the visual
changes between two images. Due to the excellent zero-shot performance of the
recently proposed CLIP, we thus propose CLIP4IDC to transfer a CLIP model for
the IDC task to address those issues. Different from directly fine-tuning CLIP
to generate sentences, we introduce an adaptation training process to adapt
CLIP's visual encoder to capture and align differences in image pairs based on
the textual descriptions. Experiments on three IDC benchmark datasets,
CLEVR-Change, Spot-the-Diff, and Image-Editing-Request, demonstrate the
effectiveness of CLIP4IDC.",-0.14064875,-0.17539018,-0.17072219,C
7039,"Under the at-
tributes of IA, AI, DS, SV, ABR, and BC, our method is better                                     In order to further study the role of synthetic data, we set
than other methods in RA score, which further proves the                                       up an ablation experiment to gradually increase the proportion
diversity of our synthetic enlarged license plates and simulates                               of training data, including 0, 2k, 5K and 7K respectively, and
the real data better.","Note                                      D. Ablation Study
that RA is more important evaluation metric.",evaluate them on testing data.,2022-06-02 03:26:50+00:00,Disentangled Generation Network for Enlarged License Plate Recognition and A Unified Dataset,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chenglong Li'), arxiv.Result.Author('Xiaobin Yang'), arxiv.Result.Author('Guohao Wang'), arxiv.Result.Author('Aihua Zheng'), arxiv.Result.Author('Chang Tan'), arxiv.Result.Author('Ruoran Jia'), arxiv.Result.Author('Jin Tang')]","License plate recognition plays a critical role in many practical
applications, but license plates of large vehicles are difficult to be
recognized due to the factors of low resolution, contamination, low
illumination, and occlusion, to name a few. To overcome the above factors, the
transportation management department generally introduces the enlarged license
plate behind the rear of a vehicle. However, enlarged license plates have high
diversity as they are non-standard in position, size, and style. Furthermore,
the background regions contain a variety of noisy information which greatly
disturbs the recognition of license plate characters. Existing works have not
studied this challenging problem. In this work, we first address the enlarged
license plate recognition problem and contribute a dataset containing 9342
images, which cover most of the challenges of real scenes. However, the created
data are still insufficient to train deep methods of enlarged license plate
recognition, and building large-scale training data is very time-consuming and
high labor cost. To handle this problem, we propose a novel task-level
disentanglement generation framework based on the Disentangled Generation
Network (DGNet), which disentangles the generation into the text generation and
background generation in an end-to-end manner to effectively ensure diversity
and integrity, for robust enlarged license plate recognition. Extensive
experiments on the created dataset are conducted, and we demonstrate the
effectiveness of the proposed approach in three representative text recognition
frameworks.",0.24273883,0.023667637,-0.12581147,A
7043,"These issues warrant
further research and consideration when generating images
based on this work.","The model may generate inexis-
tent images with unexpected content.","Layout

Context-L2I LostGAN-V2 Ground truth

HCSS

Ours

Figure 9.",2022-06-02 08:34:25+00:00,Modeling Image Composition for Complex Scene Generation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zuopeng Yang'), arxiv.Result.Author('Daqing Liu'), arxiv.Result.Author('Chaoyue Wang'), arxiv.Result.Author('Jie Yang'), arxiv.Result.Author('Dacheng Tao')]","We present a method that achieves state-of-the-art results on challenging
(few-shot) layout-to-image generation tasks by accurately modeling textures,
structures and relationships contained in a complex scene. After compressing
RGB images into patch tokens, we propose the Transformer with Focal Attention
(TwFA) for exploring dependencies of object-to-object, object-to-patch and
patch-to-patch. Compared to existing CNN-based and Transformer-based generation
models that entangled modeling on pixel-level&patch-level and
object-level&patch-level respectively, the proposed focal attention predicts
the current patch token by only focusing on its highly-related tokens that
specified by the spatial layout, thereby achieving disambiguation during
training. Furthermore, the proposed TwFA largely increases the data efficiency
during training, therefore we propose the first few-shot complex scene
generation strategy based on the well-trained TwFA. Comprehensive experiments
show the superiority of our method, which significantly increases both
quantitative metrics and qualitative visual realism with respect to
state-of-the-art CNN-based and transformer-based methods. Code is available at
https://github.com/JohnDreamer/TwFA.",-0.021375258,0.07684778,0.1599578,C
7053,"There could
      step for establishing a dataset for further research, data       be 8 to 10 stroke actions in less than 6 seconds, which
      collection and annotation draw more attention and the            means the action recognition algorithms should be more
      qualities of them directly affect the performance of the         sensitive to the boundary of two actions and it is proved
      action recognition task [7], [312], [313].","As one of the crucial             to other sports (e.g., soccer and basketball).","However, the          to be a challenging task for some of the state of the
      main difference of sports datasets comparing to other            art models [191], [234]‚Äì[236].",2022-06-02 13:19:36+00:00,"A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications",cs.CV,"['cs.CV', 'cs.AI', 'cs.MM']","[arxiv.Result.Author('Fei Wu'), arxiv.Result.Author('Qingzhong Wang'), arxiv.Result.Author('Jian Bian'), arxiv.Result.Author('Haoyi Xiong'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Feixiang Lu'), arxiv.Result.Author('Jun Cheng'), arxiv.Result.Author('Dejing Dou')]","To understand human behaviors, action recognition based on videos is a common
approach. Compared with image-based action recognition, videos provide much
more information. Reducing the ambiguity of actions and in the last decade,
many works focused on datasets, novel models and learning approaches have
improved video action recognition to a higher level. However, there are
challenges and unsolved problems, in particular in sports analytics where data
collection and labeling are more sophisticated, requiring sport professionals
to annotate data. In addition, the actions could be extremely fast and it
becomes difficult to recognize them. Moreover, in team sports like football and
basketball, one action could involve multiple players, and to correctly
recognize them, we need to analyse all players, which is relatively
complicated. In this paper, we present a survey on video action recognition for
sports analytics. We introduce more than ten types of sports, including team
sports, such as football, basketball, volleyball, hockey and individual sports,
such as figure skating, gymnastics, table tennis, tennis, diving and badminton.
Then we compare numerous existing frameworks for sports analysis to present
status quo of video action recognition in both team sports and individual
sports. Finally, we discuss the challenges and unsolved problems in this area
and to facilitate sports analytics, we develop a toolbox using PaddlePaddle,
which supports football, basketball, table tennis and figure skating action
recognition.",-0.086760685,-0.053028807,-0.31770745,B
7071,"To further study the efficacy of the
proposed UTEP, especially compared with semi-supervised pseudo labeling methods
(such as UPS [33]), we conduct 3-shot semi-supervised learning (SSL) experiments on
Office-Home in the right part of Table 5.",Evaluation on Semi-Supervised Learning.,"In the SSL setting, only three samples of
each class are selected as the labeled domain while the rest are used as the unlabeled
domain.",2022-06-02 21:58:54+00:00,Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jian Hu'), arxiv.Result.Author('Haowen Zhong'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Shaogang Gong'), arxiv.Result.Author('Guile Wu'), arxiv.Result.Author('Fei Yang')]","Domain adaptation (DA) aims to transfer knowledge learned from a labeled
source domain to an unlabeled or a less labeled but related target domain.
Ideally, the source and target distributions should be aligned to each other
equally to achieve unbiased knowledge transfer. However, due to the significant
imbalance between the amount of annotated data in the source and target
domains, usually only the target distribution is aligned to the source domain,
leading to adapting unnecessary source specific knowledge to the target domain,
i.e., biased domain adaptation. To resolve this problem, in this work, we delve
into the transferability estimation problem in domain adaptation and propose a
non-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling
the uncertainty of a discriminator in adversarial-based DA methods to optimize
unbiased transfer. We theoretically analyze the effectiveness of the proposed
approach to unbiased transferability learning in DA. Furthermore, to alleviate
the impact of imbalanced annotated data, we utilize the estimated uncertainty
for pseudo label selection of unlabeled samples in the target domain, which
helps achieve better marginal and conditional distribution alignments between
domains. Extensive experimental results on a high variety of DA benchmark
datasets show that the proposed approach can be readily incorporated into
various adversarial-based DA methods, achieving state-of-the-art performance.",-0.021845944,-0.26409188,-0.2520671,C
7072,"To further study the efficacy of the
proposed UTEP, especially compared with semi-supervised pseudo labeling methods
(such as UPS [33]), we conduct 3-shot semi-supervised learning (SSL) experiments on
Office-Home in the right part of Table 5.",Evaluation on Semi-Supervised Learning.,"In the SSL setting, only three samples of
each class are selected as the labeled domain while the rest are used as the unlabeled
domain.",2022-06-02 21:58:54+00:00,Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jian Hu'), arxiv.Result.Author('Haowen Zhong'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Shaogang Gong'), arxiv.Result.Author('Guile Wu'), arxiv.Result.Author('Fei Yang')]","Domain adaptation (DA) aims to transfer knowledge learned from a labeled
source domain to an unlabeled or a less labeled but related target domain.
Ideally, the source and target distributions should be aligned to each other
equally to achieve unbiased knowledge transfer. However, due to the significant
imbalance between the amount of annotated data in the source and target
domains, usually only the target distribution is aligned to the source domain,
leading to adapting unnecessary source specific knowledge to the target domain,
i.e., biased domain adaptation. To resolve this problem, in this work, we delve
into the transferability estimation problem in domain adaptation and propose a
non-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling
the uncertainty of a discriminator in adversarial-based DA methods to optimize
unbiased transfer. We theoretically analyze the effectiveness of the proposed
approach to unbiased transferability learning in DA. Furthermore, to alleviate
the impact of imbalanced annotated data, we utilize the estimated uncertainty
for pseudo label selection of unlabeled samples in the target domain, which
helps achieve better marginal and conditional distribution alignments between
domains. Extensive experimental results on a high variety of DA benchmark
datasets show that the proposed approach can be readily incorporated into
various adversarial-based DA methods, achieving state-of-the-art performance.",-0.021845944,-0.26409188,-0.2520671,C
7073,"To further study the efficacy of the
proposed UTEP, especially compared with semi-supervised pseudo labeling methods
(such as UPS [33]), we conduct 3-shot semi-supervised learning (SSL) experiments on
Office-Home in the right part of Table 5.",Evaluation on Semi-Supervised Learning.,"In the SSL setting, only three samples of
each class are selected as the labeled domain while the rest are used as the unlabeled
domain.",2022-06-02 21:58:54+00:00,Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jian Hu'), arxiv.Result.Author('Haowen Zhong'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Shaogang Gong'), arxiv.Result.Author('Guile Wu'), arxiv.Result.Author('Fei Yang')]","Domain adaptation (DA) aims to transfer knowledge learned from a labeled
source domain to an unlabeled or a less labeled but related target domain.
Ideally, the source and target distributions should be aligned to each other
equally to achieve unbiased knowledge transfer. However, due to the significant
imbalance between the amount of annotated data in the source and target
domains, usually only the target distribution is aligned to the source domain,
leading to adapting unnecessary source specific knowledge to the target domain,
i.e., biased domain adaptation. To resolve this problem, in this work, we delve
into the transferability estimation problem in domain adaptation and propose a
non-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling
the uncertainty of a discriminator in adversarial-based DA methods to optimize
unbiased transfer. We theoretically analyze the effectiveness of the proposed
approach to unbiased transferability learning in DA. Furthermore, to alleviate
the impact of imbalanced annotated data, we utilize the estimated uncertainty
for pseudo label selection of unlabeled samples in the target domain, which
helps achieve better marginal and conditional distribution alignments between
domains. Extensive experimental results on a high variety of DA benchmark
datasets show that the proposed approach can be readily incorporated into
various adversarial-based DA methods, achieving state-of-the-art performance.",-0.021845944,-0.26409188,-0.2520671,C
7081,"We can see that CF-YOLO with the kernel size of K = 1
   To further study how CF works, we conduct PCA for the                 or K = 3 achieves close results to YOLOv5s.",the comparison between CF-YOLO and SOTAs on MSCOCO.,"It means our
outputs of both our CF block and the aggregation module                  CF-YOLO performs well in snowy weather while still being
in YOLOv5s (see Fig.",2022-06-03 04:00:26+00:00,CF-YOLO: Cross Fusion YOLO for Object Detection in Adverse Weather with a High-quality Real Snow Dataset,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qiqi Ding'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Xuefeng Yan'), arxiv.Result.Author('Ding Shi'), arxiv.Result.Author('Luming Liang'), arxiv.Result.Author('Weiming Wang'), arxiv.Result.Author('Haoran Xie'), arxiv.Result.Author('Jonathan Li'), arxiv.Result.Author('Mingqiang Wei')]","Snow is one of the toughest adverse weather conditions for object detection
(OD). Currently, not only there is a lack of snowy OD datasets to train
cutting-edge detectors, but also these detectors have difficulties learning
latent information beneficial for detection in snow. To alleviate the two above
problems, we first establish a real-world snowy OD dataset, named RSOD.
Besides, we develop an unsupervised training strategy with a distinctive
activation function, called $Peak \ Act$, to quantitatively evaluate the effect
of snow on each object. Peak Act helps grading the images in RSOD into
four-difficulty levels. To our knowledge, RSOD is the first quantitatively
evaluated and graded snowy OD dataset. Then, we propose a novel Cross Fusion
(CF) block to construct a lightweight OD network based on YOLOv5s (call
CF-YOLO). CF is a plug-and-play feature aggregation module, which integrates
the advantages of Feature Pyramid Network and Path Aggregation Network in a
simpler yet more flexible form. Both RSOD and CF lead our CF-YOLO to possess an
optimization ability for OD in real-world snow. That is, CF-YOLO can handle
unfavorable detection problems of vagueness, distortion and covering of snow.
Experiments show that our CF-YOLO achieves better detection results on RSOD,
compared to SOTAs. The code and dataset are available at
https://github.com/qqding77/CF-YOLO-and-RSOD.",0.22239324,0.03334614,0.14768738,A
7082,"2) Demon-
strating the proposed method actually out-speed existing works requires further study to
benchmark more existing works and incorporate latest development on efÔ¨Åcient deep learn-
ing, such as MobileNet [7] and Compression [6].","1)
This approach cannot handle multiple hands or cases with hand/object interaction.","5 Conclusion

In this paper, we have proposed an end-to-end approach to estimate the full 3D hand pose
from stereo cameras.",2022-06-03 04:18:58+00:00,End-to-End 3D Hand Pose Estimation from Stereo Cameras,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuncheng Li'), arxiv.Result.Author('Zehao Xue'), arxiv.Result.Author('Yingying Wang'), arxiv.Result.Author('Liuhao Ge'), arxiv.Result.Author('Zhou Ren'), arxiv.Result.Author('Jonathan Rodriguez')]","This work proposes an end-to-end approach to estimate full 3D hand pose from
stereo cameras. Most existing methods of estimating hand pose from stereo
cameras apply stereo matching to obtain depth map and use depth-based solution
to estimate hand pose. In contrast, we propose to bypass the stereo matching
and directly estimate the 3D hand pose from the stereo image pairs. The
proposed neural network architecture extends from any keypoint predictor to
estimate the sparse disparity of the hand joints. In order to effectively train
the model, we propose a large scale synthetic dataset that is composed of
stereo image pairs and ground truth 3D hand pose annotations. Experiments show
that the proposed approach outperforms the existing methods based on the stereo
depth.",-0.28760523,0.06744509,0.07385372,B
7089,"We hope that our work
will encourage further research into biodiversity mapping, and may serve as a first step towards unlocking
the treasure trove of biological field guides, beyond Billow.","The experiments show that Billow
matches the performance of other, more structured forms of side information, confirming the hypothesis that
field guides are a valuable auxiliary source of information for species recognition.","2 Related Work

The use of illustrations for zero shot learning is not new.",2022-06-03 09:13:46+00:00,Zero-Shot Bird Species Recognition by Learning from Field Guides,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Andr√©s C. Rodr√≠guez'), arxiv.Result.Author(""Stefano D'Aronco""), arxiv.Result.Author('Rodrigo Caye Daudt'), arxiv.Result.Author('Jan D. Wegner'), arxiv.Result.Author('Konrad Schindler')]","We exploit field guides to learn bird species recognition, in particular
zero-shot recognition of unseen species. The illustrations contained in field
guides deliberately focus on discriminative properties of a species, and can
serve as side information to transfer knowledge from seen to unseen classes. We
study two approaches: (1) a contrastive encoding of illustrations that can be
fed into zero-shot learning schemes; and (2) a novel method that leverages the
fact that illustrations are also images and as such structurally more similar
to photographs than other kinds of side information. Our results show that
illustrations from field guides, which are readily available for a wide range
of species, are indeed a competitive source of side information. On the
iNaturalist2021 subset, we obtain a harmonic mean from 749 seen and 739 unseen
classes greater than $45\%$ (@top-10) and $15\%$ (@top-1). Which shows that
field guides are a valuable option for challenging real-world scenarios with
many species.",-0.12237132,-0.121905565,-0.1611485,C
7090,"To ensure reproducibility and support
further research and comparisons, we will make available the latent encodings from both our approaches (see
Sec.","Unfortunately, the original artworks may only be accessed with a valid subscription to the Birds of the
World project, and are subject to restrictive licensing conditions.",4) Further details are given in the supplementary material.,2022-06-03 09:13:46+00:00,Zero-Shot Bird Species Recognition by Learning from Field Guides,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Andr√©s C. Rodr√≠guez'), arxiv.Result.Author(""Stefano D'Aronco""), arxiv.Result.Author('Rodrigo Caye Daudt'), arxiv.Result.Author('Jan D. Wegner'), arxiv.Result.Author('Konrad Schindler')]","We exploit field guides to learn bird species recognition, in particular
zero-shot recognition of unseen species. The illustrations contained in field
guides deliberately focus on discriminative properties of a species, and can
serve as side information to transfer knowledge from seen to unseen classes. We
study two approaches: (1) a contrastive encoding of illustrations that can be
fed into zero-shot learning schemes; and (2) a novel method that leverages the
fact that illustrations are also images and as such structurally more similar
to photographs than other kinds of side information. Our results show that
illustrations from field guides, which are readily available for a wide range
of species, are indeed a competitive source of side information. On the
iNaturalist2021 subset, we obtain a harmonic mean from 749 seen and 739 unseen
classes greater than $45\%$ (@top-10) and $15\%$ (@top-1). Which shows that
field guides are a valuable option for challenging real-world scenarios with
many species.",0.09955034,-0.10992494,-0.07003094,C
7091,"Despite real datasets

                                                       12
are diÔ¨Écult to construct and label, further research          of general-activity human poses from depth im-
using the CNN-HL model on GNSS data should fo-                ages,‚Äù 2011 International Conference on Com-
cus on data that incorporates real receiving condi-           puter Vision, pp.","This
                                                           reduction of image resolution is a Ô¨Årst step towards
                                                           the implementation of such models in physical re-
                                                           ceivers that are limited in the number of correlator
                                                           outputs that can be designed.","415‚Äì422, 2011.
tions.",2022-06-03 09:45:12+00:00,Distributional loss for convolutional neural network regression and application to GNSS multi-path estimation,cs.CV,"['cs.CV', 'eess.SP']","[arxiv.Result.Author('Thomas Gonzalez'), arxiv.Result.Author('Antoine Blais'), arxiv.Result.Author('Nicolas Cou√´llan'), arxiv.Result.Author('Christian Ruiz')]","Convolutional Neural Network (CNN) have been widely used in image
classification. Over the years, they have also benefited from various
enhancements and they are now considered as state of the art techniques for
image like data. However, when they are used for regression to estimate some
function value from images, fewer recommendations are available. In this study,
a novel CNN regression model is proposed. It combines convolutional neural
layers to extract high level features representations from images with a soft
labelling technique. More specifically, as the deep regression task is
challenging, the idea is to account for some uncertainty in the targets that
are seen as distributions around their mean. The estimations are carried out by
the model in the form of distributions. Building from earlier work, a specific
histogram loss function based on the Kullback-Leibler (KL) divergence is
applied during training. The model takes advantage of the CNN feature
representation and is able to carry out estimation from multi-channel input
images. To assess and illustrate the technique, the model is applied to Global
Navigation Satellite System (GNSS) multi-path estimation where multi-path
signal parameters have to be estimated from correlator output images from the I
and Q channels. The multi-path signal delay, magnitude, Doppler shift frequency
and phase parameters are estimated from synthetically generated datasets of
satellite signals. Experiments are conducted under various receiving conditions
and various input images resolutions to test the estimation performances
quality and robustness. The results show that the proposed soft labelling CNN
technique using distributional loss outperforms classical CNN regression under
all conditions. Furthermore, the extra learning performance achieved by the
model allows the reduction of input image resolution from 80x80 down to 40x40
or sometimes 20x20.",-0.27136332,0.14360766,0.06317123,B
7103,"Therefore, a
comparison of bigger cell size prescription maps must be conducted in further research.","Alternative map grids with
approximately 10√ó10 ft cell size would have been useful for better efficacy.","Furthermore, we suggest that the sprayer would have done a poor job if the sprayer speed was
operated at 12 mph or more.",2022-06-02 18:33:22+00:00,Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn,cs.CV,"['cs.CV', 'cs.AI', 'eess.IV']","[arxiv.Result.Author('Ranjan Sapkota'), arxiv.Result.Author('Paulo Flores')]","Currently, weed control in a corn field is performed by a blanket application
of herbicides that do not consider spatial distribution information of weeds
and also uses an extensive amount of chemical herbicides. To reduce the amount
of chemicals, we used drone-based high-resolution imagery and computer-vision
techniques to perform site-specific weed control in corn.",0.25151077,0.26986152,0.020910546,A
7114,"To further study our method, we utilize the ground truth of 2D key points as
our input to evaluate our model, with results shown in Table 3 under Protocol
1 and Protocol 2.","Compared to results in Table 1, their method uses the
feature of GANs but makes it hard to detect the global position of the 3D human
pose, leading the better performance in Protocol 1.","By using 2D ground truth, the models generally get better
performance than Table 2.",2022-06-04 00:51:00+00:00,SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zihan Wang'), arxiv.Result.Author('Ruimin Chen'), arxiv.Result.Author('Mengxuan Liu'), arxiv.Result.Author('Guanfang Dong'), arxiv.Result.Author('Anup Basu')]","We propose a method SPGNet for 3D human pose estimation that mixes
multi-dimensional re-projection into supervised learning. In this method, the
2D-to-3D-lifting network predicts the global position and coordinates of the 3D
human pose. Then, we re-project the estimated 3D pose back to the 2D key points
along with spatial adjustments. The loss functions compare the estimated 3D
pose with the 3D pose ground truth, and re-projected 2D pose with the input 2D
pose. In addition, we propose a kinematic constraint to restrict the predicted
target with constant human bone length. Based on the estimation results for the
dataset Human3.6M, our approach outperforms many state-of-the-art methods both
qualitatively and quantitatively.",-0.18923777,0.20465839,-0.02284147,B
7116,"In addition, for
                                                                 each image, three questions are collected and each question
   In order to further study the effect of the oder of channel-  is answered by ten subjects alone with conÔ¨Ådence.","Compared with COCO-QA, the answer form is di-
                                                                 versity with an answer containing one, two or three words re-
Vs = fs (Œ∑, Vc)            (24)                                  spectively being 89.32%, 6.91%, and 2.74%.","Follow-
wise and spatial attentions, we propose an CVA variant which     ing [Hyeonseob Nam and Kim, 2017], we set th number of
exchange the order of two attentions by Ô¨Årstly applying spa-     possible answer for VQA as 2,000.
tial attention As and then following by a channel-wise atten-
tion Ac.",2022-06-04 07:03:18+00:00,From Pixels to Objects: Cubic Visual Attention for Visual Question Answering,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jingkuan Song'), arxiv.Result.Author('Pengpeng Zeng'), arxiv.Result.Author('Lianli Gao'), arxiv.Result.Author('Heng Tao Shen')]","Recently, attention-based Visual Question Answering (VQA) has achieved great
success by utilizing question to selectively target different visual areas that
are related to the answer. Existing visual attention models are generally
planar, i.e., different channels of the last conv-layer feature map of an image
share the same weight. This conflicts with the attention mechanism because CNN
features are naturally spatial and channel-wise. Also, visual attention models
are usually conducted on pixel-level, which may cause region discontinuous
problems. In this paper, we propose a Cubic Visual Attention (CVA) model by
successfully applying a novel channel and spatial attention on object regions
to improve VQA task. Specifically, instead of attending to pixels, we first
take advantage of the object proposal networks to generate a set of object
candidates and extract their associated conv features. Then, we utilize the
question to guide channel attention and spatial attention calculation based on
the con-layer feature map. Finally, the attended visual features and the
question are combined to infer the answer. We assess the performance of our
proposed CVA on three public image QA datasets, including COCO-QA, VQA and
Visual7W. Experimental results show that our proposed method significantly
outperforms the state-of-the-arts.",0.15175977,-0.15286048,-0.17347072,C
7142,"Finally, Section 5 concludes the paper and outlines

In recent times, Generative Adversarial Networks (GANs) potential avenues for further research.",cussed.,"have displayed state-of-the-art performance for the task of

synthetic image generation.",2022-06-05 06:54:36+00:00,Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Carmina P√©rez-Guerrero'), arxiv.Result.Author('Jorge Francisco Cipri√°n-S√°nchez'), arxiv.Result.Author('Adriana Palacios'), arxiv.Result.Author('Gilberto Ochoa-Ruiz'), arxiv.Result.Author('Miguel Gonzalez-Mendoza'), arxiv.Result.Author('Vahid Foroughi'), arxiv.Result.Author('Elsa Pastor'), arxiv.Result.Author('Gerardo Rodriguez-Hernandez')]","Among the different kinds of fire accidents that can occur during industrial
activities that involve hazardous materials, jet fires are one of the
lesser-known types. This is because they are often involved in a process that
generates a sequence of other accidents of greater magnitude, known as domino
effect. Flame impingement usually causes domino effects, and jet fires present
specific features that can significantly increase the probability of this
happening. These features become relevant from a risk analysis perspective,
making their proper characterization a crucial task. Deep Learning approaches
have become extensively used for tasks such as jet fire characterization;
however, these methods are heavily dependent on the amount of data and the
quality of the labels. Data acquisition of jet fires involve expensive
experiments, especially so if infrared imagery is used. Therefore, this paper
proposes the use of Generative Adversarial Networks to produce plausible
infrared images from visible ones, making experiments less expensive and
allowing for other potential applications. The results suggest that it is
possible to realistically replicate the results for experiments carried out
using both visible and infrared cameras. The obtained results are compared with
some previous experiments, and it is shown that similar results were obtained.",-0.19189782,-0.104972914,0.19875646,C
7154,"We publish network weights and
                                       relevant facilitating source code with this paper for full
                                       reproducibility and as inspiration for further research.","However, animal pose variations and defor-
                                       too small accurately to estimate the full performance             mations still cause challenges in individual re-identiÔ¨Åcation
                                       potential achievable in larger-scale real-world application       despite the fact that metric learning [4] has opened new
                                       settings and in comparisons against polished tools, our
                                       work lays the conceptual and practical foundations for
                                       a next step in animal biometrics towards deep metric
                                       learning driven, fully 3D-aware animal identiÔ¨Åcation in
                                       open population settings.",I.,2022-06-05 20:44:54+00:00,Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Maria Stennett'), arxiv.Result.Author('Daniel I. Rubenstein'), arxiv.Result.Author('Tilo Burghardt')]","This paper combines deep learning techniques for species detection, 3D model
fitting, and metric learning in one pipeline to perform individual animal
identification from photographs by exploiting unique coat patterns. This is the
first work to attempt this and, compared to traditional 2D bounding box or
segmentation based CNN identification pipelines, the approach provides
effective and explicit view-point normalisation and allows for a straight
forward visualisation of the learned biometric population space. Note that due
to the use of metric learning the pipeline is also readily applicable to open
set and zero shot re-identification scenarios. We apply the proposed approach
to individual Grevy's zebra (Equus grevyi) identification and show in a small
study on the SMALST dataset that the use of 3D model fitting can indeed benefit
performance. In particular, back-projected textures from 3D fitted models
improve identification accuracy from 48.0% to 56.8% compared to 2D bounding box
approaches for the dataset. Whilst the study is far too small accurately to
estimate the full performance potential achievable in larger-scale real-world
application settings and in comparisons against polished tools, our work lays
the conceptual and practical foundations for a next step in animal biometrics
towards deep metric learning driven, fully 3D-aware animal identification in
open population settings. We publish network weights and relevant facilitating
source code with this paper for full reproducibility and as inspiration for
further research.",-0.21721828,-0.05501183,0.038983013,B
7155,"We publish network weights and
                                       relevant facilitating source code with this paper for full
                                       reproducibility and as inspiration for further research.","However, animal pose variations and defor-
                                       too small accurately to estimate the full performance             mations still cause challenges in individual re-identiÔ¨Åcation
                                       potential achievable in larger-scale real-world application       despite the fact that metric learning [4] has opened new
                                       settings and in comparisons against polished tools, our
                                       work lays the conceptual and practical foundations for
                                       a next step in animal biometrics towards deep metric
                                       learning driven, fully 3D-aware animal identiÔ¨Åcation in
                                       open population settings.",I.,2022-06-05 20:44:54+00:00,Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Maria Stennett'), arxiv.Result.Author('Daniel I. Rubenstein'), arxiv.Result.Author('Tilo Burghardt')]","This paper combines deep learning techniques for species detection, 3D model
fitting, and metric learning in one pipeline to perform individual animal
identification from photographs by exploiting unique coat patterns. This is the
first work to attempt this and, compared to traditional 2D bounding box or
segmentation based CNN identification pipelines, the approach provides
effective and explicit view-point normalisation and allows for a straight
forward visualisation of the learned biometric population space. Note that due
to the use of metric learning the pipeline is also readily applicable to open
set and zero shot re-identification scenarios. We apply the proposed approach
to individual Grevy's zebra (Equus grevyi) identification and show in a small
study on the SMALST dataset that the use of 3D model fitting can indeed benefit
performance. In particular, back-projected textures from 3D fitted models
improve identification accuracy from 48.0% to 56.8% compared to 2D bounding box
approaches for the dataset. Whilst the study is far too small accurately to
estimate the full performance potential achievable in larger-scale real-world
application settings and in comparisons against polished tools, our work lays
the conceptual and practical foundations for a next step in animal biometrics
towards deep metric learning driven, fully 3D-aware animal identification in
open population settings. We publish network weights and relevant facilitating
source code with this paper for full reproducibility and as inspiration for
further research.",-0.21721828,-0.05501183,0.038983013,B
7156,"We publish network weights and
                                       relevant facilitating source code with this paper for full
                                       reproducibility and as inspiration for further research.","However, animal pose variations and defor-
                                       too small accurately to estimate the full performance             mations still cause challenges in individual re-identiÔ¨Åcation
                                       potential achievable in larger-scale real-world application       despite the fact that metric learning [4] has opened new
                                       settings and in comparisons against polished tools, our
                                       work lays the conceptual and practical foundations for
                                       a next step in animal biometrics towards deep metric
                                       learning driven, fully 3D-aware animal identiÔ¨Åcation in
                                       open population settings.",I.,2022-06-05 20:44:54+00:00,Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Maria Stennett'), arxiv.Result.Author('Daniel I. Rubenstein'), arxiv.Result.Author('Tilo Burghardt')]","This paper combines deep learning techniques for species detection, 3D model
fitting, and metric learning in one pipeline to perform individual animal
identification from photographs by exploiting unique coat patterns. This is the
first work to attempt this and, compared to traditional 2D bounding box or
segmentation based CNN identification pipelines, the approach provides
effective and explicit view-point normalisation and allows for a straight
forward visualisation of the learned biometric population space. Note that due
to the use of metric learning the pipeline is also readily applicable to open
set and zero shot re-identification scenarios. We apply the proposed approach
to individual Grevy's zebra (Equus grevyi) identification and show in a small
study on the SMALST dataset that the use of 3D model fitting can indeed benefit
performance. In particular, back-projected textures from 3D fitted models
improve identification accuracy from 48.0% to 56.8% compared to 2D bounding box
approaches for the dataset. Whilst the study is far too small accurately to
estimate the full performance potential achievable in larger-scale real-world
application settings and in comparisons against polished tools, our work lays
the conceptual and practical foundations for a next step in animal biometrics
towards deep metric learning driven, fully 3D-aware animal identification in
open population settings. We publish network weights and relevant facilitating
source code with this paper for full reproducibility and as inspiration for
further research.",-0.21721828,-0.05501183,0.038983013,B
7159,"To begin with, this is the Ô¨Årst work which pre-     conduct further research regarding the architectural choices.","Regression                                57.93                  55.71  56.15
                                                                    57.71                  56.01  56.45

tributions.","processes, utilizes, and evaluates satellite-borne heat loss
data to estimate building-level attributes such as building        There are multiple ways to build upon this work and to
energy efÔ¨Åciency.",2022-06-05 21:04:20+00:00,"Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Kevin Mayer'), arxiv.Result.Author('Lukas Haas')]","In the race towards carbon neutrality, the building sector has fallen behind
and bears the potential to endanger the progress made across other industries.
This is because buildings exhibit a life span of several decades which creates
substantial inertia in the face of climate change. This inertia is further
exacerbated by the scale of the existing building stock. With several billion
operational buildings around the globe, working towards a carbon-neutral
building sector requires solutions which enable stakeholders to accurately
identify and retrofit subpar buildings at scale. However, improving the energy
efficiency of the existing building stock through retrofits in a targeted and
efficient way remains challenging. This is because, as of today, the energy
efficiency of buildings is generally determined by on-site visits of certified
energy auditors which makes the process slow, costly, and geographically
incomplete. In order to accelerate the identification of promising retrofit
targets, this work proposes a new method which can estimate a building's energy
efficiency using purely remotely sensed data such as street view and aerial
imagery, OSM-derived footprint areas, and satellite-borne land surface
temperature (LST) measurements. We find that in the binary setting of
distinguishing efficient from inefficient buildings, our end-to-end deep
learning model achieves a macro-averaged F1-score of 62.06\%. As such, this
work shows the potential and complementary nature of remotely sensed data in
predicting building attributes such as energy efficiency and opens up new
opportunities for future work to integrate additional data sources.",0.2847392,0.12101698,-0.055663407,A
7165,How to alleviate the bias in the language model still needs further research.,"Since our method
directly employs existing language models, the bias in the language models will be also inherited by
our method.","In this paper, we have presented the language-powered paradigm for ordinal regression.",2022-06-06 03:54:53+00:00,OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wanhua Li'), arxiv.Result.Author('Xiaoke Huang'), arxiv.Result.Author('Zheng Zhu'), arxiv.Result.Author('Yansong Tang'), arxiv.Result.Author('Xiu Li'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Jie Zhou')]","This paper presents a language-powered paradigm for ordinal regression.
Existing methods usually treat each rank as a category and employ a set of
weights to learn these concepts. These methods are easy to overfit and usually
attain unsatisfactory performance as the learned concepts are mainly derived
from the training set. Recent large pre-trained vision-language models like
CLIP have shown impressive performance on various visual tasks. In this paper,
we propose to learn the rank concepts from the rich semantic CLIP latent space.
Specifically, we reformulate this task as an image-language matching problem
with a contrastive objective, which regards labels as text and obtains a
language prototype from a text encoder for each rank. While prompt engineering
for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable
prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists
of learnable context tokens and learnable rank embeddings; The learnable rank
embeddings are constructed by explicitly modeling numerical continuity,
resulting in well-ordered, compact language prototypes in the CLIP space. Once
learned, we can only save the language prototypes and discard the huge language
model, resulting in zero additional computational overhead compared with the
linear head counterpart. Experimental results show that our paradigm achieves
competitive performance in general ordinal regression tasks, and gains
improvements in few-shot and distribution shift settings for age estimation.",0.24624822,-0.24127665,-0.2789677,A
7166,How to alleviate the bias in the language model still needs further research.,"Since our method
directly employs existing language models, the bias in the language models will be also inherited by
our method.","In this paper, we have presented the language-powered paradigm for ordinal regression.",2022-06-06 03:54:53+00:00,OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wanhua Li'), arxiv.Result.Author('Xiaoke Huang'), arxiv.Result.Author('Zheng Zhu'), arxiv.Result.Author('Yansong Tang'), arxiv.Result.Author('Xiu Li'), arxiv.Result.Author('Jie Zhou'), arxiv.Result.Author('Jiwen Lu')]","This paper presents a language-powered paradigm for ordinal regression.
Existing methods usually treat each rank as a category and employ a set of
weights to learn these concepts. These methods are easy to overfit and usually
attain unsatisfactory performance as the learned concepts are mainly derived
from the training set. Recent large pre-trained vision-language models like
CLIP have shown impressive performance on various visual tasks. In this paper,
we propose to learn the rank concepts from the rich semantic CLIP latent space.
Specifically, we reformulate this task as an image-language matching problem
with a contrastive objective, which regards labels as text and obtains a
language prototype from a text encoder for each rank. While prompt engineering
for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable
prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists
of learnable context tokens and learnable rank embeddings; The learnable rank
embeddings are constructed by explicitly modeling numerical continuity,
resulting in well-ordered, compact language prototypes in the CLIP space. Once
learned, we can only save the language prototypes and discard the huge language
model, resulting in zero additional computational overhead compared with the
linear head counterpart. Experimental results show that our paradigm achieves
competitive performance in general ordinal regression tasks, and gains
improvements in few-shot and distribution shift settings for age estimation.
The code is available at https://github.com/xk-huang/OrdinalCLIP.",0.24624822,-0.24127665,-0.2789677,A
7168,"To facilitate further research, this paper         putation and optimization, and disparities reÔ¨Ånement [4].","However, datasets for stereo matching of satellite images are               Generally, the stereo matching pipeline consists of four steps:
                                       scarce, which profoundly blocks the research and usage of DL                matching cost computation, cost aggregation, disparity com-
                                       techniques in this Ô¨Åeld.","creates and publishes a challenging dataset, termed WHU-Stereo,             Existing methods can be classiÔ¨Åed into two categories, in-
                                       for stereo matching DL network training and testing.",2022-06-06 04:01:46+00:00,WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Shenhong Li'), arxiv.Result.Author('Sheng He'), arxiv.Result.Author('San Jiang'), arxiv.Result.Author('Wanshou Jiang'), arxiv.Result.Author('Lin Zhang')]","Stereo matching of high-resolution satellite images (HRSI) is still a
fundamental but challenging task in the field of photogrammetry and remote
sensing. Recently, deep learning (DL) methods, especially convolutional neural
networks (CNNs), have demonstrated tremendous potential for stereo matching on
public benchmark datasets. However, datasets for stereo matching of satellite
images are scarce. To facilitate further research, this paper creates and
publishes a challenging dataset, termed WHU-Stereo, for stereo matching DL
network training and testing. This dataset is created by using airborne LiDAR
point clouds and high-resolution stereo imageries taken from the Chinese
GaoFen-7 satellite (GF-7). The WHU-Stereo dataset contains more than 1700
epipolar rectified image pairs, which cover six areas in China and includes
various kinds of landscapes. We have assessed the accuracy of ground-truth
disparity maps, and it is proved that our dataset achieves comparable precision
compared with existing state-of-the-art stereo matching datasets. To verify its
feasibility, in experiments, the hand-crafted SGM stereo matching algorithm and
recent deep learning networks have been tested on the WHU-Stereo dataset.
Experimental results show that deep learning networks can be well trained and
achieves higher performance than hand-crafted SGM algorithm, and the dataset
has great potential in remote sensing application. The WHU-Stereo dataset can
serve as a challenging benchmark for stereo matching of high-resolution
satellite images, and performance evaluation of deep learning models. Our
dataset is available at https://github.com/Sheng029/WHU-Stereo",-0.22313976,0.19898218,0.10585728,B
7169,"One thing
                                                                             to note is that Stereo-Net‚Äôs performance on Kunming shows a
                                                                             slight decrease, thus how to maximize the beneÔ¨Åt of transfer
                                                                             learning still needs further study.","Therefore, it might be a
                                                                             good choice to utilize available pre-trained models.",Fig.,2022-06-06 04:01:46+00:00,WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Shenhong Li'), arxiv.Result.Author('Sheng He'), arxiv.Result.Author('San Jiang'), arxiv.Result.Author('Wanshou Jiang'), arxiv.Result.Author('Lin Zhang')]","Stereo matching of high-resolution satellite images (HRSI) is still a
fundamental but challenging task in the field of photogrammetry and remote
sensing. Recently, deep learning (DL) methods, especially convolutional neural
networks (CNNs), have demonstrated tremendous potential for stereo matching on
public benchmark datasets. However, datasets for stereo matching of satellite
images are scarce. To facilitate further research, this paper creates and
publishes a challenging dataset, termed WHU-Stereo, for stereo matching DL
network training and testing. This dataset is created by using airborne LiDAR
point clouds and high-resolution stereo imageries taken from the Chinese
GaoFen-7 satellite (GF-7). The WHU-Stereo dataset contains more than 1700
epipolar rectified image pairs, which cover six areas in China and includes
various kinds of landscapes. We have assessed the accuracy of ground-truth
disparity maps, and it is proved that our dataset achieves comparable precision
compared with existing state-of-the-art stereo matching datasets. To verify its
feasibility, in experiments, the hand-crafted SGM stereo matching algorithm and
recent deep learning networks have been tested on the WHU-Stereo dataset.
Experimental results show that deep learning networks can be well trained and
achieves higher performance than hand-crafted SGM algorithm, and the dataset
has great potential in remote sensing application. The WHU-Stereo dataset can
serve as a challenging benchmark for stereo matching of high-resolution
satellite images, and performance evaluation of deep learning models. Our
dataset is available at https://github.com/Sheng029/WHU-Stereo",-0.031712778,-0.10712212,0.13633326,C
7181,"Depend-       opens interesting possibilities for further research
ing on the Ô¨Ånal application and available data,     as sequences of game camera images can be uti-
the relationship between the top-k accuracy and     lized to create a single descriptor for a larger
k can be used to determine the optimal number of    portion of a pelage pattern by Ô¨Ålling in the gaps
matches to be returned by the algorithm.","This
compared to a fully manual approach).",created by obstructions and viewpoints.,2022-06-06 11:04:16+00:00,NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ekaterina Nepovinnykh'), arxiv.Result.Author('Ilia Chelak'), arxiv.Result.Author('Tuomas Eerola'), arxiv.Result.Author('Heikki K√§lvi√§inen')]","We propose a method for Saimaa ringed seal (Pusa hispida saimensis)
re-identification. Access to large image volumes through camera trapping and
crowdsourcing provides novel possibilities for animal monitoring and
conservation and calls for automatic methods for analysis, in particular, when
re-identifying individual animals from the images. The proposed method NOvel
Ringed seal re-identification by Pelage Pattern Aggregation (NORPPA) utilizes
the permanent and unique pelage pattern of Saimaa ringed seals and
content-based image retrieval techniques. First, the query image is
preprocessed, and each seal instance is segmented. Next, the seal's pelage
pattern is extracted using a U-net encoder-decoder based method. Then,
CNN-based affine invariant features are embedded and aggregated into Fisher
Vectors. Finally, the cosine distance between the Fisher Vectors is used to
find the best match from a database of known individuals. We perform extensive
experiments of various modifications of the method on a new challenging Saimaa
ringed seals re-identification dataset. The proposed method is shown to produce
the best re-identification accuracy on our dataset in comparisons with
alternative approaches.",-0.021216862,0.2904833,-0.19479153,B
7182,"Depend-       opens interesting possibilities for further research
ing on the Ô¨Ånal application and available data,     as sequences of game camera images can be uti-
the relationship between the top-k accuracy and     lized to create a single descriptor for a larger
k can be used to determine the optimal number of    portion of a pelage pattern by Ô¨Ålling in the gaps
matches to be returned by the algorithm.","This
compared to a fully manual approach).",created by obstructions and viewpoints.,2022-06-06 11:04:16+00:00,NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ekaterina Nepovinnykh'), arxiv.Result.Author('Ilia Chelak'), arxiv.Result.Author('Tuomas Eerola'), arxiv.Result.Author('Heikki K√§lvi√§inen')]","We propose a method for Saimaa ringed seal (Pusa hispida saimensis)
re-identification. Access to large image volumes through camera trapping and
crowdsourcing provides novel possibilities for animal monitoring and
conservation and calls for automatic methods for analysis, in particular, when
re-identifying individual animals from the images. The proposed method NOvel
Ringed seal re-identification by Pelage Pattern Aggregation (NORPPA) utilizes
the permanent and unique pelage pattern of Saimaa ringed seals and
content-based image retrieval techniques. First, the query image is
preprocessed, and each seal instance is segmented. Next, the seal's pelage
pattern is extracted using a U-net encoder-decoder based method. Then,
CNN-based affine invariant features are embedded and aggregated into Fisher
Vectors. Finally, the cosine distance between the Fisher Vectors is used to
find the best match from a database of known individuals. We perform extensive
experiments of various modifications of the method on a new challenging Saimaa
ringed seals re-identification dataset. The proposed method is shown to produce
the best re-identification accuracy on our dataset in comparisons with
alternative approaches.",-0.021216862,0.2904833,-0.19479153,B
7183,"Depend-       opens interesting possibilities for further research
ing on the Ô¨Ånal application and available data,     as sequences of game camera images can be uti-
the relationship between the top-k accuracy and     lized to create a single descriptor for a larger
k can be used to determine the optimal number of    portion of a pelage pattern by Ô¨Ålling in the gaps
matches to be returned by the algorithm.","This
compared to a fully manual approach).",created by obstructions and viewpoints.,2022-06-06 11:04:16+00:00,NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ekaterina Nepovinnykh'), arxiv.Result.Author('Ilia Chelak'), arxiv.Result.Author('Tuomas Eerola'), arxiv.Result.Author('Heikki K√§lvi√§inen')]","We propose a method for Saimaa ringed seal (Pusa hispida saimensis)
re-identification. Access to large image volumes through camera trapping and
crowdsourcing provides novel possibilities for animal monitoring and
conservation and calls for automatic methods for analysis, in particular, when
re-identifying individual animals from the images. The proposed method NOvel
Ringed seal re-identification by Pelage Pattern Aggregation (NORPPA) utilizes
the permanent and unique pelage pattern of Saimaa ringed seals and
content-based image retrieval techniques. First, the query image is
preprocessed, and each seal instance is segmented. Next, the seal's pelage
pattern is extracted using a U-net encoder-decoder based method. Then,
CNN-based affine invariant features are embedded and aggregated into Fisher
Vectors. Finally, the cosine distance between the Fisher Vectors is used to
find the best match from a database of known individuals. We perform extensive
experiments of various modifications of the method on a new challenging Saimaa
ringed seals re-identification dataset. The proposed method is shown to produce
the best re-identification accuracy on our dataset in comparisons with
alternative approaches.",-0.021216862,0.2904833,-0.19479153,B
7193,"to collect soil samples from the surface of Mars and return them             The rover is equipped with a manipulator, a gripper and two stereo cameras:
                                       to Earth for further study.",Picture of ExoTeR [3] taken at the Planetary Utilisation Testbed.,The samples will be acquired and                 LocCam and NavCam.,2022-06-06 14:05:25+00:00,Hardware-accelerated Mars Sample Localization via deep transfer learning from photorealistic simulations,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Ra√∫l Castilla-Arquillo'), arxiv.Result.Author('Carlos Jes√∫s P√©rez-del-Pulgar'), arxiv.Result.Author('Gonzalo Jes√∫s Paz-Delgado'), arxiv.Result.Author('Levin Gerdes')]","The goal of the Mars Sample Return campaign is to collect soil samples from
the surface of Mars and return them to Earth for further study. The samples
will be acquired and stored in metal tubes by the Perseverance rover and
deposited on the Martian surface. As part of this campaign, it is expected the
Sample Fetch Rover will be in charge of localizing and gathering up to 35
sample tubes over 150 Martian sols. Autonomous capabilities are critical for
the success of the overall campaign and for the Sample Fetch Rover in
particular. This work proposes a novel approach for the autonomous detection
and pose estimation of the sample tubes. For the detection stage, a Deep Neural
Network and transfer learning from a synthetic dataset are proposed. The
dataset is created from photorealistic 3D simulations of Martian scenarios.
Additionally, Computer Vision techniques are used to estimate the detected
sample tubes poses. Finally, laboratory tests of the Sample Localization
procedure are performed using the ExoMars Testing Rover on a Mars-like testbed.
These tests validate the proposed approach in different hardware architectures,
providing promising results related to the sample detection and pose
estimation.",-0.07599696,0.20515472,-0.12108739,B
7194,"collect soil samples from the surface of Mars and return them to               The rover is equipped with a manipulator, a gripper and two stereo cameras:
                                       Earth for further study.",Picture of ExoTeR [3] taken at the Planetary Utilisation Testbed.,The samples will be acquired and stored               LocCam and NavCam.,2022-06-06 14:05:25+00:00,Hardware-accelerated Mars Sample Localization via deep transfer learning from photorealistic simulations,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Ra√∫l Castilla-Arquillo'), arxiv.Result.Author('Carlos Jes√∫s P√©rez-del-Pulgar'), arxiv.Result.Author('Gonzalo Jes√∫s Paz-Delgado'), arxiv.Result.Author('Levin Gerdes')]","The goal of the Mars Sample Return campaign is to collect soil samples from
the surface of Mars and return them to Earth for further study. The samples
will be acquired and stored in metal tubes by the Perseverance rover and
deposited on the Martian surface. As part of this campaign, it is expected that
the Sample Fetch Rover will be in charge of localizing and gathering up to 35
sample tubes over 150 Martian sols. Autonomous capabilities are critical for
the success of the overall campaign and for the Sample Fetch Rover in
particular. This work proposes a novel system architecture for the autonomous
detection and pose estimation of the sample tubes. For the detection stage, a
Deep Neural Network and transfer learning from a synthetic dataset are
proposed. The dataset is created from photorealistic 3D simulations of Martian
scenarios. Additionally, the sample tubes poses are estimated using Computer
Vision techniques such as contour detection and line fitting on the detected
area. Finally, laboratory tests of the Sample Localization procedure are
performed using the ExoMars Testing Rover on a Mars-like testbed. These tests
validate the proposed approach in different hardware architectures, providing
promising results related to the sample detection and pose estimation.",-0.07778941,0.19688475,-0.11444178,B
7199,"SpeciÔ¨Åcally, further study and comparison of approx-
types of slot representations as shown in Figure 24.            imately equivariant models to their counterparts in terms of
                                                                equivariance metrics, sample/runtime complexity, genera-
12.3 Limitations & Open Challenges                              tive modelling and semi-supervised learning performance
                                                                in 2D/3D complex tasks could be particularly important
It remains to be seen whether capsule networks will become      going forward.","Since most previous work      is carving out the role of approximately equivariant models
on capsule networks have only considered the category slot      like capsule networks in geometric deep learning [36], [93],
format, there is an opportunity to extend them to different     [142].","Moreover, given that capsule networks are
the next big thing.",2022-06-06 15:05:36+00:00,Learning with Capsules: A Survey,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Fabio De Sousa Ribeiro'), arxiv.Result.Author('Kevin Duarte'), arxiv.Result.Author('Miles Everett'), arxiv.Result.Author('Georgios Leontidis'), arxiv.Result.Author('Mubarak Shah')]","Capsule networks were proposed as an alternative approach to Convolutional
Neural Networks (CNNs) for learning object-centric representations, which can
be leveraged for improved generalization and sample complexity. Unlike CNNs,
capsule networks are designed to explicitly model part-whole hierarchical
relationships by using groups of neurons to encode visual entities, and learn
the relationships between those entities. Promising early results achieved by
capsule networks have motivated the deep learning community to continue trying
to improve their performance and scalability across several application areas.
However, a major hurdle for capsule network research has been the lack of a
reliable point of reference for understanding their foundational ideas and
motivations. The aim of this survey is to provide a comprehensive overview of
the capsule network research landscape, which will serve as a valuable resource
for the community going forward. To that end, we start with an introduction to
the fundamental concepts and motivations behind capsule networks, such as
equivariant inference in computer vision. We then cover the technical advances
in the capsule routing mechanisms and the various formulations of capsule
networks, e.g. generative and geometric. Additionally, we provide a detailed
explanation of how capsule networks relate to the popular attention mechanism
in Transformers, and highlight non-trivial conceptual similarities between them
in the context of representation learning. Afterwards, we explore the extensive
applications of capsule networks in computer vision, video and motion, graph
representation learning, natural language processing, medical imaging and many
others. To conclude, we provide an in-depth discussion regarding the main
hurdles in capsule network research, and highlight promising research
directions for future work.",-0.0145975165,-0.18000953,0.10560075,C
7203,"These reparameterizations     purpose solvers require learning complex dual witnesses
allow us to learn consistent attention units without solving     and are subject to further research.","Such general-
tention probability distributions.",the primal convex optimization program.,2022-06-06 17:38:00+00:00,Dual Decomposition of Convex Optimization Layers for Consistent Attention in Medical Images,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Tom Ron'), arxiv.Result.Author('Michal Weiler-Sagie'), arxiv.Result.Author('Tamir Hazan')]","A key concern in integrating machine learning models in medicine is the
ability to interpret their reasoning. Popular explainability methods have
demonstrated satisfactory results in natural image recognition, yet in medical
image analysis, many of these approaches provide partial and noisy
explanations. Recently, attention mechanisms have shown compelling results both
in their predictive performance and in their interpretable qualities. A
fundamental trait of attention is that it leverages salient parts of the input
which contribute to the model's prediction. To this end, our work focuses on
the explanatory value of attention weight distributions. We propose a
multi-layer attention mechanism that enforces consistent interpretations
between attended convolutional layers using convex optimization. We apply
duality to decompose the consistency constraints between the layers by
reparameterizing their attention probability distributions. We further suggest
learning the dual witness by optimizing with respect to our objective; thus,
our implementation uses standard back-propagation, hence it is highly
efficient. While preserving predictive performance, our proposed method
leverages weakly annotated medical imaging data and provides complete and
faithful explanations to the model's prediction.",0.14570573,-0.065342635,0.08381205,A
7204,"These reparameterizations     purpose solvers require learning complex dual witnesses
allow us to learn consistent attention units without solving     and are subject to further research.","Such general-
tention probability distributions.",the primal convex optimization program.,2022-06-06 17:38:00+00:00,Dual Decomposition of Convex Optimization Layers for Consistent Attention in Medical Images,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Tom Ron'), arxiv.Result.Author('Michal Weiler-Sagie'), arxiv.Result.Author('Tamir Hazan')]","A key concern in integrating machine learning models in medicine is the
ability to interpret their reasoning. Popular explainability methods have
demonstrated satisfactory results in natural image recognition, yet in medical
image analysis, many of these approaches provide partial and noisy
explanations. Recently, attention mechanisms have shown compelling results both
in their predictive performance and in their interpretable qualities. A
fundamental trait of attention is that it leverages salient parts of the input
which contribute to the model's prediction. To this end, our work focuses on
the explanatory value of attention weight distributions. We propose a
multi-layer attention mechanism that enforces consistent interpretations
between attended convolutional layers using convex optimization. We apply
duality to decompose the consistency constraints between the layers by
reparameterizing their attention probability distributions. We further suggest
learning the dual witness by optimizing with respect to our objective; thus,
our implementation uses standard back-propagation, hence it is highly
efficient. While preserving predictive performance, our proposed method
leverages weakly annotated medical imaging data and provides complete and
faithful explanations to the model's prediction.",0.14570573,-0.065342635,0.08381205,A
7267,"at comparing diÔ¨Äerent approaches in deep neural net-         However, if we could predict the charge levels through
works to fully understand which architecture performs        the research, then understanding their driving patterns
the best in terms of creating an optimal deep learning       would be more handy for further research projects.",Our project aims       vanced algorithm for further analysis in the dataset.,"solution for battery-equipped vehicles that considers
the charging problem from the utility (Power distri-            Previous works use relatively simple models like
bution system) point of view.",2022-06-07 22:56:40+00:00,Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.SP']","[arxiv.Result.Author('Seongwoo Choi'), arxiv.Result.Author('Chongzhou Fang'), arxiv.Result.Author('David Haddad'), arxiv.Result.Author('Minsung Kim')]","Convolutional Neural Networks (CNN) have been a good solution for
understanding a vast image dataset. As the increased number of battery-equipped
electric vehicles is flourishing globally, there has been much research on
understanding which charge levels electric vehicle drivers would choose to
charge their vehicles to get to their destination without any prevention. We
implemented deep learning approaches to analyze the tabular datasets to
understand their state of charge and which charge levels they would choose. In
addition, we implemented the Image Generator for Tabular Dataset algorithm to
utilize tabular datasets as image datasets to train convolutional neural
networks. Also, we integrated other CNN architecture such as EfficientNet to
prove that CNN is a great learner for reading information from images that were
converted from the tabular dataset, and able to predict charge levels for
battery-equipped electric vehicles. We also evaluated several optimization
methods to enhance the learning rate of the models and examined further
analysis on improving the model architecture.",-0.057201363,-0.25015453,0.146621,C
7271,"To the best of our knowledge,
this the the Ô¨Årst hyper-initializer for medical imaging tasks, and we expect that
the idea of hyper-initializing can be extent by the further researchers.","Optimizing the proposed algorithm with a simple self-supervised learn-
ing approach, extensive experimental evaluations reveal that the hyper-initializer
can enhance the accuracy and speed up the convergence for multiple medical im-
age modalities, especially in small data regime.","We will
try more architectures and self-supervised methods for our hyper-initializer in
the future.",2022-06-08 03:18:55+00:00,One Hyper-Initializer for All Network Architectures in Medical Image Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fangxin Shang'), arxiv.Result.Author('Yehui Yang'), arxiv.Result.Author('Dalu Yang'), arxiv.Result.Author('Junde Wu'), arxiv.Result.Author('Xiaorong Wang'), arxiv.Result.Author('Yanwu Xu')]","Pre-training is essential to deep learning model performance, especially in
medical image analysis tasks where limited training data are available.
However, existing pre-training methods are inflexible as the pre-trained
weights of one model cannot be reused by other network architectures. In this
paper, we propose an architecture-irrelevant hyper-initializer, which can
initialize any given network architecture well after being pre-trained for only
once. The proposed initializer is a hypernetwork which takes a downstream
architecture as input graphs and outputs the initialization parameters of the
respective architecture. We show the effectiveness and efficiency of the
hyper-initializer through extensive experimental results on multiple medical
imaging modalities, especially in data-limited fields. Moreover, we prove that
the proposed algorithm can be reused as a favorable plug-and-play initializer
for any downstream architecture and task (both classification and segmentation)
of the same modality.",0.06578197,-0.13785687,0.026394507,C
7272,"We further study the impact of the association quality                                 Wrong                                      Correct
towards our proposed method.","How is the performance affected by association
noise?","The fourth group shows                                       Association                                 Association
the results with the groundtruth association.",2022-06-08 03:37:59+00:00,Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Longlong Jing'), arxiv.Result.Author('Ruichi Yu'), arxiv.Result.Author('Henrik Kretzschmar'), arxiv.Result.Author('Kang Li'), arxiv.Result.Author('Charles R. Qi'), arxiv.Result.Author('Hang Zhao'), arxiv.Result.Author('Alper Ayvaci'), arxiv.Result.Author('Xu Chen'), arxiv.Result.Author('Dillon Cower'), arxiv.Result.Author('Yingwei Li'), arxiv.Result.Author('Yurong You'), arxiv.Result.Author('Han Deng'), arxiv.Result.Author('Congcong Li'), arxiv.Result.Author('Dragomir Anguelov')]","Monocular image-based 3D perception has become an active research area in
recent years owing to its applications in autonomous driving. Approaches to
monocular 3D perception including detection and tracking, however, often yield
inferior performance when compared to LiDAR-based techniques. Through
systematic analysis, we identified that per-object depth estimation accuracy is
a major factor bounding the performance. Motivated by this observation, we
propose a multi-level fusion method that combines different representations
(RGB and pseudo-LiDAR) and temporal information across multiple frames for
objects (tracklets) to enhance per-object depth estimation. Our proposed fusion
method achieves the state-of-the-art performance of per-object depth estimation
on the Waymo Open Dataset, the KITTI detection dataset, and the KITTI MOT
dataset. We further demonstrate that by simply replacing estimated depth with
fusion-enhanced depth, we can achieve significant improvements in monocular 3D
perception tasks, including detection and tracking.",0.34706166,0.07290209,-0.12414483,A
7273,"This result clearly demonstrates
that debiasing is a non-trivial task which cannot be solved by simply using more
parameters or layers, necessitating further research.","This strong correlation prevents models from learning intrinsic
attributes regardless of the backbone capacity.",14  Lee et al.,2022-06-08 05:24:13+00:00,DebiasBench: Benchmark for Fair Comparison of Debiasing in Image Classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jungsoo Lee'), arxiv.Result.Author('Juyoung Lee'), arxiv.Result.Author('Sanghun Jung'), arxiv.Result.Author('Jaegul Choo')]","Image classifiers often rely overly on peripheral attributes that have a
strong correlation with the target class (i.e., dataset bias) when making
predictions. Recently, a myriad of studies focus on mitigating such dataset
bias, the task of which is referred to as debiasing. However, these debiasing
methods often have inconsistent experimental settings (e.g., datasets and
neural network architectures). Additionally, most of the previous studies in
debiasing do not specify how they select their model parameters which involve
early stopping and hyper-parameter tuning. The goal of this paper is to
standardize the inconsistent experimental settings and propose a consistent
model parameter selection criterion for debiasing. Based on such unified
experimental settings and model parameter selection criterion, we build a
benchmark named DebiasBench which includes five datasets and seven debiasing
methods. We carefully conduct extensive experiments in various aspects and show
that different state-of-the-art methods work best in different datasets,
respectively. Even, the vanilla method, the method with no debiasing module,
also shows competitive results in datasets with low bias severity. We publicly
release the implementation of existing debiasing methods in DebiasBench to
encourage future researchers in debiasing to conduct fair comparisons and
further push the state-of-the-art performances.",0.06596015,-0.29124385,-0.008829102,C
7277,"ial Robustness
F. Ablation Study
                                                                                                                                                                                       In this study, there exists a postulation that the deep wide
   To further study the applicability of our method, the ablation                                                                                                                   neural network would boost adversarial robustness.","attack further validates the feasible robustness of our proposed                                                                                                                    H. InÔ¨Çuence Of Model Width And Model Depth On Adversar-
method.","To validate
study is implemented.",2022-06-08 08:00:30+00:00,Wavelet Regularization Benefits Adversarial Training,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jun Yan'), arxiv.Result.Author('Huilin Yin'), arxiv.Result.Author('Xiaoyang Deng'), arxiv.Result.Author('Ziming Zhao'), arxiv.Result.Author('Wancheng Ge'), arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Gerhard Rigoll')]","Adversarial training methods are state-of-the-art (SOTA) empirical defense
methods against adversarial examples. Many regularization methods have been
proven to be effective with the combination of adversarial training.
Nevertheless, such regularization methods are implemented in the time domain.
Since adversarial vulnerability can be regarded as a high-frequency phenomenon,
it is essential to regulate the adversarially-trained neural network models in
the frequency domain. Faced with these challenges, we make a theoretical
analysis on the regularization property of wavelets which can enhance
adversarial training. We propose a wavelet regularization method based on the
Haar wavelet decomposition which is named Wavelet Average Pooling. This wavelet
regularization module is integrated into the wide residual neural network so
that a new WideWaveletResNet model is formed. On the datasets of CIFAR-10 and
CIFAR-100, our proposed Adversarial Wavelet Training method realizes
considerable robustness under different types of attacks. It verifies the
assumption that our wavelet regularization method can enhance adversarial
robustness especially in the deep wide neural networks. The visualization
experiments of the Frequency Principle (F-Principle) and interpretability are
implemented to show the effectiveness of our method. A detailed comparison
based on different wavelet base functions is presented. The code is available
at the repository:
\url{https://github.com/momo1986/AdversarialWaveletTraining}.",0.1053531,-0.1847933,0.124279216,C
7278,"further research in the field of WSML to reach full label
model‚Äôs explanation is                                                                  performance.",Pointing Game.,"related to the human reasoning process [19, 34, 60].",2022-06-08 08:30:24+00:00,Large Loss Matters in Weakly Supervised Multi-Label Classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Youngwook Kim'), arxiv.Result.Author('Jae Myung Kim'), arxiv.Result.Author('Zeynep Akata'), arxiv.Result.Author('Jungwoo Lee')]","Weakly supervised multi-label classification (WSML) task, which is to learn a
multi-label classification using partially observed labels per image, is
becoming increasingly important due to its huge annotation cost. In this work,
we first regard unobserved labels as negative labels, casting the WSML task
into noisy multi-label classification. From this point of view, we empirically
observe that memorization effect, which was first discovered in a noisy
multi-class setting, also occurs in a multi-label setting. That is, the model
first learns the representation of clean labels, and then starts memorizing
noisy labels. Based on this finding, we propose novel methods for WSML which
reject or correct the large loss samples to prevent model from memorizing the
noisy label. Without heavy and complex components, our proposed methods
outperform previous state-of-the-art WSML methods on several partial label
settings including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3
datasets. Various analysis also show that our methodology actually works well,
validating that treating large loss properly matters in a weakly supervised
multi-label classification. Our code is available at
https://github.com/snucml/LargeLossMatters.",0.20274179,-0.089193374,-0.3752665,A
7291,"However, progressive GANomaly does need some further research to prove that it performs better
than GANomaly in other cases with diÔ¨Äerent pathologies.","This would in practice be a method that highlights designated spots that need to be checked and veriÔ¨Åed by
radiologists.","Since it performs better using toy images, does not
directly translate to the performance on real deformities, like seen in the WMH experiment on our in-house brain
MRI.",2022-06-08 13:13:01+00:00,Progressive GANomaly: Anomaly detection with progressively growing GANs,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Djennifer K. Madzia-Madzou'), arxiv.Result.Author('Hugo J. Kuijf')]","In medical imaging, obtaining large amounts of labeled data is often a
hurdle, because annotations and pathologies are scarce. Anomaly detection is a
method that is capable of detecting unseen abnormal data while only being
trained on normal (unannotated) data. Several algorithms based on generative
adversarial networks (GANs) exist to perform this task, yet certain limitations
are in place because of the instability of GANs. This paper proposes a new
method by combining an existing method, GANomaly, with progressively growing
GANs. The latter is known to be more stable, considering its ability to
generate high-resolution images. The method is tested using Fashion MNIST,
Medical Out-of-Distribution Analysis Challenge (MOOD), and in-house brain MRI;
using patches of sizes 16x16 and 32x32. Progressive GANomaly outperforms a
one-class SVM or regular GANomaly on Fashion MNIST. Artificial anomalies are
created in MOOD images with varying intensities and diameters. Progressive
GANomaly detected the most anomalies with varying intensity and size.
Additionally, the intermittent reconstructions are proven to be better from
progressive GANomaly. On the in-house brain MRI dataset, regular GANomaly
outperformed the other methods.",0.10242447,0.07508152,0.08165304,A
7301,"The reason behind this phenomenon needs a
further study.","Note, in the main paper, by default we estimate the parameters using DFT, except LSUN datasets
where we Ô¨Ånd empirically that DCT is a better choice.",Computational cost.,2022-06-08 17:41:14+00:00,Accelerating Score-based Generative Models for High-Resolution Image Synthesis,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jingfeng Zhang'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. The key idea is to produce high-quality images by
recurrently adding Gaussian noises and gradients to a Gaussian sample until
converging to the target distribution, a.k.a. the diffusion sampling. To ensure
stability of convergence in sampling and generation quality, however, this
sequential sampling process has to take a small step size and many sampling
iterations (e.g., 2000). Several acceleration methods have been proposed with
focus on low-resolution generation. In this work, we consider the acceleration
of high-resolution generation with SGMs, a more challenging yet more important
problem. We prove theoretically that this slow convergence drawback is
primarily due to the ignorance of the target distribution. Further, we
introduce a novel Target Distribution Aware Sampling (TDAS) method by
leveraging the structural priors in space and frequency domains. Extensive
experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can
consistently accelerate state-of-the-art SGMs, particularly on more challenging
high resolution (1024x1024) image generation tasks by up to 18.4x, whilst
largely maintaining the synthesis quality. With fewer sampling iterations, TDAS
can still generate good quality images. In contrast, the existing methods
degrade drastically or even fails completely",0.34117287,0.11247122,0.2702241,A
7302,"The reason behind this
phenomenon needs a further study.","Note, in the main paper, by default we estimate the parameters using DFT, except
LSUN datasets where we Ô¨Ånd empirically that DCT is a better choice.",Computational cost.,2022-06-08 17:41:14+00:00,Accelerating Score-based Generative Models for High-Resolution Image Synthesis,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jingfeng Zhang'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. The key idea is to produce high-quality images by
recurrently adding Gaussian noises and gradients to a Gaussian sample until
converging to the target distribution, a.k.a. the diffusion sampling. To ensure
stability of convergence in sampling and generation quality, however, this
sequential sampling process has to take a small step size and many sampling
iterations (e.g., 2000). Several acceleration methods have been proposed with
focus on low-resolution generation. In this work, we consider the acceleration
of high-resolution generation with SGMs, a more challenging yet more important
problem. We prove theoretically that this slow convergence drawback is
primarily due to the ignorance of the target distribution. Further, we
introduce a novel Target Distribution Aware Sampling (TDAS) method by
leveraging the structural priors in space and frequency domains. Extensive
experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can
consistently accelerate state-of-the-art SGMs, particularly on more challenging
high resolution (1024x1024) image generation tasks by up to 18.4x, whilst
largely maintaining the synthesis quality. With fewer sampling iterations, TDAS
can still generate good quality images. In contrast, the existing methods
degrade drastically or even fails completely",0.34117287,0.11247122,0.2702241,A
7303,"The reason behind this
phenomenon needs a further study.","Note, in the main paper, by default we estimate the parameters using DFT, except
LSUN datasets where we Ô¨Ånd empirically that DCT is a better choice.",Computational cost.,2022-06-08 17:41:14+00:00,Accelerating Score-based Generative Models for High-Resolution Image Synthesis,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jingfeng Zhang'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. The key idea is to produce high-quality images by
recurrently adding Gaussian noises and gradients to a Gaussian sample until
converging to the target distribution, a.k.a. the diffusion sampling. To ensure
stability of convergence in sampling and generation quality, however, this
sequential sampling process has to take a small step size and many sampling
iterations (e.g., 2000). Several acceleration methods have been proposed with
focus on low-resolution generation. In this work, we consider the acceleration
of high-resolution generation with SGMs, a more challenging yet more important
problem. We prove theoretically that this slow convergence drawback is
primarily due to the ignorance of the target distribution. Further, we
introduce a novel Target Distribution Aware Sampling (TDAS) method by
leveraging the structural priors in space and frequency domains. Extensive
experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can
consistently accelerate state-of-the-art SGMs, particularly on more challenging
high resolution (1024x1024) image generation tasks by up to 18.4x, whilst
largely maintaining the synthesis quality. With fewer sampling iterations, TDAS
can still generate good quality images. In contrast, the existing methods
degrade drastically or even fails completely",0.34117287,0.11247122,0.2702241,A
7399,We hope this work can serve as a solid baseline for generalist models and motivate further research.,"‚Ä¢ Compared with previous SOTAs, our generalist model with 1% downstream data prompt tuning
   achieves competitive performance, while only <5% training data and <10% training cost are used.","2 Related Works

Specialized Models.",2022-06-09 17:59:59+00:00,Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinguo Zhu'), arxiv.Result.Author('Xizhou Zhu'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Xiaohua Wang'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Xiaogang Wang'), arxiv.Result.Author('Jifeng Dai')]","To build an artificial neural network like the biological intelligence
system, recent works have unified numerous tasks into a generalist model, which
can process various tasks with shared parameters and do not have any
task-specific modules. While generalist models achieve promising results on
various benchmarks, they have performance degradation on some tasks compared
with task-specialized models. In this work, we find that interference among
different tasks and modalities is the main factor to this phenomenon. To
mitigate such interference, we introduce the Conditional Mixture-of-Experts
(Conditional MoEs) to generalist models. Routing strategies under different
levels of conditions are proposed to take both the training/inference cost and
generalization ability into account. By incorporating the proposed Conditional
MoEs, the recently proposed generalist model Uni-Perceiver can effectively
mitigate the interference across tasks and modalities, and achieves
state-of-the-art results on a series of downstream tasks via prompt tuning on
1% of downstream data. Moreover, the introduction of Conditional MoEs still
holds the generalization ability of generalist models to conduct zero-shot
inference on new tasks, e.g., video-text retrieval and video caption. Code and
pre-trained generalist models shall be released.",0.19946492,-0.3528813,-0.093131274,A
7400,We hope this work can motivate further research in generalist models.,"With prompt tuning on 1% downstream data, the proposed sparse
generalist model achieves competitive performance with previous SOTAs using only <5% training
data and <10% training cost.",Acknowledgments.,2022-06-09 17:59:59+00:00,Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinguo Zhu'), arxiv.Result.Author('Xizhou Zhu'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Xiaohua Wang'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Xiaogang Wang'), arxiv.Result.Author('Jifeng Dai')]","To build an artificial neural network like the biological intelligence
system, recent works have unified numerous tasks into a generalist model, which
can process various tasks with shared parameters and do not have any
task-specific modules. While generalist models achieve promising results on
various benchmarks, they have performance degradation on some tasks compared
with task-specialized models. In this work, we find that interference among
different tasks and modalities is the main factor to this phenomenon. To
mitigate such interference, we introduce the Conditional Mixture-of-Experts
(Conditional MoEs) to generalist models. Routing strategies under different
levels of conditions are proposed to take both the training/inference cost and
generalization ability into account. By incorporating the proposed Conditional
MoEs, the recently proposed generalist model Uni-Perceiver can effectively
mitigate the interference across tasks and modalities, and achieves
state-of-the-art results on a series of downstream tasks via prompt tuning on
1% of downstream data. Moreover, the introduction of Conditional MoEs still
holds the generalization ability of generalist models to conduct zero-shot
inference on new tasks, e.g., video-text retrieval and video caption. Code and
pre-trained generalist models shall be released.",0.21494445,-0.32740355,0.0037824688,A
7401,We hope this work can serve as a solid baseline for generalist models and motivate further research.,"‚Ä¢ Compared with previous SOTAs, our generalist model with 1% downstream data prompt tuning
   achieves competitive performance, while only <5% training data and <10% training cost are used.","2 Related Works

Specialized Models.",2022-06-09 17:59:59+00:00,Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinguo Zhu'), arxiv.Result.Author('Xizhou Zhu'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Xiaohua Wang'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Xiaogang Wang'), arxiv.Result.Author('Jifeng Dai')]","To build an artificial neural network like the biological intelligence
system, recent works have unified numerous tasks into a generalist model, which
can process various tasks with shared parameters and do not have any
task-specific modules. While generalist models achieve promising results on
various benchmarks, they have performance degradation on some tasks compared
with task-specialized models. In this work, we find that interference among
different tasks and modalities is the main factor to this phenomenon. To
mitigate such interference, we introduce the Conditional Mixture-of-Experts
(Conditional MoEs) to generalist models. Routing strategies under different
levels of conditions are proposed to take both the training/inference cost and
generalization ability into account. By incorporating the proposed Conditional
MoEs, the recently proposed generalist model Uni-Perceiver can effectively
mitigate the interference across tasks and modalities, and achieves
state-of-the-art results on a series of downstream tasks via prompt tuning on
1% of downstream data. Moreover, the introduction of Conditional MoEs still
holds the generalization ability of generalist models to conduct zero-shot
inference on new tasks, e.g., video-text retrieval and video caption. Code and
pre-trained generalist models shall be released.",0.19946492,-0.3528813,-0.093131274,A
7402,We hope this work can motivate further research in generalist models.,"With prompt tuning on 1% downstream data, the proposed sparse
generalist model achieves competitive performance with previous SOTAs using only <5% training
data and <10% training cost.",Acknowledgments.,2022-06-09 17:59:59+00:00,Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinguo Zhu'), arxiv.Result.Author('Xizhou Zhu'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Xiaohua Wang'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Xiaogang Wang'), arxiv.Result.Author('Jifeng Dai')]","To build an artificial neural network like the biological intelligence
system, recent works have unified numerous tasks into a generalist model, which
can process various tasks with shared parameters and do not have any
task-specific modules. While generalist models achieve promising results on
various benchmarks, they have performance degradation on some tasks compared
with task-specialized models. In this work, we find that interference among
different tasks and modalities is the main factor to this phenomenon. To
mitigate such interference, we introduce the Conditional Mixture-of-Experts
(Conditional MoEs) to generalist models. Routing strategies under different
levels of conditions are proposed to take both the training/inference cost and
generalization ability into account. By incorporating the proposed Conditional
MoEs, the recently proposed generalist model Uni-Perceiver can effectively
mitigate the interference across tasks and modalities, and achieves
state-of-the-art results on a series of downstream tasks via prompt tuning on
1% of downstream data. Moreover, the introduction of Conditional MoEs still
holds the generalization ability of generalist models to conduct zero-shot
inference on new tasks, e.g., video-text retrieval and video caption. Code and
pre-trained generalist models shall be released.",0.21494445,-0.32740355,0.0037824688,A
7452,"The resulting network                                     on their own, and may foster further research as well as
is then used to produce predictions on a batch of training                                        identiÔ¨Åcation of the most fruitful outcomes toward applications.","In each step of the optimization, the embedding                                        Yet, we deem it worth communicating these Ô¨Åndings to the
is processed by the frozen decoder, which predicts a PRep                                         community as we believe they are non-obvious and valuable
that is loaded into a ResNet8 instance.",images from TIN.,2022-06-10 15:53:35+00:00,Learning the Space of Deep Models,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Gianluca Berardi'), arxiv.Result.Author('Luca De Luigi'), arxiv.Result.Author('Samuele Salti'), arxiv.Result.Author('Luigi Di Stefano')]","Embedding of large but redundant data, such as images or text, in a hierarchy
of lower-dimensional spaces is one of the key features of representation
learning approaches, which nowadays provide state-of-the-art solutions to
problems once believed hard or impossible to solve. In this work, in a plot
twist with a strong meta aftertaste, we show how trained deep models are as
redundant as the data they are optimized to process, and how it is therefore
possible to use deep learning models to embed deep learning models. In
particular, we show that it is possible to use representation learning to learn
a fixed-size, low-dimensional embedding space of trained deep models and that
such space can be explored by interpolation or optimization to attain
ready-to-use models. We find that it is possible to learn an embedding space of
multiple instances of the same architecture and of multiple architectures. We
address image classification and neural representation of signals, showing how
our embedding space can be learnt so as to capture the notions of performance
and 3D shape, respectively. In the Multi-Architecture setting we also show how
an embedding trained only on a subset of architectures can learn to generate
already-trained instances of architectures it never sees instantiated at
training time.",0.06342222,-0.29999837,0.14812213,C
7484,"‚Ä¢ To encourage further research in developing multi-task perception algo-
              rithms, the code was made public on the Github, which proved to be quite
              popular in the vision community and signiÔ¨Åcantly helped the discernibility
              of the approach.","The dataset and the corresponding parsers
              will be released to the public on Github and will be part of the long-term
              maintenance.","‚Ä¢ Integrated my novel approaches of distance estimation on Ô¨Åsheye camera
              images which are discussed in papers [2, 3, 4, 5, 6].",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.23548287,0.16614506,-0.13769399,B
7485,"To encourage further research in
1.2.","We present novel represen-
tations of bounding boxes on raw Ô¨Åsheye camera images i.e., oriented bounding box,
ellipse, and generic polygon for object detection.","Outline  7

this direction, we will also make a public release of the dataset comprising 10, 000
images with annotations for all the object representations.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.35932532,0.18863392,-0.0609505,B
7486,"A baseline code is released along with the dataset
on GitHub to encourage further research to the community in developing uniÔ¨Åed
perception models for autonomous driving.","A sub-set of 10,000 images from the dataset
will be made public on Github2.","It contains several perception tasks listed
in Figure 2.24.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.29685527,0.097726315,-0.09850338,B
7487,"To encourage further research, we also made a
public release of a dataset of 10,000 images with annotations for all considered object
representations.","In this thesis, to the best of our knowledge, we
perform the Ô¨Årst detailed study on object detection based on Ô¨Åsheye camera images
for autonomous driving scenarios.","This work was formally presented as Generalized Object Detection [4]
as an oral at the WACV conference in 2021.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.4562745,0.06718566,-0.1715768,B
7488,"We consider our method to be a baseline for further research
into this area and will make the dataset with ground truth annotation for various
representations publicly available.","Overall, the
proposed model improves the relative mIoU accuracy signiÔ¨Åcantly by 40% compared
to a YOLOv3 baseline.","We hope this encourages further research in this
area leading to a more mature object detection on raw Ô¨Åsheye imagery.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.35811096,0.027706329,0.010793554,B
7489,"We hope this encourages further research in this
area leading to a more mature object detection on raw Ô¨Åsheye imagery.","We consider our method to be a baseline for further research
into this area and will make the dataset with ground truth annotation for various
representations publicly available.","The Ô¨Ånal chapter will look into a holistic scene understanding of an autonomous
136  Chapter 6.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.45372093,0.06953942,-0.11383082,B
7490,"To
encourage further research in developing multi-task perception algorithms, the code
was made public on the Github, which proved to be quite popular in the vision
community and signiÔ¨Åcantly helped the discernibility of the approach.","The ablation on Adversarial Attacks [15, 470] was presented at the ITSC in 2021.","Further
details about the dataset usage and demo code can be found on the WoodScape
website https://woodscape.valeo.com.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.08221962,-0.05800563,-0.091094375,C
7491,"We hope that this thesis encourages
further research in building a uniÔ¨Åed perception model for autonomous driving.","There are still many practical challenges in scaling to a higher
number of tasks: building a diverse and balanced dataset, corner case mining, stable
training mechanisms, and designing an optimal map representation that combines
all tasks, thereby reducing the post-processing.","This chapter also applied various adversarial attacks to understand the vulnerabilities
of the perception network.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.2447698,-0.0994066,0.012425683,B
7492,"We hope this encourages
further research in this area leading to a mature object detection on undistorted
Ô¨Åsheye images.","Henceforth, we compiled a 2D object detection
dataset on Ô¨Åsheye images and released 10,000 images.","8.3.4 Limitations of Generalized Object Detection

Polygon representation is a better way to represent objects, but it does not improve the
CNN ability to detect objects on Ô¨Åsheye.",2022-06-11 14:51:30+00:00,Surround-View Cameras based Holistic Visual Perception for Automated Driving,cs.CV,['cs.CV'],[arxiv.Result.Author('Varun Ravi Kumar')],"The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",-0.45918676,0.08214833,0.03355419,B
7524,"For a further study,                  difficult.","In
baseline situations, the user only needs to click several vertices to          Q1 Attention annotating with the click-based user interface was
draw the boundary of the target in the image.","we also conduct another comparison test with polygon-based user
interface and proposed attention-based data selection strategy.",2022-06-13 09:04:32+00:00,Efficient Human-in-the-loop System for Guiding DNNs Attention,cs.CV,"['cs.CV', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Yi He'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Chia-Ming Chang'), arxiv.Result.Author('Haoran Xie'), arxiv.Result.Author('Takeo Igarashi')]","Attention guidance is an approach to addressing dataset bias in deep
learning, where the model relies on incorrect features to make decisions.
Focusing on image classification tasks, we propose an efficient
human-in-the-loop system to interactively direct the attention of classifiers
to the regions specified by users, thereby reducing the influence of
co-occurrence bias and improving the transferability and interpretability of a
DNN. Previous approaches for attention guidance require the preparation of
pixel-level annotations and are not designed as interactive systems. We present
a new interactive method to allow users to annotate images with simple clicks,
and study a novel active learning strategy to significantly reduce the number
of annotations. We conducted both a numerical evaluation and a user study to
evaluate the proposed system on multiple datasets. Compared to the existing
non-active-learning approach which usually relies on huge amounts of
polygon-based segmentation masks to fine-tune or train the DNNs, our system can
save lots of labor and money and obtain a fine-tuned network that works better
even when the dataset is biased. The experiment results indicate that the
proposed system is efficient, reasonable, and reliable.",0.036122877,-0.05791394,-0.27384806,C
7525,"For a further study,                  difficult.","In
baseline situations, the user only needs to click several vertices to          Q1 Attention annotating with the click-based user interface was
draw the boundary of the target in the image.","we also conduct another comparison test with polygon-based user
interface and proposed attention-based data selection strategy.",2022-06-13 09:04:32+00:00,Efficient Human-in-the-loop System for Guiding DNNs Attention,cs.CV,"['cs.CV', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Yi He'), arxiv.Result.Author('Xi Yang'), arxiv.Result.Author('Chia-Ming Chang'), arxiv.Result.Author('Haoran Xie'), arxiv.Result.Author('Takeo Igarashi')]","Attention guidance is an approach to addressing dataset bias in deep
learning, where the model relies on incorrect features to make decisions.
Focusing on image classification tasks, we propose an efficient
human-in-the-loop system to interactively direct the attention of classifiers
to the regions specified by users, thereby reducing the influence of
co-occurrence bias and improving the transferability and interpretability of a
DNN. Previous approaches for attention guidance require the preparation of
pixel-level annotations and are not designed as interactive systems. We present
a new interactive method to allow users to annotate images with simple clicks,
and study a novel active learning strategy to significantly reduce the number
of annotations. We conducted both a numerical evaluation and a user study to
evaluate the proposed system on multiple datasets. Compared to the existing
non-active-learning approach which usually relies on huge amounts of
polygon-based segmentation masks to fine-tune or train the DNNs, our system can
save lots of labor and money and obtain a fine-tuned network that works better
even when the dataset is biased. The experiment results indicate that the
proposed system is efficient, reasonable, and reliable.",0.036122877,-0.05791394,-0.27384806,C
7562,"In the future, we will further study the
                                                                                                          challenging blind LF image SR problem, and try to design a
   In this paper, we proposed the Ô¨Årst work to handle LF image                                            more practical method for real-world LF image SR. We believe
SR with multiple degradations.",CONCLUSION AND DISCUSSION                                                               or over-smoothness.,"We developed an LF degra-                                                  that our LF-DAnet will serve as a fundamental work and can
dation model that considers both blur and noise at multiple                                               inspire more researchers to focus on real-world LF image SR.
levels, and proposed an LF-DAnet that can adapt to different
   Blur Kernel Width                       Blur Kernel Width                                                                                                            12

                                                                                                       Blur Kernel Width

4                                       4                                                           4

3                                       3                                                           3

2                                       2                                                           2

1                                       1                                                           1

0                                       0                                                           0

   0  15              30  45  60 Noise     0  15                                  30  45  60 Noise     0  15  30  45             60 Noise

      (a) EPFL_ISO_Chart      Level                           (b) STFlytro_general_11     Level               (c) Raytrix_boxer  Level

Fig.",2022-06-13 14:44:46+00:00,Learning a Degradation-Adaptive Network for Light Field Image Super-Resolution,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Yingqian Wang'), arxiv.Result.Author('Zhengyu Liang'), arxiv.Result.Author('Longguang Wang'), arxiv.Result.Author('Jungang Yang'), arxiv.Result.Author('Wei An'), arxiv.Result.Author('Yulan Guo')]","Recent years have witnessed the great advances of deep neural networks (DNNs)
in light field (LF) image super-resolution (SR). However, existing DNN-based LF
image SR methods are developed on a single fixed degradation (e.g., bicubic
downsampling), and thus cannot be applied to super-resolve real LF images with
diverse degradations. In this paper, we propose the first method to handle LF
image SR with multiple degradations. In our method, a practical LF degradation
model that considers blur and noise is developed to approximate the degradation
process of real LF images. Then, a degradation-adaptive network (LF-DAnet) is
designed to incorporate the degradation prior into the SR process. By training
on LF images with multiple synthetic degradations, our method can learn to
adapt to different degradations while incorporating the spatial and angular
information. Extensive experiments on both synthetically degraded and
real-world LFs demonstrate the effectiveness of our method. Compared with
existing state-of-the-art single and LF image SR methods, our method achieves
superior SR performance under a wide range of degradations, and generalizes
better to real LF images. Codes and models are available at
https://github.com/YingqianWang/LF-DAnet.",0.052319862,0.2247509,0.22153598,C
7577,"Overall, we believe our work has shown the promise of energy-based
approaches for SSL, and hope that it will spur further research in this direction.","Devising ways to better understand it would
be interesting future work.","In terms of societal impact, research in SSL has the potential to positively impact real-world applica-
tions that require lots of labeled data by reducing annotation effort and cost.",2022-06-13 17:55:07+00:00,EnergyMatch: Energy-based Pseudo-Labeling for Semi-Supervised Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zhuoran Yu'), arxiv.Result.Author('Yin Li'), arxiv.Result.Author('Yong Jae Lee')]","Recent state-of-the-art methods in semi-supervised learning (SSL) combine
consistency regularization with confidence-based pseudo-labeling. To obtain
high-quality pseudo-labels, a high confidence threshold is typically adopted.
However, it has been shown that softmax-based confidence scores in deep
networks can be arbitrarily high for samples far from the training data, and
thus, the pseudo-labels for even high-confidence unlabeled samples may still be
unreliable. In this work, we present a new perspective of pseudo-labeling:
instead of relying on model confidence, we instead measure whether an unlabeled
sample is likely to be ""in-distribution""; i.e., close to the current training
data. To classify whether an unlabeled sample is ""in-distribution"" or
""out-of-distribution"", we adopt the energy score from out-of-distribution
detection literature. As training progresses and more unlabeled samples become
in-distribution and contribute to training, the combined labeled and
pseudo-labeled data can better approximate the true distribution to improve the
model. Experiments demonstrate that our energy-based pseudo-labeling method,
albeit conceptually simple, significantly outperforms confidence-based methods
on imbalanced SSL benchmarks, and achieves competitive performance on
class-balanced data. For example, it produces a 4-6% absolute accuracy
improvement on CIFAR10-LT when the imbalance ratio is higher than 50. When
combined with state-of-the-art long-tailed SSL methods, further improvements
are attained.",0.23148027,-0.055000186,-0.2195797,A
7579,"In addition, we hope this work could in-
spire further study on exploring the potential of MLP-Like architecture in more
skeleton-based tasks.","As the MLPs and GCNs in our GraphMLP
are straightforward, we look forward to combining more powerful MLPs or GCNs
to further improve the performance.","GraphMLP  15

References

 1.",2022-06-13 18:59:31+00:00,GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Wenhao Li'), arxiv.Result.Author('Hong Liu'), arxiv.Result.Author('Tianyu Guo'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Runwei Ding')]","Modern multi-layer perceptron (MLP) models have shown competitive results in
learning visual representations without self-attention. However, existing MLP
models are not good at capturing local details and lack prior knowledge of
human configurations, which limits their modeling power for skeletal
representation learning. To address these issues, we propose a simple yet
effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines
MLPs and graph convolutional networks (GCNs) in a global-local-graphical
unified architecture for 3D human pose estimation. GraphMLP incorporates the
graph structure of human bodies into an MLP model to meet the domain-specific
demand while also allowing for both local and global spatial interactions.
Extensive experiments show that the proposed GraphMLP achieves state-of-the-art
performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Our source code
and pretrained models will be publicly available.",0.04785494,-0.01006085,-0.052709833,A
7580,"In addition, we hope
this work could inspire further study on exploring the potential              [10] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas, ‚ÄúSemantic
of MLP-Like architecture in more skeleton-based tasks.","1, 5, 7
GCNs to further improve the performance.","graph convolutional networks for 3D human pose regression,‚Äù in CVPR,
                                                                                    2019, pp.",2022-06-13 18:59:31+00:00,GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Wenhao Li'), arxiv.Result.Author('Hong Liu'), arxiv.Result.Author('Tianyu Guo'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Runwei Ding')]","Modern multi-layer perceptron (MLP) models have shown competitive results in
learning visual representations without self-attention. However, existing MLP
models are not good at capturing local details and lack prior knowledge of
human body configurations, which limits their modeling power for skeletal
representation learning. To address these issues, we propose a simple yet
effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines
MLPs and graph convolutional networks (GCNs) in a global-local-graphical
unified architecture for 3D human pose estimation. GraphMLP incorporates the
graph structure of human bodies into an MLP model to meet the domain-specific
demand of the 3D human pose, while allowing for both local and global spatial
interactions. Furthermore, we propose to flexibly and efficiently extend the
GraphMLP to the video domain and show that complex temporal dynamics can be
effectively modeled in a simple way with negligible computational cost gains in
the sequence length. To the best of our knowledge, this is the first MLP-Like
architecture for 3D human pose estimation in a single frame and a video
sequence. Extensive experiments show that the proposed GraphMLP achieves
state-of-the-art performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP.
Our source code will be open-sourced.",-0.2595116,-0.06121864,-0.033982314,B
7584,"To apply AutoDA
techniques in practice, further research and experiments are required.","Neither approach is able to make signiÔ¨Åcant progress on
model accuracy when compared with previous approaches.","3 Automated Data Augmentation Techniques

This section aims at introducing the basic concepts and terminologies of
Automated Data Augmentation (AutoDA) techniques.",2022-06-14 01:40:09+00:00,A Survey of Automated Data Augmentation Algorithms for Deep Learning-based Image Classication Tasks,cs.CV,"['cs.CV', 'A.1, I.4.3, I.5.2']","[arxiv.Result.Author('Zihan Yang'), arxiv.Result.Author('Richard O. Sinnott'), arxiv.Result.Author('James Bailey'), arxiv.Result.Author('Qiuhong Ke')]","In recent years, one of the most popular techniques in the computer vision
community has been the deep learning technique. As a data-driven technique,
deep model requires enormous amounts of accurately labelled training data,
which is often inaccessible in many real-world applications. A data-space
solution is Data Augmentation (DA), that can artificially generate new images
out of original samples. Image augmentation strategies can vary by dataset, as
different data types might require different augmentations to facilitate model
training. However, the design of DA policies has been largely decided by the
human experts with domain knowledge, which is considered to be highly
subjective and error-prone. To mitigate such problem, a novel direction is to
automatically learn the image augmentation policies from the given dataset
using Automated Data Augmentation (AutoDA) techniques. The goal of AutoDA
models is to find the optimal DA policies that can maximize the model
performance gains. This survey discusses the underlying reasons of the
emergence of AutoDA technology from the perspective of image classification. We
identify three key components of a standard AutoDA model: a search space, a
search algorithm and an evaluation function. Based on their architecture, we
provide a systematic taxonomy of existing image AutoDA approaches. This paper
presents the major works in AutoDA field, discussing their pros and cons, and
proposing several potential directions for future improvements.",0.07595224,-0.07774491,-0.10011288,A
7585,"To apply AutoDA
techniques in practice, further research and experiments are required.","Neither approach is able to make signiÔ¨Åcant progress on
model accuracy when compared with previous approaches.","3 Automated Data Augmentation Techniques

This section aims at introducing the basic concepts and terminologies of
Automated Data Augmentation (AutoDA) techniques.",2022-06-14 01:40:09+00:00,A Survey of Automated Data Augmentation Algorithms for Deep Learning-based Image Classification Tasks,cs.CV,"['cs.CV', 'A.1, I.4.3, I.5.2']","[arxiv.Result.Author('Zihan Yang'), arxiv.Result.Author('Richard O. Sinnott'), arxiv.Result.Author('James Bailey'), arxiv.Result.Author('Qiuhong Ke')]","In recent years, one of the most popular techniques in the computer vision
community has been the deep learning technique. As a data-driven technique,
deep model requires enormous amounts of accurately labelled training data,
which is often inaccessible in many real-world applications. A data-space
solution is Data Augmentation (DA), that can artificially generate new images
out of original samples. Image augmentation strategies can vary by dataset, as
different data types might require different augmentations to facilitate model
training. However, the design of DA policies has been largely decided by the
human experts with domain knowledge, which is considered to be highly
subjective and error-prone. To mitigate such problem, a novel direction is to
automatically learn the image augmentation policies from the given dataset
using Automated Data Augmentation (AutoDA) techniques. The goal of AutoDA
models is to find the optimal DA policies that can maximize the model
performance gains. This survey discusses the underlying reasons of the
emergence of AutoDA technology from the perspective of image classification. We
identify three key components of a standard AutoDA model: a search space, a
search algorithm and an evaluation function. Based on their architecture, we
provide a systematic taxonomy of existing image AutoDA approaches. This paper
presents the major works in AutoDA field, discussing their pros and cons, and
proposing several potential directions for future improvements.",0.07595224,-0.07774491,-0.10011288,A
7606,"such a joint-removal task becomes an open problem in the
community and calls for further study.",Thus        by a considerable margin.,II.,2022-06-14 12:50:41+00:00,Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Yuan Feng'), arxiv.Result.Author('Yaojun Hu'), arxiv.Result.Author('Pengfei Fang'), arxiv.Result.Author('Yanhong Yang'), arxiv.Result.Author('Sheng Liu'), arxiv.Result.Author('Shengyong Chen')]","This work studies the joint rain and haze removal problem. In real-life
scenarios, rain and haze, two often co-occurring common weather phenomena, can
greatly degrade the clarity and quality of the scene images, leading to a
performance drop in the visual applications, such as autonomous driving.
However, jointly removing the rain and haze in scene images is ill-posed and
challenging, where the existence of haze and rain and the change of atmosphere
light, can both degrade the scene information. Current methods focus on the
contamination removal part, thus ignoring the restoration of the scene
information affected by the change of atmospheric light. We propose a novel
deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address
the aforementioned challenge. The ADU-Net produces both the contamination
residual and the scene residual to efficiently remove the rain and haze while
preserving the fidelity of the scene information. Extensive experiments show
our work outperforms the existing state-of-the-art methods by a considerable
margin in both synthetic data and real-world data benchmarks, including
RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the
state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data,
respectively.
  Codes will be made available freely to the research community.",0.19280778,0.034075327,-0.26292562,A
7607,"such a joint-removal task becomes an open problem in the
community and calls for further study.",Thus        by a considerable margin.,II.,2022-06-14 12:50:41+00:00,Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Yuan Feng'), arxiv.Result.Author('Yaojun Hu'), arxiv.Result.Author('Pengfei Fang'), arxiv.Result.Author('Yanhong Yang'), arxiv.Result.Author('Sheng Liu'), arxiv.Result.Author('Shengyong Chen')]","This work studies the joint rain and haze removal problem. In real-life
scenarios, rain and haze, two often co-occurring common weather phenomena, can
greatly degrade the clarity and quality of the scene images, leading to a
performance drop in the visual applications, such as autonomous driving.
However, jointly removing the rain and haze in scene images is ill-posed and
challenging, where the existence of haze and rain and the change of atmosphere
light, can both degrade the scene information. Current methods focus on the
contamination removal part, thus ignoring the restoration of the scene
information affected by the change of atmospheric light. We propose a novel
deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address
the aforementioned challenge. The ADU-Net produces both the contamination
residual and the scene residual to efficiently remove the rain and haze while
preserving the fidelity of the scene information. Extensive experiments show
our work outperforms the existing state-of-the-art methods by a considerable
margin in both synthetic data and real-world data benchmarks, including
RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the
state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data,
respectively.
  Codes will be made available freely to the research community.",0.19280778,0.034075327,-0.26292562,A
7654,"We believe further research in this direction can beneÔ¨Åt a variety of
image matching, visual localization, and recognition problems in computer vision.","Moreover, the use of our patch pose estimation has been shown to im-
prove matching performance on HPatches and IMC2021 benchmarks, on which our method
has never been trained.","Acknowledgement

This work was supported by Samsung Electronics Co., Ltd., the NRF grant (NRF-2021R1A2
C3012728; NRF-2019H1A2A1076171 - Global Ph.D. Fellowship Program), and the IITP
grant (No.2019-0-01906, AI Graduate School Program - POSTECH) funded by Ministry of
Science and ICT, Korea.",2022-06-15 02:43:39+00:00,Self-Supervised Learning of Image Scale and Orientation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jongmin Lee'), arxiv.Result.Author('Yoonwoo Jeong'), arxiv.Result.Author('Minsu Cho')]","We study the problem of learning to assign a characteristic pose, i.e., scale
and orientation, for an image region of interest. Despite its apparent
simplicity, the problem is non-trivial; it is hard to obtain a large-scale set
of image regions with explicit pose annotations that a model directly learns
from. To tackle the issue, we propose a self-supervised learning framework with
a histogram alignment technique. It generates pairs of image patches by random
rescaling/rotating and then train an estimator to predict their
scale/orientation values so that their relative difference is consistent with
the rescaling/rotating used. The estimator learns to predict a non-parametric
histogram distribution of scale/orientation without any supervision.
Experiments show that it significantly outperforms previous methods in
scale/orientation estimation and also improves image matching and 6 DoF camera
pose estimation by incorporating our patch poses into a matching process.",-0.21803938,0.15206024,-0.0011319965,B
7681,"Please refer to our supplementary material for
further study of the deformation feature grid design.","This result clearly demonstrates the effectiveness of the pro-
posed deformation feature grid.","6 Conclusions

In this paper, we have presented a fast optimization method for dynamic view
synthesis based on a hybrid implicit-explicit representation.",2022-06-15 17:49:08+00:00,Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiang Guo'), arxiv.Result.Author('Guanying Chen'), arxiv.Result.Author('Yuchao Dai'), arxiv.Result.Author('Xiaoqing Ye'), arxiv.Result.Author('Jiadai Sun'), arxiv.Result.Author('Xiao Tan'), arxiv.Result.Author('Errui Ding')]","Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel
view synthesis (NVS) for its superior performance. However, NeRF and its
variants generally require a lengthy per-scene training procedure, where a
multi-layer perceptron (MLP) is fitted to the captured images. To remedy the
challenge, the voxel-grid representation has been proposed to significantly
speed up the training. However, these existing methods can only deal with
static scenes. How to develop an efficient and accurate dynamic view synthesis
method remains an open problem. Extending the methods for static scenes to
dynamic scenes is not straightforward as both the scene geometry and appearance
change over time. In this paper, built on top of the recent advances in
voxel-grid optimization, we propose a fast deformable radiance field method to
handle dynamic scenes. Our method consists of two modules. The first module
adopts a deformation grid to store 3D dynamic features, and a light-weight MLP
for decoding the deformation that maps a 3D point in observation space to the
canonical space using the interpolated features. The second module contains a
density and a color grid to model the geometry and density of the scene. The
occlusion is explicitly modeled to further improve the rendering quality.
Experimental results show that our method achieves comparable performance to
D-NeRF using only 20 minutes for training, which is more than 70x faster than
D-NeRF, clearly demonstrating the efficiency of our proposed method.",-0.081174396,0.27614784,-0.015807662,B
7682,"Please refer to our supplementary material for
further study of the deformation feature grid design.","This result clearly demonstrates the effectiveness of the pro-
posed deformation feature grid.","Canonical (t = 0)             t = 0.2      t = 0.4  t = 0.6  t = 0.8     t = 1.0
                                           ùë° = 0.4                        ùë° = 1.0
14             X. Guo et al.",2022-06-15 17:49:08+00:00,Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiang Guo'), arxiv.Result.Author('Guanying Chen'), arxiv.Result.Author('Yuchao Dai'), arxiv.Result.Author('Xiaoqing Ye'), arxiv.Result.Author('Jiadai Sun'), arxiv.Result.Author('Xiao Tan'), arxiv.Result.Author('Errui Ding')]","Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel
view synthesis (NVS) for its superior performance. In this paper, we propose to
synthesize dynamic scenes. Extending the methods for static scenes to dynamic
scenes is not straightforward as both the scene geometry and appearance change
over time, especially under monocular setup. Also, the existing dynamic NeRF
methods generally require a lengthy per-scene training procedure, where
multi-layer perceptrons (MLP) are fitted to model both motions and radiance. In
this paper, built on top of the recent advances in voxel-grid optimization, we
propose a fast deformable radiance field method to handle dynamic scenes. Our
method consists of two modules. The first module adopts a deformation grid to
store 3D dynamic features, and a light-weight MLP for decoding the deformation
that maps a 3D point in the observation space to the canonical space using the
interpolated features. The second module contains a density and a color grid to
model the geometry and density of the scene. The occlusion is explicitly
modeled to further improve the rendering quality. Experimental results show
that our method achieves comparable performance to D-NeRF using only 20 minutes
for training, which is more than 70x faster than D-NeRF, clearly demonstrating
the efficiency of our proposed method.",0.18063533,0.33383894,0.065140255,A
7693,"H.266/VVC and compare it with the other GAN-based video
enhancement methods, i.e., the VPE-GAN, MPRNet and DC-             [2] G. J. Sullivan, J. Ohm, W. Han and T. Wiegand, ‚ÄúOverview of the High
NGAN, to further study the effectiveness of our proposed                EfÔ¨Åciency Video Coding (HEVC) Standard,‚Äù in IEEE Transactions on
method.","560-576,
QuENet on the standard test sequences of H.264/AVC and                  July 2003, doi: 10.1109/TCSVT.2003.815165.","Note that we directly use models trained with HEVC              Circuits and Systems for Video Technology, vol.",2022-06-16 02:49:28+00:00,PeQuENet: Perceptual Quality Enhancement of Compressed Video with Adaptation- and Attention-based Network,cs.CV,"['cs.CV', 'cs.MM', 'eess.IV']","[arxiv.Result.Author('Saiping Zhang'), arxiv.Result.Author('Luis Herranz'), arxiv.Result.Author('Marta Mrak'), arxiv.Result.Author('Marc Gorriz Blanch'), arxiv.Result.Author('Shuai Wan'), arxiv.Result.Author('Fuzheng Yang')]","In this paper we propose a generative adversarial network (GAN) framework to
enhance the perceptual quality of compressed videos. Our framework includes
attention and adaptation to different quantization parameters (QPs) in a single
model. The attention module exploits global receptive fields that can capture
and align long-range correlations between consecutive frames, which can be
beneficial for enhancing perceptual quality of videos. The frame to be enhanced
is fed into the deep network together with its neighboring frames, and in the
first stage features at different depths are extracted. Then extracted features
are fed into attention blocks to explore global temporal correlations, followed
by a series of upsampling and convolution layers. Finally, the resulting
features are processed by the QP-conditional adaptation module which leverages
the corresponding QP information. In this way, a single model can be used to
enhance adaptively to various QPs without requiring multiple models specific
for every QP value, while having similar performance. Experimental results
demonstrate the superior performance of the proposed PeQuENet compared with the
state-of-the-art compressed video quality enhancement algorithms.",0.025787126,0.035019502,0.23247245,C
7709,"Despite these limitations, CARLANE serves as
a supportive dataset for further research in the Ô¨Åeld of UDA.","For the synthetically generated data, we limited ourselves to using existing CARLA
maps without deÔ¨Åning new simulation environments.","8
Ethical and Responsible Use.",2022-06-16 10:53:18+00:00,CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.2; I.4; I.5; I.6']","[arxiv.Result.Author('Julian Gebele'), arxiv.Result.Author('Bonifaz Stuhr'), arxiv.Result.Author('Johann Haselberger')]","Unsupervised Domain Adaptation demonstrates great potential to mitigate
domain shifts by transferring models from labeled source domains to unlabeled
target domains. While Unsupervised Domain Adaptation has been applied to a wide
variety of complex vision tasks, only few works focus on lane detection for
autonomous driving. This can be attributed to the lack of publicly available
datasets. To facilitate research in these directions, we propose CARLANE, a
3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE
encompasses the single-target datasets MoLane and TuLane and the multi-target
dataset MuLane. These datasets are built from three different domains, which
cover diverse scenes and contain a total of 163K unique images, 118K of which
are annotated. In addition we evaluate and report systematic baselines,
including our own method, which builds upon Prototypical Cross-domain
Self-supervised Learning. We find that false positive and false negative rates
of the evaluated domain adaptation methods are high compared to those of fully
supervised baselines. This affirms the need for benchmarks such as CARLANE to
further strengthen research in Unsupervised Domain Adaptation for lane
detection. CARLANE, all evaluated models and the corresponding implementations
are publicly available at https://carlanebenchmark.github.io.",0.13171041,0.0076369327,-0.15627712,A
7710,"Despite
these limitations, CARLANE serves as a supportive dataset for further research in the Ô¨Åeld of UDA.","For the synthetically generated data, we limited
ourselves to using existing CARLA maps without deÔ¨Åning new simulation environments.",Ethical and Responsible Use.,2022-06-16 10:53:18+00:00,CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.2; I.4; I.5; I.6']","[arxiv.Result.Author('Julian Gebele'), arxiv.Result.Author('Bonifaz Stuhr'), arxiv.Result.Author('Johann Haselberger')]","Unsupervised Domain Adaptation demonstrates great potential to mitigate
domain shifts by transferring models from labeled source domains to unlabeled
target domains. While Unsupervised Domain Adaptation has been applied to a wide
variety of complex vision tasks, only few works focus on lane detection for
autonomous driving. This can be attributed to the lack of publicly available
datasets. To facilitate research in these directions, we propose CARLANE, a
3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE
encompasses the single-target datasets MoLane and TuLane and the multi-target
dataset MuLane. These datasets are built from three different domains, which
cover diverse scenes and contain a total of 163K unique images, 118K of which
are annotated. In addition we evaluate and report systematic baselines,
including our own method, which builds upon Prototypical Cross-domain
Self-supervised Learning. We find that false positive and false negative rates
of the evaluated domain adaptation methods are high compared to those of fully
supervised baselines. This affirms the need for benchmarks such as CARLANE to
further strengthen research in Unsupervised Domain Adaptation for lane
detection. CARLANE, all evaluated models and the corresponding implementations
are publicly available at https://carlanebenchmark.github.io.",0.12506601,0.017355295,-0.15174319,A
7711,"Despite
these limitations, CARLANE serves as a supportive dataset for further research in the Ô¨Åeld of UDA.","For the synthetically generated data, we limited
ourselves to using existing CARLA maps without deÔ¨Åning new simulation environments.",Ethical and Responsible Use.,2022-06-16 10:53:18+00:00,CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.2; I.4; I.5; I.6']","[arxiv.Result.Author('Julian Gebele'), arxiv.Result.Author('Bonifaz Stuhr'), arxiv.Result.Author('Johann Haselberger')]","Unsupervised Domain Adaptation demonstrates great potential to mitigate
domain shifts by transferring models from labeled source domains to unlabeled
target domains. While Unsupervised Domain Adaptation has been applied to a wide
variety of complex vision tasks, only few works focus on lane detection for
autonomous driving. This can be attributed to the lack of publicly available
datasets. To facilitate research in these directions, we propose CARLANE, a
3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE
encompasses the single-target datasets MoLane and TuLane and the multi-target
dataset MuLane. These datasets are built from three different domains, which
cover diverse scenes and contain a total of 163K unique images, 118K of which
are annotated. In addition we evaluate and report systematic baselines,
including our own method, which builds upon Prototypical Cross-domain
Self-supervised Learning. We find that false positive and false negative rates
of the evaluated domain adaptation methods are high compared to those of fully
supervised baselines. This affirms the need for benchmarks such as CARLANE to
further strengthen research in Unsupervised Domain Adaptation for lane
detection. CARLANE, all evaluated models and the corresponding implementations
are publicly available at https://carlanebenchmark.github.io.",0.12506601,0.017355295,-0.15174319,A
7723,"Our model gener-      each layer in this paper are the same, but the case that re-
alizes well to other test datasets of molecules with different   quires the input and output to have different homogeneous
numbers of atoms and outperforms the tensor Ô¨Åeld network         spaces (different stabilizer group) still needs further study.","The input and output homogeneous spaces of
illustrates the equivariance of our model.","on every test dataset, which shows the effectiveness of our      Another future direction is to implement our uniÔ¨Åed archi-
activation compared to the norm activation in the TFN.",2022-06-16 17:59:01+00:00,Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yinshuang Xu'), arxiv.Result.Author('Jiahui Lei'), arxiv.Result.Author('Edgar Dobriban'), arxiv.Result.Author('Kostas Daniilidis')]","We introduce a unified framework for group equivariant networks on
homogeneous spaces derived from a Fourier perspective. We address the case of
feature fields being tensor valued before and after a convolutional layer. We
present a unified derivation of kernels via the Fourier domain by taking
advantage of the sparsity of Fourier coefficients of the lifted feature fields.
The sparsity emerges when the stabilizer subgroup of the homogeneous space is a
compact Lie group. We further introduce an activation method via an elementwise
nonlinearity on the regular representation after lifting and projecting back to
the field through an equivariant convolution. We show that other methods
treating features as the Fourier coefficients in the stabilizer subgroup are
special cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show
state-of-the-art performance in spherical vector field regression, point cloud
classification, and molecular completion.",0.24748376,-0.15269285,0.22225598,A
7724,"Our model gen-         same, and the setting with different homogeneous spaces
eralizes well to test datasets of molecules with different        (different stabilizer groups) needs further study.","The input and out-
rotations exist naturally in molecule structures, the result      put homogeneous spaces of each layer in this paper are the
illustrates the equivariance of our model.","Another
numbers of atoms, and outperforms tensor Ô¨Åeld networks            future direction is to use our uniÔ¨Åed architecture for other
on every test dataset, which shows the effectiveness of our       groups, for example, the Lorentz group and Ô¨Ånite permuta-
activation compared to the norm activation in the TFN.",2022-06-16 17:59:01+00:00,Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yinshuang Xu'), arxiv.Result.Author('Jiahui Lei'), arxiv.Result.Author('Edgar Dobriban'), arxiv.Result.Author('Kostas Daniilidis')]","We introduce a unified framework for group equivariant networks on
homogeneous spaces derived from a Fourier perspective. We consider
tensor-valued feature fields, before and after a convolutional layer. We
present a unified derivation of kernels via the Fourier domain by leveraging
the sparsity of Fourier coefficients of the lifted feature fields. The sparsity
emerges when the stabilizer subgroup of the homogeneous space is a compact Lie
group. We further introduce a nonlinear activation, via an elementwise
nonlinearity on the regular representation after lifting and projecting back to
the field through an equivariant convolution. We show that other methods
treating features as the Fourier coefficients in the stabilizer subgroup are
special cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show
state-of-the-art performance in spherical vector field regression, point cloud
classification, and molecular completion.",0.17707497,-0.12630193,0.15600225,A
7725,lizer groups) needs further study.,Every dataset has 1000 molecules.,"Another future direction
                                                                  is to use our uniÔ¨Åed architecture for other groups like Ô¨Ånite
4.2.2 SE(3): ModelNet40 classiÔ¨Åcation                             permutation groups.",2022-06-16 17:59:01+00:00,Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yinshuang Xu'), arxiv.Result.Author('Jiahui Lei'), arxiv.Result.Author('Edgar Dobriban'), arxiv.Result.Author('Kostas Daniilidis')]","We introduce a unified framework for group equivariant networks on
homogeneous spaces derived from a Fourier perspective. We consider
tensor-valued feature fields, before and after a convolutional layer. We
present a unified derivation of kernels via the Fourier domain by leveraging
the sparsity of Fourier coefficients of the lifted feature fields. The sparsity
emerges when the stabilizer subgroup of the homogeneous space is a compact Lie
group. We further introduce a nonlinear activation, via an elementwise
nonlinearity on the regular representation after lifting and projecting back to
the field through an equivariant convolution. We show that other methods
treating features as the Fourier coefficients in the stabilizer subgroup are
special cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show
state-of-the-art performance in spherical vector field regression, point cloud
classification, and molecular completion.",0.20404172,-0.14610103,0.036854845,A
7739,"We believe that multi-task learning may suppress certain shortcut
learning brought by a single task, and in the future, we will do further research based on this and
design a more comprehensive evaluation framework for shortcut learning phenomenon.","Therefore, to fully evaluate the
performance of the model, multiple test datasets should be used as much as possible to reduce the
false positive caused by shortcuts.","References

 [1] Bertram, R., Kaakinen, J.K., Bensch, F.V., Helle, L., Lantto, E., Niemi, P., Lundbom, N.: Eye
      movements of radiologists reÔ¨Çect expertise in ct study interpretation: A potential tool to measure
      resident development.",2022-06-17 05:54:07+00:00,Rectify ViT Shortcut Learning by Visual Saliency,cs.CV,"['cs.CV', '68T07, 68T10', 'J.6; I.5.2; I.4.9']","[arxiv.Result.Author('Chong Ma'), arxiv.Result.Author('Lin Zhao'), arxiv.Result.Author('Yuzhong Chen'), arxiv.Result.Author('David Weizhong Liu'), arxiv.Result.Author('Xi Jiang'), arxiv.Result.Author('Tuo Zhang'), arxiv.Result.Author('Xintao Hu'), arxiv.Result.Author('Dinggang Shen'), arxiv.Result.Author('Dajiang Zhu'), arxiv.Result.Author('Tianming Liu')]","Shortcut learning is common but harmful to deep learning models, leading to
degenerated feature representations and consequently jeopardizing the model's
generalizability and interpretability. However, shortcut learning in the widely
used Vision Transformer framework is largely unknown. Meanwhile, introducing
domain-specific knowledge is a major approach to rectifying the shortcuts,
which are predominated by background related factors. For example, in the
medical imaging field, eye-gaze data from radiologists is an effective human
visual prior knowledge that has the great potential to guide the deep learning
models to focus on meaningful foreground regions of interest. However,
obtaining eye-gaze data is time-consuming, labor-intensive and sometimes even
not practical. In this work, we propose a novel and effective saliency-guided
vision transformer (SGT) model to rectify shortcut learning in ViT with the
absence of eye-gaze data. Specifically, a computational visual saliency model
is adopted to predict saliency maps for input image samples. Then, the saliency
maps are used to distil the most informative image patches. In the proposed
SGT, the self-attention among image patches focus only on the distilled
informative ones. Considering this distill operation may lead to global
information lost, we further introduce, in the last encoder layer, a residual
connection that captures the self-attention across all the image patches. The
experiment results on four independent public datasets show that our SGT
framework can effectively learn and leverage human prior knowledge without eye
gaze data and achieves much better performance than baselines. Meanwhile, it
successfully rectifies the harmful shortcut learning and significantly improves
the interpretability of the ViT model, demonstrating the promise of
transferring human prior knowledge derived visual saliency in rectifying
shortcut learning",0.08039679,-0.14973223,-0.1921235,C
7785,"We hope our Ô¨Åndings can help
                                                                                 further research how to better utilize such pre-training for other zero-
                                                                                 shot multimodal tasks.","The task is to predict            (e.g., <person-sleep on-bench) even though it has only seen
                                                                                 examples of <person-on-bench>.",IV.,2022-06-18 04:08:19+00:00,VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yu Cui'), arxiv.Result.Author('Moshiur Farazi')]","Visual Relationship Detection (VRD) impels a computer vision model to 'see'
beyond an individual object instance and 'understand' how different objects in
a scene are related. The traditional way of VRD is first to detect objects in
an image and then separately predict the relationship between the detected
object instances. Such a disjoint approach is prone to predict redundant
relationship tags (i.e., predicate) between the same object pair with similar
semantic meaning, or incorrect ones that have a similar meaning to the ground
truth but are semantically incorrect. To remedy this, we propose to jointly
train a VRD model with visual object features and semantic relationship
features. To this end, we propose VReBERT, a BERT-like transformer model for
Visual Relationship Detection with a multi-stage training strategy to jointly
process visual and semantic features. We show that our simple BERT-like model
is able to outperform the state-of-the-art VRD models in predicate prediction.
Furthermore, we show that by using the pre-trained VReBERT model, our model
pushes the state-of-the-art zero-shot predicate prediction by a significant
margin (+8.49 R@50 and +8.99 R@100).",-0.092637405,-0.15646403,-0.14963609,C
7786,"CE+Dice 25√ó1
MSCMR Dataset: To further study the effect of DCL with
limited training data, we conducted additional six sets of     MMEE 25√ó1
experiments on the MSCMRseg dataset, which contains only
25 training images.","These results further prove the general applica-     CE          25√ó1
bility and robustness of the proposed DCL.","In this more challenging task, DCL         strated that the compatibility framework achieved much
demonstrated a more signiÔ¨Åcant impact.",2022-06-18 08:38:31+00:00,Deep Compatible Learning for Partially-Supervised Medical Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ke Zhang'), arxiv.Result.Author('Xiahai Zhuang')]","Partially-supervised learning can be challenging for segmentation due to the
lack of supervision for unlabeled structures, and the methods directly applying
fully-supervised learning could lead to incompatibility, meaning ground truth
is not in the solution set of the optimization problem given the loss function.
To address the challenge, we propose a deep compatible learning (DCL)
framework, which trains a single multi-label segmentation network using images
with only partial structures annotated. We first formulate the
partially-supervised segmentation as an optimization problem compatible with
missing labels, and prove its compatibility. Then, we equip the model with a
conditional segmentation strategy, to propagate labels from multiple
partially-annotated images to the target. Additionally, we propose a dual
learning strategy, which learns two opposite mappings of label propagation
simultaneously, to provide substantial supervision for unlabeled structures.
The two strategies are formulated into compatible forms, termed as conditional
compatibility and dual compatibility, respectively. We show this framework is
generally applicable for conventional loss functions. The approach attains
significant performance improvement over existing methods, especially in the
situation where only a small training dataset is available. Results on three
segmentation tasks have shown that the proposed framework could achieve
performance matching fully-supervised models.",0.16507933,-0.040752925,0.09538488,A
7796,"We further study the effects of several
Thus, our approach is also well suited for unsupervised 3D             components in our pipeline, including the specular term in
shape learning.",Ablation study.,Eq.,2022-06-18 16:58:49+00:00,GAN2X: Non-Lambertian Inverse Rendering of Image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xingang Pan'), arxiv.Result.Author('Ayush Tewari'), arxiv.Result.Author('Lingjie Liu'), arxiv.Result.Author('Christian Theobalt')]","2D images are observations of the 3D physical world depicted with the
geometry, material, and illumination components. Recovering these underlying
intrinsic components from 2D images, also known as inverse rendering, usually
requires a supervised setting with paired images collected from multiple
viewpoints and lighting conditions, which is resource-demanding. In this work,
we present GAN2X, a new method for unsupervised inverse rendering that only
uses unpaired images for training. Unlike previous Shape-from-GAN approaches
that mainly focus on 3D shapes, we take the first attempt to also recover
non-Lambertian material properties by exploiting the pseudo paired data
generated by a GAN. To achieve precise inverse rendering, we devise a
specularity-aware neural surface representation that continuously models the
geometry and material properties. A shading-based refinement technique is
adopted to further distill information in the target image and recover more
fine details. Experiments demonstrate that GAN2X can accurately decompose 2D
images to 3D shape, albedo, and specular properties for different object
categories, and achieves the state-of-the-art performance for unsupervised
single-view 3D face reconstruction. We also show its applications in downstream
tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.",-0.077743694,0.11313492,-0.015502095,B
7797,"We further study the effects of several
Thus, our approach is also well suited for unsupervised 3D             components in our pipeline, including the specular term in
shape learning.",Ablation study.,Eq.,2022-06-18 16:58:49+00:00,GAN2X: Non-Lambertian Inverse Rendering of Image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xingang Pan'), arxiv.Result.Author('Ayush Tewari'), arxiv.Result.Author('Lingjie Liu'), arxiv.Result.Author('Christian Theobalt')]","2D images are observations of the 3D physical world depicted with the
geometry, material, and illumination components. Recovering these underlying
intrinsic components from 2D images, also known as inverse rendering, usually
requires a supervised setting with paired images collected from multiple
viewpoints and lighting conditions, which is resource-demanding. In this work,
we present GAN2X, a new method for unsupervised inverse rendering that only
uses unpaired images for training. Unlike previous Shape-from-GAN approaches
that mainly focus on 3D shapes, we take the first attempt to also recover
non-Lambertian material properties by exploiting the pseudo paired data
generated by a GAN. To achieve precise inverse rendering, we devise a
specularity-aware neural surface representation that continuously models the
geometry and material properties. A shading-based refinement technique is
adopted to further distill information in the target image and recover more
fine details. Experiments demonstrate that GAN2X can accurately decompose 2D
images to 3D shape, albedo, and specular properties for different object
categories, and achieves the state-of-the-art performance for unsupervised
single-view 3D face reconstruction. We also show its applications in downstream
tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.",-0.077743694,0.11313492,-0.015502095,B
7798,"We further study the effects of several
Thus, our approach is also well suited for unsupervised 3D             components in our pipeline, including the specular term in
shape learning.",Ablation study.,Eq.,2022-06-18 16:58:49+00:00,GAN2X: Non-Lambertian Inverse Rendering of Image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xingang Pan'), arxiv.Result.Author('Ayush Tewari'), arxiv.Result.Author('Lingjie Liu'), arxiv.Result.Author('Christian Theobalt')]","2D images are observations of the 3D physical world depicted with the
geometry, material, and illumination components. Recovering these underlying
intrinsic components from 2D images, also known as inverse rendering, usually
requires a supervised setting with paired images collected from multiple
viewpoints and lighting conditions, which is resource-demanding. In this work,
we present GAN2X, a new method for unsupervised inverse rendering that only
uses unpaired images for training. Unlike previous Shape-from-GAN approaches
that mainly focus on 3D shapes, we take the first attempt to also recover
non-Lambertian material properties by exploiting the pseudo paired data
generated by a GAN. To achieve precise inverse rendering, we devise a
specularity-aware neural surface representation that continuously models the
geometry and material properties. A shading-based refinement technique is
adopted to further distill information in the target image and recover more
fine details. Experiments demonstrate that GAN2X can accurately decompose 2D
images to 3D shape, albedo, and specular properties for different object
categories, and achieves the state-of-the-art performance for unsupervised
single-view 3D face reconstruction. We also show its applications in downstream
tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.",-0.077743694,0.11313492,-0.015502095,B
7799,"We further study the effects of several
and more natural-looking 3D shapes than other baselines.","1, our method reconstructs more accurate            Ablation study.","components in our pipeline, including the specular term in
Thus, our approach is also well suited for unsupervised 3D             Eq.",2022-06-18 16:58:49+00:00,GAN2X: Non-Lambertian Inverse Rendering of Image GANs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xingang Pan'), arxiv.Result.Author('Ayush Tewari'), arxiv.Result.Author('Lingjie Liu'), arxiv.Result.Author('Christian Theobalt')]","2D images are observations of the 3D physical world depicted with the
geometry, material, and illumination components. Recovering these underlying
intrinsic components from 2D images, also known as inverse rendering, usually
requires a supervised setting with paired images collected from multiple
viewpoints and lighting conditions, which is resource-demanding. In this work,
we present GAN2X, a new method for unsupervised inverse rendering that only
uses unpaired images for training. Unlike previous Shape-from-GAN approaches
that mainly focus on 3D shapes, we take the first attempt to also recover
non-Lambertian material properties by exploiting the pseudo paired data
generated by a GAN. To achieve precise inverse rendering, we devise a
specularity-aware neural surface representation that continuously models the
geometry and material properties. A shading-based refinement technique is
adopted to further distill information in the target image and recover more
fine details. Experiments demonstrate that GAN2X can accurately decompose 2D
images to 3D shape, albedo, and specular properties for different object
categories, and achieves the state-of-the-art performance for unsupervised
single-view 3D face reconstruction. We also show its applications in downstream
tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.",0.03802808,0.34877568,-0.018904908,B
7804,"The layouts of different network variants used to further study our
PERFORMANCE COMPARISON BETWEEN THE PROPOSED NETWORK AND                               solution design.",9.,"(a) is built using a single set of conv blocks to process the
                                                                                      multistream input data jointly; (b) is built using two sets of conv blocks to
                   THE VARIANTS ON ALL THREE DATASETS.",2022-06-18 17:57:32+00:00,Multistream Gaze Estimation with Anatomical Eye Region Isolation by Synthetic to Real Transfer Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zunayed Mahmud'), arxiv.Result.Author('Paul Hungler'), arxiv.Result.Author('Ali Etemad')]","We propose a novel neural pipeline, MSGazeNet, that learns gaze
representations by taking advantage of the eye anatomy information through a
multistream framework. Our proposed solution comprises two components, first a
network for isolating anatomical eye regions, and a second network for
multistream gaze estimation. The eye region isolation is performed with a U-Net
style network which we train using a synthetic dataset that contains eye region
masks for the visible eyeball and the iris region. The synthetic dataset used
in this stage is a new dataset consisting of 60,000 eye images, which we create
using an eye-gaze simulator, UnityEyes. Successive to training, the eye region
isolation network is then transferred to the real domain for generating masks
for the real-world eye images. In order to successfully make the transfer, we
exploit domain randomization in the training process, which allows for the
synthetic images to benefit from a larger variance with the help of
augmentations that resemble artifacts. The generated eye region masks along
with the raw eye images are then used together as a multistream input to our
gaze estimation network. We evaluate our framework on three benchmark gaze
estimation datasets, MPIIGaze, Eyediap, and UTMultiview, where we set a new
state-of-the-art on Eyediap and UTMultiview datasets by obtaining a performance
gain of 7.57% and 1.85% respectively, while achieving competitive performance
on MPIIGaze. We also study the robustness of our method with respect to the
noise in the data and demonstrate that our model is less sensitive to noisy
data. Lastly, we perform a variety of experiments including ablation studies to
evaluate the contribution of different components and design choices in our
solution.",0.20056623,0.0067098457,0.16885844,A
7805,"This work can serve to motivate
                                                                                   further research into using various regions of the eye for
                         (b) Early feature fusion                                  gaze estimation.","Block 1
                                                                                      In this study, we presented the relevance and importance
 Conv                                                                              of using information pertaining to anatomical eye regions
Block 1                                                                            towards gaze estimation.","We believe such approaches can lead to
                                                                                   learning better and more generalized gaze representations.",2022-06-18 17:57:32+00:00,Multistream Gaze Estimation with Anatomical Eye Region Isolation by Synthetic to Real Transfer Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zunayed Mahmud'), arxiv.Result.Author('Paul Hungler'), arxiv.Result.Author('Ali Etemad')]","We propose a novel neural pipeline, MSGazeNet, that learns gaze
representations by taking advantage of the eye anatomy information through a
multistream framework. Our proposed solution comprises two components, first a
network for isolating anatomical eye regions, and a second network for
multistream gaze estimation. The eye region isolation is performed with a U-Net
style network which we train using a synthetic dataset that contains eye region
masks for the visible eyeball and the iris region. The synthetic dataset used
in this stage is a new dataset consisting of 60,000 eye images, which we create
using an eye-gaze simulator, UnityEyes. Successive to training, the eye region
isolation network is then transferred to the real domain for generating masks
for the real-world eye images. In order to successfully make the transfer, we
exploit domain randomization in the training process, which allows for the
synthetic images to benefit from a larger variance with the help of
augmentations that resemble artifacts. The generated eye region masks along
with the raw eye images are then used together as a multistream input to our
gaze estimation network. We evaluate our framework on three benchmark gaze
estimation datasets, MPIIGaze, Eyediap, and UTMultiview, where we set a new
state-of-the-art on Eyediap and UTMultiview datasets by obtaining a performance
gain of 7.57% and 1.85% respectively, while achieving competitive performance
on MPIIGaze. We also study the robustness of our method with respect to the
noise in the data and demonstrate that our model is less sensitive to noisy
data. Lastly, we perform a variety of experiments including ablation studies to
evaluate the contribution of different components and design choices in our
solution.",-0.04200678,0.07993869,-0.181393,C
7817,"Since FFN takes up most of the parame-     ent separaFtFiNon ratios (c.f ., left axis), and the radius                                                                                                            S24
ters and calculations, we can conduct further research
on optimizing this module to obtain better-integrated       represents the relative number of parameters.","Results indicate that                                                                                                                                                                                          Base384
each component contributes to the model performance,        represents GtLIhe Top-1 accuracy of EATFormer at diÔ¨Äer-                                                                                                                                     B
and our EATFormer obtains the best result when using
all three parts.","Orange
model performance.",2022-06-19 04:49:35+00:00,EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm,cs.CV,"['cs.CV', 'cs.ET']","[arxiv.Result.Author('Jiangning Zhang'), arxiv.Result.Author('Xiangtai Li'), arxiv.Result.Author('Yabiao Wang'), arxiv.Result.Author('Chengjie Wang'), arxiv.Result.Author('Yibo Yang'), arxiv.Result.Author('Yong Liu'), arxiv.Result.Author('Dacheng Tao')]","Motivated by biological evolution, this paper explains the rationality of
Vision Transformer by analogy with the proven practical Evolutionary Algorithm
(EA) and derives that both have consistent mathematical formulation. Then
inspired by effective EA variants, we propose a novel pyramid EATFormer
backbone that only contains the proposed \emph{EA-based Transformer} (EAT)
block, which consists of three residual parts, \ie, \emph{Multi-Scale Region
Aggregation} (MSRA), \emph{Global and Local Interaction} (GLI), and
\emph{Feed-Forward Network} (FFN) modules, to model multi-scale, interactive,
and individual information separately. Moreover, we design a \emph{Task-Related
Head} (TRH) docked with transformer backbone to complete final information
fusion more flexibly and \emph{improve} a \emph{Modulated Deformable MSA}
(MD-MSA) to dynamically model irregular locations. Massive quantitative and
quantitative experiments on image classification, downstream tasks, and
explanatory experiments demonstrate the effectiveness and superiority of our
approach over State-Of-The-Art (SOTA) methods. \Eg, our Mobile (1.8M), Tiny
(6.1M), Small (24.3M), and Base (49.0M) models achieve 69.4, 78.4, 83.1, and
83.9 Top-1 only trained on ImageNet-1K with naive training recipe;
EATFormer-Tiny/Small/Base armed Mask-R-CNN obtain 45.4/47.4/49.0 box AP and
41.4/42.9/44.2 mask AP on COCO detection, surpassing contemporary MPViT-T,
Swin-T, and Swin-S by 0.6/1.4/0.5 box AP and 0.4/1.3/0.9 mask AP separately
with less FLOPs; Our EATFormer-Small/Base achieve 47.3/49.3 mIoU on ADE20K by
Upernet that exceeds Swin-T/S by 2.8/1.7. Code will be available at
\url{https://https://github.com/zhangzjn/EATFormer}.",0.3056972,0.15707149,0.17446665,A
7856,"Our work serves as a fertile starting ground for further research address-
ing the potential for manipulating videos to deceive both human viewers and deepfake discrimina-
tors.","Moreover, viewers without a considerable degree of technologi-
cal literacy may not know that certain data corruption methods are inorganic ‚Äì malicious actors can
target such viewers by applying inorganic corruption methods to fool both the judgments of deepfake
discriminators and viewers.","Although we focus on video corruption, previous work in generating adversarial examples,
such as that described in Eykholt et al.",2022-06-20 15:24:55+00:00,Practical Deepfake Detection: Vulnerabilities in Global Contexts,cs.CV,"['cs.CV', 'cs.CY']","[arxiv.Result.Author('Yang A. Chuming'), arxiv.Result.Author('Daniel J. Wu'), arxiv.Result.Author('Ken Hong')]","Recent advances in deep learning have enabled realistic digital alterations
to videos, known as deepfakes. This technology raises important societal
concerns regarding disinformation and authenticity, galvanizing the development
of numerous deepfake detection algorithms. At the same time, there are
significant differences between training data and in-the-wild video data, which
may undermine their practical efficacy. We simulate data corruption techniques
and examine the performance of a state-of-the-art deepfake detection algorithm
on corrupted variants of the FaceForensics++ dataset.
  While deepfake detection models are robust against video corruptions that
align with training-time augmentations, we find that they remain vulnerable to
video corruptions that simulate decreases in video quality. Indeed, in the
controversial case of the video of Gabonese President Bongo's new year address,
the algorithm, which confidently authenticates the original video, judges
highly corrupted variants of the video to be fake. Our work opens up both
technical and ethical avenues of exploration into practical deepfake detection
in global contexts.",-0.06530511,-0.21664858,-0.047463965,C
7914,"Unresolved, this will likely form a signiÔ¨Åcant hurdle that will discourage further research
in this area.",ual faces [35].,"This paper describes our efforts (and a tool) to address this challenge, and make the study of physical
backdoors more accessible to the research community.",2022-06-21 18:52:25+00:00,Natural Backdoor Datasets,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Emily Wenger'), arxiv.Result.Author('Roma Bhattacharjee'), arxiv.Result.Author('Arjun Nitin Bhagoji'), arxiv.Result.Author('Josephine Passananti'), arxiv.Result.Author('Emilio Andere'), arxiv.Result.Author('Haitao Zheng'), arxiv.Result.Author('Ben Y. Zhao')]","Extensive literature on backdoor poison attacks has studied attacks and
defenses for backdoors using ""digital trigger patterns."" In contrast, ""physical
backdoors"" use physical objects as triggers, have only recently been
identified, and are qualitatively different enough to resist all defenses
targeting digital trigger backdoors. Research on physical backdoors is limited
by access to large datasets containing real images of physical objects
co-located with targets of classification. Building these datasets is time- and
labor-intensive. This works seeks to address the challenge of accessibility for
research on physical backdoor attacks. We hypothesize that there may be
naturally occurring physically co-located objects already present in popular
datasets such as ImageNet. Once identified, a careful relabeling of these data
can transform them into training samples for physical backdoor attacks. We
propose a method to scalably identify these subsets of potential triggers in
existing datasets, along with the specific classes they can poison. We call
these naturally occurring trigger-class subsets natural backdoor datasets. Our
techniques successfully identify natural backdoors in widely-available
datasets, and produce models behaviorally equivalent to those trained on
manually curated datasets. We release our code to allow the research community
to create their own datasets for research on physical backdoor attacks.",0.19199656,0.11503944,-0.08521571,A
7920,"[89]), until further research has examined questions of efÔ¨Åcacy and utility,
since text and image convey meaning in distinct ways and with distinct limitations.","However, we caution against the use of text-to-image models as communication aids,
including for education (cf.","Cross-cultural
considerations are of special concern, as little research has considered questions of accessibility of
computer-generated images to members of non-Western cultures.",2022-06-22 01:11:29+00:00,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jiahui Yu'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Jing Yu Koh'), arxiv.Result.Author('Thang Luong'), arxiv.Result.Author('Gunjan Baid'), arxiv.Result.Author('Zirui Wang'), arxiv.Result.Author('Vijay Vasudevan'), arxiv.Result.Author('Alexander Ku'), arxiv.Result.Author('Yinfei Yang'), arxiv.Result.Author('Burcu Karagol Ayan'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Wei Han'), arxiv.Result.Author('Zarana Parekh'), arxiv.Result.Author('Xin Li'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Jason Baldridge'), arxiv.Result.Author('Yonghui Wu')]","We present the Pathways Autoregressive Text-to-Image (Parti) model, which
generates high-fidelity photorealistic images and supports content-rich
synthesis involving complex compositions and world knowledge. Parti treats
text-to-image generation as a sequence-to-sequence modeling problem, akin to
machine translation, with sequences of image tokens as the target outputs
rather than text tokens in another language. This strategy can naturally tap
into the rich body of prior work on large language models, which have seen
continued advances in capabilities and performance through scaling data and
model sizes. Our approach is simple: First, Parti uses a Transformer-based
image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens.
Second, we achieve consistent quality improvements by scaling the
encoder-decoder Transformer model up to 20B parameters, with a new
state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on
MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts
(P2), a new holistic benchmark of over 1600 English prompts, demonstrate the
effectiveness of Parti across a wide variety of categories and difficulty
aspects. We also explore and highlight limitations of our models in order to
define and exemplify key areas of focus for further improvements. See
https://parti.research.google/ for high-resolution images.",-0.009732803,-0.11742834,-0.28246182,C
7949,"With the evolution of 3D video and       early stages, the lack of publicly available 3D HDR datasets
                                        display technologies, the consumer market for High Dynamic         is a signiÔ¨Åcant roadblock to further research and development.","As several of these technologies are still in their
                                        to conventional displays.",Range (HDR) cameras and displays is quickly growing.,2022-06-22 13:54:02+00:00,A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Rohit Choudhary'), arxiv.Result.Author('Mansi Sharma'), arxiv.Result.Author('Aditya Wadaskar')]","Immersive displays such as VR headsets, AR glasses, Multiview displays, Free
point televisions have emerged as a new class of display technologies in recent
years, offering a better visual experience and viewer engagement as compared to
conventional displays. With the evolution of 3D video and display technologies,
the consumer market for High Dynamic Range (HDR) cameras and displays is
quickly growing. The lack of appropriate experimental data is a critical
hindrance for the development of primary research efforts in the field of 3D
HDR video technology. Also, the unavailability of sufficient real world
multi-exposure experimental dataset is a major bottleneck for HDR imaging
research, thereby limiting the quality of experience (QoE) for the viewers. In
this paper, we introduce a diversified stereoscopic multi-exposure dataset
captured within the campus of Indian Institute of Technology Madras, which is
home to a diverse flora and fauna. The dataset is captured using ZED
stereoscopic camera and provides intricate scenes of outdoor locations such as
gardens, roadside views, festival venues, buildings and indoor locations such
as academic and residential areas. The proposed dataset accommodates wide depth
range, complex depth structure, complicate object movement, illumination
variations, rich color dynamics, texture discrepancy in addition to significant
randomness introduced by moving camera and background motion. The proposed
dataset is made publicly available to the research community. Furthermore, the
procedure for capturing, aligning and calibrating multi-exposure stereo videos
and images is described in detail. Finally, we have discussed the progress,
challenges, potential use cases and future research opportunities with respect
to HDR imaging, depth estimation, consistent tone mapping and 3D HDR coding.",-0.07804553,0.36509687,-0.068055466,B
7950,"Methods such as [61]            video becomes more popular, reliable benchmark datasets of
generated HDR images using low-dynamic images acquired              natural scenes are required for further study.","As the research area of 3D HDR
create synthetic stereo HDR images.","The proposed
by two inexpensive LDR cameras for the same scene.",2022-06-22 13:54:02+00:00,A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Rohit Choudhary'), arxiv.Result.Author('Mansi Sharma'), arxiv.Result.Author('Aditya Wadaskar')]","Immersive displays such as VR headsets, AR glasses, Multiview displays, Free
point televisions have emerged as a new class of display technologies in recent
years, offering a better visual experience and viewer engagement as compared to
conventional displays. With the evolution of 3D video and display technologies,
the consumer market for High Dynamic Range (HDR) cameras and displays is
quickly growing. The lack of appropriate experimental data is a critical
hindrance for the development of primary research efforts in the field of 3D
HDR video technology. Also, the unavailability of sufficient real world
multi-exposure experimental dataset is a major bottleneck for HDR imaging
research, thereby limiting the quality of experience (QoE) for the viewers. In
this paper, we introduce a diversified stereoscopic multi-exposure dataset
captured within the campus of Indian Institute of Technology Madras, which is
home to a diverse flora and fauna. The dataset is captured using ZED
stereoscopic camera and provides intricate scenes of outdoor locations such as
gardens, roadside views, festival venues, buildings and indoor locations such
as academic and residential areas. The proposed dataset accommodates wide depth
range, complex depth structure, complicate object movement, illumination
variations, rich color dynamics, texture discrepancy in addition to significant
randomness introduced by moving camera and background motion. The proposed
dataset is made publicly available to the research community. Furthermore, the
procedure for capturing, aligning and calibrating multi-exposure stereo videos
and images is described in detail. Finally, we have discussed the progress,
challenges, potential use cases and future research opportunities with respect
to HDR imaging, depth estimation, consistent tone mapping and 3D HDR coding.",-0.122836545,0.35636866,0.019118458,B
7952,"Introducing a small dataset called Web Gallery of Art 500 (or WGA500 ) to
       evaluate our retrieval method and make it publicly available to encourage
       further research in this domain.",2.,3.,2022-06-22 14:06:29+00:00,ICC++: Explainable Image Retrieval for Art Historical Corpora using Image Composition Canvas,cs.CV,['cs.CV'],"[arxiv.Result.Author('Prathmesh Madhu'), arxiv.Result.Author('Tilman Marquart'), arxiv.Result.Author('Ronak Kosti'), arxiv.Result.Author('Dirk Suckow'), arxiv.Result.Author('Peter Bell'), arxiv.Result.Author('Andreas Maier'), arxiv.Result.Author('Vincent Christlein')]","Image compositions are helpful in the study of image structures and assist in
discovering the semantics of the underlying scene portrayed across art forms
and styles. With the digitization of artworks in recent years, thousands of
images of a particular scene or narrative could potentially be linked together.
However, manually linking this data with consistent objectiveness can be a
highly challenging and time-consuming task. In this work, we present a novel
approach called Image Composition Canvas (ICC++) to compare and retrieve images
having similar compositional elements. ICC++ is an improvement over ICC
specializing in generating low and high-level features (compositional elements)
motivated by Max Imdahl's work. To this end, we present a rigorous quantitative
and qualitative comparison of our approach with traditional and
state-of-the-art (SOTA) methods showing that our proposed method outperforms
all of them. In combination with deep features, our method outperforms the best
deep learning-based method, opening the research direction for explainable
machine learning for digital humanities. We will release the code and the data
post-publication.",0.07714025,-0.13417275,-0.25001043,C
7958,"We         Compared with glass surface detection, research on trans-
will release our RGB-D dataset and codes to the public to        parent object detection (TOD) focuses on detecting small
facilitate further research.","We have conducted extensive experiments to evaluate our
method, in comparison with the state-of-the-art methods          2.2 Transparent Object Detection
from the relevant areas, and show that the proposed model
outperforms existing methods on our proposed dataset.","transparent objects, such as wine glass and glass balls, which
                                                                 typically have well-deÔ¨Åned shapes or boundary properties.",2022-06-22 17:56:09+00:00,Depth-aware Glass Surface Detection with Cross-modal Context Mining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiaying Lin'), arxiv.Result.Author('Yuen Hei Yeung'), arxiv.Result.Author('Rynson W. H. Lau')]","Glass surfaces are becoming increasingly ubiquitous as modern buildings tend
to use a lot of glass panels. This however poses substantial challenges on the
operations of autonomous systems such as robots, self-driving cars and drones,
as the glass panels can become transparent obstacles to the navigation.Existing
works attempt to exploit various cues, including glass boundary context or
reflections, as a prior. However, they are all based on input RGB images.We
observe that the transmission of 3D depth sensor light through glass surfaces
often produces blank regions in the depth maps, which can offer additional
insights to complement the RGB image features for glass surface detection. In
this paper, we propose a novel framework for glass surface detection by
incorporating RGB-D information, with two novel modules: (1) a cross-modal
context mining (CCM) module to adaptively learn individual and mutual context
features from RGB and depth information, and (2) a depth-missing aware
attention (DAA) module to explicitly exploit spatial locations where missing
depths occur to help detect the presence of glass surfaces. In addition, we
propose a large-scale RGB-D glass surface detection dataset, called
\textit{RGB-D GSD}, for RGB-D glass surface detection. Our dataset comprises
3,009 real-world RGB-D glass surface images with precise annotations. Extensive
experimental results show that our proposed model outperforms state-of-the-art
methods.",-0.24586488,0.16407849,-0.039413404,B
7959,"In order to train our model and to encourage
explicit boundary maps [72] for training, or a transformer            further research, we propose an RGB-D Glass Surface De-
architecture [73], [81] for improved performances.",Other methods use            information.,"However,           tection (RGB-D GSD) dataset here.",2022-06-22 17:56:09+00:00,Depth-aware Glass Surface Detection with Cross-modal Context Mining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiaying Lin'), arxiv.Result.Author('Yuen Hei Yeung'), arxiv.Result.Author('Rynson W. H. Lau')]","Glass surfaces are becoming increasingly ubiquitous as modern buildings tend
to use a lot of glass panels. This however poses substantial challenges on the
operations of autonomous systems such as robots, self-driving cars and drones,
as the glass panels can become transparent obstacles to the navigation.Existing
works attempt to exploit various cues, including glass boundary context or
reflections, as a prior. However, they are all based on input RGB images.We
observe that the transmission of 3D depth sensor light through glass surfaces
often produces blank regions in the depth maps, which can offer additional
insights to complement the RGB image features for glass surface detection. In
this paper, we propose a novel framework for glass surface detection by
incorporating RGB-D information, with two novel modules: (1) a cross-modal
context mining (CCM) module to adaptively learn individual and mutual context
features from RGB and depth information, and (2) a depth-missing aware
attention (DAA) module to explicitly exploit spatial locations where missing
depths occur to help detect the presence of glass surfaces. In addition, we
propose a large-scale RGB-D glass surface detection dataset, called
\textit{RGB-D GSD}, for RGB-D glass surface detection. Our dataset comprises
3,009 real-world RGB-D glass surface images with precise annotations. Extensive
experimental results show that our proposed model outperforms state-of-the-art
methods.",-0.07777105,0.22577967,0.20452358,B
7960,"A, we present the further study
on the designs of Codebook C and Transformer T as well as the runtimes of methods.",In Sec.,In Sec.,2022-06-22 17:58:01+00:00,Towards Robust Blind Face Restoration with Codebook Lookup Transformer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shangchen Zhou'), arxiv.Result.Author('Kelvin C. K. Chan'), arxiv.Result.Author('Chongyi Li'), arxiv.Result.Author('Chen Change Loy')]","Blind face restoration is a highly ill-posed problem that often requires
auxiliary guidance to 1) improve the mapping from degraded inputs to desired
outputs, or 2) complement high-quality details lost in the inputs. In this
paper, we demonstrate that a learned discrete codebook prior in a small proxy
space largely reduces the uncertainty and ambiguity of restoration mapping by
casting blind face restoration as a code prediction task, while providing rich
visual atoms for generating high-quality faces. Under this paradigm, we propose
a Transformer-based prediction network, named CodeFormer, to model global
composition and context of the low-quality faces for code prediction, enabling
the discovery of natural faces that closely approximate the target faces even
when the inputs are severely degraded. To enhance the adaptiveness for
different degradation, we also propose a controllable feature transformation
module that allows a flexible trade-off between fidelity and quality. Thanks to
the expressive codebook prior and global modeling, CodeFormer outperforms state
of the arts in both quality and fidelity, showing superior robustness to
degradation. Extensive experimental results on synthetic and real-world
datasets verify the effectiveness of our method.",0.31734344,0.13971432,0.021191595,A
7972,"We hope our work will attract further researches into this
                                        valuable yet challenging task.","Experiments conducted on            and background; ED measures the average edge magnitude
                                        IOD-Video dataset demonstrate that spatio-temporal aggre-            as closed boundary characteristics; SS segments an image
                                        gation can signiÔ¨Åcantly improve the performance of IOD.","The code will be available at:
                                        https://github.com/CalayZhou/IOD-Video.",2022-06-23 02:39:09+00:00,Explore Spatio-temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kailai Zhou'), arxiv.Result.Author('Yibo Wang'), arxiv.Result.Author('Tao Lv'), arxiv.Result.Author('Yunqian Li'), arxiv.Result.Author('Linsen Chen'), arxiv.Result.Author('Qiu Shen'), arxiv.Result.Author('Xun Cao')]","We endeavor on a rarely explored task named Insubstantial Object Detection
(IOD), which aims to localize the object with following characteristics: (1)
amorphous shape with indistinct boundary; (2) similarity to surroundings; (3)
absence in color. Accordingly, it is far more challenging to distinguish
insubstantial objects in a single static frame and the collaborative
representation of spatial and temporal information is crucial. Thus, we
construct an IOD-Video dataset comprised of 600 videos (141,017 frames)
covering various distances, sizes, visibility, and scenes captured by different
spectral ranges. In addition, we develop a spatio-temporal aggregation
framework for IOD, in which different backbones are deployed and a
spatio-temporal aggregation loss (STAloss) is elaborately designed to leverage
the consistency along the time axis. Experiments conducted on IOD-Video dataset
demonstrate that spatio-temporal aggregation can significantly improve the
performance of IOD. We hope our work will attract further researches into this
valuable yet challenging task. The code will be available at:
\url{https://github.com/CalayZhou/IOD-Video}.",-0.06633334,0.27376896,0.063302875,B
7983,We simply ensemble models from              method provides a strong baseline for further research.,"We hope that our
prove the performance.",two different training iterations as the planner.,2022-06-23 10:36:53+00:00,1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022),cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Dong An'), arxiv.Result.Author('Zun Wang'), arxiv.Result.Author('Yangguang Li'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Yicong Hong'), arxiv.Result.Author('Yan Huang'), arxiv.Result.Author('Liang Wang'), arxiv.Result.Author('Jing Shao')]","This report presents the methods of the winning entry of the RxR-Habitat
Competition in CVPR 2022. The competition addresses the problem of
Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
requires an agent to follow step-by-step natural language instructions to reach
a target. We present a modular plan-and-control approach for the task. Our
model consists of three modules: the candidate waypoints predictor (CWP), the
history enhanced planner and the tryout controller. In each decision loop, CWP
first predicts a set of candidate waypoints based on depth observations from
multiple views. It can reduce the complexity of the action space and facilitate
planning. Then, a history-enhanced planner is adopted to select one of the
candidate waypoints as the subgoal. The planner additionally encodes historical
memory to track the navigation progress, which is especially effective for
long-horizon navigation. Finally, we propose a non-parametric heuristic
controller named tryout to execute low-level actions to reach the planned
subgoal. It is based on the trial-and-error mechanism which can help the agent
to avoid obstacles and escape from getting stuck. All three modules work
hierarchically until the agent stops. We further take several recent advances
of Vision-and-Language Navigation (VLN) to improve the performance such as
pretraining based on large-scale synthetic in-domain dataset, environment-level
data augmentation and snapshot model ensemble. Our model won the RxR-Habitat
Competition 2022, with 48% and 90% relative improvements over existing methods
on NDTW and SR metrics respectively.",0.22192296,-0.12011106,-0.120637335,A
7984,We simply ensemble models from              method provides a strong baseline for further research.,"We hope that our
prove the performance.",two different training iterations as the planner.,2022-06-23 10:36:53+00:00,1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022),cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Dong An'), arxiv.Result.Author('Zun Wang'), arxiv.Result.Author('Yangguang Li'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Yicong Hong'), arxiv.Result.Author('Yan Huang'), arxiv.Result.Author('Liang Wang'), arxiv.Result.Author('Jing Shao')]","This report presents the methods of the winning entry of the RxR-Habitat
Competition in CVPR 2022. The competition addresses the problem of
Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
requires an agent to follow step-by-step natural language instructions to reach
a target. We present a modular plan-and-control approach for the task. Our
model consists of three modules: the candidate waypoints predictor (CWP), the
history enhanced planner and the tryout controller. In each decision loop, CWP
first predicts a set of candidate waypoints based on depth observations from
multiple views. It can reduce the complexity of the action space and facilitate
planning. Then, a history-enhanced planner is adopted to select one of the
candidate waypoints as the subgoal. The planner additionally encodes historical
memory to track the navigation progress, which is especially effective for
long-horizon navigation. Finally, we propose a non-parametric heuristic
controller named tryout to execute low-level actions to reach the planned
subgoal. It is based on the trial-and-error mechanism which can help the agent
to avoid obstacles and escape from getting stuck. All three modules work
hierarchically until the agent stops. We further take several recent advances
of Vision-and-Language Navigation (VLN) to improve the performance such as
pretraining based on large-scale synthetic in-domain dataset, environment-level
data augmentation and snapshot model ensemble. Our model won the RxR-Habitat
Competition 2022, with 48% and 90% relative improvements over existing methods
on NDTW and SR metrics respectively.",0.22192296,-0.12011106,-0.120637335,A
8004,"We acknowledge
the carbon footprint of training such work and encourage further research in the domain of reducing
the resource demands of such methods.","Impact Even with our proposed method, training NeRF and related neural radiance Ô¨Åelds models on
GPUs may span several hours if not days, imposing signiÔ¨Åcant environmental harm.","As memory availability can be a crucial bottleneck for
training such methods, we hope the memory savings introduced in this work increase access for
researchers with limited resources.",2022-06-23 19:57:07+00:00,UNeRF: Time and Memory Conscious U-Shaped Network for Training Neural Radiance Fields,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Abiramy Kuganesan'), arxiv.Result.Author('Shih-yang Su'), arxiv.Result.Author('James J. Little'), arxiv.Result.Author('Helge Rhodin')]","Neural Radiance Fields (NeRFs) increase reconstruction detail for novel view
synthesis and scene reconstruction, with applications ranging from large static
scenes to dynamic human motion. However, the increased resolution and
model-free nature of such neural fields come at the cost of high training times
and excessive memory requirements. Recent advances improve the inference time
by using complementary data structures yet these methods are ill-suited for
dynamic scenes and often increase memory consumption. Little has been done to
reduce the resources required at training time. We propose a method to exploit
the redundancy of NeRF's sample-based computations by partially sharing
evaluations across neighboring sample points. Our UNeRF architecture is
inspired by the UNet, where spatial resolution is reduced in the middle of the
network and information is shared between adjacent samples. Although this
change violates the strict and conscious separation of view-dependent
appearance and view-independent density estimation in the NeRF method, we show
that it improves novel view synthesis. We also introduce an alternative
subsampling strategy which shares computation while minimizing any violation of
view invariance. UNeRF is a plug-in module for the original NeRF network. Our
major contributions include reduction of the memory footprint, improved
accuracy, and reduced amortized processing time both during training and
inference. With only weak assumptions on locality, we achieve improved resource
utilization on a variety of neural radiance fields tasks. We demonstrate
applications to the novel view synthesis of static scenes as well as dynamic
human shape and motion.",0.11117443,-0.17605184,0.19078478,A
8051,"3.4 Co-training view analysis

In this section, we further study the suitability of the H and E channels for
co-training in the context of the ccRCC dataset.","Finally, contrastive co-training was able to reach the same accuracy
levels independent of the amount of labeled data that was used for supervised
training (rows 2 and 7, Table 1).","First, we explore whether the
H and E channels are suÔ¨Écient on their own to provide a basis for accurate
classiÔ¨Åcation in a supervised setting.",2022-06-24 22:25:31+00:00,Stain based contrastive co-training for histopathological image analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bodong Zhang'), arxiv.Result.Author('Beatrice Knudsen'), arxiv.Result.Author('Deepika Sirohi'), arxiv.Result.Author('Alessandro Ferrero'), arxiv.Result.Author('Tolga Tasdizen')]","We propose a novel semi-supervised learning approach for classification of
histopathology images. We employ strong supervision with patch-level
annotations combined with a novel co-training loss to create a semi-supervised
learning framework. Co-training relies on multiple conditionally independent
and sufficient views of the data. We separate the hematoxylin and eosin
channels in pathology images using color deconvolution to create two views of
each slide that can partially fulfill these requirements. Two separate CNNs are
used to embed the two views into a joint feature space. We use a contrastive
loss between the views in this feature space to implement co-training. We
evaluate our approach in clear cell renal cell and prostate carcinomas, and
demonstrate improvement over state-of-the-art semi-supervised learning methods.",-0.03643028,-0.23740503,-0.1334917,C
8052,"Another potential direction for further research is to investigate
whether a more sophisticated separation into H and E stain channels can pro-
vide improved results with co-training.","Stain based contrastive co-training for histopathological image analysis  9

    Since our proposed approach uses a complementary learning strategy to con-
sistency regularization approaches that use data transformations, a potential
avenue for future research is to combine them to obtain further improvements
in accuracy.","Methods based on Cycle-GAN have been
used for stain-to-stain translation such as H&E to immunohistochemistry and
can also be used for separation of H and E stains.",2022-06-24 22:25:31+00:00,Stain based contrastive co-training for histopathological image analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bodong Zhang'), arxiv.Result.Author('Beatrice Knudsen'), arxiv.Result.Author('Deepika Sirohi'), arxiv.Result.Author('Alessandro Ferrero'), arxiv.Result.Author('Tolga Tasdizen')]","We propose a novel semi-supervised learning approach for classification of
histopathology images. We employ strong supervision with patch-level
annotations combined with a novel co-training loss to create a semi-supervised
learning framework. Co-training relies on multiple conditionally independent
and sufficient views of the data. We separate the hematoxylin and eosin
channels in pathology images using color deconvolution to create two views of
each slide that can partially fulfill these requirements. Two separate CNNs are
used to embed the two views into a joint feature space. We use a contrastive
loss between the views in this feature space to implement co-training. We
evaluate our approach in clear cell renal cell and prostate carcinomas, and
demonstrate improvement over state-of-the-art semi-supervised learning methods.",0.041337494,-0.13722558,0.12536009,C
8053,"3.4 Co-training View Analysis

In this section, we further study the suitability of the H and E channels for
co-training in the context of the ccRCC dataset.","Finally, contrastive co-training was able to reach the same accuracy
levels independent of the amount of labeled data that was used for supervised
training (rows 2 and 7, Table 1).","First, we explore whether the
H and E channels are suÔ¨Écient on their own to provide a basis for accurate
classiÔ¨Åcation in a supervised setting.",2022-06-24 22:25:31+00:00,Stain Based Contrastive Co-training for Histopathological Image Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bodong Zhang'), arxiv.Result.Author('Beatrice Knudsen'), arxiv.Result.Author('Deepika Sirohi'), arxiv.Result.Author('Alessandro Ferrero'), arxiv.Result.Author('Tolga Tasdizen')]","We propose a novel semi-supervised learning approach for classification of
histopathology images. We employ strong supervision with patch-level
annotations combined with a novel co-training loss to create a semi-supervised
learning framework. Co-training relies on multiple conditionally independent
and sufficient views of the data. We separate the hematoxylin and eosin
channels in pathology images using color deconvolution to create two views of
each slide that can partially fulfill these requirements. Two separate CNNs are
used to embed the two views into a joint feature space. We use a contrastive
loss between the views in this feature space to implement co-training. We
evaluate our approach in clear cell renal cell and prostate carcinomas, and
demonstrate improvement over state-of-the-art semi-supervised learning methods.",-0.03643028,-0.23740503,-0.1334917,C
8054,"An-
other potential direction for further research is to investigate whether a more so-
phisticated separation into H and E stain channels can provide improved results
with co-training.","Since our proposed approach uses a complementary learning strategy to con-
sistency regularization approaches that use data transformations, a potential
avenue for future research is to combine them for further improvements.","Methods based on Cycle-GAN have been used for stain-to-stain
translation such as H&E to immunohistochemistry and they could also be used
for separation of H and E stains.",2022-06-24 22:25:31+00:00,Stain Based Contrastive Co-training for Histopathological Image Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bodong Zhang'), arxiv.Result.Author('Beatrice Knudsen'), arxiv.Result.Author('Deepika Sirohi'), arxiv.Result.Author('Alessandro Ferrero'), arxiv.Result.Author('Tolga Tasdizen')]","We propose a novel semi-supervised learning approach for classification of
histopathology images. We employ strong supervision with patch-level
annotations combined with a novel co-training loss to create a semi-supervised
learning framework. Co-training relies on multiple conditionally independent
and sufficient views of the data. We separate the hematoxylin and eosin
channels in pathology images using color deconvolution to create two views of
each slide that can partially fulfill these requirements. Two separate CNNs are
used to embed the two views into a joint feature space. We use a contrastive
loss between the views in this feature space to implement co-training. We
evaluate our approach in clear cell renal cell and prostate carcinomas, and
demonstrate improvement over state-of-the-art semi-supervised learning methods.",0.087758236,-0.16861562,0.08628978,C
8065,"This enables pharmaceutical companies to conduct follow-up
investigations from the time the patients begin taking drugs, so as to know the effect
of drugs in time and get patients‚Äô data quickly for further research.","The models are always very sensitive;
even if there are slight changes in the biomarkers, the models will show them up
by the predicted age.","Here, we will show the potential of supervized ML methods to estimate BA,
focusing on the latest developments in computing a systemic (blood-based) brain-
specific-age and facial-age estimation.",2022-06-25 13:38:39+00:00,Machine Learning-based Biological Ageing Estimation Technologies: A Survey,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zhaonian Zhang'), arxiv.Result.Author('Richard Jiang'), arxiv.Result.Author('Danny Crookes'), arxiv.Result.Author('Paul Chazot')]","In recent years, there are various methods of estimating Biological Age (BA)
have been developed. Especially with the development of machine learning (ML),
there are more and more types of BA predictions, and the accuracy has been
greatly improved. The models for the estimation of BA play an important role in
monitoring healthy aging, and could provide new tools to detect health status
in the general population and give warnings to sub-healthy people. We will
mainly review three age prediction methods by using ML. They are based on blood
biomarkers, facial images, and structural neuroimaging features. For now, the
model using blood biomarkers is the simplest, most direct, and most accurate
method. The face image method is affected by various aspects such as race,
environment, etc., the prediction accuracy is not very good, which cannot make
a great contribution to the medical field. In summary, we are here to track the
way forward in the era of big data for us and other potential general
populations and show ways to leverage the vast amounts of data available today.",0.18944785,0.02102548,-0.036670253,A
8077,"However, further research is needed to build a digital
                                                                                                                                     human system for any speaker.","                                           Perceptual Conversational Head Generation
                                        with Regularized Driver and Enhanced Renderer

                                            Ailin Huang‚àó           Zhewei Huang‚àó                                                     Shuchang Zhou

                                            Megvii Research          Megvii Research                                                  Megvii Research
                                           Wuhan University
                                        huangailin@megvii.com  huangzhewei@megvii.com                                                 zsc@megvii.com

arXiv:2206.12837v1 [cs.CV] 26 Jun 2022  ABSTRACT                                                                                     same speaker.","This paper reports our solution for MultiMedia ViCo 2022 Con-
                                        versational Head Generation Challenge, which aims to generate                                   In our paper, we focus on training an audio-to-head driver with
                                        vivid face-to-face conversation videos based on audio and reference                          limited data and assembling a powerful renderer to generate vivid
                                        images.",2022-06-26 10:12:59+00:00,Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ailin Huang'), arxiv.Result.Author('Zhewei Huang'), arxiv.Result.Author('Shuchang Zhou')]","This paper reports our solution for MultiMedia ViCo 2022 Conversational Head
Generation Challenge, which aims to generate vivid face-to-face conversation
videos based on audio and reference images. Our solution focuses on training a
generalized audio-to-head driver using regularization and assembling a high
visual quality renderer. We carefully tweak the audio-to-behavior model and
post-process the generated video using our foreground-background fusion module.
We get first place in the listening head generation track and second place in
the talking head generation track in the official ranking. Our code will be
released.",-0.10726507,-0.05465631,-0.0538798,C
8078,"head driver using regularization and assembling a high-visual qual-                          However, further research is needed to build a digital human system
                                       ity renderer.",Our solution focuses on training a generalized audio-to-                             can be generated from a large number of videos of the same speaker.,We carefully tweak the audio-to-behavior model and                             for any speaker with less available digital information.,2022-06-26 10:12:59+00:00,Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ailin Huang'), arxiv.Result.Author('Zhewei Huang'), arxiv.Result.Author('Shuchang Zhou')]","This paper reports our solution for ACM Multimedia ViCo 2022 Conversational
Head Generation Challenge, which aims to generate vivid face-to-face
conversation videos based on audio and reference images. Our solution focuses
on training a generalized audio-to-head driver using regularization and
assembling a high-visual quality renderer. We carefully tweak the
audio-to-behavior model and post-process the generated video using our
foreground-background fusion module. We get first place in the listening head
generation track and second place in the talking head generation track on the
official leaderboard. Our code is available at
https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration.",-0.0824245,0.053043596,-0.08197085,C
8080,"INTRODUCTION                              complement existing automobile datasets with limited FOV
                                                                                                              images and encourage further research in multi-task multi-
                                           Autonomous Driving is a challenging problem and it re-             camera computer vision algorithms for self-driving vehicles.","It is designed to
                                                                 I.","quires multiple sensors handling different aspects and robust         It is built based on industrialization needs addressing the
                                        sensor fusion algorithms which combine the senor informa-             diversity challenges [8].",2022-06-26 16:07:37+00:00,Woodscape Fisheye Object Detection for Autonomous Driving -- CVPR 2022 OmniCV Workshop Challenge,cs.CV,['cs.CV'],"[arxiv.Result.Author('Saravanabalagi Ramachandran'), arxiv.Result.Author('Ganesh Sistu'), arxiv.Result.Author('Varun Ravi Kumar'), arxiv.Result.Author('John McDonald'), arxiv.Result.Author('Senthil Yogamani')]","Object detection is a comprehensively studied problem in autonomous driving.
However, it has been relatively less explored in the case of fisheye cameras.
The strong radial distortion breaks the translation invariance inductive bias
of Convolutional Neural Networks. Thus, we present the WoodScape fisheye object
detection challenge for autonomous driving which was held as part of the CVPR
2022 Workshop on Omnidirectional Computer Vision (OmniCV). This is one of the
first competitions focused on fisheye camera object detection. We encouraged
the participants to design models which work natively on fisheye images without
rectification. We used CodaLab to host the competition based on the publicly
available WoodScape fisheye dataset. In this paper, we provide a detailed
analysis on the competition which attracted the participation of 120 global
teams and a total of 1492 submissions. We briefly discuss the details of the
winning methods and analyze their qualitative and quantitative results.",-0.29597127,0.2235088,-0.07194331,B
8081,"In the individual class scores, they achieved second     to encourage further research and novel solutions to Ô¨Åsheye
place for Bicycle class, and third place for the rest of       object detection.","We have started accepting
Institute of Technology, Xidian University belonged to this    submissions again keeping the challenge open to everyone
team.","In our future work, we plan to organize
the classes.",2022-06-26 16:07:37+00:00,Woodscape Fisheye Object Detection for Autonomous Driving -- CVPR 2022 OmniCV Workshop Challenge,cs.CV,['cs.CV'],"[arxiv.Result.Author('Saravanabalagi Ramachandran'), arxiv.Result.Author('Ganesh Sistu'), arxiv.Result.Author('Varun Ravi Kumar'), arxiv.Result.Author('John McDonald'), arxiv.Result.Author('Senthil Yogamani')]","Object detection is a comprehensively studied problem in autonomous driving.
However, it has been relatively less explored in the case of fisheye cameras.
The strong radial distortion breaks the translation invariance inductive bias
of Convolutional Neural Networks. Thus, we present the WoodScape fisheye object
detection challenge for autonomous driving which was held as part of the CVPR
2022 Workshop on Omnidirectional Computer Vision (OmniCV). This is one of the
first competitions focused on fisheye camera object detection. We encouraged
the participants to design models which work natively on fisheye images without
rectification. We used CodaLab to host the competition based on the publicly
available WoodScape fisheye dataset. In this paper, we provide a detailed
analysis on the competition which attracted the participation of 120 global
teams and a total of 1492 submissions. We briefly discuss the details of the
winning methods and analyze their qualitative and quantitative results.",-0.18846361,0.12224765,-0.23978475,B
8100,"Therefore, apart from                In fact, Ô¨Åne tuning on 10% of labels gives already very close
cropping which is a key augmentation, further study is re-                    performance compared to full-label supervised learning.",distortion) bear big importance as well.,"quired on the design of other augmentations to work well on
remote sensing multispectral images.",2022-06-27 11:04:47+00:00,Self-supervised Learning in Remote Sensing: A Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Conrad M Albrecht'), arxiv.Result.Author('Nassim Ait Ali Braham'), arxiv.Result.Author('Lichao Mou'), arxiv.Result.Author('Xiao Xiang Zhu')]","In deep learning research, self-supervised learning (SSL) has received great
attention triggering interest within both the computer vision and remote
sensing communities. While there has been a big success in computer vision,
most of the potential of SSL in the domain of earth observation remains locked.
In this paper, we provide an introduction to, and a review of the concepts and
latest developments in SSL for computer vision in the context of remote
sensing. Further, we provide a preliminary benchmark of modern SSL algorithms
on popular remote sensing datasets, verifying the potential of SSL in remote
sensing and providing an extended study on data augmentations. Finally, we
identify a list of promising directions of future research in SSL for earth
observation (SSL4EO) to pave the way for fruitful interaction of both domains.",-0.0713591,-0.024015514,0.09579995,C
8101,"Therefore, apart from
cropping which is a key augmentation, further study is re-
ACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2022       21

for Ô¨Åne-tuning, which outperforms supervised learning on all        some early contributions following this thread: Leenstra et
scenarios and provide huge improvements on tiny amount of           al.","This is a signiÔ¨Åcant difference compared
to natural images, where other augmentations (especially color
distortion) bear big importance as well.","[186] proposed a Sentinel-2 multitemporal cities pairs
labels.",2022-06-27 11:04:47+00:00,Self-supervised Learning in Remote Sensing: A Review,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Conrad M Albrecht'), arxiv.Result.Author('Nassim Ait Ali Braham'), arxiv.Result.Author('Lichao Mou'), arxiv.Result.Author('Xiao Xiang Zhu')]","In deep learning research, self-supervised learning (SSL) has received great
attention triggering interest within both the computer vision and remote
sensing communities. While there has been a big success in computer vision,
most of the potential of SSL in the domain of earth observation remains locked.
In this paper, we provide an introduction to, and a review of the concepts and
latest developments in SSL for computer vision in the context of remote
sensing. Further, we provide a preliminary benchmark of modern SSL algorithms
on popular remote sensing datasets, verifying the potential of SSL in remote
sensing and providing an extended study on data augmentations. Finally, we
identify a list of promising directions of future research in SSL for earth
observation (SSL4EO) to pave the way for fruitful interaction of both domains.",-0.15341547,0.010904285,0.010770874,B
8111,"In: IEEE Computer Society
which is not critical for obstacle detection, but        Conference on Computer Vision and Pat-
indicates that further research could be invested in     tern Recognition Workshops, https://doi.org/
maritime segmentation networks to address such           10.1109/CVPRW50498.2020.00033
cases.","We also observe less accurate seg-        of cranberries using point supervision and
mentation of top-most parts of static obstacles,         shape priors.","Bai X, Chen Z, Zhang Y, et al (2016) Infrared Ship
5 Conclusion                                             Target Segmentation Based on Spatial Infor-
                                                         mation Improved FCM.",2022-06-27 12:52:26+00:00,Learning with Weak Annotations for Robust Maritime Obstacle Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lojze ≈Ωust'), arxiv.Result.Author('Matej Kristan')]","Robust maritime obstacle detection is crucial for safe navigation of
autonomous boats and timely collision avoidance. The current state-of-the-art
is based on deep segmentation networks trained on large datasets. Per-pixel
ground truth labeling of such datasets, however, is labor-intensive and
expensive. We propose a new scaffolding learning regime (SLR), that leverages
weak annotations consisting of water edge, horizon and obstacle bounding boxes
to train segmentation-based obstacle detection networks, and thus reduces the
required ground truth labelling effort by twenty-fold. SLR trains an initial
model from weak annotations, then alternates between re-estimating the
segmentation pseudo labels and improving the network parameters. Experiments
show that maritime obstacle segmentation networks trained using SLR on weak
labels not only match, but outperform the same networks trained with dense
ground truth labels, which is a remarkable result. In addition to increased
accuracy, SLR also increases domain generalization and can be used for domain
adaptation with a low manual annotation load. The code and pre-trained models
are available at https://github.com/lojzezust/SLR .",-0.12303236,0.16238087,-0.10569127,B
8112,"We also observe less accurate segmentation of the top-most parts of static obstacles, which
                        is not critical for obstacle detection, but indicates that further research could be invested in
                        maritime segmentation networks to address such cases.","We observe failure cases on thin structures (e.g., part
                        of the surfboard missing) and false-positive detections on objects seen through shallow water.","Sensors 2022, 22, 9139                                    17 of 23
                              Input image
                                           WaSR  WaSRSLR

                                           Figure 7.",2022-06-27 12:52:26+00:00,Learning with Weak Annotations for Robust Maritime Obstacle Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lojze ≈Ωust'), arxiv.Result.Author('Matej Kristan')]","Robust maritime obstacle detection is critical for safe navigation of
autonomous boats and timely collision avoidance. The current state-of-the-art
is based on deep segmentation networks trained on large datasets. However,
per-pixel ground truth labeling of such datasets is labor-intensive and
expensive. We propose a new scaffolding learning regime (SLR) that leverages
weak annotations consisting of water edges, the horizon location, and obstacle
bounding boxes to train segmentation-based obstacle detection networks, thereby
reducing the required ground truth labeling effort by a factor of twenty. SLR
trains an initial model from weak annotations and then alternates between
re-estimating the segmentation pseudo-labels and improving the network
parameters. Experiments show that maritime obstacle segmentation networks
trained using SLR on weak annotations not only match but outperform the same
networks trained with dense ground truth labels, which is a remarkable result.
In addition to the increased accuracy, SLR also increases domain generalization
and can be used for domain adaptation with a low manual annotation load. The
SLR code and pre-trained models are available at
https://github.com/lojzezust/SLR .",-0.15134084,0.21688896,-0.0029844511,B
8120,"We further study the model by add the genetic distance as the
representation space and using the genetic distance between species as the embedding
of image distance as a new image distance generation method, the model has good
performance in predicting species genetic distance and species identification.","Therefore, through the selection of attention
mechanism and different attention structures, we embed them into the lightweight
convolution neural network to construct the mushroom recognition model, that is,
MushroomNet.","Due to
the small amount of parameters of skeleton structure and less computational resources,
it is suitable for mobile terminal and is the best choice for field researchers.",2022-06-27 15:43:03+00:00,Mushroom image recognition and distance generation based on attention-mechanism model and genetic information,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wenbin Liao'), arxiv.Result.Author('Jiewen Xiao'), arxiv.Result.Author('Chengbo Zhao'), arxiv.Result.Author('Yonggong Han'), arxiv.Result.Author('ZhiJie Geng'), arxiv.Result.Author('Jianxin Wang'), arxiv.Result.Author('Yihua Yang')]","The species identification of Macrofungi, i.e. mushrooms, has always been a
challenging task. There are still a large number of poisonous mushrooms that
have not been found, which poses a risk to people's life. However, the
traditional identification method requires a large number of experts with
knowledge in the field of taxonomy for manual identification, it is not only
inefficient but also consumes a lot of manpower and capital costs. In this
paper, we propose a new model based on attention-mechanism, MushroomNet, which
applies the lightweight network MobileNetV3 as the backbone model, combined
with the attention structure proposed by us, and has achieved excellent
performance in the mushroom recognition task. On the public dataset, the test
accuracy of the MushroomNet model has reached 83.9%, and on the local dataset,
the test accuracy has reached 77.4%. The proposed attention mechanisms well
focused attention on the bodies of mushroom image for mixed channel attention
and the attention heat maps visualized by Grad-CAM. Further, in this study,
genetic distance was added to the mushroom image recognition task, the genetic
distance was used as the representation space, and the genetic distance between
each pair of mushroom species in the dataset was used as the embedding of the
genetic distance representation space, so as to predict the image distance and
species. identify. We found that using the MES activation function can predict
the genetic distance of mushrooms very well, but the accuracy is lower than
that of SoftMax. The proposed MushroomNet was demonstrated it shows great
potential for automatic and online mushroom image and the proposed automatic
procedure would assist and be a reference to traditional mushroom
classification.",-0.12355596,-0.14680538,0.027643932,C
8121,"We further study the rank-wise improvement in recognition
accuracy after reconstructing the above occluded sequences
separately by each of the three models and present the corre-
            V. CONCLUSIONS AND FUTURE WORK                                     [12] K. Shiraga, Y. Makihara, D. Muramatsu, T. Echigo, and Y. Yagi,
                                                                                     ‚ÄúGEINet: View-Invariant Gait Recognition Using a Convolutional Neu-
In this work, we propose a novel occlusion reconstruction                            ral Network,‚Äù in Proc.","The average
Dice scores obtained from the three networks by comparing
the reconstruction frames with the ground truth for the varying
degrees of occlusion are 0.82, 0.87, and 0.90, respectively.",of the Intl.,2022-06-20 16:04:31+00:00,Gait Cycle Reconstruction and Human Identification from Occluded Sequences,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Abhishek Paul'), arxiv.Result.Author('Manav Mukesh Jain'), arxiv.Result.Author('Jinesh Jain'), arxiv.Result.Author('Pratik Chattopadhyay')]","Gait-based person identification from videos captured at surveillance sites
using Computer Vision-based techniques is quite challenging since these walking
sequences are usually corrupted with occlusion, and a complete cycle of gait is
not always available. In this work, we propose an effective neural
network-based model to reconstruct the occluded frames in an input sequence
before carrying out gait recognition. Specifically, we employ LSTM networks to
predict an embedding for each occluded frame both from the forward and the
backward directions, and next fuse the predictions from the two LSTMs by
employing a network of residual blocks and convolutional layers. While the
LSTMs are trained to minimize the mean-squared loss, the fusion network is
trained to optimize the pixel-wise cross-entropy loss between the ground-truth
and the reconstructed samples. Evaluation of our approach has been done using
synthetically occluded sequences generated from the OU-ISIR LP and CASIA-B data
and real-occluded sequences present in the TUM-IITKGP data. The effectiveness
of the proposed reconstruction model has been verified through the Dice score
and gait-based recognition accuracy using some popular gait recognition
methods. Comparative study with existing occlusion handling methods in gait
recognition highlights the superiority of our proposed occlusion reconstruction
approach over the others.",-0.13683605,0.04007165,0.0769873,B
8137,"To promote further research, we have
                                        made key-points dataset and code publicly available.","We further demonstrate that the classification accu-
                                        racies critically depend on the cross validation method employed
                                        and can often be misleading.","CCS CONCEPTS                                                           Figure 1: A simplified view of the 2-stage architecture of the
                                                                                                               Yogasana Classifier with feature extractor and classifier.",2022-06-27 18:40:34+00:00,A View Independent Classification Framework for Yoga Postures,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mustafa Chasmai'), arxiv.Result.Author('Nirjhar Das'), arxiv.Result.Author('Aman Bhardwaj'), arxiv.Result.Author('Rahul Garg')]","Yoga is a globally acclaimed and widely recommended practice for a healthy
living. Maintaining correct posture while performing a Yogasana is of utmost
importance. In this work, we employ transfer learning from Human Pose
Estimation models for extracting 136 key-points spread all over the body to
train a Random Forest classifier which is used for estimation of the Yogasanas.
The results are evaluated on an in-house collected extensive yoga video
database of 51 subjects recorded from 4 different camera angles. We propose a 3
step scheme for evaluating the generalizability of a Yoga classifier by testing
it on 1) unseen frames, 2) unseen subjects, and 3) unseen camera angles. We
argue that for most of the applications, validation accuracies on unseen
subjects and unseen camera angles would be most important. We empirically
analyze over three public datasets, the advantage of transfer learning and the
possibilities of target leakage. We further demonstrate that the classification
accuracies critically depend on the cross validation method employed and can
often be misleading. To promote further research, we have made key-points
dataset and code publicly available.",0.10793648,-0.10663767,-0.16763306,A
8138,"This deep learn-
code and the extracted keypoints dataset will be made public                               ing approach for extracting key points is a relatively inexpensive
for the reproducibility of results and to encourage further research.2                     alternative which only requires RGB images as compared to the
                                                                                           depth and infra-red based approach used by Kinect.",The                        frameworks like AlphaPose [8] and OpenPose [5].,"We propose a novel scheme of evaluation specifically designed
for better generalizability of a Yoga classifier and to prevent possible                      Yadav et al.",2022-06-27 18:40:34+00:00,A View Independent Classification Framework for Yoga Postures,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mustafa Chasmai'), arxiv.Result.Author('Nirjhar Das'), arxiv.Result.Author('Aman Bhardwaj'), arxiv.Result.Author('Rahul Garg')]","Yoga is a globally acclaimed and widely recommended practice for a healthy
living. Maintaining correct posture while performing a Yogasana is of utmost
importance. In this work, we employ transfer learning from Human Pose
Estimation models for extracting 136 key-points spread all over the body to
train a Random Forest classifier which is used for estimation of the Yogasanas.
The results are evaluated on an in-house collected extensive yoga video
database of 51 subjects recorded from 4 different camera angles. We propose a 3
step scheme for evaluating the generalizability of a Yoga classifier by testing
it on 1) unseen frames, 2) unseen subjects, and 3) unseen camera angles. We
argue that for most of the applications, validation accuracies on unseen
subjects and unseen camera angles would be most important. We empirically
analyze over three public datasets, the advantage of transfer learning and the
possibilities of target leakage. We further demonstrate that the classification
accuracies critically depend on the cross validation method employed and can
often be misleading. To promote further research, we have made key-points
dataset and code publicly available.",-0.18490314,-0.03945803,-0.06257159,B
8139,"To promote further research, we have
                                        made key-points dataset and code publicly available.","We further demonstrate that the classification accu-
                                        racies critically depend on the cross validation method employed
                                        and can often be misleading.","CCS CONCEPTS                                                           Figure 1: A simplified view of the 2-stage architecture of the
                                                                                                               Yogasana Classifier with feature extractor and classifier.",2022-06-27 18:40:34+00:00,A View Independent Classification Framework for Yoga Postures,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mustafa Chasmai'), arxiv.Result.Author('Nirjhar Das'), arxiv.Result.Author('Aman Bhardwaj'), arxiv.Result.Author('Rahul Garg')]","Yoga is a globally acclaimed and widely recommended practice for a healthy
living. Maintaining correct posture while performing a Yogasana is of utmost
importance. In this work, we employ transfer learning from Human Pose
Estimation models for extracting 136 key-points spread all over the body to
train a Random Forest classifier which is used for estimation of the Yogasanas.
The results are evaluated on an in-house collected extensive yoga video
database of 51 subjects recorded from 4 different camera angles. We propose a 3
step scheme for evaluating the generalizability of a Yoga classifier by testing
it on 1) unseen frames, 2) unseen subjects, and 3) unseen camera angles. We
argue that for most of the applications, validation accuracies on unseen
subjects and unseen camera angles would be most important. We empirically
analyze over three public datasets, the advantage of transfer learning and the
possibilities of target leakage. We further demonstrate that the classification
accuracies critically depend on the cross validation method employed and can
often be misleading. To promote further research, we have made key-points
dataset and code publicly available.",0.10793648,-0.10663767,-0.16763306,A
8140,"The limitations show significant    [9] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang, and Lei
scope for further research in view independent pose classification            Zhang.",alleviate many of these limitations.,2020.,2022-06-27 18:40:34+00:00,A View Independent Classification Framework for Yoga Postures,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mustafa Chasmai'), arxiv.Result.Author('Nirjhar Das'), arxiv.Result.Author('Aman Bhardwaj'), arxiv.Result.Author('Rahul Garg')]","Yoga is a globally acclaimed and widely recommended practice for a healthy
living. Maintaining correct posture while performing a Yogasana is of utmost
importance. In this work, we employ transfer learning from Human Pose
Estimation models for extracting 136 key-points spread all over the body to
train a Random Forest classifier which is used for estimation of the Yogasanas.
The results are evaluated on an in-house collected extensive yoga video
database of 51 subjects recorded from 4 different camera angles. We propose a 3
step scheme for evaluating the generalizability of a Yoga classifier by testing
it on 1) unseen frames, 2) unseen subjects, and 3) unseen camera angles. We
argue that for most of the applications, validation accuracies on unseen
subjects and unseen camera angles would be most important. We empirically
analyze over three public datasets, the advantage of transfer learning and the
possibilities of target leakage. We further demonstrate that the classification
accuracies critically depend on the cross validation method employed and can
often be misleading. To promote further research, we have made key-points
dataset and code publicly available.",-0.1439811,0.059767894,-0.2082692,B
8162,"Theoretically, we discuss the critical issues for gait-speciÔ¨Åc contrastive framework and present some insights for
                                        further study.","After transfer learning, our method outperforms existing methods by a large
                                        margin in most cases.","As far as we know, GaitLU-1M is the Ô¨Årst large-scale unlabelled gait dataset, and GaitSSB is the Ô¨Årst method that
                                        achieves remarkable unsupervised results on the aforementioned benchmarks.",2022-06-28 12:33:42+00:00,Learning Gait Representation from Massive Unlabelled Walking Videos: A Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Fan'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Jilong Wang'), arxiv.Result.Author('Yongzhen Huang'), arxiv.Result.Author('Shiqi Yu')]","Gait depicts individuals' unique and distinguishing walking patterns and has
become one of the most promising biometric features for human identification.
As a fine-grained recognition task, gait recognition is easily affected by many
factors and usually requires a large amount of completely annotated data that
is costly and insatiable. This paper proposes a large-scale self-supervised
benchmark for gait recognition with contrastive learning, aiming to learn the
general gait representation from massive unlabelled walking videos for
practical applications via offering informative walking priors and diverse
real-world variations. Specifically, we collect a large-scale unlabelled gait
dataset GaitLU-1M consisting of 1.02M walking sequences and propose a
conceptually simple yet empirically powerful baseline model GaitSSB.
Experimentally, we evaluate the pre-trained model on four widely-used gait
benchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer
learning. The unsupervised results are comparable to or even better than the
early model-based and GEI-based methods. After transfer learning, our method
outperforms existing methods by a large margin in most cases. Theoretically, we
discuss the critical issues for gait-specific contrastive framework and present
some insights for further study. As far as we know, GaitLU-1M is the first
large-scale unlabelled gait dataset, and GaitSSB is the first method that
achieves remarkable unsupervised results on the aforementioned benchmarks. The
source code of GaitSSB will be integrated into OpenGait which is available at
https://github.com/ShiqiYu/OpenGait.",-0.06445491,-0.11026037,-0.015681962,C
8163,"Therefore, forming positive pairs by
                                                                 effectively simulating the cloth-changing cases should be
                                                                 critical for further study.","The underlying reason may
                                                                 be that the unlabelled GaitLU-1M lacks the natural dress-
                                                                 changing factors, giving rise to the invalidity of enlarging
                                                                 the pre-training scale.","ùëëùïã&(

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL.",2022-06-28 12:33:42+00:00,Learning Gait Representation from Massive Unlabelled Walking Videos: A Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Fan'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Jilong Wang'), arxiv.Result.Author('Yongzhen Huang'), arxiv.Result.Author('Shiqi Yu')]","Gait depicts individuals' unique and distinguishing walking patterns and has
become one of the most promising biometric features for human identification.
As a fine-grained recognition task, gait recognition is easily affected by many
factors and usually requires a large amount of completely annotated data that
is costly and insatiable. This paper proposes a large-scale self-supervised
benchmark for gait recognition with contrastive learning, aiming to learn the
general gait representation from massive unlabelled walking videos for
practical applications via offering informative walking priors and diverse
real-world variations. Specifically, we collect a large-scale unlabelled gait
dataset GaitLU-1M consisting of 1.02M walking sequences and propose a
conceptually simple yet empirically powerful baseline model GaitSSB.
Experimentally, we evaluate the pre-trained model on four widely-used gait
benchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer
learning. The unsupervised results are comparable to or even better than the
early model-based and GEI-based methods. After transfer learning, our method
outperforms existing methods by a large margin in most cases. Theoretically, we
discuss the critical issues for gait-specific contrastive framework and present
some insights for further study. As far as we know, GaitLU-1M is the first
large-scale unlabelled gait dataset, and GaitSSB is the first method that
achieves remarkable unsupervised results on the aforementioned benchmarks. The
source code of GaitSSB will be integrated into OpenGait which is available at
https://github.com/ShiqiYu/OpenGait.",0.21488333,-0.0058962777,-0.09001126,A
8164,suggestions for further research.,"Also, we propose some methodological        Œ†(x) = {x‚àí|x‚àí ‚àà/ Œ†(x)} and œÄ(x) = {x‚àí|x‚àí ‚àà/ œÄ(x)}.","Consequently,
                                                                                   d‚àíœÄ (x) d‚àíŒ†(x)                                   (4)
    Let a map Œ†(¬∑) present the corruptions caused by various
noisy factors.",2022-06-28 12:33:42+00:00,Learning Gait Representation from Massive Unlabelled Walking Videos: A Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Fan'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Jilong Wang'), arxiv.Result.Author('Yongzhen Huang'), arxiv.Result.Author('Shiqi Yu')]","Gait depicts individuals' unique and distinguishing walking patterns and has
become one of the most promising biometric features for human identification.
As a fine-grained recognition task, gait recognition is easily affected by many
factors and usually requires a large amount of completely annotated data that
is costly and insatiable. This paper proposes a large-scale self-supervised
benchmark for gait recognition with contrastive learning, aiming to learn the
general gait representation from massive unlabelled walking videos for
practical applications via offering informative walking priors and diverse
real-world variations. Specifically, we collect a large-scale unlabelled gait
dataset GaitLU-1M consisting of 1.02M walking sequences and propose a
conceptually simple yet empirically powerful baseline model GaitSSB.
Experimentally, we evaluate the pre-trained model on four widely-used gait
benchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer
learning. The unsupervised results are comparable to or even better than the
early model-based and GEI-based methods. After transfer learning, our method
outperforms existing methods by a large margin in most cases. Theoretically, we
discuss the critical issues for gait-specific contrastive framework and present
some insights for further study. As far as we know, GaitLU-1M is the first
large-scale unlabelled gait dataset, and GaitSSB is the first method that
achieves remarkable unsupervised results on the aforementioned benchmarks. The
source code of GaitSSB will be integrated into OpenGait which is available at
https://github.com/ShiqiYu/OpenGait.",0.34896898,0.19301,0.044113487,A
8165,"YY, MONTH 2022  14

N > 1 times by œÄ(¬∑), i.e., x1 ‚àà œÄ(x), x2 ‚àà œÄ(x1), ..., xN ‚àà              Totally, exploring the more advanced augmentation strategy
œÄ(xN‚àí1) with                                                             dominating dramatically view-changing variations should
                                                                         be vital for further research.","XX, NO.","xi = arg max xi‚àí1 ‚àí x+ 2  (5)
                                                                             Last but not least, the most challenging bag-carrying and
x+ ‚ààœÄ (xi‚àí1 )                                                            dress-changing cases, which have been neglected for clarity
                                                                         analysis, are worth being discussed seriously as well.",2022-06-28 12:33:42+00:00,Learning Gait Representation from Massive Unlabelled Walking Videos: A Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Fan'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Jilong Wang'), arxiv.Result.Author('Yongzhen Huang'), arxiv.Result.Author('Shiqi Yu')]","Gait depicts individuals' unique and distinguishing walking patterns and has
become one of the most promising biometric features for human identification.
As a fine-grained recognition task, gait recognition is easily affected by many
factors and usually requires a large amount of completely annotated data that
is costly and insatiable. This paper proposes a large-scale self-supervised
benchmark for gait recognition with contrastive learning, aiming to learn the
general gait representation from massive unlabelled walking videos for
practical applications via offering informative walking priors and diverse
real-world variations. Specifically, we collect a large-scale unlabelled gait
dataset GaitLU-1M consisting of 1.02M walking sequences and propose a
conceptually simple yet empirically powerful baseline model GaitSSB.
Experimentally, we evaluate the pre-trained model on four widely-used gait
benchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer
learning. The unsupervised results are comparable to or even better than the
early model-based and GEI-based methods. After transfer learning, our method
outperforms existing methods by a large margin in most cases. Theoretically, we
discuss the critical issues for gait-specific contrastive framework and present
some insights for further study. As far as we know, GaitLU-1M is the first
large-scale unlabelled gait dataset, and GaitSSB is the first method that
achieves remarkable unsupervised results on the aforementioned benchmarks. The
source code of GaitSSB will be integrated into OpenGait which is available at
https://github.com/ShiqiYu/OpenGait.",0.17842358,0.128694,-0.03189106,A
8316,"We hope
the intensity-free version, while Voxel-MAE still shows a        that our work can encourage further research toward the use
substantial increase compared to the baseline.","However, Transformer backbones are
found this to help Ô¨Ånal detection performance compared to        still lagging behind CNN-based feature extractors.","of Transformer backbones for automotive point clouds, to
                                                                 make use of vast amounts of raw data.",2022-07-01 16:31:45+00:00,Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Georg Hess'), arxiv.Result.Author('Johan Jaxing'), arxiv.Result.Author('Elias Svensson'), arxiv.Result.Author('David Hagerman'), arxiv.Result.Author('Christoffer Petersson'), arxiv.Result.Author('Lennart Svensson')]","Masked autoencoding has become a successful pretraining paradigm for
Transformer models for text, images, and, recently, point clouds. Raw
automotive datasets are suitable candidates for self-supervised pre-training as
they generally are cheap to collect compared to annotations for tasks like 3D
object detection (OD). However, the development of masked autoencoders for
point clouds has focused solely on synthetic and indoor data. Consequently,
existing methods have tailored their representations and models toward small
and dense point clouds with homogeneous point densities.In this work, we study
masked autoencoding for point clouds in an automotive setting, which are sparse
and for which the point density can vary drastically among objects in the same
scene. To this end, we propose Voxel-MAE, a simple masked autoencoding
pre-training scheme designed for voxel representations. We pre-train the
backbone of a Transformer-based 3D object detector to reconstruct masked voxels
and to distinguish between empty and non-empty voxels. Our method improves the
3D OD performance by 1.75 mAP points and 1.05 NDS on the challenging nuScenes
dataset. Further, we show that by pre-training with Voxel-MAE, we require only
40% of the annotated data to outperform a randomly initialized equivalent. Code
available at https://github.com/georghess/voxel-mae",-0.23833841,0.13733676,0.19865046,B
8338,"An avenue to overcome this negative impact might
certain views, which necessitates all inputs to be aligned                                                          be further research in digital watermarks such as [49] for
with the geometry.","or objects, which may be used for potentially harmful pur-
However, the per-pixel lighting bakes-in the geometry in                                                            poses.","So, we warp the reference photo Iphoto                                                           materials generated through our material priors, embedded
and the material part mask Mphoto to match the geometry.",2022-07-02 06:52:44+00:00,PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu-Ying Yeh'), arxiv.Result.Author('Zhengqin Li'), arxiv.Result.Author('Yannick Hold-Geoffroy'), arxiv.Result.Author('Rui Zhu'), arxiv.Result.Author('Zexiang Xu'), arxiv.Result.Author('Milo≈° Ha≈°an'), arxiv.Result.Author('Kalyan Sunkavalli'), arxiv.Result.Author('Manmohan Chandraker')]","Most indoor 3D scene reconstruction methods focus on recovering 3D geometry
and scene layout. In this work, we go beyond this to propose PhotoScene, a
framework that takes input image(s) of a scene along with approximately aligned
CAD geometry (either reconstructed automatically or manually specified) and
builds a photorealistic digital twin with high-quality materials and similar
lighting. We model scene materials using procedural material graphs; such
graphs represent photorealistic and resolution-independent materials. We
optimize the parameters of these graphs and their texture scale and rotation,
as well as the scene lighting to best match the input image via a
differentiable rendering layer. We evaluate our technique on objects and layout
reconstructions from ScanNet, SUN RGB-D and stock photographs, and demonstrate
that our method reconstructs high-quality, fully relightable 3D scenes that can
be re-rendered under arbitrary viewpoints, zooms and lighting.",-0.07625832,0.30715334,0.032087587,B
8347,"By open-sourcing
our evaluation framework, we aim to stimulate further research in this area.","Uncertainty sampling and
model-driven representativeness sampling strategies seem to require more Ô¨Åne-tuning and
possibly the integration of expert knowledge on dataset characteristics.","6
References

Alexandra Albu, Trevor Beugeling, and Denis Laurendeau.",2022-07-02 14:27:58+00:00,Less Is More: A Comparison of Active Learning Strategies for 3D Medical Image Segmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.4.6; I.2.10; J.3']","[arxiv.Result.Author('Josafat-Mattias Burmeister'), arxiv.Result.Author('Marcel Fernandez Rosas'), arxiv.Result.Author('Johannes Hagemann'), arxiv.Result.Author('Jonas Kordt'), arxiv.Result.Author('Jasper Blum'), arxiv.Result.Author('Simon Shabo'), arxiv.Result.Author('Benjamin Bergner'), arxiv.Result.Author('Christoph Lippert')]","Since labeling medical image data is a costly and labor-intensive process,
active learning has gained much popularity in the medical image segmentation
domain in recent years. A variety of active learning strategies have been
proposed in the literature, but their effectiveness is highly dependent on the
dataset and training scenario. To facilitate the comparison of existing
strategies and provide a baseline for evaluating novel strategies, we evaluate
the performance of several well-known active learning strategies on three
datasets from the Medical Segmentation Decathlon. Additionally, we consider a
strided sampling strategy specifically tailored to 3D image data. We
demonstrate that both random and strided sampling act as strong baselines and
discuss the advantages and disadvantages of the studied methods. To allow other
researchers to compare their work to our results, we provide an open-source
framework for benchmarking active learning strategies on a variety of medical
segmentation datasets.",0.109818794,-0.19003558,-0.20889293,A
8352,"Most importantly, 3) the lack of large-scale annotated
data restricts the performance of deep models that rely on suÔ¨Écient training
data, thereby hindering further research in this Ô¨Åeld.","2) The high appearance similarity between
Trichomonas and other cells (e.g., leukocyte) makes them easily confused with
complex surroundings.","It is worth noting that the
above factors also reÔ¨Çect the clear diÔ¨Äerences in object segmentation between
the microscope images of Trichomonas in our work and conventional cells (e.g.,
HeLa cells [23] and blood cells [16]).",2022-07-03 07:29:05+00:00,Trichomonas Vaginalis Segmentation in Microscope Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lin Li'), arxiv.Result.Author('Jingyi Liu'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Xunkun Wang'), arxiv.Result.Author('Tian-Zhu Xiang')]","Trichomoniasis is a common infectious disease with high incidence caused by
the parasite Trichomonas vaginalis, increasing the risk of getting HIV in
humans if left untreated. Automated detection of Trichomonas vaginalis from
microscopic images can provide vital information for the diagnosis of
trichomoniasis. However, accurate Trichomonas vaginalis segmentation (TVS) is a
challenging task due to the high appearance similarity between the Trichomonas
and other cells (e.g., leukocyte), the large appearance variation caused by
their motility, and, most importantly, the lack of large-scale annotated data
for deep model training. To address these challenges, we elaborately collected
the first large-scale Microscopic Image dataset of Trichomonas Vaginalis, named
TVMI3K, which consists of 3,158 images covering Trichomonas of various
appearances in diverse backgrounds, with high-quality annotations including
object-level mask labels, object boundaries, and challenging attributes.
Besides, we propose a simple yet effective baseline, termed TVNet, to
automatically segment Trichomonas from microscopic images, including
high-resolution fusion and foreground-background attention modules. Extensive
experiments demonstrate that our model achieves superior segmentation
performance and outperforms various cutting-edge object detection models both
quantitatively and qualitatively, making it a promising framework to promote
future research in TVS tasks. The dataset and results will be publicly
available at: https://github.com/CellRecog/cellRecog.",-0.14360231,-0.16823623,0.061604116,C
8353,"To our knowledge, this
is the Ô¨Årst large-scale dataset for TVS that can serve as a catalyst for promot-
ing further research in this Ô¨Åeld in the deep learning era.","In a nutshell, our main contributions
are threefold: (1) We carefully collect TVMI3K, a large-scale dataset for
TVS, which consists of 3,158 microscopic images covering Trichomonas of vari-
ous appearances in diverse backgrounds, with high-quality annotations of object-
level labels, object boundaries and challenging attributes.","(2) We proposed a
                      Trichomonas Vaginalis Segmentation in Microscope Images                 3

                      Image-level Attribute                       Object-level Attribute

               MO/SO  MO/OS/SO               MO/OS/SO  CS OC  OC  SQ  OF                  SQ

Trichomonas
   Vaginalis

Object-leve l
   Annotation

Edge
   Annotation

Fig.",2022-07-03 07:29:05+00:00,Trichomonas Vaginalis Segmentation in Microscope Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lin Li'), arxiv.Result.Author('Jingyi Liu'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Xunkun Wang'), arxiv.Result.Author('Tian-Zhu Xiang')]","Trichomoniasis is a common infectious disease with high incidence caused by
the parasite Trichomonas vaginalis, increasing the risk of getting HIV in
humans if left untreated. Automated detection of Trichomonas vaginalis from
microscopic images can provide vital information for the diagnosis of
trichomoniasis. However, accurate Trichomonas vaginalis segmentation (TVS) is a
challenging task due to the high appearance similarity between the Trichomonas
and other cells (e.g., leukocyte), the large appearance variation caused by
their motility, and, most importantly, the lack of large-scale annotated data
for deep model training. To address these challenges, we elaborately collected
the first large-scale Microscopic Image dataset of Trichomonas Vaginalis, named
TVMI3K, which consists of 3,158 images covering Trichomonas of various
appearances in diverse backgrounds, with high-quality annotations including
object-level mask labels, object boundaries, and challenging attributes.
Besides, we propose a simple yet effective baseline, termed TVNet, to
automatically segment Trichomonas from microscopic images, including
high-resolution fusion and foreground-background attention modules. Extensive
experiments demonstrate that our model achieves superior segmentation
performance and outperforms various cutting-edge object detection models both
quantitatively and qualitatively, making it a promising framework to promote
future research in TVS tasks. The dataset and results will be publicly
available at: https://github.com/CellRecog/cellRecog.",-0.31454378,-0.122664124,0.040776737,C
8356,"Accordingly, we hope that our direct calibration
                                                                   method will enable further research on event cameras and
                                                                   sensor fusion.","As event-driven vision technology continues to mature
                                                                   with more applications being explored, multi-sensor setups
                                                                   with event cameras may become more prominent in the Ô¨Åeld
                                                                   of robotics.","References                                                          [13] Andreas Geiger, Frank Moosmann, O¬® mer Car, and Bernhard
                                                                          Schuster.",2022-07-03 11:05:45+00:00,Lasers to Events: Automatic Extrinsic Calibration of Lidars and Event Cameras,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Kevin Ta'), arxiv.Result.Author('David Bruggemann'), arxiv.Result.Author('Tim Br√∂dermann'), arxiv.Result.Author('Christos Sakaridis'), arxiv.Result.Author('Luc Van Gool')]","Despite significant academic and corporate efforts, autonomous driving under
adverse visual conditions still proves challenging. As neuromorphic technology
has matured, its application to robotics and autonomous vehicle systems has
become an area of active research. Low-light and latency-demanding situations
can benefit. To enable event cameras to operate alongside staple sensors like
lidar in perception tasks, we propose a direct, temporally-decoupled
calibration method between event cameras and lidars. The high dynamic range and
low-light operation of event cameras are exploited to directly register lidar
laser returns, allowing information-based correlation methods to optimize for
the 6-DoF extrinsic calibration between the two sensors. This paper presents
the first direct calibration method between event cameras and lidars, removing
dependencies on frame-based camera intermediaries and/or highly-accurate hand
measurements. Code will be made publicly available.",-0.20397994,0.45429522,-0.21360537,B
8357,"Accordingly, we hope that
                                                                              our direct calibration method will enable further research on
                                                                              event cameras and sensor fusion.","As event-driven vision technology continues to mature,
                                                                              multi-sensor setups with event cameras may become more
                                                                              prominent in the Ô¨Åeld of robotics.","REFERENCES                                                 Compressive Data-Formatting Pipeline,‚Äù in International Solid- State
                                                                                        Circuits Conference (ISSCC).",2022-07-03 11:05:45+00:00,L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Kevin Ta'), arxiv.Result.Author('David Bruggemann'), arxiv.Result.Author('Tim Br√∂dermann'), arxiv.Result.Author('Christos Sakaridis'), arxiv.Result.Author('Luc Van Gool')]","As neuromorphic technology is maturing, its application to robotics and
autonomous vehicle systems has become an area of active research. In
particular, event cameras have emerged as a compelling alternative to
frame-based cameras in low-power and latency-demanding applications. To enable
event cameras to operate alongside staple sensors like lidar in perception
tasks, we propose a direct, temporally-decoupled extrinsic calibration method
between event cameras and lidars. The high dynamic range, high temporal
resolution, and low-latency operation of event cameras are exploited to
directly register lidar laser returns, allowing information-based correlation
methods to optimize for the 6-DoF extrinsic calibration between the two
sensors. This paper presents the first direct calibration method between event
cameras and lidars, removing dependencies on frame-based camera intermediaries
and/or highly-accurate hand measurements. Code will be made publicly available.",-0.102613315,0.40851358,-0.026839843,B
8358,"As event-driven vision technology contin-
                                                                              ues to mature, we hope that our direct calibration method will
                                                                              enable further research on event cameras and sensor fusion.","L2E offers a Ô¨Çexible automatic ap-
                                                                              proach to extrinsic calibration that leverages direct correlation
                                                                              between the lidar active signals and the corresponding regis-
                                                                              tered events without the need for precise time synchronization
                                                                              or dynamic scenes.","REFERENCES                                                 Compressive Data-Formatting Pipeline,‚Äù in International Solid- State
                                                                                        Circuits Conference (ISSCC).",2022-07-03 11:05:45+00:00,L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Kevin Ta'), arxiv.Result.Author('David Bruggemann'), arxiv.Result.Author('Tim Br√∂dermann'), arxiv.Result.Author('Christos Sakaridis'), arxiv.Result.Author('Luc Van Gool')]","As neuromorphic technology is maturing, its application to robotics and
autonomous vehicle systems has become an area of active research. In
particular, event cameras have emerged as a compelling alternative to
frame-based cameras in low-power and latency-demanding applications. To enable
event cameras to operate alongside staple sensors like lidar in perception
tasks, we propose a direct, temporally-decoupled extrinsic calibration method
between event cameras and lidars. The high dynamic range, high temporal
resolution, and low-latency operation of event cameras are exploited to
directly register lidar laser returns, allowing information-based correlation
methods to optimize for the 6-DoF extrinsic calibration between the two
sensors. This paper presents the first direct calibration method between event
cameras and lidars, removing dependencies on frame-based camera intermediaries
and/or highly-accurate hand measurements.",-0.092474245,0.34641632,0.034724362,B
8359,"As event-driven vision technology contin-
                                                                              ues to mature, we hope that our direct calibration method will
                                                                              enable further research on event cameras and sensor fusion.","L2E offers a Ô¨Çexible automatic ap-
                                                                              proach to extrinsic calibration that leverages direct correlation
                                                                              between the lidar active signals and the corresponding regis-
                                                                              tered events without the need for precise time synchronization
                                                                              or dynamic scenes.","REFERENCES                                                Compressive Data-Formatting Pipeline,‚Äù in International Solid- State
                                                                                       Circuits Conference (ISSCC).",2022-07-03 11:05:45+00:00,L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Kevin Ta'), arxiv.Result.Author('David Bruggemann'), arxiv.Result.Author('Tim Br√∂dermann'), arxiv.Result.Author('Christos Sakaridis'), arxiv.Result.Author('Luc Van Gool')]","As neuromorphic technology is maturing, its application to robotics and
autonomous vehicle systems has become an area of active research. In
particular, event cameras have emerged as a compelling alternative to
frame-based cameras in low-power and latency-demanding applications. To enable
event cameras to operate alongside staple sensors like lidar in perception
tasks, we propose a direct, temporally-decoupled extrinsic calibration method
between event cameras and lidars. The high dynamic range, high temporal
resolution, and low-latency operation of event cameras are exploited to
directly register lidar laser returns, allowing information-based correlation
methods to optimize for the 6-DoF extrinsic calibration between the two
sensors. This paper presents the first direct calibration method between event
cameras and lidars, removing dependencies on frame-based camera intermediaries
and/or highly-accurate hand measurements.",-0.092474245,0.34641632,0.034724362,B
8382,We developed a mobile application with functions         to further study this problem.,"NTU-Outdoor-38
To address privacy/consent concerns, we apply a new privacy-      captures the inter-camera performance imbalances present in
aware data collection strategy during the collection of our new   real-life camera networks and serves as an excellent test-bed
dataset.","To the best of our knowledge,
for participants to declare consent, log their own appearance     it is the only publicly available Person ReID dataset collected
attributes, upload a reference picture of themselves and record   from over 30 cameras.",2022-07-04 05:16:16+00:00,Adversarial Pairwise Reverse Attention for Camera Performance Imbalance in Person Re-identification: New Dataset and Metrics,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eugene P. W. Ang'), arxiv.Result.Author('Shan Lin'), arxiv.Result.Author('Rahul Ahuja'), arxiv.Result.Author('Nemath Ahmed'), arxiv.Result.Author('Alex C. Kot')]","Existing evaluation metrics for Person Re-Identification (Person ReID) models
focus on system-wide performance. However, our studies reveal weaknesses due to
the uneven data distributions among cameras and different camera properties
that expose the ReID system to exploitation. In this work, we raise the
long-ignored ReID problem of camera performance imbalance and collect a
real-world privacy-aware dataset from 38 cameras to assist the study of the
imbalance issue. We propose new metrics to quantify camera performance
imbalance and further propose the Adversarial Pairwise Reverse Attention (APRA)
Module to guide the model learning the camera invariant feature with a novel
pairwise attention inversion mechanism.",-0.1404905,0.11669409,-0.16357344,B
8423,"The conducted experiments demonstrate
the feasibility of the task and pave the path for further research in the Ô¨Åeld.","Ablation studies are con-
ducted with various architectures and feature fusion strategies for the repre-
sentation of the human trajectories.","Keywords: Forensics, Human-related crime classiÔ¨Åcation, Human
behaviour analysis, Surveillance videos

                                                2
1.",2022-07-04 19:37:06+00:00,Crime scene classification from skeletal trajectory analysis in surveillance settings,cs.CV,['cs.CV'],"[arxiv.Result.Author('Alina-Daniela Matei'), arxiv.Result.Author('Estefania Talavera'), arxiv.Result.Author('Maya Aghaei')]","Video anomaly analysis is a core task actively pursued in the field of
computer vision, with applications extending to real-world crime detection in
surveillance footage. In this work, we address the task of human-related crime
classification. In our proposed approach, the human body in video frames,
represented as skeletal joints trajectories, is used as the main source of
exploration. First, we introduce the significance of extending the ground truth
labels for HR-Crime dataset and hence, propose a supervised and unsupervised
methodology to generate trajectory-level ground truth labels. Next, given the
availability of the trajectory-level ground truth, we introduce a
trajectory-based crime classification framework. Ablation studies are conducted
with various architectures and feature fusion strategies for the representation
of the human trajectories. The conducted experiments demonstrate the
feasibility of the task and pave the path for further research in the field.",0.08256896,0.14873256,-0.34962207,A
8424,"promote further research, we propose a new bench-
                                                       mark based on the Epic-kitchen (Damen et al.,
   SpeciÔ¨Åcally, DARK extracts verb and noun fea-       2018, 2020) dataset, which is an order of mag-
tures separately, and relies on separate verb and      nitude bigger both in number of classes and sample
noun knowledge graphs to predict unseen concepts       size.","To
early with respect to the vocabulary size.","The key contributions of our paper are:
before composing the action label.",2022-07-04 20:19:13+00:00,Disentangled Action Recognition with Knowledge Bases,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Zhekun Luo'), arxiv.Result.Author('Shalini Ghosh'), arxiv.Result.Author('Devin Guillory'), arxiv.Result.Author('Keizo Kato'), arxiv.Result.Author('Trevor Darrell'), arxiv.Result.Author('Huijuan Xu')]","Action in video usually involves the interaction of human with objects.
Action labels are typically composed of various combinations of verbs and
nouns, but we may not have training data for all possible combinations. In this
paper, we aim to improve the generalization ability of the compositional action
recognition model to novel verbs or novel nouns that are unseen during training
time, by leveraging the power of knowledge graphs. Previous work utilizes
verb-noun compositional action nodes in the knowledge graph, making it
inefficient to scale since the number of compositional action nodes grows
quadratically with respect to the number of verbs and nouns. To address this
issue, we propose our approach: Disentangled Action Recognition with
Knowledge-bases (DARK), which leverages the inherent compositionality of
actions. DARK trains a factorized model by first extracting disentangled
feature representations for verbs and nouns, and then predicting classification
weights using relations in external knowledge graphs. The type constraint
between verb and noun is extracted from external knowledge bases and finally
applied when composing actions. DARK has better scalability in the number of
objects and verbs, and achieves state-of-the-art performance on the Charades
dataset. We further propose a new benchmark split based on the Epic-kitchen
dataset which is an order of magnitude bigger in the numbers of classes and
samples, and benchmark various models on this benchmark.",0.14353243,-0.3432844,-0.29105026,C
8448,"We release our code and models to allow for reproducibility and stimulate
   further research in the field.",3.,"Federated Self-supervised Learning for Video Understanding  3

2 Background and Related Work

2.1 Video Self-supervised Representation Learning

Video-SSL approaches often rely on solving a pretext task [8] in an unsupervised
fashion to learn representations that can be reused in solving other downstream
tasks.",2022-07-05 11:39:35+00:00,Federated Self-supervised Learning for Video Understanding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yasar Abbas Ur Rehman'), arxiv.Result.Author('Yan Gao'), arxiv.Result.Author('Jiajun Shen'), arxiv.Result.Author('Pedro Porto Buarque de Gusmao'), arxiv.Result.Author('Nicholas Lane')]","The ubiquity of camera-enabled mobile devices has lead to large amounts of
unlabelled video data being produced at the edge. Although various
self-supervised learning (SSL) methods have been proposed to harvest their
latent spatio-temporal representations for task-specific training, practical
challenges including privacy concerns and communication costs prevent SSL from
being deployed at large scales. To mitigate these issues, we propose the use of
Federated Learning (FL) to the task of video SSL. In this work, we evaluate the
performance of current state-of-the-art (SOTA) video-SSL techniques and
identify their shortcomings when integrated into the large-scale FL setting
simulated with kinetics-400 dataset. We follow by proposing a novel federated
SSL framework for video, dubbed FedVSSL, that integrates different aggregation
strategies and partial weight updating. Extensive experiments demonstrate the
effectiveness and significance of FedVSSL as it outperforms the centralized
SOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on
HMDB-51.",-0.1687153,-0.2762226,-0.09781795,C
8449,"We release our code and models on GitHub 4 to allow for reproducibility
    and stimulate further research in the field.",3.,"4 https://github.com/yasar-rehman/FEDVSSL
Federated Self-supervised Learning for Video Understanding  3

2 Background and Related Work

2.1 Video Self-supervised Representation Learning

Video-SSL approaches often rely on solving a pretext task [8] in an unsupervised
fashion to learn representations that can be reused in solving other downstream
tasks.",2022-07-05 11:39:35+00:00,Federated Self-supervised Learning for Video Understanding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yasar Abbas Ur Rehman'), arxiv.Result.Author('Yan Gao'), arxiv.Result.Author('Jiajun Shen'), arxiv.Result.Author('Pedro Porto Buarque de Gusmao'), arxiv.Result.Author('Nicholas Lane')]","The ubiquity of camera-enabled mobile devices has lead to large amounts of
unlabelled video data being produced at the edge. Although various
self-supervised learning (SSL) methods have been proposed to harvest their
latent spatio-temporal representations for task-specific training, practical
challenges including privacy concerns and communication costs prevent SSL from
being deployed at large scales. To mitigate these issues, we propose the use of
Federated Learning (FL) to the task of video SSL. In this work, we evaluate the
performance of current state-of-the-art (SOTA) video-SSL techniques and
identify their shortcomings when integrated into the large-scale FL setting
simulated with kinetics-400 dataset. We follow by proposing a novel federated
SSL framework for video, dubbed FedVSSL, that integrates different aggregation
strategies and partial weight updating. Extensive experiments demonstrate the
effectiveness and significance of FedVSSL as it outperforms the centralized
SOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on
HMDB-51.",-0.16788027,-0.24188755,-0.08856994,C
8464,"These perturbations are also simulating real-world scenarios and therefore call for further research on how to
improve robustness in this area.","This indicates that
multimodal models are not robust to human perceivable changes such as character changes in word(s) and additive
noise.","Cross-Attention Next, we compare models that use cross-attention between video and text and those that keep video
and text encoders separate.",2022-07-05 16:26:05+00:00,Multi-modal Robustness Analysis Against Language and Visual Perturbations,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Madeline C. Schiappa'), arxiv.Result.Author('Yogesh S. Rawat'), arxiv.Result.Author('Shruti Vyas'), arxiv.Result.Author('Vibhav Vineet'), arxiv.Result.Author('Hamid Palangi')]","Joint visual and language modeling on large-scale datasets has recently shown
a good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of such models against various real-world perturbations focusing on video and
language. We focus on text-to-video retrieval and propose two large-scale
benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual
and 35 different textual perturbations. The study reveals some interesting
findings: 1) The studied models are more robust when text is perturbed versus
when video is perturbed 2) The transformer text encoder is more robust on
non-semantic changing text perturbations and visual perturbations compared to
word embedding approaches. 3) Using two-branch encoders in isolation is
typically more robust than when architectures use cross-attention. We hope this
study will serve as a benchmark and guide future research in robust multimodal
learning.",-0.05734955,-0.19238308,-0.1348751,C
8465,"These perturbations are also simulating real-world scenarios and therefore call for further research on how to
improve robustness in this area.","This indicates that
multimodal models are not robust to human perceivable changes such as character changes in word(s) and additive
noise.","Cross-Attention Next, we compare models that use cross-attention between video and text and those that keep video
and text encoders separate.",2022-07-05 16:26:05+00:00,Multi-modal Robustness Analysis Against Language and Visual Perturbations,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Madeline C. Schiappa'), arxiv.Result.Author('Shruti Vyas'), arxiv.Result.Author('Hamid Palangi'), arxiv.Result.Author('Yogesh S. Rawat'), arxiv.Result.Author('Vibhav Vineet')]","Joint visual and language modeling on large-scale datasets has recently shown
a good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of such models against various real-world perturbations focusing on video and
language. We focus on text-to-video retrieval and propose two large-scale
benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual
and 35 different textual perturbations. The study reveals some interesting
findings: 1) The studied models are more robust when text is perturbed versus
when video is perturbed 2) The transformer text encoder is more robust on
non-semantic changing text perturbations and visual perturbations compared to
word embedding approaches. 3) Using two-branch encoders in isolation is
typically more robust than when architectures use cross-attention. We hope this
study will serve as a benchmark and guide future research in robust multimodal
learning.",-0.057349604,-0.19238321,-0.13487507,C
8466,"A further study is needed to enable PDS Ô¨Ånd the best
parameter settings automatically.","6 Limitations

In general, there are several parameters in the preconditioning matrix of PDS
need to be determined.","Although DDPMs are a variant of SGMs, we
Ô¨Ånd PDS can not directly used on DDPMs, since the diÔ¨Äusion process of DDPMs
is not a Langevin dynamics.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.44821945,0.14531365,0.23054036,A
8467,"Nevertheless, we only study the
eÔ¨Äect of some special cases of the solenoidal terms, which does not mean there
are no solenoidal terms that can accelerate the diÔ¨Äusion process, and the search
for these solenoidal terms is in a further study.","9, these solenoidal terms also
do not make an obvious eÔ¨Äect on acceleration.","Accelerating SGMs with Preconditioned DiÔ¨Äusion Sampling  23

‚Äî‚Äî   T = 66  T = 57  T = 50  T = 44

S6

S5

S4

S3

S2

S1

S=0

Fig.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.3843622,0.058588564,0.08337197,A
8468,"A further study is needed to enable PDS Ô¨Ånd the best
parameter settings automatically.","6 Limitations

In general, there are several parameters in the preconditioning matrix of PDS
need to be determined.","Although DDPMs are a variant of SGMs, we
Ô¨Ånd PDS can not directly used on DDPMs, since the diÔ¨Äusion process of DDPMs
is not a Langevin dynamics.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.44821945,0.14531365,0.23054036,A
8469,"Nevertheless, we only study the
eÔ¨Äect of some special cases of the solenoidal terms, which does not mean there
are no solenoidal terms that can accelerate the diÔ¨Äusion process, and the search
for these solenoidal terms is in a further study.","9, these solenoidal terms also
do not make an obvious eÔ¨Äect on acceleration.","24   H. Ma, L. Zhang, et al.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.35934392,0.051373914,-0.03600359,A
8470,"A further study is needed to enable PDS Ô¨Ånd the best
parameter settings automatically.","6 Limitations

In general, there are several parameters in the preconditioning matrix of PDS
need to be determined.","Although DDPMs are a variant of SGMs, we
Ô¨Ånd PDS can not directly used on DDPMs, since the diÔ¨Äusion process of DDPMs
is not a Langevin dynamics.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.44821945,0.14531365,0.23054036,A
8471,"Nevertheless, we only study
the eÔ¨Äect of some special cases of the solenoidal terms, which does not mean
there are no solenoidal terms that can accelerate the diÔ¨Äusion process, and the
search for these solenoidal terms is in a further study.","10, these solenoidal terms
also do not make an obvious eÔ¨Äect on acceleration.","24   H. Ma, L. Zhang, et al.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.36040196,0.053118292,-0.040162284,A
8472,"A further study is needed to enable PDS Ô¨Ånd the best
parameter settings automatically.","6 Limitations

In general, there are several parameters in the preconditioning matrix of PDS
need to be determined.","Although DDPMs are a variant of SGMs, we
Ô¨Ånd PDS can not directly used on DDPMs, since the diÔ¨Äusion process of DDPMs
is not a Langevin dynamics.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.44821945,0.14531365,0.23054036,A
8473,"Nevertheless, we only study
the eÔ¨Äect of some special cases of the solenoidal terms, which does not mean
there are no solenoidal terms that can accelerate the diÔ¨Äusion process, and the
search for these solenoidal terms is in a further study.","10, these solenoidal terms
also do not make an obvious eÔ¨Äect on acceleration.","24   H. Ma, L. Zhang, et al.",2022-07-05 17:55:42+00:00,Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hengyuan Ma'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Xiatian Zhu'), arxiv.Result.Author('Jianfeng Feng')]","Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.",0.36040196,0.053118292,-0.040162284,A
8476,"We will release this additional labeled dataset to
                                                                              facilitate further research.","We label 12
                                                                              sequences of KITTI-road, where 6 sequences (2,905 frames)
         1                             1‚àíxi(c)                if c = yi(c) ,  are used for training and 6 sequences (2,889 frames) for
Lls = |C| ‚àÜJc (m(c)), mi(c) =          xi(c)                  otherwise       validation.","c‚ààC                                                  (6)
                                                                                 Because of the unequal distribution of dynamic and static
where |C| is the class number, ‚àÜJc represents the Lova¬¥sz                     training samples, as also indicated in [41], we omit frames
                                                                              from continuous static frames to speed up the training, i.e.,
extension of the Jaccard index, xi(c) ‚àà [0, 1] and yi(c) ‚àà                    using a smaller downsampled dataset for faster experiments.",2022-07-05 17:59:17+00:00,Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Jiadai Sun'), arxiv.Result.Author('Yuchao Dai'), arxiv.Result.Author('Xianjing Zhang'), arxiv.Result.Author('Jintao Xu'), arxiv.Result.Author('Rui Ai'), arxiv.Result.Author('Weihao Gu'), arxiv.Result.Author('Xieyuanli Chen')]","Accurate moving object segmentation is an essential task for autonomous
driving. It can provide effective information for many downstream tasks, such
as collision avoidance, path planning, and static map construction. How to
effectively exploit the spatial-temporal information is a critical question for
3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a
novel deep neural network exploiting both spatial-temporal information and
different representation modalities of LiDAR scans to improve LiDAR-MOS
performance. Specifically, we first use a range image-based dual-branch
structure to separately deal with spatial and temporal information that can be
obtained from sequential LiDAR scans, and later combine them using
motion-guided attention modules. We also use a point refinement module via 3D
sparse convolution to fuse the information from both LiDAR range image and
point cloud representations and reduce the artifacts on the borders of the
objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS
benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods
significantly in terms of LiDAR-MOS IoU. Benefiting from the devised
coarse-to-fine architecture, our method operates online at sensor frame rate.
The implementation of our method is available as open source at:
https://github.com/haomo-ai/MotionSeg3D.",0.089020886,-0.026624793,0.021343628,A
8477,"We will release the curated dataset, together with the re-annotation
codes for further research.","The full list of curated sequences includes: bmx-
trees, horsejump-high, india, kite-surf, lab-coat, mbike-trick, motocross-jump, paragliding-launch,
scooter-black, shooting, soapbox.","E Ablation study

In this section, we present more details on our ablation studies, with a single parameter varied every
time.",2022-07-05 17:59:43+00:00,Segmenting Moving Objects via an Object-Centric Layered Representation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junyu Xie'), arxiv.Result.Author('Weidi Xie'), arxiv.Result.Author('Andrew Zisserman')]","The objective of this paper is a model that is able to discover, track and
segment multiple moving objects in a video. We make four contributions: First,
we introduce an object-centric segmentation model with a depth-ordered layer
representation. This is implemented using a variant of the transformer
architecture that ingests optical flow, where each query vector specifies an
object and its layer for the entire video. The model can effectively discover
multiple moving objects and handle mutual occlusions; Second, we introduce a
scalable pipeline for generating synthetic training data with multiple objects,
significantly reducing the requirements for labour-intensive annotations, and
supporting Sim2Real generalisation; Third, we show that the model is able to
learn object permanence and temporal shape consistency, and is able to predict
amodal segmentation masks; Fourth, we evaluate the model on standard video
segmentation benchmarks, DAVIS, MoCA, SegTrack, FBMS-59, and achieve
state-of-the-art unsupervised segmentation performance, even outperforming
several supervised approaches. With test-time adaptation, we observe further
performance boosts.",0.20913231,0.00918183,-0.30867565,A
8478,"We will release the curated dataset, together with the re-annotation
codes for further research.","The full list of curated sequences includes: bmx-
trees, horsejump-high, india, kite-surf, lab-coat, mbike-trick, motocross-jump, paragliding-launch,
scooter-black, shooting, soapbox.","20
RGB

Flow

DAVIS2017 DAVIS2017
   -motion GT GT

Figure 10: Curated DAVIS2017-motion dataset.",2022-07-05 17:59:43+00:00,Segmenting Moving Objects via an Object-Centric Layered Representation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junyu Xie'), arxiv.Result.Author('Weidi Xie'), arxiv.Result.Author('Andrew Zisserman')]","The objective of this paper is a model that is able to discover, track and
segment multiple moving objects in a video. We make four contributions: First,
we introduce an object-centric segmentation model with a depth-ordered layer
representation. This is implemented using a variant of the transformer
architecture that ingests optical flow, where each query vector specifies an
object and its layer for the entire video. The model can effectively discover
multiple moving objects and handle mutual occlusions; Second, we introduce a
scalable pipeline for generating multi-object synthetic training data via layer
compositions, that is used to train the proposed model, significantly reducing
the requirements for labour-intensive annotations, and supporting Sim2Real
generalisation; Third, we conduct thorough ablation studies, showing that the
model is able to learn object permanence and temporal shape consistency, and is
able to predict amodal segmentation masks; Fourth, we evaluate our model,
trained only on synthetic data, on standard video segmentation benchmarks,
DAVIS, MoCA, SegTrack, FBMS-59, and achieve state-of-the-art performance among
existing methods that do not rely on any manual annotations. With test-time
adaptation, we observe further performance boosts.",-0.12298741,-0.05536076,-0.20087919,B
8506,"In the future, we will further research the impact of multi-branch ar-
chitecture on the FSM.","Extensive experiments and rigorous
ablation studies demonstrate the eÔ¨Äectiveness of the proposed FSM for channel
pruning.","Also, the combination with other pruning methods is
worth trying.",2022-07-06 12:50:26+00:00,Network Pruning via Feature Shift Minimization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuanzhi Duan'), arxiv.Result.Author('Yue Zhou'), arxiv.Result.Author('Peng He'), arxiv.Result.Author('Qiang Liu'), arxiv.Result.Author('Shukai Duan'), arxiv.Result.Author('Xiaofang Hu')]","Channel pruning is widely used to reduce the complexity of deep network
models. Recent pruning methods usually identify which parts of the network to
discard by proposing a channel importance criterion. However, recent studies
have shown that these criteria do not work well in all conditions. In this
paper, we propose a novel Feature Shift Minimization (FSM) method to compress
CNN models, which evaluates the feature shift by converging the information of
both features and filters. Specifically, we first investigate the compression
efficiency with some prevalent methods in different layer-depths and then
propose the feature shift concept. Then, we introduce an approximation method
to estimate the magnitude of the feature shift, since it is difficult to
compute it directly. Besides, we present a distribution-optimization algorithm
to compensate for the accuracy loss and improve the network compression
efficiency. The proposed method yields state-of-the-art performance on various
benchmark networks and datasets, verified by extensive experiments. Our codes
are available at: https://github.com/lscgx/FSM.",0.4133124,0.17886433,0.17406751,A
8556,"Though stabilizing the model will
be a challenge, there is potential for producing greater results and merits further research.","This can be done through more stable generative
model variants such as DCGAN [32], which excel at image synthesis.","Although significant progress has been achieved for image generation with GANs, the challenge
remains of accomplishing greater efficiency in aligning input text with a generated image.",2022-07-06 13:43:56+00:00,Text to Image Synthesis using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Haileleol Tibebu'), arxiv.Result.Author('Aadin Malik'), arxiv.Result.Author('Varuna De Silva')]","Synthesizing a realistic image from textual description is a major challenge
in computer vision. Current text to image synthesis approaches falls short of
producing a highresolution image that represent a text descriptor. Most
existing studies rely either on Generative Adversarial Networks (GANs) or
Variational Auto Encoders (VAEs). GANs has the capability to produce sharper
images but lacks the diversity of outputs, whereas VAEs are good at producing a
diverse range of outputs, but the images generated are often blurred. Taking
into account the relative advantages of both GANs and VAEs, we proposed a new
stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture
for synthesizing images conditioned on a text description. This study uses
Conditional VAEs as an initial generator to produce a high-level sketch of the
text descriptor. This high-level sketch output from first stage and a text
descriptor is used as an input to the conditional GAN network. The second stage
GAN produces a 256x256 high resolution image. The proposed architecture
benefits from a conditioning augmentation and a residual block on the
Conditional GAN network to achieve the results. Multiple experiments were
conducted using CUB and Oxford-102 dataset and the result of the proposed
approach is compared against state-ofthe-art techniques such as StackGAN. The
experiments illustrate that the proposed method generates a high-resolution
image conditioned on text descriptions and yield competitive results based on
Inception and Frechet Inception Score using both datasets",-0.07898921,-0.09830506,0.20637012,C
8557,"Though stabilizing the model will
be a challenge, there is potential for producing greater results and merits further research.","This can be done through more stable generative
model variants such as DCGAN [32], which excel at image synthesis.","Although significant progress has been achieved for image generation with GANs, the challenge
remains of accomplishing greater efficiency in aligning input text with a generated image.",2022-07-06 13:43:56+00:00,Text to Image Synthesis using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Haileleol Tibebu'), arxiv.Result.Author('Aadil Malik'), arxiv.Result.Author('Varuna De Silva')]","Synthesizing a realistic image from textual description is a major challenge
in computer vision. Current text to image synthesis approaches falls short of
producing a highresolution image that represent a text descriptor. Most
existing studies rely either on Generative Adversarial Networks (GANs) or
Variational Auto Encoders (VAEs). GANs has the capability to produce sharper
images but lacks the diversity of outputs, whereas VAEs are good at producing a
diverse range of outputs, but the images generated are often blurred. Taking
into account the relative advantages of both GANs and VAEs, we proposed a new
stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture
for synthesizing images conditioned on a text description. This study uses
Conditional VAEs as an initial generator to produce a high-level sketch of the
text descriptor. This high-level sketch output from first stage and a text
descriptor is used as an input to the conditional GAN network. The second stage
GAN produces a 256x256 high resolution image. The proposed architecture
benefits from a conditioning augmentation and a residual block on the
Conditional GAN network to achieve the results. Multiple experiments were
conducted using CUB and Oxford-102 dataset and the result of the proposed
approach is compared against state-ofthe-art techniques such as StackGAN. The
experiments illustrate that the proposed method generates a high-resolution
image conditioned on text descriptions and yield competitive results based on
Inception and Frechet Inception Score using both datasets",-0.07898921,-0.09830506,0.20637012,C
8586,"We further study the effect of our loss terms to uniformity and
alignment trade-off in depth in the appendix.","The advantage of modeling dynam-
ical system is that we can study the motion of embeddings and control them
with this model.","3 Method

3.1 BYOL Architecture

We follow the recent BYOL architecture [16] that learns a joint embedding of
an image x ‚àà X with two networks ‚Äì consists of two neural networks referred
to as the online (or fast learner) and target (or slow learner) network.",2022-07-07 19:56:20+00:00,An Embedding-Dynamic Approach to Self-supervised Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Suhong Moon'), arxiv.Result.Author('Domas Buracas'), arxiv.Result.Author('Seunghyun Park'), arxiv.Result.Author('Jinkyu Kim'), arxiv.Result.Author('John Canny')]","A number of recent self-supervised learning methods have shown impressive
performance on image classification and other tasks. A somewhat bewildering
variety of techniques have been used, not always with a clear understanding of
the reasons for their benefits, especially when used in combination. Here we
treat the embeddings of images as point particles and consider model
optimization as a dynamic process on this system of particles. Our dynamic
model combines an attractive force for similar images, a locally dispersive
force to avoid local collapse, and a global dispersive force to achieve a
globally-homogeneous distribution of particles. The dynamic perspective
highlights the advantage of using a delayed-parameter image embedding (a la
BYOL) together with multiple views of the same image. It also uses a
purely-dynamic local dispersive force (Brownian motion) that shows improved
performance over other methods and does not require knowledge of other particle
coordinates. The method is called MSBReg which stands for (i) a Multiview
centroid loss, which applies an attractive force to pull different image view
embeddings toward their centroid, (ii) a Singular value loss, which pushes the
particle system toward spatially homogeneous density, (iii) a Brownian
diffusive loss. We evaluate downstream classification performance of MSBReg on
ImageNet as well as transfer learning tasks including fine-grained
classification, multi-class object classification, object detection, and
instance segmentation. In addition, we also show that applying our
regularization term to other methods further improves their performance and
stabilize the training by preventing a mode collapse.",-0.08058193,-0.14009894,0.16748804,C
8587,"We further study the sensitivity of the tuning of the two new hyperparameters
Œªs and Œªb.","Concretely,
applying all our proposed regularizations together shows the best performance.",We report the result in the supplementary.,2022-07-07 19:56:20+00:00,An Embedding-Dynamic Approach to Self-supervised Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Suhong Moon'), arxiv.Result.Author('Domas Buracas'), arxiv.Result.Author('Seunghyun Park'), arxiv.Result.Author('Jinkyu Kim'), arxiv.Result.Author('John Canny')]","A number of recent self-supervised learning methods have shown impressive
performance on image classification and other tasks. A somewhat bewildering
variety of techniques have been used, not always with a clear understanding of
the reasons for their benefits, especially when used in combination. Here we
treat the embeddings of images as point particles and consider model
optimization as a dynamic process on this system of particles. Our dynamic
model combines an attractive force for similar images, a locally dispersive
force to avoid local collapse, and a global dispersive force to achieve a
globally-homogeneous distribution of particles. The dynamic perspective
highlights the advantage of using a delayed-parameter image embedding (a la
BYOL) together with multiple views of the same image. It also uses a
purely-dynamic local dispersive force (Brownian motion) that shows improved
performance over other methods and does not require knowledge of other particle
coordinates. The method is called MSBReg which stands for (i) a Multiview
centroid loss, which applies an attractive force to pull different image view
embeddings toward their centroid, (ii) a Singular value loss, which pushes the
particle system toward spatially homogeneous density, (iii) a Brownian
diffusive loss. We evaluate downstream classification performance of MSBReg on
ImageNet as well as transfer learning tasks including fine-grained
classification, multi-class object classification, object detection, and
instance segmentation. In addition, we also show that applying our
regularization term to other methods further improves their performance and
stabilize the training by preventing a mode collapse.",0.383789,-0.017074483,0.16074957,A
8597,"In addition, to facilitate further research on acne detection,
                                       we construct a new dataset named AcneSCU, with high-resolution imageries, precise annotations, and
                                       Ô¨Åne-grained lesion categories.","Then, we
                                       propose a Normalized Wasserstein Distance prediction branch to improve the correlation between the
                                       proposals‚Äô classiÔ¨Åcation scores and IoUs.","Extensive experiments are conducted on both AcneSCU and the public
                                       dataset ACNE04, and the results demonstrate the proposed method could improve the proposals‚Äô qual-
                                       ity, consistently outperforming state-of-the-art approaches.",2022-07-08 03:28:29+00:00,Learning High-quality Proposals for Acne Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Zhang'), arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Junyou Wang'), arxiv.Result.Author('Xin Wei'), arxiv.Result.Author('Jiaqi Li'), arxiv.Result.Author('Xian Jiang'), arxiv.Result.Author('Dan Du')]","Acne detection is crucial for interpretative diagnosis and precise treatment
of skin disease. The arbitrary boundary and small size of acne lesions lead to
a significant number of poor-quality proposals in two-stage detection. In this
paper, we propose a novel head structure for Region Proposal Network to improve
the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH)
structure is proposed to disentangle the representation learning for
classification and localization from two different spatial perspectives. The
proposed SADH ensures a steeper classification confidence gradient and
suppresses the proposals having low intersection-over-union(IoU) with the
matched ground truth. Then, we propose a Normalized Wasserstein Distance
prediction branch to improve the correlation between the proposals'
classification scores and IoUs. In addition, to facilitate further research on
acne detection, we construct a new dataset named AcneSCU, with high-resolution
imageries, precise annotations, and fine-grained lesion categories. Extensive
experiments are conducted on both AcneSCU and the public dataset ACNE04, and
the results demonstrate the proposed method could improve the proposals'
quality, consistently outperforming state-of-the-art approaches. Code and the
collected dataset are available in
https://github.com/pingguokiller/acnedetection.",-0.06335771,-0.12006077,0.047040593,C
8598,"In addition, to comprehen-     'FBUVSF.BQ
sively evaluate the proposed method and facilitate                          'FBUVSF.BQ ¬ÄDPOW
further research on acne detection, a new dataset
named AcneSCU, containing 276 images and 31777                                                              'JOBM'FBUVSF.BQ
annotations with 10 lesions categories, is collected.","Then, the NWDs are uti-
lized as localization conÔ¨Ådence to rectify the origi-    '1/         *OUFSNFEJBUF                                         CPY
nal classiÔ¨Åcation scores.","Experimental results on both AcneSCU and pub-                               (a)origin RPN Head
lic dataset ACNE04 show that the proposed method
outperforms state-of-the-art methods.",2022-07-08 03:28:29+00:00,Learning High-quality Proposals for Acne Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Zhang'), arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Junyou Wang'), arxiv.Result.Author('Xin Wei'), arxiv.Result.Author('Jiaqi Li'), arxiv.Result.Author('Xian Jiang'), arxiv.Result.Author('Dan Du')]","Acne detection is crucial for interpretative diagnosis and precise treatment
of skin disease. The arbitrary boundary and small size of acne lesions lead to
a significant number of poor-quality proposals in two-stage detection. In this
paper, we propose a novel head structure for Region Proposal Network to improve
the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH)
structure is proposed to disentangle the representation learning for
classification and localization from two different spatial perspectives. The
proposed SADH ensures a steeper classification confidence gradient and
suppresses the proposals having low intersection-over-union(IoU) with the
matched ground truth. Then, we propose a Normalized Wasserstein Distance
prediction branch to improve the correlation between the proposals'
classification scores and IoUs. In addition, to facilitate further research on
acne detection, we construct a new dataset named AcneSCU, with high-resolution
imageries, precise annotations, and fine-grained lesion categories. Extensive
experiments are conducted on both AcneSCU and the public dataset ACNE04, and
the results demonstrate the proposed method could improve the proposals'
quality, consistently outperforming state-of-the-art approaches. Code and the
collected dataset are available in
https://github.com/pingguokiller/acnedetection.",0.0346078,-0.06319772,0.0046487832,C
8599,"are mo-       and facilitate further research on acne detection, we
tivated by the fact fc-head has more spatial sensi-        construct a new dataset called AcneSCU, which has
tivity than conv-head, while the proposed SADH is          higher-resolution and more normalized imageries,
designed to improve the steepness of the classiÔ¨Åca-        more Ô¨Åne-grained acne categories, and more precise
tion conÔ¨Ådence gradient.",Wu et al.,annotations than previous acne dataset 2.,2022-07-08 03:28:29+00:00,Learning High-quality Proposals for Acne Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Zhang'), arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Junyou Wang'), arxiv.Result.Author('Xin Wei'), arxiv.Result.Author('Jiaqi Li'), arxiv.Result.Author('Xian Jiang'), arxiv.Result.Author('Dan Du')]","Acne detection is crucial for interpretative diagnosis and precise treatment
of skin disease. The arbitrary boundary and small size of acne lesions lead to
a significant number of poor-quality proposals in two-stage detection. In this
paper, we propose a novel head structure for Region Proposal Network to improve
the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH)
structure is proposed to disentangle the representation learning for
classification and localization from two different spatial perspectives. The
proposed SADH ensures a steeper classification confidence gradient and
suppresses the proposals having low intersection-over-union(IoU) with the
matched ground truth. Then, we propose a Normalized Wasserstein Distance
prediction branch to improve the correlation between the proposals'
classification scores and IoUs. In addition, to facilitate further research on
acne detection, we construct a new dataset named AcneSCU, with high-resolution
imageries, precise annotations, and fine-grained lesion categories. Extensive
experiments are conducted on both AcneSCU and the public dataset ACNE04, and
the results demonstrate the proposed method could improve the proposals'
quality, consistently outperforming state-of-the-art approaches. Code and the
collected dataset are available in
https://github.com/pingguokiller/acnedetection.",-0.047983088,-0.041485697,0.08570731,C
8600,"We argue a steeper classiÔ¨Åcation        lesion categories, to facilitate further research on
conÔ¨Ådence gradient could generate more independent       acne detection.","6, it could be found that SADH          dataset named AcneSCU, with high-resolution im-
had a steeper classiÔ¨Åcation conÔ¨Ådence gradient than      ageries, more precise annotations, and Ô¨Åne-grained
the vanilla RPN.","Extensive experiments on both Ac-
classiÔ¨Åcation scores at adjacent positions on the Ô¨Ånal   neSCU and the public dataset ACNE04 demonstrate
feature map, suppress the proposals with low IoUs        the eÔ¨Éciency of the proposed method.",2022-07-08 03:28:29+00:00,Learning High-quality Proposals for Acne Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Zhang'), arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Junyou Wang'), arxiv.Result.Author('Xin Wei'), arxiv.Result.Author('Jiaqi Li'), arxiv.Result.Author('Xian Jiang'), arxiv.Result.Author('Dan Du')]","Acne detection is crucial for interpretative diagnosis and precise treatment
of skin disease. The arbitrary boundary and small size of acne lesions lead to
a significant number of poor-quality proposals in two-stage detection. In this
paper, we propose a novel head structure for Region Proposal Network to improve
the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH)
structure is proposed to disentangle the representation learning for
classification and localization from two different spatial perspectives. The
proposed SADH ensures a steeper classification confidence gradient and
suppresses the proposals having low intersection-over-union(IoU) with the
matched ground truth. Then, we propose a Normalized Wasserstein Distance
prediction branch to improve the correlation between the proposals'
classification scores and IoUs. In addition, to facilitate further research on
acne detection, we construct a new dataset named AcneSCU, with high-resolution
imageries, precise annotations, and fine-grained lesion categories. Extensive
experiments are conducted on both AcneSCU and the public dataset ACNE04, and
the results demonstrate the proposed method could improve the proposals'
quality, consistently outperforming state-of-the-art approaches. Code and the
collected dataset are available in
https://github.com/pingguokiller/acnedetection.",-0.057646893,-0.1312631,0.0578324,C
8618,"We hope that
                                                                                 this work inspires further research to tackle more complex warp models.","To the best of
                                                                                 our knowledge, regularizers based on the proposed metrics are the only effective solution against
                                                                                 event collapse in the experimental settings considered, compared with other methods.","Keywords: computer vision; intelligent sensors; robotics; event-based camera; contrast maximization;
                                                                                 optical Ô¨Çow; motion estimation

                                                                            1.",2022-07-08 16:52:35+00:00,Event Collapse in Contrast Maximization Frameworks,cs.CV,"['cs.CV', 'cs.RO', 'math.DG']","[arxiv.Result.Author('Shintaro Shiba'), arxiv.Result.Author('Yoshimitsu Aoki'), arxiv.Result.Author('Guillermo Gallego')]","Context maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.",-0.10725257,0.29591185,-0.043857932,B
8619,"We hope that
                                                                                  this work inspires further research to tackle more complex warp models.","To the best of
                                                                                  our knowledge, regularizers based on the proposed metrics are the only effective solution against
                                                                                  event collapse in the experimental settings considered, compared with other methods.","Keywords: computer vision; intelligent sensors; robotics; event-based camera; contrast maximization;
                                                                                  optical Ô¨Çow; motion estimation

                                                                             1.",2022-07-08 16:52:35+00:00,Event Collapse in Contrast Maximization Frameworks,cs.CV,"['cs.CV', 'cs.RO', 'math.DG']","[arxiv.Result.Author('Shintaro Shiba'), arxiv.Result.Author('Yoshimitsu Aoki'), arxiv.Result.Author('Guillermo Gallego')]","Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.",-0.10725257,0.29591185,-0.043857932,B
8637,"this superficial conjecture, giving a more comprehensive and rigorous analysis is
worth further research.","When more groups are used, the
strength of the target style is increased and the details are better preserved.","To sum up, our ED solver has demonstrated the superior batch efficiency
for small matrices in various real-world experiments and numerical tests.",2022-07-09 09:14:12+00:00,Batch-efficient EigenDecomposition for Small and Medium Matrices,cs.CV,"['cs.CV', 'cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Yue Song'), arxiv.Result.Author('Nicu Sebe'), arxiv.Result.Author('Wei Wang')]","EigenDecomposition (ED) is at the heart of many computer vision algorithms
and applications. One crucial bottleneck limiting its usage is the expensive
computation cost, particularly for a mini-batch of matrices in the deep neural
networks. In this paper, we propose a QR-based ED method dedicated to the
application scenarios of computer vision. Our proposed method performs the ED
entirely by batched matrix/vector multiplication, which processes all the
matrices simultaneously and thus fully utilizes the power of GPUs. Our
technique is based on the explicit QR iterations by Givens rotation with double
Wilkinson shifts. With several acceleration techniques, the time complexity of
QR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test
shows that for small and medium batched matrices (\emph{e.g.,} $dim{<}32$) our
method can be much faster than the Pytorch SVD function. Experimental results
on visual recognition and image generation demonstrate that our methods also
achieve competitive performances.",0.35336077,0.028645605,0.17720087,A
8664,Thus further research is needed on the topic.,"However, much of the early work is hard to reproduce due to data-leakage prob-
lems [3].","Contrastive learning has been recently shown to be a powerful technique
to learn semantics-preserving visual representations in a self-supervised manner
[4,5].",2022-07-11 01:17:35+00:00,Brain-Aware Replacements for Supervised Contrastive Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Mehmet Saygƒ±n Seyfioƒülu'), arxiv.Result.Author('Zixuan Liu'), arxiv.Result.Author('Pranav Kamath'), arxiv.Result.Author('Sadjyot Gangolli'), arxiv.Result.Author('Sheng Wang'), arxiv.Result.Author('Thomas Grabowski'), arxiv.Result.Author('Linda Shapiro')]","We propose a novel framework for Alzheimer's disease (AD) detection using
brain MRIs. The framework starts with a data augmentation method called
Brain-Aware Replacements (BAR), which leverages a standard brain parcellation
to replace medically-relevant 3D brain regions in an anchor MRI from a randomly
picked MRI to create synthetic samples. Ground truth ""hard"" labels are also
linearly mixed depending on the replacement ratio in order to create ""soft""
labels. BAR produces a great variety of realistic-looking synthetic MRIs with
higher local variability compared to other mix-based methods, such as CutMix.
On top of BAR, we propose using a soft-label-capable supervised contrastive
loss, aiming to learn the relative similarity of representations that reflect
how mixed are the synthetic MRIs using our soft labels. This way, we do not
fully exhaust the entropic capacity of our hard labels, since we only use them
to create soft labels and synthetic MRIs through BAR. We show that a model
pre-trained using our framework can be further fine-tuned with a cross-entropy
loss using the hard labels that were used to create the synthetic samples. We
validated the performance of our framework in a binary AD detection task
against both from-scratch supervised training and state-of-the-art
self-supervised training plus fine-tuning approaches. Then we evaluated BAR's
individual performance compared to another mix-based method CutMix by
integrating it within our framework. We show that our framework yields superior
results in both precision and recall for the AD detection task.",-0.12785535,-0.3336283,-0.07248156,C
8665,Thus further research is needed on the topic.,"However, much of the early work is hard to reproduce due to data-leakage prob-
lems [3].","Contrastive learning has been recently shown to be a powerful technique
to learn semantics-preserving visual representations in a self-supervised manner
[4,5].",2022-07-11 01:17:35+00:00,Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Mehmet Saygƒ±n Seyfioƒülu'), arxiv.Result.Author('Zixuan Liu'), arxiv.Result.Author('Pranav Kamath'), arxiv.Result.Author('Sadjyot Gangolli'), arxiv.Result.Author('Sheng Wang'), arxiv.Result.Author('Thomas Grabowski'), arxiv.Result.Author('Linda Shapiro')]","We propose a novel framework for Alzheimer's disease (AD) detection using
brain MRIs. The framework starts with a data augmentation method called
Brain-Aware Replacements (BAR), which leverages a standard brain parcellation
to replace medically-relevant 3D brain regions in an anchor MRI from a randomly
picked MRI to create synthetic samples. Ground truth ""hard"" labels are also
linearly mixed depending on the replacement ratio in order to create ""soft""
labels. BAR produces a great variety of realistic-looking synthetic MRIs with
higher local variability compared to other mix-based methods, such as CutMix.
On top of BAR, we propose using a soft-label-capable supervised contrastive
loss, aiming to learn the relative similarity of representations that reflect
how mixed are the synthetic MRIs using our soft labels. This way, we do not
fully exhaust the entropic capacity of our hard labels, since we only use them
to create soft labels and synthetic MRIs through BAR. We show that a model
pre-trained using our framework can be further fine-tuned with a cross-entropy
loss using the hard labels that were used to create the synthetic samples. We
validated the performance of our framework in a binary AD detection task
against both from-scratch supervised training and state-of-the-art
self-supervised training plus fine-tuning approaches. Then we evaluated BAR's
individual performance compared to another mix-based method CutMix by
integrating it within our framework. We show that our framework yields superior
results in both precision and recall for the AD detection task.",-0.12785535,-0.3336283,-0.07248156,C
8667,"Finally, discard-
4.6 Ablation Study                                                        ing both PC and BF, we observe a more considerable margin of
                                                                          reduction on mR@50 and DP@K, demonstrating the strengths of
To deeply investigate our FGPL and FGPL-A, we further study               both PC and BF.","1.                                                                   suppression to tail classes, which leads an efficient discriminating
                                                                          process among classes with different frequencies.","different ablation variants of CDL/CDL-A, EDL/EDL-A, and PL-              Batch-Refinement (BR) regime in PL-A: To prove that our
A under the PredCls task on the VG-SGG dataset.",2022-07-11 03:37:57+00:00,Adaptive Fine-Grained Predicates Learning for Scene Graph Generation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xinyu Lyu'), arxiv.Result.Author('Lianli Gao'), arxiv.Result.Author('Pengpeng Zeng'), arxiv.Result.Author('Heng Tao Shen'), arxiv.Result.Author('Jingkuan Song')]","The performance of current Scene Graph Generation (SGG) models is severely
hampered by hard-to-distinguish predicates, e.g., woman-on/standing on/walking
on-beach. As general SGG models tend to predict head predicates and
re-balancing strategies prefer tail categories, none of them can appropriately
handle hard-to-distinguish predicates. To tackle this issue, inspired by
fine-grained image classification, which focuses on differentiating
hard-to-distinguish objects, we propose an Adaptive Fine-Grained Predicates
Learning (FGPL-A) which aims at differentiating hard-to-distinguish predicates
for SGG. First, we introduce an Adaptive Predicate Lattice (PL-A) to figure out
hard-to-distinguish predicates, which adaptively explores predicate
correlations in keeping with model's dynamic learning pace. Practically, PL-A
is initialized from SGG dataset, and gets refined by exploring model's
predictions of current mini-batch. Utilizing PL-A, we propose an Adaptive
Category Discriminating Loss (CDL-A) and an Adaptive Entity Discriminating Loss
(EDL-A), which progressively regularize model's discriminating process with
fine-grained supervision concerning model's dynamic learning status, ensuring
balanced and efficient learning process. Extensive experimental results show
that our proposed model-agnostic strategy significantly boosts performance of
benchmark models on VG-SGG and GQA-SGG datasets by up to 175% and 76% on Mean
Recall@100, achieving new state-of-the-art performance. Moreover, experiments
on Sentence-to-Graph Retrieval and Image Captioning tasks further demonstrate
practicability of our method.",0.3798008,-0.047468636,0.036362786,A
8699,"We open-source our                     ICCV, 2017.
approach to facilitate further research in this area.","If the poses in an               [17] R. Zhu, H. Kiani Galoogahi, et al., ‚ÄúRethinking reprojection: Closing
environment are known to be constrained, such constraints can                     the loop for pose-aware shape reconstruction from a single image,‚Äù in
easily be incorporated into our framework.","[18] F. Engelmann, K. Rematas, et al., ‚ÄúFrom points to multi-object 3d
   Our modular architecture naturally lends itself to various                     reconstruction,‚Äù in CVPR, 2021.
extensions and modiÔ¨Åcations.",2022-07-11 13:53:50+00:00,SDFEst: Categorical Pose and Shape Estimation of Objects from RGB-D using Signed Distance Fields,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Leonard Bruns'), arxiv.Result.Author('Patric Jensfelt')]","Rich geometric understanding of the world is an important component of many
robotic applications such as planning and manipulation. In this paper, we
present a modular pipeline for pose and shape estimation of objects from RGB-D
images given their category. The core of our method is a generative shape
model, which we integrate with a novel initialization network and a
differentiable renderer to enable 6D pose and shape estimation from a single or
multiple views. We investigate the use of discretized signed distance fields as
an efficient shape representation for fast analysis-by-synthesis optimization.
Our modular framework enables multi-view optimization and extensibility. We
demonstrate the benefits of our approach over state-of-the-art methods in
several experiments on both synthetic and real data. We open-source our
approach at https://github.com/roym899/sdfest.",-0.2600395,0.22975394,-0.028536133,B
8722,"Therefore, we con-
ducted an ablation study experiment to further study the performance of PLD-PIRL
under diÔ¨Äerent parameter settings.","4.4 Ablation study

Our experiments indicate that the settings of Œª and temperature œÑ parameters in the
proposed PLD-PIRL approach will aÔ¨Äect the model performance.","We set œÑ = {0.2, 0.4, 0.6} and Œª = {0.1, 0.25, 0.5, 1.0}.",2022-07-11 21:06:29+00:00,Patch-level instance-group discrimination with pretext-invariant learning for colitis scoring,cs.CV,"['cs.CV', 'cs.AI', 'cs.MM']","[arxiv.Result.Author('Ziang Xu'), arxiv.Result.Author('Sharib Ali'), arxiv.Result.Author('Soumya Gupta'), arxiv.Result.Author('Simon Leedham'), arxiv.Result.Author('James E East'), arxiv.Result.Author('Jens Rittscher')]","Inflammatory bowel disease (IBD), in particular ulcerative colitis (UC), is
graded by endoscopists and this assessment is the basis for risk stratification
and therapy monitoring. Presently, endoscopic characterisation is largely
operator dependant leading to sometimes undesirable clinical outcomes for
patients with IBD. We focus on the Mayo Endoscopic Scoring (MES) system which
is widely used but requires the reliable identification of subtle changes in
mucosal inflammation. Most existing deep learning classification methods cannot
detect these fine-grained changes which make UC grading such a challenging
task. In this work, we introduce a novel patch-level instance-group
discrimination with pretext-invariant representation learning (PLD-PIRL) for
self-supervised learning (SSL). Our experiments demonstrate both improved
accuracy and robustness compared to the baseline supervised network and several
state-of-the-art SSL methods. Compared to the baseline (ResNet50) supervised
classification our proposed PLD-PIRL obtained an improvement of 4.75% on
hold-out test data and 6.64% on unseen center test data for top-1 accuracy.",0.5447024,0.2460193,-0.041030265,A
8740,"We hope the method and the datasets presented in this
paper will inspire further research.","The eÔ¨Äectiveness of the
method is encouraging.","7 Appendix

7.1 Experiment Details

The synthesized part of the dataset contains multiple-pose and multiple-view
images.",2022-07-12 08:21:35+00:00,Collaborative Neural Rendering using Anime Character Sheets,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zuzeng Lin'), arxiv.Result.Author('Ailin Huang'), arxiv.Result.Author('Zhewei Huang'), arxiv.Result.Author('Chen Hu'), arxiv.Result.Author('Shuchang Zhou')]","Drawing images of characters at desired poses is an essential but laborious
task in anime production. In this paper, we present the Collaborative Neural
Rendering~(CoNR) method to create new images from a few arbitrarily posed
reference images available in character sheets. In general, the high diversity
of body shapes of anime characters defies the employment of universal body
models for real-world humans, like SMPL. To overcome this difficulty, CoNR uses
a compact and easy-to-obtain landmark encoding to avoid creating a unified UV
mapping in the pipeline. In addition, CoNR's performance can be significantly
increased when having multiple reference images by using feature space
cross-view dense correspondence and warping in a specially designed neural
network construct. Moreover, we collect a character sheet dataset containing
over 700,000 hand-drawn and synthesized images of diverse poses to facilitate
research in this area.",-0.22637059,0.2707581,-0.09119962,B
8741,"We hope the method and the datasets presented in this
                                                                paper will inspire further research.","The effectiveness of the method is encour-
                                                                aging.","References                              Gao, C.; Shih, Y.; Lai, W.-S.; Liang, C.-K.; and Huang, J.-B.",2022-07-12 08:21:35+00:00,Collaborative Neural Rendering using Anime Character Sheets,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zuzeng Lin'), arxiv.Result.Author('Ailin Huang'), arxiv.Result.Author('Zhewei Huang'), arxiv.Result.Author('Chen Hu'), arxiv.Result.Author('Shuchang Zhou')]","Drawing images of characters with desired poses is an essential but laborious
task in anime production. In this paper, we present the Collaborative Neural
Rendering (CoNR) method to create new images from a few reference images with
various poses, namely Character Sheets. In general, the high diversity of body
shapes of anime characters defies the employment of universal body models for
real-world humans, like SMPL. To overcome this difficulty, CoNR uses a compact
and easy-to-obtain landmark encoding to avoid creating a unified UV mapping in
the pipeline. In addition, the performance of CoNR can be significantly
improved when referring to multiple reference images and using feature space
cross-view warping in a carefully designed neural network. Moreover, we
collected a character sheet dataset containing over 700,000 hand-drawn and
synthesized images of diverse poses to facilitate research in this area. Our
code and demo are available at https://github.com/megvii-research/CoNR.",0.41503465,0.10398972,-0.122336164,A
8742,"We hope the method and the datasets presented in this
                                                                paper will inspire our community and further researchers.","To bypass the UDP Detector,         potential of this data-driven method in assisting animation
we can rely on additional technologies like garment cap-        art.","References

                                                                Alldieck, T.; Pons-Moll, G.; Theobalt, C.; and Magnor, M.
                                                                2019.",2022-07-12 08:21:35+00:00,Collaborative Neural Rendering using Anime Character Sheets,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zuzeng Lin'), arxiv.Result.Author('Ailin Huang'), arxiv.Result.Author('Zhewei Huang'), arxiv.Result.Author('Chen Hu'), arxiv.Result.Author('Shuchang Zhou')]","Drawing images of characters with desired poses is an essential but laborious
task in anime production. In this paper, we present the Collaborative Neural
Rendering (CoNR) method, which creates new images for specified poses from a
few reference images (AKA Character Sheets). In general, the high diversity of
body shapes of anime characters defies the employment of universal body models
like SMPL, which are developed from real-world humans. To overcome this
difficulty, CoNR uses a compact and easy-to-obtain landmark encoding to avoid
creating a unified UV mapping in the pipeline. In addition, the performance of
CoNR can be significantly improved when referring to multiple reference images,
thanks to feature space cross-view warping in a carefully designed neural
network. Moreover, we have collected a character sheet dataset containing over
700,000 hand-drawn and synthesized images of diverse poses to facilitate
research in this area. Our code and demo are available at
https://github.com/megvii-research/CoNR.",-0.10809983,0.20368972,-0.2072148,B
8743,"The different behavior of different approaches to fin-
gerprint recognition under varying image quality, motivates                                                                   [9] J. Bigun, G. H. Granlund, and J. Wiklund, ""Multidimen-
us to conduct further research focused on multi-algorithm                                                                          sional orientation estimation with applications to texture
fingerprint verification schemes adapted to the image qual-                                                                        analysis and optical flow,"" IEEE Trans.","160-170, 2005.","PAMI, vol.",2022-07-12 10:28:36+00:00,On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Julian Fierrez-Aguilar'), arxiv.Result.Author('Luis-Miguel Mu√±oz-Serrano'), arxiv.Result.Author('Fernando Alonso-Fernandez'), arxiv.Result.Author('Javier Ortega-Garcia')]","The effect of image quality degradation on the verification performance of
automatic fingerprint recognition is investigated. We study the performance of
two fingerprint matchers based on minutiae and ridge information under varying
fingerprint image quality. The ridge-based system is found to be more robust to
image quality degradation than the minutiae-based system for a number of
different image quality criteria.",0.0108162835,0.15005048,0.038958352,C
8745,"the       We further study the impact of different normalization lay-
number of channels in E-MHSA module, will reduce the                   ers and activation functions in Next-ViT.","As stated in Table 8, decreasing the shrinking ratio r, i.e.","As shown in Ta-
model latency.",2022-07-12 12:50:34+00:00,Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiashi Li'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Huixia Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Xin Pan')]","Due to the complex attention mechanisms and model design, most existing
vision Transformers (ViTs) can not perform as efficiently as convolutional
neural networks (CNNs) in realistic industrial deployment scenarios, e.g.
TensorRT and CoreML. This poses a distinct challenge: Can a visual neural
network be designed to infer as fast as CNNs and perform as powerful as ViTs?
Recent works have tried to design CNN-Transformer hybrid architectures to
address this issue, yet the overall performance of these works is far away from
satisfactory. To end these, we propose a next generation vision Transformer for
efficient deployment in realistic industrial scenarios, namely Next-ViT, which
dominates both CNNs and ViTs from the perspective of latency/accuracy
trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer
Block (NTB) are respectively developed to capture local and global information
with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is
designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts
performance in various downstream tasks. Extensive experiments show that
Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer
hybrid architectures with respect to the latency/accuracy trade-off across
various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from
40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K
segmentation under similar latency. Meanwhile, it achieves comparable
performance with CSWin, while the inference speed is accelerated by 3.6x. On
CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on
COCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under
similar latency. Code will be released recently.",0.46637243,0.030037306,0.24257576,A
8746,"the       We further study the impact of different normalization lay-
number of channels in E-MHSA module, will reduce the                   ers and activation functions in Next-ViT.","As stated in Table 8, decreasing the shrinking ratio r, i.e.","As shown in Ta-
model latency.",2022-07-12 12:50:34+00:00,Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiashi Li'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Huixia Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Xin Pan')]","Due to the complex attention mechanisms and model design, most existing
vision Transformers (ViTs) can not perform as efficiently as convolutional
neural networks (CNNs) in realistic industrial deployment scenarios, e.g.
TensorRT and CoreML. This poses a distinct challenge: Can a visual neural
network be designed to infer as fast as CNNs and perform as powerful as ViTs?
Recent works have tried to design CNN-Transformer hybrid architectures to
address this issue, yet the overall performance of these works is far away from
satisfactory. To end these, we propose a next generation vision Transformer for
efficient deployment in realistic industrial scenarios, namely Next-ViT, which
dominates both CNNs and ViTs from the perspective of latency/accuracy
trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer
Block (NTB) are respectively developed to capture local and global information
with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is
designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts
performance in various downstream tasks. Extensive experiments show that
Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer
hybrid architectures with respect to the latency/accuracy trade-off across
various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from
40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K
segmentation under similar latency. Meanwhile, it achieves comparable
performance with CSWin, while the inference speed is accelerated by 3.6x. On
CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on
COCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under
similar latency. Code will be released recently.",0.46637243,0.030037306,0.24257576,A
8747,"We further study the impact of different normalization lay-
                                                                       ers and activation functions in Next-ViT.","For a fair comparison, we consistently use NTB           4.4.3 Impact of Normalization and Activation
and NHS to build different models under similar latency on
TensorRT.","As shown in Ta-
   As shown in Table 7, NCB achieves the best la-                      ble 9, both the LN and GELU bring negligible performance
tency/accuracy trade-off on all of the three tasks, which              improvement but with signiÔ¨Åcantly higher inference latency
veriÔ¨Åes the advantage of the proposed NCB.",2022-07-12 12:50:34+00:00,Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiashi Li'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Huixia Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Xin Pan')]","Due to the complex attention mechanisms and model design, most existing
vision Transformers (ViTs) can not perform as efficiently as convolutional
neural networks (CNNs) in realistic industrial deployment scenarios, e.g.
TensorRT and CoreML. This poses a distinct challenge: Can a visual neural
network be designed to infer as fast as CNNs and perform as powerful as ViTs?
Recent works have tried to design CNN-Transformer hybrid architectures to
address this issue, yet the overall performance of these works is far away from
satisfactory. To end these, we propose a next generation vision Transformer for
efficient deployment in realistic industrial scenarios, namely Next-ViT, which
dominates both CNNs and ViTs from the perspective of latency/accuracy
trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer
Block (NTB) are respectively developed to capture local and global information
with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is
designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts
performance in various downstream tasks. Extensive experiments show that
Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer
hybrid architectures with respect to the latency/accuracy trade-off across
various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from
40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K
segmentation under similar latency. Meanwhile, it achieves comparable
performance with CSWin, while the inference speed is accelerated by 3.6x. On
CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on
COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under
similar latency. Our code and models are made public at:
https://github.com/bytedance/Next-ViT",0.24142365,-0.13749745,0.20280428,A
8748,"the
number of channels in E-MHSA module, will reduce the                  We further study the impact of different normalization lay-
model latency.","4.4.3 Impact of Normalization and Activation
As stated in Table 8, decreasing the shrinking ratio r, i.e.","Furthermore, model with r = 0.75 and r =               ers and activation functions in Next-ViT.",2022-07-12 12:50:34+00:00,Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiashi Li'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Huixia Li'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Xin Pan')]","Due to the complex attention mechanisms and model design, most existing
vision Transformers (ViTs) can not perform as efficiently as convolutional
neural networks (CNNs) in realistic industrial deployment scenarios, e.g.
TensorRT and CoreML. This poses a distinct challenge: Can a visual neural
network be designed to infer as fast as CNNs and perform as powerful as ViTs?
Recent works have tried to design CNN-Transformer hybrid architectures to
address this issue, yet the overall performance of these works is far away from
satisfactory. To end these, we propose a next generation vision Transformer for
efficient deployment in realistic industrial scenarios, namely Next-ViT, which
dominates both CNNs and ViTs from the perspective of latency/accuracy
trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer
Block (NTB) are respectively developed to capture local and global information
with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is
designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts
performance in various downstream tasks. Extensive experiments show that
Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer
hybrid architectures with respect to the latency/accuracy trade-off across
various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from
40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K
segmentation under similar latency. Meanwhile, it achieves comparable
performance with CSWin, while the inference speed is accelerated by 3.6x. On
CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on
COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under
similar latency. Our code and models are made public at:
https://github.com/bytedance/Next-ViT",0.45935184,0.07267816,0.24605475,A
8749,"In short, for a given target task, our OTCE-
V. Besides, we further study the performance of our OTCE-                based Ô¨Ånetune algorithm is a simple but effective approach to
based Ô¨Ånetune algorithm using different model architectures              improve the transferability of the source model and achieve
including the famous LeNet [37] for character recognition                more accurate classiÔ¨Åcation results in the end.","9 also visually demonstrates that our OTCE-based
we Ô¨Årst evaluate on the widely-used model architecture, de-              Ô¨Ånetune method indeed improves both the transferability score
noted as Conv4 [30]‚Äì[33], which contains four convolutional              and the transfer accuracy of the source model for most target
layers and one fully connected layer as described in Table               tasks as expected.","(MNIST‚ÜíOmniglot) and the ResNet-18 [28] for natural im-
age classiÔ¨Åcation (Caltech101‚ÜíMiniImageNet).",2022-07-12 13:06:16+00:00,Transferability-Guided Cross-Domain Cross-Task Transfer Learning,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yang Tan'), arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Shao-Lun Huang'), arxiv.Result.Author('Xiao-Ping Zhang')]","We propose two novel transferability metrics F-OTCE (Fast Optimal Transport
based Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate
how much the source model (task) can benefit the learning of the target task
and to learn more transferable representations for cross-domain cross-task
transfer learning. Unlike the existing metric that requires evaluating the
empirical transferability on auxiliary tasks, our metrics are auxiliary-free
such that they can be computed much more efficiently. Specifically, F-OTCE
estimates transferability by first solving an Optimal Transport (OT) problem
between source and target distributions, and then uses the optimal coupling to
compute the Negative Conditional Entropy between source and target labels. It
can also serve as a loss function to maximize the transferability of the source
model before finetuning on the target task. Meanwhile, JC-OTCE improves the
transferability robustness of F-OTCE by including label distances in the OT
problem, though it may incur additional computation cost. Extensive experiments
demonstrate that F-OTCE and JC-OTCE outperform state-of-the-art auxiliary-free
metrics by 18.85% and 28.88%, respectively in correlation coefficient with the
ground-truth transfer accuracy. By eliminating the training cost of auxiliary
tasks, the two metrics reduces the total computation time of the previous
method from 43 minutes to 9.32s and 10.78s, respectively, for a pair of tasks.
When used as a loss function, F-OTCE shows consistent improvements on the
transfer accuracy of the source model in few-shot classification experiments,
with up to 4.41% accuracy gain.",-0.020190768,-0.2625237,0.15517746,C
8777,"To facilitate further research
well as there exists not much pixel-level difference of water       in this Ô¨Åeld, we have created a benchmark dataset for two
areas between dam reservoirs and natural water bodies.","I, a multi-class segmentation network can not work          reservoirs in previous studies.","geographically distinct regions, the Volta basin in western
                                                                    Africa and the Cauvery basin in India.",2022-07-12 19:46:01+00:00,Dam reservoir extraction from remote sensing imagery using tailored metric learning strategies,cs.CV,['cs.CV'],"[arxiv.Result.Author('Arnout van Soesbergen'), arxiv.Result.Author('Zedong Chu'), arxiv.Result.Author('Miaojing Shi'), arxiv.Result.Author('Mark Mulligan')]","Dam reservoirs play an important role in meeting sustainable development
goals and global climate targets. However, particularly for small dam
reservoirs, there is a lack of consistent data on their geographical location.
To address this data gap, a promising approach is to perform automated dam
reservoir extraction based on globally available remote sensing imagery. It can
be considered as a fine-grained task of water body extraction, which involves
extracting water areas in images and then separating dam reservoirs from
natural water bodies. We propose a novel deep neural network (DNN) based
pipeline that decomposes dam reservoir extraction into water body segmentation
and dam reservoir recognition. Water bodies are firstly separated from
background lands in a segmentation model and each individual water body is then
predicted as either dam reservoir or natural water body in a classification
model. For the former step, point-level metric learning with triplets across
images is injected into the segmentation model to address contour ambiguities
between water areas and land regions. For the latter step, prior-guided metric
learning with triplets from clusters is injected into the classification model
to optimize the image embedding space in a fine-grained level based on
reservoir clusters. To facilitate future research, we establish a benchmark
dataset with earth imagery data and human labelled reservoirs from river basins
in West Africa and India. Extensive experiments were conducted on this
benchmark in the water body segmentation task, dam reservoir recognition task,
and the joint dam reservoir extraction task. Superior performance has been
observed in the respective tasks when comparing our method with state of the
art approaches.",-0.07491735,0.074440874,0.088306785,B
8787,"4.7 Backbones

We further study the generalizability of our proposed method by incorporating
it with multiple backbone architectures.","That demonstrates that our proposed method remarkably
balances accuracy and time on a larger-scale dataset.","These backbones include VGG-19 [31],
ResNet-18 [13], ResNet-50 [13] and ResNet-101 [13].",2022-07-13 02:44:05+00:00,Rapid Person Re-Identification via Sub-space Consistency Regularization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qingze Yin'), arxiv.Result.Author('Guanan Wang'), arxiv.Result.Author('Guodong Ding'), arxiv.Result.Author('Qilei Li'), arxiv.Result.Author('Shaogang Gong'), arxiv.Result.Author('Zhenmin Tang')]","Person Re-Identification (ReID) matches pedestrians across disjoint cameras.
Existing ReID methods adopting real-value feature descriptors have achieved
high accuracy, but they are low in efficiency due to the slow Euclidean
distance computation as well as complex quick-sort algorithms. Recently, some
works propose to yield binary encoded person descriptors which instead only
require fast Hamming distance computation and simple counting-sort algorithms.
However, the performances of such binary encoded descriptors, especially with
short code (e.g., 32 and 64 bits), are hardly satisfactory given the sparse
binary space. To strike a balance between the model accuracy and efficiency, we
propose a novel Sub-space Consistency Regularization (SCR) algorithm that can
speed up the ReID procedure by $0.25$ times than real-value features under the
same dimensions whilst maintaining a competitive accuracy, especially under
short codes. SCR transforms real-value features vector (e.g., 2048 float32)
with short binary codes (e.g., 64 bits) by first dividing real-value features
vector into $M$ sub-spaces, each with $C$ clustered centroids. Thus the
distance between two samples can be expressed as the summation of the
respective distance to the centroids, which can be sped up by offline
calculation and maintained via a look-up table. On the other side, these
real-value centroids help to achieve significantly higher accuracy than using
binary code. Lastly, we convert the distance look-up table to be integer and
apply the counting-sort algorithm to speed up the ranking stage.
  We also propose a novel consistency regularization with an iterative
framework. Experimental results on Market-1501 and DukeMTMC-reID show promising
and exciting results. Under short code, our proposed SCR enjoys
Real-value-level accuracy and Hashing-level speed.",0.039723326,-0.039759945,0.21260047,A
8795,"Their research is dedicated to the initial work of collating and building a library of the character forms
of the League Book, which will provide a good foundation for further research on the Houma League Book.","[11] and its copy provide a paper version of
the text template.",3.,2022-07-13 06:56:21+00:00,A new database of Houma Alliance Book ancient handwritten characters and its baseline algorithm,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xiaoyu Yuan'), arxiv.Result.Author('Zhibo Zhang'), arxiv.Result.Author('Yabo Sun'), arxiv.Result.Author('Zekai Xue'), arxiv.Result.Author('Xiuyan Shao'), arxiv.Result.Author('Xiaohua Huang')]","The Houma Alliance Book is one of the national treasures of the Museum in
Shanxi Museum Town in China. It has great historical significance in
researching ancient history. To date, the research on the Houma Alliance Book
has been staying in the identification of paper documents, which is inefficient
to identify and difficult to display, study and publicize. Therefore, the
digitization of the recognized ancient characters of Houma League can
effectively improve the efficiency of recognizing ancient characters and
provide more reliable technical support and text data. This paper proposes a
new database of Houma Alliance Book ancient handwritten characters and a
multi-modal fusion method to recognize ancient handwritten characters. In the
database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten
characters are collected from the original book collection and by human
imitative writing. Furthermore, the decision-level classifier fusion strategy
is applied to fuse three well-known deep neural network architectures for
ancient handwritten character recognition. Experiments are performed on our new
database. The experimental results first provide the baseline result of the new
database to the research community and then demonstrate the efficiency of our
proposed method.",0.31400234,-0.0075135725,-0.32929337,A
8796,"Their research is dedicated to the initial work of collating and building a library of the character forms
of the League Book, which will provide a good foundation for further research on the Houma League Book.","[11] and its copy provide a paper version of
the text template.",3.,2022-07-13 06:56:21+00:00,A new database of Houma Alliance Book ancient handwritten characters and its baseline algorithm,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xiaoyu Yuan'), arxiv.Result.Author('Zhibo Zhang'), arxiv.Result.Author('Yabo Sun'), arxiv.Result.Author('Zekai Xue'), arxiv.Result.Author('Xiuyan Shao'), arxiv.Result.Author('Xiaohua Huang')]","The Houma Alliance Book is one of the national treasures of the Museum in
Shanxi Museum Town in China. It has great historical significance in
researching ancient history. To date, the research on the Houma Alliance Book
has been staying in the identification of paper documents, which is inefficient
to identify and difficult to display, study and publicize. Therefore, the
digitization of the recognized ancient characters of Houma League can
effectively improve the efficiency of recognizing ancient characters and
provide more reliable technical support and text data. This paper proposes a
new database of Houma Alliance Book ancient handwritten characters and a
multi-modal fusion method to recognize ancient handwritten characters. In the
database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten
characters are collected from the original book collection and by human
imitative writing. Furthermore, the decision-level classifier fusion strategy
is applied to fuse three well-known deep neural network architectures for
ancient handwritten character recognition. Experiments are performed on our new
database. The experimental results first provide the baseline result of the new
database to the research community and then demonstrate the efficiency of our
proposed method.",0.31400234,-0.0075135725,-0.32929337,A
8797,"Their research is dedicated to the initial work of collating and building a library of the character forms of the League Book,
which will provide a good foundation for further research on the Houma League Book.",[11] and its copy provide a paper version of the text template.,3.,2022-07-13 06:56:21+00:00,A new database of Houma Alliance Book ancient handwritten characters and classifier fusion approach,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xiaoyu Yuan'), arxiv.Result.Author('Zhibo Zhang'), arxiv.Result.Author('Yabo Sun'), arxiv.Result.Author('Zekai Xue'), arxiv.Result.Author('Xiuyan Shao'), arxiv.Result.Author('Xiaohua Huang')]","The Houma Alliance Book is one of the national treasures of the Museum in
Shanxi Museum Town in China. It has great historical significance in
researching ancient history. To date, the research on the Houma Alliance Book
has been staying in the identification of paper documents, which is inefficient
to identify and difficult to display, study and publicize. Therefore, the
digitization of the recognized ancient characters of Houma League can
effectively improve the efficiency of recognizing ancient characters and
provide more reliable technical support and text data. This paper proposes a
new database of Houma Alliance Book ancient handwritten characters and a
multi-modal fusion method to recognize ancient handwritten characters. In the
database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten
characters are collected from the original book collection and by human
imitative writing. Furthermore, the decision-level classifier fusion strategy
is applied to fuse three well-known deep neural network architectures for
ancient handwritten character recognition. Experiments are performed on our new
database. The experimental results first provide the baseline result of the new
database to the research community and then demonstrate the efficiency of our
proposed method.",0.31400234,-0.0075135725,-0.32929337,A
8822,"In the future, we will further study depth                        [22] K.-J.",": PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (FEBRUARY 2017)           9

on LiDAR data.","Yoon and I.-S. Kweon, ‚ÄúLocally adaptive support-weight approach
                                                                                       for visual correspondence search,‚Äù in 2005 IEEE Computer Society
estimation methods that are not sensitive to LiDAR data.",2022-07-13 11:55:15+00:00,Robust and accurate depth estimation by fusing LiDAR and Stereo,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Guangyao Xu'), arxiv.Result.Author('Junfeng Fan'), arxiv.Result.Author('En Li'), arxiv.Result.Author('Xiaoyu Long'), arxiv.Result.Author('Rui Guo')]","Depth estimation is one of the key technologies in some fields such as
autonomous driving and robot navigation. However, the traditional method of
using a single sensor is inevitably limited by the performance of the sensor.
Therefore, a precision and robust method for fusing the LiDAR and stereo
cameras is proposed. This method fully combines the advantages of the LiDAR and
stereo camera, which can retain the advantages of the high precision of the
LiDAR and the high resolution of images respectively. Compared with the
traditional stereo matching method, the texture of the object and lighting
conditions have less influence on the algorithm. Firstly, the depth of the
LiDAR data is converted to the disparity of the stereo camera. Because the
density of the LiDAR data is relatively sparse on the y-axis, the converted
disparity map is up-sampled using the interpolation method. Secondly, in order
to make full use of the precise disparity map, the disparity map and stereo
matching are fused to propagate the accurate disparity. Finally, the disparity
map is converted to the depth map. Moreover, the converted disparity map can
also increase the speed of the algorithm. We evaluate the proposed pipeline on
the KITTI benchmark. The experiment demonstrates that our algorithm has higher
accuracy than several classic methods.",-0.11365904,0.29544303,-0.045231644,B
8834,"By doing so,
both publications demonstrate an improved performance and a higher robustness, while also claiming
that further research in dealing with human noise is still inevitable.","Datasets like CIFAR-10H [54] and CIFAR-10N [77] address the problem of realistic noise by
providing multiple annotations per image for example of the original CIFAR-10 dataset.","The utilization of soft label
distributions instead of hard one-hot label encodings enables the detailed representation of subjective
disagreement and improves the generalization with ambiguous datasets [54, 5, 34].",2022-07-13 14:17:21+00:00,Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lars Schmarje'), arxiv.Result.Author('Vasco Grossmann'), arxiv.Result.Author('Claudius Zelenka'), arxiv.Result.Author('Sabine Dippel'), arxiv.Result.Author('Rainer Kiko'), arxiv.Result.Author('Mariusz Oszust'), arxiv.Result.Author('Matti Pastell'), arxiv.Result.Author('Jenny Stracke'), arxiv.Result.Author('Anna Valros'), arxiv.Result.Author('Nina Volkmann'), arxiv.Result.Author('Reinahrd Koch')]","High-quality data is necessary for modern machine learning. However, the
acquisition of such data is difficult due to noisy and ambiguous annotations of
humans. The aggregation of such annotations to determine the label of an image
leads to a lower data quality. We propose a data-centric image classification
benchmark with ten real-world datasets and multiple annotations per image to
allow researchers to investigate and quantify the impact of such data quality
issues. With the benchmark we can study the impact of annotation costs and
(semi-)supervised methods on the data quality for image classification by
applying a novel methodology to a range of different algorithms and diverse
datasets. Our benchmark uses a two-phase approach via a data label improvement
method in the first phase and a fixed evaluation model in the second phase.
Thereby, we give a measure for the relation between the input labeling effort
and the performance of (semi-)supervised algorithms to enable a deeper insight
into how labels should be created for effective model training. Across
thousands of experiments, we show that one annotation is not enough and that
the inclusion of multiple annotations allows for a better approximation of the
real underlying class distribution. We identify that hard labels can not
capture the ambiguity of the data and this might lead to the common issue of
overconfident models. Based on the presented datasets, benchmarked methods, and
analysis, we create multiple research opportunities for the future directed at
the improvement of label noise estimation approaches, data annotation schemes,
realistic (semi-)supervised learning, or more reliable image collection.",-0.047302343,-0.20151356,-0.14056125,C
8835,"By doing so,
both publications demonstrate an improved performance and a higher robustness, while also claiming
that further research in dealing with human noise is still inevitable.","Datasets like CIFAR-10H [54] and CIFAR-10N [77] address the problem of realistic noise by
providing multiple annotations per image for example of the original CIFAR-10 dataset.","The utilization of soft label
distributions instead of hard one-hot label encodings enables the detailed representation of subjective
disagreement and improves the generalization with ambiguous datasets [54, 5, 34].",2022-07-13 14:17:21+00:00,Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lars Schmarje'), arxiv.Result.Author('Vasco Grossmann'), arxiv.Result.Author('Claudius Zelenka'), arxiv.Result.Author('Sabine Dippel'), arxiv.Result.Author('Rainer Kiko'), arxiv.Result.Author('Mariusz Oszust'), arxiv.Result.Author('Matti Pastell'), arxiv.Result.Author('Jenny Stracke'), arxiv.Result.Author('Anna Valros'), arxiv.Result.Author('Nina Volkmann'), arxiv.Result.Author('Reinhard Koch')]","High-quality data is necessary for modern machine learning. However, the
acquisition of such data is difficult due to noisy and ambiguous annotations of
humans. The aggregation of such annotations to determine the label of an image
leads to a lower data quality. We propose a data-centric image classification
benchmark with ten real-world datasets and multiple annotations per image to
allow researchers to investigate and quantify the impact of such data quality
issues. With the benchmark we can study the impact of annotation costs and
(semi-)supervised methods on the data quality for image classification by
applying a novel methodology to a range of different algorithms and diverse
datasets. Our benchmark uses a two-phase approach via a data label improvement
method in the first phase and a fixed evaluation model in the second phase.
Thereby, we give a measure for the relation between the input labeling effort
and the performance of (semi-)supervised algorithms to enable a deeper insight
into how labels should be created for effective model training. Across
thousands of experiments, we show that one annotation is not enough and that
the inclusion of multiple annotations allows for a better approximation of the
real underlying class distribution. We identify that hard labels can not
capture the ambiguity of the data and this might lead to the common issue of
overconfident models. Based on the presented datasets, benchmarked methods, and
analysis, we create multiple research opportunities for the future directed at
the improvement of label noise estimation approaches, data annotation schemes,
realistic (semi-)supervised learning, or more reliable image collection.",-0.047302343,-0.20151356,-0.14056125,C
8867,"In the future, we will further study the implications of the       [7] T. Bouwmans et al.","2
several challenges of the CDNet2014 and UCSD datasets.","Decomposition into low-rank plus ad-
architecture of GCNNs in the problem of MOD.",2022-07-13 18:00:12+00:00,Graph CNN for Moving Object Detection in Complex Environments from Unseen Videos,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jhony H. Giraldo'), arxiv.Result.Author('Sajid Javed'), arxiv.Result.Author('Naoufel Werghi'), arxiv.Result.Author('Thierry Bouwmans')]","Moving Object Detection (MOD) is a fundamental step for many computer vision
applications. MOD becomes very challenging when a video sequence captured from
a static or moving camera suffers from the challenges: camouflage, shadow,
dynamic backgrounds, and lighting variations, to name a few. Deep learning
methods have been successfully applied to address MOD with competitive
performance. However, in order to handle the overfitting problem, deep learning
methods require a large amount of labeled data which is a laborious task as
exhaustive annotations are always not available. Moreover, some MOD deep
learning methods show performance degradation in the presence of unseen video
sequences because the testing and training splits of the same sequences are
involved during the network learning process. In this work, we pose the problem
of MOD as a node classification problem using Graph Convolutional Neural
Networks (GCNNs). Our algorithm, dubbed as GraphMOD-Net, encompasses instance
segmentation, background initialization, feature extraction, and graph
construction. GraphMOD-Net is tested on unseen videos and outperforms
state-of-the-art methods in unsupervised, semi-supervised, and supervised
learning in several challenges of the Change Detection 2014 (CDNet2014) and
UCSD background subtraction datasets.",0.061753068,-0.19879293,0.25770766,C
8886,"The outcomes of the contest‚Äôs evaluation demonstrate the necessity of further research
                                                                         to reduce recognition errors, while the computational cost of the algorithms proposed
                                                                         is suÔ¨Éciently low.","The evaluation is based not only on the detection
                                                                         performances but also on the latency and the false positives, making it possible to un-
                                                                         derstand the feasibility of practical interaction tools based on the algorithms proposed.",¬© 2022 Elsevier B.V. All rights reserved.,2022-07-14 07:24:02+00:00,SHREC 2022 Track on Online Detection of Heterogeneous Gestures,cs.CV,"['cs.CV', '68T10', 'I.5.2']","[arxiv.Result.Author('Ariel Caputo'), arxiv.Result.Author('Marco Emporio'), arxiv.Result.Author('Andrea Giachetti'), arxiv.Result.Author('Marco Cristani'), arxiv.Result.Author('Guido Borghi'), arxiv.Result.Author(""Andrea D'Eusanio""), arxiv.Result.Author('Minh-Quan Le'), arxiv.Result.Author('Hai-Dang Nguyen'), arxiv.Result.Author('Minh-Triet Tran'), arxiv.Result.Author('F. Ambellan'), arxiv.Result.Author('M. Hanik'), arxiv.Result.Author('E. Nava-Yazdani'), arxiv.Result.Author('C. von Tycowicz')]","This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.",-0.11118228,0.028753344,-0.18799296,B
8887,"The outcomes of the contest‚Äôs evaluation demonstrate the necessity of further research
                                                                         to reduce recognition errors, while the computational cost of the algorithms proposed
                                                                         is suÔ¨Éciently low.","The evaluation is based not only on the detection
                                                                         performances but also on the latency and the false positives, making it possible to un-
                                                                         derstand the feasibility of practical interaction tools based on the algorithms proposed.",¬© 2022 Elsevier B.V. All rights reserved.,2022-07-14 07:24:02+00:00,SHREC 2022 Track on Online Detection of Heterogeneous Gestures,cs.CV,"['cs.CV', '68T10', 'I.5.2']","[arxiv.Result.Author('Ariel Caputo'), arxiv.Result.Author('Marco Emporio'), arxiv.Result.Author('Andrea Giachetti'), arxiv.Result.Author('Marco Cristani'), arxiv.Result.Author('Guido Borghi'), arxiv.Result.Author(""Andrea D'Eusanio""), arxiv.Result.Author('Minh-Quan Le'), arxiv.Result.Author('Hai-Dang Nguyen'), arxiv.Result.Author('Minh-Triet Tran'), arxiv.Result.Author('F. Ambellan'), arxiv.Result.Author('M. Hanik'), arxiv.Result.Author('E. Nava-Yazdani'), arxiv.Result.Author('C. von Tycowicz')]","This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.",-0.11118228,0.028753344,-0.18799296,B
8889,"5 Conclusion

This work conducts further research on low-/cross-resolution face recognition and proposes a novel Ô¨Åne-tuning strategy
with an octuplet loss function for existing models to boost their robustness against varying image resolutions.","This maximum is also consistent for different batch sizes and indicates that the margin is
independent of the batch size.","Our
contribution involves a combination of four triplet loss terms applied simultaneously to high- and low-resolution images.",2022-07-14 08:22:58+00:00,Octuplet Loss: Make Face Recognition Robust to Image Resolution,cs.CV,['cs.CV'],"[arxiv.Result.Author('Martin Knoche'), arxiv.Result.Author('Mohamed Elkadeem'), arxiv.Result.Author('Stefan H√∂rmann'), arxiv.Result.Author('Gerhard Rigoll')]","Image resolution, or in general, image quality, plays an essential role in
the performance of today's face recognition systems. To address this problem,
we propose a novel combination of the popular triplet loss to improve
robustness against image resolution via fine-tuning of existing face
recognition models. With octuplet loss, we leverage the relationship between
high-resolution images and their synthetically down-sampled variants jointly
with their identity labels. Fine-tuning several state-of-the-art approaches
with our method proves that we can significantly boost performance for
cross-resolution (high-to-low resolution) face verification on various datasets
without meaningfully exacerbating the performance on high-to-high resolution
images. Our method applied on the FaceTransformer network achieves 95.12% face
verification accuracy on the challenging XQLFW dataset while reaching 99.73% on
the LFW database. Moreover, the low-to-low face verification accuracy benefits
from our method. We release our code to allow seamless integration of the
octuplet loss into existing frameworks.",-0.0240726,0.022722188,0.19935742,C
8913,"We further study several variants that only a subset of the
masked patches are fed into the momentum encoder and the prediction loss is
only evaluated on this subset of masked patches.","The validation is conducted with A100 GPU and batch size 256 per
GPU for all models.","As the masking ratio is 75%, we
study three fractions: 75% (all the masked patches), 50% (sampled from masked
patches), 25% (also sampled from masked patches).",2022-07-14 17:59:58+00:00,Bootstrapped Masked Autoencoders for Vision BERT Pretraining,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Ting Zhang'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Lu Yuan'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Nenghai Yu')]","We propose bootstrapped masked autoencoders (BootMAE), a new approach for
vision BERT pretraining. BootMAE improves the original masked autoencoders
(MAE) with two core designs: 1) momentum encoder that provides online feature
as extra BERT prediction targets; 2) target-aware decoder that tries to reduce
the pressure on the encoder to memorize target-specific information in BERT
pretraining. The first design is motivated by the observation that using a
pretrained MAE to extract the features as the BERT prediction target for masked
tokens can achieve better pretraining performance. Therefore, we add a momentum
encoder in parallel with the original MAE encoder, which bootstraps the
pretraining performance by using its own representation as the BERT prediction
target. In the second design, we introduce target-specific information (e.g.,
pixel values of unmasked patches) from the encoder directly to the decoder to
reduce the pressure on the encoder of memorizing the target-specific
information. Thus, the encoder focuses on semantic modeling, which is the goal
of BERT pretraining, and does not need to waste its capacity in memorizing the
information of unmasked tokens related to the prediction target. Through
extensive experiments, our BootMAE achieves $84.2\%$ Top-1 accuracy on
ImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\%$ under the same
pre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic
segmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object
detection and segmentation on COCO dataset. Code is released at
https://github.com/LightDXY/BootMAE.",0.114648685,0.046631817,0.2962671,A
8959,"In addition, Ô¨Åguring out how
to improve the efÔ¨Åciency of MSSA remains a great challenge
and requires further research in the future work.","2249-2258, May 2016.
issues in other research Ô¨Åelds.","SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING                                13

(a)  (b)                                                                               Tian, and H. Wu, ‚ÄúERNIE: Enhanced Representation through Knowledge
                                                                                       Integration,‚Äù 2019, arXiv:1904.09223.",2022-07-16 01:33:13+00:00,Masked Spatial-Spectral Autoencoders Are Excellent Hyperspectral Defenders,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jiahao Qi'), arxiv.Result.Author('Zhiqiang Gong'), arxiv.Result.Author('Xingyue Liu'), arxiv.Result.Author('Kangcheng Bin'), arxiv.Result.Author('Chen Chen'), arxiv.Result.Author('Yongqian Li'), arxiv.Result.Author('Wei Xue'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Ping Zhong')]","Deep learning methodology contributes a lot to the development of
hyperspectral image (HSI) analysis community. However, it also makes HSI
analysis systems vulnerable to adversarial attacks. To this end, we propose a
masked spatial-spectral autoencoder (MSSA) in this paper under self-supervised
learning theory, for enhancing the robustness of HSI analysis systems. First, a
masked sequence attention learning module is conducted to promote the inherent
robustness of HSI analysis systems along spectral channel. Then, we develop a
graph convolutional network with learnable graph structure to establish global
pixel-wise combinations.In this way, the attack effect would be dispersed by
all the related pixels among each combination, and a better defense performance
is achievable in spatial aspect.Finally, to improve the defense transferability
and address the problem of limited labelled samples, MSSA employs spectra
reconstruction as a pretext task and fits the datasets in a self-supervised
manner.Comprehensive experiments over three benchmarks verify the effectiveness
of MSSA in comparison with the state-of-the-art hyperspectral classification
methods and representative adversarial defense strategies.",0.006810376,0.07249265,0.10186718,C
9003,"Given the practical nature of class-iNCD and encouraging results
with our FRoST, we believe this work will stimulate further research.","We compared our
method to many relevant works and obtained superior performance on various
benchmarks.",Acknowledgements.,2022-07-18 13:49:27+00:00,Class-incremental Novel Class Discovery,cs.CV,['cs.CV'],"[arxiv.Result.Author('Subhankar Roy'), arxiv.Result.Author('Mingxuan Liu'), arxiv.Result.Author('Zhun Zhong'), arxiv.Result.Author('Nicu Sebe'), arxiv.Result.Author('Elisa Ricci')]","We study the new task of class-incremental Novel Class Discovery
(class-iNCD), which refers to the problem of discovering novel categories in an
unlabelled data set by leveraging a pre-trained model that has been trained on
a labelled data set containing disjoint yet related categories. Apart from
discovering novel classes, we also aim at preserving the ability of the model
to recognize previously seen base categories. Inspired by rehearsal-based
incremental learning methods, in this paper we propose a novel approach for
class-iNCD which prevents forgetting of past information about the base classes
by jointly exploiting base class feature prototypes and feature-level knowledge
distillation. We also propose a self-training clustering strategy that
simultaneously clusters novel categories and trains a joint classifier for both
the base and novel classes. This makes our method able to operate in a
class-incremental setting. Our experiments, conducted on three common
benchmarks, demonstrate that our method significantly outperforms
state-of-the-art approaches. Code is available at
https://github.com/OatmealLiu/class-iNCD",0.30459058,0.05192945,0.013836848,A
9028,"Source codes and model weights
are offered along with the paper for full reproducibility and to facilitate further research in this area.","In addition, we
presented in-painting and noise reduction capabilities of the proposed model.","References

[1] Next    Generation   IdentiÔ¨Åcation           (NGI).",2022-07-18 23:23:23+00:00,DeformIrisNet: An Identity-Preserving Model of Iris Texture Deformation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siamul Karim Khan'), arxiv.Result.Author('Patrick Tinsley'), arxiv.Result.Author('Adam Czajka')]","Nonlinear iris texture deformations due to pupil size variations are one of
the main factors responsible for within-class variance of genuine comparison
scores in iris recognition. In dominant approaches to iris recognition, the
size of a ring-shaped iris region is linearly scaled to a canonical rectangle,
used further in encoding and matching. However, the biological complexity of
iris sphincter and dilator muscles causes the movements of iris features to be
nonlinear in a function of pupil size, and not solely organized along radial
paths. Alternatively to the existing theoretical models based on biomechanics
of iris musculature, in this paper we propose a novel deep autoencoder-based
model that can effectively learn complex movements of iris texture features
directly from the data. The proposed model takes two inputs, (a) an
ISO-compliant near-infrared iris image with initial pupil size, and (b) the
binary mask defining the target shape of the iris. The model makes all the
necessary nonlinear deformations to the iris texture to match the shape of iris
in image (a) with the shape provided by the target mask (b). The
identity-preservation component of the loss function helps the model in finding
deformations that preserve identity and not only visual realism of generated
samples. We also demonstrate two immediate applications of this model: better
compensation for iris texture deformations in iris recognition algorithms,
compared to linear models, and creation of generative algorithm that can aid
human forensic examiners, who may need to compare iris images with large
difference in pupil dilation. We offer the source codes and model weights
available along with this paper.",0.2434149,0.045272823,0.16486877,A
9030,"Finally,
Section 5 concludes with insights for further research and development.","Section 3 presents the proposed design model
pipeline, Section 4 shows and examines critically the obtained results.","2 Related work

A large body of works has emerged in the literature exploiting the highly per-
forming abilities of generative models such as GANs.",2022-07-18 23:53:55+00:00,"Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation",cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Tomas Cabezon Pedroso'), arxiv.Result.Author('Javier Del Ser'), arxiv.Result.Author('Natalia Diaz-Rodrƒ±guez')]","Rendering programs have changed the design process completely as they permit
to see how the products will look before they are fabricated. However, the
rendering process is complicated and takes a significant amount of time, not
only in the rendering itself but in the setting of the scene as well.
Materials, lights and cameras need to be set in order to get the best quality
results. Nevertheless, the optimal output may not be obtained in the first
render. This all makes the rendering process a tedious process. Since
Goodfellow et al. introduced Generative Adversarial Networks (GANs) in 2014
[1], they have been used to generate computer-assigned synthetic data, from
non-existing human faces to medical data analysis or image style transfer. GANs
have been used to transfer image textures from one domain to another. However,
paired data from both domains was needed. When Zhu et al. introduced the
CycleGAN model, the elimination of this expensive constraint permitted
transforming one image from one domain into another, without the need for
paired data. This work validates the applicability of CycleGANs on style
transfer from an initial sketch to a final render in 2D that represents a 3D
design, a step that is paramount in every product design process. We inquiry
the possibilities of including CycleGANs as part of the design pipeline, more
precisely, applied to the rendering of ring designs. Our contribution entails a
crucial part of the process as it allows the customer to see the final product
before buying. This work sets a basis for future research, showing the
possibilities of GANs in design and establishing a starting point for novel
applications to approach crafts design.",0.11139338,-0.1861999,0.092909016,C
9039,"[13] J. Wurst et al., ‚ÄúAn entropy based outlier score and its application
                                                                                       to novelty detection for road infrastructure images,‚Äù in 2020 IEEE
   Including further objects can be one possible direction                             Intelligent Vehicles Symposium (IV), 2020.
for further research on the proposed method.","analysis of scenarios as well as the detection of representative
and novel scenarios.","Also, the
performance when using real-world data can be analyzed in                        [14] F. Hauer et al., ‚ÄúClustering trafÔ¨Åc scenarios using mental models as
a next step.",2022-07-19 08:20:05+00:00,Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jonas Wurst'), arxiv.Result.Author('Lakshman Balasubramanian'), arxiv.Result.Author('Michael Botsch'), arxiv.Result.Author('Wolfgang Utschick')]","Clustering traffic scenarios and detecting novel scenario types are required
for scenario-based testing of autonomous vehicles. These tasks benefit from
either good similarity measures or good representations for the traffic
scenarios. In this work, an expert-knowledge aided representation learning for
traffic scenarios is presented. The latent space so formed is used for
successful clustering and novel scenario type detection. Expert-knowledge is
used to define objectives that the latent representations of traffic scenarios
shall fulfill. It is presented, how the network architecture and loss is
designed from these objectives, thereby incorporating expert-knowledge. An
automatic mining strategy for traffic scenarios is presented, such that no
manual labeling is required. Results show the performance advantage compared to
baseline methods. Additionally, extensive analysis of the latent space is
performed.",-0.15499105,0.039144453,-0.1899853,B
9040,"[13] J. Wurst et al., ‚ÄúAn entropy based outlier score and its application
                                                                                       to novelty detection for road infrastructure images,‚Äù in 2020 IEEE
   Including further objects can be one possible direction                             Intelligent Vehicles Symposium (IV), 2020.
for further research on the proposed method.","analysis of scenarios as well as the detection of representative
and novel scenarios.","Also, the
performance when using real-world data can be analyzed in                        [14] F. Hauer et al., ‚ÄúClustering trafÔ¨Åc scenarios using mental models as
a next step.",2022-07-19 08:20:05+00:00,Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jonas Wurst'), arxiv.Result.Author('Lakshman Balasubramanian'), arxiv.Result.Author('Michael Botsch'), arxiv.Result.Author('Wolfgang Utschick')]","Clustering traffic scenarios and detecting novel scenario types are required
for scenario-based testing of autonomous vehicles. These tasks benefit from
either good similarity measures or good representations for the traffic
scenarios. In this work, an expert-knowledge aided representation learning for
traffic scenarios is presented. The latent space so formed is used for
successful clustering and novel scenario type detection. Expert-knowledge is
used to define objectives that the latent representations of traffic scenarios
shall fulfill. It is presented, how the network architecture and loss is
designed from these objectives, thereby incorporating expert-knowledge. An
automatic mining strategy for traffic scenarios is presented, such that no
manual labeling is required. Results show the performance advantage compared to
baseline methods. Additionally, extensive analysis of the latent space is
performed.",-0.15499105,0.039144453,-0.1899853,B
9051,"This concern is
                                        lays a foundation for further researches.","Our study reveals the               whether Ô¨Åne-tuning CLIP would hurt its zero-shot learning
                                        particular challenges of CLIP continual learning problem and            and/or image-text matching capabilities.","Moreover, we propose          related to the catastrophic forgetting problem that has been
                                        a new algorithm, dubbed Learning without Forgetting via Re-             intensively studied in continual learning with the context of
                                        played Vocabulary (VR-LwF), which shows exact effectiveness             image classiÔ¨Åcation.",2022-07-19 13:03:14+00:00,Don't Stop Learning: Towards Continual Learning for the CLIP Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuxuan Ding'), arxiv.Result.Author('Lingqiao Liu'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Jingyuan Yang'), arxiv.Result.Author('Haoxuan Ding')]","The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.",-0.035802983,-0.23297548,-0.032759186,C
9052,"For example, the candidate class names are Ô¨Ålled into a       further research, where looses the penalty only on the last old
textual prompt like ‚Äúthis is a photo of [Class Name].‚Äù, then       task.","Online-EWC [26] is a
ing.","MAS [27] and SI [28] estimate the neuron importance
the prompt text is encoded to match the visual feature.",2022-07-19 13:03:14+00:00,Don't Stop Learning: Towards Continual Learning for the CLIP Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuxuan Ding'), arxiv.Result.Author('Lingqiao Liu'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Jingyuan Yang'), arxiv.Result.Author('Haoxuan Ding')]","The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.",0.114716105,-0.28059483,-0.22864598,C
9053,"Unfortunately, VR-LwF is still around 15% lower than the
Joint-FT. A further study with more focus on increasing the           Method      UT-Acc  ZS-Acc  TR@1   IR@1   A-Acc
performance of updated classes is therefore suggested.",REPLAYING NUMBERS.,"25.36   62.62  69.34  49.67  43.99
                                                                  Original CLIP    73.19   61.67  67.64  47.28  67.43
      e) RKR keeps the original performance best, but learns                       72.82   62.03  68.34  47.82  67.43
least: Surprisingly, the RKR method was found to get the best         Ks =50       73.09   61.86  67.76  47.98  67.47
zero-shot performance.",2022-07-19 13:03:14+00:00,Don't Stop Learning: Towards Continual Learning for the CLIP Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuxuan Ding'), arxiv.Result.Author('Lingqiao Liu'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Jingyuan Yang'), arxiv.Result.Author('Haoxuan Ding')]","The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.",0.19039664,0.11765216,0.10184662,A
9054,"This concern is
                                        lays a foundation for further researches.","Our study reveals the               whether Ô¨Åne-tuning CLIP would hurt its zero-shot learning
                                        particular challenges of CLIP continual learning problem and            and/or image-text matching capabilities.","Moreover, we propose          related to the catastrophic forgetting problem that has been
                                        a new algorithm, dubbed Learning without Forgetting via Re-             intensively studied in continual learning with the context of
                                        played Vocabulary (VR-LwF), which shows exact effectiveness             image classiÔ¨Åcation.",2022-07-19 13:03:14+00:00,Don't Stop Learning: Towards Continual Learning for the CLIP Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuxuan Ding'), arxiv.Result.Author('Lingqiao Liu'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Jingyuan Yang'), arxiv.Result.Author('Haoxuan Ding')]","The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.",-0.035802983,-0.23297548,-0.032759186,C
9055,"For example, the candidate class names are Ô¨Ålled into a       further research, where looses the penalty only on the last old
textual prompt like ‚Äúthis is a photo of [Class Name].‚Äù, then       task.","Online-EWC [26] is a
ing.","MAS [27] and SI [28] estimate the neuron importance
the prompt text is encoded to match the visual feature.",2022-07-19 13:03:14+00:00,Don't Stop Learning: Towards Continual Learning for the CLIP Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuxuan Ding'), arxiv.Result.Author('Lingqiao Liu'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Jingyuan Yang'), arxiv.Result.Author('Haoxuan Ding')]","The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.",0.114716105,-0.28059483,-0.22864598,C
9056,"Unfortunately, VR-LwF is still around 15% lower than the
Joint-FT. A further study with more focus on increasing the           Method      UT-Acc  ZS-Acc  TR@1   IR@1   A-Acc
performance of updated classes is therefore suggested.",REPLAYING NUMBERS.,"25.36   62.62  69.34  49.67  43.99
                                                                  Original CLIP    73.19   61.67  67.64  47.28  67.43
      e) RKR keeps the original performance best, but learns                       72.82   62.03  68.34  47.82  67.43
least: Surprisingly, the RKR method was found to get the best         Ks =50       73.09   61.86  67.76  47.98  67.47
zero-shot performance.",2022-07-19 13:03:14+00:00,Don't Stop Learning: Towards Continual Learning for the CLIP Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuxuan Ding'), arxiv.Result.Author('Lingqiao Liu'), arxiv.Result.Author('Chunna Tian'), arxiv.Result.Author('Jingyuan Yang'), arxiv.Result.Author('Haoxuan Ding')]","The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.",0.19039664,0.11765216,0.10184662,A
9066,"age, as well as the assessments derived from the 2D and 3D
                                                                                infant pose models (FiDIP and HW-HuP, respectively) on
   We now turn to the task of calibrating an end-to-end                         top of their respective predicted pose skeletons, with green
pose-based system for the evaluation of symmetry, for use                       indicating symmetric judgements and red indicating asym-
in practical applications or further research3.","These Ô¨Ågures visualize the Bayesian ag-
most adult human, or 2D pose models for infants or adults‚Äî                      gregate assessments of symmetry on top of the original im-
offer weaker ability to predict the human assessments.",In concrete                     metric judgements.,2022-07-19 15:59:40+00:00,Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Xiaofei Huang'), arxiv.Result.Author('Michael Wan'), arxiv.Result.Author('Lingfei Luan'), arxiv.Result.Author('Bethany Tunik'), arxiv.Result.Author('Sarah Ostadabbas')]","Bilateral postural symmetry plays a key role as a potential risk marker for
autism spectrum disorder (ASD) and as a symptom of congenital muscular
torticollis (CMT) in infants, but current methods of assessing symmetry require
laborious clinical expert assessments. In this paper, we develop a computer
vision based infant symmetry assessment system, leveraging 3D human pose
estimation for infants. Evaluation and calibration of our system against ground
truth assessments is complicated by our findings from a survey of human ratings
of angle and symmetry, that such ratings exhibit low inter-rater reliability.
To rectify this, we develop a Bayesian estimator of the ground truth derived
from a probabilistic graphical model of fallible human raters. We show that the
3D infant pose estimation model can achieve 68% area under the receiver
operating characteristic curve performance in predicting the Bayesian aggregate
labels, compared to only 61% from a 2D infant pose estimation model and 60%
from a 3D adult pose estimation model, highlighting the importance of 3D poses
and infant domain knowledge in assessing infant body symmetry. Our survey
analysis also suggests that human ratings are susceptible to higher levels of
bias and inconsistency, and hence our final 3D pose-based symmetry assessment
system is calibrated but not directly supervised by Bayesian aggregate human
ratings, yielding higher levels of consistency and lower levels of inter-limb
assessment bias.",-0.061604135,0.1769247,-0.26045197,B
9073,"RELATED WORK

                                           Since AlexNet‚Äôs [5] remarkable success in the                         The use of Convolutional Neural Networks (CNNs) has
                                        ILSVRC-2012 image classiÔ¨Åcation competition, which                    been a key method of recognizing images, such as classifying
                                        combined GPU and CNN, further research has focused on                 them [7], [12], recognizing actions [13], and locate objects
                                        enhancing the CNN architecture and integrating it with new            [14].",II.,"As deep learning models require a lot of training
                                        concepts to get higher performance.",2022-07-19 20:01:11+00:00,A Block-based Convolutional Neural Network for Low-Resolution Image Classification,cs.CV,"['cs.CV', 'I.4.0']","[arxiv.Result.Author('Ashkan Ganj'), arxiv.Result.Author('Mohsen Ebadpour'), arxiv.Result.Author('Mahdi Darvish'), arxiv.Result.Author('Hamid Bahador')]","The success of CNN-based architecture on image classification in learning and
extracting features made them so popular these days, but the task of image
classification becomes more challenging when we use state of art models to
classify noisy and low-quality images. To solve this problem, we proposed a
novel image classification architecture that learns subtle details in
low-resolution images that are blurred and noisy. In order to build our new
blocks, we used the idea of Res Connections and the Inception module ideas.
Using the MNIST datasets, we have conducted extensive experiments that show
that the introduced architecture is more accurate and faster than other
state-of-the-art Convolutional neural networks. As a result of the special
characteristics of our model, it can achieve a better result with fewer
parameters.",-0.3779844,-0.2220327,0.1525076,C
9074,"                                        LR-Net : A Block-based Convolutional Neural
                                       Network for Low-Resolution Image ClassiÔ¨Åcation

                                                              Ashkan Ganj1, Mohsen Ebadpour2, Mahdi Darvish1, Hamid Bahador1
                                                                                   Department of Electrical and Computer Science

                                                  University of Mohaghegh Ardabili,Ardabil, Iran1, Amirkabir University of Technology, Tehran, Iran2

arXiv:2207.09531v2 [cs.CV] 4 Aug 2022     Abstract‚ÄîThe success of CNN-based architecture on image          combined GPU and CNN, further research has focused on
                                       classiÔ¨Åcation in learning and extracting features made them         enhancing the CNN architecture and integrating it with new
                                       so popular these days, but the task of image classiÔ¨Åcation          concepts to get higher performance.VGG [7], GoogleLeNet
                                       becomes more challenging when we apply state of art models          [8], and ResNet [9] are three popular attempts to improve
                                       to classify noisy and low-quality images.","735‚Äì744, 12 2018.","It is still difÔ¨Åcult for  performance by using CNNs.To improve performance, the
                                       models to extract meaningful features from this type of image       Visual Geometry Group (VGG) [7] is looking into deeper
                                       due to its low-resolution and the lack of meaningful global         network performance by extracting as many features as
                                       features.",2022-07-19 20:01:11+00:00,A Block-based Convolutional Neural Network for Low-Resolution Image Classification,cs.CV,"['cs.CV', 'I.4.0']","[arxiv.Result.Author('Ashkan Ganj'), arxiv.Result.Author('Mohsen Ebadpour'), arxiv.Result.Author('Mahdi Darvish'), arxiv.Result.Author('Hamid Bahador')]","The success of CNN-based architecture on image classification in learning and
extracting features made them so popular these days, but the task of image
classification becomes more challenging when we use state of art models to
classify noisy and low-quality images. To solve this problem, we proposed a
novel image classification architecture that learns subtle details in
low-resolution images that are blurred and noisy. In order to build our new
blocks, we used the idea of Res Connections and the Inception module ideas.
Using the MNIST datasets, we have conducted extensive experiments that show
that the introduced architecture is more accurate and faster than other
state-of-the-art Convolutional neural networks. As a result of the special
characteristics of our model, it can achieve a better result with fewer
parameters.",-0.2493685,-0.18996593,0.3289863,C
9075,"                                         LR-Net : A Block-based Convolutional Neural
                                        Network for Low-Resolution Image ClassiÔ¨Åcation

                                                               Ashkan Ganj1, Mohsen Ebadpour2, Mahdi Darvish1, Hamid Bahador1
                                                                                    Department of Electrical and Computer Science

                                                   University of Mohaghegh Ardabili,Ardabil, Iran1, Amirkabir University of Technology, Tehran, Iran2

arXiv:2207.09531v3 [cs.CV] 11 Aug 2022     Abstract‚ÄîThe success of CNN-based architecture on image          combined GPU and CNN, further research has focused on
                                        classiÔ¨Åcation in learning and extracting features made them         enhancing the CNN architecture and integrating it with new
                                        so popular these days, but the task of image classiÔ¨Åcation          concepts to get higher performance.VGG [6], GoogleLeNet
                                        becomes more challenging when we apply state of art models          [7], and ResNet [8] are three popular attempts to improve
                                        to classify noisy and low-quality images.","735‚Äì744, 12 2018.","It is still difÔ¨Åcult for  performance by using CNNs.To improve performance, the
                                        models to extract meaningful features from this type of image       Visual Geometry Group (VGG) [6] is looking into deeper
                                        due to its low-resolution and the lack of meaningful global         network performance by extracting as many features as
                                        features.",2022-07-19 20:01:11+00:00,LR-Net: A Block-based Convolutional Neural Network for Low-Resolution Image Classification,cs.CV,"['cs.CV', 'I.4.0']","[arxiv.Result.Author('Ashkan Ganj'), arxiv.Result.Author('Mohsen Ebadpour'), arxiv.Result.Author('Mahdi Darvish'), arxiv.Result.Author('Hamid Bahador')]","The success of CNN-based architecture on image classification in learning and
extracting features made them so popular these days, but the task of image
classification becomes more challenging when we apply state of art models to
classify noisy and low-quality images. It is still difficult for models to
extract meaningful features from this type of image due to its low-resolution
and the lack of meaningful global features. Moreover, high-resolution images
need more layers to train which means they take more time and computational
power to train. Our method also addresses the problem of vanishing gradients as
the layers become deeper in deep neural networks that we mentioned earlier. In
order to address all these issues, we developed a novel image classification
architecture, composed of blocks that are designed to learn both low level and
global features from blurred and noisy low-resolution images. Our design of the
blocks was heavily influenced by Residual Connections and Inception modules in
order to increase performance and reduce parameter sizes. We also assess our
work using the MNIST family datasets, with a particular emphasis on the
Oracle-MNIST dataset, which is the most difficult to classify due to its
low-quality and noisy images. We have performed in-depth tests that demonstrate
the presented architecture is faster and more accurate than existing
cutting-edge convolutional neural networks. Furthermore, due to the unique
properties of our model, it can produce a better result with fewer parameters.",-0.250225,-0.18819489,0.3289088,C
9076,"                                          LR-Net: A Block-based Convolutional Neural
                                        Network for Low-Resolution Image ClassiÔ¨Åcation

                                                              Ashkan Ganj1, Mohsen Ebadpour2, Mahdi Darvish1, Hamid Bahador1,‚àó
                                                  1Department of Electrical and Computer Engineering, University of Mohaghegh Ardabili, Ardabil, Iran

                                                          2Department of Computer Engineering, Amirkabir University of Technology, Tehran, Iran

                                                                                 * Corresponding author: hamid.bahador@uma.ac.ir

arXiv:2207.09531v4 [cs.CV] 17 Oct 2022     Abstract‚ÄîThe success of CNN-based architecture on image            Since AlexNet‚Äôs [5] remarkable success in the
                                        classiÔ¨Åcation in learning and extracting features made them so     ILSVRC-2012 image classiÔ¨Åcation competition, which
                                        popular these days, but the task of image classiÔ¨Åcation becomes    combined GPU and CNN, further research has focused on
                                        more challenging when we apply state of art models to classify     enhancing the CNN architecture and integrating it with new
                                        noisy and low-quality images.","735‚Äì744, 12 2018.","It is still difÔ¨Åcult for models      concepts to get higher performance.VGG [6], GoogleLeNet
                                        to extract meaningful features from this type of image due         [7], and ResNet [8] are three popular attempts to improve
                                        to its low resolution and lack of meaningful global features.",2022-07-19 20:01:11+00:00,LR-Net: A Block-based Convolutional Neural Network for Low-Resolution Image Classification,cs.CV,"['cs.CV', 'I.4.0']","[arxiv.Result.Author('Ashkan Ganj'), arxiv.Result.Author('Mohsen Ebadpour'), arxiv.Result.Author('Mahdi Darvish'), arxiv.Result.Author('Hamid Bahador')]","The success of CNN-based architecture on image classification in learning and
extracting features made them so popular these days, but the task of image
classification becomes more challenging when we apply state of art models to
classify noisy and low-quality images. It is still difficult for models to
extract meaningful features from this type of image due to its low-resolution
and the lack of meaningful global features. Moreover, high-resolution images
need more layers to train which means they take more time and computational
power to train. Our method also addresses the problem of vanishing gradients as
the layers become deeper in deep neural networks that we mentioned earlier. In
order to address all these issues, we developed a novel image classification
architecture, composed of blocks that are designed to learn both low level and
global features from blurred and noisy low-resolution images. Our design of the
blocks was heavily influenced by Residual Connections and Inception modules in
order to increase performance and reduce parameter sizes. We also assess our
work using the MNIST family datasets, with a particular emphasis on the
Oracle-MNIST dataset, which is the most difficult to classify due to its
low-quality and noisy images. We have performed in-depth tests that demonstrate
the presented architecture is faster and more accurate than existing
cutting-edge convolutional neural networks. Furthermore, due to the unique
properties of our model, it can produce a better result with fewer parameters.",-0.24295944,-0.20974398,0.3235462,C
9086,"A rigorous causal understanding of the behaviour that
               eventually yields a model producing outlier confident incorrect classifications for both
               categories is beyond the scope of the present work, and would make for an involved
               subject of further study.","However, the phenomenon of increasing loss
               was no longer apparent when cross-entropy loss was replaced with the ‚Äúbalanced‚Äù loss
               function (see S1 Choice of loss).","4.2 Model-database duality

               We observe that models perform better on the DOVS database in single-domain tasks.",2022-07-20 02:47:29+00:00,Learning from few examples: Classifying sex from retinal images via deep learning,cs.CV,"['cs.CV', 'cs.LG', '68T07, 62P10, 92C50, 92C55, 94A08, 94A12', 'I.2.1; I.4.7; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.3']","[arxiv.Result.Author('Aaron Berk'), arxiv.Result.Author('Gulcenur Ozturan'), arxiv.Result.Author('Parsa Delavari'), arxiv.Result.Author('David Maberley'), arxiv.Result.Author('√ñzg√ºr Yƒ±lmaz'), arxiv.Result.Author('Ipek Oruc')]","Deep learning has seen tremendous interest in medical imaging, particularly
in the use of convolutional neural networks (CNNs) for developing automated
diagnostic tools. The facility of its non-invasive acquisition makes retinal
fundus imaging amenable to such automated approaches. Recent work in analyzing
fundus images using CNNs relies on access to massive data for training and
validation - hundreds of thousands of images. However, data residency and data
privacy restrictions stymie the applicability of this approach in medical
settings where patient confidentiality is a mandate. Here, we showcase results
for the performance of DL on small datasets to classify patient sex from fundus
images - a trait thought not to be present or quantifiable in fundus images
until recently. We fine-tune a Resnet-152 model whose last layer has been
modified for binary classification. In several experiments, we assess
performance in the small dataset context using one private (DOVS) and one
public (ODIR) data source. Our models, developed using approximately 2500
fundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]).
This corresponds to a mere 25% decrease in performance despite a nearly
1000-fold decrease in the dataset size compared to prior work in the
literature. Even with a hard task like sex categorization from retinal images,
we find that classification is possible with very small datasets. Additionally,
we perform domain adaptation experiments between DOVS and ODIR; explore the
effect of data curation on training and generalizability; and investigate model
ensembling to maximize CNN classifier performance in the context of small
development datasets.",0.17407286,-0.21258816,-0.066606574,A
9087,Several interesting avenues present themselves for further study.,"Thus, adequately
               performing models found during this development process are apt candidates to
               comprise an ensemble classifier.","For instance, we do
               not investigate how the ensemble could be tuned to produce improved results.",2022-07-20 02:47:29+00:00,Learning from few examples: Classifying sex from retinal images via deep learning,cs.CV,"['cs.CV', 'cs.LG', '68T07, 62P10, 92C50, 92C55, 94A08, 94A12', 'I.2.1; I.4.7; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.3']","[arxiv.Result.Author('Aaron Berk'), arxiv.Result.Author('Gulcenur Ozturan'), arxiv.Result.Author('Parsa Delavari'), arxiv.Result.Author('David Maberley'), arxiv.Result.Author('√ñzg√ºr Yƒ±lmaz'), arxiv.Result.Author('Ipek Oruc')]","Deep learning has seen tremendous interest in medical imaging, particularly
in the use of convolutional neural networks (CNNs) for developing automated
diagnostic tools. The facility of its non-invasive acquisition makes retinal
fundus imaging amenable to such automated approaches. Recent work in analyzing
fundus images using CNNs relies on access to massive data for training and
validation - hundreds of thousands of images. However, data residency and data
privacy restrictions stymie the applicability of this approach in medical
settings where patient confidentiality is a mandate. Here, we showcase results
for the performance of DL on small datasets to classify patient sex from fundus
images - a trait thought not to be present or quantifiable in fundus images
until recently. We fine-tune a Resnet-152 model whose last layer has been
modified for binary classification. In several experiments, we assess
performance in the small dataset context using one private (DOVS) and one
public (ODIR) data source. Our models, developed using approximately 2500
fundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]).
This corresponds to a mere 25% decrease in performance despite a nearly
1000-fold decrease in the dataset size compared to prior work in the
literature. Even with a hard task like sex categorization from retinal images,
we find that classification is possible with very small datasets. Additionally,
we perform domain adaptation experiments between DOVS and ODIR; explore the
effect of data curation on training and generalizability; and investigate model
ensembling to maximize CNN classifier performance in the context of small
development datasets.",0.18200094,-0.1311732,-0.11649054,A
9093,"We further study its effect                  is not very sensitive to this parameter, achieving the stable
with different quantities by varying the value of positive                   mIoU around 57%.","Overall, our method
guided by pseudo labels (Ours).","Analyzing in detail, we observe that
and negative pairs, resulting in six conÔ¨Ågurations.",2022-07-20 05:42:19+00:00,Pseudo-label Guided Cross-video Pixel Contrast for Robotic Surgical Scene Segmentation with Limited Annotations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yang Yu'), arxiv.Result.Author('Zixu Zhao'), arxiv.Result.Author('Yueming Jin'), arxiv.Result.Author('Guangyong Chen'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Pheng-Ann Heng')]","Surgical scene segmentation is fundamentally crucial for prompting cognitive
assistance in robotic surgery. However, pixel-wise annotating surgical video in
a frame-by-frame manner is expensive and time consuming. To greatly reduce the
labeling burden, in this work, we study semi-supervised scene segmentation from
robotic surgical video, which is practically essential yet rarely explored
before. We consider a clinically suitable annotation situation under the
equidistant sampling. We then propose PGV-CL, a novel pseudo-label guided
cross-video contrast learning method to boost scene segmentation. It
effectively leverages unlabeled data for a trusty and global model
regularization that produces more discriminative feature representation.
Concretely, for trusty representation learning, we propose to incorporate
pseudo labels to instruct the pair selection, obtaining more reliable
representation pairs for pixel contrast. Moreover, we expand the representation
learning space from previous image-level to cross-video, which can capture the
global semantics to benefit the learning process. We extensively evaluate our
method on a public robotic surgery dataset EndoVis18 and a public cataract
dataset CaDIS. Experimental results demonstrate the effectiveness of our
method, consistently outperforming the state-of-the-art semi-supervised methods
under different labeling ratios, and even surpassing fully supervised training
on EndoVis18 with 10.1% labeling.",0.44164371,0.18319479,0.09578739,A
9098,"fidelity, further research is still needed to neutralize the domain shift between
real and synthetic domains [32].",Augmentations are applied at both local and global levels.,"Unsupervised Domain Adaptation (UDA) for semantic segmentation has been
widely studied for image data [9,25,28,44,45], however less attention has been paid
to adaptation techniques for point clouds.",2022-07-20 09:33:42+00:00,CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Cristiano Saltori'), arxiv.Result.Author('Fabio Galasso'), arxiv.Result.Author('Giuseppe Fiameni'), arxiv.Result.Author('Nicu Sebe'), arxiv.Result.Author('Elisa Ricci'), arxiv.Result.Author('Fabio Poiesi')]","3D LiDAR semantic segmentation is fundamental for autonomous driving. Several
Unsupervised Domain Adaptation (UDA) methods for point cloud data have been
recently proposed to improve model generalization for different sensors and
environments. Researchers working on UDA problems in the image domain have
shown that sample mixing can mitigate domain shift. We propose a new approach
of sample mixing for point cloud UDA, namely Compositional Semantic Mix
(CoSMix), the first UDA approach for point cloud segmentation based on sample
mixing. CoSMix consists of a two-branch symmetric network that can process
labelled synthetic data (source) and real-world unlabelled point clouds
(target) concurrently. Each branch operates on one domain by mixing selected
pieces of data from the other one, and by using the semantic information
derived from source labels and target pseudo-labels. We evaluate CoSMix on two
large-scale datasets, showing that it outperforms state-of-the-art methods by a
large margin. Our code is available at
https://github.com/saltoricristiano/cosmix-uda.",-0.27796894,-0.004801266,0.12238492,B
9121,"Furthermore, as we revisited the potential of self-supervision
in a simple applicable way in supervised settings, we believe this line is worth
further study to be a standard technique in supervised learning.","Our extensive
experiments demonstrated the merits of LoRot as well as the complementary
benefits to prior arts.",Acknowledgements.,2022-07-20 16:41:14+00:00,Tailoring Self-Supervision for Supervised Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('WonJun Moon'), arxiv.Result.Author('Ji-Hwan Kim'), arxiv.Result.Author('Jae-Pil Heo')]","Recently, it is shown that deploying a proper self-supervision is a
prospective way to enhance the performance of supervised learning. Yet, the
benefits of self-supervision are not fully exploited as previous pretext tasks
are specialized for unsupervised representation learning. To this end, we begin
by presenting three desirable properties for such auxiliary tasks to assist the
supervised objective. First, the tasks need to guide the model to learn rich
features. Second, the transformations involved in the self-supervision should
not significantly alter the training distribution. Third, the tasks are
preferred to be light and generic for high applicability to prior arts.
Subsequently, to show how existing pretext tasks can fulfill these and be
tailored for supervised learning, we propose a simple auxiliary
self-supervision task, predicting localizable rotation (LoRot). Our exhaustive
experiments validate the merits of LoRot as a pretext task tailored for
supervised learning in terms of robustness and generalization capability. Our
code is available at https://github.com/wjun0830/Localizable-Rotation.",0.022063952,-0.28985918,-0.28454325,C
9127,"Another interesting direction
for further research is the generation of dance sequences in a finer granularity
and more controlled manner.","This is just one of the possible ideas
one could develop with our fine-grained labels.","Current methods lack the ability to adjust gen-
erated sequences according to human intervention or conditioning.",2022-07-20 18:03:54+00:00,BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Davide Moltisanti'), arxiv.Result.Author('Jinyi Wu'), arxiv.Result.Author('Bo Dai'), arxiv.Result.Author('Chen Change Loy')]","Generative models for audio-conditioned dance motion synthesis map music
features to dance movements. Models are trained to associate motion patterns to
audio patterns, usually without an explicit knowledge of the human body. This
approach relies on a few assumptions: strong music-dance correlation,
controlled motion data and relatively simple poses and movements. These
characteristics are found in all existing datasets for dance motion synthesis,
and indeed recent methods can achieve good results.We introduce a new dataset
aiming to challenge these common assumptions, compiling a set of dynamic dance
sequences displaying complex human poses. We focus on breakdancing which
features acrobatic moves and tangled postures. We source our data from the Red
Bull BC One competition videos. Estimating human keypoints from these videos is
difficult due to the complexity of the dance, as well as the multiple moving
cameras recording setup. We adopt a hybrid labelling pipeline leveraging deep
estimation models as well as manual annotations to obtain good quality keypoint
sequences at a reduced cost. Our efforts produced the BRACE dataset, which
contains over 3 hours and 30 minutes of densely annotated poses. We test
state-of-the-art methods on BRACE, showing their limitations when evaluated on
complex sequences. Our dataset can readily foster advance in dance motion
synthesis. With intricate poses and swift movements, models are forced to go
beyond learning a mapping between modalities and reason more effectively about
body structure and movements.",0.20570526,-0.011408326,-0.27997023,A
9128,"Another interesting direction
for further research is the generation of dance sequences in a finer granularity
and more controlled manner.","This is just one of the possible ideas
one could develop with our fine-grained labels.","Current methods lack the ability to adjust gen-
erated sequences according to human intervention or conditioning.",2022-07-20 18:03:54+00:00,BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Davide Moltisanti'), arxiv.Result.Author('Jinyi Wu'), arxiv.Result.Author('Bo Dai'), arxiv.Result.Author('Chen Change Loy')]","Generative models for audio-conditioned dance motion synthesis map music
features to dance movements. Models are trained to associate motion patterns to
audio patterns, usually without an explicit knowledge of the human body. This
approach relies on a few assumptions: strong music-dance correlation,
controlled motion data and relatively simple poses and movements. These
characteristics are found in all existing datasets for dance motion synthesis,
and indeed recent methods can achieve good results.We introduce a new dataset
aiming to challenge these common assumptions, compiling a set of dynamic dance
sequences displaying complex human poses. We focus on breakdancing which
features acrobatic moves and tangled postures. We source our data from the Red
Bull BC One competition videos. Estimating human keypoints from these videos is
difficult due to the complexity of the dance, as well as the multiple moving
cameras recording setup. We adopt a hybrid labelling pipeline leveraging deep
estimation models as well as manual annotations to obtain good quality keypoint
sequences at a reduced cost. Our efforts produced the BRACE dataset, which
contains over 3 hours and 30 minutes of densely annotated poses. We test
state-of-the-art methods on BRACE, showing their limitations when evaluated on
complex sequences. Our dataset can readily foster advance in dance motion
synthesis. With intricate poses and swift movements, models are forced to go
beyond learning a mapping between modalities and reason more effectively about
body structure and movements.",0.20570526,-0.011408326,-0.27997023,A
9130,"In this experiment, we further study the ratio of real (R) and synthetic
(S) images in each mini-batch.","3.2 (main paper), we combine the original real images
and their corresponding synthesized version as a mini-batch for the FR model
training.",As shown in Fig.,2022-07-20 20:13:29+00:00,Controllable and Guided Face Synthesis for Unconstrained Face Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Feng Liu'), arxiv.Result.Author('Minchul Kim'), arxiv.Result.Author('Anil Jain'), arxiv.Result.Author('Xiaoming Liu')]","Although significant advances have been made in face recognition (FR), FR in
unconstrained environments remains challenging due to the domain gap between
the semi-constrained training datasets and unconstrained testing scenarios. To
address this problem, we propose a controllable face synthesis model (CFSM)
that can mimic the distribution of target datasets in a style latent space.
CFSM learns a linear subspace with orthogonal bases in the style latent space
with precise control over the diversity and degree of synthesis. Furthermore,
the pre-trained synthesis model can be guided by the FR model, making the
resulting images more beneficial for FR model training. Besides, target dataset
distributions are characterized by the learned orthogonal bases, which can be
utilized to measure the distributional similarity among face datasets. Our
approach yields significant performance gains on unconstrained benchmarks, such
as IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).",-0.008649388,-0.0021616165,0.27268696,C
9132,"We further study the robustness of diÔ¨Äerent
                                                 modules against local and global variations.","To this
                                                 end, we design and evaluate corruptions that involve data addition, re-
                                                 duction, and alteration.","Our experimental results
                                                 reveal several intriguing Ô¨Åndings.",2022-07-20 21:47:15+00:00,On the Robustness of 3D Object Detectors,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Fatima Albreiki'), arxiv.Result.Author('Sultan Abughazal'), arxiv.Result.Author('Jean Lahoud'), arxiv.Result.Author('Rao Anwer'), arxiv.Result.Author('Hisham Cholakkal'), arxiv.Result.Author('Fahad Khan')]","In recent years, significant progress has been achieved for 3D object
detection on point clouds thanks to the advances in 3D data collection and deep
learning techniques. Nevertheless, 3D scenes exhibit a lot of variations and
are prone to sensor inaccuracies as well as information loss during
pre-processing. Thus, it is crucial to design techniques that are robust
against these variations. This requires a detailed analysis and understanding
of the effect of such variations. This work aims to analyze and benchmark
popular point-based 3D object detectors against several data corruptions. To
the best of our knowledge, we are the first to investigate the robustness of
point-based 3D object detectors. To this end, we design and evaluate
corruptions that involve data addition, reduction, and alteration. We further
study the robustness of different modules against local and global variations.
Our experimental results reveal several intriguing findings. For instance, we
show that methods that integrate Transformers at a patch or object level lead
to increased robustness, compared to using Transformers at the point level.",0.27875996,0.06245904,0.07772398,A
9143,"The dataset information is available to the vision community to promote
further research and development in this Ô¨Åeld.","This balanced dataset aims to mitigate the performance diÔ¨Äerential of deepfake

3 https://c2pa.org/post/release_1_pr/
4  Nadimpalli and Rattani

detectors due to existing gender unbalanced training sets along with irregular
swaps.","Note that according to ISO/IEC
22116 [7], the term ‚Äúsex‚Äù, understood as ‚Äúthe state of being male or female‚Äù
would be more appropriate instead of ‚Äúgender‚Äù in the context of this study.",2022-07-21 01:00:40+00:00,GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Aakash Varma Nadimpalli'), arxiv.Result.Author('Ajita Rattani')]","Facial forgery by deepfakes has raised severe societal concerns. Several
solutions have been proposed by the vision community to effectively combat the
misinformation on the internet via automated deepfake detection systems. Recent
studies have demonstrated that facial analysis-based deep learning models can
discriminate based on protected attributes. For the commercial adoption and
massive roll-out of the deepfake detection technology, it is vital to evaluate
and understand the fairness (the absence of any prejudice or favoritism) of
deepfake detectors across demographic variations such as gender and race. As
the performance differential of deepfake detectors between demographic
subgroups would impact millions of people of the deprived sub-group. This paper
aims to evaluate the fairness of the deepfake detectors across males and
females. However, existing deepfake datasets are not annotated with demographic
labels to facilitate fairness analysis. To this aim, we manually annotated
existing popular deepfake datasets with gender labels and evaluated the
performance differential of current deepfake detectors across gender. Our
analysis on the gender-labeled version of the datasets suggests (a) current
deepfake datasets have skewed distribution across gender, and (b) commonly
adopted deepfake detectors obtain unequal performance across gender with mostly
males outperforming females. Finally, we contributed a gender-balanced and
annotated deepfake dataset, GBDF, to mitigate the performance differential and
to promote research and development towards fairness-aware deep fake detectors.
The GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF",-0.12995917,-0.09791532,-0.014607195,C
9144,"We hope that our results spark further research towards Ô¨Ånding more eÔ¨Écient
                                                vision architectures and facilitate the development of MLP-like models.","On CIFAR-100
                                                dataset, SplitMixer achieves around 73% accuracy, on par with ConvMixer, but with about 52% fewer
                                                parameters and FLOPS.","Code is available at https:
                                                //github.com/aliborji/splitmixer.",2022-07-21 01:37:07+00:00,SplitMixer: Fat Trimmed From MLP-like Models,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ali Borji'), arxiv.Result.Author('Sikun Lin')]","We present SplitMixer, a simple and lightweight isotropic MLP-like
architecture, for visual recognition. It contains two types of interleaving
convolutional operations to mix information across spatial locations (spatial
mixing) and channels (channel mixing). The first one includes sequentially
applying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial
information. The second one is splitting the channels into overlapping or
non-overlapping segments, with or without shared parameters, and applying our
proposed channel mixing approaches or 3D convolution to mix channel
information. Depending on design choices, a number of SplitMixer variants can
be constructed to balance accuracy, the number of parameters, and speed. We
show, both theoretically and experimentally, that SplitMixer performs on par
with the state-of-the-art MLP-like models while having a significantly lower
number of parameters and FLOPS. For example, without strong data augmentation
and optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only
0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M
parameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On
CIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with
ConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our
results spark further research towards finding more efficient vision
architectures and facilitate the development of MLP-like models. Code is
available at https://github.com/aliborji/splitmixer.",-0.09857649,0.05337125,0.19325559,B
9145,"We hope that our results spark further research towards Ô¨Ånding more eÔ¨Écient
                                                vision architectures and facilitate the development of MLP-like models.","On CIFAR-100
                                                dataset, SplitMixer achieves around 73% accuracy, on par with ConvMixer, but with about 52% fewer
                                                parameters and FLOPS.","Code is available at https:
                                                //github.com/aliborji/splitmixer.",2022-07-21 01:37:07+00:00,SplitMixer: Fat Trimmed From MLP-like Models,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ali Borji'), arxiv.Result.Author('Sikun Lin')]","We present SplitMixer, a simple and lightweight isotropic MLP-like
architecture, for visual recognition. It contains two types of interleaving
convolutional operations to mix information across spatial locations (spatial
mixing) and channels (channel mixing). The first one includes sequentially
applying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial
information. The second one is splitting the channels into overlapping or
non-overlapping segments, with or without shared parameters, and applying our
proposed channel mixing approaches or 3D convolution to mix channel
information. Depending on design choices, a number of SplitMixer variants can
be constructed to balance accuracy, the number of parameters, and speed. We
show, both theoretically and experimentally, that SplitMixer performs on par
with the state-of-the-art MLP-like models while having a significantly lower
number of parameters and FLOPS. For example, without strong data augmentation
and optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only
0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M
parameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On
CIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with
ConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our
results spark further research towards finding more efficient vision
architectures and facilitate the development of MLP-like models. Code is
available at https://github.com/aliborji/splitmixer.",-0.09857649,0.05337125,0.19325559,B
9148,"Variants of VGNet To further study the impact of the
N√óN kernels, several variants of VGNets are introduced:
VGNetC, VGNetG, and VGNetF.","The overall VGNetG-1.0MP architecture
is listed in Table 1.","VGNetC: All parameters
are randomly initialized and learnable.",2022-07-21 06:22:15+00:00,Efficient CNN Architecture Design Guided by Visualization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Liangqi Zhang'), arxiv.Result.Author('Haibo Shen'), arxiv.Result.Author('Yihao Luo'), arxiv.Result.Author('Xiang Cao'), arxiv.Result.Author('Leixilan Pan'), arxiv.Result.Author('Tianjiang Wang'), arxiv.Result.Author('Qi Feng')]","Modern efficient Convolutional Neural Networks(CNNs) always use Depthwise
Separable Convolutions(DSCs) and Neural Architecture Search(NAS) to reduce the
number of parameters and the computational complexity. But some inherent
characteristics of networks are overlooked. Inspired by visualizing feature
maps and N$\times$N(N$>$1) convolution kernels, several guidelines are
introduced in this paper to further improve parameter efficiency and inference
speed. Based on these guidelines, our parameter-efficient CNN architecture,
called \textit{VGNetG}, achieves better accuracy and lower latency than
previous networks with about 30%$\thicksim$50% parameters reduction. Our
VGNetG-1.0MP achieves 67.7% top-1 accuracy with 0.99M parameters and 69.2%
top-1 accuracy with 1.14M parameters on ImageNet classification dataset.
  Furthermore, we demonstrate that edge detectors can replace learnable
depthwise convolution layers to mix features by replacing the N$\times$N
kernels with fixed edge detection kernels. And our VGNetF-1.5MP archives
64.4%(-3.2%) top-1 accuracy and 66.2%(-1.4%) top-1 accuracy with additional
Gaussian kernels.",0.32364458,-0.005680654,0.23822239,A
9181,"We further study using renderings instead of real images to obtain the 2D-2D
matches in Stage 2 of the pipeline, using 3D meshes of different levels of quality
and renderings of different levels of detail.","We show that the proposed approach can reach state-of-the-art
performance compared to the commonly used SfM-based scene representations.","A main result is that modern features
are robust enough to match real photos against non-photo-realistic renderings
of raw scene geometry, even though they were never trained for such a scenario,
resulting in surprisingly accurate pose estimates.",2022-07-21 21:21:10+00:00,MeshLoc: Mesh-Based Visual Localization,cs.CV,"['cs.CV', 'I.2.10; I.4.9']","[arxiv.Result.Author('Vojtech Panek'), arxiv.Result.Author('Zuzana Kukelova'), arxiv.Result.Author('Torsten Sattler')]","Visual localization, i.e., the problem of camera pose estimation, is a
central component of applications such as autonomous robots and augmented
reality systems. A dominant approach in the literature, shown to scale to large
scenes and to handle complex illumination and seasonal changes, is based on
local features extracted from images. The scene representation is a sparse
Structure-from-Motion point cloud that is tied to a specific local feature.
Switching to another feature type requires an expensive feature matching step
between the database images used to construct the point cloud. In this work, we
thus explore a more flexible alternative based on dense 3D meshes that does not
require features matching between database images to build the scene
representation. We show that this approach can achieve state-of-the-art
results. We further show that surprisingly competitive results can be obtained
when extracting features on renderings of these meshes, without any neural
rendering stage, and even when rendering raw scene geometry without color or
texture. Our results show that dense 3D model-based representations are a
promising alternative to existing representations and point to interesting and
challenging directions for future research.",-0.32200062,0.19836082,0.036927316,B
9182,"We further study using renderings instead of real images to obtain the 2D-2D
matches in Stage 2 of the pipeline, using 3D meshes of different levels of quality
and renderings of different levels of detail.","We show that the proposed approach can reach state-of-the-art
performance compared to the commonly used SfM-based scene representations.","A main result is that modern features
are robust enough to match real photos against non-photo-realistic renderings
of raw scene geometry, even though they were never trained for such a scenario,
resulting in surprisingly accurate pose estimates.",2022-07-21 21:21:10+00:00,MeshLoc: Mesh-Based Visual Localization,cs.CV,"['cs.CV', 'I.2.10; I.4.9']","[arxiv.Result.Author('Vojtech Panek'), arxiv.Result.Author('Zuzana Kukelova'), arxiv.Result.Author('Torsten Sattler')]","Visual localization, i.e., the problem of camera pose estimation, is a
central component of applications such as autonomous robots and augmented
reality systems. A dominant approach in the literature, shown to scale to large
scenes and to handle complex illumination and seasonal changes, is based on
local features extracted from images. The scene representation is a sparse
Structure-from-Motion point cloud that is tied to a specific local feature.
Switching to another feature type requires an expensive feature matching step
between the database images used to construct the point cloud. In this work, we
thus explore a more flexible alternative based on dense 3D meshes that does not
require features matching between database images to build the scene
representation. We show that this approach can achieve state-of-the-art
results. We further show that surprisingly competitive results can be obtained
when extracting features on renderings of these meshes, without any neural
rendering stage, and even when rendering raw scene geometry without color or
texture. Our results show that dense 3D model-based representations are a
promising alternative to existing representations and point to interesting and
challenging directions for future research.",-0.32200062,0.19836082,0.036927316,B
9190,"24
A.2 EÔ¨Äective Rotation Angle

We further study the inÔ¨Çuence of backdoored rotation angles on the ASR.","As shown in the Ô¨Ågure, the optimal backdoor at the test time increases as œÉ grows, which
matches our explanation.",As Zhang et al.,2022-07-22 00:21:18+00:00,Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation,cs.CV,"['cs.CV', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Tong Wu'), arxiv.Result.Author('Tianhao Wang'), arxiv.Result.Author('Vikash Sehwag'), arxiv.Result.Author('Saeed Mahloujifar'), arxiv.Result.Author('Prateek Mittal')]","Recent works have demonstrated that deep learning models are vulnerable to
backdoor poisoning attacks, where these attacks instill spurious correlations
to external trigger patterns or objects (e.g., stickers, sunglasses, etc.). We
find that such external trigger signals are unnecessary, as highly effective
backdoors can be easily inserted using rotation-based image transformation. Our
method constructs the poisoned dataset by rotating a limited amount of objects
and labeling them incorrectly; once trained with it, the victim's model will
make undesirable predictions during run-time inference. It exhibits a
significantly high attack success rate while maintaining clean performance
through comprehensive empirical studies on image classification and object
detection tasks. Furthermore, we evaluate standard data augmentation techniques
and four different backdoor defenses against our attack and find that none of
them can serve as a consistent mitigation approach. Our attack can be easily
deployed in the real world since it only requires rotating the object, as we
show in both image classification and object detection applications. Overall,
our work highlights a new, simple, physically realizable, and highly effective
vector for backdoor attacks. Our video demo is available at
https://youtu.be/6JIF8wnX34M.",0.36289585,0.22679196,0.10546517,A
9196,"In long-tailed instance segmentation, one can use the cumulative density function
of the standard Gumbel distribution as the activation function (we further study
the choice of non-standard Gumbel activation in supplementary material):

     \eta _{\gamma }(q_i)=F_{\text {Gumbel}}(q;0,1)=\exp (-\exp (-q_i)) \label {gumbel_activation}                                                                                                     (7)

Combining Eq.","Other applications of Gumbel can be found in finance and biology and the readers
are referred to this work [19] for more information on extreme value distribution.","6 and Gumbel activation we derive Gumbel Loss (GL) as:

     GL(\eta _{\gamma }(q_i),y_i)= \begin {cases} -\log (\eta _{\gamma }(q_i)),\;\; if \;\; y_i=1\\ -\log (1-\eta _{\gamma }(q_i)),\;\; if\;\; y_i=0 \end {cases} \label {gumbel_loss}                 (8)

The gradient of Eq.",2022-07-22 08:20:23+00:00,Long-tailed Instance Segmentation using Gumbel Optimized Loss,cs.CV,['cs.CV'],"[arxiv.Result.Author('Konstantinos Panagiotis Alexandridis'), arxiv.Result.Author('Jiankang Deng'), arxiv.Result.Author('Anh Nguyen'), arxiv.Result.Author('Shan Luo')]","Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL",0.23765609,-0.0035519917,0.14482781,A
9197,"In long-tailed instance segmentation, one can use the cumulative density function
of the standard Gumbel distribution as the activation function (we further study
the choice of non-standard Gumbel activation in supplementary material):

     \eta _{\gamma }(q_i)=F_{\text {Gumbel}}(q;0,1)=\exp (-\exp (-q_i)) \label {gumbel_activation}                                                                                                     (7)

Combining Eq.","Other applications of Gumbel can be found in finance and biology and the readers
are referred to this work [19] for more information on extreme value distribution.","6 and Gumbel activation we derive Gumbel Loss (GL) as:

     GL(\eta _{\gamma }(q_i),y_i)= \begin {cases} -\log (\eta _{\gamma }(q_i)),\;\; if \;\; y_i=1\\ -\log (1-\eta _{\gamma }(q_i)),\;\; if\;\; y_i=0 \end {cases} \label {gumbel_loss}                 (8)

The gradient of Eq.",2022-07-22 08:20:23+00:00,Long-tailed Instance Segmentation using Gumbel Optimized Loss,cs.CV,['cs.CV'],"[arxiv.Result.Author('Konstantinos Panagiotis Alexandridis'), arxiv.Result.Author('Jiankang Deng'), arxiv.Result.Author('Anh Nguyen'), arxiv.Result.Author('Shan Luo')]","Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL",0.23765609,-0.0035519917,0.14482781,A
9252,"So further research can fo-
cus on developing better frameworks to improve the accu-
racy of CL condition continuously.","However, based
on our framework, there is still a large development space
for the CL condition to improve.","References

Chao, H.; He, Y.; Zhang, J.; and Feng, J.",2022-07-24 11:26:53+00:00,Progressive Feature Learning for Realistic Cloth-Changing Gait Recognition,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xuqian Ren'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Chunshui Cao'), arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Yongzhen Huang')]","Gait recognition is instrumental in crime prevention and social security, for
it can be conducted at a long distance without the cooperation of subjects.
However, existing datasets and methods cannot deal with the most challenging
problem in realistic gait recognition effectively: walking in different clothes
(CL). In order to tackle this problem, we propose two benchmarks: CASIA-BN-RCC
and OUMVLP-RCC, to simulate the cloth-changing condition in practice. The two
benchmarks can force the algorithm to realize cross-view and cross-cloth with
two sub-datasets. Furthermore, we propose a new framework that can be applied
with off-the-shelf backbones to improve its performance in the Realistic
Cloth-Changing problem with Progressive Feature Learning. Specifically, in our
framework, we design Progressive Mapping and Progressive Uncertainty to extract
the cross-view features and then extract cross-cloth features on the basis. In
this way, the features from the cross-view sub-dataset can first dominate the
feature space and relieve the uneven distribution caused by the adverse effect
from the cross-cloth sub-dataset. The experiments on our benchmarks show that
our framework can effectively improve the recognition performance in CL
conditions. Our codes and datasets will be released after accepted.",0.36016142,0.012239257,-0.07556369,A
9262,"Long documents are still a big challenge for document     financial reports, in which each document must have both tabular
understanding tasks, where further research efforts are demanded.","We can see that the performance    In this work, we propose a new challenging Document VQA dataset,
on ‚ÄúShort documents‚Äù is significantly better than that on ‚ÄúLong       named TAT-DQA, constructed based on real-world high-quality
documents‚Äù.",and textual data.,2022-07-25 01:43:19+00:00,Towards Complex Document Understanding By Discrete Reasoning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Fengbin Zhu'), arxiv.Result.Author('Wenqiang Lei'), arxiv.Result.Author('Fuli Feng'), arxiv.Result.Author('Chao Wang'), arxiv.Result.Author('Haozhou Zhang'), arxiv.Result.Author('Tat-Seng Chua')]","Document Visual Question Answering (VQA) aims to understand visually-rich
documents to answer questions in natural language, which is an emerging
research topic for both Natural Language Processing and Computer Vision. In
this work, we introduce a new Document VQA dataset, named TAT-DQA, which
consists of 3,067 document pages comprising semi-structured table(s) and
unstructured text as well as 16,558 question-answer pairs by extending the
TAT-QA dataset. These documents are sampled from real-world financial reports
and contain lots of numbers, which means discrete reasoning capability is
demanded to answer questions on this dataset. Based on TAT-DQA, we further
develop a novel model named MHST that takes into account the information in
multi-modalities, including text, layout and visual image, to intelligently
address different types of questions with corresponding strategies, i.e.,
extraction or reasoning. Extensive experiments show that the MHST model
significantly outperforms the baseline methods, demonstrating its
effectiveness. However, the performance still lags far behind that of expert
humans. We expect that our new TAT-DQA dataset would facilitate the research on
deep understanding of visually-rich documents combining vision and language,
especially for scenarios that require discrete reasoning. Also, we hope the
proposed model would inspire researchers to design more advanced Document VQA
models in future.",0.1995621,-0.23232442,-0.16650465,A
9263,"Long documents are still a big challenge for document     financial reports, in which each document must have both tabular
understanding tasks, where further research efforts are demanded.","We can see that the performance    In this work, we propose a new challenging Document VQA dataset,
on ‚ÄúShort documents‚Äù is significantly better than that on ‚ÄúLong       named TAT-DQA, constructed based on real-world high-quality
documents‚Äù.",and textual data.,2022-07-25 01:43:19+00:00,Towards Complex Document Understanding By Discrete Reasoning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Fengbin Zhu'), arxiv.Result.Author('Wenqiang Lei'), arxiv.Result.Author('Fuli Feng'), arxiv.Result.Author('Chao Wang'), arxiv.Result.Author('Haozhou Zhang'), arxiv.Result.Author('Tat-Seng Chua')]","Document Visual Question Answering (VQA) aims to understand visually-rich
documents to answer questions in natural language, which is an emerging
research topic for both Natural Language Processing and Computer Vision. In
this work, we introduce a new Document VQA dataset, named TAT-DQA, which
consists of 3,067 document pages comprising semi-structured table(s) and
unstructured text as well as 16,558 question-answer pairs by extending the
TAT-QA dataset. These documents are sampled from real-world financial reports
and contain lots of numbers, which means discrete reasoning capability is
demanded to answer questions on this dataset. Based on TAT-DQA, we further
develop a novel model named MHST that takes into account the information in
multi-modalities, including text, layout and visual image, to intelligently
address different types of questions with corresponding strategies, i.e.,
extraction or reasoning. Extensive experiments show that the MHST model
significantly outperforms the baseline methods, demonstrating its
effectiveness. However, the performance still lags far behind that of expert
humans. We expect that our new TAT-DQA dataset would facilitate the research on
deep understanding of visually-rich documents combining vision and language,
especially for scenarios that require discrete reasoning. Also, we hope the
proposed model would inspire researchers to design more advanced Document VQA
models in future. Our dataset will be publicly available for non-commercial use
at https://nextplusplus.github.io/TAT-DQA/.",0.1995621,-0.23232442,-0.16650465,A
9264,"Extensive experiments and discussions reveal the
                                                 promising potential of this research field, paving the way for further study.","Thanks to incorporating the task-tailored designs, our
                                                 method shows visible superiority over other baselines, producing more
                                                 satisfactory results.","Keywords: Salient object detection, point cloud, dataset, baseline.",2022-07-25 03:35:46+00:00,Salient Object Detection for Point Clouds,cs.CV,['cs.CV'],"[arxiv.Result.Author('Songlin Fan'), arxiv.Result.Author('Wei Gao'), arxiv.Result.Author('Ge Li')]","This paper researches the unexplored task-point cloud salient object
detection (SOD). Differing from SOD for images, we find the attention shift of
point clouds may provoke saliency conflict, i.e., an object paradoxically
belongs to salient and non-salient categories. To eschew this issue, we present
a novel view-dependent perspective of salient objects, reasonably reflecting
the most eye-catching objects in point cloud scenarios. Following this
formulation, we introduce PCSOD, the first dataset proposed for point cloud SOD
consisting of 2,872 in-/out-door 3D views. The samples in our dataset are
labeled with hierarchical annotations, e.g., super-/sub-class, bounding box,
and segmentation map, which endows the brilliant generalizability and broad
applicability of our dataset verifying various conjectures. To evidence the
feasibility of our solution, we further contribute a baseline model and
benchmark five representative models for a comprehensive comparison. The
proposed model can effectively analyze irregular and unordered points for
detecting salient objects. Thanks to incorporating the task-tailored designs,
our method shows visible superiority over other baselines, producing more
satisfactory results. Extensive experiments and discussions reveal the
promising potential of this research field, paving the way for further study.",-0.28501257,0.10310909,-0.11006136,B
9265,"Our
work reveals the potential of point cloud SOD and pave the way for further study.","Experimental results show that our baseline
model has significant superiority and produces visually favorable predictions.",Acknowledgements.,2022-07-25 03:35:46+00:00,Salient Object Detection for Point Clouds,cs.CV,['cs.CV'],"[arxiv.Result.Author('Songlin Fan'), arxiv.Result.Author('Wei Gao'), arxiv.Result.Author('Ge Li')]","This paper researches the unexplored task-point cloud salient object
detection (SOD). Differing from SOD for images, we find the attention shift of
point clouds may provoke saliency conflict, i.e., an object paradoxically
belongs to salient and non-salient categories. To eschew this issue, we present
a novel view-dependent perspective of salient objects, reasonably reflecting
the most eye-catching objects in point cloud scenarios. Following this
formulation, we introduce PCSOD, the first dataset proposed for point cloud SOD
consisting of 2,872 in-/out-door 3D views. The samples in our dataset are
labeled with hierarchical annotations, e.g., super-/sub-class, bounding box,
and segmentation map, which endows the brilliant generalizability and broad
applicability of our dataset verifying various conjectures. To evidence the
feasibility of our solution, we further contribute a baseline model and
benchmark five representative models for a comprehensive comparison. The
proposed model can effectively analyze irregular and unordered points for
detecting salient objects. Thanks to incorporating the task-tailored designs,
our method shows visible superiority over other baselines, producing more
satisfactory results. Extensive experiments and discussions reveal the
promising potential of this research field, paving the way for further study.",0.06588764,0.31858134,-0.0029570162,B
9290,"Lecc, where the masks in each stage are merged by union
Optical Ô¨Çow networks: We further study the impacts from             operator.","The three distinct distributions illus-     its effectiveness, we train another model with a variant of
trate the error-based segmentation is meaningful.",We denote it as Lm ecc.,2022-07-25 16:15:38+00:00,Error-Aware Spatial Ensembles for Video Frame Interpolation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhixiang Chi'), arxiv.Result.Author('Rasoul Mohammadi Nasiri'), arxiv.Result.Author('Zheng Liu'), arxiv.Result.Author('Yuanhao Yu'), arxiv.Result.Author('Juwei Lu'), arxiv.Result.Author('Jin Tang'), arxiv.Result.Author('Konstantinos N Plataniotis')]","Video frame interpolation~(VFI) algorithms have improved considerably in
recent years due to unprecedented progress in both data-driven algorithms and
their implementations. Recent research has introduced advanced motion
estimation or novel warping methods as the means to address challenging VFI
scenarios. However, none of the published VFI works considers the spatially
non-uniform characteristics of the interpolation error (IE). This work
introduces such a solution. By closely examining the correlation between
optical flow and IE, the paper proposes novel error prediction metrics that
partition the middle frame into distinct regions corresponding to different IE
levels. Building upon this IE-driven segmentation, and through the use of novel
error-controlled loss functions, it introduces an ensemble of spatially
adaptive interpolation units that progressively processes and integrates the
segmented regions. This spatial ensemble results in an effective and
computationally attractive VFI solution. Extensive experimentation on popular
video interpolation benchmarks indicates that the proposed solution outperforms
the current state-of-the-art (SOTA) in applications of current interest.",0.029968038,0.10217071,0.13331869,A
9314,"To further study
to get the attended Residual (Motion Vectors) features.","[49, 50, 52]
features into the Residuals (Motion Vectors) features by multiplying     propose an actor-action semantic segmentation task, and intro-
channel gated attention and spatial attention in a cascade manner        duces the challenging large-scale A2D dataset.","It is worth      cross-modal video understanding, Gavrilyuk et al.",2022-07-26 03:00:52+00:00,Multi-Attention Network for Compressed Video Referring Object Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weidong Chen'), arxiv.Result.Author('Dexiang Hong'), arxiv.Result.Author('Yuankai Qi'), arxiv.Result.Author('Zhenjun Han'), arxiv.Result.Author('Shuhui Wang'), arxiv.Result.Author('Laiyun Qing'), arxiv.Result.Author('Qingming Huang'), arxiv.Result.Author('Guorong Li')]","Referring video object segmentation aims to segment the object referred by a
given language expression. Existing works typically require compressed video
bitstream to be decoded to RGB frames before being segmented, which increases
computation and storage requirements and ultimately slows the inference down.
This may hamper its application in real-world computing resource limited
scenarios, such as autonomous cars and drones. To alleviate this problem, in
this paper, we explore the referring object segmentation task on compressed
videos, namely on the original video data flow. Besides the inherent difficulty
of the video referring object segmentation task itself, obtaining
discriminative representation from compressed video is also rather challenging.
To address this problem, we propose a multi-attention network which consists of
dual-path dual-attention module and a query-based cross-modal Transformer
module. Specifically, the dual-path dual-attention module is designed to
extract effective representation from compressed data in three modalities,
i.e., I-frame, Motion Vector and Residual. The query-based cross-modal
Transformer firstly models the correlation between linguistic and visual
modalities, and then the fused multi-modality features are used to guide object
queries to generate a content-aware dynamic kernel and to predict final
segmentation masks. Different from previous works, we propose to learn just one
kernel, which thus removes the complicated post mask-matching procedure of
existing methods. Extensive promising experimental results on three challenging
datasets show the effectiveness of our method compared against several
state-of-the-art methods which are proposed for processing RGB data. Source
code is available at: https://github.com/DexiangHong/MANet.",-0.31483692,-0.09860513,-0.110795125,B
9326,for further research purposes.,"The last step for annotation is to clean the dataset
step.","Since the whole 6-minute video
                                                                              dataset has some chunks without valid game rounds/turns (e.g., the
   Annotation.",2022-07-26 08:34:17+00:00,$\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jiang Bian'), arxiv.Result.Author('Qingzhong Wang'), arxiv.Result.Author('Haoyi Xiong'), arxiv.Result.Author('Jun Huang'), arxiv.Result.Author('Chen Liu'), arxiv.Result.Author('Xuhong Li'), arxiv.Result.Author('Jun Cheng'), arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Feixiang Lu'), arxiv.Result.Author('Dejing Dou')]","While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video dataset $\textbf{P$^2$A}$ for $\underline{P}$ing
$\underline{P}$ong-$\underline{A}$ction detection, which consists of 2,721
video clips collected from the broadcasting videos of professional table tennis
matches in World Table Tennis Championships and Olympiads. We work with a crew
of table tennis professionals and referees to obtain fine-grained action labels
(in 14 classes) for every ping-pong action that appeared in the dataset and
formulate two sets of action detection problems - action localization and
action recognition. We evaluate a number of commonly-seen action recognition
(e.g., TSM, TSN, Video SwinTransformer, and Slowfast) and action localization
models (e.g., BSN, BSN++, BMN, TCANet), using $\textbf{P$^2$A}$ for both
problems, under various settings. These models can only achieve 48% area under
the AR-AN curve for localization and 82% top-one accuracy for recognition since
the ping-pong actions are dense with fast-moving subjects but broadcasting
videos are with only 25 FPS. The results confirm that $\textbf{P$^2$A}$ is
still a challenging task and can be used as a benchmark for action detection
from videos.",-0.0022196323,-0.07051849,-0.18971169,C
9328,"The reconstruction         all the other comparative methods in various criteria under
loss is enlarged again in the blank place of this chart for         two different imbalanced datasets randomly formed, which
further study.","Compared with Phase 1, the training process of this       the AUC of seed 3, the MEDA LUDE algorithm outperforms
Phase Ô¨Çuctuates less and is more stable.","From the curve in the red dashed square frame,       validates the effectiveness of the proposed method in handling
it is found that the reconstruction loss rises at the beginning,    the imbalanced problem for fabric defects from DHU.",2022-07-26 08:51:47+00:00,Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yudi Zhao'), arxiv.Result.Author('Kuangrong Hao'), arxiv.Result.Author('Chaochen Gu'), arxiv.Result.Author('Bing Wei')]","To address the trade-off problem of quality-diversity for the generated
images in imbalanced classification tasks, we research on over-sampling based
methods at the feature level instead of the data level and focus on searching
the latent feature space for optimal distributions. On this basis, we propose
an iMproved Estimation Distribution Algorithm based Latent featUre Distribution
Evolution (MEDA_LUDE) algorithm, where a joint learning procedure is programmed
to make the latent features both optimized and evolved by the deep neural
networks and the evolutionary algorithm, respectively. We explore the effect of
the Large-margin Gaussian Mixture (L-GM) loss function on distribution learning
and design a specialized fitness function based on the similarities among
samples to increase diversity. Extensive experiments on benchmark based
imbalanced datasets validate the effectiveness of our proposed algorithm, which
can generate images with both quality and diversity. Furthermore, the MEDA_LUDE
algorithm is also applied to the industrial field and successfully alleviates
the imbalanced issue in fabric defect classification.",0.24624594,0.08461408,0.13960344,A
9330,"A further study by [10] indicated that
and hand pose estimation [3, 9, 28, 47].","In a meta-analysis, [26] found significant
                                                                                 correlations between perceived social verticality and for example
   Over recent decades, huge advances were made in human body-                   self-touching and gesturing.","At the same time, a large               people believe power is expressed with nonverbal cues like open
number of works investigated the prediction of high-level attributes             posture (i.e.",2022-07-26 11:24:00+00:00,Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation,cs.CV,"['cs.CV', '68T05, 68T10', 'I.5']","[arxiv.Result.Author('Michal Balazia'), arxiv.Result.Author('Philipp M√ºller'), arxiv.Result.Author('√Åkos Levente T√°nczos'), arxiv.Result.Author('August von Liechtenstein'), arxiv.Result.Author('Fran√ßois Br√©mond')]","Body language is an eye-catching social signal and its automatic analysis can
significantly advance artificial intelligence systems to understand and
actively participate in social interactions. While computer vision has made
impressive progress in low-level tasks like head and body pose estimation, the
detection of more subtle behaviors such as gesturing, grooming, or fumbling is
not well explored. In this paper we present BBSI, the first set of annotations
of complex Bodily Behaviors embedded in continuous Social Interactions in a
group setting. Based on previous work in psychology, we manually annotated 26
hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15
distinct body language classes. We present comprehensive descriptive statistics
on the resulting dataset as well as results of annotation quality evaluations.
For automatic detection of these behaviors, we adapt the Pyramid Dilated
Attention Network (PDAN), a state-of-the-art approach for human action
detection. We perform experiments using four variants of spatial-temporal
features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment
Networks, Temporal Shift Module and Swin Transformer. Results are promising and
indicate a great room for improvement in this difficult task. Representing a
key piece in the puzzle towards automatic understanding of social behavior,
BBSI is fully available to the research community.",0.06433696,0.06527059,-0.35027194,A
9331,"A further study by [10] indicated that
and hand pose estimation [3, 9, 28, 47].","In a meta-analysis, [26] found significant
                                                                                 correlations between perceived social verticality and for example
   Over recent decades, huge advances were made in human body-                   self-touching and gesturing.","At the same time, a large               people believe power is expressed with nonverbal cues like open
number of works investigated the prediction of high-level attributes             posture (i.e.",2022-07-26 11:24:00+00:00,Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation,cs.CV,"['cs.CV', '68T05, 68T10', 'I.5']","[arxiv.Result.Author('Michal Balazia'), arxiv.Result.Author('Philipp M√ºller'), arxiv.Result.Author('√Åkos Levente T√°nczos'), arxiv.Result.Author('August von Liechtenstein'), arxiv.Result.Author('Fran√ßois Br√©mond')]","Body language is an eye-catching social signal and its automatic analysis can
significantly advance artificial intelligence systems to understand and
actively participate in social interactions. While computer vision has made
impressive progress in low-level tasks like head and body pose estimation, the
detection of more subtle behaviors such as gesturing, grooming, or fumbling is
not well explored. In this paper we present BBSI, the first set of annotations
of complex Bodily Behaviors embedded in continuous Social Interactions in a
group setting. Based on previous work in psychology, we manually annotated 26
hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15
distinct body language classes. We present comprehensive descriptive statistics
on the resulting dataset as well as results of annotation quality evaluations.
For automatic detection of these behaviors, we adapt the Pyramid Dilated
Attention Network (PDAN), a state-of-the-art approach for human action
detection. We perform experiments using four variants of spatial-temporal
features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment
Networks, Temporal Shift Module and Swin Transformer. Results are promising and
indicate a great room for improvement in this difficult task. Representing a
key piece in the puzzle towards automatic understanding of social behavior,
BBSI is fully available to the research community.",0.06433696,0.06527059,-0.35027194,A
9332,"to conduct further research for more efÔ¨Åcient open-world
                                                                                                         weapon detection.","This brings about the need
                                        for action recognition and video classiÔ¨Åcation.","Index Terms‚ÄîAction Recognition, Violence Detection,
                                        Weaponized Violence Detection, Smart Cities, Deep Neural         Furthermore, spatio-temporal models such as C3D,
                                        Networks, Signal Processing                                      ConvLSTM used for violence detection, and DNNs used
                                                                                                         for weapons detection(i.e., YOLO, RCNN) consume a
                                                                  I.",2022-07-26 12:31:01+00:00,Any Object is a Potential Weapon! Weaponized Violence Detection using Salient Image,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Toluwani Aremu'), arxiv.Result.Author('Li Zhiyuan'), arxiv.Result.Author('Reem Alameeri')]","In every connected smart city around the world, CCTVs have played a pivotal
role in enforcing the safety and security of the citizens by recording unlawful
activities for the authorities to take action. To ensure the efficiency and
effectiveness of CCTVs in this domain, different DNN architectures were created
and used by researchers and developers to either detect violence or detect
weapons using bounding boxes or masks. These weapons are limited to guns,
knives, and other obvious handheld weapons. To remove these limits and detect
weapons more efficiently, non-weaponized violence footage from CCTV must be
differentiable from weaponized ones. Since there are no current datasets that
are tailored to this purpose of generalizability in weaponized violence
detection, we introduced a new dataset that contains videos depicting
weaponized violence, non-weaponized violence, and non-violent events. We also
propose a novel data-centric method that arranges video frames into salient
images while minimizing information loss for comfortable inference by SOTA
image classifiers. This was done to simplify video classification tasks and
optimize inference latency to improve sustainability in smart cities. Our
experiments show that Image Classifiers can efficiently detect and distinguish
violence with weapons from violence without weapons with performances as high
as 99\%, which are comparable with current SOTA 3D networks for action
recognition and video classification.",-0.25998813,-0.08796759,-0.0753741,B
9350,"equipping a captioning architecture with retrieval abilities, opening
                                                                         up further research in this direction.","Experimental results
other state-of-the-art approaches, surpassing them according to all      conducted on the COCO dataset demonstrate the effectiveness of
evaluation metrics.","4.5 Qualitative Results
                                                                         ACKNOWLEDGMENTS
Finally, in Fig.",2022-07-26 19:35:49+00:00,Retrieval-Augmented Transformer for Image Captioning,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Sara Sarto'), arxiv.Result.Author('Marcella Cornia'), arxiv.Result.Author('Lorenzo Baraldi'), arxiv.Result.Author('Rita Cucchiara')]","Image captioning models aim at connecting Vision and Language by providing
natural language descriptions of input images. In the past few years, the task
has been tackled by learning parametric models and proposing visual feature
extraction advancements or by modeling better multi-modal connections. In this
paper, we investigate the development of an image captioning approach with a
kNN memory, with which knowledge can be retrieved from an external corpus to
aid the generation process. Our architecture combines a knowledge retriever
based on visual similarities, a differentiable encoder, and a kNN-augmented
attention layer to predict tokens based on the past context and on text
retrieved from the external memory. Experimental results, conducted on the COCO
dataset, demonstrate that employing an explicit external memory can aid the
generation process and increase caption quality. Our work opens up new avenues
for improving image captioning models at larger scale.",-0.1299044,-0.2578084,-0.31493336,C
9351,"equipping a captioning architecture with retrieval abilities, opening
                                                                         up further research in this direction.","Experimental results
other state-of-the-art approaches, surpassing them according to all      conducted on the COCO dataset demonstrate the effectiveness of
evaluation metrics.","4.5 Qualitative Results
                                                                         ACKNOWLEDGMENTS
Finally, in Fig.",2022-07-26 19:35:49+00:00,Retrieval-Augmented Transformer for Image Captioning,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Sara Sarto'), arxiv.Result.Author('Marcella Cornia'), arxiv.Result.Author('Lorenzo Baraldi'), arxiv.Result.Author('Rita Cucchiara')]","Image captioning models aim at connecting Vision and Language by providing
natural language descriptions of input images. In the past few years, the task
has been tackled by learning parametric models and proposing visual feature
extraction advancements or by modeling better multi-modal connections. In this
paper, we investigate the development of an image captioning approach with a
kNN memory, with which knowledge can be retrieved from an external corpus to
aid the generation process. Our architecture combines a knowledge retriever
based on visual similarities, a differentiable encoder, and a kNN-augmented
attention layer to predict tokens based on the past context and on text
retrieved from the external memory. Experimental results, conducted on the COCO
dataset, demonstrate that employing an explicit external memory can aid the
generation process and increase caption quality. Our work opens up new avenues
for improving image captioning models at larger scale.",-0.1299044,-0.2578084,-0.31493336,C
9360,"We further study the impact of different object queries (e.g.,
learnable queries and constant queries) used in Multi-task SiRi.","The multi-task SiRi always performs better than single SiRi during all the re-
training periods.","The results of
the initial trained models using different quires in multi-task learning are shown
in Table 3.",2022-07-27 07:01:01+00:00,SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mengxue Qu'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Wu Liu'), arxiv.Result.Author('Qiqi Gong'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Olga Russakovsky'), arxiv.Result.Author('Yao Zhao'), arxiv.Result.Author('Yunchao Wei')]","In this paper, we investigate how to achieve better visual grounding with
modern vision-language transformers, and propose a simple yet powerful
Selective Retraining (SiRi) mechanism for this challenging task. Particularly,
SiRi conveys a significant principle to the research of visual grounding, i.e.,
a better initialized vision-language encoder would help the model converge to a
better local minimum, advancing the performance accordingly. In specific, we
continually update the parameters of the encoder as the training goes on, while
periodically re-initialize rest of the parameters to compel the model to be
better optimized based on an enhanced encoder. SiRi can significantly
outperform previous approaches on three popular benchmarks. Specifically, our
method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the
state-of-the-art approaches (training from scratch) by more than 10.21%.
Additionally, we reveal that SiRi performs surprisingly superior even with
limited training data. We also extend it to transformer-based visual grounding
models and other vision-language tasks to verify the validity.",0.07070215,-0.23569913,-0.070767626,C
9370,"We provide the code2 that generates the Polyvore-MISFITs dataset in order to encourage
           further research in the Ô¨Åeld.","‚Ä¢ We propose a methodology for generating partially mismatching outÔ¨Åts and create a new dataset called
           Polyvore-MISFITs.","‚Ä¢ We utilize fashion-speciÔ¨Åc contrastive language image pre-training (FLIP) for Ô¨Åne-tuning computer vision
           neural networks on fashion imagery.",2022-07-27 11:18:55+00:00,VICTOR: Visual Incompatibility Detection with Transformers and Fashion-specific contrastive pre-training,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Stefanos-Iordanis Papadopoulos'), arxiv.Result.Author('Christos Koutlis'), arxiv.Result.Author('Symeon Papadopoulos'), arxiv.Result.Author('Ioannis Kompatsiaris')]","In order to consider fashion outfits as aesthetically pleasing, the garments
that constitute them need to be compatible in terms of visual aspects, such as
style, category and color. With the advent and omnipresence of computer vision
deep learning models, increased interest has also emerged for the task of
visual compatibility detection with the aim to develop quality fashion outfit
recommendation systems. Previous works have defined visual compatibility as a
binary classification task with items in a garment being considered as fully
compatible or fully incompatible. However, this is not applicable to Outfit
Maker applications where users create their own outfits and need to know which
specific items may be incompatible with the rest of the outfit. To address
this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is
optimized for two tasks: 1) overall compatibility as regression and 2) the
detection of mismatching items. Unlike previous works that either rely on
feature extraction from ImageNet-pretrained models or by end-to-end fine
tuning, we utilize fashion-specific contrastive language-image pre-training for
fine tuning computer vision neural networks on fashion imagery. Moreover, we
build upon the Polyvore outfit benchmark to generate partially mismatching
outfits, creating a new dataset termed Polyvore-MISFITs, that is used to train
VICTOR. A series of ablation and comparative analyses show that the proposed
architecture can compete and even surpass the current state-of-the-art on
Polyvore datasets while reducing the instance-wise floating operations by 88%,
striking a balance between high performance and efficiency.",-0.16596502,-0.26096728,-0.010172217,C
9371,"We provide the code5 for generating the Polyvore-MISFITs for
reproducibility and in order to encourage further research in the Ô¨Åeld.","However, since each category contains thousands of items, we expect that random selections will more

                                                                      6
VICTOR: Visual Incompatibility Detection  A PREPRINT

often than not lead to incompatible combinations.","4.3 Implementation Details

We perform an ablation and comparative analysis and in order to distinguish different versions of VICTOR, we
denote the training task in square brackets.",2022-07-27 11:18:55+00:00,VICTOR: Visual Incompatibility Detection with Transformers and Fashion-specific contrastive pre-training,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Stefanos-Iordanis Papadopoulos'), arxiv.Result.Author('Christos Koutlis'), arxiv.Result.Author('Symeon Papadopoulos'), arxiv.Result.Author('Ioannis Kompatsiaris')]","In order to consider fashion outfits as aesthetically pleasing, the garments
that constitute them need to be compatible in terms of visual aspects, such as
style, category and color. With the advent and omnipresence of computer vision
deep learning models, increased interest has also emerged for the task of
visual compatibility detection with the aim to develop quality fashion outfit
recommendation systems. Previous works have defined visual compatibility as a
binary classification task with items in a garment being considered as fully
compatible or fully incompatible. However, this is not applicable to Outfit
Maker applications where users create their own outfits and need to know which
specific items may be incompatible with the rest of the outfit. To address
this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is
optimized for two tasks: 1) overall compatibility as regression and 2) the
detection of mismatching items. Unlike previous works that either rely on
feature extraction from ImageNet-pretrained models or by end-to-end fine
tuning, we utilize fashion-specific contrastive language-image pre-training for
fine tuning computer vision neural networks on fashion imagery. Moreover, we
build upon the Polyvore outfit benchmark to generate partially mismatching
outfits, creating a new dataset termed Polyvore-MISFITs, that is used to train
VICTOR. A series of ablation and comparative analyses show that the proposed
architecture can compete and even surpass the current state-of-the-art on
Polyvore datasets while reducing the instance-wise floating operations by 88%,
striking a balance between high performance and efficiency.",0.18627918,-0.18056396,-0.24985927,A
9372,"We provide the code3 for generating the Polyvore-MISFITs for
reproducibility and in order to encourage further research in the Ô¨Åeld.","However, since each category contains thousands of items, we expect that random selections will more
often than not lead to incompatible combinations.","4.3 Implementation Details

We perform an ablation and comparative analysis and in order to distinguish different versions of VICTOR, we
denote the training task in square brackets.",2022-07-27 11:18:55+00:00,VICTOR: Visual Incompatibility Detection with Transformers and Fashion-specific contrastive pre-training,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Stefanos-Iordanis Papadopoulos'), arxiv.Result.Author('Christos Koutlis'), arxiv.Result.Author('Symeon Papadopoulos'), arxiv.Result.Author('Ioannis Kompatsiaris')]","For fashion outfits to be considered aesthetically pleasing, the garments
that constitute them need to be compatible in terms of visual aspects, such as
style, category and color. Previous works have defined visual compatibility as
a binary classification task with items in a garment being considered as fully
compatible or fully incompatible. However, this is not applicable to Outfit
Maker applications where users create their own outfits and need to know which
specific items may be incompatible with the rest of the outfit. To address
this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is
optimized for two tasks: 1) overall compatibility as regression and 2) the
detection of mismatching items and utilize fashion-specific contrastive
language-image pre-training for fine tuning computer vision neural networks on
fashion imagery. We build upon the Polyvore outfit benchmark to generate
partially mismatching outfits, creating a new dataset termed Polyvore-MISFITs,
that is used to train VICTOR. A series of ablation and comparative analyses
show that the proposed architecture can compete and even surpass the current
state-of-the-art on Polyvore datasets while reducing the instance-wise floating
operations by 88%, striking a balance between high performance and efficiency.
We release our code at
https://github.com/stevejpapad/Visual-InCompatibility-Transformer",0.34280407,-0.19294688,-0.23950383,A
9379,We further study different settings of pixel shifting by varying shift ranges.,"By using color transfer, the result further improves to 83.8%, suggesting
color transfer is complementary to our method.","Intuitively, larger shifts
leads to greater differences between two views.",2022-07-27 14:04:22+00:00,Contrastive Masked Autoencoders are Stronger Vision Learners,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhicheng Huang'), arxiv.Result.Author('Xiaojie Jin'), arxiv.Result.Author('Chengze Lu'), arxiv.Result.Author('Qibin Hou'), arxiv.Result.Author('Ming-Ming Cheng'), arxiv.Result.Author('Dongmei Fu'), arxiv.Result.Author('Xiaohui Shen'), arxiv.Result.Author('Jiashi Feng')]","Masked image modeling (MIM) has achieved promising results on various vision
tasks. However, the limited discriminability of learned representation
manifests there is still plenty to go for making a stronger vision learner.
Towards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new
self-supervised pre-training method for learning more comprehensive and capable
vision representations. By elaboratively unifying contrastive learning (CL) and
masked image model (MIM) through novel designs, CMAE leverages their respective
advantages and learns representations with both strong instance
discriminability and local perceptibility. Specifically, CMAE consists of two
branches where the online branch is an asymmetric encoder-decoder and the
target branch is a momentum updated encoder. During training, the online
encoder reconstructs original images from latent representations of masked
images to learn holistic features. The target encoder, fed with the full
images, enhances the feature discriminability via contrastive learning with its
online counterpart. To make CL compatible with MIM, CMAE introduces two new
components, i.e. pixel shift for generating plausible positive views and
feature decoder for complementing features of contrastive pairs. Thanks to
these novel designs, CMAE effectively improves the representation quality and
transfer performance over its MIM counterpart. CMAE achieves the
state-of-the-art performance on highly competitive benchmarks of image
classification, semantic segmentation and object detection. Notably, CMAE-Base
achieves $85.3\%$ top-1 accuracy on ImageNet and $52.5\%$ mIoU on ADE20k,
surpassing previous best results by $0.7\%$ and $1.8\%$ respectively. Codes
will be made publicly available.",-0.07905079,0.24160534,0.13240713,B
9380,We further study different settings of pixel shifting by varying shift ranges.,"By using color transfer, the result further improves to 83.8%, suggesting
color transfer is complementary to our method.","Intuitively, larger shifts
leads to greater differences between two views.",2022-07-27 14:04:22+00:00,Contrastive Masked Autoencoders are Stronger Vision Learners,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhicheng Huang'), arxiv.Result.Author('Xiaojie Jin'), arxiv.Result.Author('Chengze Lu'), arxiv.Result.Author('Qibin Hou'), arxiv.Result.Author('Ming-Ming Cheng'), arxiv.Result.Author('Dongmei Fu'), arxiv.Result.Author('Xiaohui Shen'), arxiv.Result.Author('Jiashi Feng')]","Masked image modeling (MIM) has achieved promising results on various vision
tasks. However, the limited discriminability of learned representation
manifests there is still plenty to go for making a stronger vision learner.
Towards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new
self-supervised pre-training method for learning more comprehensive and capable
vision representations. By elaboratively unifying contrastive learning (CL) and
masked image model (MIM) through novel designs, CMAE leverages their respective
advantages and learns representations with both strong instance
discriminability and local perceptibility. Specifically, CMAE consists of two
branches where the online branch is an asymmetric encoder-decoder and the
target branch is a momentum updated encoder. During training, the online
encoder reconstructs original images from latent representations of masked
images to learn holistic features. The target encoder, fed with the full
images, enhances the feature discriminability via contrastive learning with its
online counterpart. To make CL compatible with MIM, CMAE introduces two new
components, i.e. pixel shift for generating plausible positive views and
feature decoder for complementing features of contrastive pairs. Thanks to
these novel designs, CMAE effectively improves the representation quality and
transfer performance over its MIM counterpart. CMAE achieves the
state-of-the-art performance on highly competitive benchmarks of image
classification, semantic segmentation and object detection. Notably, CMAE-Base
achieves $85.3\%$ top-1 accuracy on ImageNet and $52.5\%$ mIoU on ADE20k,
surpassing previous best results by $0.7\%$ and $1.8\%$ respectively. Codes
will be made publicly available at \url{https://github.com/ZhichengHuang/CMAE}.",-0.07905079,0.24160534,0.13240713,B
9394,"We will release our code and pre-trained model for further research
                                                 at https://virtualhumans.mpi-inf.mpg.de/posendf/.","Furthermore, we show that it can be used to generate more
                                                 diverse poses by random sampling and projection than VAE-based meth-
                                                 ods.","1 Introduction

                                        Realistic and accurate human motion capture and generation is essential for un-
                                        derstanding human behavior and human interaction in the scene [23,41,68,67,9].",2022-07-27 21:46:47+00:00,Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields,cs.CV,['cs.CV'],"[arxiv.Result.Author('Garvita Tiwari'), arxiv.Result.Author('Dimitrije Antic'), arxiv.Result.Author('Jan Eric Lenssen'), arxiv.Result.Author('Nikolaos Sarafianos'), arxiv.Result.Author('Tony Tung'), arxiv.Result.Author('Gerard Pons-Moll')]","We present Pose-NDF, a continuous model for plausible human poses based on
neural distance fields (NDFs). Pose or motion priors are important for
generating realistic new poses and for reconstructing accurate poses from noisy
or partial observations. Pose-NDF learns a manifold of plausible poses as the
zero level set of a neural implicit function, extending the idea of modeling
implicit surfaces in 3D to the high-dimensional domain SO(3)^K, where a human
pose is defined by a single data point, represented by K quaternions. The
resulting high-dimensional implicit function can be differentiated with respect
to the input poses and thus can be used to project arbitrary poses onto the
manifold by using gradient descent on the set of 3-dimensional hyperspheres. In
contrast to previous VAE-based human pose priors, which transform the pose
space into a Gaussian distribution, we model the actual pose manifold,
preserving the distances between poses. We demonstrate that PoseNDF outperforms
existing state-of-the-art methods as a prior in various downstream tasks,
ranging from denoising real-world human mocap data, pose recovery from occluded
data to 3D pose reconstruction from images. Furthermore, we show that it can be
used to generate more diverse poses by random sampling and projection than
VAE-based methods.",-0.18402502,0.14173737,-0.10252561,B
9405,Code is made available to facilitate further research.,"We conducted extensive experiments on CARLA
                                                  benchmarks, where our model outperforms prior methods, ranking #1 on the public
                                                  CARLA Leaderboard.","Keywords: Autonomous driving, sensor fusion, transformer, safety

                                        1 Introduction

                                        Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns.",2022-07-28 11:36:21+00:00,Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Hao Shao'), arxiv.Result.Author('Letian Wang'), arxiv.Result.Author('RuoBing Chen'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Yu Liu')]","Large-scale deployment of autonomous vehicles has been continually delayed
due to safety concerns. On the one hand, comprehensive scene understanding is
indispensable, a lack of which would result in vulnerability to rare but
complex traffic situations, such as the sudden emergence of unknown objects.
However, reasoning from a global context requires access to sensors of multiple
types and adequate fusion of multi-modal sensor signals, which is difficult to
achieve. On the other hand, the lack of interpretability in learning models
also hampers the safety with unverifiable failure causes. In this paper, we
propose a safety-enhanced autonomous driving framework, named Interpretable
Sensor Fusion Transformer(InterFuser), to fully process and fuse information
from multi-modal multi-view sensors for achieving comprehensive scene
understanding and adversarial event detection. Besides, intermediate
interpretable features are generated from our framework, which provide more
semantics and are exploited to better constrain actions to be within the safe
sets. We conducted extensive experiments on CARLA benchmarks, where our model
outperforms prior methods, ranking the first on the public CARLA Leaderboard.
Our code will be made available at https://github.com/opendilab/InterFuser",0.02734832,0.07105319,-0.15229027,B
9406,Code is made available to facilitate further research.,"We conducted extensive
                                        experiments on CARLA benchmarks, where our model outperforms prior methods, ranking #1 on
                                        the public CARLA Leaderboard.",Safe and reliable driving necessitates comprehensive scene understanding.,2022-07-28 11:36:21+00:00,Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Hao Shao'), arxiv.Result.Author('Letian Wang'), arxiv.Result.Author('RuoBing Chen'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Yu Liu')]","Large-scale deployment of autonomous vehicles has been continually delayed
due to safety concerns. On the one hand, comprehensive scene understanding is
indispensable, a lack of which would result in vulnerability to rare but
complex traffic situations, such as the sudden emergence of unknown objects.
However, reasoning from a global context requires access to sensors of multiple
types and adequate fusion of multi-modal sensor signals, which is difficult to
achieve. On the other hand, the lack of interpretability in learning models
also hampers the safety with unverifiable failure causes. In this paper, we
propose a safety-enhanced autonomous driving framework, named Interpretable
Sensor Fusion Transformer(InterFuser), to fully process and fuse information
from multi-modal multi-view sensors for achieving comprehensive scene
understanding and adversarial event detection. Besides, intermediate
interpretable features are generated from our framework, which provide more
semantics and are exploited to better constrain actions to be within the safe
sets. We conducted extensive experiments on CARLA benchmarks, where our model
outperforms prior methods, ranking the first on the public CARLA Leaderboard.
Our code will be made available at https://github.com/opendilab/InterFuser",-0.11072766,0.033245526,-0.22024551,B
9407,Code is made available to facilitate further research.,"We conducted extensive experiments on CARLA
                                                 benchmarks, where our model outperforms prior methods, ranking #1 on the public
                                                 CARLA Leaderboard.","Keywords: Autonomous driving, sensor fusion, transformer, safety

                                       1 Introduction

                                       Recently, rapid progress has been witnessed in the Ô¨Åeld of autonomous driving, while the scalable
                                       and practical deployment of autonomous vehicles on public roads is still far from feasible.",2022-07-28 11:36:21+00:00,Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Hao Shao'), arxiv.Result.Author('Letian Wang'), arxiv.Result.Author('RuoBing Chen'), arxiv.Result.Author('Hongsheng Li'), arxiv.Result.Author('Yu Liu')]","Large-scale deployment of autonomous vehicles has been continually delayed
due to safety concerns. On the one hand, comprehensive scene understanding is
indispensable, a lack of which would result in vulnerability to rare but
complex traffic situations, such as the sudden emergence of unknown objects.
However, reasoning from a global context requires access to sensors of multiple
types and adequate fusion of multi-modal sensor signals, which is difficult to
achieve. On the other hand, the lack of interpretability in learning models
also hampers the safety with unverifiable failure causes. In this paper, we
propose a safety-enhanced autonomous driving framework, named Interpretable
Sensor Fusion Transformer(InterFuser), to fully process and fuse information
from multi-modal multi-view sensors for achieving comprehensive scene
understanding and adversarial event detection. Besides, intermediate
interpretable features are generated from our framework, which provide more
semantics and are exploited to better constrain actions to be within the safe
sets. We conducted extensive experiments on CARLA benchmarks, where our model
outperforms prior methods, ranking the first on the public CARLA Leaderboard.
Our code will be made available at https://github.com/opendilab/InterFuser",-0.0045348597,0.11367208,-0.1307421,B
9411,framework and facilitate the further research of SOD?,"This inspires us to think: can we build a                                        images, to name a few, we provide a systematic survey of
large-scale dataset, where the objects of multiple categories                                      small object detection and an understandable taxonomy that
have very limited sizes, to serve as a benchmark that can                                          organizes SOD approaches into six major categories based
be adopted to verify the design of small object detection                                          on the techniques used.",2.,2022-07-28 14:02:18+00:00,Towards Large-Scale Small Object Detection: Survey and Benchmarks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Gong Cheng'), arxiv.Result.Author('Xiang Yuan'), arxiv.Result.Author('Xiwen Yao'), arxiv.Result.Author('Kebing Yan'), arxiv.Result.Author('Qinghua Zeng'), arxiv.Result.Author('Junwei Han')]","With the rise of deep convolutional neural networks, object detection has
achieved prominent advances in past years. However, such prosperity could not
camouflage the unsatisfactory situation of Small Object Detection (SOD), one of
the notoriously challenging tasks in computer vision, owing to the poor visual
appearance and noisy representation caused by the intrinsic structure of small
targets. In addition, large-scale dataset for benchmarking small object
detection methods remains a bottleneck. In this paper, we first conduct a
thorough review of small object detection. Then, to catalyze the development of
SOD, we construct two large-scale Small Object Detection dAtasets (SODA),
SODA-D and SODA-A, which focus on the Driving and Aerial scenarios
respectively. SODA-D includes 24704 high-quality traffic images and 277596
instances of 9 categories. For SODA-A, we harvest 2510 high-resolution aerial
images and annotate 800203 instances over 9 classes. The proposed datasets, as
we know, are the first-ever attempt to large-scale benchmarks with a vast
collection of exhaustively annotated instances tailored for multi-category SOD.
Finally, we evaluate the performance of mainstream methods on SODA. We expect
the released benchmarks could facilitate the development of SOD and spawn more
breakthroughs in this field. Datasets and codes will be available soon at:
\url{https://shaunyuan22.github.io/SODA}.",-0.2071845,0.019661663,-0.09733643,B
9412,framework and facilitate the further research of SOD?,"This inspires us to think: can we build a                                        images, to name a few, we provide a systematic survey of
large-scale dataset, where the objects of multiple categories                                      small object detection and an understandable taxonomy that
have very limited sizes, to serve as a benchmark that can                                          organizes SOD approaches into six major categories based
be adopted to verify the design of small object detection                                          on the techniques used.",2.,2022-07-28 14:02:18+00:00,Towards Large-Scale Small Object Detection: Survey and Benchmarks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Gong Cheng'), arxiv.Result.Author('Xiang Yuan'), arxiv.Result.Author('Xiwen Yao'), arxiv.Result.Author('Kebing Yan'), arxiv.Result.Author('Qinghua Zeng'), arxiv.Result.Author('Junwei Han')]","With the rise of deep convolutional neural networks, object detection has
achieved prominent advances in past years. However, such prosperity could not
camouflage the unsatisfactory situation of Small Object Detection (SOD), one of
the notoriously challenging tasks in computer vision, owing to the poor visual
appearance and noisy representation caused by the intrinsic structure of small
targets. In addition, large-scale dataset for benchmarking small object
detection methods remains a bottleneck. In this paper, we first conduct a
thorough review of small object detection. Then, to catalyze the development of
SOD, we construct two large-scale Small Object Detection dAtasets (SODA),
SODA-D and SODA-A, which focus on the Driving and Aerial scenarios
respectively. SODA-D includes 24704 high-quality traffic images and 277596
instances of 9 categories. For SODA-A, we harvest 2510 high-resolution aerial
images and annotate 800203 instances over 9 classes. The proposed datasets, as
we know, are the first-ever attempt to large-scale benchmarks with a vast
collection of exhaustively annotated instances tailored for multi-category SOD.
Finally, we evaluate the performance of mainstream methods on SODA. We expect
the released benchmarks could facilitate the development of SOD and spawn more
breakthroughs in this field. Datasets and codes will be available soon at:
\url{https://shaunyuan22.github.io/SODA}.",-0.2071845,0.019661663,-0.09733643,B
9413,"As a result,
framework and facilitate the further research of SOD?","This inspires us to think: can we build a                                                  hundreds of literature related to SOD task which covers a
large-scale dataset, where the objects of multiple categories                                                broad spectrum of research Ô¨Åelds, including face detection,
have very limited sizes, to serve as a benchmark that can                                                    pedestrian detection, trafÔ¨Åc sign detection, vehicle detection,
be adopted to verify the design of small object detection                                                    object detection in aerial images, to name a few.","we provide a systematic survey of small object detection
                                                                                                             and an understandable and highly structured taxonomy,
    Taking the aforementioned problems into account, we                                                      which organizes SOD approaches into six major categories
construct two large-scale Small Object Detection dAtasets                                                    based on the techniques involved and is radically differ-
(SODA), SODA-D and SODA-A, which focus on the Driving                                                        ent from previous ones.",2022-07-28 14:02:18+00:00,Towards Large-Scale Small Object Detection: Survey and Benchmarks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Gong Cheng'), arxiv.Result.Author('Xiang Yuan'), arxiv.Result.Author('Xiwen Yao'), arxiv.Result.Author('Kebing Yan'), arxiv.Result.Author('Qinghua Zeng'), arxiv.Result.Author('Junwei Han')]","With the rise of deep convolutional neural networks, object detection has
achieved prominent advances in past years. However, such prosperity could not
camouflage the unsatisfactory situation of Small Object Detection (SOD), one of
the notoriously challenging tasks in computer vision, owing to the poor visual
appearance and noisy representation caused by the intrinsic structure of small
targets. In addition, large-scale dataset for benchmarking small object
detection methods remains a bottleneck. In this paper, we first conduct a
thorough review of small object detection. Then, to catalyze the development of
SOD, we construct two large-scale Small Object Detection dAtasets (SODA),
SODA-D and SODA-A, which focus on the Driving and Aerial scenarios
respectively. SODA-D includes 24828 high-quality traffic images and 278433
instances of nine categories. For SODA-A, we harvest 2513 high resolution
aerial images and annotate 872069 instances over nine classes. The proposed
datasets, as we know, are the first-ever attempt to large-scale benchmarks with
a vast collection of exhaustively annotated instances tailored for
multi-category SOD. Finally, we evaluate the performance of mainstream methods
on SODA. We expect the released benchmarks could facilitate the development of
SOD and spawn more breakthroughs in this field. Datasets and codes are
available at: \url{https://shaunyuan22.github.io/SODA}.",-0.25833562,-0.03691788,-0.08469921,B
9443,"Fite, Frte,
ence images dataset, to facilitate further research and perfor-   Fist and Frst are the texture features and the structure fea-
mance evaluation of reference-guided image inpainting.",3) We build a refer-      where G(¬∑) denotes the feature alignment module.,tures from the input and reference images.,2022-07-29 06:26:03+00:00,Reference-Guided Texture and Structure Inference for Image Inpainting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Taorong Liu'), arxiv.Result.Author('Liang Liao'), arxiv.Result.Author('Zheng Wang'), arxiv.Result.Author(""Shin'ichi Satoh"")]","Existing learning-based image inpainting methods are still in challenge when
facing complex semantic environments and diverse hole patterns. The prior
information learned from the large scale training data is still insufficient
for these situations. Reference images captured covering the same scenes share
similar texture and structure priors with the corrupted images, which offers
new prospects for the image inpainting tasks. Inspired by this, we first build
a benchmark dataset containing 10K pairs of input and reference images for
reference-guided inpainting. Then we adopt an encoder-decoder structure to
separately infer the texture and structure features of the input image
considering their pattern discrepancy of texture and structure during
inpainting. A feature alignment module is further designed to refine these
features of the input image with the guidance of a reference image. Both
quantitative and qualitative evaluations demonstrate the superiority of our
method over the state-of-the-art methods in terms of completing complex holes.",-0.10792585,0.1296456,0.028203221,C
9445,"5.2 Encoder Output Representations

With the above-mentioned Ô¨Åndings, it is important to further study the representations that emerge when
pretraining with a classiÔ¨Åcation task, a segmentation task and self-supervised tasks.","This qualitative analysis underlines the reasoning
that the classiÔ¨Åcation-pretrained encoder is focussed on higher-level image characteristics.","Therefore, we compare

                                8
Figure 6: Segmentation predictions on the VOC cat dataset after Ô¨Ånetuning on 131 training samples.",2022-07-29 07:02:05+00:00,Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jonas Dippel'), arxiv.Result.Author('Matthias Lenga'), arxiv.Result.Author('Thomas Goerttler'), arxiv.Result.Author('Klaus Obermayer'), arxiv.Result.Author('Johannes H√∂hne')]","It is common practice to reuse models initially trained on different data to
increase downstream task performance. Especially in the computer vision domain,
ImageNet-pretrained weights have been successfully used for various tasks. In
this work, we investigate the impact of transfer learning for segmentation
problems, being pixel-wise classification problems that can be tackled with
encoder-decoder architectures. We find that transfer learning the decoder does
not help downstream segmentation tasks, while transfer learning the encoder is
truly beneficial. We demonstrate that pretrained weights for a decoder may
yield faster convergence, but they do not improve the overall model performance
as one can obtain equivalent results with randomly initialized decoders.
However, we show that it is more effective to reuse encoder weights trained on
a segmentation or reconstruction task than reusing encoder weights trained on
classification tasks. This finding implicates that using ImageNet-pretrained
encoders for downstream segmentation problems is suboptimal. We also propose a
contrastive self-supervised approach with multiple self-reconstruction tasks,
which provides encoders that are suitable for transfer learning in segmentation
problems in the absence of segmentation labels.",-0.12418487,-0.22999957,0.07474983,C
9464,"5.4 Failed Cases
                                                                                             We hope this work will spur further research on multi-modal
Fig.","Promising results on 6 benchmark
                                                                                          datasets verify the effectiveness of our proposed ProTrack.",9 shows failed cases of our tracker.,2022-07-29 09:35:02+00:00,Prompting for Multi-Modal Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinyu Yang'), arxiv.Result.Author('Zhe Li'), arxiv.Result.Author('Feng Zheng'), arxiv.Result.Author('Ale≈° Leonardis'), arxiv.Result.Author('Jingkuan Song')]","Multi-modal tracking gains attention due to its ability to be more accurate
and robust in complex scenarios compared to traditional RGB-based tracking. Its
key lies in how to fuse multi-modal data and reduce the gap between modalities.
However, multi-modal tracking still severely suffers from data deficiency, thus
resulting in the insufficient learning of fusion modules. Instead of building
such a fusion module, in this paper, we provide a new perspective on
multi-modal tracking by attaching importance to the multi-modal visual prompts.
We design a novel multi-modal prompt tracker (ProTrack), which can transfer the
multi-modal inputs to a single modality by the prompt paradigm. By best
employing the tracking ability of pre-trained RGB trackers learning at scale,
our ProTrack can achieve high-performance multi-modal tracking by only altering
the inputs, even without any extra training on multi-modal data. Extensive
experiments on 5 benchmark datasets demonstrate the effectiveness of the
proposed ProTrack.",0.075298235,0.15202515,-0.19171388,A
9465,"5.4 Failed Cases
                                                                                             We hope this work will spur further research on multi-modal
Fig.","Promising results on 6 benchmark
                                                                                          datasets verify the effectiveness of our proposed ProTrack.",9 shows failed cases of our tracker.,2022-07-29 09:35:02+00:00,Prompting for Multi-Modal Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinyu Yang'), arxiv.Result.Author('Zhe Li'), arxiv.Result.Author('Feng Zheng'), arxiv.Result.Author('Ale≈° Leonardis'), arxiv.Result.Author('Jingkuan Song')]","Multi-modal tracking gains attention due to its ability to be more accurate
and robust in complex scenarios compared to traditional RGB-based tracking. Its
key lies in how to fuse multi-modal data and reduce the gap between modalities.
However, multi-modal tracking still severely suffers from data deficiency, thus
resulting in the insufficient learning of fusion modules. Instead of building
such a fusion module, in this paper, we provide a new perspective on
multi-modal tracking by attaching importance to the multi-modal visual prompts.
We design a novel multi-modal prompt tracker (ProTrack), which can transfer the
multi-modal inputs to a single modality by the prompt paradigm. By best
employing the tracking ability of pre-trained RGB trackers learning at scale,
our ProTrack can achieve high-performance multi-modal tracking by only altering
the inputs, even without any extra training on multi-modal data. Extensive
experiments on 5 benchmark datasets demonstrate the effectiveness of the
proposed ProTrack.",0.075298235,0.15202515,-0.19171388,A
9486,"We hope this work
Note that only methods whose parameter sizes were reported         will spark further research beyond the realms of prevalent
in previous literature are presented.",16.           our method out of the Ô¨Åeld of HCTR.,It can be observed that      CTC/attention-based methods.,2022-07-29 17:30:43+00:00,Recognition of Handwritten Chinese Text by Segmentation: A Segment-annotation-free Approach,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dezhi Peng'), arxiv.Result.Author('Lianwen Jin'), arxiv.Result.Author('Weihong Ma'), arxiv.Result.Author('Canyu Xie'), arxiv.Result.Author('Hesuo Zhang'), arxiv.Result.Author('Shenggao Zhu'), arxiv.Result.Author('Jing Li')]","Online and offline handwritten Chinese text recognition (HTCR) has been
studied for decades. Early methods adopted oversegmentation-based strategies
but suffered from low speed, insufficient accuracy, and high cost of character
segmentation annotations. Recently, segmentation-free methods based on
connectionist temporal classification (CTC) and attention mechanism, have
dominated the field of HCTR. However, people actually read text character by
character, especially for ideograms such as Chinese. This raises the question:
are segmentation-free strategies really the best solution to HCTR? To explore
this issue, we propose a new segmentation-based method for recognizing
handwritten Chinese text that is implemented using a simple yet efficient fully
convolutional network. A novel weakly supervised learning method is proposed to
enable the network to be trained using only transcript annotations; thus, the
expensive character segmentation annotations required by previous
segmentation-based methods can be avoided. Owing to the lack of context
modeling in fully convolutional networks, we propose a contextual
regularization method to integrate contextual information into the network
during the training stage, which can further improve the recognition
performance. Extensive experiments conducted on four widely used benchmarks,
namely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our
method significantly surpasses existing methods on both online and offline
HCTR, and exhibits a considerably higher inference speed than
CTC/attention-based approaches.",0.27036923,0.019792918,-0.13683198,A
9487,"These     and Delteil, 2019; Carbonell et al., 2019; Yang et al., 2018;
                                        experimental results may spark further research beyond the    Xie et al., 2019a; Ma et al., 2020; Moysset et al., 2017;
                                        realms of existing methods based on connectionist temporal    Wigington et al., 2018; Tensmeyer and Wigington, 2019;
                                        classiÔ¨Åcation or attention.","One
                                        demonstrate the superiority of PageNet over existing weakly   category of page-level methods (Huang et al., 2019; Chung
                                        supervised and fully supervised page-level methods.","The source code is available at   Yang et al., 2018; Liu et al., 2021; Feng et al., 2021;
                                        https://github.com/shannanyinxiang/PageNet.",2022-07-29 17:47:45+00:00,PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dezhi Peng'), arxiv.Result.Author('Lianwen Jin'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Canjie Luo'), arxiv.Result.Author('Songxuan Lai')]","Handwritten Chinese text recognition (HCTR) has been an active research topic
for decades. However, most previous studies solely focus on the recognition of
cropped text line images, ignoring the error caused by text line detection in
real-world applications. Although some approaches aimed at page-level text
recognition have been proposed in recent years, they either are limited to
simple layouts or require very detailed annotations including expensive
line-level and even character-level bounding boxes. To this end, we propose
PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and
recognizes characters and predicts the reading order between them, which is
more robust and flexible when dealing with complex layouts including
multi-directional and curved text lines. Utilizing the proposed weakly
supervised learning framework, PageNet requires only transcripts to be
annotated for real data; however, it can still output detection and recognition
results at both the character and line levels, avoiding the labor and cost of
labeling bounding boxes of characters and text lines. Extensive experiments
conducted on five datasets demonstrate the superiority of PageNet over existing
weakly supervised and fully supervised page-level methods. These experimental
results may spark further research beyond the realms of existing methods based
on connectionist temporal classification or attention. The source code is
available at https://github.com/shannanyinxiang/PageNet.",0.110665396,-0.22791797,-0.1161229,C
9488,"International Journal of
scenarios such as end-to-end scene text recognition, which                 Computer Vision 129(6):1972‚Äì1992
is still an open problem that deserves further study.",It may be less effective for other complex               for multi-orientation scene text detection.,"Luo C, Lin Q, Liu Y, Jin L, Shen C (2021) Separating content from
5 Conclusion                                                               style using adversarial learning for recognizing text in the wild.",2022-07-29 17:47:45+00:00,PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dezhi Peng'), arxiv.Result.Author('Lianwen Jin'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Canjie Luo'), arxiv.Result.Author('Songxuan Lai')]","Handwritten Chinese text recognition (HCTR) has been an active research topic
for decades. However, most previous studies solely focus on the recognition of
cropped text line images, ignoring the error caused by text line detection in
real-world applications. Although some approaches aimed at page-level text
recognition have been proposed in recent years, they either are limited to
simple layouts or require very detailed annotations including expensive
line-level and even character-level bounding boxes. To this end, we propose
PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and
recognizes characters and predicts the reading order between them, which is
more robust and flexible when dealing with complex layouts including
multi-directional and curved text lines. Utilizing the proposed weakly
supervised learning framework, PageNet requires only transcripts to be
annotated for real data; however, it can still output detection and recognition
results at both the character and line levels, avoiding the labor and cost of
labeling bounding boxes of characters and text lines. Extensive experiments
conducted on five datasets demonstrate the superiority of PageNet over existing
weakly supervised and fully supervised page-level methods. These experimental
results may spark further research beyond the realms of existing methods based
on connectionist temporal classification or attention. The source code is
available at https://github.com/shannanyinxiang/PageNet.",-0.2740007,-0.18763825,-0.1704295,C
9505,"We hope this
                                                                       work can offer good insights and inspire further researches
                                                                       in few-shot object detection and other related topics.","It can also be easily extended to
                                                                       other instance-level few-shot learning tasks.","ACKNOWLEDGMENT

                                                                       This research is supported by the Ministry of Educa-
                                                                       tion, Singapore, under its Academic Research Fund Tier 1
                                                                       (RG94/20).",2022-07-30 13:46:07+00:00,Meta-DETR: Image-Level Few-Shot Detection with Inter-Class Correlation Exploitation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Gongjie Zhang'), arxiv.Result.Author('Zhipeng Luo'), arxiv.Result.Author('Kaiwen Cui'), arxiv.Result.Author('Shijian Lu'), arxiv.Result.Author('Eric P. Xing')]","Few-shot object detection has been extensively investigated by incorporating
meta-learning into region-based detection frameworks. Despite its success, the
said paradigm is still constrained by several factors, such as (i) low-quality
region proposals for novel classes and (ii) negligence of the inter-class
correlation among different classes. Such limitations hinder the generalization
of base-class knowledge for the detection of novel-class objects. In this work,
we design Meta-DETR, which (i) is the first image-level few-shot detector, and
(ii) introduces a novel inter-class correlational meta-learning strategy to
capture and leverage the correlation among different classes for robust and
accurate few-shot object detection. Meta-DETR works entirely at image level
without any region proposals, which circumvents the constraint of inaccurate
proposals in prevalent few-shot detection frameworks. In addition, the
introduced correlational meta-learning enables Meta-DETR to simultaneously
attend to multiple support classes within a single feedforward, which allows to
capture the inter-class correlation among different classes, thus significantly
reducing the misclassification over similar classes and enhancing knowledge
generalization to novel classes. Experiments over multiple few-shot object
detection benchmarks show that the proposed Meta-DETR outperforms
state-of-the-art methods by large margins. The implementation codes are
available at https://github.com/ZhangGongjie/Meta-DETR.",-0.26512977,-0.102427855,-0.09617011,C
9512,"In addition, we compare the performance of using 1, 2 and 3 DDT layers
(DDT-1, DDT-2 and DDT-3) to further study the effects of the number of DDT
layers.","4.3 Ablation Study and Analysis

Ablation Study To validate the effects of the proposed two modules, i.e., GP-
based kernel learning (GP-KL) module and DDT module, we conduct ablation
studies.",The mIoU results on the PASCAL-5i dataset are presented in Table 4.,2022-07-30 20:41:38+00:00,Doubly Deformable Aggregation of Covariance Matrices for Few-shot Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhitong Xiong'), arxiv.Result.Author('Haopeng Li'), arxiv.Result.Author('Xiao Xiang Zhu')]","Training semantic segmentation models with few annotated samples has great
potential in various real-world applications. For the few-shot segmentation
task, the main challenge is how to accurately measure the semantic
correspondence between the support and query samples with limited training
data. To address this problem, we propose to aggregate the learnable covariance
matrices with a deformable 4D Transformer to effectively predict the
segmentation map. Specifically, in this work, we first devise a novel hard
example mining mechanism to learn covariance kernels for the Gaussian process.
The learned covariance kernel functions have great advantages over existing
cosine similarity-based methods in correspondence measurement. Based on the
learned covariance kernels, an efficient doubly deformable 4D Transformer
module is designed to adaptively aggregate feature similarity maps into
segmentation results. By combining these two designs, the proposed method can
not only set new state-of-the-art performance on public benchmarks, but also
converge extremely faster than existing methods. Experiments on three public
datasets have demonstrated the effectiveness of our method.",0.28255355,-0.059335515,0.10602932,A
9523,"2402‚Äì2410, 2016.
can thoroughly understand the background knowledge before
to conducting further research.","22, pp.","We also revisited the existing       [10] D. S. W. Ting, C. Y.-L. Cheung, G. Lim, G. S. W. Tan, N. D.
CNN algorithms in novel directions infused with symbolic                    Quang, A. Gan, H. Hamzah, R. Garcia-Franco, I. Y. San Yeo, S. Y.
                                                                            Lee, et al., ‚ÄùDevelopment and validation of a deep learning system
                                                                            for diabetic retinopathy and related eye diseases using retinal images
                                                                            from multiethnic populations with diabetes,‚Äù Jama, vol.",2022-07-31 06:48:19+00:00,Neuro-Symbolic Learning: Principles and Applications in Ophthalmology,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Muhammad Hassan'), arxiv.Result.Author('Haifei Guan'), arxiv.Result.Author('Aikaterini Melliou'), arxiv.Result.Author('Yuqi Wang'), arxiv.Result.Author('Qianhui Sun'), arxiv.Result.Author('Sen Zeng'), arxiv.Result.Author('Wen Liang'), arxiv.Result.Author('Yiwei Zhang'), arxiv.Result.Author('Ziheng Zhang'), arxiv.Result.Author('Qiuyue Hu'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Shunkai Shi'), arxiv.Result.Author('Lin An'), arxiv.Result.Author('Shuyue Ma'), arxiv.Result.Author('Ijaz Gul'), arxiv.Result.Author('Muhammad Akmal Rahee'), arxiv.Result.Author('Zhou You'), arxiv.Result.Author('Canyang Zhang'), arxiv.Result.Author('Vijay Kumar Pandey'), arxiv.Result.Author('Yuxing Han'), arxiv.Result.Author('Yongbing Zhang'), arxiv.Result.Author('Ming Xu'), arxiv.Result.Author('Qiming Huang'), arxiv.Result.Author('Jiefu Tan'), arxiv.Result.Author('Qi Xing'), arxiv.Result.Author('Peiwu Qin'), arxiv.Result.Author('Dongmei Yu')]","Neural networks have been rapidly expanding in recent years, with novel
strategies and applications. However, challenges such as interpretability,
explainability, robustness, safety, trust, and sensibility remain unsolved in
neural network technologies, despite the fact that they will unavoidably be
addressed for critical applications. Attempts have been made to overcome the
challenges in neural network computing by representing and embedding domain
knowledge in terms of symbolic representations. Thus, the neuro-symbolic
learning (NeSyL) notion emerged, which incorporates aspects of symbolic
representation and bringing common sense into neural networks (NeSyL). In
domains where interpretability, reasoning, and explainability are crucial, such
as video and image captioning, question-answering and reasoning, health
informatics, and genomics, NeSyL has shown promising outcomes. This review
presents a comprehensive survey on the state-of-the-art NeSyL approaches, their
principles, advances in machine and deep learning algorithms, applications such
as opthalmology, and most importantly, future perspectives of this emerging
field.",-0.1555256,-0.18889907,0.21178001,C
9530,"The results encourage
further research in this direction, especially for annotation-starved medical images, with the objective of learning more
robust and reliable representational features in the pretext task.","This
is the Ô¨Årst work of its kind that incorporates learning of medical videos using the mentioned gradient accumulation
technique and data adapted BYOL, the downstream task ultimately being a classiÔ¨Åcation problem.","References

 [1] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick, ‚ÄúMomentum contrast for unsupervised visual representation learning,‚Äù in
      2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020,
      pp.",2022-07-31 14:48:06+00:00,BYOLMed3D: Self-Supervised Representation Learning of Medical Videos using Gradient Accumulation Assisted 3D BYOL Framework,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siladittya Manna'), arxiv.Result.Author('Souvik Chakraborty')]","Applications on Medical Image Analysis suffer from acute shortage of large
volume of data properly annotated by medical experts. Supervised Learning
algorithms require a large volumes of balanced data to learn robust
representations. Often supervised learning algorithms require various
techniques to deal with imbalanced data. Self-supervised learning algorithms on
the other hand are robust to imbalance in the data and are capable of learning
robust representations. In this work, we train a 3D BYOL self-supervised model
using gradient accumulation technique to deal with the large number of samples
in a batch generally required in a self-supervised algorithm. To the best of
our knowledge, this work is one of the first of its kind in this domain. We
compare the results obtained through our experiments in the downstream task of
ACL Tear Injury detection with the contemporary self-supervised pre-training
methods and also with ResNet3D-18 initialized with the Kinetics-400 pre-trained
weights. From the downstream task experiments, it is evident that the proposed
framework outperforms the existing baselines.",-0.22308034,-0.2335568,-0.044682518,C
9531,"The results encourage
further research in this direction, especially for annotation-starved medical images, with the objective of learning more
robust and reliable representational features in the pretext task.","This
is the Ô¨Årst work of its kind that incorporates learning of medical videos using the mentioned gradient accumulation
technique and data adapted BYOL, the downstream task ultimately being a classiÔ¨Åcation problem.","References

 [1] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick, ‚ÄúMomentum contrast for unsupervised visual representation learning,‚Äù in
      2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020,
      pp.",2022-07-31 14:48:06+00:00,BYOLMed3D: Self-Supervised Representation Learning of Medical Videos using Gradient Accumulation Assisted 3D BYOL Framework,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siladittya Manna'), arxiv.Result.Author('Rakesh Dey'), arxiv.Result.Author('Souvik Chakraborty')]","Applications on Medical Image Analysis suffer from acute shortage of large
volume of data properly annotated by medical experts. Supervised Learning
algorithms require a large volumes of balanced data to learn robust
representations. Often supervised learning algorithms require various
techniques to deal with imbalanced data. Self-supervised learning algorithms on
the other hand are robust to imbalance in the data and are capable of learning
robust representations. In this work, we train a 3D BYOL self-supervised model
using gradient accumulation technique to deal with the large number of samples
in a batch generally required in a self-supervised algorithm. To the best of
our knowledge, this work is one of the first of its kind in this domain. We
compare the results obtained through our experiments in the downstream task of
ACL Tear Injury detection with the contemporary self-supervised pre-training
methods and also with ResNet3D-18 initialized with the Kinetics-400 pre-trained
weights. From the downstream task experiments, it is evident that the proposed
framework outperforms the existing baselines.",-0.22308034,-0.2335568,-0.044682518,C
9537,"To
                                        facilitate further research in this area, we make our code publicly
                                        available at: https://github.com/ShuvenduRoy/SSL FER.",fully-supervised methods trained on the full labeled datasets.,"Index Terms‚ÄîSemi-Supervised Learning, Facial Expressions,
                                        Affective Computing

                                                                  I.",2022-07-31 23:58:35+00:00,Analysis of Semi-Supervised Methods for Facial Expression Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shuvendu Roy'), arxiv.Result.Author('Ali Etemad')]","Training deep neural networks for image recognition often requires
large-scale human annotated data. To reduce the reliance of deep neural
solutions on labeled data, state-of-the-art semi-supervised methods have been
proposed in the literature. Nonetheless, the use of such semi-supervised
methods has been quite rare in the field of facial expression recognition
(FER). In this paper, we present a comprehensive study on recently proposed
state-of-the-art semi-supervised learning methods in the context of FER. We
conduct comparative study on eight semi-supervised learning methods, namely
Pi-Model, Pseudo-label, Mean-Teacher, VAT, MixMatch, ReMixMatch, UDA, and
FixMatch, on three FER datasets (FER13, RAF-DB, and AffectNet), when various
amounts of labeled samples are used. We also compare the performance of these
methods against fully-supervised training. Our study shows that when training
existing semi-supervised methods on as little as 250 labeled samples per class
can yield comparable performances to that of fully-supervised methods trained
on the full labeled datasets. To facilitate further research in this area, we
make our code publicly available at: https://github.com/ShuvenduRoy/SSL_FER",0.015006841,-0.22792998,-0.25454435,C
9538,"Then the model is trained with a hard
augmentation of the same image using the predicted pseudo-        ‚Ä¢ To facilitate quick reproduction and further research on
label.",pseudo-label for it.,"the topic of semi-supervised FER, we release the code
                                                                     for this work which contains the implementations of all
   Although there has been some progress in semi-supervised          the methods in this study.",2022-07-31 23:58:35+00:00,Analysis of Semi-Supervised Methods for Facial Expression Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shuvendu Roy'), arxiv.Result.Author('Ali Etemad')]","Training deep neural networks for image recognition often requires
large-scale human annotated data. To reduce the reliance of deep neural
solutions on labeled data, state-of-the-art semi-supervised methods have been
proposed in the literature. Nonetheless, the use of such semi-supervised
methods has been quite rare in the field of facial expression recognition
(FER). In this paper, we present a comprehensive study on recently proposed
state-of-the-art semi-supervised learning methods in the context of FER. We
conduct comparative study on eight semi-supervised learning methods, namely
Pi-Model, Pseudo-label, Mean-Teacher, VAT, MixMatch, ReMixMatch, UDA, and
FixMatch, on three FER datasets (FER13, RAF-DB, and AffectNet), when various
amounts of labeled samples are used. We also compare the performance of these
methods against fully-supervised training. Our study shows that when training
existing semi-supervised methods on as little as 250 labeled samples per class
can yield comparable performances to that of fully-supervised methods trained
on the full labeled datasets. To facilitate further research in this area, we
make our code publicly available at: https://github.com/ShuvenduRoy/SSL_FER",-0.13614227,-0.092923254,-0.02156595,C
9556,"5 LIMITATIONS AND FAILURE CASES
4.5.4 Ablation on Numbers of KNN
                                                                     Table 9 summarizes the limitations among these compared meth-
We also conduct an ablation experiment on the number of nearest      ods to provoke further research insights.",relatively complete shape.,"Besides, we visualize
neighbors in the local reÔ¨Ånement unit.",2022-08-01 11:20:56+00:00,CSDN: Cross-modal Shape-transfer Dual-refinement Network for Point Cloud Completion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhe Zhu'), arxiv.Result.Author('Liangliang Nan'), arxiv.Result.Author('Haoran Xie'), arxiv.Result.Author('Honghua Chen'), arxiv.Result.Author('Mingqiang Wei'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Jing Qin')]","How will you repair a physical object with some missings? You may imagine its
original shape from previously captured images, recover its overall (global)
but coarse shape first, and then refine its local details. We are motivated to
imitate the physical repair procedure to address point cloud completion. To
this end, we propose a cross-modal shape-transfer dual-refinement network
(termed CSDN), a coarse-to-fine paradigm with images of full-cycle
participation, for quality point cloud completion. CSDN mainly consists of
""shape fusion"" and ""dual-refinement"" modules to tackle the cross-modal
challenge. The first module transfers the intrinsic shape characteristics from
single images to guide the geometry generation of the missing regions of point
clouds, in which we propose IPAdaIN to embed the global features of both the
image and the partial point cloud into completion. The second module refines
the coarse output by adjusting the positions of the generated points, where the
local refinement unit exploits the geometric relation between the novel and the
input points by graph convolution, and the global constraint unit utilizes the
input image to fine-tune the generated offset. Different from most existing
approaches, CSDN not only explores the complementary information from images
but also effectively exploits cross-modal data in the whole coarse-to-fine
completion procedure. Experimental results indicate that CSDN performs
favorably against ten competitors on the cross-modal benchmark.",0.4395334,0.13460237,-0.09570159,A
9557,The visualization is           ods to provoke further research insights.,"Same     2.570     0.695   3.656  0.631
           2.637     0.689   3.778  0.624                                   5 LIMITATIONS AND FAILURE CASES

Then, we compute the Euclidean distances between the target                 Table 14 summarizes the limitations among these compared meth-
point and other points in the feature space.","Besides, we visualize
generated by coloring points based on the distances.",2022-08-01 11:20:56+00:00,CSDN: Cross-modal Shape-transfer Dual-refinement Network for Point Cloud Completion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhe Zhu'), arxiv.Result.Author('Liangliang Nan'), arxiv.Result.Author('Haoran Xie'), arxiv.Result.Author('Honghua Chen'), arxiv.Result.Author('Mingqiang Wei'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Jing Qin')]","How will you repair a physical object with some missings? You may imagine its
original shape from previously captured images, recover its overall (global)
but coarse shape first, and then refine its local details. We are motivated to
imitate the physical repair procedure to address point cloud completion. To
this end, we propose a cross-modal shape-transfer dual-refinement network
(termed CSDN), a coarse-to-fine paradigm with images of full-cycle
participation, for quality point cloud completion. CSDN mainly consists of
""shape fusion"" and ""dual-refinement"" modules to tackle the cross-modal
challenge. The first module transfers the intrinsic shape characteristics from
single images to guide the geometry generation of the missing regions of point
clouds, in which we propose IPAdaIN to embed the global features of both the
image and the partial point cloud into completion. The second module refines
the coarse output by adjusting the positions of the generated points, where the
local refinement unit exploits the geometric relation between the novel and the
input points by graph convolution, and the global constraint unit utilizes the
input image to fine-tune the generated offset. Different from most existing
approaches, CSDN not only explores the complementary information from images
but also effectively exploits cross-modal data in the whole coarse-to-fine
completion procedure. Experimental results indicate that CSDN performs
favorably against ten competitors on the cross-modal benchmark.",0.15480623,0.20233405,-0.09647425,A
9559,"This motivates further research into ViT-based SS
models.","We note that backbones based on the ViT [Kolesnikov et al., 2021] architecture (both SS and supervised) perform well,
and are the best performing models for the majority of the datasets.","Further, we note that wider ResNets (ResNet50w2) tend to perform better than narrower ones (ResNet50), as
evidenced in the Color, Caltech101, and Food datasets.",2022-07-27 17:24:55+00:00,On the robustness of self-supervised representations for multi-view object classification,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('David Torpey'), arxiv.Result.Author('Richard Klein')]","It is known that representations from self-supervised pre-training can
perform on par, and often better, on various downstream tasks than
representations from fully-supervised pre-training. This has been shown in a
host of settings such as generic object classification and detection, semantic
segmentation, and image retrieval. However, some issues have recently come to
the fore that demonstrate some of the failure modes of self-supervised
representations, such as performance on non-ImageNet-like data, or complex
scenes. In this paper, we show that self-supervised representations based on
the instance discrimination objective lead to better representations of objects
that are more robust to changes in the viewpoint and perspective of the object.
We perform experiments of modern self-supervised methods against multiple
supervised baselines to demonstrate this, including approximating object
viewpoint variation through homographies, and real-world tests based on several
multi-view datasets. We find that self-supervised representations are more
robust to object viewpoint and appear to encode more pertinent information
about objects that facilitate the recognition of objects from novel views.",-0.025155406,-0.17057359,0.10668701,C
9562,"To further study the effect of separately
computing K-NN, we experiment with combined K-NN computation with d = 4
in Table 4(c).",Combining K-NN computation.,"It can be seen that the performance exceed the lower bound of
(a) and similar to DGCNN‚Äôs performance in Table 1.",2022-08-01 14:05:23+00:00,S$^2$Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tze Ho Elden Tse'), arxiv.Result.Author('Zhongqun Zhang'), arxiv.Result.Author('Kwang In Kim'), arxiv.Result.Author('Ales Leonardis'), arxiv.Result.Author('Feng Zheng'), arxiv.Result.Author('Hyung Jin Chang')]","Despite the recent efforts in accurate 3D annotations in hand and object
datasets, there still exist gaps in 3D hand and object reconstructions.
Existing works leverage contact maps to refine inaccurate hand-object pose
estimations and generate grasps given object models. However, they require
explicit 3D supervision which is seldom available and therefore, are limited to
constrained settings, e.g., where thermal cameras observe residual heat left on
manipulated objects. In this paper, we propose a novel semi-supervised
framework that allows us to learn contact from monocular images. Specifically,
we leverage visual and geometric consistency constraints in large-scale
datasets for generating pseudo-labels in semi-supervised learning and propose
an efficient graph-based network to infer contact. Our semi-supervised learning
framework achieves a favourable improvement over the existing supervised
learning methods trained on data with `limited' annotations. Notably, our
proposed model is able to achieve superior results with less than half the
network parameters and memory access cost when compared with the commonly-used
PointNet-based approach. We show benefits from using a contact map that rules
hand-object interactions to produce more accurate reconstructions. We further
demonstrate that training with pseudo-labels can extend contact map estimations
to out-of-domain objects and generalise better across multiple datasets.",0.2710915,-0.08179575,0.22710702,A
9574,"Both real-world and synthetic data are        we extend NeRF to simulate the aperture of lens and model the
  included for further study.","In this paper,
  points for each scene.","DoF effect by optimizing two learnable parameters, i.e., aperture
                                                                       size and focus distance.",2022-08-01 15:53:14+00:00,DoF-NeRF: Depth-of-Field Meets Neural Radiance Fields,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zijin Wu'), arxiv.Result.Author('Xingyi Li'), arxiv.Result.Author('Juewen Peng'), arxiv.Result.Author('Hao Lu'), arxiv.Result.Author('Zhiguo Cao'), arxiv.Result.Author('Weicai Zhong')]","Neural Radiance Field (NeRF) and its variants have exhibited great success on
representing 3D scenes and synthesizing photo-realistic novel views. However,
they are generally based on the pinhole camera model and assume all-in-focus
inputs. This limits their applicability as images captured from the real world
often have finite depth-of-field (DoF). To mitigate this issue, we introduce
DoF-NeRF, a novel neural rendering approach that can deal with shallow DoF
inputs and can simulate DoF effect. In particular, it extends NeRF to simulate
the aperture of lens following the principles of geometric optics. Such a
physical guarantee allows DoF-NeRF to operate views with different focus
configurations. Benefiting from explicit aperture modeling, DoF-NeRF also
enables direct manipulation of DoF effect by adjusting virtual aperture and
focus parameters. It is plug-and-play and can be inserted into NeRF-based
frameworks. Experiments on synthetic and real-world datasets show that,
DoF-NeRF not only performs comparably with NeRF in the all-in-focus setting,
but also can synthesize all-in-focus novel views conditioned on shallow DoF
inputs. An interesting application of DoF-NeRF to DoF rendering is also
demonstrated. The source code will be made available at
https://github.com/zijinwuzijin/DoF-NeRF.",0.016934687,0.19309536,0.14788565,B
9576,"We note that
       k-means [61]     65.5¬±0.0%  56.6¬±1.6%                          mining the discrimination of such similar categories is an
                        66.5¬±3.9%  14.3¬±1.3%                          interesting problem and worthy of further study.","Meanwhile, some instances
                                                                      of the dog (5) and horse (7) are close since they maintain
          Method         CIFAR10   CIFAR100                           similar appearances as four-legged mammals.","KCL [15]       64.2¬±0.1%  21.3¬±3.4%
         MCL [14]       87.5¬±0.3%  56.7¬±1.2%                          4.3 Ablation Study
         DTC [12]       88.7¬±0.3%  67.3¬±1.2%
 DTC [12] w/ S.S. [16]  90.4¬±0.5%  73.2¬±2.1%                          To analyze the contribution of proposed components in two
     AutoNovel [17]     91.7¬±0.9%  75.2¬±4.2%                          stages, we conduct several ablation studies on the CIFAR10,
AutoNovel [17] w/ I.L.",2022-08-01 16:34:33+00:00,Automatically Discovering Novel Visual Categories with Self-supervised Prototype Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lu Zhang'), arxiv.Result.Author('Lu Qi'), arxiv.Result.Author('Xu Yang'), arxiv.Result.Author('Hong Qiao'), arxiv.Result.Author('Ming-Hsuan Yang'), arxiv.Result.Author('Zhiyong Liu')]","This paper tackles the problem of novel category discovery (NCD), which aims
to discriminate unknown categories in large-scale image collections. The NCD
task is challenging due to the closeness to the real-world scenarios, where we
have only encountered some partial classes and images. Unlike other works on
the NCD, we leverage the prototypes to emphasize the importance of category
discrimination and alleviate the issue of missing annotations of novel classes.
Concretely, we propose a novel adaptive prototype learning method consisting of
two main stages: prototypical representation learning and prototypical
self-training. In the first stage, we obtain a robust feature extractor, which
could serve for all images with base and novel categories. This ability of
instance and category discrimination of the feature extractor is boosted by
self-supervised learning and adaptive prototypes. In the second stage, we
utilize the prototypes again to rectify offline pseudo labels and train a final
parametric classifier for category clustering. We conduct extensive experiments
on four benchmark datasets and demonstrate the effectiveness and robustness of
the proposed method with state-of-the-art performance.",0.29702133,0.017575093,-0.14654353,A
9584,"By
                                                                   using data across repeated traversals of the same route, we
serves as a baseline to further study the use of our dataset for   can efÔ¨Åciently create a large scale, diverse dataset with
3D object detection across weather conditions and repeated         amodal labels.","This paper develops
Sunny         9.48      23.58  24.41  17.79                        a unique dataset for the research community, with amodal
Rain         12.42      24.05  27.45  19.90                        road and object mask labels across varying scenes (univer-
Snow         13.01      27.84  20.99  22.83                        sity campus, downtown, highway, urban, residential, and
Night        10.58      23.6   24.46  11.99                        rural) and environmental conditions (snow, rain, night).","The repeated routes also opens potential new
routes.",2022-08-01 22:55:32+00:00,Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Carlos A. Diaz-Ruiz'), arxiv.Result.Author('Youya Xia'), arxiv.Result.Author('Yurong You'), arxiv.Result.Author('Jose Nino'), arxiv.Result.Author('Junan Chen'), arxiv.Result.Author('Josephine Monica'), arxiv.Result.Author('Xiangyu Chen'), arxiv.Result.Author('Katie Luo'), arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Marc Emond'), arxiv.Result.Author('Wei-Lun Chao'), arxiv.Result.Author('Bharath Hariharan'), arxiv.Result.Author('Kilian Q. Weinberger'), arxiv.Result.Author('Mark Campbell')]","Advances in perception for self-driving cars have accelerated in recent years
due to the availability of large-scale datasets, typically collected at
specific locations and under nice weather conditions. Yet, to achieve the high
safety requirement, these perceptual systems must operate robustly under a wide
variety of weather conditions including snow and rain. In this paper, we
present a new dataset to enable robust autonomous driving via a novel data
collection process - data is repeatedly recorded along a 15 km route under
diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time
(day/night), and traffic conditions (pedestrians, cyclists and cars). The
dataset includes images and point clouds from cameras and LiDAR sensors, along
with high-precision GPS/INS to establish correspondence across routes. The
dataset includes road and object annotations using amodal masks to capture
partial occlusions and 3D bounding boxes. We demonstrate the uniqueness of this
dataset by analyzing the performance of baselines in amodal segmentation of
road and objects, depth estimation, and 3D object detection. The repeated
routes opens new research directions in object discovery, continual learning,
and anomaly detection. Link to Ithaca365: https://ithaca365.mae.cornell.edu/",-0.31506312,0.15207404,-0.10161868,B
9604,"Nevertheless,
                                                                          this still requires some further research before the method we
                                                                          named Opto-Electronic Encephalography (OEG) will be fully
                                                                          suitable for everyday use.","We hope to report about a successful alignment of the
                                                                          presented methodology in one of our future works.",Fig.,2022-08-02 11:28:17+00:00,The Face of Affective Disorders,cs.CV,"['cs.CV', 'eess.IV', 'q-bio.NC']","[arxiv.Result.Author('Christian S. Pilz'), arxiv.Result.Author('Benjamin Clemens'), arxiv.Result.Author('Inka C. Hiss'), arxiv.Result.Author('Christoph Weiss'), arxiv.Result.Author('Ulrich Canzler'), arxiv.Result.Author('Jarek Krajewski'), arxiv.Result.Author('Ute Habel'), arxiv.Result.Author('Steffen Leonhardt')]","We study the statistical properties of facial behaviour altered by the
regulation of brain arousal in the clinical domain of psychiatry. The
underlying mechanism is linked to the empirical interpretation of the vigilance
continuum as behavioral surrogate measurement for certain states of mind. We
name the presented measurement in the sense of the classical scalp based
obtrusive sensors Opto Electronic Encephalography (OEG) which relies solely on
modern camera based real-time signal processing and computer vision. Based upon
a stochastic representation as coherence of the face dynamics, reflecting the
hemifacial asymmetry in emotion expressions, we demonstrate an almost flawless
distinction between patients and healthy controls as well as between the mental
disorders depression and schizophrenia and the symptom severity. In contrast to
the standard diagnostic process, which is time-consuming, subjective and does
not incorporate neurobiological data such as real-time face dynamics, the
objective stochastic modeling of the affective responsiveness only requires a
few minutes of video-based facial recordings. We also highlight the potential
of the methodology as a causal inference model in transdiagnostic analysis to
predict the outcome of pharmacological treatment. All results are obtained on a
clinical longitudinal data collection with an amount of 100 patients and 50
controls.",0.2021884,0.25958523,-0.06943873,A
9605,"Nevertheless,
                                                                          this still requires some further research before the method we
                                                                          named Opto-Electronic Encephalography (OEG) will be fully
                                                                          suitable for everyday use.","We hope to report about a successful alignment of the
                                                                          presented methodology in one of our future works.",Fig.,2022-08-02 11:28:17+00:00,The Face of Affective Disorders,cs.CV,"['cs.CV', 'eess.IV', 'q-bio.NC']","[arxiv.Result.Author('Christian S. Pilz'), arxiv.Result.Author('Benjamin Clemens'), arxiv.Result.Author('Inka C. Hiss'), arxiv.Result.Author('Christoph Weiss'), arxiv.Result.Author('Ulrich Canzler'), arxiv.Result.Author('Jarek Krajewski'), arxiv.Result.Author('Ute Habel'), arxiv.Result.Author('Steffen Leonhardt')]","We study the statistical properties of facial behaviour altered by the
regulation of brain arousal in the clinical domain of psychiatry. The
underlying mechanism is linked to the empirical interpretation of the vigilance
continuum as behavioral surrogate measurement for certain states of mind. We
name the presented measurement in the sense of the classical scalp based
obtrusive sensors Opto Electronic Encephalography (OEG) which relies solely on
modern camera based real-time signal processing and computer vision. Based upon
a stochastic representation as coherence of the face dynamics, reflecting the
hemifacial asymmetry in emotion expressions, we demonstrate an almost flawless
distinction between patients and healthy controls as well as between the mental
disorders depression and schizophrenia and the symptom severity. In contrast to
the standard diagnostic process, which is time-consuming, subjective and does
not incorporate neurobiological data such as real-time face dynamics, the
objective stochastic modeling of the affective responsiveness only requires a
few minutes of video-based facial recordings. We also highlight the potential
of the methodology as a causal inference model in transdiagnostic analysis to
predict the outcome of pharmacological treatment. All results are obtained on a
clinical longitudinal data collection with an amount of 100 patients and 50
controls.",0.2021884,0.25958523,-0.06943873,A
9606,"Especially with respect to anxi-         factors, such as gender or comorbidity, that further contribute to
olytics and SSRI, indicating that further research is needed here to        the enormous complexity of psychiatric disorders and the thera-
investigate speciÔ¨Åc clinical indications and the overall prognostic         peutic mechanisms.","Limitations                                                                 6 FUTURE WORK

The results of the model are, in part, in contrast to standard,             Future studies in this domain will help to recognize additional
guideline-based clinical routines.","While our results might be preliminary, they
utility of the present causal inference model in greater detail.",2022-08-02 11:28:17+00:00,The Face of Affective Disorders,cs.CV,"['cs.CV', 'eess.IV', 'q-bio.NC']","[arxiv.Result.Author('Christian S. Pilz'), arxiv.Result.Author('Benjamin Clemens'), arxiv.Result.Author('Inka C. Hiss'), arxiv.Result.Author('Christoph Weiss'), arxiv.Result.Author('Ulrich Canzler'), arxiv.Result.Author('Jarek Krajewski'), arxiv.Result.Author('Ute Habel'), arxiv.Result.Author('Steffen Leonhardt')]","We study the statistical properties of facial behaviour altered by the
regulation of brain arousal in the clinical domain of psychiatry. The
underlying mechanism is linked to the empirical interpretation of the vigilance
continuum as behavioral surrogate measurement for certain states of mind.
Referring to the classical scalp-based obtrusive measurements, we name the
presented method Opto-Electronic Encephalography (OEG) which solely relies on
modern camera-based real-time signal processing and computer vision. Based upon
a stochastic representation as coherence of the face dynamics, reflecting the
hemifacial asymmetry in emotion expressions, we demonstrate an almost flawless
distinction between patients and healthy controls as well as between the mental
disorders depression and schizophrenia and the symptom severity. In contrast to
the standard diagnostic process, which is time-consuming, subjective and does
not incorporate neurobiological data such as real-time face dynamics, the
objective stochastic modeling of the affective responsiveness only requires a
few minutes of video-based facial recordings. We also highlight the potential
of the methodology as a causal inference model in transdiagnostic analysis to
predict the outcome of pharmacological treatment. All results are obtained on a
clinical longitudinal data collection with an amount of 99 patients and 43
controls.",0.3848393,0.004512094,-0.23421778,A
9607,"816‚Äì820, 2021.
this still requires some further research before the method we
named Opto-Electronic Encephalography (OEG) will be fully                      [17] C. Granger, ‚ÄúInvestigating causal relations by econometric models and
suitable for everyday use.","5, pp.","cross-spectral methods.‚Äù Econometrica, vol.",2022-08-02 11:28:17+00:00,The Face of Affective Disorders,cs.CV,"['cs.CV', 'eess.IV', 'q-bio.NC']","[arxiv.Result.Author('Christian S. Pilz'), arxiv.Result.Author('Benjamin Clemens'), arxiv.Result.Author('Inka C. Hiss'), arxiv.Result.Author('Christoph Weiss'), arxiv.Result.Author('Ulrich Canzler'), arxiv.Result.Author('Jarek Krajewski'), arxiv.Result.Author('Ute Habel'), arxiv.Result.Author('Steffen Leonhardt')]","We study the statistical properties of facial behaviour altered by the
regulation of brain arousal in the clinical domain of psychiatry. The
underlying mechanism is linked to the empirical interpretation of the vigilance
continuum as behavioral surrogate measurement for certain states of mind.
Referring to the classical scalp-based obtrusive measurements, we name the
presented method Opto-Electronic Encephalography (OEG) which solely relies on
modern camera-based real-time signal processing and computer vision. Based upon
a stochastic representation as coherence of the face dynamics, reflecting the
hemifacial asymmetry in emotion expressions, we demonstrate an almost flawless
distinction between patients and healthy controls as well as between the mental
disorders depression and schizophrenia and the symptom severity. In contrast to
the standard diagnostic process, which is time-consuming, subjective and does
not incorporate neurobiological data such as real-time face dynamics, the
objective stochastic modeling of the affective responsiveness only requires a
few minutes of video-based facial recordings. We also highlight the potential
of the methodology as a causal inference model in transdiagnostic analysis to
predict the outcome of pharmacological treatment. All results are obtained on a
clinical longitudinal data collection with an amount of 99 patients and 43
controls.",0.3877246,0.15372258,-0.09841343,A
9649,"Our
pre-trained Places2 MAE will be released, which is benefit for the community
to further study the representation learning based image inpainting.","In our opinion, such
improvements are orthogonal to other proposed components in this paper.","Moreover,
although our MAE pre-trained on Places2 is generalized enough for the inpaint-
ing, pre-training MAEs on larger datasets (such as ImageNet-22K [38] or even
JFT-3B [54]) may achieve superior downstream performance.",2022-08-03 04:32:53+00:00,Learning Prior Feature and Attention Enhanced Image Inpainting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chenjie Cao'), arxiv.Result.Author('Qiaole Dong'), arxiv.Result.Author('Yanwei Fu')]","Many recent inpainting works have achieved impressive results by leveraging
Deep Neural Networks (DNNs) to model various prior information for image
restoration. Unfortunately, the performance of these methods is largely limited
by the representation ability of vanilla Convolutional Neural Networks (CNNs)
backbones.On the other hand, Vision Transformers (ViT) with self-supervised
pre-training have shown great potential for many visual recognition and object
detection tasks. A natural question is whether the inpainting task can be
greatly benefited from the ViT backbone? However, it is nontrivial to directly
replace the new backbones in inpainting networks, as the inpainting is an
inverse problem fundamentally different from the recognition tasks. To this
end, this paper incorporates the pre-training based Masked AutoEncoder (MAE)
into the inpainting model, which enjoys richer informative priors to enhance
the inpainting process. Moreover, we propose to use attention priors from MAE
to make the inpainting model learn more long-distance dependencies between
masked and unmasked regions. Sufficient ablations have been discussed about the
inpainting and the self-supervised pre-training models in this paper. Besides,
experiments on both Places2 and FFHQ demonstrate the effectiveness of our
proposed model. Codes and pre-trained models are released in
https://github.com/ewrfcas/MAE-FAR.",-0.19132026,-0.17548075,0.19798455,C
9653,"In this work, we further study           for the per-clip inference, we propose a new training scheme
the scenario with a periodic memory update, namely per-          and a variant of the memory matching module, progres-
clip inference.","In addition, to specialize the model
proved memory matching.",Different from STCN that processes each          sive memory matching mechanism.,2022-08-03 09:02:29+00:00,Per-Clip Video Object Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kwanyong Park'), arxiv.Result.Author('Sanghyun Woo'), arxiv.Result.Author('Seoung Wug Oh'), arxiv.Result.Author('In So Kweon'), arxiv.Result.Author('Joon-Young Lee')]","Recently, memory-based approaches show promising results on semi-supervised
video object segmentation. These methods predict object masks frame-by-frame
with the help of frequently updated memory of the previous mask. Different from
this per-frame inference, we investigate an alternative perspective by treating
video object segmentation as clip-wise mask propagation. In this per-clip
inference scheme, we update the memory with an interval and simultaneously
process a set of consecutive frames (i.e. clip) between the memory updates. The
scheme provides two potential benefits: accuracy gain by clip-level
optimization and efficiency gain by parallel computation of multiple frames. To
this end, we propose a new method tailored for the per-clip inference.
Specifically, we first introduce a clip-wise operation to refine the features
based on intra-clip correlation. In addition, we employ a progressive matching
mechanism for efficient information-passing within a clip. With the synergy of
two modules and a newly proposed per-clip based training, our network achieves
state-of-the-art performance on Youtube-VOS 2018/2019 val (84.6% and 84.6%) and
DAVIS 2016/2017 val (91.9% and 86.1%). Furthermore, our model shows a great
speed-accuracy trade-off with varying memory update intervals, which leads to
huge flexibility.",0.091860525,-0.17024636,0.059848063,A
9682,"Furthermore, all images are made public with permission of Metis Systems AG and may be used for further research.","To the author‚Äôs knowledge, this dataset is the most extensive public collection of honeycombs in existence and is the
only one comprised of images that are neither scraped from the internet nor taken especially for research.","While our deÔ¨Ånition for honeycombs may not Ô¨Åt that of other researchers, the raw images are also published and
increase the number of publicly available photos of concrete structures with honeycombs, pores, etc., caused by errors
during construction rather than deterioration.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects. Automating
defect detection would reduce documentation efforts that are necessary to
decrease the risk of defects delaying construction projects. Since concrete is
a widely used construction material, this work focuses on detecting honeycombs,
a substantial defect in concrete structures that may even affect structural
integrity. First, images were compared that were either scraped from the web or
obtained from actual practice. The results demonstrate that web images
represent just a selection of honeycombs and do not capture the complete
variance. Second, Mask R-CNN and EfficientNet-B0 were trained for honeycomb
detection to evaluate instance segmentation and patch-based classification,
respectively achieving 47.7% precision and 34.2% recall as well as 68.5%
precision and 55.7% recall. Although the performance of those models is not
sufficient for completely automated defect detection, the models could be used
for active learning integrated into defect documentation systems. In
conclusion, CNNs can assist detecting honeycombs in concrete.",-0.030944385,0.23322518,-0.098648824,B
9683,"Although
an experiment classifying large honeycombs failed, as mentioned in section 3.2, the challenge of unclear instance
divisions may justify further research in this direction.",One could address this issue by changing the problem type to classiÔ¨Åcation and segmentation.,"Nevertheless, this problem does not affect veriÔ¨Åcation by
inspectors and, therefore, might be addressed by increasing the training data.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects. Automating
defect detection would reduce documentation efforts that are necessary to
decrease the risk of defects delaying construction projects. Since concrete is
a widely used construction material, this work focuses on detecting honeycombs,
a substantial defect in concrete structures that may even affect structural
integrity. First, images were compared that were either scraped from the web or
obtained from actual practice. The results demonstrate that web images
represent just a selection of honeycombs and do not capture the complete
variance. Second, Mask R-CNN and EfficientNet-B0 were trained for honeycomb
detection to evaluate instance segmentation and patch-based classification,
respectively achieving 47.7% precision and 34.2% recall as well as 68.5%
precision and 55.7% recall. Although the performance of those models is not
sufficient for completely automated defect detection, the models could be used
for active learning integrated into defect documentation systems. In
conclusion, CNNs can assist detecting honeycombs in concrete.",0.14260602,-0.059740394,-0.117314674,A
9684,"10: Continuity of detections

In conclusion, both instance segmentation and classiÔ¨Åcation with Grad-CAM are valid approaches for further research
with larger datasets.","(a)                                     (b)

(c)                                     (d)

     Fig.","Since the labeling of honeycombs for instance segmentation is signiÔ¨Åcantly more challenging than
classiÔ¨Åcation, the latter could be superior for future research, especially considering the expert knowledge required and
the ambiguities dividing honeycombs.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects. Automating
defect detection would reduce documentation efforts that are necessary to
decrease the risk of defects delaying construction projects. Since concrete is
a widely used construction material, this work focuses on detecting honeycombs,
a substantial defect in concrete structures that may even affect structural
integrity. First, images were compared that were either scraped from the web or
obtained from actual practice. The results demonstrate that web images
represent just a selection of honeycombs and do not capture the complete
variance. Second, Mask R-CNN and EfficientNet-B0 were trained for honeycomb
detection to evaluate instance segmentation and patch-based classification,
respectively achieving 47.7% precision and 34.2% recall as well as 68.5%
precision and 55.7% recall. Although the performance of those models is not
sufficient for completely automated defect detection, the models could be used
for active learning integrated into defect documentation systems. In
conclusion, CNNs can assist detecting honeycombs in concrete.",-0.07352194,-0.056367364,-0.14684299,B
9685,"These datasets provide
a basis for further research into honeycomb detection.","HiCIS contains datasets for detecting honey-
16

combs with bounding boxes and instance segmentation masks labeled in the MS COCO format.",The raw images are also included.,2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects. Automating
defect detection would reduce documentation efforts that are necessary to
decrease the risk of defects delaying construction projects. Since concrete is
a widely used construction material, this work focuses on detecting honeycombs,
a substantial defect in concrete structures that may even affect structural
integrity. First, images were compared that were either scraped from the web or
obtained from actual practice. The results demonstrate that web images
represent just a selection of honeycombs and do not capture the complete
variance. Second, Mask R-CNN and EfficientNet-B0 were trained for honeycomb
detection to evaluate instance segmentation and patch-based classification,
respectively achieving 47.7% precision and 34.2% recall as well as 68.5%
precision and 55.7% recall. Although the performance of those models is not
sufficient for completely automated defect detection, the models could be used
for active learning integrated into defect documentation systems. In
conclusion, CNNs can assist detecting honeycombs in concrete.",-0.09675556,0.08722224,-0.06750649,B
9686,"Therefore, the classiÔ¨Åcation enabled by the trained EfÔ¨ÅcientNet-B0 model is essential for further research
in this direction.","[31]
stated that in cases of a high ratio of negative to positive pixels, a model would be likely to learn to classify each pixel
as negative.","The Mask R-CNN trained on the HiCIS web and metis datasets achieved an 12.4% APIoU‚â•50, 47.7% precision, and
34.2% recall on the metis test set as well as an 25.6% APIoU‚â•50, 64.9% precision, and 42.1% recall on the web test
set.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects. Automating
defect detection would reduce documentation efforts that are necessary to
decrease the risk of defects delaying construction projects. Since concrete is
a widely used construction material, this work focuses on detecting honeycombs,
a substantial defect in concrete structures that may even affect structural
integrity. First, images were compared that were either scraped from the web or
obtained from actual practice. The results demonstrate that web images
represent just a selection of honeycombs and do not capture the complete
variance. Second, Mask R-CNN and EfficientNet-B0 were trained for honeycomb
detection to evaluate instance segmentation and patch-based classification,
respectively achieving 47.7% precision and 34.2% recall as well as 68.5%
precision and 55.7% recall. Although the performance of those models is not
sufficient for completely automated defect detection, the models could be used
for active learning integrated into defect documentation systems. In
conclusion, CNNs can assist detecting honeycombs in concrete.",-0.008558734,-0.14146282,0.19531685,C
9687,"Our dataset is therefore freely available
                                       for further research.","We found that web images do not capture the complete variance found in real-case
                                       scenarios and that there is still a lack of data in this domain.",A Mask R-CNN and EfÔ¨ÅcientNet-B0 were trained for honeycomb detection.,2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects as they
require significant inspection and documentation efforts. Automating defect
detection could significantly reduce these efforts. This work focuses on
detecting honeycombs, a substantial defect in concrete structures that may
affect structural integrity. We compared honeycomb images scraped from the web
with images obtained from real construction inspections. We found that web
images do not capture the complete variance found in real-case scenarios and
that there is still a lack of data in this domain. Our dataset is therefore
freely available for further research. A Mask R-CNN and EfficientNet-B0 were
trained for honeycomb detection. The Mask R-CNN model allows detecting
honeycombs based on instance segmentation, whereas the EfficientNet-B0 model
allows a patch-based classification. Our experiments demonstrate that both
approaches are suitable for solving and automating honeycomb detection. In the
future, this solution can be incorporated into defect documentation systems.",-0.14331228,-0.07931366,0.06371238,C
9688,"Furthermore, all images are made public with permission of Metis Systems AG and may be used for further research.","To the author‚Äôs knowledge, this dataset is the most extensive public collection of honeycombs in existence and is the
only one comprised of images that are neither scraped from the internet nor taken especially for research.","While our deÔ¨Ånition for honeycombs may not Ô¨Åt that of other researchers, the raw images are also published and
increase the number of publicly available photos of concrete structures with honeycombs, pores, etc., caused by errors
during construction rather than deterioration.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects as they
require significant inspection and documentation efforts. Automating defect
detection could significantly reduce these efforts. This work focuses on
detecting honeycombs, a substantial defect in concrete structures that may
affect structural integrity. We compared honeycomb images scraped from the web
with images obtained from real construction inspections. We found that web
images do not capture the complete variance found in real-case scenarios and
that there is still a lack of data in this domain. Our dataset is therefore
freely available for further research. A Mask R-CNN and EfficientNet-B0 were
trained for honeycomb detection. The Mask R-CNN model allows detecting
honeycombs based on instance segmentation, whereas the EfficientNet-B0 model
allows a patch-based classification. Our experiments demonstrate that both
approaches are suitable for solving and automating honeycomb detection. In the
future, this solution can be incorporated into defect documentation systems.",-0.030944385,0.23322518,-0.098648824,B
9689,"Although
an experiment classifying large honeycombs failed, as mentioned in section 3.2, the challenge of unclear instance
divisions may justify further research in this direction.",One could address this issue by changing the problem type to classiÔ¨Åcation and segmentation.,"Nevertheless, this problem does not affect veriÔ¨Åcation by
inspectors and, therefore, might be addressed by increasing the training data.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects as they
require significant inspection and documentation efforts. Automating defect
detection could significantly reduce these efforts. This work focuses on
detecting honeycombs, a substantial defect in concrete structures that may
affect structural integrity. We compared honeycomb images scraped from the web
with images obtained from real construction inspections. We found that web
images do not capture the complete variance found in real-case scenarios and
that there is still a lack of data in this domain. Our dataset is therefore
freely available for further research. A Mask R-CNN and EfficientNet-B0 were
trained for honeycomb detection. The Mask R-CNN model allows detecting
honeycombs based on instance segmentation, whereas the EfficientNet-B0 model
allows a patch-based classification. Our experiments demonstrate that both
approaches are suitable for solving and automating honeycomb detection. In the
future, this solution can be incorporated into defect documentation systems.",0.14260602,-0.059740394,-0.117314674,A
9690,"10: Continuity of detections

In conclusion, both instance segmentation and classiÔ¨Åcation with Grad-CAM are valid approaches for further research
with larger datasets.","(a)                                     (b)

(c)                                     (d)

     Fig.","Since the labeling of honeycombs for instance segmentation is signiÔ¨Åcantly more challenging than
classiÔ¨Åcation, the latter could be superior for future research, especially considering the expert knowledge required and
the ambiguities dividing honeycombs.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects as they
require significant inspection and documentation efforts. Automating defect
detection could significantly reduce these efforts. This work focuses on
detecting honeycombs, a substantial defect in concrete structures that may
affect structural integrity. We compared honeycomb images scraped from the web
with images obtained from real construction inspections. We found that web
images do not capture the complete variance found in real-case scenarios and
that there is still a lack of data in this domain. Our dataset is therefore
freely available for further research. A Mask R-CNN and EfficientNet-B0 were
trained for honeycomb detection. The Mask R-CNN model allows detecting
honeycombs based on instance segmentation, whereas the EfficientNet-B0 model
allows a patch-based classification. Our experiments demonstrate that both
approaches are suitable for solving and automating honeycomb detection. In the
future, this solution can be incorporated into defect documentation systems.",-0.07352194,-0.056367364,-0.14684299,B
9691,"These datasets provide
a basis for further research into honeycomb detection.","HiCIS contains datasets for detecting honey-
16

combs with bounding boxes and instance segmentation masks labeled in the MS COCO format.",The raw images are also included.,2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects as they
require significant inspection and documentation efforts. Automating defect
detection could significantly reduce these efforts. This work focuses on
detecting honeycombs, a substantial defect in concrete structures that may
affect structural integrity. We compared honeycomb images scraped from the web
with images obtained from real construction inspections. We found that web
images do not capture the complete variance found in real-case scenarios and
that there is still a lack of data in this domain. Our dataset is therefore
freely available for further research. A Mask R-CNN and EfficientNet-B0 were
trained for honeycomb detection. The Mask R-CNN model allows detecting
honeycombs based on instance segmentation, whereas the EfficientNet-B0 model
allows a patch-based classification. Our experiments demonstrate that both
approaches are suitable for solving and automating honeycomb detection. In the
future, this solution can be incorporated into defect documentation systems.",-0.09675556,0.08722224,-0.06750649,B
9692,"Therefore, the classiÔ¨Åcation enabled by the trained EfÔ¨ÅcientNet-B0 model is essential for further research
in this direction.","[31]
stated that in cases of a high ratio of negative to positive pixels, a model would be likely to learn to classify each pixel
as negative.","The Mask R-CNN trained on the HiCIS web and metis datasets achieved an 12.4% APIoU‚â•50, 47.7% precision, and
34.2% recall on the metis test set as well as an 25.6% APIoU‚â•50, 64.9% precision, and 42.1% recall on the web test
set.",2022-08-03 19:05:12+00:00,Image-based Detection of Surface Defects in Concrete during Construction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dominik Kuhnke'), arxiv.Result.Author('Monika Kwiatkowski'), arxiv.Result.Author('Olaf Hellwich')]","Defects increase the cost and duration of construction projects as they
require significant inspection and documentation efforts. Automating defect
detection could significantly reduce these efforts. This work focuses on
detecting honeycombs, a substantial defect in concrete structures that may
affect structural integrity. We compared honeycomb images scraped from the web
with images obtained from real construction inspections. We found that web
images do not capture the complete variance found in real-case scenarios and
that there is still a lack of data in this domain. Our dataset is therefore
freely available for further research. A Mask R-CNN and EfficientNet-B0 were
trained for honeycomb detection. The Mask R-CNN model allows detecting
honeycombs based on instance segmentation, whereas the EfficientNet-B0 model
allows a patch-based classification. Our experiments demonstrate that both
approaches are suitable for solving and automating honeycomb detection. In the
future, this solution can be incorporated into defect documentation systems.",-0.008558734,-0.14146282,0.19531685,C
9695,"We hope the Ô¨Åndings of our work can facilitate
                                              further research in visual information extraction from audio.","In particular, we consider
                                              the prediction of the following visual modalities from audio: depth and
                                              semantic segmentation.","Code is
                                              available at: https://github.com/ubc-vision/audio_manifold.",2022-08-03 20:47:11+00:00,Estimating Visual Information From Audio Through Manifold Learning,cs.CV,"['cs.CV', 'cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Fabrizio Pedersoli'), arxiv.Result.Author('Dryden Wiebe'), arxiv.Result.Author('Amin Banitalebi'), arxiv.Result.Author('Yong Zhang'), arxiv.Result.Author('Kwang Moo Yi')]","We propose a new framework for extracting visual information about a scene
only using audio signals. Audio-based methods can overcome some of the
limitations of vision-based methods i.e., they do not require ""line-of-sight"",
are robust to occlusions and changes in illumination, and can function as a
backup in case vision/lidar sensors fail. Therefore, audio-based methods can be
useful even for applications in which only visual information is of interest
Our framework is based on Manifold Learning and consists of two steps. First,
we train a Vector-Quantized Variational Auto-Encoder to learn the data manifold
of the particular visual modality we are interested in. Second, we train an
Audio Transformation network to map multi-channel audio signals to the latent
representation of the corresponding visual sample. We show that our method is
able to produce meaningful images from audio using a publicly available
audio/visual dataset. In particular, we consider the prediction of the
following visual modalities from audio: depth and semantic segmentation. We
hope the findings of our work can facilitate further research in visual
information extraction from audio. Code is available at:
https://github.com/ubc-vision/audio_manifold.",-0.18467008,0.06945413,-0.09853066,B
9696,"We hope the Ô¨Åndings of our work can facilitate
                                               further research in visual information extraction from audio.","In particular, we consider
                                               the prediction of the following visual modalities from audio: depth and
                                               semantic segmentation.","Code is
                                               available at: https://github.com/ubc-vision/audio_manifold.",2022-08-03 20:47:11+00:00,Estimating Visual Information From Audio Through Manifold Learning,cs.CV,"['cs.CV', 'cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Fabrizio Pedersoli'), arxiv.Result.Author('Dryden Wiebe'), arxiv.Result.Author('Amin Banitalebi'), arxiv.Result.Author('Yong Zhang'), arxiv.Result.Author('George Tzanetakis'), arxiv.Result.Author('Kwang Moo Yi')]","We propose a new framework for extracting visual information about a scene
only using audio signals. Audio-based methods can overcome some of the
limitations of vision-based methods i.e., they do not require ""line-of-sight"",
are robust to occlusions and changes in illumination, and can function as a
backup in case vision/lidar sensors fail. Therefore, audio-based methods can be
useful even for applications in which only visual information is of interest
Our framework is based on Manifold Learning and consists of two steps. First,
we train a Vector-Quantized Variational Auto-Encoder to learn the data manifold
of the particular visual modality we are interested in. Second, we train an
Audio Transformation network to map multi-channel audio signals to the latent
representation of the corresponding visual sample. We show that our method is
able to produce meaningful images from audio using a publicly available
audio/visual dataset. In particular, we consider the prediction of the
following visual modalities from audio: depth and semantic segmentation. We
hope the findings of our work can facilitate further research in visual
information extraction from audio. Code is available at:
https://github.com/ubc-vision/audio_manifold.",-0.18467008,0.06945413,-0.09853066,B
9717,"As mentioned in Section 2.2, these datasets, along with
pretrained models which achieved these benchmarks, are available through the open-source framework AgML,
enabling further research on developing even more efficient data and model pipelines.","Our standard pipelines enable our models to achieve comparable performance with existing benchmarks on our
collected datasets, in certain cases even exceeding them.","3.2 Performance of Agricultural Pretrained Weights for Object Detection

A summary of the results of our experiments using agricultural pretrained weights for object detection are
shown in Figure 6, with each of the fruits representing the 7 one-class models, and complete representing the
one 7-class model.",2022-08-04 15:10:36+00:00,Standardizing and Centralizing Datasets to Enable Efficient Training of Agricultural Deep Learning Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Amogh Joshi'), arxiv.Result.Author('Dario Guevara'), arxiv.Result.Author('Mason Earles')]","In recent years, deep learning models have become the standard for
agricultural computer vision. Such models are typically fine-tuned to
agricultural tasks using model weights that were originally fit to more
general, non-agricultural datasets. This lack of agriculture-specific
fine-tuning potentially increases training time and resource use, and decreases
model performance, leading an overall decrease in data efficiency. To overcome
this limitation, we collect a wide range of existing public datasets for three
distinct tasks, standardize them, and construct standard training and
evaluation pipelines, providing us with a set of benchmarks and pretrained
models. We then conduct a number of experiments using methods which are
commonly used in deep learning tasks, but unexplored in their domain-specific
applications for agriculture. Our experiments guide us in developing a number
of approaches to improve data efficiency when training agricultural deep
learning models, without large-scale modifications to existing pipelines. Our
results demonstrate that even slight training modifications, such as using
agricultural pretrained model weights, or adopting specific spatial
augmentations into data processing pipelines, can significantly boost model
performance and result in shorter convergence time, saving training resources.
Furthermore, we find that even models trained on low-quality annotations can
produce comparable levels of performance to their high-quality equivalents,
suggesting that datasets with poor annotations can still be used for training,
expanding the pool of currently available datasets. Our methods are broadly
applicable throughout agricultural deep learning, and present high potential
for significant data efficiency improvements.",-0.19752625,-0.11699635,0.061160646,B
9722,"In a sense, this result creates new grounds for
and SMT-OCFR2 increases as these submitted solutions fall          further research on which situations are beneÔ¨Åcial to dis-
short to have an FMR100 smaller than 90%, which might              card the alignment step.","The gap between the Baseline, SMT-OCFR1,               cluded FR.","be caused by attempts to align the input images with a suc-
cess rate of 61.2%.",2022-08-04 16:39:08+00:00,OCFR 2022: Competition on Occluded Face Recognition From Synthetically Generated Structure-Aware Occlusions,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Pedro C. Neto'), arxiv.Result.Author('Fadi Boutros'), arxiv.Result.Author('Joao Ribeiro Pinto'), arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('Ana F. Sequeira'), arxiv.Result.Author('Jaime S. Cardoso'), arxiv.Result.Author('Messaoud Bengherabi'), arxiv.Result.Author('Abderaouf Bousnat'), arxiv.Result.Author('Sana Boucheta'), arxiv.Result.Author('Nesrine Hebbadj'), arxiv.Result.Author('Bahia Yahya-Zoubir'), arxiv.Result.Author('Mustafa Ekrem Erakƒ±n'), arxiv.Result.Author('Uƒüur Demir'), arxiv.Result.Author('Hazƒ±m Kemal Ekenel'), arxiv.Result.Author('Pedro Beber de Queiroz Vidal'), arxiv.Result.Author('David Menotti')]","This work summarizes the IJCB Occluded Face Recognition Competition 2022
(IJCB-OCFR-2022) embraced by the 2022 International Joint Conference on
Biometrics (IJCB 2022). OCFR-2022 attracted a total of 3 participating teams,
from academia. Eventually, six valid submissions were submitted and then
evaluated by the organizers. The competition was held to address the challenge
of face recognition in the presence of severe face occlusions. The participants
were free to use any training data and the testing data was built by the
organisers by synthetically occluding parts of the face images using a
well-known dataset. The submitted solutions presented innovations and performed
very competitively with the considered baseline. A major output of this
competition is a challenging, realistic, and diverse, and publicly available
occluded face recognition benchmark with well defined evaluation protocols.",0.31028083,0.21136469,0.0585689,A
9723,"In a sense, this result creates new grounds for
and SMT-OCFR2 increases as these submitted solutions fall          further research on which situations are beneÔ¨Åcial to dis-
short to have an FMR100 smaller than 90%, which might              card the alignment step.","The gap between the Baseline, SMT-OCFR1,               cluded FR.","be caused by attempts to align the input images with a suc-
cess rate of 61.2%.",2022-08-04 16:39:08+00:00,OCFR 2022: Competition on Occluded Face Recognition From Synthetically Generated Structure-Aware Occlusions,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Pedro C. Neto'), arxiv.Result.Author('Fadi Boutros'), arxiv.Result.Author('Joao Ribeiro Pinto'), arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('Ana F. Sequeira'), arxiv.Result.Author('Jaime S. Cardoso'), arxiv.Result.Author('Messaoud Bengherabi'), arxiv.Result.Author('Abderaouf Bousnat'), arxiv.Result.Author('Sana Boucheta'), arxiv.Result.Author('Nesrine Hebbadj'), arxiv.Result.Author('Mustafa Ekrem Erakƒ±n'), arxiv.Result.Author('Uƒüur Demir'), arxiv.Result.Author('Hazƒ±m Kemal Ekenel'), arxiv.Result.Author('Pedro Beber de Queiroz Vidal'), arxiv.Result.Author('David Menotti')]","This work summarizes the IJCB Occluded Face Recognition Competition 2022
(IJCB-OCFR-2022) embraced by the 2022 International Joint Conference on
Biometrics (IJCB 2022). OCFR-2022 attracted a total of 3 participating teams,
from academia. Eventually, six valid submissions were submitted and then
evaluated by the organizers. The competition was held to address the challenge
of face recognition in the presence of severe face occlusions. The participants
were free to use any training data and the testing data was built by the
organisers by synthetically occluding parts of the face images using a
well-known dataset. The submitted solutions presented innovations and performed
very competitively with the considered baseline. A major output of this
competition is a challenging, realistic, and diverse, and publicly available
occluded face recognition benchmark with well defined evaluation protocols.",0.31028083,0.21136469,0.0585689,A
9725,"Given that massive, massive outstanding achievements in          Compared with LiDAR-based approaches, vision-centric
both academy and industry related to this area have emerged,     methods can obtain richer semantic information from im-
we presents a comprehensive review of recent progress to         ages and rely on high-level understanding of them to
facilitate the further research.",height and width of the input image.,"The main contributions of this  reason the scene geometry but are short on accurate depth
work can be summarized as follows:                               measurements.",2022-08-04 17:53:17+00:00,Vision-Centric BEV Perception: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuexin Ma'), arxiv.Result.Author('Tai Wang'), arxiv.Result.Author('Xuyang Bai'), arxiv.Result.Author('Huitong Yang'), arxiv.Result.Author('Yuenan Hou'), arxiv.Result.Author('Yaming Wang'), arxiv.Result.Author('Yu Qiao'), arxiv.Result.Author('Ruigang Yang'), arxiv.Result.Author('Dinesh Manocha'), arxiv.Result.Author('Xinge Zhu')]","Vision-centric BEV perception has recently received increased attention from
both industry and academia due to its inherent merits, including presenting a
natural representation of the world and being fusion-friendly. With the rapid
development of deep learning, numerous methods have been proposed to address
the vision-centric BEV perception. However, there is no recent survey for this
novel and growing research field. To stimulate its future research, this paper
presents a comprehensive survey of recent progress of vision-centric BEV
perception and its extensions. It collects and organizes the recent knowledge,
and gives a systematic review and summary of commonly used algorithms. It also
provides in-depth analyses and comparative results on several BEV perception
tasks, facilitating the comparisons of future works and inspiring future
research directions. Moreover, empirical implementation details are also
discussed and shown to benefit the development of related algorithms.",-0.31831333,0.23586695,0.0069091413,B
9743,"[50] followed the clas-
those limitations in further research.",Scherhag et al.,"siÔ¨Åcation of pretrained deep features in differential sce-
                                                                nario.",2022-08-05 11:39:22+00:00,MorDeephy: Face Morphing Detection Via Fused Classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Iurii Medvedev'), arxiv.Result.Author('Farhad Shadmand'), arxiv.Result.Author('Nuno Gon√ßalves')]","Face morphing attack detection (MAD) is one of the most challenging tasks in
the field of face recognition nowadays. In this work, we introduce a novel deep
learning strategy for a single image face morphing detection, which implies the
discrimination of morphed face images along with a sophisticated face
recognition task in a complex classification scheme. It is directed onto
learning the deep facial features, which carry information about the
authenticity of these features. Our work also introduces several additional
contributions: the public and easy-to-use face morphing detection benchmark and
the results of our wild datasets filtering strategy. Our method, which we call
MorDeephy, achieved the state of the art performance and demonstrated a
prominent ability for generalising the task of morphing detection to unseen
scenarios.",0.0041545797,-0.0818076,0.15469718,C
9756,made publicly available for further research.,4700‚Äì4708.,[14] Diederik P Kingma and Jimmy Ba.,2022-08-05 17:29:14+00:00,Convolutional Ensembling based Few-Shot Defect Detection Technique,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Soumyajit Karmakar'), arxiv.Result.Author('Abeer Banerjee'), arxiv.Result.Author('Sanjay Singh')]","Over the past few years, there has been a significant improvement in the
domain of few-shot learning. This learning paradigm has shown promising results
for the challenging problem of anomaly detection, where the general task is to
deal with heavy class imbalance. Our paper presents a new approach to few-shot
classification, where we employ the knowledge-base of multiple pre-trained
convolutional models that act as the backbone for our proposed few-shot
framework. Our framework uses a novel ensembling technique for boosting the
accuracy while drastically decreasing the total parameter count, thus paving
the way for real-time implementation. We perform an extensive hyperparameter
search using a power-line defect detection dataset and obtain an accuracy of
92.30% for the 5-way 5-shot task. Without further tuning, we evaluate our model
on competing standards with the existing state-of-the-art methods and
outperform them.",0.26021796,0.124573275,-0.120241314,A
9757,"The complete code will be
made publicly available for further research.",set should be done with extreme care.,"[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",2022-08-05 17:29:14+00:00,Convolutional Ensembling based Few-Shot Defect Detection Technique,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Soumyajit Karmakar'), arxiv.Result.Author('Abeer Banerjee'), arxiv.Result.Author('Prashant Sadashiv Gidde'), arxiv.Result.Author('Sumeet Saurav'), arxiv.Result.Author('Sanjay Singh')]","Over the past few years, there has been a significant improvement in the
domain of few-shot learning. This learning paradigm has shown promising results
for the challenging problem of anomaly detection, where the general task is to
deal with heavy class imbalance. Our paper presents a new approach to few-shot
classification, where we employ the knowledge-base of multiple pre-trained
convolutional models that act as the backbone for our proposed few-shot
framework. Our framework uses a novel ensembling technique for boosting the
accuracy while drastically decreasing the total parameter count, thus paving
the way for real-time implementation. We perform an extensive hyperparameter
search using a power-line defect detection dataset and obtain an accuracy of
92.30% for the 5-way 5-shot task. Without further tuning, we evaluate our model
on competing standards with the existing state-of-the-art methods and
outperform them.",0.25787613,0.1311134,-0.17219755,A
9758,"The complete code will be
                                                                           made publicly available for further research.","Therefore, the selection of support
                                                                           set should be done with extreme care.","6
ACKNOWLEDGMENTS                                                                                 [23] Maria-Elena Nilsback and Andrew Zisserman.",2022-08-05 17:29:14+00:00,Convolutional Ensembling based Few-Shot Defect Detection Technique,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Soumyajit Karmakar'), arxiv.Result.Author('Abeer Banerjee'), arxiv.Result.Author('Prashant Sadashiv Gidde'), arxiv.Result.Author('Sumeet Saurav'), arxiv.Result.Author('Sanjay Singh')]","Over the past few years, there has been a significant improvement in the
domain of few-shot learning. This learning paradigm has shown promising results
for the challenging problem of anomaly detection, where the general task is to
deal with heavy class imbalance. Our paper presents a new approach to few-shot
classification, where we employ the knowledge-base of multiple pre-trained
convolutional models that act as the backbone for our proposed few-shot
framework. Our framework uses a novel ensembling technique for boosting the
accuracy while drastically decreasing the total parameter count, thus paving
the way for real-time implementation. We perform an extensive hyperparameter
search using a power-line defect detection dataset and obtain an accuracy of
92.30% for the 5-way 5-shot task. Without further tuning, we evaluate our model
on competing standards with the existing state-of-the-art methods and
outperform them.",0.31481978,0.0686346,-0.1953001,A
9826,"Lucid data dreaming for video object seg-
baselines for further research.","Our            vision (ECCV), pages 54‚Äì70, 2018.
framework achieves the new state-of-the-art performance
on both DAVIS and the large-scale YouTube-VOS and we               [9] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and
believe that the proposed framework is a simple and strong              B. Schiele.",mentation.,2022-08-08 10:22:42+00:00,Two-Stream Networks for Object Segmentation in Videos,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hannan Lu'), arxiv.Result.Author('Zhi Tian'), arxiv.Result.Author('Lirong Yang'), arxiv.Result.Author('Haibing Ren'), arxiv.Result.Author('Wangmeng Zuo')]","Existing matching-based approaches perform video object segmentation (VOS)
via retrieving support features from a pixel-level memory, while some pixels
may suffer from lack of correspondence in the memory (i.e., unseen), which
inevitably limits their segmentation performance. In this paper, we present a
Two-Stream Network (TSN). Our TSN includes (i) a pixel stream with a
conventional pixel-level memory, to segment the seen pixels based on their
pixellevel memory retrieval. (ii) an instance stream for the unseen pixels,
where a holistic understanding of the instance is obtained with dynamic
segmentation heads conditioned on the features of the target instance. (iii) a
pixel division module generating a routing map, with which output embeddings of
the two streams are fused together. The compact instance stream effectively
improves the segmentation accuracy of the unseen pixels, while fusing two
streams with the adaptive routing map leads to an overall performance boost.
Through extensive experiments, we demonstrate the effectiveness of our proposed
TSN, and we also report state-of-the-art performance of 86.1% on YouTube-VOS
2018 and 87.5% on the DAVIS-2017 validation split.",-0.1812686,0.08436319,-0.061215065,B
9827,"Consequently, the developed
models and techniques will enable further research in non-invasive personaliza-
tion of the ventricular activation sequences for real patients.","In the future, we will extend this work by including a more realistic
representation of the cardiac conduction system.",Acknowledgement.,2022-08-08 10:23:43+00:00,Deep Computational Model for the Inference of Ventricular Activation Properties,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Julia Camps'), arxiv.Result.Author('Abhirup Banerjee'), arxiv.Result.Author('Marcel Beetz'), arxiv.Result.Author('Blanca Rodriguez'), arxiv.Result.Author('Vicente Grau')]","Patient-specific cardiac computational models are essential for the efficient
realization of precision medicine and in-silico clinical trials using digital
twins. Cardiac digital twins can provide non-invasive characterizations of
cardiac functions for individual patients, and therefore are promising for the
patient-specific diagnosis and therapy stratification. However, current
workflows for both the anatomical and functional twinning phases, referring to
the inference of model anatomy and parameter from clinical data, are not
sufficiently efficient, robust, and accurate. In this work, we propose a deep
learning based patient-specific computational model, which can fuse both
anatomical and electrophysiological information for the inference of
ventricular activation properties, i.e., conduction velocities and root nodes.
The activation properties can provide a quantitative assessment of cardiac
electrophysiological function for the guidance of interventional procedures. We
employ the Eikonal model to generate simulated electrocardiogram (ECG) with
ground truth properties to train the inference model, where specific patient
information has also been considered. For evaluation, we test the model on the
simulated data and obtain generally promising results with fast computational
time.",0.26220542,0.037172914,-0.02848152,A
9828,"but diÔ¨Äerent illumination types still constitute a gap for
How well a threshold selected on this test set translates    further research [46].",the test set).,"Nevertheless, for face recognition
to previously unseen subjects is not clear, but from com-    on mobile phones, we were able to decrease FNMR from
paring Fig.",2022-08-08 10:40:29+00:00,"Eight Years of Face Recognition Research: Reproducibility, Achievements and Open Issues",cs.CV,['cs.CV'],"[arxiv.Result.Author('Tiago de Freitas Pereira'), arxiv.Result.Author('Dominic Schimdli'), arxiv.Result.Author('Yu Linghu'), arxiv.Result.Author('Xinyi Zhang'), arxiv.Result.Author('S√©bastien Marcel'), arxiv.Result.Author('Manuel G√ºnther')]","Automatic face recognition is a research area with high popularity. Many
different face recognition algorithms have been proposed in the last thirty
years of intensive research in the field. With the popularity of deep learning
and its capability to solve a huge variety of different problems, face
recognition researchers have concentrated effort on creating better models
under this paradigm. From the year 2015, state-of-the-art face recognition has
been rooted in deep learning models. Despite the availability of large-scale
and diverse datasets for evaluating the performance of face recognition
algorithms, many of the modern datasets just combine different factors that
influence face recognition, such as face pose, occlusion, illumination, facial
expression and image quality. When algorithms produce errors on these datasets,
it is not clear which of the factors has caused this error and, hence, there is
no guidance in which direction more research is required. This work is a
followup from our previous works developed in 2014 and eventually published in
2016, showing the impact of various facial aspects on face recognition
algorithms. By comparing the current state-of-the-art with the best systems
from the past, we demonstrate that faces under strong occlusions, some types of
illumination, and strong expressions are problems mastered by deep learning
algorithms, whereas recognition with low-resolution images, extreme pose
variations, and open-set recognition is still an open problem. To show this, we
run a sequence of experiments using six different datasets and five different
face recognition algorithms in an open-source and reproducible manner. We
provide the source code to run all of our experiments, which is easily
extensible so that utilizing your own deep network in our evaluation is just a
few minutes away.",0.1200234,0.060340405,0.08562504,A
9829,"but diÔ¨Äerent illumination types still constitute a gap for
How well a threshold selected on this test set translates    further research [46].",the test set).,"Nevertheless, for face recognition
to previously unseen subjects is not clear, but from com-    on mobile phones, we were able to decrease FNMR from
paring Fig.",2022-08-08 10:40:29+00:00,"Eight Years of Face Recognition Research: Reproducibility, Achievements and Open Issues",cs.CV,['cs.CV'],"[arxiv.Result.Author('Tiago de Freitas Pereira'), arxiv.Result.Author('Dominic Schmidli'), arxiv.Result.Author('Yu Linghu'), arxiv.Result.Author('Xinyi Zhang'), arxiv.Result.Author('S√©bastien Marcel'), arxiv.Result.Author('Manuel G√ºnther')]","Automatic face recognition is a research area with high popularity. Many
different face recognition algorithms have been proposed in the last thirty
years of intensive research in the field. With the popularity of deep learning
and its capability to solve a huge variety of different problems, face
recognition researchers have concentrated effort on creating better models
under this paradigm. From the year 2015, state-of-the-art face recognition has
been rooted in deep learning models. Despite the availability of large-scale
and diverse datasets for evaluating the performance of face recognition
algorithms, many of the modern datasets just combine different factors that
influence face recognition, such as face pose, occlusion, illumination, facial
expression and image quality. When algorithms produce errors on these datasets,
it is not clear which of the factors has caused this error and, hence, there is
no guidance in which direction more research is required. This work is a
followup from our previous works developed in 2014 and eventually published in
2016, showing the impact of various facial aspects on face recognition
algorithms. By comparing the current state-of-the-art with the best systems
from the past, we demonstrate that faces under strong occlusions, some types of
illumination, and strong expressions are problems mastered by deep learning
algorithms, whereas recognition with low-resolution images, extreme pose
variations, and open-set recognition is still an open problem. To show this, we
run a sequence of experiments using six different datasets and five different
face recognition algorithms in an open-source and reproducible manner. We
provide the source code to run all of our experiments, which is easily
extensible so that utilizing your own deep network in our evaluation is just a
few minutes away.",0.1200234,0.060340405,0.08562504,A
9841,"Because this is an extensive topic,
we believe it warrants further research.","The other mask shapes are
very different, and they could potentially be used to make classifiers more robust
when mixed with regular data during training.","Noise Rotation Transparency

Train Accuracy 99.9 99.1                          94.7

Test Accuracy 14.96 13.51                         58.86

Table 1: Classifier test results for shape IM analysis.",2022-08-08 15:56:49+00:00,SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sameer Ambekar'), arxiv.Result.Author('Ankit Ankit'), arxiv.Result.Author('Diego van der Mast'), arxiv.Result.Author('Mark Alence'), arxiv.Result.Author('Matteo Tafuro')]","With the usage of appropriate inductive biases, Counterfactual Generative
Networks (CGNs) can generate novel images from random combinations of shape,
texture, and background manifolds. These images can be utilized to train an
invariant classifier, avoiding the wide spread problem of deep architectures
learning spurious correlations rather than meaningful ones. As a consequence,
out-of-domain robustness is improved. However, the CGN architecture comprises
multiple over parameterized networks, namely BigGAN and U2-Net. Training these
networks requires appropriate background knowledge and extensive computation.
Since one does not always have access to the precise training details, nor do
they always possess the necessary knowledge of counterfactuals, our work
addresses the following question: Can we use the knowledge embedded in
pre-trained CGNs to train a lower-capacity model, assuming black-box access
(i.e., only access to the pretrained CGN model) to the components of the
architecture? In this direction, we propose a novel work named SKDCGN that
attempts knowledge transfer using Knowledge Distillation (KD). In our proposed
architecture, each independent mechanism (shape, texture, background) is
represented by a student 'TinyGAN' that learns from the pretrained teacher
'BigGAN'. We demonstrate the efficacy of the proposed method using
state-of-the-art datasets such as ImageNet, and MNIST by using KD and
appropriate loss functions. Moreover, as an additional contribution, our paper
conducts a thorough study on the composition mechanism of the CGNs, to gain a
better understanding of how each mechanism influences the classification
accuracy of an invariant classifier. Code available at:
https://github.com/ambekarsameer96/SKDCGN",0.06447331,0.070799515,0.0476038,A
9842,"Because this is an extensive topic,
we believe it warrants further research.","The other mask shapes are
very different, and they could potentially be used to make classifiers more robust
when mixed with regular data during training.","Noise Rotation Transparency

Train Accuracy 99.9 99.1                          94.7

Test Accuracy 14.96 13.51                         58.86

Table 1: Classifier test results for shape IM analysis.",2022-08-08 15:56:49+00:00,SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sameer Ambekar'), arxiv.Result.Author('Ankit Ankit'), arxiv.Result.Author('Diego van der Mast'), arxiv.Result.Author('Mark Alence'), arxiv.Result.Author('Matteo Tafuro'), arxiv.Result.Author('Christos Athanasiadis')]","With the usage of appropriate inductive biases, Counterfactual Generative
Networks (CGNs) can generate novel images from random combinations of shape,
texture, and background manifolds. These images can be utilized to train an
invariant classifier, avoiding the wide spread problem of deep architectures
learning spurious correlations rather than meaningful ones. As a consequence,
out-of-domain robustness is improved. However, the CGN architecture comprises
multiple over parameterized networks, namely BigGAN and U2-Net. Training these
networks requires appropriate background knowledge and extensive computation.
Since one does not always have access to the precise training details, nor do
they always possess the necessary knowledge of counterfactuals, our work
addresses the following question: Can we use the knowledge embedded in
pre-trained CGNs to train a lower-capacity model, assuming black-box access
(i.e., only access to the pretrained CGN model) to the components of the
architecture? In this direction, we propose a novel work named SKDCGN that
attempts knowledge transfer using Knowledge Distillation (KD). In our proposed
architecture, each independent mechanism (shape, texture, background) is
represented by a student 'TinyGAN' that learns from the pretrained teacher
'BigGAN'. We demonstrate the efficacy of the proposed method using
state-of-the-art datasets such as ImageNet, and MNIST by using KD and
appropriate loss functions. Moreover, as an additional contribution, our paper
conducts a thorough study on the composition mechanism of the CGNs, to gain a
better understanding of how each mechanism influences the classification
accuracy of an invariant classifier. Code available at:
https://github.com/ambekarsameer96/SKDCGN",0.06447331,0.070799515,0.0476038,A
9843,"Because this is an extensive topic,
we believe it warrants further research.",when mixed with regular data during training.,"5 Discussion and conclusion

With the prevalence of heavily parameterized architectures such as BigGANs, and
with the advent of limited-access models like the trending DALL¬∑E 2, source-free
compression becomes a growing necessity.",2022-08-08 15:56:49+00:00,SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sameer Ambekar'), arxiv.Result.Author('Matteo Tafuro'), arxiv.Result.Author('Ankit Ankit'), arxiv.Result.Author('Diego van der Mast'), arxiv.Result.Author('Mark Alence'), arxiv.Result.Author('Christos Athanasiadis')]","With the usage of appropriate inductive biases, Counterfactual Generative
Networks (CGNs) can generate novel images from random combinations of shape,
texture, and background manifolds. These images can be utilized to train an
invariant classifier, avoiding the wide spread problem of deep architectures
learning spurious correlations rather than meaningful ones. As a consequence,
out-of-domain robustness is improved. However, the CGN architecture comprises
multiple over parameterized networks, namely BigGAN and U2-Net. Training these
networks requires appropriate background knowledge and extensive computation.
Since one does not always have access to the precise training details, nor do
they always possess the necessary knowledge of counterfactuals, our work
addresses the following question: Can we use the knowledge embedded in
pre-trained CGNs to train a lower-capacity model, assuming black-box access
(i.e., only access to the pretrained CGN model) to the components of the
architecture? In this direction, we propose a novel work named SKDCGN that
attempts knowledge transfer using Knowledge Distillation (KD). In our proposed
architecture, each independent mechanism (shape, texture, background) is
represented by a student 'TinyGAN' that learns from the pretrained teacher
'BigGAN'. We demonstrate the efficacy of the proposed method using
state-of-the-art datasets such as ImageNet, and MNIST by using KD and
appropriate loss functions. Moreover, as an additional contribution, our paper
conducts a thorough study on the composition mechanism of the CGNs, to gain a
better understanding of how each mechanism influences the classification
accuracy of an invariant classifier. Code available at:
https://github.com/ambekarsameer96/SKDCGN",0.17594221,-0.20032793,0.20582077,A
9844,"Although this paper unveils its potential, SKDCGN requires further research
that we encourage other researchers to undertake.",work to a lower-capacity model while still maintaining competitive performances.,"In addition to the suggestions
offered throughout the sections, possible avenues of research include and are not
limited to: improving the image generation process by using higher-order activa-
tion functions, since the utilized datasets consist of rich image data; improving
the teacher-student architecture by introducing additional loss functions; using a
learnable, neural network-based composition function instead of an analytical
expression.",2022-08-08 15:56:49+00:00,SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sameer Ambekar'), arxiv.Result.Author('Matteo Tafuro'), arxiv.Result.Author('Ankit Ankit'), arxiv.Result.Author('Diego van der Mast'), arxiv.Result.Author('Mark Alence'), arxiv.Result.Author('Christos Athanasiadis')]","With the usage of appropriate inductive biases, Counterfactual Generative
Networks (CGNs) can generate novel images from random combinations of shape,
texture, and background manifolds. These images can be utilized to train an
invariant classifier, avoiding the wide spread problem of deep architectures
learning spurious correlations rather than meaningful ones. As a consequence,
out-of-domain robustness is improved. However, the CGN architecture comprises
multiple over parameterized networks, namely BigGAN and U2-Net. Training these
networks requires appropriate background knowledge and extensive computation.
Since one does not always have access to the precise training details, nor do
they always possess the necessary knowledge of counterfactuals, our work
addresses the following question: Can we use the knowledge embedded in
pre-trained CGNs to train a lower-capacity model, assuming black-box access
(i.e., only access to the pretrained CGN model) to the components of the
architecture? In this direction, we propose a novel work named SKDCGN that
attempts knowledge transfer using Knowledge Distillation (KD). In our proposed
architecture, each independent mechanism (shape, texture, background) is
represented by a student 'TinyGAN' that learns from the pretrained teacher
'BigGAN'. We demonstrate the efficacy of the proposed method using
state-of-the-art datasets such as ImageNet, and MNIST by using KD and
appropriate loss functions. Moreover, as an additional contribution, our paper
conducts a thorough study on the composition mechanism of the CGNs, to gain a
better understanding of how each mechanism influences the classification
accuracy of an invariant classifier. Code available at:
https://github.com/ambekarsameer96/SKDCGN",0.01243577,-0.17208046,0.12163088,C
9845,"Because this is an extensive topic,
we believe it warrants further research.","As such, they can potentially be used to make classifiers more robust
when mixed with regular data during training.","5 Discussion and conclusion

With the prevalence of heavily parameterized architectures such as BigGANs, and
with the advent of limited-access models like the trending DALL¬∑E 2, source-free
compression becomes a growing necessity.",2022-08-08 15:56:49+00:00,SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sameer Ambekar'), arxiv.Result.Author('Matteo Tafuro'), arxiv.Result.Author('Ankit Ankit'), arxiv.Result.Author('Diego van der Mast'), arxiv.Result.Author('Mark Alence'), arxiv.Result.Author('Christos Athanasiadis')]","With the usage of appropriate inductive biases, Counterfactual Generative
Networks (CGNs) can generate novel images from random combinations of shape,
texture, and background manifolds. These images can be utilized to train an
invariant classifier, avoiding the wide spread problem of deep architectures
learning spurious correlations rather than meaningful ones. As a consequence,
out-of-domain robustness is improved. However, the CGN architecture comprises
multiple over parameterized networks, namely BigGAN and U2-Net. Training these
networks requires appropriate background knowledge and extensive computation.
Since one does not always have access to the precise training details, nor do
they always possess the necessary knowledge of counterfactuals, our work
addresses the following question: Can we use the knowledge embedded in
pre-trained CGNs to train a lower-capacity model, assuming black-box access
(i.e., only access to the pretrained CGN model) to the components of the
architecture? In this direction, we propose a novel work named SKDCGN that
attempts knowledge transfer using Knowledge Distillation (KD). In our proposed
architecture, each independent mechanism (shape, texture, background) is
represented by a student 'TinyGAN' that learns from the pretrained teacher
'BigGAN'. We demonstrate the efficacy of the proposed method using
state-of-the-art datasets such as ImageNet, and MNIST by using KD and
appropriate loss functions. Moreover, as an additional contribution, our paper
conducts a thorough study on the composition mechanism of the CGNs, to gain a
better understanding of how each mechanism influences the classification
accuracy of an invariant classifier. Code available at:
https://github.com/ambekarsameer96/SKDCGN",0.09014783,-0.25987208,0.15211174,C
9846,"Although this paper unveils its potential, SKDCGN requires further research
that we encourage other researchers to undertake.","6 Future work

To conclude, the experimental findings of SKDCGN prove that, upon the usage of
Knowledge Distillation, one can transfer the capacity/ability of a cumbersome net-
work to a lower-capacity model while still maintaining competitive performances.","In addition to the suggestions
offered throughout the sections, possible avenues of research include and are not
limited to: improving the image generation process by using higher-order activa-
tion functions, since the utilized datasets consist of rich image data; improving
the teacher-student architecture by introducing additional loss functions; using a
learnable, neural network-based composition function instead of an analytical
expression.",2022-08-08 15:56:49+00:00,SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sameer Ambekar'), arxiv.Result.Author('Matteo Tafuro'), arxiv.Result.Author('Ankit Ankit'), arxiv.Result.Author('Diego van der Mast'), arxiv.Result.Author('Mark Alence'), arxiv.Result.Author('Christos Athanasiadis')]","With the usage of appropriate inductive biases, Counterfactual Generative
Networks (CGNs) can generate novel images from random combinations of shape,
texture, and background manifolds. These images can be utilized to train an
invariant classifier, avoiding the wide spread problem of deep architectures
learning spurious correlations rather than meaningful ones. As a consequence,
out-of-domain robustness is improved. However, the CGN architecture comprises
multiple over parameterized networks, namely BigGAN and U2-Net. Training these
networks requires appropriate background knowledge and extensive computation.
Since one does not always have access to the precise training details, nor do
they always possess the necessary knowledge of counterfactuals, our work
addresses the following question: Can we use the knowledge embedded in
pre-trained CGNs to train a lower-capacity model, assuming black-box access
(i.e., only access to the pretrained CGN model) to the components of the
architecture? In this direction, we propose a novel work named SKDCGN that
attempts knowledge transfer using Knowledge Distillation (KD). In our proposed
architecture, each independent mechanism (shape, texture, background) is
represented by a student 'TinyGAN' that learns from the pretrained teacher
'BigGAN'. We demonstrate the efficacy of the proposed method using
state-of-the-art datasets such as ImageNet, and MNIST by using KD and
appropriate loss functions. Moreover, as an additional contribution, our paper
conducts a thorough study on the composition mechanism of the CGNs, to gain a
better understanding of how each mechanism influences the classification
accuracy of an invariant classifier. Code available at:
https://github.com/ambekarsameer96/SKDCGN",0.08136725,-0.24914202,0.13387302,C
9862,"426‚Äì435, 2020.
unlabeled images to improve the performance of text-to-image
generation deserves further study.","1, pp.","We will try to explore this    [12] Y. Xu and P. Ghamisi, ‚ÄúConsistency-regularized region-growing network
topic in our future work.",2022-08-08 22:02:10+00:00,Txt2Img-MHN: Remote Sensing Image Generation from Text Using Modern Hopfield Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yonghao Xu'), arxiv.Result.Author('Weikang Yu'), arxiv.Result.Author('Pedram Ghamisi'), arxiv.Result.Author('Michael Kopp'), arxiv.Result.Author('Sepp Hochreiter')]","The synthesis of high-resolution remote sensing images based on text
descriptions has great potential in many practical application scenarios.
Although deep neural networks have achieved great success in many important
remote sensing tasks, generating realistic remote sensing images from text
descriptions is still very difficult. To address this challenge, we propose a
novel text-to-image modern Hopfield network (Txt2Img-MHN). The main idea of
Txt2Img-MHN is to conduct hierarchical prototype learning on both text and
image embeddings with modern Hopfield layers. Instead of directly learning
concrete but highly diverse text-image joint feature representations for
different semantics, Txt2Img-MHN aims to learn the most representative
prototypes from text-image embeddings, achieving a coarse-to-fine learning
strategy. These learned prototypes can then be utilized to represent more
complex semantics in the text-to-image generation task. To better evaluate the
realism and semantic consistency of the generated images, we further conduct
zero-shot classification on real remote sensing data using the classification
model trained on synthesized images. Despite its simplicity, we find that the
overall accuracy in the zero-shot classification may serve as a good metric to
evaluate the ability to generate an image from text. Extensive experiments on
the benchmark remote sensing text-image dataset demonstrate that the proposed
Txt2Img-MHN can generate more realistic remote sensing images than existing
methods. Code and pre-trained models are available online
(https://github.com/YonghaoXu/Txt2Img-MHN).",-0.12567422,-0.1416367,0.043958157,C
9871,"The proposed methodology focuses solely on melanoma, however,
we suggest that further research can facilitate exact likelihood-based OOD detec-
tion for other areas of oncology with large data imbalances to improve detection
accuracy.","We recommend includ-
ing a term in the likelihood calculations that correct for presence of hairs in
future work.","EÔ¨Écient OOD Detection with Normalizing Flows  9

References

 1.",2022-08-09 09:57:56+00:00,Efficient Out-of-Distribution Detection of Melanoma with Wavelet-based Normalizing Flows,cs.CV,['cs.CV'],"[arxiv.Result.Author('M. M. Amaan Valiuddin'), arxiv.Result.Author('Christiaan G. A. Viviers'), arxiv.Result.Author('Ruud J. G. van Sloun'), arxiv.Result.Author('Peter H. N. de With'), arxiv.Result.Author('Fons van der Sommen')]","Melanoma is a serious form of skin cancer with high mortality rate at later
stages. Fortunately, when detected early, the prognosis of melanoma is
promising and malignant melanoma incidence rates are relatively low. As a
result, datasets are heavily imbalanced which complicates training current
state-of-the-art supervised classification AI models. We propose to use
generative models to learn the benign data distribution and detect
Out-of-Distribution (OOD) malignant images through density estimation.
Normalizing Flows (NFs) are ideal candidates for OOD detection due to their
ability to compute exact likelihoods. Nevertheless, their inductive biases
towards apparent graphical features rather than semantic context hamper
accurate OOD detection. In this work, we aim at using these biases with
domain-level knowledge of melanoma, to improve likelihood-based OOD detection
of malignant images. Our encouraging results demonstrate potential for OOD
detection of melanoma using NFs. We achieve a 9\% increase in Area Under Curve
of the Receiver Operating Characteristics by using wavelet-based NFs. This
model requires significantly less parameters for inference making it more
applicable on edge devices. The proposed methodology can aid medical experts
with diagnosis of skin-cancer patients and continuously increase survival
rates. Furthermore, this research paves the way for other areas in oncology
with similar data imbalance issues\footnote{Code available at:
https://github.com/A-Vzer/WaveletFlowPytorch}",0.07678657,0.036445457,0.020074662,A
9872,"The proposed methodology focuses solely on melanoma, however,
we suggest that further research can facilitate exact likelihood-based OOD detec-
tion for other areas of oncology with large data imbalances to improve detection
accuracy.","We recommend includ-
ing a term in the likelihood calculations that correct for presence of hairs in
future work.","EÔ¨Écient OOD Detection with Normalizing Flows  9

References

 1.",2022-08-09 09:57:56+00:00,Efficient Out-of-Distribution Detection of Melanoma with Wavelet-based Normalizing Flows,cs.CV,['cs.CV'],"[arxiv.Result.Author('M. M. Amaan Valiuddin'), arxiv.Result.Author('Christiaan G. A. Viviers'), arxiv.Result.Author('Ruud J. G. van Sloun'), arxiv.Result.Author('Peter H. N. de With'), arxiv.Result.Author('Fons van der Sommen')]","Melanoma is a serious form of skin cancer with high mortality rate at later
stages. Fortunately, when detected early, the prognosis of melanoma is
promising and malignant melanoma incidence rates are relatively low. As a
result, datasets are heavily imbalanced which complicates training current
state-of-the-art supervised classification AI models. We propose to use
generative models to learn the benign data distribution and detect
Out-of-Distribution (OOD) malignant images through density estimation.
Normalizing Flows (NFs) are ideal candidates for OOD detection due to their
ability to compute exact likelihoods. Nevertheless, their inductive biases
towards apparent graphical features rather than semantic context hamper
accurate OOD detection. In this work, we aim at using these biases with
domain-level knowledge of melanoma, to improve likelihood-based OOD detection
of malignant images. Our encouraging results demonstrate potential for OOD
detection of melanoma using NFs. We achieve a 9% increase in Area Under Curve
of the Receiver Operating Characteristics by using wavelet-based NFs. This
model requires significantly less parameters for inference making it more
applicable on edge devices. The proposed methodology can aid medical experts
with diagnosis of skin-cancer patients and continuously increase survival
rates. Furthermore, this research paves the way for other areas in oncology
with similar data imbalance issues.",0.07678657,0.036445457,0.020074662,A
9960,"We hope this work will inspire further research
towards information mixture to improve the performance of visual recognition.","The overall
MixSKD outperforms state-of-the-art data augmentation and Self-KD methods
on computer vision benchmarks.",Acknowledgment.,2022-08-11 11:57:26+00:00,MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chuanguang Yang'), arxiv.Result.Author('Zhulin An'), arxiv.Result.Author('Helong Zhou'), arxiv.Result.Author('Linhang Cai'), arxiv.Result.Author('Xiang Zhi'), arxiv.Result.Author('Jiwen Wu'), arxiv.Result.Author('Yongjun Xu'), arxiv.Result.Author('Qian Zhang')]","Unlike the conventional Knowledge Distillation (KD), Self-KD allows a network
to learn knowledge from itself without any guidance from extra networks. This
paper proposes to perform Self-KD from image Mixture (MixSKD), which integrates
these two techniques into a unified framework. MixSKD mutually distills feature
maps and probability distributions between the random pair of original images
and their mixup images in a meaningful way. Therefore, it guides the network to
learn cross-image knowledge by modelling supervisory signals from mixup images.
Moreover, we construct a self-teacher network by aggregating multi-stage
feature maps for providing soft labels to supervise the backbone classifier,
further improving the efficacy of self-boosting. Experiments on image
classification and transfer learning to object detection and semantic
segmentation demonstrate that MixSKD outperforms other state-of-the-art Self-KD
and data augmentation methods. The code is available at
https://github.com/winycg/Self-KD-Lib.",-0.17082772,0.0019618636,-0.0055223648,B
9974,Visualizations                                                  speaking countries requires further research.,"on an uncurated image-text dataset containing English-only
                                                                     captions, performance on images collected from non-English
A.3.","In future works,
                                                                     one may apply the MILAN method by taking multi-lingual
   In Figure 4, we provide visualizations of the learned repre-      language assisted representations as the reconstruction target.",2022-08-11 21:58:36+00:00,MILAN: Masked Image Pretraining on Language Assisted Representation,cs.CV,"['cs.CV', 'cs.CL', 'cs.LG']","[arxiv.Result.Author('Zejiang Hou'), arxiv.Result.Author('Fei Sun'), arxiv.Result.Author('Yen-Kuang Chen'), arxiv.Result.Author('Yuan Xie'), arxiv.Result.Author('Sun-Yuan Kung')]","Self-attention based transformer models have been dominating many computer
vision tasks in the past few years. Their superb model qualities heavily depend
on the excessively large labeled image datasets. In order to reduce the
reliance on large labeled datasets, reconstruction based masked autoencoders
are gaining popularity, which learn high quality transferable representations
from unlabeled images. For the same purpose, recent weakly supervised image
pretraining methods explore language supervision from text captions
accompanying the images. In this work, we propose masked image pretraining on
language assisted representation, dubbed as MILAN. Instead of predicting raw
pixels or low level features, our pretraining objective is to reconstruct the
image features with substantial semantic signals that are obtained using
caption supervision. Moreover, to accommodate our reconstruction target, we
propose a more effective prompting decoder architecture and a semantic aware
mask sampling mechanism, which further advance the transfer performance of the
pretrained model. Experimental results demonstrate that MILAN delivers higher
accuracy than the previous works. When the masked autoencoder is pretrained and
finetuned on ImageNet-1K dataset with an input resolution of 224x224, MILAN
achieves a top-1 accuracy of 85.4% on ViT-Base, surpassing previous
state-of-the-arts by 1%. In the downstream semantic segmentation task, MILAN
achieves 52.7 mIoU using ViT-Base on ADE20K dataset, outperforming previous
masked pretraining results by 4 points.",-0.18536066,-0.18692377,-0.1759675,C
10002,"We can clearly see
retrieval scheme due to the problem of satellite image and           that there is a signiÔ¨Åcant decrease in positioning accuracy in
UAV image scale, however, we did not do further research in          the case of >1.0.",7(b).,This result is not difÔ¨Åcult to explain.,2022-08-13 03:25:50+00:00,Finding Point with Image: An End-to-End Benchmark for Vision-based UAV Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ming Dai'), arxiv.Result.Author('Jiahao Chen'), arxiv.Result.Author('Yusheng Lu'), arxiv.Result.Author('Wenlong Hao'), arxiv.Result.Author('Enhui Zheng')]","In the past, image retrieval was the mainstream solution for cross-view
geolocation and UAV visual localization tasks. In a nutshell, the way of image
retrieval is to obtain the final required information, such as GPS, through a
transitional perspective. However, the way of image retrieval is not completely
end-to-end. And there are some redundant operations such as the need to prepare
the feature library in advance, and the sampling interval problem of the
gallery construction, which make it difficult to implement large-scale
applications. In this article we propose an end-to-end positioning scheme,
Finding Point with Image (FPI), which aims to directly find the corresponding
location in the image of source B (satellite-view) through the image of source
A (drone-view). To verify the feasibility of our framework, we construct a new
dataset (UL14), which is designed to solve the UAV visual self-localization
task. At the same time, we also build a transformer-based baseline to achieve
end-to-end training. In addition, the previous evaluation methods are no longer
applicable under the framework of FPI. Thus, Metre-level Accuracy (MA) and
Relative Distance Score (RDS) are proposed to evaluate the accuracy of UAV
localization. At the same time, we preliminarily compare FPI and image
retrieval method, and the structure of FPI achieves better performance in both
speed and efficiency. In particular, the task of FPI remains great challenges
due to the large differences between different views and the drastic spatial
scale transformation.",0.02031286,0.24156478,-0.012929181,B
10025,"While the problem
we investigated in this work remains open challenge in the domain of visual
perception, we hope that it can inspire further researches in this direction.","Lastly, we also
evaluate the explainability of the models trained on our task.",14  P. Saranrittichai et al.,2022-08-14 09:04:52+00:00,Multi-Attribute Open Set Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Piyapat Saranrittichai'), arxiv.Result.Author('Chaithanya Kumar Mummadi'), arxiv.Result.Author('Claudia Blaiotta'), arxiv.Result.Author('Mauricio Munoz'), arxiv.Result.Author('Volker Fischer')]","Open Set Recognition (OSR) extends image classification to an open-world
setting, by simultaneously classifying known classes and identifying unknown
ones. While conventional OSR approaches can detect Out-of-Distribution (OOD)
samples, they cannot provide explanations indicating which underlying visual
attribute(s) (e.g., shape, color or background) cause a specific sample to be
unknown. In this work, we introduce a novel problem setup that generalizes
conventional OSR to a multi-attribute setting, where multiple visual attributes
are simultaneously recognized. Here, OOD samples can be not only identified but
also categorized by their unknown attribute(s). We propose simple extensions of
common OSR baselines to handle this novel scenario. We show that these
baselines are vulnerable to shortcuts when spurious correlations exist in the
training dataset. This leads to poor OOD performance which, according to our
experiments, is mainly due to unintended cross-attribute correlations of the
predicted confidence scores. We provide an empirical evidence showing that this
behavior is consistent across different baselines on both synthetic and real
world datasets.",-0.24118069,0.0021745004,-0.07090295,B
10032,"We were also able to formulate constraints that would aid further researchers
to choose valid values for the height and width of temporal images and for the
number of frames to skip.","20
Figure 10: Learning curves for validation accuracy for all models on attempt
without restoring weights from best epoch

5 Discussion

Thus we were able to successfully build and check the validity of temporal im-
ages using multiple ImageNet models and for several diÔ¨Äerent hyperparameters.","It was found that the dataset must be split into train,
validation and test sets at the video level and not at the frame/temporal image
level to avoid overÔ¨Åtting the model on the given dataset.",2022-08-15 03:32:28+00:00,Deepfake Detection using ImageNet models and Temporal Images of 468 Facial Landmarks,cs.CV,['cs.CV'],[arxiv.Result.Author('Christeen T Jose')],"This paper presents our results and findings on the use of temporal images
for deepfake detection. We modelled temporal relations that exist in the
movement of 468 facial landmarks across frames of a given video as spatial
relations by constructing an image (referred to as temporal image) using the
pixel values at these facial landmarks. CNNs are capable of recognizing spatial
relationships that exist between the pixels of a given image. 10 different
ImageNet models were considered for the study.",-0.14968145,-0.03800933,0.031111136,C
10050,"MSE MMSE
                           w. face extraction 0.0667 0.0806
                           w/o face extraction 0.0759 0.1227

We leave it open for further research.","18
     Table 5: Ablation study on the face extraction approach with CavT on EmotiW-EP.","The fourth column of Table IV shows the number of frames in a video se-

quence.",2022-08-12 01:21:30+00:00,Class-attention Video Transformer for Engagement Intensity Prediction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xusheng Ai'), arxiv.Result.Author('Victor S. Sheng'), arxiv.Result.Author('Chunhua Li')]","In order to deal with variant-length long videos, prior works extract
multi-modal features and fuse them to predict students' engagement intensity.
In this paper, we present a new end-to-end method Class Attention in Video
Transformer (CavT), which involves a single vector to process class embedding
and to uniformly perform end-to-end learning on variant-length long videos and
fixed-length short videos. Furthermore, to address the lack of sufficient
samples, we propose a binary-order representatives sampling method (BorS) to
add multiple video sequences of each video to augment the training set.
BorS+CavT not only achieves the state-of-the-art MSE (0.0495) on the EmotiW-EP
dataset, but also obtains the state-of-the-art MSE (0.0377) on the DAiSEE
dataset. The code and models will be made publicly available at
https://github.com/mountainai/cavt.",0.25304678,0.17572895,-0.09047963,A
10051,We leave it open for further research.,"This indicates that class imbalance
instead of insuÔ¨Écient data is a major obstacle to the prediction of engagement
intensity.","The fourth column of Table IV shows the number of frames in a video
sequence.",2022-08-12 01:21:30+00:00,Class-attention Video Transformer for Engagement Intensity Prediction,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xusheng Ai'), arxiv.Result.Author('Victor S. Sheng'), arxiv.Result.Author('Chunhua Li'), arxiv.Result.Author('Zhiming Cui')]","In order to deal with variant-length long videos, prior works extract
multi-modal features and fuse them to predict students' engagement intensity.
In this paper, we present a new end-to-end method Class Attention in Video
Transformer (CavT), which involves a single vector to process class embedding
and to uniformly perform end-to-end learning on variant-length long videos and
fixed-length short videos. Furthermore, to address the lack of sufficient
samples, we propose a binary-order representatives sampling method (BorS) to
add multiple video sequences of each video to augment the training set.
BorS+CavT not only achieves the state-of-the-art MSE (0.0495) on the EmotiW-EP
dataset, but also obtains the state-of-the-art MSE (0.0377) on the DAiSEE
dataset. The code and models have been made publicly available at
https://github.com/mountainai/cavt.",0.082615025,-0.031030534,-0.23388702,A
10089,"1 - 95.3 94.6 95.3 84.9 91.1 86.9 77.2 84.4 79.7 77.3 44.4 95.6 85.2 76.9 66.7 82.4

            BN‚Äì1    1 16 86.6 85.2 86.0 86.6 86.5 75.7 65.1 67.8 70.5 55.2 37.8 84.6 59.1 55.0 63.7 71.0

ImageNet-C  BN‚Äì1    1 32 85.4 84.0 84.9 85.6 85.0 74.1 61.9 66.8 68.4 52.6 36.1 83.9 57.1 52.3 61.7 69.3

            GTTA-MIX 1 16 80.8 73.8 71.6 79.3 79.3 70.0 61.1 58.1 59.8 47.6 37.5 73.8 54.2 51.1 53.3 63.4

            GTTA-MIX 1 32 79.8 73.4 70.7 77.3 76.0 64.6 55.8 57.4 58.5 45.5 34.4 67.1 50.5 45.7 49.4 60.4

            GTTA-MIX 64 - 80.5 74.7 72.4 77.8 75.7 64.3 54.0 57.0 58.6 44.6 33.9 67.5 49.4 44.7 49.3 60.3

            GTTA-ST 1 16 80.9 74.3 74.7 77.7 77.3 66.3 58.2 59.0 60.4 47.6 37.6 62.9 53.5 48.8 52.6 62.1

            GTTA-ST 1 32 80.5 74.4 74.0 77.3 74.8 63.5 54.3 56.4 57.6 45.2 33.5 61.8 50.0 45.7 50.7 60.0

            GTTA-ST 64 - 80.6 74.1 74.3 76.8 74.9 62.3 53.9 56.4 58.0 44.1 33.4 62.2 48.6 44.9 50.4 59.7

B Ablation studies for CarlaTTA

B.1 Gradual shifts for CarlaTTA

Denoting ‚àÜt as a proxy for the gradual shift, we generate a dynamic and a day2night sequence which allow to further study
the beneÔ¨Åts of gradual TTA.","1 - 73.0 68.0 39.4 29.3 54.1 30.8 28.8 39.5 45.8 50.3 29.5 55.1 37.2 74.7 41.2 46.4

CIFAR100C   BN‚Äì1    1 16 46.2 44.5 47.1 31.4 46.5 33.3 31.7 39.2 38.6 45.4 30.8 34.7 40.6 37.5 45.2 39.5

            BN‚Äì1    1 32 44.1 42.4 45.2 29.6 43.6 31.0 30.0 37.1 36.7 43.8 28.7 32.6 38.0 35.3 42.9 37.4

            GTTA-MIX 1 16 40.1 36.1 37.8 28.3 40.4 29.8 28.3 32.7 31.1 34.2 26.2 27.5 33.5 30.4 38.2 33.0

            GTTA-MIX 1 32 38.9 35.0 36.8 26.6 38.3 28.0 26.5 31.2 30.1 34.0 24.4 26.2 32.1 28.6 36.4 31.5

            GTTA-MIX 200 - 39.4 34.4 36.6 24.7 36.8 26.6 24.3 30.1 28.9 34.6 22.8 25.1 30.7 26.9 34.7 30.4

            BN‚Äì0 (src.)","By sub-sampling the sequences, we achieve varying degrees of gradual shift.",2022-08-16 13:12:19+00:00,Introducing Intermediate Domains for Effective Self-Training during Test-Time,cs.CV,['cs.CV'],"[arxiv.Result.Author('Robert A. Marsden'), arxiv.Result.Author('Mario D√∂bler'), arxiv.Result.Author('Bin Yang')]","Experiencing domain shifts during test-time is nearly inevitable in practice
and likely results in a severe performance degradation. To overcome this issue,
test-time adaptation continues to update the initial source model during
deployment. A promising direction are methods based on self-training which have
been shown to be well suited for gradual domain adaptation, since reliable
pseudo-labels can be provided. In this work, we address two problems that exist
when applying self-training in the setting of test-time adaptation. First,
adapting a model to long test sequences that contain multiple domains can lead
to error accumulation. Second, naturally, not all shifts are gradual in
practice. To tackle these challenges, we introduce GTTA. By creating artificial
intermediate domains that divide the current domain shift into a more gradual
one, effective self-training through high quality pseudo-labels can be
performed. To create the intermediate domains, we propose two independent
variations: mixup and light-weight style transfer. We demonstrate the
effectiveness of our approach on the continual and gradual corruption
benchmarks, as well as ImageNet-R. To further investigate gradual shifts in the
context of urban scene segmentation, we publish a new benchmark: CarlaTTA. It
enables the exploration of several non-stationary domain shifts.",0.20056787,0.18049988,0.16891806,A
10105,"Datasets and code will be released
upon publication to spur further research.","Our
GERA, KARIMI, RENAUD, PJN, LALONDE: PANOHDR-NERF  3

approach can render 360‚ó¶ HDR light probes, which can be used to provide correct lighting
effects when the scene is augmented with virtual objects.","2 Related work

Inverse tonemapping Inverse tonemapping aims to recover missing information in the
over- and under-saturated areas of an LDR image.",2022-08-16 18:45:27+00:00,Casual Indoor HDR Radiance Capture from Omnidirectional Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pulkit Gera'), arxiv.Result.Author('Mohammad Reza Karimi Dastjerdi'), arxiv.Result.Author('Charles Renaud'), arxiv.Result.Author('P. J. Narayanan'), arxiv.Result.Author('Jean-Fran√ßois Lalonde')]","We present PanoHDR-NeRF, a novel pipeline to casually capture a plausible
full HDR radiance field of a large indoor scene without elaborate setups or
complex capture protocols. First, a user captures a low dynamic range (LDR)
omnidirectional video of the scene by freely waving an off-the-shelf camera
around the scene. Then, an LDR2HDR network uplifts the captured LDR frames to
HDR, subsequently used to train a tailored NeRF++ model. The resulting
PanoHDR-NeRF pipeline can estimate full HDR panoramas from any location of the
scene. Through experiments on a novel test dataset of a variety of real scenes
with the ground truth HDR radiance captured at locations not seen during
training, we show that PanoHDR-NeRF predicts plausible radiance from any scene
point. We also show that the HDR images produced by PanoHDR-NeRF can synthesize
correct lighting effects, enabling the augmentation of indoor scenes with
synthetic objects that are lit correctly.",0.0025705062,0.29473698,0.11463662,B
10121,"Multiple frameworks were also open-sourced to                   ReID challenge, we provide a simple CNN-based feature extractor as
support further research on supervised [50, 19] or unsupervised                  a baseline.","To address the DeepSportradar
based ReID [47].","This feature extractor was implemented using the Open-
ReID [14].",2022-08-17 09:55:02+00:00,DeepSportradar-v1: Computer Vision Dataset for Sports Understanding with High Quality Annotations,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV', 'I.2.10; I.2.6; I.4.9; I.4.8; I.4.6; I.4.5; I.4; I.5.4']","[arxiv.Result.Author('Gabriel Van Zandycke'), arxiv.Result.Author('Vladimir Somers'), arxiv.Result.Author('Maxime Istasse'), arxiv.Result.Author('Carlo Del Don'), arxiv.Result.Author('Davide Zambrano')]","With the recent development of Deep Learning applied to Computer Vision,
sport video understanding has gained a lot of attention, providing much richer
information for both sport consumers and leagues. This paper introduces
DeepSportradar-v1, a suite of computer vision tasks, datasets and benchmarks
for automated sport understanding. The main purpose of this framework is to
close the gap between academic research and real world settings. To this end,
the datasets provide high-resolution raw images, camera parameters and high
quality annotations. DeepSportradar currently supports four challenging tasks
related to basketball: ball 3D localization, camera calibration, player
instance segmentation and player re-identification. For each of the four tasks,
a detailed description of the dataset, objective, performance metrics, and the
proposed baseline method are provided. To encourage further research on
advanced methods for sport understanding, a competition is organized as part of
the MMSports workshop from the ACM Multimedia 2022 conference, where
participants have to develop state-of-the-art methods to solve the above tasks.
The four datasets, development kits and baselines are publicly available.",-0.32025644,-0.15295286,-0.0002593249,C
10155,"To further study the importance of each M3Depth component, the various
components of M3Depth were removed in turn.","M3Depth generated depth maps
with high contrast between the foreground and background and performed better at
distinguishing diÔ¨Äerent parts of the scene, reÔ¨Çecting the superior quantitative results
in Table 2.

mean squared logarithmic error (RMSE log), and the ratio between ground truth
and prediction values, for which the threshold was denoted as Œ¥.

Baseline The M3Depth model was compared with several recent deep learning
methods including Monodepth [4], Monodepth2 [5], and PackNet [6], and both
quantitative and qualitative results were generated and reported for compari-
son.","Implementation Details M3Depth was implemented in PyTorch [20], with
an input/output resolution of 256 √ó 320 and a batch size of 18.",2022-08-17 17:03:48+00:00,Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency,cs.CV,"['cs.CV', 'cs.CG']","[arxiv.Result.Author('Baoru Huang'), arxiv.Result.Author('Jian-Qing Zheng'), arxiv.Result.Author('Anh Nguyen'), arxiv.Result.Author('Chi Xu'), arxiv.Result.Author('Ioannis Gkouzionis'), arxiv.Result.Author('Kunal Vyas'), arxiv.Result.Author('David Tuch'), arxiv.Result.Author('Stamatia Giannarou'), arxiv.Result.Author('Daniel S. Elson')]","Depth estimation is a crucial step for image-guided intervention in robotic
surgery and laparoscopic imaging system. Since per-pixel depth ground truth is
difficult to acquire for laparoscopic image data, it is rarely possible to
apply supervised depth estimation to surgical applications. As an alternative,
self-supervised methods have been introduced to train depth estimators using
only synchronized stereo image pairs. However, most recent work focused on the
left-right consistency in 2D and ignored valuable inherent 3D information on
the object in real world coordinates, meaning that the left-right 3D geometric
structural consistency is not fully utilized. To overcome this limitation, we
present M3Depth, a self-supervised depth estimator to leverage 3D geometric
structural information hidden in stereo pairs while keeping monocular
inference. The method also removes the influence of border regions unseen in at
least one of the stereo images via masking, to enhance the correspondences
between left and right images in overlapping areas. Intensive experiments show
that our method outperforms previous self-supervised approaches on both a
public dataset and a newly acquired dataset by a large margin, indicating a
good generalization across different samples and laparoscopes.",-0.05686026,0.010979819,0.28450155,C
10212,"This suggests,
further research in the diversity aspect is important.","Even though our model has the best diversity
scores, looking at some samples of generated captions, they are often rather
general and might miss some important detail about the video.","As indicated in Section 4.4, the current extraction of concepts for video cap-
tioning might need further improvement, potentially by the use of a larger train-
ing data set.",2022-08-19 11:21:59+00:00,Diverse Video Captioning by Adaptive Spatio-temporal Attention,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zohreh Ghaderi'), arxiv.Result.Author('Leonard Salewski'), arxiv.Result.Author('Hendrik P. A. Lensch')]","To generate proper captions for videos, the inference needs to identify
relevant concepts and pay attention to the spatial relationships between them
as well as to the temporal development in the clip. Our end-to-end
encoder-decoder video captioning framework incorporates two transformer-based
architectures, an adapted transformer for a single joint spatio-temporal video
analysis as well as a self-attention-based decoder for advanced text
generation. Furthermore, we introduce an adaptive frame selection scheme to
reduce the number of required incoming frames while maintaining the relevant
content when training both transformers. Additionally, we estimate semantic
concepts relevant for video captioning by aggregating all ground truth captions
of each sample. Our approach achieves state-of-the-art results on the MSVD, as
well as on the large-scale MSR-VTT and the VATEX benchmark datasets considering
multiple Natural Language Generation (NLG) metrics. Additional evaluations on
diversity scores highlight the expressiveness and diversity in the structure of
our generated captions.",-0.09664374,-0.13658297,-0.19960654,C
10216,"We further study the effectiveness of
KNN expansion (SPTM+F(K)) , and (5) SPTM+A+F(K).","augmentation (SPTM+A), (4) SPTM with feature-neighbor                 Dynamic KNN.","dynamic KNN compared to Ô¨Åxed KNN in TF-VPR, i.e.,
Note that the contraction step is not used for this simulated      SPTM+A+F(D) vs. SPTM+A+F(K).",2022-08-19 12:59:46+00:00,Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Chen'), arxiv.Result.Author('Xinhao Liu'), arxiv.Result.Author('Xuchu Xu'), arxiv.Result.Author('Yiming Li'), arxiv.Result.Author('Li Ding'), arxiv.Result.Author('Ruoyu Wang'), arxiv.Result.Author('Chen Feng')]","Visual place recognition (VPR) using deep networks has achieved
state-of-the-art performance. However, most of them require a training set with
ground truth sensor poses to obtain positive and negative samples of each
observation's spatial neighborhood for supervised learning. When such
information is unavailable, temporal neighborhoods from a sequentially
collected data stream could be exploited for self-supervised training, although
we find its performance suboptimal. Inspired by noisy label learning, we
propose a novel self-supervised framework named \textit{TF-VPR} that uses
temporal neighborhoods and learnable feature neighborhoods to discover unknown
spatial neighborhoods. Our method follows an iterative training paradigm which
alternates between: (1) representation learning with data augmentation, (2)
positive set expansion to include the current feature space neighbors, and (3)
positive set contraction via geometric verification. We conduct comprehensive
experiments on both simulated and real datasets, with either RGB images or
point clouds as inputs. The results show that our method outperforms our
baselines in recall rate, robustness, and heading diversity, a novel metric we
propose for VPR. Our code and datasets can be found at
https://ai4ce.github.io/TF-VPR/.",0.27019298,0.060943317,0.17824872,A
10234,"The core idea is simple and general, which can
architecture for dealing with images with large image sizes        potentially inspire further research works and broaden the
(e.g., 1333 √ó 800).",There exist 5 FPN levels in the original             vious arts.,To match the situation of the ImageNet         applicable scenarios related to video object detection.,2022-08-20 14:12:06+00:00,YOLOV: Making Still Image Object Detectors Great at Video Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuheng Shi'), arxiv.Result.Author('Naiyan Wang'), arxiv.Result.Author('Xiaojie Guo')]","Video object detection (VID) is challenging because of the high variation of
object appearance as well as the diverse deterioration in some frames. On the
positive side, the detection in a certain frame of a video, compared with in a
still image, can draw support from other frames. Hence, how to aggregate
features across different frames is pivotal to the VID problem. Most of
existing aggregation algorithms are customized for two-stage detectors. But,
the detectors in this category are usually computationally expensive due to the
two-stage nature. This work proposes a simple yet effective strategy to address
the above concerns, which spends marginal overheads with significant gains in
accuracy. Concretely, different from the traditional two-stage pipeline, we
advocate putting the region-level selection after the one-stage detection to
avoid processing massive low-quality candidates. Besides, a novel module is
constructed to evaluate the relationship between a target frame and its
reference ones, and guide the aggregation. Extensive experiments and ablation
studies are conducted to verify the efficacy of our design, and reveal its
superiority over other state-of-the-art VID approaches in both effectiveness
and efficiency. Our YOLOX-based model can achieve promising performance (e.g.,
87.5\% AP50 at over 30 FPS on the ImageNet VID dataset on a single 2080Ti GPU),
making it attractive for large-scale or real-time applications. The
implementation is simple, the demo code and models have been made available at
https://github.com/YuHengsss/YOLOV .",-0.22630286,0.01594475,0.17449765,C
10249,"We
DPTNet-normal(ours)             19M 3.1G 88.6                 would hope that our method can serve as a solid baseline
                                                              for segmentation-based methods on scene text detection and
   Comparisons of models combining CNN with Trans-            also motivate further research.","The extensive exper-
                                                              iments on four challenging benchmarks demonstrate the ef-
DPTNet-Tiny(ours)               11M 1.2G 86.3                 fectiveness and generalization ability of our DPTNet.",former.,2022-08-21 12:58:45+00:00,DPTNet: A Dual-Path Transformer Architecture for Scene Text Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jingyu Lin'), arxiv.Result.Author('Jie Jiang'), arxiv.Result.Author('Yan Yan'), arxiv.Result.Author('Chunchao Guo'), arxiv.Result.Author('Hongfa Wang'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Hanzi Wang')]","The prosperity of deep learning contributes to the rapid progress in scene
text detection. Among all the methods with convolutional networks,
segmentation-based ones have drawn extensive attention due to their superiority
in detecting text instances of arbitrary shapes and extreme aspect ratios.
However, the bottom-up methods are limited to the performance of their
segmentation models. In this paper, we propose DPTNet (Dual-Path Transformer
Network), a simple yet effective architecture to model the global and local
information for the scene text detection task. We further propose a parallel
design that integrates the convolutional network with a powerful self-attention
mechanism to provide complementary clues between the attention path and
convolutional path. Moreover, a bi-directional interaction module across the
two paths is developed to provide complementary clues in the channel and
spatial dimensions. We also upgrade the concentration operation by adding an
extra multi-head attention layer to it. Our DPTNet achieves state-of-the-art
results on the MSRA-TD500 dataset, and provides competitive results on other
standard benchmarks in terms of both detection accuracy and speed.",-0.16415828,-0.16640736,0.18732968,C
10250,"We hope that this work leads to further research
on improving GANs for real-world long-tailed datasets.","Developing an hyperparameter
free decorrelated parameterization for alleviating class-specific mode collapse is a
good direction for future work.","Acknowledgements: This work was supported in part by SERB-STAR Project
(Project:STR/2020/000128), Govt.",2022-08-21 17:51:05+00:00,Improving GANs for Long-Tailed Data through Group Spectral Regularization,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Harsh Rangwani'), arxiv.Result.Author('Naman Jaswani'), arxiv.Result.Author('Tejan Karmali'), arxiv.Result.Author('Varun Jampani'), arxiv.Result.Author('R. Venkatesh Babu')]","Deep long-tailed learning aims to train useful deep networks on practical,
real-world imbalanced distributions, wherein most labels of the tail classes
are associated with a few samples. There has been a large body of work to train
discriminative models for visual recognition on long-tailed distribution. In
contrast, we aim to train conditional Generative Adversarial Networks, a class
of image generation models on long-tailed distributions. We find that similar
to recognition, state-of-the-art methods for image generation also suffer from
performance degradation on tail classes. The performance degradation is mainly
due to class-specific mode collapse for tail classes, which we observe to be
correlated with the spectral explosion of the conditioning parameter matrix. We
propose a novel group Spectral Regularizer (gSR) that prevents the spectral
explosion alleviating mode collapse, which results in diverse and plausible
image generation even for tail classes. We find that gSR effectively combines
with existing augmentation and regularization techniques, leading to
state-of-the-art image generation performance on long-tailed data. Extensive
experiments demonstrate the efficacy of our regularizer on long-tailed datasets
with different degrees of imbalance.",0.029232342,-0.08087553,0.25574556,C
10268,"Furthermore, there are still many
aspects worthy of further study: i) How to formulate the whole optimization
process into a one-stage procedure?","5 Discussion

In this paper, we provide an effective solution on how to obtain a lightweight seg-
mentation model in a semi-supervised setting.","ii) How to further narrow the performance
gap between the lightweight model and the large model?",2022-08-22 09:32:06+00:00,Multi-Granularity Distillation Scheme Towards Lightweight Semi-Supervised Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jie Qin'), arxiv.Result.Author('Jie Wu'), arxiv.Result.Author('Ming Li'), arxiv.Result.Author('Xuefeng Xiao'), arxiv.Result.Author('Min Zheng'), arxiv.Result.Author('Xingang Wang')]","Albeit with varying degrees of progress in the field of Semi-Supervised
Semantic Segmentation, most of its recent successes are involved in unwieldy
models and the lightweight solution is still not yet explored. We find that
existing knowledge distillation techniques pay more attention to pixel-level
concepts from labeled data, which fails to take more informative cues within
unlabeled data into account. Consequently, we offer the first attempt to
provide lightweight SSSS models via a novel multi-granularity distillation
(MGD) scheme, where multi-granularity is captured from three aspects: i)
complementary teacher structure; ii) labeled-unlabeled data cooperative
distillation; iii) hierarchical and multi-levels loss setting. Specifically,
MGD is formulated as a labeled-unlabeled data cooperative distillation scheme,
which helps to take full advantage of diverse data characteristics that are
essential in the semi-supervised setting. Image-level semantic-sensitive loss,
region-level content-aware loss, and pixel-level consistency loss are set up to
enrich hierarchical distillation abstraction via structurally complementary
teachers. Experimental results on PASCAL VOC2012 and Cityscapes reveal that MGD
can outperform the competitive approaches by a large margin under diverse
partition protocols. For example, the performance of ResNet-18 and MobileNet-v2
backbone is boosted by 11.5% and 4.6% respectively under 1/16 partition
protocol on Cityscapes. Although the FLOPs of the model backbone is compressed
by 3.4-5.3x (ResNet-18) and 38.7-59.6x (MobileNetv2), the model manages to
achieve satisfactory segmentation results.",0.32498646,-0.041331213,-0.10095078,A
10273,"To
further study how PoseBERT impacts the Ô¨Ånal predictions, we            4.2.3 Ablation study for the training strategy
plot in Figure 7 the per-frame MPJPE for sequences of 3DPW
and MuPots-3D.",to 52.4% and 50.2% respectively for 3DPW and MuPots-3D.,"We conclude that PoseBERT is able to recover            We study the impact on training when varying the length of the
from large errors made by the image-based method by leveraging         input sequence, the percentage of time-steps replaced by random
the contextual information.",2022-08-22 11:30:14+00:00,PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fabien Baradel'), arxiv.Result.Author('Romain Br√©gier'), arxiv.Result.Author('Thibault Groueix'), arxiv.Result.Author('Philippe Weinzaepfel'), arxiv.Result.Author('Yannis Kalantidis'), arxiv.Result.Author('Gr√©gory Rogez')]","Training state-of-the-art models for human pose estimation in videos requires
datasets with annotations that are really hard and expensive to obtain.
Although transformers have been recently utilized for body pose sequence
modeling, related methods rely on pseudo-ground truth to augment the currently
limited training data available for learning such models. In this paper, we
introduce PoseBERT, a transformer module that is fully trained on 3D Motion
Capture (MoCap) data via masked modeling. It is simple, generic and versatile,
as it can be plugged on top of any image-based model to transform it in a
video-based model leveraging temporal information. We showcase variants of
PoseBERT with different inputs varying from 3D skeleton keypoints to rotations
of a 3D parametric model for either the full body (SMPL) or just the hands
(MANO). Since PoseBERT training is task agnostic, the model can be applied to
several tasks such as pose refinement, future pose prediction or motion
completion without finetuning. Our experimental results validate that adding
PoseBERT on top of various state-of-the-art pose estimation methods
consistently improves their performances, while its low computational cost
allows us to use it in a real-time demo for smoothly animating a robotic hand
via a webcam. Test code and models are available at
https://github.com/naver/posebert.",0.03170586,0.01710077,-0.03390674,C
10274,"To
further study how PoseBERT impacts the Ô¨Ånal predictions, we             Method                                               MPJPE ‚Üì           PA-MPJPE ‚Üì       Accel ‚Üì
plot in Figure 7 the per-frame MPJPE for sequences of 3DPW
and MuPots-3D.",to 52.4% and 50.2% respectively for 3DPW and MuPots-3D.,"We conclude that PoseBERT is able to recover             LCR-Net++ [1]                                        153.76            105.23           37.98
from large errors made by the image-based method by leveraging              (matched groundtruths only)                      136.79            85.53            32.86
the contextual information.",2022-08-22 11:30:14+00:00,PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fabien Baradel'), arxiv.Result.Author('Romain Br√©gier'), arxiv.Result.Author('Thibault Groueix'), arxiv.Result.Author('Philippe Weinzaepfel'), arxiv.Result.Author('Yannis Kalantidis'), arxiv.Result.Author('Gr√©gory Rogez')]","Training state-of-the-art models for human pose estimation in videos requires
datasets with annotations that are really hard and expensive to obtain.
Although transformers have been recently utilized for body pose sequence
modeling, related methods rely on pseudo-ground truth to augment the currently
limited training data available for learning such models. In this paper, we
introduce PoseBERT, a transformer module that is fully trained on 3D Motion
Capture (MoCap) data via masked modeling. It is simple, generic and versatile,
as it can be plugged on top of any image-based model to transform it in a
video-based model leveraging temporal information. We showcase variants of
PoseBERT with different inputs varying from 3D skeleton keypoints to rotations
of a 3D parametric model for either the full body (SMPL) or just the hands
(MANO). Since PoseBERT training is task agnostic, the model can be applied to
several tasks such as pose refinement, future pose prediction or motion
completion without finetuning. Our experimental results validate that adding
PoseBERT on top of various state-of-the-art pose estimation methods
consistently improves their performances, while its low computational cost
allows us to use it in a real-time demo for smoothly animating a robotic hand
via a webcam. Test code and models are available at
https://github.com/naver/posebert.",0.08511172,0.18515706,0.076790705,A
10312,"ing a uniÔ¨Åed framework will be a systematic challenge and
                                                                  can lay a solid foundation for further research on cooperative
B.","Thus, build-
cooperative perception.",Future Trends                                                  perception.,2022-08-22 20:47:35+00:00,A Survey and Framework of Cooperative Perception: From Heterogeneous Singleton to Hierarchical Cooperation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhengwei Bai'), arxiv.Result.Author('Guoyuan Wu'), arxiv.Result.Author('Matthew J. Barth'), arxiv.Result.Author('Yongkang Liu'), arxiv.Result.Author('Emrah Akin Sisbot'), arxiv.Result.Author('Kentaro Oguchi'), arxiv.Result.Author('Zhitong Huang')]","Perceiving the environment is one of the most fundamental keys to enabling
Cooperative Driving Automation (CDA), which is regarded as the revolutionary
solution to addressing the safety, mobility, and sustainability issues of
contemporary transportation systems. Although an unprecedented evolution is now
happening in the area of computer vision for object perception,
state-of-the-art perception methods are still struggling with sophisticated
real-world traffic environments due to the inevitably physical occlusion and
limited receptive field of single-vehicle systems. Based on multiple spatially
separated perception nodes, Cooperative Perception (CP) is born to unlock the
bottleneck of perception for driving automation. In this paper, we
comprehensively review and analyze the research progress on CP and, to the best
of our knowledge, this is the first time to propose a unified CP framework.
Architectures and taxonomy of CP systems based on different types of sensors
are reviewed to show a high-level description of the workflow and different
structures for CP systems. Node structure, sensor modality, and fusion schemes
are reviewed and analyzed with comprehensive literature to provide detailed
explanations of specific methods. A Hierarchical CP framework is proposed,
followed by a review of existing Datasets and Simulators to sketch an overall
landscape of CP. Discussion highlights the current opportunities, open
challenges, and anticipated future trends.",0.042469617,0.07604273,-0.25354832,A
10316,"We further study the impact of the
ratio of anatomy information used in AWCL pre-training in Section 6.4.","Both strategies of the proposed learning approach
are evaluated and compared in Section 6.3.","4.3 Implementation details

Algorithm 1 provides the pseudo-code of AWCL.",2022-08-22 22:49:26+00:00,Anatomy-Aware Contrastive Representation Learning for Fetal Ultrasound,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Zeyu Fu'), arxiv.Result.Author('Jianbo Jiao'), arxiv.Result.Author('Robail Yasrab'), arxiv.Result.Author('Lior Drukker'), arxiv.Result.Author('Aris T. Papageorghiou'), arxiv.Result.Author('J. Alison Noble')]","Self-supervised contrastive representation learning offers the advantage of
learning meaningful visual representations from unlabeled medical datasets for
transfer learning. However, applying current contrastive learning approaches to
medical data without considering its domain-specific anatomical characteristics
may lead to visual representations that are inconsistent in appearance and
semantics. In this paper, we propose to improve visual representations of
medical images via anatomy-aware contrastive learning (AWCL), which
incorporates anatomy information to augment the positive/negative pair sampling
in a contrastive learning manner. The proposed approach is demonstrated for
automated fetal ultrasound imaging tasks, enabling the positive pairs from the
same or different ultrasound scans that are anatomically similar to be pulled
together and thus improving the representation learning. We empirically
investigate the effect of inclusion of anatomy information with coarse- and
fine-grained granularity, for contrastive learning and find that learning with
fine-grained anatomy information which preserves intra-class difference is more
effective than its counterpart. We also analyze the impact of anatomy ratio on
our AWCL framework and find that using more distinct but anatomically similar
samples to compose positive pairs results in better quality representations.
Experiments on a large-scale fetal ultrasound dataset demonstrate that our
approach is effective for learning representations that transfer well to three
clinical downstream tasks, and achieves superior performance compared to
ImageNet supervised and the current state-of-the-art contrastive learning
methods. In particular, AWCL outperforms ImageNet supervised method by 13.8%
and state-of-the-art contrastive-based method by 7.1% on a cross-domain
segmentation task.",0.17343305,-0.16895077,-0.13456766,A
10339,"As a direction for further research, we plan to carry out
   In this section, we compare our method with the white-box      experiments to understand the limits of the system in terms of
multi-bit DNN watermarking methods of Uchida et al.",Comparison with the state-of-the-art.,"[8] and       capacity (paylod), that is, to characterize the payload that can
Li et al.",2022-08-23 13:45:15+00:00,Robust DNN Watermarking via Fixed Embedding Weights with Optimized Distribution,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Benedetta Tondi'), arxiv.Result.Author('Andrea Costanzo'), arxiv.Result.Author('Mauro Barni')]","Watermarking has been proposed as a way to protect the Intellectual Property
Rights (IPR) of Deep Neural Networks (DNNs) and track their use. Several
methods have been proposed that embed the watermark into the trainable
parameters of the network (white box watermarking) or into the input-output
mappping implemented by the network in correspondence to specific inputs (black
box watermarking). In both cases, achieving robustness against fine tuning,
model compression and, even more, transfer learning, is one of the most
difficult challenges researchers are trying to face with. In this paper, we
propose a new white-box, multi-bit watermarking algorithm with strong
robustness properties, including retraining for transfer learning. Robustness
is achieved thanks to a new information coding strategy according to which the
watermark message is spread across a number of fixed weights, whose position
depends on a secret key. The weights hosting the watermark are set prior to
training, and are left unchanged throughout the entire training procedure. The
distribution of the weights carrying out the message is theoretically optimised
to make sure that the watermarked weights are indistinguishable from the other
weights, while at the same time keeping their amplitude as large as possible to
improve robustness against retraining. We carried out several experiments
demonstrating the capability of the proposed scheme to provide high payloads
with practically no impact on the network accuracy, at the same time retaining
excellent robustness against network modifications an re-use, including
retraining for transfer learning.",0.06883349,-0.047794603,0.19407266,C
10340,"[6] M. Barni, F. P√©rez-Gonz√°lez, and B. Tondi, ‚ÄúDnn watermarking: Four
                                                                                        challenges and a funeral,‚Äù in Proceedings of the 2021 ACM Workshop
   As a direction for further research, we plan to carry out ex-                        on Information Hiding and Multimedia Security, 2021, pp.","Crc Press, 2004.
is used as pre-trained model for re-training the network on a
different dataset and task.",189‚Äì196.,2022-08-23 13:45:15+00:00,Robust DNN Watermarking via Fixed Embedding Weights with Optimized Distribution,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Benedetta Tondi'), arxiv.Result.Author('Andrea Costanzo'), arxiv.Result.Author('Mauro Barni')]","Watermarking has been proposed as a way to protect the Intellectual Property
Rights of Deep Neural Networks and track their use. Several methods have been
proposed to embed the watermark into the trainable parameters of the network
(white box watermarking) or into the input-output mapping implemented by the
network in correspondence to specific inputs (black box watermarking). In both
cases, achieving robustness against fine tuning, model compression and, even
more, transfer learning, is one of the most difficult challenges researchers
are facing with. In this paper, we propose a new white-box, multi-bit
watermarking algorithm with strong robustness properties, including robustness
against retraining for transfer learning. Robustness is achieved thanks to a
new embedding strategy according to which the watermark message is spread
across a number of fixed weights, whose position depends on a secret key. The
weights hosting the watermark are set prior to training, and are left unchanged
throughout the training procedure. The distribution of the weights carrying the
watermark is theoretically optimised to make sure that they are
indistinguishable from the non-watermarked weights, while at the same time
setting their amplitude to as large as possible values to improve robustness
against retraining. We carried out several experiments demonstrating the
capability of the proposed scheme to provide high payloads with no significant
impact on network accuracy, at the same time ensuring excellent robustness
against network modifications an re-use, including retraining and transfer
learning.",-0.08476589,-0.23620272,0.10271734,C
10367,"To facilitate further research, all codes and pre-
                                           Although many long-range imaging systems are de-           trained models will be made public.","Abstract                              synthetic and real-world data to show the signiÔ¨Åcance of
                                                                                                      our model.","signed to support extended vision applications, a natural
                                        obstacle to their operation is degradation due to atmo-       1.",2022-08-24 03:13:04+00:00,AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Nithin Gopalakrishnan Nair'), arxiv.Result.Author('Kangfu Mei'), arxiv.Result.Author('Vishal M Patel')]","Although many long-range imaging systems are designed to support extended
vision applications, a natural obstacle to their operation is degradation due
to atmospheric turbulence. Atmospheric turbulence causes significant
degradation to image quality by introducing blur and geometric distortion. In
recent years, various deep learning-based single image atmospheric turbulence
mitigation methods, including CNN-based and GAN inversion-based, have been
proposed in the literature which attempt to remove the distortion in the image.
However, some of these methods are difficult to train and often fail to
reconstruct facial features and produce unrealistic results especially in the
case of high turbulence. Denoising Diffusion Probabilistic Models (DDPMs) have
recently gained some traction because of their stable training process and
their ability to generate high quality images. In this paper, we propose the
first DDPM-based solution for the problem of atmospheric turbulence mitigation.
We also propose a fast sampling technique for reducing the inference times for
conditional DDPMs. Extensive experiments are conducted on synthetic and
real-world data to show the significance of our model. To facilitate further
research, all codes and pretrained models will be made public after the review
process.",-0.19458753,0.22959861,0.20608637,B
10368,"To facilitate further research,               Tk = Dk(Hk(I))) + nk, k = 1, 2, ..., N, (1)
where Tk is the degraded image at the kth time instant, I        manifold of realistic faces, hence being able to produce real-
is the clean latent image, Dk is the deformation operator        istic face outputs even for strong distortions.","Extensive experiments are         degradation due to atmospheric turbulence is modelled as
                                        conducted on synthetic and real-world data to show the
                                        signiÔ¨Åcance of our model.","Restoration of
which is assumed to deform randomly, nk is additive noise,       images degraded by atmospheric turbulence is an ill-posed
and Hk is air turbulence-caused blurring operator [16, 17,       problem.",2022-08-24 03:13:04+00:00,AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Nithin Gopalakrishnan Nair'), arxiv.Result.Author('Kangfu Mei'), arxiv.Result.Author('Vishal M. Patel')]","Although many long-range imaging systems are designed to support extended
vision applications, a natural obstacle to their operation is degradation due
to atmospheric turbulence. Atmospheric turbulence causes significant
degradation to image quality by introducing blur and geometric distortion. In
recent years, various deep learning-based single image atmospheric turbulence
mitigation methods, including CNN-based and GAN inversion-based, have been
proposed in the literature which attempt to remove the distortion in the image.
However, some of these methods are difficult to train and often fail to
reconstruct facial features and produce unrealistic results especially in the
case of high turbulence. Denoising Diffusion Probabilistic Models (DDPMs) have
recently gained some traction because of their stable training process and
their ability to generate high quality images. In this paper, we propose the
first DDPM-based solution for the problem of atmospheric turbulence mitigation.
We also propose a fast sampling technique for reducing the inference times for
conditional DDPMs. Extensive experiments are conducted on synthetic and
real-world data to show the significance of our model. To facilitate further
research, all codes and pretrained models are publically available at
http://github.com/Nithin-GK/AT-DDPM",0.082120284,0.26364082,0.21989483,A
10370,"Furthermore, cameras are ubiquitous in                 foster further research in this area.","Cameras are frequently used in robotic perception, for
                                        tasks such as localization, path planning, scene understanding             ‚Ä¢ We open-source our code and the simulated datasets to
                                        as well as inspection.","virtual and augmented reality systems, where they are used
                                        for ego localization, spatial reasoning and visualizations.",2022-08-24 04:53:32+00:00,E-NeRF: Neural Radiance Fields from a Moving Event Camera,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Simon Klenk'), arxiv.Result.Author('Lukas Koestler'), arxiv.Result.Author('Davide Scaramuzza'), arxiv.Result.Author('Daniel Cremers')]","Estimating neural radiance fields (NeRFs) from ideal images has been
extensively studied in the computer vision community. Most approaches assume
optimal illumination and slow camera motion. These assumptions are often
violated in robotic applications, where images contain motion blur and the
scene may not have suitable illumination. This can cause significant problems
for downstream tasks such as navigation, inspection or visualization of the
scene. To alleviate these problems we present E-NeRF, the first method which
estimates a volumetric scene representation in the form of a NeRF from a
fast-moving event camera. Our method can recover NeRFs during very fast motion
and in high dynamic range conditions, where frame-based approaches fail. We
show that rendering high-quality frames is possible by only providing an event
stream as input. Furthermore, by combining events and frames, we can estimate
NeRFs of higher quality than state-of-the-art approaches under severe motion
blur. We also show that combining events and frames can overcome failure cases
of NeRF estimation in scenarios where only few input views are available,
without requiring additional regularization.",-0.34007174,0.27013856,-0.26652902,B
10373,"To conclude, through this work we discover and understand T-FF in
universal detectors for counterfeit detection, and hope that our contributions
will inspire further research in image forensics and image synthesis methods.","This may result in noticeable discrepancies between real and GAN images
(counterfeits) in the color space which can be used as T-FF to detect coun-
terfeits.",Limitations / Broader Impact.,2022-08-24 07:48:07+00:00,Discovering Transferable Forensic Features for CNN-generated Images Detection,cs.CV,"['cs.CV', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Keshigeyan Chandrasegaran'), arxiv.Result.Author('Ngoc-Trung Tran'), arxiv.Result.Author('Alexander Binder'), arxiv.Result.Author('Ngai-Man Cheung')]","Visual counterfeits are increasingly causing an existential conundrum in
mainstream media with rapid evolution in neural image synthesis methods. Though
detection of such counterfeits has been a taxing problem in the image forensics
community, a recent class of forensic detectors -- universal detectors -- are
able to surprisingly spot counterfeit images regardless of generator
architectures, loss functions, training datasets, and resolutions. This
intriguing property suggests the possible existence of transferable forensic
features (T-FF) in universal detectors. In this work, we conduct the first
analytical study to discover and understand T-FF in universal detectors. Our
contributions are 2-fold: 1) We propose a novel forensic feature relevance
statistic (FF-RS) to quantify and discover T-FF in universal detectors and, 2)
Our qualitative and quantitative investigations uncover an unexpected finding:
color is a critical T-FF in universal detectors. Code and models are available
at https://keshik6.github.io/transferable-forensic-features/",-0.09406522,0.07510796,0.16063985,C
10385,"And these challenges warrant further research
and consideration when deploying the face recognition model in real scenarios.","In the comparative study in Table 2, the performance
diÔ¨Äerence here may be mainly resulted from the diÔ¨Äerence of the processing
method on some hard samples.","Springer Nature 2021 LATEX template

 SubFace: Learning with Softmax Approximation for Face Recognition 15

5 Conclusion

In this paper, we propose an approximate training strategy named SubFace
to enhance the distinguishing ability of features.",2022-08-24 12:31:08+00:00,SubFace: Learning with Softmax Approximation for Face Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hongwei Xu'), arxiv.Result.Author('Suncheng Xiang'), arxiv.Result.Author('Dahong Qian')]","The softmax-based loss functions and its variants (e.g., cosface, sphereface,
and arcface) significantly improve the face recognition performance in wild
unconstrained scenes. A common practice of these algorithms is to perform
optimizations on the multiplication between the embedding features and the
linear transformation matrix. However in most cases, the dimension of embedding
features is given based on traditional design experience, and there is
less-studied on improving performance using the feature itself when giving a
fixed size. To address this challenge, this paper presents a softmax
approximation method called SubFace, which employs the subspace feature to
promote the performance of face recognition. Specifically, we dynamically
select the non-overlapping subspace features in each batch during training, and
then use the subspace features to approximate full-feature among softmax-based
loss, so the discriminability of the deep model can be significantly enhanced
for face recognition. Comprehensive experiments conducted on benchmark datasets
demonstrate that our method can significantly improve the performance of
vanilla CNN baseline, which strongly proves the effectiveness of subspace
strategy with the margin-based loss.",-0.013719864,-0.13928,0.002079825,C
10386,"For further research, we
will combine other optimization strategies, such as distributed computing, and
neural architecture search, to further improve the performance of the proposed
method.","Comprehensive experiments conducted on benchmarks demonstrates that Sub-
Face can signiÔ¨Åcantly improve the performance of vanilla CNN baseline with
margined-based loss on face recognition, proving its superiority and compet-
itiveness when compared with the state-of-the-arts.",Acknowledgments.,2022-08-24 12:31:08+00:00,SubFace: Learning with Softmax Approximation for Face Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hongwei Xu'), arxiv.Result.Author('Suncheng Xiang'), arxiv.Result.Author('Dahong Qian')]","The softmax-based loss functions and its variants (e.g., cosface, sphereface,
and arcface) significantly improve the face recognition performance in wild
unconstrained scenes. A common practice of these algorithms is to perform
optimizations on the multiplication between the embedding features and the
linear transformation matrix. However in most cases, the dimension of embedding
features is given based on traditional design experience, and there is
less-studied on improving performance using the feature itself when giving a
fixed size. To address this challenge, this paper presents a softmax
approximation method called SubFace, which employs the subspace feature to
promote the performance of face recognition. Specifically, we dynamically
select the non-overlapping subspace features in each batch during training, and
then use the subspace features to approximate full-feature among softmax-based
loss, so the discriminability of the deep model can be significantly enhanced
for face recognition. Comprehensive experiments conducted on benchmark datasets
demonstrate that our method can significantly improve the performance of
vanilla CNN baseline, which strongly proves the effectiveness of subspace
strategy with the margin-based loss.",-0.066591814,-0.24032143,0.25727987,C
10401,"We further study the spatial and temporal attention region
of activated by the recognition models by generating class activation map [33].","The
vehicle bounding box information is embedded into each frame of the RGB video
data to enhance the detection and prediction and passed to a pre-trained action
recognition model.","Furthermore, we propose a better way to extract motion clues by optimizing the
pooling stride.",2022-08-24 16:40:27+00:00,Lane Change Classification and Prediction with Action Recognition Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kai Liang'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Abhir Bhalerao')]","Anticipating lane change intentions of surrounding vehicles is crucial for
efficient and safe driving decision making in an autonomous driving system.
Previous works often adopt physical variables such as driving speed,
acceleration and so forth for lane change classification. However, physical
variables do not contain semantic information. Although 3D CNNs have been
developing rapidly, the number of methods utilising action recognition models
and appearance feature for lane change recognition is low, and they all require
additional information to pre-process data. In this work, we propose an
end-to-end framework including two action recognition methods for lane change
recognition, using video data collected by cameras. Our method achieves the
best lane change classification results using only the RGB video data of the
PREVENTION dataset. Class activation maps demonstrate that action recognition
models can efficiently extract lane change motions. A method to better extract
motion clues is also proposed in this paper.",-0.35501078,-0.020602642,-0.13924786,B
10402,"We further study the spatial and temporal attention region
of activated by the recognition models by generating class activation map [33].","The
vehicle bounding box information is embedded into each frame of the RGB video
data to enhance the detection and prediction and passed to a pre-trained action
recognition model.","Furthermore, we propose a better way to extract motion clues by optimizing the
pooling stride.",2022-08-24 16:40:27+00:00,Lane Change Classification and Prediction with Action Recognition Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kai Liang'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Abhir Bhalerao')]","Anticipating lane change intentions of surrounding vehicles is crucial for
efficient and safe driving decision making in an autonomous driving system.
Previous works often adopt physical variables such as driving speed,
acceleration and so forth for lane change classification. However, physical
variables do not contain semantic information. Although 3D CNNs have been
developing rapidly, the number of methods utilising action recognition models
and appearance feature for lane change recognition is low, and they all require
additional information to pre-process data. In this work, we propose an
end-to-end framework including two action recognition methods for lane change
recognition, using video data collected by cameras. Our method achieves the
best lane change classification results using only the RGB video data of the
PREVENTION dataset. Class activation maps demonstrate that action recognition
models can efficiently extract lane change motions. A method to better extract
motion clues is also proposed in this paper.",-0.35501078,-0.020602642,-0.13924786,B
10414,"The survey also discusses the need for larger and more robust datasets of identical
twins, as well as the need for further research in speaker recognition and handwriting recognition of identical twins.","They highlight the difÔ¨Åculty of
facial recognition of twins through multiple studies, discuss the minor accuracy degradation seen in twin Ô¨Ångerprint
recognition as compared to non-twin Ô¨Ångerprint recognition, and show the success of several works exploring iris
recognition of identical twins.","2.2 Facial Recognition of Identical Twins

Facial recognition is quickly becoming one of the most widely used biometric modalities, due to its high acceptance
among the general public, ease of data capture, and accuracy.",2022-08-25 01:45:02+00:00,Benchmarking Human Face Similarity Using Identical Twins,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shoaib Meraj Sami'), arxiv.Result.Author('John McCauley'), arxiv.Result.Author('Sobhan Soleymani'), arxiv.Result.Author('Nasser Nasrabadi'), arxiv.Result.Author('Jeremy Dawson')]","The problem of distinguishing identical twins and non-twin look-alikes in
automated facial recognition (FR) applications has become increasingly
important with the widespread adoption of facial biometrics. Due to the high
facial similarity of both identical twins and look-alikes, these face pairs
represent the hardest cases presented to facial recognition tools. This work
presents an application of one of the largest twin datasets compiled to date to
address two FR challenges: 1) determining a baseline measure of facial
similarity between identical twins and 2) applying this similarity measure to
determine the impact of doppelgangers, or look-alikes, on FR performance for
large face datasets. The facial similarity measure is determined via a deep
convolutional neural network. This network is trained on a tailored
verification task designed to encourage the network to group together highly
similar face pairs in the embedding space and achieves a test AUC of 0.9799.
The proposed network provides a quantitative similarity score for any two given
faces and has been applied to large-scale face datasets to identify similar
face pairs. An additional analysis which correlates the comparison score
returned by a facial recognition tool and the similarity score returned by the
proposed network has also been performed.",0.061420277,0.030922063,-0.12421251,C
10415,"This demonstrates that
these situations are challenging for image inpainting and need further study.","When lacking sufficient
prior knowledge, our method fails to reconstruct details.","5 Conclusion

In this paper, we propose an encoder-based GAN inversion method InvertFill
for image inpainting.",2022-08-25 03:39:24+00:00,High-Fidelity Image Inpainting with GAN Inversion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yongsheng Yu'), arxiv.Result.Author('Libo Zhang'), arxiv.Result.Author('Heng Fan'), arxiv.Result.Author('Tiejian Luo')]","Image inpainting seeks a semantically consistent way to recover the corrupted
image in the light of its unmasked content. Previous approaches usually reuse
the well-trained GAN as effective prior to generate realistic patches for
missing holes with GAN inversion. Nevertheless, the ignorance of a hard
constraint in these algorithms may yield the gap between GAN inversion and
image inpainting. Addressing this problem, in this paper, we devise a novel GAN
inversion model for image inpainting, dubbed InvertFill, mainly consisting of
an encoder with a pre-modulation module and a GAN generator with F&W+ latent
space. Within the encoder, the pre-modulation network leverages multi-scale
structures to encode more discriminative semantics into style vectors. In order
to bridge the gap between GAN inversion and image inpainting, F&W+ latent space
is proposed to eliminate glaring color discrepancy and semantic inconsistency.
To reconstruct faithful and photorealistic images, a simple yet effective
Soft-update Mean Latent module is designed to capture more diverse in-domain
patterns that synthesize high-fidelity textures for large corruptions.
Comprehensive experiments on four challenging datasets, including Places2,
CelebA-HQ, MetFaces, and Scenery, demonstrate that our InvertFill outperforms
the advanced approaches qualitatively and quantitatively and supports the
completion of out-of-domain images well.",-0.11374931,0.06364909,0.22401062,C
10419,After          work can spark further research on the CC-ReID problem.,"We hope this
racy and mAP can be improved with the CSA module.","adding Latt to the total loss function, the performance can be
further boosted.",2022-08-25 12:01:49+00:00,Identity-Sensitive Knowledge Propagation for Cloth-Changing Person Re-identification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianbing Wu'), arxiv.Result.Author('Hong Liu'), arxiv.Result.Author('Wei Shi'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Jingwen Guo')]","Cloth-changing person re-identification (CC-ReID), which aims to match person
identities under clothing changes, is a new rising research topic in recent
years. However, typical biometrics-based CC-ReID methods often require
cumbersome pose or body part estimators to learn cloth-irrelevant features from
human biometric traits, which comes with high computational costs. Besides, the
performance is significantly limited due to the resolution degradation of
surveillance images. To address the above limitations, we propose an effective
Identity-Sensitive Knowledge Propagation framework (DeSKPro) for CC-ReID.
Specifically, a Cloth-irrelevant Spatial Attention module is introduced to
eliminate the distraction of clothing appearance by acquiring knowledge from
the human parsing module. To mitigate the resolution degradation issue and mine
identity-sensitive cues from human faces, we propose to restore the missing
facial details using prior facial knowledge, which is then propagated to a
smaller network. After training, the extra computations for human parsing or
face restoration are no longer required. Extensive experiments show that our
framework outperforms state-of-the-art methods by a large margin. Our code is
available at https://github.com/KimbingNg/DeskPro.",0.08155356,0.06937747,0.16058761,A
10444,Accuracy with real world data sets in different application areas remains a subject for further study.,"Accuracy of
the generated registrations is good on the data sets with synthetically generated deformations and could probably be
improved.","15
                                                                                                    A PREPRINT - AUGUST 29, 2022

Acknowledgments

This work was supported by Academy of Finland (Flagship programme: Finnish Center for ArtiÔ¨Åcial Intelligence [grant
no.",2022-08-26 08:12:40+00:00,Deformation equivariant cross-modality image synthesis with paired non-aligned training data,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Joel Honkamaa'), arxiv.Result.Author('Umair Khan'), arxiv.Result.Author('Sonja Koivukoski'), arxiv.Result.Author('Leena Latonen'), arxiv.Result.Author('Pekka Ruusuvuori'), arxiv.Result.Author('Pekka Marttinen')]","Cross-modality image synthesis is an active research topic with multiple
medical clinically relevant applications. Recently, methods allowing training
with paired but misaligned data have started to emerge. However, no robust and
well-performing methods applicable to a wide range of real world data sets
exist. In this work, we propose a generic solution to the problem of
cross-modality image synthesis with paired but non-aligned data by introducing
new deformation equivariance encouraging loss functions. The method consists of
joint training of an image synthesis network together with separate
registration networks and allows adversarial training conditioned on the input
even with misaligned data. The work lowers the bar for new clinical
applications by allowing effortless training of cross-modality image synthesis
networks for more difficult data sets and opens up opportunities for the
development of new generic learning based cross-modality registration
algorithms.",-0.027627407,0.18927285,-0.12917942,B
10457,"We
                                                 Ô¨Ånd the results promising and believe this elicits further research on how
                                                 image manipulation can be used for privacy preservation.","Additionally, we examine the eÔ¨Äect of
                                                 naturally occurring correlations and residual information on Ô¨Åltering.","Keywords: Privacy ¬∑ Image manipulation ¬∑ Privacy-preserving machine
                                                 learning ¬∑ Disentangled representation learning

                                        1 Introduction

                                        Recently, there has been an increase in popularity in using camera sensors and
                                        cloud services to capture and process large amounts of data.",2022-08-26 12:47:18+00:00,Selective manipulation of disentangled representations for privacy-aware facial image processing,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sander De Coninck'), arxiv.Result.Author('Wei-Cheng Wang'), arxiv.Result.Author('Sam Leroux'), arxiv.Result.Author('Pieter Simoens')]","Camera sensors are increasingly being combined with machine learning to
perform various tasks such as intelligent surveillance. Due to its
computational complexity, most of these machine learning algorithms are
offloaded to the cloud for processing. However, users are increasingly
concerned about privacy issues such as function creep and malicious usage by
third-party cloud providers. To alleviate this, we propose an edge-based
filtering stage that removes privacy-sensitive attributes before the sensor
data are transmitted to the cloud. We use state-of-the-art image manipulation
techniques that leverage disentangled representations to achieve privacy
filtering. We define opt-in and opt-out filter operations and evaluate their
effectiveness for filtering private attributes from face images. Additionally,
we examine the effect of naturally occurring correlations and residual
information on filtering. We find the results promising and believe this
elicits further research on how image manipulation can be used for privacy
preservation.",-0.211635,-0.046046223,0.002787373,C
10458,"Therefore, we also collect RAW data for HR and LR images and enable burst mode when
capturing LR images to beneÔ¨Åt others who want to use our dataset for further research.","Researchers in the SR area are also interested in building an SR model for the arbitrary scale
factor.",Our DroneSR dataset contains HR and LR image pairs at different altitudes.,2022-08-21 18:19:19+00:00,Towards Robust Drone Vision in the Wild,cs.CV,['cs.CV'],[arxiv.Result.Author('Xiaoyu Lin')],"The past few years have witnessed the burst of drone-based applications where
computer vision plays an essential role. However, most public drone-based
vision datasets focus on detection and tracking. On the other hand, the
performance of most existing image super-resolution methods is sensitive to the
dataset, specifically, the degradation model between high-resolution and
low-resolution images. In this thesis, we propose the first image
super-resolution dataset for drone vision. Image pairs are captured by two
cameras on the drone with different focal lengths. We collect data at different
altitudes and then propose pre-processing steps to align image pairs. Extensive
empirical studies show domain gaps exist among images captured at different
altitudes. Meanwhile, the performance of pretrained image super-resolution
networks also suffers a drop on our dataset and varies among altitudes.
Finally, we propose two methods to build a robust image super-resolution
network at different altitudes. The first feeds altitude information into the
network through altitude-aware layers. The second uses one-shot learning to
quickly adapt the super-resolution model to unknown altitudes. Our results
reveal that the proposed methods can efficiently improve the performance of
super-resolution networks at varying altitudes.",0.017926237,0.26448324,0.048564862,B
10459,"In the last chapter, the conclusion of the proposed dataset and methods, as well as some plans
and suggestions for further research, are discussed.","We propose two methods to build a robust image super-
resolution network at different altitudes in the sixth chapter.","4
2 Literature review

      The last few years have witnessed a burst of drone technology and signiÔ¨Åcant improvement in
      image SR.",2022-08-21 18:19:19+00:00,Towards Robust Drone Vision in the Wild,cs.CV,['cs.CV'],[arxiv.Result.Author('Xiaoyu Lin')],"The past few years have witnessed the burst of drone-based applications where
computer vision plays an essential role. However, most public drone-based
vision datasets focus on detection and tracking. On the other hand, the
performance of most existing image super-resolution methods is sensitive to the
dataset, specifically, the degradation model between high-resolution and
low-resolution images. In this thesis, we propose the first image
super-resolution dataset for drone vision. Image pairs are captured by two
cameras on the drone with different focal lengths. We collect data at different
altitudes and then propose pre-processing steps to align image pairs. Extensive
empirical studies show domain gaps exist among images captured at different
altitudes. Meanwhile, the performance of pretrained image super-resolution
networks also suffers a drop on our dataset and varies among altitudes.
Finally, we propose two methods to build a robust image super-resolution
network at different altitudes. The first feeds altitude information into the
network through altitude-aware layers. The second uses one-shot learning to
quickly adapt the super-resolution model to unknown altitudes. Our results
reveal that the proposed methods can efficiently improve the performance of
super-resolution networks at varying altitudes.",-0.16536331,0.34326252,0.21564597,B
10460,"Finally, we summarise and analyze our results in Section 5.4 for further research.","We propose two
      experimental setups to Ô¨Åne-tune and evaluate those SR networks separately in Section 5.3.","5.1 From synthetic downsampling to real-world degradation

      Most of the existing learning-based SISR networks are trained and evaluated on synthetic
      dataset such as Div2K (Agustsson and Timofte, 2017), Flickr2K (Lim et al., 2017; Timofte et al.,
      2017), WED (Ma et al., 2016), FFHQ (Karras et al., 2019), Set5 (Bevilacqua et al., 2012), Set14
      (Zeyde et al., 2010), B100 (Martin et al., 2001) and Urban100 (Huang et al., 2015), where the LR
      images are generated by applying some simple and uniform degradation model (i.e., MATLAB
      bicubic downsampling) to HR images.",2022-08-21 18:19:19+00:00,Towards Robust Drone Vision in the Wild,cs.CV,['cs.CV'],[arxiv.Result.Author('Xiaoyu Lin')],"The past few years have witnessed the burst of drone-based applications where
computer vision plays an essential role. However, most public drone-based
vision datasets focus on detection and tracking. On the other hand, the
performance of most existing image super-resolution methods is sensitive to the
dataset, specifically, the degradation model between high-resolution and
low-resolution images. In this thesis, we propose the first image
super-resolution dataset for drone vision. Image pairs are captured by two
cameras on the drone with different focal lengths. We collect data at different
altitudes and then propose pre-processing steps to align image pairs. Extensive
empirical studies show domain gaps exist among images captured at different
altitudes. Meanwhile, the performance of pretrained image super-resolution
networks also suffers a drop on our dataset and varies among altitudes.
Finally, we propose two methods to build a robust image super-resolution
network at different altitudes. The first feeds altitude information into the
network through altitude-aware layers. The second uses one-shot learning to
quickly adapt the super-resolution model to unknown altitudes. Our results
reveal that the proposed methods can efficiently improve the performance of
super-resolution networks at varying altitudes.",-0.076621786,-0.07898557,0.28257844,C
10461,"Our dataset contains burst LR sequences and RAW data, which
      beneÔ¨Åts further research.","On the other hand, datasets are critical for learning-based
      computer vision techniques.","We believe our dataset will encourage more researchers to delve into
      image SR for drone vision.",2022-08-21 18:19:19+00:00,Towards Robust Drone Vision in the Wild,cs.CV,['cs.CV'],[arxiv.Result.Author('Xiaoyu Lin')],"The past few years have witnessed the burst of drone-based applications where
computer vision plays an essential role. However, most public drone-based
vision datasets focus on detection and tracking. On the other hand, the
performance of most existing image super-resolution methods is sensitive to the
dataset, specifically, the degradation model between high-resolution and
low-resolution images. In this thesis, we propose the first image
super-resolution dataset for drone vision. Image pairs are captured by two
cameras on the drone with different focal lengths. We collect data at different
altitudes and then propose pre-processing steps to align image pairs. Extensive
empirical studies show domain gaps exist among images captured at different
altitudes. Meanwhile, the performance of pretrained image super-resolution
networks also suffers a drop on our dataset and varies among altitudes.
Finally, we propose two methods to build a robust image super-resolution
network at different altitudes. The first feeds altitude information into the
network through altitude-aware layers. The second uses one-shot learning to
quickly adapt the super-resolution model to unknown altitudes. Our results
reveal that the proposed methods can efficiently improve the performance of
super-resolution networks at varying altitudes.",-0.2873133,0.09192316,0.0040616863,B
10498,"We further study the eÔ¨Äects of synthesized blurry
video parameters for video deblurring, e.g., exposure time and FPS.",Exposure Time and FPS.,"To achieve
so, we further synthesize Ô¨Åve new deblurring datasets with diÔ¨Äerent exposure
times and the FPS (by changing T and œÑ ).",2022-08-28 09:24:52+00:00,Towards Real-World Video Deblurring by Exploring Blur Formation Process,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mingdeng Cao'), arxiv.Result.Author('Zhihang Zhong'), arxiv.Result.Author('Yanbo Fan'), arxiv.Result.Author('Jiahao Wang'), arxiv.Result.Author('Yong Zhang'), arxiv.Result.Author('Jue Wang'), arxiv.Result.Author('Yujiu Yang'), arxiv.Result.Author('Yinqiang Zheng')]","This paper aims at exploring how to synthesize close-to-real blurs that
existing video deblurring models trained on them can generalize well to
real-world blurry videos. In recent years, deep learning-based approaches have
achieved promising success on video deblurring task. However, the models
trained on existing synthetic datasets still suffer from generalization
problems over real-world blurry scenarios with undesired artifacts. The factors
accounting for the failure remain unknown. Therefore, we revisit the classical
blur synthesis pipeline and figure out the possible reasons, including shooting
parameters, blur formation space, and image signal processor~(ISP). To analyze
the effects of these potential factors, we first collect an ultra-high
frame-rate (940 FPS) RAW video dataset as the data basis to synthesize various
kinds of blurs. Then we propose a novel realistic blur synthesis pipeline
termed as RAW-Blur by leveraging blur formation cues. Through numerous
experiments, we demonstrate that synthesizing blurs in the RAW space and
adopting the same ISP as the real-world testing data can effectively eliminate
the negative effects of synthetic data. Furthermore, the shooting parameters of
the synthesized blurry video, e.g., exposure time and frame-rate play
significant roles in improving the performance of deblurring models.
Impressively, the models trained on the blurry data synthesized by the proposed
RAW-Blur pipeline can obtain more than 5dB PSNR gain against those trained on
the existing synthetic blur datasets. We believe the novel realistic synthesis
pipeline and the corresponding RAW video dataset can help the community to
easily construct customized blur datasets to improve real-world video
deblurring performance largely, instead of laboriously collecting real data
pairs.",0.0760051,0.2206967,0.19223036,A
10522,"We
                                                                  believe further research is needed to more deeply understand
                                                                  the differences between the explainability of the two studies
                                                                  computer vision architectures.","In particular, the highlighted areas by the GradCAM
                                                                  turned out to be better interpretable for the transformer than for
                                                                  the CNN model, The highlighted regions for the transformer
                                                                  are often larger and better shaped around visible objects.","Relatedly, we believe it would
                                                                  be beneÔ¨Åcial to have human participants (as opposed to the
                                                                  machine learning researcher) assess the degree of explainabil-
                                                                  ity provided by CNN and transformer models.",2022-08-29 12:44:48+00:00,Explainability of Deep Learning models for Urban Space perception,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ruben Sangers'), arxiv.Result.Author('Jan van Gemert'), arxiv.Result.Author('Sander van Cranenburgh')]","Deep learning based computer vision models are increasingly used by urban
planners to support decision making for shaping urban environments. Such models
predict how people perceive the urban environment quality in terms of e.g. its
safety or beauty. However, the blackbox nature of deep learning models hampers
urban planners to understand what landscape objects contribute to a
particularly high quality or low quality urban space perception. This study
investigates how computer vision models can be used to extract relevant policy
information about peoples' perception of the urban space. To do so, we train
two widely used computer vision architectures; a Convolutional Neural Network
and a transformer, and apply GradCAM -- a well-known ex-post explainable AI
technique -- to highlight the image regions important for the model's
prediction. Using these GradCAM visualizations, we manually annotate the
objects relevant to the models' perception predictions. As a result, we are
able to discover new objects that are not represented in present object
detection models used for annotation in previous studies. Moreover, our
methodological results suggest that transformer architectures are better suited
to be used in combination with GradCAM techniques. Code is available on Github.",-0.24577579,-0.024032146,0.03112059,C
10572,Code will be released for facilitating further research and application.,MapTR is of great application value in autonomous driving.,"1 INTRODUCTION

                                        High-deÔ¨Ånition (HD) map contains rich semantic information of road topology and trafÔ¨Åc rules,
                                        serving as a fundamental and indispensable component of self-driving system.",2022-08-30 17:55:59+00:00,MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Bencheng Liao'), arxiv.Result.Author('Shaoyu Chen'), arxiv.Result.Author('Xinggang Wang'), arxiv.Result.Author('Tianheng Cheng'), arxiv.Result.Author('Qian Zhang'), arxiv.Result.Author('Wenyu Liu'), arxiv.Result.Author('Chang Huang')]","We present MapTR, a structured end-to-end framework for efficient online
vectorized HD map construction. We propose a unified permutation-based modeling
approach, i.e., modeling map element as a point set with a group of equivalent
permutations, which avoids the definition ambiguity of map element and eases
learning. We adopt a hierarchical query embedding scheme to flexibly encode
structured map information and perform hierarchical bipartite matching for map
element learning. MapTR achieves the best performance and efficiency among
existing vectorized map construction approaches on nuScenes dataset. In
particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX
3090, $8\times$ faster than the existing state-of-the-art camera-based method
while achieving $3.3$ higher mAP. MapTR-tiny significantly outperforms the
existing state-of-the-art multi-modality method by $13.5$ mAP while being
faster. Qualitative results show that MapTR maintains stable and robust map
construction quality in complex and various driving scenes. Abundant demos are
available at \url{https://github.com/hustvl/MapTR} to prove the effectiveness
in real-world scenarios. MapTR is of great application value in autonomous
driving. Code will be released for facilitating further research and
application.",-0.02089373,0.22689092,-0.14473896,B
10640,"This work has the potential to inspire both further research, and commercial applications
of intelligent parking solutions to enhance and advance the parking lot management systems in South Africa.","In this paper, we illustrate how: by leveraging deep learning and the power of transfer learning, a small dataset of public
parking lot images captured using mobile phone cameras in Johannesburg, South African, together with the parking
lot‚Äôs longitude and latitude coordinates information can be used to guide drivers to the most convenient parking lot
based on their current location.","In the
next section, we explore and critique existing literature on this problem.",2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use 76
images from an expanding dataset of images, and annotate these with polygon
outlines of the four different types of objects of interest: cars, open parking
spots, people, and car number plates. We use the segmentation model to ensure
number plates can be occluded in production for car registration anonymity
purposes. We get an 89% and 82% intersection over union cover score on cars and
parking spaces respectively. This work has the potential to help reduce the
amount of time commuters spend searching for free public parking, hence easing
traffic congestion in and around shopping complexes and other public places,
and maximize people's utility with respect to driving on public roads.",-0.34212732,-0.10758697,-0.042893585,B
10641,"We do not demonstrate identiÔ¨Åcation of unique parking spots between video frames since we only show results on
captured images and not video feed from a mounted live camera, this is left for further research as it requires investment
into the camera hardware or permission to access surveillance feed from existing shopping complex cameras.","In this setting,
each image in the test set belongs to exactly one parking lot, and when the Ô¨Årst simulated driver location is presented,
the system then returns a list of possible parking lots, ranked from best to worst based on the objective function in 4.3.",Table 2 shows the number of instances of each class in the training dataset.,2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use 76
images from an expanding dataset of images, and annotate these with polygon
outlines of the four different types of objects of interest: cars, open parking
spots, people, and car number plates. We use the segmentation model to ensure
number plates can be occluded in production for car registration anonymity
purposes. We get an 89% and 82% intersection over union cover score on cars and
parking spaces respectively. This work has the potential to help reduce the
amount of time commuters spend searching for free public parking, hence easing
traffic congestion in and around shopping complexes and other public places,
and maximize people's utility with respect to driving on public roads.",-0.20456281,0.10086888,-0.14347202,B
10642,"One avenue
of further research into this problem is to investigate the implementation details of such systems, do they scale well,

Table 5: Recommended parking lot # based on space and distance optimization for a driver in different starting points

and preference for space availability vs distance

                                                   Starting Point

Œ±  Bushhill Waterval Ct Dobsonville Germiston S Eldoraigne

10‚àí3 6             6                               6               6           6

10‚àí2 6             6                               6               6           6

10‚àí1 5             6                               6               6           6

0.25 5             6                               6               6           5

0.5 3              4                               2               2           5

0.75 3             4                               5               2           5

0.9 3              4                               2               2           3

0.999 3            7                               2               2           3

                                                      9
Parking Spot Detection Using ML  A PREPRINT

how do multiple requests from many drivers in real-time affect the reliability of inferences from the detection system.","We achieve
great results on the segmentation intersection over union measure of cars, number plates, and parking spots.","It
is also interesting to think about how a driver‚Äôs live location as he approaches a recommended parking lot, should affect
other users‚Äô parking lot recommendations in real-time.",2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use 76
images from an expanding dataset of images, and annotate these with polygon
outlines of the four different types of objects of interest: cars, open parking
spots, people, and car number plates. We use the segmentation model to ensure
number plates can be occluded in production for car registration anonymity
purposes. We get an 89% and 82% intersection over union cover score on cars and
parking spaces respectively. This work has the potential to help reduce the
amount of time commuters spend searching for free public parking, hence easing
traffic congestion in and around shopping complexes and other public places,
and maximize people's utility with respect to driving on public roads.",-0.041021816,0.06618922,-0.16157806,B
10643,"This work has the potential to inspire both further research, and commercial
applications of intelligent parking solutions to enhance and advance the parking lot management systems in South
Africa.","In this paper, we illustrate how: by leveraging deep learning and the power of transfer learning, a small dataset
of public parking lot images captured using mobile phone cameras in Johannesburg, South African, together with
the parking lot‚Äôs longitude and latitude coordinates information can be used to guide drivers to the most convenient
parking lot based on their current location.","In the next section, we explore and critique existing literature on this problem.",2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use 76
images from an expanding dataset of images, and annotate these with polygon
outlines of the four different types of objects of interest: cars, open parking
spots, people, and car number plates. We use the segmentation model to ensure
number plates can be occluded in production for car registration anonymity
purposes. We get an 89% and 82% intersection over union cover score on cars and
parking spaces respectively. This work has the potential to help reduce the
amount of time commuters spend searching for free public parking, hence easing
traffic congestion in and around shopping complexes and other public places,
and maximize people's utility with respect to driving on public roads.",-0.34212732,-0.10758697,-0.042893585,B
10644,"We do not demonstrate
identiÔ¨Åcation of unique parking spots between video frames since we only show results on captured images and not
video feed from a mounted live camera, this is left for further research as it requires investment into the camera hardware
or permission to access surveillance feed from existing shopping complex cameras.","In this setting, each image in the test set
belongs to exactly one parking lot, and when the Ô¨Årst simulated driver location is presented, the system then returns a
list of possible parking lots, ranked from best to worst based on the objective function in 4.3.",Table 2 shows the number of instances of each class in the training dataset.,2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use 76
images from an expanding dataset of images, and annotate these with polygon
outlines of the four different types of objects of interest: cars, open parking
spots, people, and car number plates. We use the segmentation model to ensure
number plates can be occluded in production for car registration anonymity
purposes. We get an 89% and 82% intersection over union cover score on cars and
parking spaces respectively. This work has the potential to help reduce the
amount of time commuters spend searching for free public parking, hence easing
traffic congestion in and around shopping complexes and other public places,
and maximize people's utility with respect to driving on public roads.",-0.20456281,0.10086888,-0.14347202,B
10645,"One avenue
of further research into this problem is to investigate the implementation details of such systems, do they scale well,
how do multiple requests from many drivers in real-time affect the reliability of inferences from the detection system.","We achieve
great results on the segmentation intersection over union measure of cars, number plates, and parking spots.","It
is also interesting to think about how a driver‚Äôs live location as he approaches a recommended parking lot, should affect
other users‚Äô parking lot recommendations in real-time.",2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use 76
images from an expanding dataset of images, and annotate these with polygon
outlines of the four different types of objects of interest: cars, open parking
spots, people, and car number plates. We use the segmentation model to ensure
number plates can be occluded in production for car registration anonymity
purposes. We get an 89% and 82% intersection over union cover score on cars and
parking spaces respectively. This work has the potential to help reduce the
amount of time commuters spend searching for free public parking, hence easing
traffic congestion in and around shopping complexes and other public places,
and maximize people's utility with respect to driving on public roads.",-0.10888894,0.100046456,-0.22850262,B
10646,"This work has the potential to inspire both further research, and commercial applications
of intelligent parking solutions to enhance and advance the parking lot management systems in South Africa.","In this paper, we illustrate how: by leveraging deep learning and the power of transfer learning, a small dataset of public
parking lot images captured using mobile phone cameras in Johannesburg, South African, together with the parking
lot‚Äôs longitude and latitude coordinates information can be used to guide drivers to the most convenient parking lot
based on their current location.","In the
next section, we explore and critique existing literature in this domain.",2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use images
from an expanding dataset of images, and annotate these with polygon outlines
of the four different types of objects of interest: cars, open parking spots,
people, and car number plates. We use the segmentation model to ensure number
plates can be occluded in production for car registration anonymity purposes.
We get an 89% and 82% intersection over union cover score on cars and parking
spaces respectively. This work has the potential to help reduce the amount of
time commuters spend searching for free public parking, hence easing traffic
congestion in and around shopping complexes and other public places, and
maximize people's utility with respect to driving on public roads.",-0.3443789,-0.116540544,-0.041248348,B
10647,"This is left for further research as it requires Ô¨Ånancial investment in
buying camera hardware or permission to access surveillance feed from existing shopping complex cameras.","We do not
demonstrate identiÔ¨Åcation of unique parking spots between video frames since we only show results on captured images
and not video feed from a mounted live camera.",Table 2 shows the number of instances of each class in the training dataset.,2022-09-01 04:09:51+00:00,Public Parking Spot Detection And Geo-localization Using Transfer Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Yao Chao Yang')]","In cities around the world, locating public parking lots with vacant parking
spots is a major problem, costing commuters time and adding to traffic
congestion. This work illustrates how a dataset of Geo-tagged images from a
mobile phone camera, can be used in navigating to the most convenient public
parking lot in Johannesburg with an available parking space, detected by a
neural network powered-public camera. The images are used to fine-tune a
Detectron2 model pre-trained on the ImageNet dataset to demonstrate detection
and segmentation of vacant parking spots, we then add the parking lot's
corresponding longitude and latitude coordinates to recommend the most
convenient parking lot to the driver based on the Haversine distance and number
of available parking spots. Using the VGG Image Annotation (VIA) we use images
from an expanding dataset of images, and annotate these with polygon outlines
of the four different types of objects of interest: cars, open parking spots,
people, and car number plates. We use the segmentation model to ensure number
plates can be occluded in production for car registration anonymity purposes.
We get an 89% and 82% intersection over union cover score on cars and parking
spaces respectively. This work has the potential to help reduce the amount of
time commuters spend searching for free public parking, hence easing traffic
congestion in and around shopping complexes and other public places, and
maximize people's utility with respect to driving on public roads.",-0.28290805,0.14292434,-0.18810716,B
10691,"We hope that our work
will encourage further research to better our understanding of what is being learned by inpainting.","But it‚Äôs also evident that contemporary
large-scale inpainting models are learning quite sophisticated long-range co-occurrences and
symmetries in the data which can often enable impressive visual reasoning.","10
Acknowledgements: We would like to thank Assaf Shocher for insightful discussions and ideas
related to the Figures dataset.",2022-09-01 17:59:33+00:00,Visual Prompting via Image Inpainting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Amir Bar'), arxiv.Result.Author('Yossi Gandelsman'), arxiv.Result.Author('Trevor Darrell'), arxiv.Result.Author('Amir Globerson'), arxiv.Result.Author('Alexei A. Efros')]","How does one adapt a pre-trained visual model to novel downstream tasks
without task-specific finetuning or any model modification? Inspired by
prompting in NLP, this paper investigates visual prompting: given input-output
image example(s) of a new task at test time and a new input image, the goal is
to automatically produce the output image, consistent with the given examples.
We show that posing this problem as simple image inpainting - literally just
filling in a hole in a concatenated visual prompt image - turns out to be
surprisingly effective, provided that the inpainting algorithm has been trained
on the right data. We train masked auto-encoders on a new dataset that we
curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply
visual prompting to these pretrained models and demonstrate results on various
downstream image-to-image tasks, including foreground segmentation, single
object detection, colorization, edge detection, etc.",-0.17827427,0.08406989,0.0033441149,B
10756,"However, a reliable evaluation
                                                          metric for I2I models is not available and demands
        1 https://zenodo.org/record/1219280#.X6E4O4hKiUk  further research (Colleoni and Stoyanov, 2021).",The live datset comprises of 3 videos      also been used.,"Table 8: Tool Navigation Datasets                                                                      Springer Nature 2021 LATEX template

Dataset             Year  Data Size        Procedure             Availability  Tools    Annotations         Tasks
EndoVis 15          2015  Images, Videos   colorectal surgery    Public        Rigid    Pixel-wise,2D pose  Sega, Ta

                                                                               Robotic

NeuroSurgicalTools  2015  2476 images      Neurosurgery          Public        Rigid    Bounding-box        Da
FetalFlexTool       2015  21 images        Fetal Surgery
                                                                 Public        Rigid    Bounding-box        Da

M2CAI16-tool        2016  One video        cholecystectomy       Public        Rigid    TPh                 Da               Skills
                    2016  15 Videos        cholecystectomy       Public        Rigid    TPh, Phase
Cholec80            2017  80 Videos        Porcine               Public        Robotic                      D,PRa
EndoVis 17b         2017  8 Videos         In-vitro Experiments  Public        Robotic  Piwel-wise
ATLAS Dione         2017  99 Videos        Cardiac               Public        NA       Bounding Box        Seg(B,P,Ic )
Hamyln              2018  2 Phantom        Nephrectomy           Public        Robotic  Depth map           D,La ,Activity,
EndoVis 18d         2018  16 Videos        Gynecologic Surgery   Public        Rigid                        TTa
LapGyn4             2018  55K images       cholecystectomy       Public        Rigid    Pixel-wise mask
m2ccai16-tool             15 Videos                                                     No annotation       Scene Sega
                                                                                        Bounding-box        Multiplee
                                                                                                            Da

locations           2019 10040 frames      proctocolectomy       Public        Rigid    Instances           Seg(B,P,Id )
EndoVis 19

UCL                 2019  16016 Synthetic  rectal resection      Public        NA       Depth map           DEa
                                           sigmoid resection‚àó

                                           colonoscopy

Cata7               2019  images           Cataract Surgery      Private       Rigid    Pixel-wise mask     Seg(Id )
SCAREDf             2019  7 Videos         Porcine               Public        NA       Depth+              3D reconstruction
                          27 Videos

UCL dVRK            2020 14 Videos+        Ex-Vivo               Public        Robotic  camera parameters   Seg(Bd )
                                                                                        Pixel-wise

Sinus Surgery-C     2020  Kinematic Data   Sinus-Cadaver         Public        Rigid    Pixel-wise mask     Seg(Bc )
Sinus Surgery-L     2020  10Videos         Sinus-Live            Public        Rigid    Pixel-wise mask     Seg(Bc )
LapSig300           2020  3 Videos         Colorectal Surgery    Private       Rigid    Pixel-wise mask     Seg(Ic), PR,ARa
                          300 Videos

EndoVis 21g         2021  33 Videos        cholecystectomy       Public        Rigid    Phase, Action       D,PR,ARa
dVPN                2021  48702 images     nephrectomy           Private       NA       TP, A,SC,Phh        DEa
CaDTD               2021  50 Videos        Cataract              Public        Rigid    NA                  Da
                                                                                        Bounding-box

                                           Surgery

aSeg=Segmentation, T=Tracking, D=Detection PR=Phase Recognition, L=Localization, DE=Depth estimation TT=Tissue Tracking AR=Action
Recognition; bRobotic Instrument Segmentation Sub-Challenge; c B=Binary, P=Parts detection, I=Instance Segmentation; d Robotic Scene
Segmentation Sub-Challenge;einstrument counts, action detection, anatomical structures; f Stereo Correspondence and Reconstruction of Endoscopic
Data Sub-Challenge; g Surgical WorkÔ¨Çow and Skill Analysis;h TP=Tool presence, A=Action, SC=Skill ClassiÔ¨Åcation, Ph=Phases; ‚àó Unkmown Surgery
    Springer Nature 2021 LATEX template

35

    Automatic labelling by a trained detector model        videos can be combined with reinforcement
    have also been proposed to perform instrument          learning for improved surgical training (Tan et al,
    detection training in semi-supervised manner           2019).Another potential future research direction
    (Yoon et al, 2020b), however the approach does         is building a cognitive robotic system which is
    not fully realise better performance.",2022-09-03 14:25:39+00:00,A comprehensive survey on recent deep learning-based methods applied to surgical data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mansoor Ali'), arxiv.Result.Author('Rafael Martinez Garcia Pena'), arxiv.Result.Author('Gilberto Ochoa Ruiz'), arxiv.Result.Author('Sharib Ali')]","Minimally invasive surgery is highly operator dependant with lengthy
procedural times causing fatigue and risk to patients. In order to mitigate
these risks, real-time systems can help assist surgeons to navigate and track
tools, by providing clear understanding of scene and avoid miscalculations
during operation. While several efforts have been made in this direction, a
lack of diverse datasets, as well as very dynamic scenes and its variability in
each patient entails major hurdle in accomplishing robust systems. In this
work, we present a systematic review of recent machine learning-based
approaches including surgical tool localisation, segmentation, tracking and 3D
scene perception. Furthermore, we present current gaps and directions of these
invented methods and provide rational behind clinical integration of these
approaches.",0.03801726,0.13904895,-0.12585735,B
10760,"We further study if the fea-
tures learned via multimodal pre-training can beneÔ¨Åt VidL               2We base our ablation experiments on these two representative datasets
pre-training.","In
                                                                    contrast, HOG renders degradation on downstream video-
Multimodal Features (MMF).","We utilize the vision branch of the ViT-Base          for fast iteration, our main results are reported on 13 benchmarks in Sec-
backbone [13] in CLIP [55] to extract such multimodal fea-          tion 6.",2022-09-04 06:30:32+00:00,An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tsu-Jui Fu'), arxiv.Result.Author('Linjie Li'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Kevin Lin'), arxiv.Result.Author('William Yang Wang'), arxiv.Result.Author('Lijuan Wang'), arxiv.Result.Author('Zicheng Liu')]","Masked visual modeling (MVM) has been recently proven effective for visual
pre-training. While similar reconstructive objectives on video inputs (e.g.,
masked frame modeling) have been explored in video-language (VidL)
pre-training, the pre-extracted video features in previous studies cannot be
refined through MVM during pre-training, and thus leading to unsatisfactory
downstream performance. In this work, we systematically examine the potential
of MVM in the context of VidL learning. Specifically, we base our study on a
fully end-to-end VIdeO-LanguagE Transformer (VIOLET), which mitigates the
disconnection between fixed video representations and MVM training. In total,
eight different reconstructive targets of MVM are explored, from low-level
pixel values and oriented gradients to high-level depth maps, optical flow,
discrete visual tokens and latent visual features. We conduct comprehensive
experiments and provide insights on the factors leading to effective MVM
training. Empirically, we show VIOLET pre-trained with MVM objective achieves
notable improvements on 13 VidL benchmarks, ranging from video question
answering, video captioning, to text-to-video retrieval.",-0.044911154,-0.04898938,0.102411404,C
10761,"We further study if the fea-        colors contributes to a relatively small gain of +0.2% on
tures learned via multimodal pre-training can beneÔ¨Åt VidL      TGIF-Frame and +0.8% on AveR for DiDeMo Retrieval.","Compared to the base-
                                                               line without MVM objective, regressing the explicit RGB
Multimodal Features (MMF).","In
pre-training.",2022-09-04 06:30:32+00:00,An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tsu-Jui Fu'), arxiv.Result.Author('Linjie Li'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Kevin Lin'), arxiv.Result.Author('William Yang Wang'), arxiv.Result.Author('Lijuan Wang'), arxiv.Result.Author('Zicheng Liu')]","Masked visual modeling (MVM) has been recently proven effective for visual
pre-training. While similar reconstructive objectives on video inputs (e.g.,
masked frame modeling) have been explored in video-language (VidL)
pre-training, previous studies fail to find a truly effective MVM strategy that
can largely benefit the downstream performance. In this work, we systematically
examine the potential of MVM in the context of VidL learning. Specifically, we
base our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), where
the supervision from MVM training can be backpropogated to the video pixel
space. In total, eight different reconstructive targets of MVM are explored,
from low-level pixel values and oriented gradients to high-level depth maps,
optical flow, discrete visual tokens and latent visual features. We conduct
comprehensive experiments and provide insights into the factors leading to
effective MVM training, resulting in an enhanced model VIOLETv2. Empirically,
we show VIOLETv2 pre-trained with MVM objective achieves notable improvements
on 13 VidL benchmarks, ranging from video question answering, video captioning,
to text-to-video retrieval.",-0.04684792,-0.10556476,0.04938133,C
10821,"We will
                                                                     release HSO to stimulate further research on this topic.","The overall loss function is:       HSO dataset are shown in Figure 7, where we can see that
                                                                     HSO has glass with reasonable property distributions in terms
                    3                                                of location, area, scene, and global color contrast.","Loverall = 2(3‚àíi)Lihybrid,                            (7)

                    i=1

where Lihybrid denotes the hybrid loss between the ground            B.",2022-09-06 08:11:17+00:00,Progressive Glass Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Letian Yu'), arxiv.Result.Author('Haiyang Mei'), arxiv.Result.Author('Wen Dong'), arxiv.Result.Author('Ziqi Wei'), arxiv.Result.Author('Li Zhu'), arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Xin Yang')]","Glass is very common in the real world. Influenced by the uncertainty about
the glass region and the varying complex scenes behind the glass, the existence
of glass poses severe challenges to many computer vision tasks, making glass
segmentation as an important computer vision task. Glass does not have its own
visual appearances but only transmit/reflect the appearances of its
surroundings, making it fundamentally different from other common objects. To
address such a challenging task, existing methods typically explore and combine
useful cues from different levels of features in the deep network. As there
exists a characteristic gap between level-different features, i.e., deep layer
features embed more high-level semantics and are better at locating the target
objects while shallow layer features have larger spatial sizes and keep richer
and more detailed low-level information, fusing these features naively thus
would lead to a sub-optimal solution. In this paper, we approach the effective
features fusion towards accurate glass segmentation in two steps. First, we
attempt to bridge the characteristic gap between different levels of features
by developing a Discriminability Enhancement (DE) module which enables
level-specific features to be a more discriminative representation, alleviating
the features incompatibility for fusion. Second, we design a
Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful
information in the fusion process by highlighting the common and exploring the
difference between level-different features.",0.15722932,0.20176351,0.13216715,A
10822,"SpeciÔ¨Åcally, we         our HSO dataset leads to a Àú10% IoU drop for all methods in
choose semantic segmentation methods ICNet [82], PSPNet             Table II, reÔ¨Çecting the large room to achieve accurate glass
[14], DeepLabv3+ [8], DenseASPP [11], BiSeNet [12], DANet           segmentation and the necessity of the new HSO dataset for
[9], CCNet [10], GFFNet [17], SFNet [16] and FaPN [18];             stimulating further research.","Noteworthy is that
of-the-art performance in the speciÔ¨Åc Ô¨Åeld.","We also retrained our PGSNet on
salient object detection methods DSS [23], PiCANet [32],            the GSD dataset [51] and presented the results in Table III.",2022-09-06 08:11:17+00:00,Progressive Glass Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Letian Yu'), arxiv.Result.Author('Haiyang Mei'), arxiv.Result.Author('Wen Dong'), arxiv.Result.Author('Ziqi Wei'), arxiv.Result.Author('Li Zhu'), arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Xin Yang')]","Glass is very common in the real world. Influenced by the uncertainty about
the glass region and the varying complex scenes behind the glass, the existence
of glass poses severe challenges to many computer vision tasks, making glass
segmentation as an important computer vision task. Glass does not have its own
visual appearances but only transmit/reflect the appearances of its
surroundings, making it fundamentally different from other common objects. To
address such a challenging task, existing methods typically explore and combine
useful cues from different levels of features in the deep network. As there
exists a characteristic gap between level-different features, i.e., deep layer
features embed more high-level semantics and are better at locating the target
objects while shallow layer features have larger spatial sizes and keep richer
and more detailed low-level information, fusing these features naively thus
would lead to a sub-optimal solution. In this paper, we approach the effective
features fusion towards accurate glass segmentation in two steps. First, we
attempt to bridge the characteristic gap between different levels of features
by developing a Discriminability Enhancement (DE) module which enables
level-specific features to be a more discriminative representation, alleviating
the features incompatibility for fusion. Second, we design a
Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful
information in the fusion process by highlighting the common and exploring the
difference between level-different features.",-0.26426333,-0.059537083,0.11053149,B
10851,"Rep., 2022.
the Future Development Leaderboard for future evaluation at
https://www.iarai.ac.at/landslide4sense/challenge/ is active to     [6] O. Ghorbanzadeh, Y. Xu, P. Ghamisi, M. Kopp, and D. Kreil, ‚ÄúLand-
allow further research developments and contributions.","M. Keiler, and B. Schneider-Muntau, ‚ÄúExtending the integrated mon-
                                                                         itoring of deep-seated landslide activity into the past using free and
   The data remain accessible after the L4S competition and              open-source photogrammetry,‚Äù Copernicus Meetings, Tech.","In this           slide4sense: Reference benchmark data and deep learning models for
way, anyone can submit landslide detection results on the test           landslide detection,‚Äù arXiv preprint arXiv:2206.00515, 2022.
data set, make comparisons of their performance to that of
                                                                    [7] B. van den Bout, C. Tang, C. van Westen, and V. Jetten, ‚ÄúPhysically-
                                                                         based modelling of co-seismic landslide, debris Ô¨Çow and Ô¨Çood cascade,‚Äù
                                                                         Natural Hazards and Earth System Sciences Discussions, pp.",2022-09-06 15:05:12+00:00,The Outcome of the 2022 Landslide4Sense Competition: Advanced Landslide Detection from Multi-Source Satellite Imagery,cs.CV,['cs.CV'],"[arxiv.Result.Author('Omid Ghorbanzadeh'), arxiv.Result.Author('Yonghao Xu'), arxiv.Result.Author('Hengwei Zhao'), arxiv.Result.Author('Junjue Wang'), arxiv.Result.Author('Yanfei Zhong'), arxiv.Result.Author('Dong Zhao'), arxiv.Result.Author('Qi Zang'), arxiv.Result.Author('Shuang Wang'), arxiv.Result.Author('Fahong Zhang'), arxiv.Result.Author('Yilei Shi'), arxiv.Result.Author('Xiao Xiang Zhu'), arxiv.Result.Author('Lin Bai'), arxiv.Result.Author('Weile Li'), arxiv.Result.Author('Weihang Peng'), arxiv.Result.Author('Pedram Ghamisi')]","The scientific outcomes of the 2022 Landslide4Sense (L4S) competition
organized by the Institute of Advanced Research in Artificial Intelligence
(IARAI) are presented here. The objective of the competition is to
automatically detect landslides based on large-scale multiple sources of
satellite imagery collected globally. The 2022 L4S aims to foster
interdisciplinary research on recent developments in deep learning (DL) models
for the semantic segmentation task using satellite imagery. In the past few
years, DL-based models have achieved performance that meets expectations on
image interpretation, due to the development of convolutional neural networks
(CNNs). The main objective of this article is to present the details and the
best-performing algorithms featured in this competition. The winning solutions
are elaborated with state-of-the-art models like the Swin Transformer,
SegFormer, and U-Net. Advanced machine learning techniques and strategies such
as hard example mining, self-training, and mix-up data augmentation are also
considered. Moreover, we describe the L4S benchmark data set in order to
facilitate further comparisons, and report the results of the accuracy
assessment online. The data is accessible on \textit{Future Development
Leaderboard} for future evaluation at
\url{https://www.iarai.ac.at/landslide4sense/challenge/}, and researchers are
invited to submit more prediction results, evaluate the accuracy of their
methods, compare them with those of other users, and, ideally, improve the
landslide detection results reported in this article.",-0.081225015,0.04553017,0.12273842,B
10852,"421‚Äì435, 2022.
the Future Development Leaderboard for future evaluation at
https://www.iarai.ac.at/landslide4sense/challenge/ is active to     [5] J. Branke, T. Zieher, J. Pfeiffer, M. Bremer, M. Rutzinger, B. Gems,
allow further research developments and contributions.","2, pp.","In this           M. Keiler, and B. Schneider-Muntau, ‚ÄúExtending the integrated mon-
way, anyone can submit landslide detection results on the test           itoring of deep-seated landslide activity into the past using free and
data set, make comparisons of their performance to that of               open-source photogrammetry,‚Äù Copernicus Meetings, Tech.",2022-09-06 15:05:12+00:00,The Outcome of the 2022 Landslide4Sense Competition: Advanced Landslide Detection from Multi-Source Satellite Imagery,cs.CV,['cs.CV'],"[arxiv.Result.Author('Omid Ghorbanzadeh'), arxiv.Result.Author('Yonghao Xu'), arxiv.Result.Author('Hengwei Zhao'), arxiv.Result.Author('Junjue Wang'), arxiv.Result.Author('Yanfei Zhong'), arxiv.Result.Author('Dong Zhao'), arxiv.Result.Author('Qi Zang'), arxiv.Result.Author('Shuang Wang'), arxiv.Result.Author('Fahong Zhang'), arxiv.Result.Author('Yilei Shi'), arxiv.Result.Author('Xiao Xiang Zhu'), arxiv.Result.Author('Lin Bai'), arxiv.Result.Author('Weile Li'), arxiv.Result.Author('Weihang Peng'), arxiv.Result.Author('Pedram Ghamisi')]","The scientific outcomes of the 2022 Landslide4Sense (L4S) competition
organized by the Institute of Advanced Research in Artificial Intelligence
(IARAI) are presented here. The objective of the competition is to
automatically detect landslides based on large-scale multiple sources of
satellite imagery collected globally. The 2022 L4S aims to foster
interdisciplinary research on recent developments in deep learning (DL) models
for the semantic segmentation task using satellite imagery. In the past few
years, DL-based models have achieved performance that meets expectations on
image interpretation, due to the development of convolutional neural networks
(CNNs). The main objective of this article is to present the details and the
best-performing algorithms featured in this competition. The winning solutions
are elaborated with state-of-the-art models like the Swin Transformer,
SegFormer, and U-Net. Advanced machine learning techniques and strategies such
as hard example mining, self-training, and mix-up data augmentation are also
considered. Moreover, we describe the L4S benchmark data set in order to
facilitate further comparisons, and report the results of the accuracy
assessment online. The data is accessible on \textit{Future Development
Leaderboard} for future evaluation at
\url{https://www.iarai.ac.at/landslide4sense/challenge/}, and researchers are
invited to submit more prediction results, evaluate the accuracy of their
methods, compare them with those of other users, and, ideally, improve the
landslide detection results reported in this article.",-0.03733909,0.2601502,0.006159555,B
10868,"Their works inspire
transformation, deletion, and blur, their data distribution        us to further study the impact of colour on deep networks.","However, when the           our base database and augmented different colour
images are affected by perturbations like geometric                distorted images from these images.","will change, which increases the probability of neural             Architectures.",2022-09-02 08:16:04+00:00,Impact of Colour Variation on Robustness of Deep Neural Networks,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Chengyin Hu'), arxiv.Result.Author('Weiwen Shi')]","Deep neural networks (DNNs) have have shown state-of-the-art performance for
computer vision applications like image classification, segmentation and object
detection. Whereas recent advances have shown their vulnerability to manual
digital perturbations in the input data, namely adversarial attacks. The
accuracy of the networks is significantly affected by the data distribution of
their training dataset. Distortions or perturbations on color space of input
images generates out-of-distribution data, which make networks more likely to
misclassify them. In this work, we propose a color-variation dataset by
distorting their RGB color on a subset of the ImageNet with 27 different
combinations. The aim of our work is to study the impact of color variation on
the performance of DNNs. We perform experiments on several state-of-the-art DNN
architectures on the proposed dataset, and the result shows a significant
correlation between color variation and loss of accuracy. Furthermore, based on
the ResNet50 architecture, we demonstrate some experiments of the performance
of recently proposed robust training techniques and strategies, such as Augmix,
revisit, and free normalizer, on our proposed dataset. Experimental results
indicate that these robust training techniques can improve the robustness of
deep networks to color variation.",-0.10551077,-0.049633164,0.2612775,C
10900,"We further study the impact of permanent faults in a real-time situation by considering
continuous video sequences and observing a signiÔ¨Åcant frequency that the error manifestation
persists for a critical time interval.","We
Ô¨Ånd that a hardware fault - if it hits the crucial bits of either neuron or weight - can silently lead to
excessive amounts of additional false positives (FPs) and increase the rate of false negatives (FNs)
misses.","In summary, this paper makes the following contributions:
  ‚Ä¢ We demonstrate that AP-based metrics lead to misleading vulnerability estimates for object

     detection DNN models (Sec.",2022-09-07 15:27:09+00:00,Hardware faults that matter: Understanding and Estimating the safety impact of hardware faults on object detection DNNs,cs.CV,"['cs.CV', 'cs.AI', 'eess.IV']","[arxiv.Result.Author('Syed Qutub'), arxiv.Result.Author('Florian Geissler'), arxiv.Result.Author('Yang Peng'), arxiv.Result.Author('Ralf Grafe'), arxiv.Result.Author('Michael Paulitsch'), arxiv.Result.Author('Gereon Hinz'), arxiv.Result.Author('Alois Knoll')]","Object detection neural network models need to perform reliably in highly
dynamic and safety-critical environments like automated driving or robotics.
Therefore, it is paramount to verify the robustness of the detection under
unexpected hardware faults like soft errors that can impact a systems
perception module. Standard metrics based on average precision produce model
vulnerability estimates at the object level rather than at an image level. As
we show in this paper, this does not provide an intuitive or representative
indicator of the safety-related impact of silent data corruption caused by bit
flips in the underlying memory but can lead to an over- or underestimation of
typical fault-induced hazards. With an eye towards safety-related real-time
applications, we propose a new metric IVMOD (Image-wise Vulnerability Metric
for Object Detection) to quantify vulnerability based on an incorrect
image-wise object detection due to false positive (FPs) or false negative (FNs)
objects, combined with a severity analysis. The evaluation of several
representative object detection models shows that even a single bit flip can
lead to a severe silent data corruption event with potentially critical safety
implications, with e.g., up to (much greater than) 100 FPs generated, or up to
approx. 90% of true positives (TPs) are lost in an image. Furthermore, with a
single stuck-at-1 fault, an entire sequence of images can be affected, causing
temporally persistent ghost detections that can be mistaken for actual objects
(covering up to approx. 83% of the image). Furthermore, actual objects in the
scene are continuously missed (up to approx. 64% of TPs are lost). Our work
establishes a detailed understanding of the safety-related vulnerability of
such critical workloads against hardware faults.",-0.09282242,-0.0723405,0.0955758,C
10978,"Conclusions                                                that TEACH will encourage further research on combining
                                                              language and 3D motion, much like the Ô¨Åeld has done with
   We presented a new task on motion generation from a        language and 2D images [34, 37].","We hope
5.","sequence of textual prompts, which we refer to as action
compositions in time.",2022-09-09 00:33:40+00:00,TEACH: Temporal Action Composition for 3D Humans,cs.CV,['cs.CV'],"[arxiv.Result.Author('Nikos Athanasiou'), arxiv.Result.Author('Mathis Petrovich'), arxiv.Result.Author('Michael J. Black'), arxiv.Result.Author('G√ºl Varol')]","Given a series of natural language descriptions, our task is to generate 3D
human motions that correspond semantically to the text, and follow the temporal
order of the instructions. In particular, our goal is to enable the synthesis
of a series of actions, which we refer to as temporal action composition. The
current state of the art in text-conditioned motion synthesis only takes a
single action or a single sentence as input. This is partially due to lack of
suitable training data containing action sequences, but also due to the
computational complexity of their non-autoregressive model formulation, which
does not scale well to long sequences. In this work, we address both issues.
First, we exploit the recent BABEL motion-text collection, which has a wide
range of labeled actions, many of which occur in a sequence with transitions
between them. Next, we design a Transformer-based approach that operates
non-autoregressively within an action, but autoregressively within the sequence
of actions. This hierarchical formulation proves effective in our experiments
when compared with multiple baselines. Our approach, called TEACH for ""TEmporal
Action Compositions for Human motions"", produces realistic human motions for a
wide variety of actions and temporal compositions from language descriptions.
To encourage work on this new task, we make our code available for research
purposes at $\href{teach.is.tue.mpg.de}{\textrm{our website}}$.",-0.13200097,0.074959114,-0.3530612,B
10979,"Conclusions                                                that TEACH will encourage further research on combining
                                                              language and 3D motion, much like the Ô¨Åeld has done with
   We presented a new task on motion generation from a        language and 2D images [34, 37].","We hope
5.","sequence of textual prompts, which we refer to as action
compositions in time.",2022-09-09 00:33:40+00:00,TEACH: Temporal Action Composition for 3D Humans,cs.CV,['cs.CV'],"[arxiv.Result.Author('Nikos Athanasiou'), arxiv.Result.Author('Mathis Petrovich'), arxiv.Result.Author('Michael J. Black'), arxiv.Result.Author('G√ºl Varol')]","Given a series of natural language descriptions, our task is to generate 3D
human motions that correspond semantically to the text, and follow the temporal
order of the instructions. In particular, our goal is to enable the synthesis
of a series of actions, which we refer to as temporal action composition. The
current state of the art in text-conditioned motion synthesis only takes a
single action or a single sentence as input. This is partially due to lack of
suitable training data containing action sequences, but also due to the
computational complexity of their non-autoregressive model formulation, which
does not scale well to long sequences. In this work, we address both issues.
First, we exploit the recent BABEL motion-text collection, which has a wide
range of labeled actions, many of which occur in a sequence with transitions
between them. Next, we design a Transformer-based approach that operates
non-autoregressively within an action, but autoregressively within the sequence
of actions. This hierarchical formulation proves effective in our experiments
when compared with multiple baselines. Our approach, called TEACH for ""TEmporal
Action Compositions for Human motions"", produces realistic human motions for a
wide variety of actions and temporal compositions from language descriptions.
To encourage work on this new task, we make our code available for research
purposes at our $\href{teach.is.tue.mpg.de}{\text{website}}$.",-0.13200097,0.074959114,-0.3530612,B
10981,"In further research, ECAPA-         the study for the inference of the keyframes of two modali-
                                       TDNN [2] was based on blocks of TDNNs and Squeeze-                ties.","Cycle consistency was introduced into
                                       traditional averaging method.","The method that combines cycle consistency and atten-
                                       Excitation(SE) [3] to reconstruct frame-level features.",2022-09-09 02:29:47+00:00,Learning Audio-Visual embedding for Wild Person Verification,cs.CV,"['cs.CV', 'cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Peiwen Sun'), arxiv.Result.Author('Shanshan Zhang'), arxiv.Result.Author('Zishan Liu'), arxiv.Result.Author('Yougen Yuan'), arxiv.Result.Author('Taotao Zhang'), arxiv.Result.Author('Honggang Zhang'), arxiv.Result.Author('Pengfei Hu')]","It has already been observed that audio-visual embedding can be extracted
from these two modalities to gain robustness for person verification. However,
the aggregator that used to generate a single utterance representation from
each frame does not seem to be well explored. In this article, we proposed an
audio-visual network that considers aggregator from a fusion perspective. We
introduced improved attentive statistics pooling for the first time in face
verification. Then we find that strong correlation exists between modalities
during pooling, so joint attentive pooling is proposed which contains cycle
consistency to learn the implicit inter-frame weight. Finally, fuse the
modality with a gated attention mechanism. All the proposed models are trained
on the VoxCeleb2 dev dataset and the best system obtains 0.18\%, 0.27\%, and
0.49\% EER on three official trail lists of VoxCeleb1 respectively, which is to
our knowledge the best-published results for person verification. As an
analysis, visualization maps are generated to explain how this system interact
between modalities.",0.027195957,0.011913686,-0.010157885,C
10982,"In further research,     modality (Sec.3), joint attentive pooling module, and fusion
                                        ECAPA-TDNN [4] was based on blocks of TDNNs and                 module (Sec.2.3).","METHOD
                                        attentive statistics pooling to focus on important frames of
                                        speaker veriÔ¨Åcation and get higher discriminative ability       The overall network (Fig.1) consists of the backbone of each
                                        than the traditional averaging method.","The joint attentive pooling module is the
                                        Squeeze-Excitation(SE) [5] to reconstruct frame-level fea-      combination of weight-enhanced attentive statistical pooling
                                        tures.",2022-09-09 02:29:47+00:00,Learning Audio-Visual embedding for Person Verification in the Wild,cs.CV,"['cs.CV', 'cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Peiwen Sun'), arxiv.Result.Author('Shanshan Zhang'), arxiv.Result.Author('Zishan Liu'), arxiv.Result.Author('Yougen Yuan'), arxiv.Result.Author('Taotao Zhang'), arxiv.Result.Author('Honggang Zhang'), arxiv.Result.Author('Pengfei Hu')]","It has already been observed that audio-visual embedding is more robust than
uni-modality embedding for person verification. Here, we proposed a novel
audio-visual strategy that considers aggregators from a fusion perspective.
First, we introduced weight-enhanced attentive statistics pooling for the first
time in face verification. We find that a strong correlation exists between
modalities during pooling, so joint attentive pooling is proposed which
contains cycle consistency to learn the implicit inter-frame weight. Finally,
each modality is fused with a gated attention mechanism to gain robust
audio-visual embedding. All the proposed models are trained on the VoxCeleb2
dev dataset and the best system obtains 0.18%, 0.27%, and 0.49% EER on three
official trial lists of VoxCeleb1 respectively, which is to our knowledge the
best-published results for person verification.",0.059739955,-0.1826643,0.08004839,C
10996,"Although this approach is viable under known lighting conditions, further research must be conducted on
the dynamic threshold setting under varying ambient lighting.","The contour detection approach requires adequate image Ô¨Åltering for
noise reduction and adjusting the algorithm thresholds to adapt to the varying lighting conditions Bonadies and Gads-
den (2019).",Gao et al.,2022-09-09 12:47:24+00:00,Deep learning-based Crop Row Following for Infield Navigation of Agri-Robots,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Rajitha de Silva'), arxiv.Result.Author('Grzegorz Cielniak'), arxiv.Result.Author('Gang Wang'), arxiv.Result.Author('Junfeng Gao')]","Autonomous navigation in agricultural environments is often challenged by
varying field conditions that may arise in arable fields. The state-of-the-art
solutions for autonomous navigation in these agricultural environments will
require expensive hardware such as RTK-GPS. This paper presents a robust crop
row detection algorithm that can withstand those variations while detecting
crop rows for visual servoing. A dataset of sugar beet images was created with
43 combinations of 11 field variations found in arable fields. The novel crop
row detection algorithm is tested both for the crop row detection performance
and also the capability of visual servoing along a crop row. The algorithm only
uses RGB images as input and a convolutional neural network was used to predict
crop row masks. Our algorithm outperformed the baseline method which uses
colour-based segmentation for all the combinations of field variations. We use
a combined performance indicator that accounts for the angular and displacement
errors of the crop row detection. Our algorithm exhibited the worst performance
during the early growth stages of the crop.",-0.04189031,0.41398686,0.053451613,B
10998,"‚Äì We develop a baseline tracker S-KeepTrack base on the KeepTrack [20] to
     encourage further research on TSFMO.","‚Äì We evaluate 20 state-of-the-art tracking algorithms with in-depth analysis
     to assess their performance and provide comparisons on TSFMO.","2 Related Works

2.1 Visual Tracking Algorithms

Visual tracking has been studied for decades with a huge literature, the compre-
hensive review of which is out of scope of this paper.",2022-09-09 13:14:44+00:00,Tracking Small and Fast Moving Objects: A Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhewen Zhang'), arxiv.Result.Author('Fuliang Wu'), arxiv.Result.Author('Yuming Qiu'), arxiv.Result.Author('Jingdong Liang'), arxiv.Result.Author('Shuiwang Li')]","With more and more large-scale datasets available for training, visual
tracking has made great progress in recent years. However, current research in
the field mainly focuses on tracking generic objects. In this paper, we present
TSFMO, a benchmark for \textbf{T}racking \textbf{S}mall and \textbf{F}ast
\textbf{M}oving \textbf{O}bjects. This benchmark aims to encourage research in
developing novel and accurate methods for this challenging task particularly.
TSFMO consists of 250 sequences with about 50k frames in total. Each frame in
these sequences is carefully and manually annotated with a bounding box. To the
best of our knowledge, TSFMO is the first benchmark dedicated to tracking small
and fast moving objects, especially connected to sports. To understand how
existing methods perform and to provide comparison for future research on
TSFMO, we extensively evaluate 20 state-of-the-art trackers on the benchmark.
The evaluation results exhibit that more effort are required to improve
tracking small and fast moving objects. Moreover, to encourage future research,
we proposed a novel tracker S-KeepTrack which surpasses all 20 evaluated
approaches. By releasing TSFMO, we expect to facilitate future researches and
applications of tracking small and fast moving objects. The TSFMO and
evaluation results as well as S-KeepTrack are available at
\url{https://github.com/CodeOfGithub/S-KeepTrack}.",-0.051301703,0.18358545,-0.12512508,B
11023,"Effect of Objectives We further study the effect of   The increased time is mainly consumed by the ex-
each component of our proposed framework over         tra adaptation and augmentations.",the inference time increase to 85 and 89 seconds.,"When applying
the R2R benchmark.",2022-09-10 19:04:40+00:00,Anticipating the Unseen Discrepancy for Vision and Language Navigation,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Yujie Lu'), arxiv.Result.Author('Huiliang Zhang'), arxiv.Result.Author('Ping Nie'), arxiv.Result.Author('Weixi Feng'), arxiv.Result.Author('Wenda Xu'), arxiv.Result.Author('Xin Eric Wang'), arxiv.Result.Author('William Yang Wang')]","Vision-Language Navigation requires the agent to follow natural language
instructions to reach a specific target. The large discrepancy between seen and
unseen environments makes it challenging for the agent to generalize well.
Previous studies propose data augmentation methods to mitigate the data bias
explicitly or implicitly and provide improvements in generalization. However,
they try to memorize augmented trajectories and ignore the distribution shifts
under unseen environments at test time. In this paper, we propose an Unseen
Discrepancy Anticipating Vision and Language Navigation (DAVIS) that learns to
generalize to unseen environments via encouraging test-time visual consistency.
Specifically, we devise: 1) a semi-supervised framework DAVIS that leverages
visual consistency signals across similar semantic observations. 2) a two-stage
learning procedure that encourages adaptation to test-time distribution. The
framework enhances the basic mixture of imitation and reinforcement learning
with Momentum Contrast to encourage stable decision-making on similar
observations under a joint training stage and a test-time adaptation stage.
Extensive experiments show that DAVIS achieves model-agnostic improvement over
previous state-of-the-art VLN baselines on R2R and RxR benchmarks. Our source
code and data are in supplemental materials.",0.27131623,-0.012881549,0.016385863,A
11053,"To achieve this
                                        which had the same model performance of 98.3 percent in           aim, MAESTRO objectives are (i) to deploy and test a plat-
                                        mAP and a similar inference speed, all of our benchmark           form integrating multiple sensing modalities as a sandpit
                                        models including the original YOLOv5 were surpassed by            for further research, (ii) to validate the multi-sensor fusion
                                        our top reÔ¨Åned model in experiments using our fresh endo-         sensing of escalating cognitive load, (iii) to perform multi-
                                        scope dataset.","With the exception of YOLOv3-SPP,                teamwork, economy, and clinical outcomes.","modal situation awareness for automated surgical check-
                                                                                                          listing, and (iv) to study the feasibility of continual learning
                                        1.",2022-09-12 07:17:40+00:00,Situation Awareness for Automated Surgical Check-listing in AI-Assisted Operating Room,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tochukwu Onyeogulu'), arxiv.Result.Author('Amirul Islam'), arxiv.Result.Author('Salman Khan'), arxiv.Result.Author('Izzeddin Teeti'), arxiv.Result.Author('Fabio Cuzzolin')]","Nowadays, there are more surgical procedures that are being performed using
minimally invasive surgery (MIS). This is due to its many benefits, such as
minimal post-operative problems, less bleeding, minor scarring, and a speedy
recovery. However, the MIS's constrained field of view, small operating room,
and indirect viewing of the operating scene could lead to surgical tools
colliding and potentially harming human organs or tissues. Therefore, MIS
problems can be considerably reduced, and surgical procedure accuracy and
success rates can be increased by using an endoscopic video feed to detect and
monitor surgical instruments in real-time. In this paper, a set of improvements
made to the YOLOV5 object detector to enhance the detection of surgical
instruments was investigated, analyzed, and evaluated. In doing this, we
performed performance-based ablation studies, explored the impact of altering
the YOLOv5 model's backbone, neck, and anchor structural elements, and
annotated a unique endoscope dataset. Additionally, we compared the
effectiveness of our ablation investigations with that of four additional SOTA
object detectors (YOLOv7, YOLOR, Scaled-YOLOv4 and YOLOv3-SPP). Except for
YOLOv3-SPP, which had the same model performance of 98.3% in mAP and a similar
inference speed, all of our benchmark models, including the original YOLOv5,
were surpassed by our top refined model in experiments using our fresh
endoscope dataset.",0.051890858,-0.049298406,-0.19096321,A
11054,"Introduction                                                   modalities as a sandpit for further research; (ii) to validate
                                                                                                          the multi-sensor fusion sensing of escalating cognitive load;
                                           According to a report by the World Health Organisation         (iii) to perform multi-modal situation awareness for auto-
                                        (WHO), 25% of surgical procedures performed out on pa-            mated surgical check-listing; and (iv) to study the feasibility
                                        tients worldwide lead to post-surgical complications [21].","To achieve this aim, MAESTRO objectives are (i)
                                                                                                          to deploy and test a platform integrating multiple sensing
                                        1.","of continual learning for adapting to new surgical teams and
                                        Seven million people in this population have serious post-        procedures, as the pillar of an AI-assisted operating room in
                                        surgical complications, of which 14% of these patients die        the future.",2022-09-12 07:17:40+00:00,Situation Awareness for Automated Surgical Check-listing in AI-Assisted Operating Room,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tochukwu Onyeogulu'), arxiv.Result.Author('Salman Khan'), arxiv.Result.Author('Izzeddin Teeti'), arxiv.Result.Author('Amirul Islam'), arxiv.Result.Author('Kaizhe Jin'), arxiv.Result.Author('Adrian Rubio-Solis'), arxiv.Result.Author('Ravi Naik'), arxiv.Result.Author('George Mylonas'), arxiv.Result.Author('Fabio Cuzzolin')]","Nowadays, there are more surgical procedures that are being performed using
minimally invasive surgery (MIS). This is due to its many benefits, such as
minimal post-operative problems, less bleeding, minor scarring, and a speedy
recovery. However, the MIS's constrained field of view, small operating room,
and indirect viewing of the operating scene could lead to surgical tools
colliding and potentially harming human organs or tissues. Therefore, MIS
problems can be considerably reduced, and surgical procedure accuracy and
success rates can be increased by using an endoscopic video feed to detect and
monitor surgical instruments in real-time. In this paper, a set of improvements
made to the YOLOV5 object detector to enhance the detection of surgical
instruments was investigated, analyzed, and evaluated. In doing this, we
performed performance-based ablation studies, explored the impact of altering
the YOLOv5 model's backbone, neck, and anchor structural elements, and
annotated a unique endoscope dataset. Additionally, we compared the
effectiveness of our ablation investigations with that of four additional SOTA
object detectors (YOLOv7, YOLOR, Scaled-YOLOv4 and YOLOv3-SPP). Except for
YOLOv3-SPP, which had the same model performance of 98.3% in mAP and a similar
inference speed, all of our benchmark models, including the original YOLOv5,
were surpassed by our top refined model in experiments using our fresh
endoscope dataset.",0.05678145,0.012064597,-0.30133644,A
11061,"tendency of softmax output on samples in the infected la-
                                                                                                     bels for detection, we further study the inÔ¨Çuence of sample
   Multiple triggers within a single image.","Since we utilize the
worse when trigger transparency is high (0.95) or low (0.7).",We also con-                                             numbers in each class.,2022-09-12 13:37:06+00:00,Universal Backdoor Attacks Detection via Adaptive Adversarial Probe,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Yuhang Wang'), arxiv.Result.Author('Huafeng Shi'), arxiv.Result.Author('Rui Min'), arxiv.Result.Author('Ruijia Wu'), arxiv.Result.Author('Siyuan Liang'), arxiv.Result.Author('Yichao Wu'), arxiv.Result.Author('Ding Liang'), arxiv.Result.Author('Aishan Liu')]","Extensive evidence has demonstrated that deep neural networks (DNNs) are
vulnerable to backdoor attacks, which motivates the development of backdoor
attacks detection. Most detection methods are designed to verify whether a
model is infected with presumed types of backdoor attacks, yet the adversary is
likely to generate diverse backdoor attacks in practice that are unforeseen to
defenders, which challenge current detection strategies. In this paper, we
focus on this more challenging scenario and propose a universal backdoor
attacks detection method named Adaptive Adversarial Probe (A2P). Specifically,
we posit that the challenge of universal backdoor attacks detection lies in the
fact that different backdoor attacks often exhibit diverse characteristics in
trigger patterns (i.e., sizes and transparencies). Therefore, our A2P adopts a
global-to-local probing framework, which adversarially probes images with
adaptive regions/budgets to fit various backdoor triggers of different
sizes/transparencies. Regarding the probing region, we propose the
attention-guided region generation strategy that generates region proposals
with different sizes/locations based on the attention of the target model,
since trigger regions often manifest higher model activation. Considering the
attack budget, we introduce the box-to-sparsity scheduling that iteratively
increases the perturbation budget from box to sparse constraint, so that we
could better activate different latent backdoors with different transparencies.
Extensive experiments on multiple datasets (CIFAR-10, GTSRB, Tiny-ImageNet)
demonstrate that our method outperforms state-of-the-art baselines by large
margins (+12%).",0.077351965,-0.040615752,0.06975191,C
11140,Copyrights for components of this work owned by others than ACM           still plenty of room for further research.,"for profit or commercial advantage and that copies bear this notice and the full citation    However, the study of this task is still in its infancy and there is
                                        on the first page.",must be honored.,2022-09-13 07:21:21+00:00,Look Before You Leap: Improving Text-based Person Retrieval by Learning A Consistent Cross-modal Common Manifold,cs.CV,"['cs.CV', 'cs.IR', 'cs.MM']","[arxiv.Result.Author('Zijie Wang'), arxiv.Result.Author('Aichun Zhu'), arxiv.Result.Author('Jingyi Xue'), arxiv.Result.Author('Xili Wan'), arxiv.Result.Author('Chao Liu'), arxiv.Result.Author('Tian Wang'), arxiv.Result.Author('Yifeng Li')]","The core problem of text-based person retrieval is how to bridge the
heterogeneous gap between multi-modal data. Many previous approaches contrive
to learning a latent common manifold mapping paradigm following a
\textbf{cross-modal distribution consensus prediction (CDCP)} manner. When
mapping features from distribution of one certain modality into the common
manifold, feature distribution of the opposite modality is completely
invisible. That is to say, how to achieve a cross-modal distribution consensus
so as to embed and align the multi-modal features in a constructed cross-modal
common manifold all depends on the experience of the model itself, instead of
the actual situation. With such methods, it is inevitable that the multi-modal
data can not be well aligned in the common manifold, which finally leads to a
sub-optimal retrieval performance. To overcome this \textbf{CDCP dilemma}, we
propose a novel algorithm termed LBUL to learn a Consistent Cross-modal Common
Manifold (C$^{3}$M) for text-based person retrieval. The core idea of our
method, just as a Chinese saying goes, is to `\textit{san si er hou xing}',
namely, to \textbf{Look Before yoU Leap (LBUL)}. The common manifold mapping
mechanism of LBUL contains a looking step and a leaping step. Compared to
CDCP-based methods, LBUL considers distribution characteristics of both the
visual and textual modalities before embedding data from one certain modality
into C$^{3}$M to achieve a more solid cross-modal distribution consensus, and
hence achieve a superior retrieval accuracy. We evaluate our proposed method on
two text-based person retrieval datasets CUHK-PEDES and RSTPReid. Experimental
results demonstrate that the proposed LBUL outperforms previous methods and
achieves the state-of-the-art performance.",0.23127341,-0.009527672,-0.17573012,A
11168,"When facing with processing a large
                                                                          number of semantic object measurements in a short period,
                                                                          effective Ô¨Åltering and association of semantic information are
                                                                          still worth further research.","In summary, the development of semantic vSLAM has
                                                                          received much attention in recent years, but many solutions
                                                                          are limited to speciÔ¨Åc scenarios and face many challenges for
                                                                          practical applications.","JOURNAL OF LATEX CLASS FILES, VOL.",2022-09-14 05:45:26+00:00,Semantic Visual Simultaneous Localization and Mapping: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kaiqi Chen'), arxiv.Result.Author('Jianhua Zhang'), arxiv.Result.Author('Jialing Liu'), arxiv.Result.Author('Qiyi Tong'), arxiv.Result.Author('Ruyu Liu'), arxiv.Result.Author('Shengyong Chen')]","Visual Simultaneous Localization and Mapping (vSLAM) has achieved great
progress in the computer vision and robotics communities, and has been
successfully used in many fields such as autonomous robot navigation and AR/VR.
However, vSLAM cannot achieve good localization in dynamic and complex
environments. Numerous publications have reported that, by combining with the
semantic information with vSLAM, the semantic vSLAM systems have the capability
of solving the above problems in recent years. Nevertheless, there is no
comprehensive survey about semantic vSLAM. To fill the gap, this paper first
reviews the development of semantic vSLAM, explicitly focusing on its strengths
and differences. Secondly, we explore three main issues of semantic vSLAM: the
extraction and association of semantic information, the application of semantic
information, and the advantages of semantic vSLAM. Then, we collect and analyze
the current state-of-the-art SLAM datasets which have been widely used in
semantic vSLAM systems. Finally, we discuss future directions that will provide
a blueprint for the future development of semantic vSLAM.",-0.022562215,0.0015987623,-0.22436383,B
11174,"Therefore, the            Extensive experiments have been carried out to provide
lack of quantitative analysis makes the SeLo task impossible       massive benchmarks for further research.","the recall metrics of the retrieval models and compared SeLo
performance only from a visual perspective.","The rest of the paper
to decouple from retrieval, which greatly hinders the develop-     is organized as follows: Section II brieÔ¨Çy summarizes related
ment of this task.",2022-09-14 09:39:03+00:00,Learning to Evaluate Performance of Multi-modal Semantic Localization,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Zhiqiang Yuan'), arxiv.Result.Author('Wenkai Zhang'), arxiv.Result.Author('Chongyang Li'), arxiv.Result.Author('Zhaoying Pan'), arxiv.Result.Author('Yongqiang Mao'), arxiv.Result.Author('Jialiang Chen'), arxiv.Result.Author('Shouke Li'), arxiv.Result.Author('Hongqi Wang'), arxiv.Result.Author('Xian Sun')]","Semantic localization (SeLo) refers to the task of obtaining the most
relevant locations in large-scale remote sensing (RS) images using semantic
information such as text. As an emerging task based on cross-modal retrieval,
SeLo achieves semantic-level retrieval with only caption-level annotation,
which demonstrates its great potential in unifying downstream tasks. Although
SeLo has been carried out successively, but there is currently no work has
systematically explores and analyzes this urgent direction. In this paper, we
thoroughly study this field and provide a complete benchmark in terms of
metrics and testdata to advance the SeLo task. Firstly, based on the
characteristics of this task, we propose multiple discriminative evaluation
metrics to quantify the performance of the SeLo task. The devised significant
area proportion, attention shift distance, and discrete attention distance are
utilized to evaluate the generated SeLo map from pixel-level and region-level.
Next, to provide standard evaluation data for the SeLo task, we contribute a
diverse, multi-semantic, multi-objective Semantic Localization Testset
(AIR-SLT). AIR-SLT consists of 22 large-scale RS images and 59 test cases with
different semantics, which aims to provide a comprehensive evaluations for
retrieval models. Finally, we analyze the SeLo performance of RS cross-modal
retrieval models in detail, explore the impact of different variables on this
task, and provide a complete benchmark for the SeLo task. We have also
established a new paradigm for RS referring expression comprehension, and
demonstrated the great advantage of SeLo in semantics through combining it with
tasks such as detection and road extraction. The proposed evaluation metrics,
semantic localization testsets, and corresponding scripts have been open to
access at github.com/xiaoyuan1996/SemanticLocalizationMetrics .",0.114104204,-0.11638694,-0.25438398,A
11175,"In such cases, it is particularly important to   massive benchmarks for further research.","lack of quantitative analysis makes the SeLo task impossible
to decouple from retrieval, which greatly hinders the develop-         Greatness experiments have been carried out to provide
ment of this task.","The rest of the paper
conduct a reasonable analysis of the task and propose a set of      is organized as follows: Section II brieÔ¨Çy summarizes related
discriminative evaluation metrics to quantify the performance       works involved with SeLo.",2022-09-14 09:39:03+00:00,Learning to Evaluate Performance of Multi-modal Semantic Localization,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Zhiqiang Yuan'), arxiv.Result.Author('Wenkai Zhang'), arxiv.Result.Author('Chongyang Li'), arxiv.Result.Author('Zhaoying Pan'), arxiv.Result.Author('Yongqiang Mao'), arxiv.Result.Author('Jialiang Chen'), arxiv.Result.Author('Shouke Li'), arxiv.Result.Author('Hongqi Wang'), arxiv.Result.Author('Xian Sun')]","Semantic localization (SeLo) refers to the task of obtaining the most
relevant locations in large-scale remote sensing (RS) images using semantic
information such as text. As an emerging task based on cross-modal retrieval,
SeLo achieves semantic-level retrieval with only caption-level annotation,
which demonstrates its great potential in unifying downstream tasks. Although
SeLo has been carried out successively, but there is currently no work has
systematically explores and analyzes this urgent direction. In this paper, we
thoroughly study this field and provide a complete benchmark in terms of
metrics and testdata to advance the SeLo task. Firstly, based on the
characteristics of this task, we propose multiple discriminative evaluation
metrics to quantify the performance of the SeLo task. The devised significant
area proportion, attention shift distance, and discrete attention distance are
utilized to evaluate the generated SeLo map from pixel-level and region-level.
Next, to provide standard evaluation data for the SeLo task, we contribute a
diverse, multi-semantic, multi-objective Semantic Localization Testset
(AIR-SLT). AIR-SLT consists of 22 large-scale RS images and 59 test cases with
different semantics, which aims to provide a comprehensive evaluations for
retrieval models. Finally, we analyze the SeLo performance of RS cross-modal
retrieval models in detail, explore the impact of different variables on this
task, and provide a complete benchmark for the SeLo task. We have also
established a new paradigm for RS referring expression comprehension, and
demonstrated the great advantage of SeLo in semantics through combining it with
tasks such as detection and road extraction. The proposed evaluation metrics,
semantic localization testsets, and corresponding scripts have been open to
access at github.com/xiaoyuan1996/SemanticLocalizationMetrics .",0.2679193,-0.18622056,-0.26196706,A
11176,"We will carry out further research on this scheme           [6] Xie, J., Hou, X., Ye, K., & Shen, L. (2022).",poor.,"Cross
      in the follow-up, hoping to make this scheme feasible          Language Image Matching for Weakly Supervised Semantic
      with the development in the Ô¨Åeld.",2022-09-14 09:39:03+00:00,Learning to Evaluate Performance of Multi-modal Semantic Localization,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Zhiqiang Yuan'), arxiv.Result.Author('Wenkai Zhang'), arxiv.Result.Author('Chongyang Li'), arxiv.Result.Author('Zhaoying Pan'), arxiv.Result.Author('Yongqiang Mao'), arxiv.Result.Author('Jialiang Chen'), arxiv.Result.Author('Shouke Li'), arxiv.Result.Author('Hongqi Wang'), arxiv.Result.Author('Xian Sun')]","Semantic localization (SeLo) refers to the task of obtaining the most
relevant locations in large-scale remote sensing (RS) images using semantic
information such as text. As an emerging task based on cross-modal retrieval,
SeLo achieves semantic-level retrieval with only caption-level annotation,
which demonstrates its great potential in unifying downstream tasks. Although
SeLo has been carried out successively, but there is currently no work has
systematically explores and analyzes this urgent direction. In this paper, we
thoroughly study this field and provide a complete benchmark in terms of
metrics and testdata to advance the SeLo task. Firstly, based on the
characteristics of this task, we propose multiple discriminative evaluation
metrics to quantify the performance of the SeLo task. The devised significant
area proportion, attention shift distance, and discrete attention distance are
utilized to evaluate the generated SeLo map from pixel-level and region-level.
Next, to provide standard evaluation data for the SeLo task, we contribute a
diverse, multi-semantic, multi-objective Semantic Localization Testset
(AIR-SLT). AIR-SLT consists of 22 large-scale RS images and 59 test cases with
different semantics, which aims to provide a comprehensive evaluations for
retrieval models. Finally, we analyze the SeLo performance of RS cross-modal
retrieval models in detail, explore the impact of different variables on this
task, and provide a complete benchmark for the SeLo task. We have also
established a new paradigm for RS referring expression comprehension, and
demonstrated the great advantage of SeLo in semantics through combining it with
tasks such as detection and road extraction. The proposed evaluation metrics,
semantic localization testsets, and corresponding scripts have been open to
access at github.com/xiaoyuan1996/SemanticLocalizationMetrics .",-0.16612369,-0.19351432,-0.109581046,C
11177,"In such cases, it is particularly important to   massive benchmarks for further research.","lack of quantitative analysis makes the SeLo task impossible
to decouple from retrieval, which greatly hinders the develop-         Greatness experiments have been carried out to provide
ment of this task.","The rest of the paper
conduct a reasonable analysis of the task and propose a set of      is organized as follows: Section II brieÔ¨Çy summarizes related
discriminative evaluation metrics to quantify the performance       works involved with SeLo.",2022-09-14 09:39:03+00:00,Learning to Evaluate Performance of Multi-modal Semantic Localization,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Zhiqiang Yuan'), arxiv.Result.Author('Wenkai Zhang'), arxiv.Result.Author('Chongyang Li'), arxiv.Result.Author('Zhaoying Pan'), arxiv.Result.Author('Yongqiang Mao'), arxiv.Result.Author('Jialiang Chen'), arxiv.Result.Author('Shouke Li'), arxiv.Result.Author('Hongqi Wang'), arxiv.Result.Author('Xian Sun')]","Semantic localization (SeLo) refers to the task of obtaining the most
relevant locations in large-scale remote sensing (RS) images using semantic
information such as text. As an emerging task based on cross-modal retrieval,
SeLo achieves semantic-level retrieval with only caption-level annotation,
which demonstrates its great potential in unifying downstream tasks. Although
SeLo has been carried out successively, but there is currently no work has
systematically explores and analyzes this urgent direction. In this paper, we
thoroughly study this field and provide a complete benchmark in terms of
metrics and testdata to advance the SeLo task. Firstly, based on the
characteristics of this task, we propose multiple discriminative evaluation
metrics to quantify the performance of the SeLo task. The devised significant
area proportion, attention shift distance, and discrete attention distance are
utilized to evaluate the generated SeLo map from pixel-level and region-level.
Next, to provide standard evaluation data for the SeLo task, we contribute a
diverse, multi-semantic, multi-objective Semantic Localization Testset
(AIR-SLT). AIR-SLT consists of 22 large-scale RS images and 59 test cases with
different semantics, which aims to provide a comprehensive evaluations for
retrieval models. Finally, we analyze the SeLo performance of RS cross-modal
retrieval models in detail, explore the impact of different variables on this
task, and provide a complete benchmark for the SeLo task. We have also
established a new paradigm for RS referring expression comprehension, and
demonstrated the great advantage of SeLo in semantics through combining it with
tasks such as detection and road extraction. The proposed evaluation metrics,
semantic localization testsets, and corresponding scripts have been open to
access at github.com/xiaoyuan1996/SemanticLocalizationMetrics .",0.2679193,-0.18622056,-0.26196706,A
11178,"We will carry out further research on this scheme           [6] Xie, J., Hou, X., Ye, K., & Shen, L. (2022).",poor.,"Cross
      in the follow-up, hoping to make this scheme feasible          Language Image Matching for Weakly Supervised Semantic
      with the development in the Ô¨Åeld.",2022-09-14 09:39:03+00:00,Learning to Evaluate Performance of Multi-modal Semantic Localization,cs.CV,"['cs.CV', 'cs.MM']","[arxiv.Result.Author('Zhiqiang Yuan'), arxiv.Result.Author('Wenkai Zhang'), arxiv.Result.Author('Chongyang Li'), arxiv.Result.Author('Zhaoying Pan'), arxiv.Result.Author('Yongqiang Mao'), arxiv.Result.Author('Jialiang Chen'), arxiv.Result.Author('Shouke Li'), arxiv.Result.Author('Hongqi Wang'), arxiv.Result.Author('Xian Sun')]","Semantic localization (SeLo) refers to the task of obtaining the most
relevant locations in large-scale remote sensing (RS) images using semantic
information such as text. As an emerging task based on cross-modal retrieval,
SeLo achieves semantic-level retrieval with only caption-level annotation,
which demonstrates its great potential in unifying downstream tasks. Although
SeLo has been carried out successively, but there is currently no work has
systematically explores and analyzes this urgent direction. In this paper, we
thoroughly study this field and provide a complete benchmark in terms of
metrics and testdata to advance the SeLo task. Firstly, based on the
characteristics of this task, we propose multiple discriminative evaluation
metrics to quantify the performance of the SeLo task. The devised significant
area proportion, attention shift distance, and discrete attention distance are
utilized to evaluate the generated SeLo map from pixel-level and region-level.
Next, to provide standard evaluation data for the SeLo task, we contribute a
diverse, multi-semantic, multi-objective Semantic Localization Testset
(AIR-SLT). AIR-SLT consists of 22 large-scale RS images and 59 test cases with
different semantics, which aims to provide a comprehensive evaluations for
retrieval models. Finally, we analyze the SeLo performance of RS cross-modal
retrieval models in detail, explore the impact of different variables on this
task, and provide a complete benchmark for the SeLo task. We have also
established a new paradigm for RS referring expression comprehension, and
demonstrated the great advantage of SeLo in semantics through combining it with
tasks such as detection and road extraction. The proposed evaluation metrics,
semantic localization testsets, and corresponding scripts have been open to
access at github.com/xiaoyuan1996/SemanticLocalizationMetrics .",-0.16612369,-0.19351432,-0.109581046,C
11214,"We hope our approach can inspire further research about new algorithms,
theoretical analyses and applications.","Besides, our work is compatible with existing appealing technologies such as contrastive learning [27]
and active learning [58].",Potential negative impact.,2022-09-15 01:39:46+00:00,Learning from Future: A Novel Self-Training Framework for Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ye Du'), arxiv.Result.Author('Yujun Shen'), arxiv.Result.Author('Haochen Wang'), arxiv.Result.Author('Jingjing Fei'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Liwei Wu'), arxiv.Result.Author('Rui Zhao'), arxiv.Result.Author('Zehua Fu'), arxiv.Result.Author('Qingjie Liu')]","Self-training has shown great potential in semi-supervised learning. Its core
idea is to use the model learned on labeled data to generate pseudo-labels for
unlabeled samples, and in turn teach itself. To obtain valid supervision,
active attempts typically employ a momentum teacher for pseudo-label prediction
yet observe the confirmation bias issue, where the incorrect predictions may
provide wrong supervision signals and get accumulated in the training process.
The primary cause of such a drawback is that the prevailing self-training
framework acts as guiding the current state with previous knowledge, because
the teacher is updated with the past student only. To alleviate this problem,
we propose a novel self-training strategy, which allows the model to learn from
the future. Concretely, at each training step, we first virtually optimize the
student (i.e., caching the gradients without applying them to the model
weights), then update the teacher with the virtual future student, and finally
ask the teacher to produce pseudo-labels for the current student as the
guidance. In this way, we manage to improve the quality of pseudo-labels and
thus boost the performance. We also develop two variants of our
future-self-training (FST) framework through peeping at the future both deeply
(FST-D) and widely (FST-W). Taking the tasks of unsupervised domain adaptive
semantic segmentation and semi-supervised semantic segmentation as the
instances, we experimentally demonstrate the effectiveness and superiority of
our approach under a wide range of settings. Code will be made publicly
available.",0.056846473,-0.18275739,0.022505598,C
11215,"We hope our approach can inspire further research about new algorithms,
theoretical analyses and applications.","Besides, our work is compatible with existing appealing technologies such as contrastive learning [27]
and active learning [58].",Potential negative impact.,2022-09-15 01:39:46+00:00,Learning from Future: A Novel Self-Training Framework for Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ye Du'), arxiv.Result.Author('Yujun Shen'), arxiv.Result.Author('Haochen Wang'), arxiv.Result.Author('Jingjing Fei'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Liwei Wu'), arxiv.Result.Author('Rui Zhao'), arxiv.Result.Author('Zehua Fu'), arxiv.Result.Author('Qingjie Liu')]","Self-training has shown great potential in semi-supervised learning. Its core
idea is to use the model learned on labeled data to generate pseudo-labels for
unlabeled samples, and in turn teach itself. To obtain valid supervision,
active attempts typically employ a momentum teacher for pseudo-label prediction
yet observe the confirmation bias issue, where the incorrect predictions may
provide wrong supervision signals and get accumulated in the training process.
The primary cause of such a drawback is that the prevailing self-training
framework acts as guiding the current state with previous knowledge, because
the teacher is updated with the past student only. To alleviate this problem,
we propose a novel self-training strategy, which allows the model to learn from
the future. Concretely, at each training step, we first virtually optimize the
student (i.e., caching the gradients without applying them to the model
weights), then update the teacher with the virtual future student, and finally
ask the teacher to produce pseudo-labels for the current student as the
guidance. In this way, we manage to improve the quality of pseudo-labels and
thus boost the performance. We also develop two variants of our
future-self-training (FST) framework through peeping at the future both deeply
(FST-D) and widely (FST-W). Taking the tasks of unsupervised domain adaptive
semantic segmentation and semi-supervised semantic segmentation as the
instances, we experimentally demonstrate the effectiveness and superiority of
our approach under a wide range of settings. Code will be made publicly
available.",0.056846473,-0.18275739,0.022505598,C
11223,"First, our method                     soon for further research.","The dataset will be publicly online available
some potential limitations in our method.","cannot work directly on raw event signals, which have to
be converted into frames Ô¨Årst.",2022-09-15 04:08:18+00:00,A Temporal Densely Connected Recurrent Network for Event-based Human Pose Estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhanpeng Shao'), arxiv.Result.Author('Wen Zhou'), arxiv.Result.Author('Wuzhen Wang'), arxiv.Result.Author('Jianyu Yang'), arxiv.Result.Author('Youfu Li')]","Event camera is an emerging bio-inspired vision sensors that report per-pixel
brightness changes asynchronously. It holds noticeable advantage of high
dynamic range, high speed response, and low power budget that enable it to best
capture local motions in uncontrolled environments. This motivates us to unlock
the potential of event cameras for human pose estimation, as the human pose
estimation with event cameras is rarely explored. Due to the novel paradigm
shift from conventional frame-based cameras, however, event signals in a time
interval contain very limited information, as event cameras can only capture
the moving body parts and ignores those static body parts, resulting in some
parts to be incomplete or even disappeared in the time interval. This paper
proposes a novel densely connected recurrent architecture to address the
problem of incomplete information. By this recurrent architecture, we can
explicitly model not only the sequential but also non-sequential geometric
consistency across time steps to accumulate information from previous frames to
recover the entire human bodies, achieving a stable and accurate human pose
estimation from event data. Moreover, to better evaluate our model, we collect
a large scale multimodal event-based dataset that comes with human pose
annotations, which is by far the most challenging one to the best of our
knowledge. The experimental results on two public datasets and our own dataset
demonstrate the effectiveness and strength of our approach. Code can be
available online for facilitating the future research.",0.03704765,0.19846901,-0.14657667,B
11228,"To perform a comprehensive evaluation and
                                                 facilitate further research, we construct a medical vision-and-language
                                                 benchmark including three tasks.","Third, we develop diÔ¨Äerent designs for vision
                                                 and language decoders (i.e., a Transformer for vision and a multi-layer
                                                 perceptron for language).","Experimental results demonstrate the
                                                 eÔ¨Äectiveness of our approach, where state-of-the-art results are achieved
                                                 on all downstream tasks.",2022-09-15 07:26:43+00:00,Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Zhihong Chen'), arxiv.Result.Author('Yuhao Du'), arxiv.Result.Author('Jinpeng Hu'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Xiang Wan'), arxiv.Result.Author('Tsung-Hui Chang')]","Medical vision-and-language pre-training provides a feasible solution to
extract effective vision-and-language representations from medical images and
texts. However, few studies have been dedicated to this field to facilitate
medical vision-and-language understanding. In this paper, we propose a
self-supervised learning paradigm with multi-modal masked autoencoders
(M$^3$AE), which learn cross-modal domain knowledge by reconstructing missing
pixels and tokens from randomly masked images and texts. There are three key
designs to make this simple approach work. First, considering the different
information densities of vision and language, we adopt different masking ratios
for the input image and text, where a considerably larger masking ratio is used
for images. Second, we use visual and textual features from different layers to
perform the reconstruction to deal with different levels of abstraction in
visual and language. Third, we develop different designs for vision and
language decoders (i.e., a Transformer for vision and a multi-layer perceptron
for language). To perform a comprehensive evaluation and facilitate further
research, we construct a medical vision-and-language benchmark including three
tasks. Experimental results demonstrate the effectiveness of our approach,
where state-of-the-art results are achieved on all downstream tasks. Besides,
we conduct further analysis to better verify the effectiveness of different
components of our approach and various settings of pre-training. The source
code is available at~\url{https://github.com/zhjohnchan/M3AE}.",-0.10879493,-0.1222451,-0.028813612,C
11229,"To verify the
eÔ¨Äectiveness of our approach and facilitate further research, we construct a
medical vision-and-language understanding benchmark including three tasks (i.e.,
Med-VQA, medical image-text classiÔ¨Åcation, and medical image-text retrieval).","We perform the pre-training on two large-scale
medical image-text datasets, i.e., ROCO [20] and MedICaT [23].","Multi-Modal Masked Autoencoders                                                                                  3

                                Vision Encoder                                    Multi-modal Fusion Module                              Vision Decoder                                        From k-th layer
                                                                        √óNv
                         ùëã!",2022-09-15 07:26:43+00:00,Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Zhihong Chen'), arxiv.Result.Author('Yuhao Du'), arxiv.Result.Author('Jinpeng Hu'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Xiang Wan'), arxiv.Result.Author('Tsung-Hui Chang')]","Medical vision-and-language pre-training provides a feasible solution to
extract effective vision-and-language representations from medical images and
texts. However, few studies have been dedicated to this field to facilitate
medical vision-and-language understanding. In this paper, we propose a
self-supervised learning paradigm with multi-modal masked autoencoders
(M$^3$AE), which learn cross-modal domain knowledge by reconstructing missing
pixels and tokens from randomly masked images and texts. There are three key
designs to make this simple approach work. First, considering the different
information densities of vision and language, we adopt different masking ratios
for the input image and text, where a considerably larger masking ratio is used
for images. Second, we use visual and textual features from different layers to
perform the reconstruction to deal with different levels of abstraction in
visual and language. Third, we develop different designs for vision and
language decoders (i.e., a Transformer for vision and a multi-layer perceptron
for language). To perform a comprehensive evaluation and facilitate further
research, we construct a medical vision-and-language benchmark including three
tasks. Experimental results demonstrate the effectiveness of our approach,
where state-of-the-art results are achieved on all downstream tasks. Besides,
we conduct further analysis to better verify the effectiveness of different
components of our approach and various settings of pre-training. The source
code is available at~\url{https://github.com/zhjohnchan/M3AE}.",-0.1828458,-0.2046338,0.034675047,C
11250,"In Table 15, we further study the inÔ¨Çuence of the output dimensionality of DNC.","However,
owing to the distance-/similarity-based nature, DNC has the Ô¨Çexibility to handle any output dimen-
sionality.","As seen,
when setting the Ô¨Ånal output dimensionality as 1280, we can achieve 76.61% top-1 acc., which
is higher than the initial 2048 dimension conÔ¨Åguration, i.e., 76.49%.",2022-09-15 15:47:31+00:00,Visual Recognition with Deep Nearest Centroids,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('Cheng Han'), arxiv.Result.Author('Tianfei Zhou'), arxiv.Result.Author('Dongfang Liu')]","We devise deep nearest centroids (DNC), a conceptually elegant yet
surprisingly effective network for large-scale visual recognition, by
revisiting Nearest Centroids, one of the most classic and simple classifiers.
Current deep models learn the classifier in a fully parametric manner, ignoring
the latent data structure and lacking simplicity and explainability. DNC
instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids
of training samples to describe class distributions and clearly explains the
classification as the proximity of test data and the class sub-centroids in the
feature space. Due to the distance-based nature, the network output
dimensionality is flexible, and all the learnable parameters are only for data
embedding. That means all the knowledge learnt for ImageNet classification can
be completely transferred for pixel recognition learning, under the
""pre-training and fine-tuning"" paradigm. Apart from its nested simplicity and
intuitive decision-making mechanism, DNC can even possess ad-hoc explainability
when the sub-centroids are selected as actual training images that humans can
view and inspect. Compared with parametric counterparts, DNC performs better on
image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition
(ADE20K, Cityscapes), with improved transparency and fewer learnable
parameters, using various network architectures (ResNet, Swin) and segmentation
models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights
into related fields.",0.2999332,-0.05508465,0.17468596,A
11253,"Finally,
                                                                                                  we further study one consequence of adversarial training by         Fig.","Additionally, investigating the reasons for        Im-1K            C100 Flowers C10 Cal101
                                                                                                  the robustness of our models, we show that it is easier to
                                                                                                  generate strong attacks during training when using our recipe                                         Fine-tuning
                                                                                                  and that this leads to better robustness at test time.",1: A light recipe is better!,2022-09-15 16:00:04+00:00,A Light Recipe to Train Robust Vision Transformers,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Edoardo Debenedetti'), arxiv.Result.Author('Vikash Sehwag'), arxiv.Result.Author('Prateek Mittal')]","In this paper, we ask whether Vision Transformers (ViTs) can serve as an
underlying architecture for improving the adversarial robustness of machine
learning models against evasion attacks. While earlier works have focused on
improving Convolutional Neural Networks, we show that also ViTs are highly
suitable for adversarial training to achieve competitive performance. We
achieve this objective using a custom adversarial training recipe, discovered
using rigorous ablation studies on a subset of the ImageNet dataset. The
canonical training recipe for ViTs recommends strong data augmentation, in part
to compensate for the lack of vision inductive bias of attention modules, when
compared to convolutions. We show that this recipe achieves suboptimal
performance when used for adversarial training. In contrast, we find that
omitting all heavy data augmentation, and adding some additional bag-of-tricks
($\varepsilon$-warmup and larger weight decay), significantly boosts the
performance of robust ViTs. We show that our recipe generalizes to different
classes of ViT architectures and large-scale models on full ImageNet-1k.
Additionally, investigating the reasons for the robustness of our models, we
show that it is easier to generate strong attacks during training when using
our recipe and that this leads to better robustness at test time. Finally, we
further study one consequence of adversarial training by proposing a way to
quantify the semantic nature of adversarial perturbations and highlight its
correlation with the robustness of the model. Overall, we recommend that the
community should avoid translating the canonical training recipes in ViTs to
robust training and rethink common training choices in the context of
adversarial training.",-0.004147191,-0.27263895,0.14275905,C
11262,"4.4 ANALYSES

Results of Other Tiny Architectures We further study DG for two other tiny neural networks,
i.e., MobileNetV2-Tiny and MCUNet, which are speciÔ¨Åcally designed for MCUs (Lin et al., 2020).","The results are reported in Table 3 where the Ô¨Åndings are
similar to those on DOSCO-2k.","As shown in Table 1, these two architectures are half the size of MobileNetV3-Small, meaning

    4CutMix+Mixup is implemented as Œ± Mixup(x) + (1 ‚àí Œ±) CutMix(x) where Œ± is sampled from a Beta
distribution.",2022-09-15 17:59:31+00:00,On-Device Domain Generalization,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Kaiyang Zhou'), arxiv.Result.Author('Yuanhan Zhang'), arxiv.Result.Author('Yuhang Zang'), arxiv.Result.Author('Jingkang Yang'), arxiv.Result.Author('Chen Change Loy'), arxiv.Result.Author('Ziwei Liu')]","We present a systematic study of domain generalization (DG) for tiny neural
networks, a problem that is critical to on-device machine learning applications
but has been overlooked in the literature where research has been focused on
large models only. Tiny neural networks have much fewer parameters and lower
complexity, and thus should not be trained the same way as their large
counterparts for DG applications. We find that knowledge distillation is a
strong candidate for solving the problem: it outperforms state-of-the-art DG
methods that were developed using large models with a large margin. Moreover,
we observe that the teacher-student performance gap on test data with domain
shift is bigger than that on in-distribution data. To improve DG for tiny
neural networks without increasing the deployment cost, we propose a simple
idea called out-of-distribution knowledge distillation (OKD), which aims to
teach the student how the teacher handles (synthetic) out-of-distribution data
and is proved to be a promising framework for solving the problem. We also
contribute a scalable method of creating DG datasets, called DOmain Shift in
COntext (DOSCO), which can be applied to broad data at scale without much human
effort. Code and models are released at
\url{https://github.com/KaiyangZhou/on-device-dg}.",0.25102565,-0.11247383,0.40048862,A
11291,"However, our spatial

                                                           9
feature learning is only built upon contrastive learning, while generative methods recently show
impressive performance for self-supervised learning, which is worthy of further research.",Extensive experiments on a variety of downstream tasks validate our method.,"We hope
our method can provide a new perspective for self-supervised video correspondence learning.",2022-09-16 08:10:17+00:00,Spatial-then-Temporal Self-Supervised Learning for Video Correspondence,cs.CV,['cs.CV'],"[arxiv.Result.Author('Rui Li'), arxiv.Result.Author('Dong Liu')]","Learning temporal correspondence from unlabeled videos is of vital importance
in computer vision, and has been tackled by different kinds of self-supervised
pretext tasks. For the self-supervised learning, recent studies suggest using
large-scale video datasets despite the training cost. We propose a
spatial-then-temporal pretext task to address the training data cost problem.
The task consists of two steps. First, we use contrastive learning from
unlabeled still image data to obtain appearance-sensitive features. Then we
switch to unlabeled video data and learn motion-sensitive features by
reconstructing frames. In the second step, we propose a global correlation
distillation loss to retain the appearance sensitivity learned in the first
step, as well as a local correlation distillation loss in a pyramid structure
to combat temporal discontinuity. Experimental results demonstrate that our
method surpasses the state-of-the-art self-supervised methods on a series of
correspondence-based tasks. The conducted ablation studies verify the
effectiveness of the proposed two-step task and loss functions.",-0.2986722,-0.09505129,0.008616807,C
11314,"‚Ä¢ We open-source our oyster generation model and dataset
                                        A set of environmental variables including water salinity,                     associated with this work to accelerate further research.","proposed to standardize monitoring metrics, units, and
                                        performance criteria for the evaluation of the oyster reefs.","temperature, and dissolved oxygen are being monitored to
                                        determine the well-being of oyster habits.",2022-09-16 21:35:45+00:00,OysterNet: Enhanced Oyster Detection Using Simulation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaomin Lin'), arxiv.Result.Author('Nitin J. Sanket'), arxiv.Result.Author('Nare Karapetyan'), arxiv.Result.Author('Yiannis Aloimonos')]","Oysters play a pivotal role in the bay living ecosystem and are considered
the living filters for the ocean. In recent years, oyster reefs have undergone
major devastation caused by commercial over-harvesting, requiring preservation
to maintain ecological balance. The foundation of this preservation is to
estimate the oyster density which requires accurate oyster detection. However,
systems for accurate oyster detection require large datasets obtaining which is
an expensive and labor-intensive task in underwater environments. To this end,
we present a novel method to mathematically model oysters and render images of
oysters in simulation to boost the detection performance with minimal real
data. Utilizing our synthetic data along with real data for oyster detection,
we obtain up to 35.1% boost in performance as compared to using only real data
with our OysterNet network. We also improve the state-of-the-art by 12.7%. This
shows that using underlying geometrical properties of objects can help to
enhance recognition task accuracy on limited datasets successfully and we hope
more researchers adopt such a strategy for hard-to-obtain datasets.",0.16244707,0.14728269,-0.10344428,A
11325,"Since the slope of logarithmic and power series models are asymptotic, the
polynomial model is used for further study.","Plot of decline in classification accuracies against the attack step
                  sizes for the networks along with suitable polynomial fits

         The r2 values for logarithmic, polynomial and power series fit are comparable and
acceptable due to the presence of positive correlation between the model-fit generated and
available data curve.","It is important to note that the polynomial fits
proposed are only applicable upto the minimum of the polynomial equations after which the
value of the function is increasing.",2022-09-17 06:25:14+00:00,A study on the deviations in performance of FNNs and CNNs in the realm of grayscale adversarial images,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Durga Shree Nagabushanam'), arxiv.Result.Author('Steve Mathew'), arxiv.Result.Author('Chiranji Lal Chowdhary')]","Neural Networks are prone to having lesser accuracy in the classification of
images with noise perturbation. Convolutional Neural Networks, CNNs are known
for their unparalleled accuracy in the classification of benign images. But our
study shows that they are extremely vulnerable to noise addition while
Feed-forward Neural Networks, FNNs show very less correspondence with noise
perturbation, maintaining their accuracy almost undisturbed. FNNs are observed
to be better at classifying noise-intensive, single-channeled images that are
just sheer noise to human vision. In our study, we have used the hand-written
digits dataset, MNIST with the following architectures: FNNs with 1 and 2
hidden layers and CNNs with 3, 4, 6 and 8 convolutions and analyzed their
accuracies. FNNs stand out to show that irrespective of the intensity of noise,
they have a classification accuracy of more than 85%. In our analysis of CNNs
with this data, the deceleration of classification accuracy of CNN with 8
convolutions was half of that of the rest of the CNNs. Correlation analysis and
mathematical modelling of the accuracy trends act as roadmaps to these
conclusions.",0.39315897,-0.040950656,0.12725501,A
11329,"Therefore, we propose
preserve the most informative motion features and improve       an RGB-Event dataset, called DSEC-MOD, to encourage
the scale invariance within a local region, yielding a simple   further research on moving object detection.","Therefore, we apply different sizes of max-pooling to     motions is crucial for driving safety.","yet efÔ¨Åcient way to aggregate rich temporal events in a         Event Processing: Event cameras have recently drawn great
coarse-to-Ô¨Åne manner.",2022-09-17 12:59:08+00:00,RGB-Event Fusion for Moving Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Zhuyun Zhou'), arxiv.Result.Author('Zongwei Wu'), arxiv.Result.Author('R√©mi Boutteau'), arxiv.Result.Author('Fan Yang'), arxiv.Result.Author('C√©dric Demonceaux'), arxiv.Result.Author('Dominique Ginhac')]","Moving Object Detection (MOD) is a critical vision task for successfully
achieving safe autonomous driving. Despite plausible results of deep learning
methods, most existing approaches are only frame-based and may fail to reach
reasonable performance when dealing with dynamic traffic participants. Recent
advances in sensor technologies, especially the Event camera, can naturally
complement the conventional camera approach to better model moving objects.
However, event-based works often adopt a pre-defined time window for event
representation, and simply integrate it to estimate image intensities from
events, neglecting much of the rich temporal information from the available
asynchronous events. Therefore, from a new perspective, we propose RENet, a
novel RGB-Event fusion Network, that jointly exploits the two complementary
modalities to achieve more robust MOD under challenging scenarios for
autonomous driving. Specifically, we first design a temporal multi-scale
aggregation module to fully leverage event frames from both the RGB exposure
time and larger intervals. Then we introduce a bi-directional fusion module to
attentively calibrate and fuse multi-modal features. To evaluate the
performance of our network, we carefully select and annotate a sub-MOD dataset
from the commonly used DSEC dataset. Extensive experiments demonstrate that our
proposed method performs significantly better than the state-of-the-art
RGB-Event fusion alternatives.",-0.3580926,0.20134579,-0.044281926,B
11330,"Finally, we propose a
positive due to the inferior lightening condition, our RGB-          novel DSEC-MOD dataset to encourage further research on
Event network can reason about more robust detection.","Moreover, when RGB baseline results in faux            help of different forms of attention.",More           RGB-Event fusion for moving object detection.,2022-09-17 12:59:08+00:00,RGB-Event Fusion for Moving Object Detection in Autonomous Driving,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Zhuyun Zhou'), arxiv.Result.Author('Zongwei Wu'), arxiv.Result.Author('R√©mi Boutteau'), arxiv.Result.Author('Fan Yang'), arxiv.Result.Author('C√©dric Demonceaux'), arxiv.Result.Author('Dominique Ginhac')]","Moving Object Detection (MOD) is a critical vision task for successfully
achieving safe autonomous driving. Despite plausible results of deep learning
methods, most existing approaches are only frame-based and may fail to reach
reasonable performance when dealing with dynamic traffic participants. Recent
advances in sensor technologies, especially the Event camera, can naturally
complement the conventional camera approach to better model moving objects.
However, event-based works often adopt a pre-defined time window for event
representation, and simply integrate it to estimate image intensities from
events, neglecting much of the rich temporal information from the available
asynchronous events. Therefore, from a new perspective, we propose RENet, a
novel RGB-Event fusion Network, that jointly exploits the two complementary
modalities to achieve more robust MOD under challenging scenarios for
autonomous driving. Specifically, we first design a temporal multi-scale
aggregation module to fully leverage event frames from both the RGB exposure
time and larger intervals. Then we introduce a bi-directional fusion module to
attentively calibrate and fuse multi-modal features. To evaluate the
performance of our network, we carefully select and annotate a sub-MOD dataset
from the commonly used DSEC dataset. Extensive experiments demonstrate that our
proposed method performs significantly better than the state-of-the-art
RGB-Event fusion alternatives.",-0.25634024,0.056241646,0.039327644,B
11338,"Our proposed ALRS
                                               is worthy of further study, especially
required to train the model to the set min-    its hyper-parameter called lr decay
                                               rate.",The number of epochs           LR Decay Rate.,"Intuitively, lr decay rate repre-
imum learning rate 1e-4.",2022-09-18 04:40:32+00:00,Bootstrap Generalization Ability from Loss Landscape Perspective,cs.CV,['cs.CV'],"[arxiv.Result.Author('Huanran Chen'), arxiv.Result.Author('Shitong Shao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Zirui Shang'), arxiv.Result.Author('Jin Chen'), arxiv.Result.Author('Xiaofeng Ji'), arxiv.Result.Author('Xinxiao Wu')]","Domain generalization aims to learn a model that can generalize well on the
unseen test dataset, i.e., out-of-distribution data, which has different
distribution from the training dataset. To address domain generalization in
computer vision, we introduce the loss landscape theory into this field.
Specifically, we bootstrap the generalization ability of the deep learning
model from the loss landscape perspective in four aspects, including backbone,
regularization, training paradigm, and learning rate. We verify the proposed
theory on the NICO++, PACS, and VLCS datasets by doing extensive ablation
studies as well as visualizations. In addition, we apply this theory in the
ECCV 2022 NICO Challenge1 and achieve the 3rd place without using any domain
invariant methods.",0.2702157,-0.070200674,0.20136346,A
11339,"To further study the multimodal fusion      the previous works commonly using LSTM [4], [7], [8].","VINet concatenates the two kind of feature     To effectively fuse visual and inertial features, we introduce
directly according to channel dimensions, and process features    an attention mechanism with memory unit assistance , unlike
with recurrent model.","problem for VIO, [18] proposes a selective fusion mechanism
for learning based VIO to fuse features from two sides.",2022-09-18 07:05:36+00:00,EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Zheming Tu'), arxiv.Result.Author('Changhao Chen'), arxiv.Result.Author('Xianfei Pan'), arxiv.Result.Author('Ruochen Liu'), arxiv.Result.Author('Jiarui Cui'), arxiv.Result.Author('Jun Mao')]","Accurate and robust localization is a fundamental need for mobile agents.
Visual-inertial odometry (VIO) algorithms exploit the information from camera
and inertial sensors to estimate position and translation. Recent deep learning
based VIO models attract attentions as they provide pose information in a
data-driven way, without the need of designing hand-crafted algorithms.
Existing learning based VIO models rely on recurrent models to fuse multimodal
data and process sensor signal, which are hard to train and not efficient
enough. We propose a novel learning based VIO framework with external memory
attention that effectively and efficiently combines visual and inertial
features for states estimation. Our proposed model is able to estimate pose
accurately and robustly, even in challenging scenarios, e.g., on overcast days
and water-filled ground , which are difficult for traditional VIO algorithms to
extract visual features. Experiments validate that it outperforms both
traditional and learning based VIO baselines in different scenes.",-0.08414057,-0.13906503,-0.036317136,C
11351,"Despite the fact that this study proposes an
advanced and efficient streetlight management mechanism with improved precision even when using
inexpensive components, further research in this area is still needed to support the validity of its real-
world applications.","In compare to the
conventional or the existing intelligent solutions, our proposed model is more affordable to set up,
demands less energy, and doesn't require any physical sensors.","Furthermore, the existing framework for controlling streetlights may be intelligently
tailored for both controlling streetlights and conducting street surveillances.",2022-09-18 19:12:43+00:00,Energy Efficient Automatic Streetlight Controlling System using Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Md Sakib Ullah Sourav'), arxiv.Result.Author('Huidong Wang')]","This study aims to develop a novel streetlight management system powered by
computer vision technology mounted with the close circuit television (CCTV)
camera that allows the light emitting diode (LED) streetlight to automatically
light up with proper brightness by recognizing the presence of pedestrians or
vehicles and reversely dimming the streetlight in their absence by semantic
image segmentation from video.",-0.038935535,0.27818573,-0.12771337,B
11355,"It requires further research to solve this
                                                                                problem.","> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <                                                   8

                                                                                and low threshold values during sampling even if the proposed
                                                                                network is well trained.","In contrast, in the DQN method, it is structurally
                                                                                possible to configure the high threshold always to have a bigger
                                                                                value than the low threshold.",2022-09-19 01:15:32+00:00,An Adaptive Threshold for the Canny Edge Detection with Actor-Critic Algorithm,cs.CV,['cs.CV'],"[arxiv.Result.Author('Keong-Hun Choi'), arxiv.Result.Author('Jong-Eun Ha')]","Visual surveillance aims to perform robust foreground object detection
regardless of the time and place. Object detection shows good results using
only spatial information, but foreground object detection in visual
surveillance requires proper temporal and spatial information processing. In
deep learning-based foreground object detection algorithms, the detection
ability is superior to classical background subtraction (BGS) algorithms in an
environment similar to training. However, the performance is lower than that of
the classical BGS algorithm in the environment different from training. This
paper proposes a spatio-temporal fusion network (STFN) that could extract
temporal and spatial information using a temporal network and a spatial
network. We suggest a method using a semi-foreground map for stable training of
the proposed STFN. The proposed algorithm shows excellent performance in an
environment different from training, and we show it through experiments with
various public datasets. Also, STFN can generate a compliant background image
in a semi-supervised method, and it can operate in real-time on a desktop with
GPU. The proposed method shows 11.28% and 18.33% higher FM than the latest deep
learning method in the LASIESTA and SBI dataset, respectively.",0.2654271,-0.091706835,0.21913877,A
11364,It is convenient to record and process the data digitally       to further study the difference of the drawings.,"It would be insightful to devise some high-level
   We implemented the data collection user interface on tablets with      metrics, e.g., line parallelism, roundness, curvature monotonicity,
styluses.","while this setting may potentially incorporate the inaccuracies for
non-digital drawing tool users to reflect their real drawing skills.",2022-09-19 06:52:18+00:00,DifferSketching: How Differently Do People Sketch 3D Objects?,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Chufeng Xiao'), arxiv.Result.Author('Wanchao Su'), arxiv.Result.Author('Jing Liao'), arxiv.Result.Author('Zhouhui Lian'), arxiv.Result.Author('Yi-Zhe Song'), arxiv.Result.Author('Hongbo Fu')]","Multiple sketch datasets have been proposed to understand how people draw 3D
objects. However, such datasets are often of small scale and cover a small set
of objects or categories. In addition, these datasets contain freehand sketches
mostly from expert users, making it difficult to compare the drawings by expert
and novice users, while such comparisons are critical in informing more
effective sketch-based interfaces for either user groups. These observations
motivate us to analyze how differently people with and without adequate drawing
skills sketch 3D objects. We invited 70 novice users and 38 expert users to
sketch 136 3D objects, which were presented as 362 images rendered from
multiple views. This leads to a new dataset of 3,620 freehand multi-view
sketches, which are registered with their corresponding 3D objects under
certain views. Our dataset is an order of magnitude larger than the existing
datasets. We analyze the collected data at three levels, i.e., sketch-level,
stroke-level, and pixel-level, under both spatial and temporal characteristics,
and within and across groups of creators. We found that the drawings by
professionals and novices show significant differences at stroke-level, both
intrinsically and extrinsically. We demonstrate the usefulness of our dataset
in two applications: (i) freehand-style sketch synthesis, and (ii) posing it as
a potential benchmark for sketch-based 3D reconstruction. Our dataset and code
are available at https://chufengxiao.github.io/DifferSketching/.",0.070993505,0.19578066,-0.16582723,A
11388,"We show that a
better pose-shape disentanglement indeed seems to improve performance, yet
further research in this direction is required.","We demonstrate how such models can be used for inferring
actions that move an agent towards a preferred observation.","In the remainder of the paper we
Ô¨Årst give an overview on related work, after which we present our method.",2022-09-16 12:53:49+00:00,Disentangling Shape and Pose for Object-Centric Deep Active Inference Models,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Stefano Ferraro'), arxiv.Result.Author('Toon Van de Maele'), arxiv.Result.Author('Pietro Mazzaglia'), arxiv.Result.Author('Tim Verbelen'), arxiv.Result.Author('Bart Dhoedt')]","Active inference is a first principles approach for understanding the brain
in particular, and sentient agents in general, with the single imperative of
minimizing free energy. As such, it provides a computational account for
modelling artificial intelligent agents, by defining the agent's generative
model and inferring the model parameters, actions and hidden state beliefs.
However, the exact specification of the generative model and the hidden state
space structure is left to the experimenter, whose design choices influence the
resulting behaviour of the agent. Recently, deep learning methods have been
proposed to learn a hidden state space structure purely from data, alleviating
the experimenter from this tedious design task, but resulting in an entangled,
non-interpreteable state space. In this paper, we hypothesize that such a
learnt, entangled state space does not necessarily yield the best model in
terms of free energy, and that enforcing different factors in the state space
can yield a lower model complexity. In particular, we consider the problem of
3D object representation, and focus on different instances of the ShapeNet
dataset. We propose a model that factorizes object shape, pose and category,
while still learning a representation for each factor using a deep neural
network. We show that models, with best disentanglement properties, perform
best when adopted by an active agent in reaching preferred observations.",-0.13295896,0.122041136,-0.15012002,B
11389,"[25] have attributed GAN generated images by em-
ploying seed reconstruction based method wherein the la-                               The core contribution of this work is introducing the
tent variable utilized in generator is explored by utilizing                        concept of deepfake phylogeny to instigate further research
gradient descent so as to Ô¨Ånd the closest match.","Proposed Deepfake Phylogeny Dataset
et al.","However,                           with this problem statement, as illustrated in Figure 2.",2022-09-19 15:30:33+00:00,DeePhy: On Deepfake Phylogeny,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kartik Narayan'), arxiv.Result.Author('Harsh Agarwal'), arxiv.Result.Author('Kartik Thakral'), arxiv.Result.Author('Surbhi Mittal'), arxiv.Result.Author('Mayank Vatsa'), arxiv.Result.Author('Richa Singh')]","Deepfake refers to tailored and synthetically generated videos which are now
prevalent and spreading on a large scale, threatening the trustworthiness of
the information available online. While existing datasets contain different
kinds of deepfakes which vary in their generation technique, they do not
consider progression of deepfakes in a ""phylogenetic"" manner. It is possible
that an existing deepfake face is swapped with another face. This process of
face swapping can be performed multiple times and the resultant deepfake can be
evolved to confuse the deepfake detection algorithms. Further, many databases
do not provide the employed generative model as target labels. Model
attribution helps in enhancing the explainability of the detection results by
providing information on the generative model employed. In order to enable the
research community to address these questions, this paper proposes DeePhy, a
novel Deepfake Phylogeny dataset which consists of 5040 deepfake videos
generated using three different generation techniques. There are 840 videos of
one-time swapped deepfakes, 2520 videos of two-times swapped deepfakes and 1680
videos of three-times swapped deepfakes. With over 30 GBs in size, the database
is prepared in over 1100 hours using 18 GPUs of 1,352 GB cumulative memory. We
also present the benchmark on DeePhy dataset using six deepfake detection
algorithms. The results highlight the need to evolve the research of model
attribution of deepfakes and generalize the process over a variety of deepfake
generation techniques. The database is available at:
http://iab-rubric.org/deephy-database",-0.046300247,-0.09466834,0.2616459,C
11413,We further study the individual and combined effects of SMAD and OmniDebias.,"For OmniDebias, since additional web media are used for
joint training, all three fairness metrics are improved (lower ‚Üí better).","Extensive experiments are conducted with both 2D and 3D baselines: models are trained
Mitigating Representation Bias in Action Recognition: Algorithms and Benchmarks  11

        Setting    GYM-1shot  GYM-5shot  HMDB51   UCF101  Diving48
   w/o.",2022-09-20 00:30:35+00:00,Mitigating Representation Bias in Action Recognition: Algorithms and Benchmarks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haodong Duan'), arxiv.Result.Author('Yue Zhao'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Yuanjun Xiong'), arxiv.Result.Author('Dahua Lin')]","Deep learning models have achieved excellent recognition results on
large-scale video benchmarks. However, they perform poorly when applied to
videos with rare scenes or objects, primarily due to the bias of existing video
datasets. We tackle this problem from two different angles: algorithm and
dataset. From the perspective of algorithms, we propose Spatial-aware
Multi-Aspect Debiasing (SMAD), which incorporates both explicit debiasing with
multi-aspect adversarial training and implicit debiasing with the spatial
actionness reweighting module, to learn a more generic representation invariant
to non-action aspects. To neutralize the intrinsic dataset bias, we propose
OmniDebias to leverage web data for joint training selectively, which can
achieve higher performance with far fewer web data. To verify the
effectiveness, we establish evaluation protocols and perform extensive
experiments on both re-distributed splits of existing datasets and a new
evaluation dataset focusing on the action with rare scenes. We also show that
the debiased representation can generalize better when transferred to other
datasets and tasks.",-0.06123536,-0.15753056,-0.13724792,C
11418,"To further study the generalization ability of our method, following GLIP [30], we also evaluate the
averaged AP on other 13 downstream detection datasets published on RoboÔ¨Çow1.","We do not focus the performance on COCO [33] since it only
contains 80 common categories that are fully covered by the training dataset Objects365 [45], which
may not sufÔ¨Åcient to reÔ¨Çect generalization ability of a model in the open-domain detection setting.","4.1 Open-world Detection Results

We train our DetCLIP with two backbones, i.e., swin-T [35] and swin-L.",2022-09-20 02:01:01+00:00,DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lewei Yao'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Youpeng Wen'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Dan Xu'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Hang Xu')]","Open-world object detection, as a more general and challenging goal, aims to
recognize and localize objects described by arbitrary category names. The
recent work GLIP formulates this problem as a grounding problem by
concatenating all category names of detection datasets into sentences, which
leads to inefficient interaction between category names. This paper presents
DetCLIP, a paralleled visual-concept pre-training method for open-world
detection by resorting to knowledge enrichment from a designed concept
dictionary. To achieve better learning efficiency, we propose a novel
paralleled concept formulation that extracts concepts separately to better
utilize heterogeneous datasets (i.e., detection, grounding, and image-text
pairs) for training. We further design a concept dictionary~(with descriptions)
from various online sources and detection datasets to provide prior knowledge
for each concept. By enriching the concepts with their descriptions, we
explicitly build the relationships among various concepts to facilitate the
open-domain learning. The proposed concept dictionary is further used to
provide sufficient negative concepts for the construction of the word-region
alignment loss\, and to complete labels for objects with missing descriptions
in captions of image-text pair data. The proposed framework demonstrates strong
zero-shot detection performances, e.g., on the LVIS dataset, our DetCLIP-T
outperforms GLIP-T by 9.9% mAP and obtains a 13.5% improvement on rare
categories compared to the fully-supervised model with the same backbone as
ours.",-0.16766748,-0.13265361,0.028044889,C
11419,"To further study the generalization ability of our method, following GLIP [30], we also evaluate the
averaged AP on other 13 downstream detection datasets published on RoboÔ¨Çow1.","We do not focus the performance on COCO [33] since it only
contains 80 common categories that are fully covered by the training dataset Objects365 [45], which

                             7
may not sufÔ¨Åcient to reÔ¨Çect generalization ability of a model in the open-domain detection setting.","4.1 Open-world Detection Results

We train our DetCLIP with two backbones, i.e., swin-T [35] and swin-L.",2022-09-20 02:01:01+00:00,DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lewei Yao'), arxiv.Result.Author('Jianhua Han'), arxiv.Result.Author('Youpeng Wen'), arxiv.Result.Author('Xiaodan Liang'), arxiv.Result.Author('Dan Xu'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Zhenguo Li'), arxiv.Result.Author('Chunjing Xu'), arxiv.Result.Author('Hang Xu')]","Open-world object detection, as a more general and challenging goal, aims to
recognize and localize objects described by arbitrary category names. The
recent work GLIP formulates this problem as a grounding problem by
concatenating all category names of detection datasets into sentences, which
leads to inefficient interaction between category names. This paper presents
DetCLIP, a paralleled visual-concept pre-training method for open-world
detection by resorting to knowledge enrichment from a designed concept
dictionary. To achieve better learning efficiency, we propose a novel
paralleled concept formulation that extracts concepts separately to better
utilize heterogeneous datasets (i.e., detection, grounding, and image-text
pairs) for training. We further design a concept dictionary~(with descriptions)
from various online sources and detection datasets to provide prior knowledge
for each concept. By enriching the concepts with their descriptions, we
explicitly build the relationships among various concepts to facilitate the
open-domain learning. The proposed concept dictionary is further used to
provide sufficient negative concepts for the construction of the word-region
alignment loss\, and to complete labels for objects with missing descriptions
in captions of image-text pair data. The proposed framework demonstrates strong
zero-shot detection performances, e.g., on the LVIS dataset, our DetCLIP-T
outperforms GLIP-T by 9.9% mAP and obtains a 13.5% improvement on rare
categories compared to the fully-supervised model with the same backbone as
ours.",-0.17041317,-0.13032618,0.02126323,C
11426,"For optimization, AdamW [77]
                                                                                We further study the design of DU and DU-Net.","For this dataset, four NVIDIA Tesla V100                  D. Design Analysis
GPUs are used for training.","All experi-
                                                                             ments are conducted on the S3DIS dataset and the experiment
                                                                             settings are the same as described in Section IV-C.

                                                                                   a) Ablation Study on DU: The results are shown in
                                                                             Table III.",2022-09-20 05:52:28+00:00,Interpretable Edge Enhancement and Suppression Learning for 3D Point Cloud Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haoyi Xiu'), arxiv.Result.Author('Xin Liu'), arxiv.Result.Author('Weimin Wang'), arxiv.Result.Author('Kyoung-Sook Kim'), arxiv.Result.Author('Takayuki Shinohara'), arxiv.Result.Author('Qiong Chang'), arxiv.Result.Author('Masashi Matsuoka')]","3D point clouds can flexibly represent continuous surfaces and can be used
for various applications; however, the lack of structural information makes
point cloud recognition challenging. Recent edge-aware methods mainly use edge
information as an extra feature that describes local structures to facilitate
learning. Although these methods show that incorporating edges into the network
design is beneficial, they generally lack interpretability, making users wonder
how exactly edges help. To shed light on this issue, in this study, we propose
the Diffusion Unit (DU) that handles edges in an interpretable manner while
providing decent improvement. Our method is interpretable in three ways. First,
we theoretically show that DU learns to perform task-beneficial edge
enhancement and suppression. Second, we experimentally observe and verify the
edge enhancement and suppression behavior. Third, we empirically demonstrate
that this behavior contributes to performance improvement. Extensive
experiments performed on challenging benchmarks verify the superiority of DU in
terms of both interpretability and performance gain. Specifically, our method
achieves state-of-the-art performance in object part segmentation using
ShapeNet part and scene segmentation using S3DIS. Our source code will be
released at https://github.com/martianxiu/DiffusionUnit.",0.17374504,0.08772764,0.2267767,A
11428,"In Figure 3, we further study the transferability of attacks

                                                           8
60.00  Pascal-VOC ‚Üí Pascal-VOC                                                     50.00  Pascal-VOC ‚Üí ImageNet                    Table 7: Pascal-VOC ‚Üí MS-COCO Object Detection task
50.00                                                                              40.00
40.00                            VGG16                                             30.00                           VGG16                        Method FRCN RNet DETR D2ETR Average
30.00                             Res50                                            20.00                            Res50            f (¬∑)

Accuracy %                                                                                                                                    No Attack 0.582 0.554 0.607 0.633 0.594
                                                                       Accuracy %                                                             GAP [10] 0.424 0.404 0.360 0.410 0.399
                                                                                                                                              CDA [11] 0.276 0.250 0.208 0.244 0.244
           Limg  Limg + Ltxt  L                                                           Limg     Limg + Ltxt  L                              TAP [12] 0.384 0.340 0.275 0.320 0.329VGG19
                                                                                                                                               BIA [13] 0.347 0.318 0.253 0.281 0.299
                 Attacks                                                                           Attacks
                                                                                                                                                 GAMA 0.234 0.207 0.117 0.122 0.170
Figure 4: Ablation analysis of loss objec-                                                                                                    GAP [10] 0.389 0.362 0.363 0.408 0.380Res152
tive.",Performance on Type of Architectures.,"We analyze the contribution due to the                                                                                                  CDA [11] 0.305 0.274 0.256 0.281 0.279
introduction of each loss function Limg and                                                                                                    TAP [12] 0.400 0.348 0.288 0.350 0.346
Ltxt towards the Ô¨Ånal objective L, both in                                                                                                     BIA [13] 0.321 0.275 0.205 0.256 0.264
same distribution (left) and different distribu-
tion (right).",2022-09-20 06:40:54+00:00,GAMA: Generative Adversarial Multi-Object Scene Attacks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Abhishek Aich'), arxiv.Result.Author('Calvin Khang-Ta'), arxiv.Result.Author('Akash Gupta'), arxiv.Result.Author('Chengyu Song'), arxiv.Result.Author('Srikanth V. Krishnamurthy'), arxiv.Result.Author('M. Salman Asif'), arxiv.Result.Author('Amit K. Roy-Chowdhury')]","The majority of methods for crafting adversarial attacks have focused on
scenes with a single dominant object (e.g., images from ImageNet). On the other
hand, natural scenes include multiple dominant objects that are semantically
related. Thus, it is crucial to explore designing attack strategies that look
beyond learning on single-object scenes or attack single-object victim
classifiers. Due to their inherent property of strong transferability of
perturbations to unknown models, this paper presents the first approach of
using generative models for adversarial attacks on multi-object scenes. In
order to represent the relationships between different objects in the input
scene, we leverage upon the open-sourced pre-trained vision-language model CLIP
(Contrastive Language-Image Pre-training), with the motivation to exploit the
encoded semantics in the language space along with the visual space. We call
this attack approach Generative Adversarial Multi-object scene Attacks (GAMA).
GAMA demonstrates the utility of the CLIP model as an attacker's tool to train
formidable perturbation generators for multi-object scenes. Using the joint
image-text features to train the generator, we show that GAMA can craft potent
transferable perturbations in order to fool victim classifiers in various
attack settings. For example, GAMA triggers ~16% more misclassification than
state-of-the-art generative approaches in black-box settings where both the
classifier architecture and data distribution of the attacker are different
from the victim. Our code will be made publicly available soon.",0.20377639,0.08238658,0.17441048,A
11429,"In Figure 3, we further study the transferability of attacks
 depending on the type of victim architecture: standard which follow the standard modules like
 Residual blocks [80] to build the classiÔ¨Åer, and custom where the victim classiÔ¨Åer doesn‚Äôt adhere to a
 speciÔ¨Åc pattern of network modules.",Performance on Type of Architectures.,"In both cases, GAMA consistently maintains better attack rates
 than other attacks.",2022-09-20 06:40:54+00:00,GAMA: Generative Adversarial Multi-Object Scene Attacks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Abhishek Aich'), arxiv.Result.Author('Calvin-Khang Ta'), arxiv.Result.Author('Akash Gupta'), arxiv.Result.Author('Chengyu Song'), arxiv.Result.Author('Srikanth V. Krishnamurthy'), arxiv.Result.Author('M. Salman Asif'), arxiv.Result.Author('Amit K. Roy-Chowdhury')]","The majority of methods for crafting adversarial attacks have focused on
scenes with a single dominant object (e.g., images from ImageNet). On the other
hand, natural scenes include multiple dominant objects that are semantically
related. Thus, it is crucial to explore designing attack strategies that look
beyond learning on single-object scenes or attack single-object victim
classifiers. Due to their inherent property of strong transferability of
perturbations to unknown models, this paper presents the first approach of
using generative models for adversarial attacks on multi-object scenes. In
order to represent the relationships between different objects in the input
scene, we leverage upon the open-sourced pre-trained vision-language model CLIP
(Contrastive Language-Image Pre-training), with the motivation to exploit the
encoded semantics in the language space along with the visual space. We call
this attack approach Generative Adversarial Multi-object scene Attacks (GAMA).
GAMA demonstrates the utility of the CLIP model as an attacker's tool to train
formidable perturbation generators for multi-object scenes. Using the joint
image-text features to train the generator, we show that GAMA can craft potent
transferable perturbations in order to fool victim classifiers in various
attack settings. For example, GAMA triggers ~16% more misclassification than
state-of-the-art generative approaches in black-box settings where both the
classifier architecture and data distribution of the attacker are different
from the victim. Our code is available here:
https://abhishekaich27.github.io/gama.html",0.3161744,-0.028452106,0.13331604,A
11440,"Abstract                                     new state-of-the-art results on three regular RIS datasets
                                                                                                             and three R-RIS datasets, which serves as a new solid base-
                                           Referring Image Segmentation (RIS) aims to connect                line for further research.","A robust RIS model should output None for negative reference
                                        and correct masks for positive reference.","The project page is at https://
                                        image and language via outputting the corresponding ob-              lxtgh.github.io/project/robust_ref_seg/.",2022-09-20 08:48:26+00:00,Towards Robust Referring Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianzong Wu'), arxiv.Result.Author('Xiangtai Li'), arxiv.Result.Author('Xia Li'), arxiv.Result.Author('Henghui Ding'), arxiv.Result.Author('Yunhai Tong'), arxiv.Result.Author('Dacheng Tao')]","Referring Image Segmentation (RIS) aims to connect image and language via
outputting the corresponding object masks given a text description, which is a
fundamental vision-language task. Despite lots of works that have achieved
considerable progress for RIS, in this work, we explore an essential question,
""what if the description is wrong or misleading of the text description?"". We
term such a sentence as a negative sentence. However, we find that existing
works cannot handle such settings. To this end, we propose a novel formulation
of RIS, named Robust Referring Image Segmentation (R-RIS). It considers the
negative sentence inputs besides the regularly given text inputs. We present
three different datasets via augmenting the input negative sentences and a new
metric to unify both input types. Furthermore, we design a new
transformer-based model named RefSegformer, where we introduce a token-based
vision and language fusion module. Such module can be easily extended to our
R-RIS setting by adding extra blank tokens. Our proposed RefSegformer achieves
the new state-of-the-art results on three regular RIS datasets and three R-RIS
datasets, which serves as a new solid baseline for further research. The
project page is at \url{https://lxtgh.github.io/project/robust_ref_seg/}.",-0.11070123,0.0928897,0.0078428015,B
11441,"6 (b) and (c), we visualize the atten-                                                    achieves the new state-of-the-art results on both RIS and R-
tion maps between conditional/blank tokens and the vision                                                      RIS datasets, making it a new baseline for further research.",In Fig.,"feature in the MHCA in the decoder for both RIS and R-
RIS.",2022-09-20 08:48:26+00:00,Towards Robust Referring Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianzong Wu'), arxiv.Result.Author('Xiangtai Li'), arxiv.Result.Author('Xia Li'), arxiv.Result.Author('Henghui Ding'), arxiv.Result.Author('Yunhai Tong'), arxiv.Result.Author('Dacheng Tao')]","Referring Image Segmentation (RIS) aims to connect image and language via
outputting the corresponding object masks given a text description, which is a
fundamental vision-language task. Despite lots of works that have achieved
considerable progress for RIS, in this work, we explore an essential question,
""what if the description is wrong or misleading of the text description?"". We
term such a sentence as a negative sentence. However, we find that existing
works cannot handle such settings. To this end, we propose a novel formulation
of RIS, named Robust Referring Image Segmentation (R-RIS). It considers the
negative sentence inputs besides the regularly given text inputs. We present
three different datasets via augmenting the input negative sentences and a new
metric to unify both input types. Furthermore, we design a new
transformer-based model named RefSegformer, where we introduce a token-based
vision and language fusion module. Such module can be easily extended to our
R-RIS setting by adding extra blank tokens. Our proposed RefSegformer achieves
the new state-of-the-art results on three regular RIS datasets and three R-RIS
datasets, which serves as a new solid baseline for further research. The
project page is at \url{https://lxtgh.github.io/project/robust_ref_seg/}.",0.012788192,-0.014697414,0.09606092,C
11463,"Our proposed approach and dataset pave the way
the number of types of anomalies available for training an           for further research aimed at safe autonomous operation of
RNVP+OE model.","We want to explore the              types, spanning from sensor defects to high-level semantic
impact of the heterogeneity of the training set by varying           anomalies.","Using the complete training and testing               mobile robots in unstructured, unpredictable environments:
sets, for N = 1, 2, 3, 4, 6, 12 types of anomalies, we run the       we foresee applications to concrete robotics tasks (such as
following experiment 30 times: (1) we randomly pick a subset         avoiding anomalous terrain for legged robots), and extensions
of N anomaly types; (2) we train RNVP+OE exposing it only            of outlier exposure to other anomaly detection approach based
to the selected anomaly types; (3) we compute the AUC over
Fig.",2022-09-20 15:18:13+00:00,An Outlier Exposure Approach to Improve Visual Anomaly Detection Performance for Mobile Robots,cs.CV,"['cs.CV', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Dario Mantegazza'), arxiv.Result.Author('Alessandro Giusti'), arxiv.Result.Author('Luca Maria Gambardella'), arxiv.Result.Author('J√©r√¥me Guzzi')]","We consider the problem of building visual anomaly detection systems for
mobile robots. Standard anomaly detection models are trained using large
datasets composed only of non-anomalous data. However, in robotics
applications, it is often the case that (potentially very few) examples of
anomalies are available. We tackle the problem of exploiting these data to
improve the performance of a Real-NVP anomaly detection model, by minimizing,
jointly with the Real-NVP loss, an auxiliary outlier exposure margin loss. We
perform quantitative experiments on a novel dataset (which we publish as
supplementary material) designed for anomaly detection in an indoor patrolling
scenario. On a disjoint test set, our approach outperforms alternatives and
shows that exposing even a small number of anomalous frames yields significant
performance improvements.",-0.06581065,-0.01546097,-0.10774815,B
11489,"The FT-HID dataset is expected to fa-
                                                           cilitate further research in TPV and FPV based human
                                                           action recognition using individual modalities, multi-
                                                           modalities, and cross-modalities protocols that covers
                                                           a broad range of real applications.","ber of samples, distinct interaction categories, diverse
                                                           camera views, various environments, and a wide range
                                                           of participants.","References

                                                            1.",2022-09-21 07:24:15+00:00,FT-HID: A Large Scale RGB-D Dataset for First and Third Person Human Interaction Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zihui Guo'), arxiv.Result.Author('Yonghong Hou'), arxiv.Result.Author('Pichao Wang'), arxiv.Result.Author('Zhimin Gao'), arxiv.Result.Author('Mingliang Xu'), arxiv.Result.Author('Wanqing Li')]","Analysis of human interaction is one important research topic of human motion
analysis. It has been studied either using first person vision (FPV) or third
person vision (TPV). However, the joint learning of both types of vision has so
far attracted little attention. One of the reasons is the lack of suitable
datasets that cover both FPV and TPV. In addition, existing benchmark datasets
of either FPV or TPV have several limitations, including the limited number of
samples, participant subjects, interaction categories, and modalities. In this
work, we contribute a large-scale human interaction dataset, namely, FT-HID
dataset. FT-HID contains pair-aligned samples of first person and third person
visions. The dataset was collected from 109 distinct subjects and has more than
90K samples for three modalities. The dataset has been validated by using
several existing action recognition methods. In addition, we introduce a novel
multi-view interaction mechanism for skeleton sequences, and a joint learning
multi-stream framework for first person and third person visions. Both methods
yield promising results on the FT-HID dataset. It is expected that the
introduction of this vision-aligned large-scale dataset will promote the
development of both FPV and TPV, and their joint learning techniques for human
action analysis. The dataset and code are available at
\href{https://github.com/ENDLICHERE/FT-HID}{here}.",-0.19541734,0.085778534,-0.2102144,B
11570,"To promote further research in                      [19] A. Bertugli, S. Calderara, P. Coscia, L. Ballan, and R. Cucchiara, ‚ÄúAc-
                                                                                      vrnn: Attentive conditional-vrnn for multi-future trajectory prediction,‚Äù
Ô¨Årst-person view trajectory prediction, we release our dataset                        Computer Vision and Image Understanding, 2021.

and software tools to the public.","We argue that this is an
                                                                important direction to move toward in enabling robots to
navigate in the real world.","[20] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and
                                                                                      Y. Bengio, ‚ÄúA recurrent latent variable model for sequential data,‚Äù
                            REFERENCES                                                CoRR, 2015.",2022-09-22 20:14:43+00:00,T2FPV: Constructing High-Fidelity First-Person View Datasets From Real-World Pedestrian Trajectories,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Benjamin Stoler'), arxiv.Result.Author('Meghdeep Jana'), arxiv.Result.Author('Soonmin Hwang'), arxiv.Result.Author('Jean Oh')]","Predicting pedestrian motion is essential for developing socially-aware
robots that interact in a crowded environment. While the natural visual
perspective for a social interaction setting is an egocentric view, the
majority of existing work in trajectory prediction has been investigated purely
in the top-down trajectory space. To support first-person view trajectory
prediction research, we present T2FPV, a method for constructing high-fidelity
first-person view datasets given a real-world, top-down trajectory dataset; we
showcase our approach on the ETH/UCY pedestrian dataset to generate the
egocentric visual data of all interacting pedestrians. We report that the
bird's-eye view assumption used in the original ETH/UCY dataset, i.e., an agent
can observe everyone in the scene with perfect information, does not hold in
the first-person views; only a fraction of agents are fully visible during each
20-timestep scene used commonly in existing work. We evaluate existing
trajectory prediction approaches under varying levels of realistic perception
-- displacement errors suffer a 356% increase compared to the top-down, perfect
information setting. To promote research in first-person view trajectory
prediction, we release our T2FPV-ETH dataset and software tools.",-0.28385568,-0.09370965,-0.10866964,B
11572,"To facilitate further research, we release the code               for describing complex events.","This demon-                  in which the temporal information is limited due to relatively
                                        strates its effectiveness and good generalization capacity across               long satellite revisit periods, an overhead video is able to
                                        different recognition tasks (event classiÔ¨Åcation and human action               deliver more Ô¨Åne-grained temporal dynamics that are essential
                                        recognition).","Therefore, moving from image
                                        at https://gitlab.lrz.de/ai4eo/reasoning/futh-net.",2022-09-22 21:15:58+00:00,FuTH-Net: Fusing Temporal Relations and Holistic Features for Aerial Video Classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pu Jin'), arxiv.Result.Author('Lichao Mou'), arxiv.Result.Author('Yuansheng Hua'), arxiv.Result.Author('Gui-Song Xia'), arxiv.Result.Author('Xiao Xiang Zhu')]","Unmanned aerial vehicles (UAVs) are now widely applied to data acquisition
due to its low cost and fast mobility. With the increasing volume of aerial
videos, the demand for automatically parsing these videos is surging. To
achieve this, current researches mainly focus on extracting a holistic feature
with convolutions along both spatial and temporal dimensions. However, these
methods are limited by small temporal receptive fields and cannot adequately
capture long-term temporal dependencies which are important for describing
complicated dynamics. In this paper, we propose a novel deep neural network,
termed FuTH-Net, to model not only holistic features, but also temporal
relations for aerial video classification. Furthermore, the holistic features
are refined by the multi-scale temporal relations in a novel fusion module for
yielding more discriminative video representations. More specially, FuTH-Net
employs a two-pathway architecture: (1) a holistic representation pathway to
learn a general feature of both frame appearances and shortterm temporal
variations and (2) a temporal relation pathway to capture multi-scale temporal
relations across arbitrary frames, providing long-term temporal dependencies.
Afterwards, a novel fusion module is proposed to spatiotemporal integrate the
two features learned from the two pathways. Our model is evaluated on two
aerial video classification datasets, ERA and Drone-Action, and achieves the
state-of-the-art results. This demonstrates its effectiveness and good
generalization capacity across different recognition tasks (event
classification and human action recognition). To facilitate further research,
we release the code at https://gitlab.lrz.de/ai4eo/reasoning/futh-net.",-0.22611281,-0.028514247,-0.18158433,B
11582,"A num-
Ô¨Åeld, so that further research questions from the interven-          ber of datasets have been created for this purpose in recent
tion space can be explored.","Like-
wise, the aim of the presented methodology is to explore a           In contrast to the domain randomization approach is the
pipeline for the generation of synthetic data for the medical        photorealistic rendering of the scene and objects.","In addition to synthetic data,           years.",2022-09-23 09:36:23+00:00,Comparison of synthetic dataset generation methods for medical intervention rooms using medical clothing detection as an example,cs.CV,"['cs.CV', 'I.5; I.2']","[arxiv.Result.Author('Patrick Sch√ºlein'), arxiv.Result.Author('Hannah Teufel'), arxiv.Result.Author('Ronja Vorpahl'), arxiv.Result.Author('Indira Emter'), arxiv.Result.Author('Yannick Bukschat'), arxiv.Result.Author('Marcus Pfister'), arxiv.Result.Author('Anke Siebert'), arxiv.Result.Author('Nils Rathmann'), arxiv.Result.Author('Steffen Diehl'), arxiv.Result.Author('Marcus Vetter')]","The availability of real data from areas with high privacy requirements, such
as the medical intervention space, is low and the acquisition legally complex.
Therefore, this work presents a way to create a synthetic dataset for the
medical context, using medical clothing as an example. The goal is to close the
reality gap between the synthetic and real data. For this purpose, methods of
3D-scanned clothing and designed clothing are compared in a
Domain-Randomization and Structured-Domain-Randomization scenario using an
Unreal-Engine plugin or Unity. Additionally a Mixed-Reality dataset in front of
a greenscreen and a target domain dataset were used. Our experiments show, that
Structured-Domain-Randomization of designed clothing together with
Mixed-Reality data provide a baseline achieving 72.0% mAP on a test dataset of
the clinical target domain. When additionally using 15% of available target
domain train data, the gap towards 100% (660 images) target domain train data
could be nearly closed 80.05% mAP (81.95% mAP). Finally we show that when
additionally using 100% target domain train data the accuracy could be
increased to 83.35% mAP.",-0.13120455,0.0742078,0.11623913,C
11597,"We thus set out to further study the on-device
learning potential in the form of federated learning [61] with HART.","Our work has demonstrated that HART can indeed run on mobile devices, exhibiting superior performances with lower
cost as compared to other transformer-based approaches during inference.","Other studies have shown that it is indeed possible
to integrate transformers into a collaborative learning ecosystem [62, 63], and we would extend our future work, in
particular, to handle heterogeneity within a federated learning environment.",2022-09-22 09:42:08+00:00,Lightweight Transformers for Human Activity Recognition on Mobile Devices,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Sannara EK'), arxiv.Result.Author('Fran√ßois Portet'), arxiv.Result.Author('Philippe Lalanda')]","Human Activity Recognition (HAR) on mobile devices has shown to be achievable
with lightweight neural models learned from data generated by the user's
inertial measurement units (IMUs). Most approaches for instanced-based HAR have
used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), or a
combination of the two to achieve state-of-the-art results with real-time
performances. Recently, the Transformers architecture in the language
processing domain and then in the vision domain has pushed further the
state-of-the-art over classical architectures. However, such Transformers
architecture is heavyweight in computing resources, which is not well suited
for embedded applications of HAR that can be found in the pervasive computing
domain. In this study, we present Human Activity Recognition Transformer
(HART), a lightweight, sensor-wise transformer architecture that has been
specifically adapted to the domain of the IMUs embedded on mobile devices. Our
experiments on HAR tasks with several publicly available datasets show that
HART uses fewer FLoating-point Operations Per Second (FLOPS) and parameters
while outperforming current state-of-the-art results. Furthermore, we present
evaluations across various architectures on their performances in heterogeneous
environments and show that our models can better generalize on different
sensing devices or on-body positions.",0.051344916,-0.2004101,0.05042068,C
11643,"seed in order to compare different methods under consistent
                                                                    settings and facilitate further research.","For a fair comparison, we adopt
obtained by tuning hyperparameter is not simply substituted         their ofÔ¨Åcial implementation2 utilizing its default training
by increasing the capacity of CNN architecture.",4.2.,2022-09-26 08:16:31+00:00,Improving Multi-fidelity Optimization with a Recurring Learning Rate for Hyperparameter Tuning,cs.CV,"['cs.CV', 'cs.LG', 'math.OC']","[arxiv.Result.Author('HyunJae Lee'), arxiv.Result.Author('Gihyeon Lee'), arxiv.Result.Author('Junhwan Kim'), arxiv.Result.Author('Sungjun Cho'), arxiv.Result.Author('Dohyun Kim'), arxiv.Result.Author('Donggeun Yoo')]","Despite the evolution of Convolutional Neural Networks (CNNs), their
performance is surprisingly dependent on the choice of hyperparameters.
However, it remains challenging to efficiently explore large hyperparameter
search space due to the long training times of modern CNNs. Multi-fidelity
optimization enables the exploration of more hyperparameter configurations
given budget by early termination of unpromising configurations. However, it
often results in selecting a sub-optimal configuration as training with the
high-performing configuration typically converges slowly in an early phase. In
this paper, we propose Multi-fidelity Optimization with a Recurring Learning
rate (MORL) which incorporates CNNs' optimization process into multi-fidelity
optimization. MORL alleviates the problem of slow-starter and achieves a more
precise low-fidelity approximation. Our comprehensive experiments on general
image classification, transfer learning, and semi-supervised learning
demonstrate the effectiveness of MORL over other multi-fidelity optimization
methods such as Successive Halving Algorithm (SHA) and Hyperband. Furthermore,
it achieves significant performance improvements over hand-tuned hyperparameter
configuration within a practical budget.",0.027765276,-0.2783342,0.24806972,C
11660,"in the CR fusion: ‚ÄúCR(+C points)‚Äù outperforms ‚ÄúCR(+R
                                                                  points)‚Äù by up to 5% AP, and adding R points in addition to
   We further study the performance of LCR fusion with re-        C points does not improve fusion, as the similar performance
spect to the density of lidar points and the detection distance.","The LCR model is most robust against
weather conditions with ‚àí2.7% mRAPD, because radars are              From the Ô¨Ågure we can see the importance of C points
less affected by rainy weather than lidars or cameras.","between ‚ÄúCR(+C points)‚Äù and ‚ÄúCR(+C,R points)‚Äù illustrates.",2022-09-26 14:33:30+00:00,"DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars",cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Florian Drews'), arxiv.Result.Author('Di Feng'), arxiv.Result.Author('Florian Faion'), arxiv.Result.Author('Lars Rosenbaum'), arxiv.Result.Author('Michael Ulrich'), arxiv.Result.Author('Claudius Gl√§ser')]","We propose DeepFusion, a modular multi-modal architecture to fuse lidars,
cameras and radars in different combinations for 3D object detection.
Specialized feature extractors take advantage of each modality and can be
exchanged easily, making the approach simple and flexible. Extracted features
are transformed into bird's-eye-view as a common representation for fusion.
Spatial and semantic alignment is performed prior to fusing modalities in the
feature space. Finally, a detection head exploits rich multi-modal features for
improved 3D detection performance. Experimental results for lidar-camera,
lidar-camera-radar and camera-radar fusion show the flexibility and
effectiveness of our fusion approach. In the process, we study the largely
unexplored task of faraway car detection up to 225~meters, showing the benefits
of our lidar-camera fusion. Furthermore, we investigate the required density of
lidar points for 3D object detection and illustrate implications at the example
of robustness against adverse weather conditions. Moreover, ablation studies on
our camera-radar fusion highlight the importance of accurate depth estimation.",0.19176787,0.39883906,0.0219066,A
11661,"in the CR fusion: ‚ÄúCR(+C points)‚Äù outperforms ‚ÄúCR(+R
                                                                  points)‚Äù by up to 5% AP, and adding R points in addition to
   We further study the performance of LCR fusion with re-        C points does not improve fusion, as the similar performance
spect to the density of lidar points and the detection distance.","The LCR model is most robust against
weather conditions with ‚àí2.7% mRAPD, because radars are              From the Ô¨Ågure we can see the importance of C points
less affected by rainy weather than lidars or cameras.","between ‚ÄúCR(+C points)‚Äù and ‚ÄúCR(+C,R points)‚Äù illustrates.",2022-09-26 14:33:30+00:00,"DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars",cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Florian Drews'), arxiv.Result.Author('Di Feng'), arxiv.Result.Author('Florian Faion'), arxiv.Result.Author('Lars Rosenbaum'), arxiv.Result.Author('Michael Ulrich'), arxiv.Result.Author('Claudius Gl√§ser')]","We propose DeepFusion, a modular multi-modal architecture to fuse lidars,
cameras and radars in different combinations for 3D object detection.
Specialized feature extractors take advantage of each modality and can be
exchanged easily, making the approach simple and flexible. Extracted features
are transformed into bird's-eye-view as a common representation for fusion.
Spatial and semantic alignment is performed prior to fusing modalities in the
feature space. Finally, a detection head exploits rich multi-modal features for
improved 3D detection performance. Experimental results for lidar-camera,
lidar-camera-radar and camera-radar fusion show the flexibility and
effectiveness of our fusion approach. In the process, we study the largely
unexplored task of faraway car detection up to 225 meters, showing the benefits
of our lidar-camera fusion. Furthermore, we investigate the required density of
lidar points for 3D object detection and illustrate implications at the example
of robustness against adverse weather conditions. Moreover, ablation studies on
our camera-radar fusion highlight the importance of accurate depth estimation.",0.19176787,0.39883906,0.0219066,A
11666,"Therefore, we
                                                                     urge the research community to conduct further research in this
S  600 1,200 2,800                                                   regard.",Table 8 shows that the highest accuracy is reached         cloud and image representation learning Ô¨Åelds.,"First, we suggest designing deeper networks and testing
                                                                     their performance on large-scale point cloud datasets.",2022-09-26 15:28:31+00:00,Shrinking unit: a Graph Convolution-Based Unit for CNN-like 3D Point Cloud Feature Extractors,cs.CV,"['cs.CV', 'cs.LG', 'I.2.10; I.5.2; I.5.4']","[arxiv.Result.Author('Alberto Tamajo'), arxiv.Result.Author('Bastian Pla√ü'), arxiv.Result.Author('Thomas Klauer')]","3D point clouds have attracted increasing attention in architecture,
engineering, and construction due to their high-quality object representation
and efficient acquisition methods. Consequently, many point cloud feature
detection methods have been proposed in the literature to automate some
workflows, such as their classification or part segmentation. Nevertheless, the
performance of point cloud automated systems significantly lags behind their
image counterparts. While part of this failure stems from the irregularity,
unstructuredness, and disorder of point clouds, which makes the task of point
cloud feature detection significantly more challenging than the image one, we
argue that a lack of inspiration from the image domain might be the primary
cause of such a gap. Indeed, given the overwhelming success of Convolutional
Neural Networks (CNNs) in image feature detection, it seems reasonable to
design their point cloud counterparts, but none of the proposed approaches
closely resembles them. Specifically, even though many approaches generalise
the convolution operation in point clouds, they fail to emulate the CNNs
multiple-feature detection and pooling operations. For this reason, we propose
a graph convolution-based unit, dubbed Shrinking unit, that can be stacked
vertically and horizontally for the design of CNN-like 3D point cloud feature
extractors. Given that self, local and global correlations between points in a
point cloud convey crucial spatial geometric information, we also leverage them
during the feature extraction process. We evaluate our proposal by designing a
feature extractor model for the ModelNet-10 benchmark dataset and achieve
90.64% classification accuracy, demonstrating that our innovative idea is
effective. Our code is available at github.com/albertotamajo/Shrinking-unit.",-0.23889865,-0.10631999,0.14510807,C
11675,"Embedding Dimension We further study the inÔ¨Çuence of embedding dimension in the range of 16,
32, 64, and 128.","Results show that, compared with a large kernel, a small local window is sufÔ¨Åcient to distinguish
the semantic clusters.","Interestingly, results suggest that SAPA is not sensitive to the embedding dimension.",2022-09-26 17:32:25+00:00,SAPA: Similarity-Aware Point Affiliation for Feature Upsampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hao Lu'), arxiv.Result.Author('Wenze Liu'), arxiv.Result.Author('Zixuan Ye'), arxiv.Result.Author('Hongtao Fu'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Zhiguo Cao')]","We introduce point affiliation into feature upsampling, a notion that
describes the affiliation of each upsampled point to a semantic cluster formed
by local decoder feature points with semantic similarity. By rethinking point
affiliation, we present a generic formulation for generating upsampling
kernels. The kernels encourage not only semantic smoothness but also boundary
sharpness in the upsampled feature maps. Such properties are particularly
useful for some dense prediction tasks such as semantic segmentation. The key
idea of our formulation is to generate similarity-aware kernels by comparing
the similarity between each encoder feature point and the spatially associated
local region of decoder features. In this way, the encoder feature point can
function as a cue to inform the semantic cluster of upsampled feature points.
To embody the formulation, we further instantiate a lightweight upsampling
operator, termed Similarity-Aware Point Affiliation (SAPA), and investigate its
variants. SAPA invites consistent performance improvements on a number of dense
prediction tasks, including semantic segmentation, object detection, depth
estimation, and image matting. Code is available at:
https://github.com/poppinace/sapa",0.10122704,-0.22861311,-0.089802735,C
11676,"Embedding Dimension We further study the inÔ¨Çuence of embedding dimension in the range of 16,
32, 64, and 128.","Results show that, compared with a large kernel, a small local window is sufÔ¨Åcient to distinguish
the semantic clusters.","Interestingly, results suggest that SAPA is not sensitive to the embedding dimension.",2022-09-26 17:32:25+00:00,SAPA: Similarity-Aware Point Affiliation for Feature Upsampling,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hao Lu'), arxiv.Result.Author('Wenze Liu'), arxiv.Result.Author('Zixuan Ye'), arxiv.Result.Author('Hongtao Fu'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Zhiguo Cao')]","We introduce point affiliation into feature upsampling, a notion that
describes the affiliation of each upsampled point to a semantic cluster formed
by local decoder feature points with semantic similarity. By rethinking point
affiliation, we present a generic formulation for generating upsampling
kernels. The kernels encourage not only semantic smoothness but also boundary
sharpness in the upsampled feature maps. Such properties are particularly
useful for some dense prediction tasks such as semantic segmentation. The key
idea of our formulation is to generate similarity-aware kernels by comparing
the similarity between each encoder feature point and the spatially associated
local region of decoder features. In this way, the encoder feature point can
function as a cue to inform the semantic cluster of upsampled feature points.
To embody the formulation, we further instantiate a lightweight upsampling
operator, termed Similarity-Aware Point Affiliation (SAPA), and investigate its
variants. SAPA invites consistent performance improvements on a number of dense
prediction tasks, including semantic segmentation, object detection, depth
estimation, and image matting. Code is available at:
https://github.com/poppinace/sapa",0.10122704,-0.22861311,-0.089802735,C
11699,"There is a                                                                         synthetic large-scale polarization image datasets are avail-
           trade-off between the quality of the RGB image and polar-                                                                     able for further research and development.","The baseline code and newly generated real and
           sor degrade the quality of the RGB images.","ization information as fewer polarization pixels reduce the
           degradation of the RGB image but decrease the resolution                                                                      1.",2022-09-27 01:41:58+00:00,Simultaneous Acquisition of High Quality RGB Image and Polarization Information using a Sparse Polarization Sensor,cs.CV,['cs.CV'],"[arxiv.Result.Author('Teppei Kurita'), arxiv.Result.Author('Yuhi Kondo'), arxiv.Result.Author('Legong Sun'), arxiv.Result.Author('Yusuke Moriuchi')]","This paper proposes a novel polarization sensor structure and network
architecture to obtain a high-quality RGB image and polarization information.
Conventional polarization sensors can simultaneously acquire RGB images and
polarization information, but the polarizers on the sensor degrade the quality
of the RGB images. There is a trade-off between the quality of the RGB image
and polarization information as fewer polarization pixels reduce the
degradation of the RGB image but decrease the resolution of polarization
information. Therefore, we propose an approach that resolves the trade-off by
sparsely arranging polarization pixels on the sensor and compensating for
low-resolution polarization information with higher resolution using the RGB
image as a guide. Our proposed network architecture consists of an RGB image
refinement network and a polarization information compensation network. We
confirmed the superiority of our proposed network in compensating the
differential component of polarization intensity by comparing its performance
with state-of-the-art methods for similar tasks: depth completion. Furthermore,
we confirmed that our approach could simultaneously acquire higher quality RGB
images and polarization information than conventional polarization sensors,
resolving the trade-off between the quality of RGB images and polarization
information. The baseline code and newly generated real and synthetic
large-scale polarization image datasets are available for further research and
development.",-0.04327681,0.1526412,0.22908607,B
11705,"However, despite
                                        promising results, the Ô¨Åeld still faces challenges that require further research e.g., allowing Ô¨Çexible upsampling, more effective loss
                                        functions, and better evaluation metrics.","                                                                                                                                                                                    1

                                            Hitchhiker‚Äôs Guide to Super-Resolution:
                                                Introduction and Recent Advances

                                        Brian Moser1,2, Federico Raue1, Stanislav Frolov1, Jo¬® rn Hees1, Sebastian Palacio1 Andreas Dengel1,2
                                                             1 German Research Center for ArtiÔ¨Åcial Intelligence (DFKI), Germany
                                                                                       2 TU Kaiserslautern, Germany

                                                                                                         first.second@dfki.de

arXiv:2209.13131v1 [cs.CV] 27 Sep 2022  Abstract‚ÄîWith the advent of Deep Learning (DL), Super-Resolution (SR) has also become a thriving research area.","We review the domain of SR in light of recent advances, and examine state-of-the-art models
                                        such as diffusion (DDPM) and transformer-based SR models.",2022-09-27 03:28:34+00:00,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Brian Moser'), arxiv.Result.Author('Federico Raue'), arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Sebastian Palacio'), arxiv.Result.Author('Andreas Dengel')]","With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",0.0002049338,-0.09354022,0.358427,C
11706,"Unfortunately, it is a theoretical publication about
Besides its promising results and similar approaches [137],                      image priors, and the approach is too slow to be useful
it requires further research to decrease learning difÔ¨Åculty                      for most practical applications, as the authors stated them-
and computational cost.",[143].,selves.,2022-09-27 03:28:34+00:00,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Brian Moser'), arxiv.Result.Author('Federico Raue'), arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Sebastian Palacio'), arxiv.Result.Author('Andreas Dengel')]","With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",-0.087655015,0.023089752,0.12129663,C
11707,"Zero-shot learning for SR marks an exciting area                      approach, which differs by using a hybrid controller and a
for further research because it is highly practical, especially                  cell-based elastic search space that enables both macro (the
for applications where application-speciÔ¨Åc datasets are rare                     connections among different cell blocks) and micro search
or non-existent.","FALSR (2019) [147] is a similar
pure ZSSR.",(cell blocks).,2022-09-27 03:28:34+00:00,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Brian Moser'), arxiv.Result.Author('Federico Raue'), arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Sebastian Palacio'), arxiv.Result.Author('Andreas Dengel')]","With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",0.03716139,-0.12265005,0.024583418,C
11708,"An alternative is
their work, which needs further research.","Therefore, using them in
However, they faced substantial stability problems during                    an application like zooming is not feasible.","meta-upscaling, which enables arbitrary scaling but comes
                                                                             with computational overhead and stability issues.",2022-09-27 03:28:34+00:00,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Brian Moser'), arxiv.Result.Author('Federico Raue'), arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Sebastian Palacio'), arxiv.Result.Author('Andreas Dengel')]","With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",0.12224148,0.30537337,0.063063994,A
11709,"It marks
an exciting area for further research because it is highly                    [5] Z. Wang, J. Chen, and S. C. H. Hoi, ‚ÄúDeep learning for image
practical, especially for applications where datasets are                             super-resolution: A survey,‚Äù IEEE Transactions on Pattern Analysis
rare or non-existent.","In such                                                                                                                               17
a case, unsupervised SR methods are interesting.","Zero-shot learning and DIP are good                             and Machine Intelligence, vol.",2022-09-27 03:28:34+00:00,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Brian Moser'), arxiv.Result.Author('Federico Raue'), arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Sebastian Palacio'), arxiv.Result.Author('Andreas Dengel')]","With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",-0.13700439,-0.10964148,0.20168072,C
11710,"109‚Äì122, 2016.
further research, e.g., Ô¨Çexible upsampling.","2, pp.","We review the
area of SR with recent advances and examine state-of-the-art                  [12] M. Kawulok, P. Benecki, D. Kostrzewa, and L. Skonieczny,
models, such as transformer-based SR, and other architec-                             ‚ÄúEvolving imaging model for super-resolution reconstruction,‚Äù
ture designs proposed lately (e.g., denoising diffusion prob-                         in Proceedings of the Genetic and Evolutionary Computation Confer-
abilistic models).",2022-09-27 03:28:34+00:00,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Brian Moser'), arxiv.Result.Author('Federico Raue'), arxiv.Result.Author('Stanislav Frolov'), arxiv.Result.Author('J√∂rn Hees'), arxiv.Result.Author('Sebastian Palacio'), arxiv.Result.Author('Andreas Dengel')]","With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",0.11615401,0.217677,0.27856028,A
11713,"Then, subgroup models that     ties in sensitive attribute prediction (post-processing), which
only use female / male / young / old subgroup samples of train       calls for further research on fairness in MedIA.","This phenomenon could result from do-
tion: First, we pre-process the ISIC 2018 skin lesion dataset as     main gap in diÔ¨Äerent medical datasets (pre-processing), small
in 4.2.2 and train a sensitive attribute classiÔ¨Åcation network on    amount of samples in medical dataset (in-processing), diÔ¨Écul-
sex and age attributes, respectively.","Besides, in or-
set are trained for skin lesion prediction.",2022-09-27 06:29:18+00:00,"A Survey of Fairness in Medical Image Analysis: Concepts, Algorithms, Evaluations, and Challenges",cs.CV,['cs.CV'],"[arxiv.Result.Author('Zikang Xu'), arxiv.Result.Author('Jun Li'), arxiv.Result.Author('Qingsong Yao'), arxiv.Result.Author('Han Li'), arxiv.Result.Author('Xin Shi'), arxiv.Result.Author('S. Kevin Zhou')]","Fairness, a criterion focuses on evaluating algorithm performance on
different demographic groups, has gained attention in natural language
processing, recommendation system and facial recognition. Since there are
plenty of demographic attributes in medical image samples, it is important to
understand the concepts of fairness, be acquainted with unfairness mitigation
techniques, evaluate fairness degree of an algorithm and recognize challenges
in fairness issues in medical image analysis (MedIA). In this paper, we first
give a comprehensive and precise definition of fairness, following by
introducing currently used techniques in fairness issues in MedIA. After that,
we list public medical image datasets that contain demographic attributes for
facilitating the fairness research and summarize current algorithms concerning
fairness in MedIA. To help achieve a better understanding of fairness, and call
attention to fairness related issues in MedIA, experiments are conducted
comparing the difference between fairness and data imbalance, verifying the
existence of unfairness in various MedIA tasks, especially in classification,
segmentation and detection, and evaluating the effectiveness of unfairness
mitigation algorithms. Finally, we conclude with opportunities and challenges
in fairness in MedIA.",0.14398512,-0.1416657,-0.11810718,A
11714,"sitive attribute prediction (post-processing), which calls upon
                                                                              further researches on fairness in medical image analysis.","This phenomenon could result from domain gap
Ô¨Åne-tune model‚Äôs output based on fairness criteria (Wang et al.,              in diÔ¨Äerent medical datasets (pre-processing), small amount of
2022).Besides, it can take full usage of pretrained models since              samples in medical dataset (in-processing), diÔ¨Éculties in sen-
the deployed model do not need to retrain a second time.","Be-
   In this section, we adopt a post-processing method called                  sides, in order to mitigate unfairness in medical applications,
‚ÄòFAAP‚Äô (Fairness-Aware Adversarial Perturbation) proposed                     more attention need to be paid on the unique characteristics of
by Wang et al.",2022-09-27 06:29:18+00:00,"A Survey of Fairness in Medical Image Analysis: Concepts, Algorithms, Evaluations, and Challenges",cs.CV,['cs.CV'],"[arxiv.Result.Author('Zikang Xu'), arxiv.Result.Author('Jun Li'), arxiv.Result.Author('Qingsong Yao'), arxiv.Result.Author('Han Li'), arxiv.Result.Author('Xin Shi'), arxiv.Result.Author('S. Kevin Zhou')]","Fairness, a criterion focuses on evaluating algorithm performance on
different demographic groups, has gained attention in natural language
processing, recommendation system and facial recognition. Since there are
plenty of demographic attributes in medical image samples, it is important to
understand the concepts of fairness, be acquainted with unfairness mitigation
techniques, evaluate fairness degree of an algorithm and recognize challenges
in fairness issues in medical image analysis (MedIA). In this paper, we first
give a comprehensive and precise definition of fairness, following by
introducing currently used techniques in fairness issues in MedIA. After that,
we list public medical image datasets that contain demographic attributes for
facilitating the fairness research and summarize current algorithms concerning
fairness in MedIA. To help achieve a better understanding of fairness, and call
attention to fairness related issues in MedIA, experiments are conducted
comparing the difference between fairness and data imbalance, verifying the
existence of unfairness in various MedIA tasks, especially in classification,
segmentation and detection, and evaluating the effectiveness of unfairness
mitigation algorithms. Finally, we conclude with opportunities and challenges
in fairness in MedIA.",0.0885459,-0.21266298,0.032481432,C
11715,"It learns to perturb input data to                    which calls upon further researches on fairness in medical
blind deployed models by training an adversarial perturbation                 image analysis.",in [79].,"Besides, in order to mitigate unfairness in
generator to avoid fairness-related features being extract by
the deployed model.",2022-09-27 06:29:18+00:00,"A Survey of Fairness in Medical Image Analysis: Concepts, Algorithms, Evaluations, and Challenges",cs.CV,['cs.CV'],"[arxiv.Result.Author('Zikang Xu'), arxiv.Result.Author('Jun Li'), arxiv.Result.Author('Qingsong Yao'), arxiv.Result.Author('Han Li'), arxiv.Result.Author('Xin Shi'), arxiv.Result.Author('S. Kevin Zhou')]","Fairness, a criterion focuses on evaluating algorithm performance on
different demographic groups, has gained attention in natural language
processing, recommendation system and facial recognition. Since there are
plenty of demographic attributes in medical image samples, it is important to
understand the concepts of fairness, be acquainted with unfairness mitigation
techniques, evaluate fairness degree of an algorithm and recognize challenges
in fairness issues in medical image analysis (MedIA). In this paper, we first
give a comprehensive and precise definition of fairness, following by
introducing currently used techniques in fairness issues in MedIA. After that,
we list public medical image datasets that contain demographic attributes for
facilitating the fairness research and summarize current algorithms concerning
fairness in MedIA. To help achieve a better understanding of fairness, and call
attention to fairness related issues in MedIA, experiments are conducted
comparing the difference between fairness and data imbalance, verifying the
existence of unfairness in various MedIA tasks, especially in classification,
segmentation and detection, and evaluating the effectiveness of unfairness
mitigation algorithms. Finally, we conclude with opportunities and challenges
in fairness in MedIA.",0.025442515,-0.18464203,0.17307569,C
11717,"For the
of the datasets are not satisfying for further research on           ERP images with discontinuous edge effects, it is usually
complex 360‚ó¶ scenarios.","Secondly, the image resolutions      effects, which is also one of the major challenges.","In this section, a new large-scale           more difÔ¨Åcult to obtain complete segmentations due to the
dataset named ODI-SOD1 is introduced from the aspects of             forced separation in space.",2022-09-27 07:44:08+00:00,View-aware Salient Object Detection for 360¬∞ Omnidirectional Image,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junjie Wu'), arxiv.Result.Author('Changqun Xia'), arxiv.Result.Author('Tianshu Yu'), arxiv.Result.Author('Jia Li')]","Image-based salient object detection (ISOD) in 360{\deg} scenarios is
significant for understanding and applying panoramic information. However,
research on 360{\deg} ISOD has not been widely explored due to the lack of
large, complex, high-resolution, and well-labeled datasets. Towards this end,
we construct a large scale 360{\deg} ISOD dataset with object-level pixel-wise
annotation on equirectangular projection (ERP), which contains rich panoramic
scenes with not less than 2K resolution and is the largest dataset for
360{\deg} ISOD by far to our best knowledge. By observing the data, we find
current methods face three significant challenges in panoramic scenarios:
diverse distortion degrees, discontinuous edge effects and changeable object
scales. Inspired by humans' observing process, we propose a view-aware salient
object detection method based on a Sample Adaptive View Transformer (SAVT)
module with two sub-modules to mitigate these issues. Specifically, the
sub-module View Transformer (VT) contains three transform branches based on
different kinds of transformations to learn various features under different
views and heighten the model's feature toleration of distortion, edge effects
and object scales. Moreover, the sub-module Sample Adaptive Fusion (SAF) is to
adjust the weights of different transform branches based on various sample
features and make transformed enhanced features fuse more appropriately. The
benchmark results of 20 state-of-the-art ISOD methods reveal the constructed
dataset is very challenging. Moreover, exhaustive experiments verify the
proposed approach is practical and outperforms the state-of-the-art methods.",-0.04592834,0.23514932,0.06775268,B
11725,"A. Velten, T. Willwacher, O. Gupta, A. Veeraraghavan, M. G. Bawendi,
driven methods, which reduces the data volume greatly, and has
great potential to facilitate the further research of learned feature                and R. Raskar, Nat.",paradigm information of NLOS moving target with end-to-end data                  9.,Commun.,2022-09-27 10:56:14+00:00,Passive Non-line-of-sight Imaging for Moving Targets with an Event Camera,cs.CV,"['cs.CV', 'eess.IV', 'physics.optics']","[arxiv.Result.Author('Conghe Wang'), arxiv.Result.Author('Yutong He'), arxiv.Result.Author('Xia Wang'), arxiv.Result.Author('Honghao Huang'), arxiv.Result.Author('Changda Yan'), arxiv.Result.Author('Xin Zhang'), arxiv.Result.Author('Hongwei Chen')]","Non-line-of-sight (NLOS) imaging is an emerging technique for detecting
objects behind obstacles or around corners. Recent studies on passive NLOS
mainly focus on steady-state measurement and reconstruction methods, which show
limitations in recognition of moving targets. To the best of our knowledge, we
propose a novel event-based passive NLOS imaging method. We acquire
asynchronous event-based data which contains detailed dynamic information of
the NLOS target, and efficiently ease the degradation of speckle caused by
movement. Besides, we create the first event-based NLOS imaging dataset,
NLOS-ES, and the event-based feature is extracted by time-surface
representation. We compare the reconstructions through event-based data with
frame-based data. The event-based method performs well on PSNR and LPIPS, which
is 20% and 10% better than frame-based method, while the data volume takes only
2% of traditional method.",0.15276702,-0.20396194,-0.031539716,A
11735,"First, one unique characteristic of remote sensing                      Meta-Learner
                                        object detection is the Oriented Bounding Boxes (OBB) of the
                                        objects and the fusion of multiple OBBs requires further research
                                        attention.","Two
                                        problems arise.","Second, the widely used deep learning object detectors
                                        provide a score for each detected object as an indicator of
                                        conÔ¨Ådence, but how to use these indicators effectively in an
                                        ensemble method remains a problem.",2022-09-27 13:17:06+00:00,OBBStacking: An Ensemble Method for Remote Sensing Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haoning Lin'), arxiv.Result.Author('Changhao Sun'), arxiv.Result.Author('Yunpeng Liu')]","Ensemble methods are a reliable way to combine several models to achieve
superior performance. However, research on the application of ensemble methods
in the remote sensing object detection scenario is mostly overlooked. Two
problems arise. First, one unique characteristic of remote sensing object
detection is the Oriented Bounding Boxes (OBB) of the objects and the fusion of
multiple OBBs requires further research attention. Second, the widely used deep
learning object detectors provide a score for each detected object as an
indicator of confidence, but how to use these indicators effectively in an
ensemble method remains a problem. Trying to address these problems, this paper
proposes OBBStacking, an ensemble method that is compatible with OBBs and
combines the detection results in a learned fashion. This ensemble method helps
take 1st place in the Challenge Track \textit{Fine-grained Object Recognition
in High-Resolution Optical Images}, which was featured in \textit{2021 Gaofen
Challenge on Automated High-Resolution Earth Observation Image Interpretation}.
The experiments on DOTA dataset and FAIR1M dataset demonstrate the improved
performance of OBBStacking and the features of OBBStacking are analyzed.",-0.26359594,-0.032628093,0.06821384,B
11745,"1
 ing in the literature, and we argue that further research on
 this problem cannot be pursued without a thorough study on       2 Related Work
 the impact of FPV on tracking.","We believe that the particular setting of-      ject tracking will gain popularity in this domain as new and
 fered by FPV deserves a dedicated analysis that is still miss-   FPV-speciÔ¨Åc methodologies are investigated.","2.1 Visual Tracking in FPV
      In this paper, we aim to extensively analyze the prob-
 lem of visual object tracking in the FPV domain in order         There have been some attempts to tackle visual tracking in
 to answer the aforementioned questions.",2022-09-27 16:18:47+00:00,Visual Object Tracking in First Person Vision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Matteo Dunnhofer'), arxiv.Result.Author('Antonino Furnari'), arxiv.Result.Author('Giovanni Maria Farinella'), arxiv.Result.Author('Christian Micheloni')]","The understanding of human-object interactions is fundamental in First Person
Vision (FPV). Visual tracking algorithms which follow the objects manipulated
by the camera wearer can provide useful information to effectively model such
interactions. In the last years, the computer vision community has
significantly improved the performance of tracking algorithms for a large
variety of target objects and scenarios. Despite a few previous attempts to
exploit trackers in the FPV domain, a methodical analysis of the performance of
state-of-the-art trackers is still missing. This research gap raises the
question of whether current solutions can be used ``off-the-shelf'' or more
domain-specific investigations should be carried out. This paper aims to
provide answers to such questions. We present the first systematic
investigation of single object tracking in FPV. Our study extensively analyses
the performance of 42 algorithms including generic object trackers and baseline
FPV-specific trackers. The analysis is carried out by focusing on different
aspects of the FPV setting, introducing new performance measures, and in
relation to FPV-specific tasks. The study is made possible through the
introduction of TREK-150, a novel benchmark dataset composed of 150 densely
annotated video sequences. Our results show that object tracking in FPV poses
new challenges to current visual trackers. We highlight the factors causing
such behavior and point out possible research directions. Despite their
difficulties, we prove that trackers bring benefits to FPV downstream tasks
requiring short-term object tracking. We expect that generic object tracking
will gain popularity in FPV as new and FPV-specific methodologies are
investigated.",-0.17567345,0.26531038,-0.13585462,B
11785,"In the following, we
structure and effectively narrows the semantic gap       further study and analyze the U-Net structure
between the encoder and the decoder, thereby             and principle and get inspiration from[2][3][4].","The output and       first experiment was to verify the feasibility of
the features before pooling are fused by semantic        using U-Net to segment the method of
clustering, which replaces the traditional jump          engineering drawings.","improving the segmentation performance of welding        Finally, we propose to replace the classic U-net
engineering drawings.",2022-09-28 13:49:45+00:00,CSSAM: U-net Network for Application and Segmentation of Welding Engineering Drawings,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Zhiwei Song'), arxiv.Result.Author('Hui Yao'), arxiv.Result.Author('Dan Tian'), arxiv.Result.Author('GaoHui Zhan')]","Heavy equipment manufacturing splits specific contours in drawings and cuts
sheet metal to scale for welding. Currently, most of the segmentation and
extraction of weld map contours is achieved manually. Its efficiency is greatly
reduced. Therefore, we propose a U-net-based contour segmentation and
extraction method for welding engineering drawings. The contours of the parts
required for engineering drawings can be automatically divided and blanked,
which significantly improves manufacturing efficiency. U-net includes an
encoder-decoder, which implements end-to-end mapping through semantic
differences and spatial location feature information between the encoder and
decoder. While U-net excels at segmenting medical images, our extensive
experiments on the Welding Structural Diagram dataset show that the classic
U-Net architecture falls short in segmenting welding engineering drawings.
Therefore, we design a novel Channel Spatial Sequence Attention Module (CSSAM)
and improve on the classic U-net. At the same time, vertical max pooling and
average horizontal pooling are proposed. Pass the pooling operation through two
equal convolutions into the CSSAM module. The output and the features before
pooling are fused by semantic clustering, which replaces the traditional jump
structure and effectively narrows the semantic gap between the encoder and the
decoder, thereby improving the segmentation performance of welding engineering
drawings. We use vgg16 as the backbone network. Compared with the classic
U-net, our network has good performance in engineering drawing dataset
segmentation.",-0.105320394,-0.077024154,0.07809946,C
11788,"We hope that our Ô¨Åndings spark further research in the realm of textless VL models
that take raw signals as input and seek to learn a more compact and efÔ¨Åcient vision-and-language
representation.","Furthermore, we also show that TVLT can capture acoustic
information beyond speech and is more effective in multimodal emotion classiÔ¨Åcation than its text-
based counterpart.","2 Related Work

Text-based Representation Learning.",2022-09-28 15:08:03+00:00,TVLT: Textless Vision-Language Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Zineng Tang'), arxiv.Result.Author('Jaemin Cho'), arxiv.Result.Author('Yixin Nie'), arxiv.Result.Author('Mohit Bansal')]","In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart, on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT",-0.024723716,-0.26382184,-0.16308567,C
11789,"We hope that our Ô¨Åndings spark further research in the realm of textless VL models
that take raw signals as input and seek to learn a more compact and efÔ¨Åcient vision-and-language
representation.","Furthermore, we also show that TVLT can capture acoustic
information beyond speech and is more effective in multimodal emotion classiÔ¨Åcation than its text-
based counterpart.","2 Related Work

Text-based Representation Learning.",2022-09-28 15:08:03+00:00,TVLT: Textless Vision-Language Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Zineng Tang'), arxiv.Result.Author('Jaemin Cho'), arxiv.Result.Author('Yixin Nie'), arxiv.Result.Author('Mohit Bansal')]","In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT",-0.024723716,-0.26382184,-0.16308567,C
11794,"this end, exploring how to construct a more natural adversarial
patch under the large magnitude constraint of perturbation is                                  REFERENCES
worth further research in the future.",To      .,"[1] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
B. Transferability of physical adversarial attacks                        large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556, 2014.",2022-09-28 17:23:52+00:00,A Survey on Physical Adversarial Attack in Computer Vision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Donghua Wang'), arxiv.Result.Author('Wen Yao'), arxiv.Result.Author('Tingsong Jiang'), arxiv.Result.Author('Guijiang Tang'), arxiv.Result.Author('Xiaoqian Chen')]","In the past decade, deep learning has dramatically changed the traditional
hand-craft feature manner with strong feature learning capability, resulting in
tremendous improvement of conventional tasks. However, deep neural networks
have recently been demonstrated vulnerable to adversarial examples, a kind of
malicious samples crafted by small elaborately designed noise, which mislead
the DNNs to make the wrong decisions while remaining imperceptible to humans.
Adversarial examples can be divided into digital adversarial attacks and
physical adversarial attacks. The digital adversarial attack is mostly
performed in lab environments, focusing on improving the performance of
adversarial attack algorithms. In contrast, the physical adversarial attack
focus on attacking the physical world deployed DNN systems, which is a more
challenging task due to the complex physical environment (i.e., brightness,
occlusion, and so on). Although the discrepancy between digital adversarial and
physical adversarial examples is small, the physical adversarial examples have
a specific design to overcome the effect of the complex physical environment.
In this paper, we review the development of physical adversarial attacks in
DNN-based computer vision tasks, including image recognition tasks, object
detection tasks, and semantic segmentation. For the sake of completeness of the
algorithm evolution, we will briefly introduce the works that do not involve
the physical adversarial attack. We first present a categorization scheme to
summarize the current physical adversarial attacks. Then discuss the advantages
and disadvantages of the existing physical adversarial attacks and focus on the
technique used to maintain the adversarial when applied into physical
environment. Finally, we point out the issues of the current physical
adversarial attacks to be solved and provide promising research directions.",-0.13040783,-0.16585615,0.26565307,C
11804,"We hope that our OOD dataset generation
and evaluation results will stimulate further research into ef-                                                  [6] D. Hendrycks, M. Mazeika, S. Kadavath, and D. Song, ‚ÄúUsing
fective OOD detection for LiDAR-based 3D object detection.","2
object detection.","self-supervised learning can improve model robustness and
                                                                                                                      uncertainty,‚Äù NeurIPS, 2019.",2022-09-28 21:39:25+00:00,Out-of-Distribution Detection for LiDAR-based 3D Object Detection,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO', 'eess.IV']","[arxiv.Result.Author('Chengjie Huang'), arxiv.Result.Author('Van Duong Nguyen'), arxiv.Result.Author('Vahdat Abdelzad'), arxiv.Result.Author('Christopher Gus Mannes'), arxiv.Result.Author('Luke Rowe'), arxiv.Result.Author('Benjamin Therien'), arxiv.Result.Author('Rick Salay'), arxiv.Result.Author('Krzysztof Czarnecki')]","3D object detection is an essential part of automated driving, and deep
neural networks (DNNs) have achieved state-of-the-art performance for this
task. However, deep models are notorious for assigning high confidence scores
to out-of-distribution (OOD) inputs, that is, inputs that are not drawn from
the training distribution. Detecting OOD inputs is challenging and essential
for the safe deployment of models. OOD detection has been studied extensively
for the classification task, but it has not received enough attention for the
object detection task, specifically LiDAR-based 3D object detection. In this
paper, we focus on the detection of OOD inputs for LiDAR-based 3D object
detection. We formulate what OOD inputs mean for object detection and propose
to adapt several OOD detection methods for object detection. We accomplish this
by our proposed feature extraction method. To evaluate OOD detection methods,
we develop a simple but effective technique of generating OOD objects for a
given object detection model. Our evaluation based on the KITTI dataset shows
that different OOD detection methods have biases toward detecting specific OOD
objects. It emphasizes the importance of combined OOD detection methods and
more research in this direction.",-0.2571143,0.12449846,0.021573914,B
11817,We let participants          opens new avenues in sign language for further research.,"We believe, this poster
regarding the necessity of our system.","upload profile images to PerSign and asked them to rate
the results on a scale of ‚óã1 to ‚óã5 according to Likert rating
method [11], with ‚óã1 being not necessary at all and ‚óã5 being
very necessary.",2022-09-29 07:07:34+00:00,PerSign: Personalized Bangladeshi Sign Letters Synthesis,cs.CV,"['cs.CV', 'cs.HC']","[arxiv.Result.Author('Mohammad Imrul Jubair'), arxiv.Result.Author('Ali Ahnaf'), arxiv.Result.Author('Tashfiq Nahiyan Khan'), arxiv.Result.Author('Ullash Bhattacharjee'), arxiv.Result.Author('Tanjila Joti')]","Bangladeshi Sign Language (BdSL) - like other sign languages - is tough to
learn for general people, especially when it comes to expressing letters. In
this poster, we propose PerSign, a system that can reproduce a person's image
by introducing sign gestures in it. We make this operation personalized, which
means the generated image keeps the person's initial image profile - face, skin
tone, attire, background - unchanged while altering the hand, palm, and finger
positions appropriately. We use an image-to-image translation technique and
build a corresponding unique dataset to accomplish the task. We believe the
translated image can reduce the communication gap between signers (person who
uses sign language) and non-signers without having prior knowledge of BdSL.",0.22117084,0.012911897,-0.2867767,A
11844,For ‚Äúgrayscale DIA‚Äù we perform alignment                     further research in this direction.,"We hope that
‚Äúmono-depth‚Äù version of our approach we replace the stereo-                our method has revealed the power of direct methods utilized
depth from DispNet3 [26] by monocular estimates from                       in the scope of multi-object tracking and that it will promote
Adabins [40].","In particular, evaluation of
based on 1-channel instead of 3-channels images.",2022-09-29 17:40:22+00:00,DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and Photometric Bundle Adjustment,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mariia Gladkova'), arxiv.Result.Author('Nikita Korobov'), arxiv.Result.Author('Nikolaus Demmel'), arxiv.Result.Author('Aljo≈°a O≈°ep'), arxiv.Result.Author('Laura Leal-Taix√©'), arxiv.Result.Author('Daniel Cremers')]","Direct methods have shown excellent performance in the applications of visual
odometry and SLAM. In this work we propose to leverage their effectiveness for
the task of 3D multi-object tracking. To this end, we propose DirectTracker, a
framework that effectively combines direct image alignment for the short-term
tracking and sliding-window photometric bundle adjustment for 3D object
detection. Object proposals are estimated based on the sparse sliding-window
pointcloud and further refined using an optimization-based cost function that
carefully combines 3D and 2D cues to ensure consistency in image and world
space. We propose to evaluate 3D tracking using the recently introduced
higher-order tracking accuracy (HOTA) metric and the generalized intersection
over union similarity measure to mitigate the limitations of the conventional
use of intersection over union for the evaluation of vision-based trackers. We
perform evaluation on the KITTI Tracking benchmark for the Car class and show
competitive performance in tracking objects both in 2D and 3D.",-0.25209048,0.3243639,0.052203782,B
11845,"To further study the effects
same number as isotropic ConvNeXt variants.",Note that these variants reduce FLOPs to almost the      ing NA throughout the model.,"They also             of different attention mechanisms, and investigate whether
reduce memory usage compared to ViT+noticeably.",2022-09-29 17:57:08+00:00,Dilated Neighborhood Attention Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ali Hassani'), arxiv.Result.Author('Humphrey Shi')]","Transformers are quickly becoming one of the most heavily applied deep
learning architectures across modalities, domains, and tasks. In vision, on top
of ongoing efforts into plain transformers, hierarchical transformers have also
gained significant attention, thanks to their performance and easy integration
into existing frameworks. These models typically employ localized attention
mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin
Transformer's Shifted Window Self Attention. While effective at reducing self
attention's quadratic complexity, local attention weakens two of the most
desirable properties of self attention: long range inter-dependency modeling,
and global receptive field. In this paper, we introduce Dilated Neighborhood
Attention (DiNA), a natural, flexible and efficient extension to NA that can
capture more global context and expand receptive fields exponentially at no
additional cost. NA's local attention and DiNA's sparse global attention
complement each other, and therefore we introduce Dilated Neighborhood
Attention Transformer (DiNAT), a new hierarchical vision transformer built upon
both. DiNAT variants enjoy significant improvements over attention-based
baselines such as NAT and Swin, as well as modern convolutional baseline
ConvNeXt. Our Large model is ahead of its Swin counterpart by 1.5% box AP in
COCO object detection, 1.3% mask AP in COCO instance segmentation, and 1.1%
mIoU in ADE20K semantic segmentation, and faster in throughput. We believe
combinations of NA and DiNA have the potential to empower various tasks beyond
those presented in this paper. To support and encourage research in this
direction, in vision and beyond, we open-source our project at:
https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.",0.24781242,-0.1726873,0.21036333,A
11846,"To further study the effects of dif-
again be attributed to the lack of fully optimized implemen-      ferent attention mechanisms, and investigate whether or not
tations.","We            a combination of NA and DiNA is more effective at pro-
Ô¨Ånd that isotropic variants of both NAT and DiNAT exhibit         ducing an alternative to self attention than simply using NA
only minor throughput improvements over ViT+, which can           throughout the model.","Note that these variants reduce FLOPs to almost          a model fully based on self-attention always yields the best
the same number as isotropic ConvNeXt variants.",2022-09-29 17:57:08+00:00,Dilated Neighborhood Attention Transformer,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ali Hassani'), arxiv.Result.Author('Humphrey Shi')]","Transformers are quickly becoming one of the most heavily applied deep
learning architectures across modalities, domains, and tasks. In vision, on top
of ongoing efforts into plain transformers, hierarchical transformers have also
gained significant attention, thanks to their performance and easy integration
into existing frameworks. These models typically employ localized attention
mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin
Transformer's Shifted Window Self Attention. While effective at reducing self
attention's quadratic complexity, local attention weakens two of the most
desirable properties of self attention: long range inter-dependency modeling,
and global receptive field. In this paper, we introduce Dilated Neighborhood
Attention (DiNA), a natural, flexible and efficient extension to NA that can
capture more global context and expand receptive fields exponentially at no
additional cost. NA's local attention and DiNA's sparse global attention
complement each other, and therefore we introduce Dilated Neighborhood
Attention Transformer (DiNAT), a new hierarchical vision transformer built upon
both. DiNAT variants enjoy significant improvements over strong baselines such
as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin
counterpart by 1.5% box AP in COCO object detection, 1.3% mask AP in COCO
instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation. Paired
with new frameworks, our large variant is the new state of the art panoptic
segmentation model on COCO (58.2 PQ) and ADE20K (48.5 PQ), and instance
segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4 AP) (no extra
data). It also matches the state of the art specialized semantic segmentation
models on ADE20K (58.2 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no
extra data). We open-source our project.",0.21929216,-0.15379559,0.08740338,A
11854,"We arbitrarily chose the value of two
                                       simply for the purpose of our experiment, and think further research needs to be con-
                                       ducted to find the optimal ratio of the weight of TP to TN or FN to FP values in finding
                                       the optimal threshold.","Furthermore, for our Threshold Optimization and Voting algorithm, we assigned a
                                       weight of two for the TF and FN values respectively.",5.,2022-09-30 00:22:57+00:00,Embedded System Performance Analysis for Implementing a Portable Drowsiness Detection System for Drivers,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Minjeong Kim'), arxiv.Result.Author('Jimin Koo')]","Drowsiness on the road is a widespread problem with fatal consequences; thus,
a multitude of solutions implementing machine learning techniques have been
proposed by researchers. Among existing methods, Ghoddoosian et al.'s
drowsiness detection method utilizes temporal blinking patterns to detect early
signs of drowsiness. Although the method reported promising results,
Ghoddoosian et al.'s algorithm was developed and tested only on a powerful
desktop computer, which is not practical to apply in a moving vehicle setting.
In this paper, we propose an embedded system that can process Ghoddoosian's
drowsiness detection algorithm on a small minicomputer and interact with the
user by phone; combined, the devices are powerful enough to run a web server
and our drowsiness detection server. We used the AioRTC protocol on GitHub to
conduct real-time transmission of video frames from the client to the server
and evaluated the communication speed and processing times of the program on
various platforms. Based on our results, we found that a Mini PC was most
suitable for our proposed system. Furthermore, we proposed an algorithm that
considers the importance of sensitivity over specificity, specifically
regarding drowsiness detection algorithms. Our algorithm optimizes the
threshold to adjust the false positive and false negative rates of the
drowsiness detection models. We anticipate our proposed platform can help many
researchers to advance their research on drowsiness detection solutions in
embedded system settings.",0.43398586,0.120892644,0.08137274,A
11855,"We arbitrarily chose the value of two
                                       simply for the purpose of our experiment, and think further research needs to be
                                       conducted to find the optimal ratio of the weight of TP to TN or FN to FP values in finding
                                       the optimal threshold.","Furthermore, for our Threshold Optimization and Voting algorithm, we assigned a
                                       weight of two for the TF and FN values respectively.",5.,2022-09-30 00:22:57+00:00,Embedded System Performance Analysis for Implementing a Portable Drowsiness Detection System for Drivers,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Minjeong Kim'), arxiv.Result.Author('Jimin Koo')]","Drowsiness on the road is a widespread problem with fatal consequences; thus,
a multitude of solutions implementing machine learning techniques have been
proposed by researchers. Among existing methods, Ghoddoosian et al.'s
drowsiness detection method utilizes temporal blink patterns to detect early
signs of drowsiness. Although the method reported promising results,
Ghoddoosian et al.'s algorithm was developed and tested only on a powerful
desktop computer, which is not practical to apply in a moving vehicle setting.
We propose an embedded system that can process Ghoddoosian et al.'s drowsiness
detection algorithm on a MiniPC and interact with the user by phone; combined,
the vehicles are powerful enough to run a web server and our drowsiness
detection server. In this paper, we explain how we implemented Ghoddoosian et
al.'s drowsiness detection method in an embedded system using the AioRTC
Protocol and propose the methods to compare the potential of various hardware
setups and ultimately choose the most practical device for our embedded system.
We evaluated the communication speed and processing times of the program on
various platforms. Based on our results, we found that the Mini PC was most
suitable for our proposed system. Furthermore, we proposed an algorithm that
prioritizes the sensitivity of the drowsiness detection over the specificity.
Based on the false positive and false negative rates, the algorithm optimizes
the threshold of alerting the driver. We anticipate that our proposed platform
can help many researchers advance their research on drowsiness detection
solutions in embedded system settings.",0.43895763,0.11522352,0.06025841,A
11856,"We arbitrarily chose the value of two
                                       simply for the purpose of our experiment and think further research can be conducted to find
                                       the optimal ratio of the weight of TP to TN or FN to FP values in finding the optimal threshold.","Furthermore, for our Threshold Optimization and Voting algorithm, we assigned a
                                       weight of two for the TF and FN values respectively.",5.,2022-09-30 00:22:57+00:00,Embedded System Performance Analysis for Implementing a Portable Drowsiness Detection System for Drivers,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Minjeong Kim'), arxiv.Result.Author('Jimin Koo')]","Drowsiness on the road is a widespread problem with fatal consequences; thus,
a multitude of systems and techniques have been proposed. Among existing
methods, Ghoddoosian et al. utilized temporal blinking patterns to detect early
signs of drowsiness, but their algorithm was tested only on a powerful desktop
computer, which is not practical to apply in a moving vehicle setting. In this
paper, we propose an efficient platform to run Ghoddosian's algorithm, detail
the performance tests we ran to determine this platform, and explain our
threshold optimization logic. After considering the Jetson Nano and Beelink
(Mini PC), we concluded that the Mini PC is the most efficient and practical to
run our embedded system in a vehicle. To determine this, we ran communication
speed tests and evaluated total processing times for inference operations.
Based on our experiments, the average total processing time to run the
drowsiness detection model was 94.27 ms for Jetson Nano and 22.73 ms for the
Beelink (Mini PC). Considering the portability and power efficiency of each
device, along with the processing time results, the Beelink (Mini PC) was
determined to be most suitable. Also, we propose a threshold optimization
algorithm, which determines whether the driver is drowsy or alert based on the
trade-off between the sensitivity and specificity of the drowsiness detection
model. Our study will serve as a crucial next step for drowsiness detection
research and its application in vehicles. Through our experiment, we have
determinend a favorable platform that can run drowsiness detection algorithms
in real-time and can be used as a foundation to further advance drowsiness
detection research. In doing so, we have bridged the gap between an existing
embedded system and its actual implementation in vehicles to bring drowsiness
technology a step closer to prevalent real-life implementation.",0.44369906,0.12198569,0.064332046,A
11915,"We believe that with
                                                                 further research, our approach can perform better and facilitate
              V. DISCUSSION AND CONCLUSION                       the development of CAD.",effectiveness and generality of WSDAN.,"The images and texts given in the MVQA task are more
difÔ¨Åcult to understand than normal because of the knowledge
associated with the medical Ô¨Åeld.",2022-10-01 08:32:40+00:00,A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaofei Huang'), arxiv.Result.Author('Hongfang Gong')]","Research in medical visual question answering (MVQA) can contribute to the
development of computeraided diagnosis. MVQA is a task that aims to predict
accurate and convincing answers based on given medical images and associated
natural language questions. This task requires extracting medical
knowledge-rich feature content and making fine-grained understandings of them.
Therefore, constructing an effective feature extraction and understanding
scheme are keys to modeling. Existing MVQA question extraction schemes mainly
focus on word information, ignoring medical information in the text. Meanwhile,
some visual and textual feature understanding schemes cannot effectively
capture the correlation between regions and keywords for reasonable visual
reasoning. In this study, a dual-attention learning network with word and
sentence embedding (WSDAN) is proposed. We design a module, transformer with
sentence embedding (TSE), to extract a double embedding representation of
questions containing keywords and medical information. A dualattention learning
(DAL) module consisting of self-attention and guided attention is proposed to
model intensive intramodal and intermodal interactions. With multiple DAL
modules (DALs), learning visual and textual co-attention can increase the
granularity of understanding and improve visual reasoning. Experimental results
on the ImageCLEF 2019 VQA-MED (VQA-MED 2019) and VQA-RAD datasets demonstrate
that our proposed method outperforms previous state-of-the-art methods.
According to the ablation studies and Grad-CAM maps, WSDAN can extract rich
textual information and has strong visual reasoning ability.",0.09174126,0.06186134,-0.1962951,A
11916,"further research, our approach can perform better and facilitate
                                                                  the development of CAD.","We believe that with
effective, allowing for more robust visual inference.","V. DISCUSSION AND CONCLUSION

   The images and texts given in the MVQA task are more
difÔ¨Åcult to understand than normal because of the knowledge
associated with the medical Ô¨Åeld.",2022-10-01 08:32:40+00:00,A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaofei Huang'), arxiv.Result.Author('Hongfang Gong')]","Research in medical visual question answering (MVQA) can contribute to the
development of computeraided diagnosis. MVQA is a task that aims to predict
accurate and convincing answers based on given medical images and associated
natural language questions. This task requires extracting medical
knowledge-rich feature content and making fine-grained understandings of them.
Therefore, constructing an effective feature extraction and understanding
scheme are keys to modeling. Existing MVQA question extraction schemes mainly
focus on word information, ignoring medical information in the text. Meanwhile,
some visual and textual feature understanding schemes cannot effectively
capture the correlation between regions and keywords for reasonable visual
reasoning. In this study, a dual-attention learning network with word and
sentence embedding (WSDAN) is proposed. We design a module, transformer with
sentence embedding (TSE), to extract a double embedding representation of
questions containing keywords and medical information. A dualattention learning
(DAL) module consisting of self-attention and guided attention is proposed to
model intensive intramodal and intermodal interactions. With multiple DAL
modules (DALs), learning visual and textual co-attention can increase the
granularity of understanding and improve visual reasoning. Experimental results
on the ImageCLEF 2019 VQA-MED (VQA-MED 2019) and VQA-RAD datasets demonstrate
that our proposed method outperforms previous state-of-the-art methods.
According to the ablation studies and Grad-CAM maps, WSDAN can extract rich
textual information and has strong visual reasoning ability.",-0.14531818,0.11740134,-0.15848085,B
11919,"Studying the
of further study.","Hence, future studies can focus on
characters in the alphabet and the experiments            extending the experiments to Ô¨Ånd the most op-
explored in this study open an interesting area           timum few-shot learning setting.","formulation of training episodes from the com-
                                                          bination of character, row, and column labels is
The results of the proposed methods in this               also an important area of research to progress
study have shown how episodic task formula-               in few-shot learning for Amharic handwritten
tion aÔ¨Äects the performance of few-shot learn-            character recognition.",2022-10-01 13:16:18+00:00,Offline Handwritten Amharic Character Recognition Using Few-shot Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Mesay Samuel'), arxiv.Result.Author('Lars Schmidt-Thieme'), arxiv.Result.Author('DP Sharma'), arxiv.Result.Author('Abiot Sinamo'), arxiv.Result.Author('Abey Bruck')]","Few-shot learning is an important, but challenging problem of machine
learning aimed at learning from only fewer labeled training examples. It has
become an active area of research due to deep learning requiring huge amounts
of labeled dataset, which is not feasible in the real world. Learning from a
few examples is also an important attempt towards learning like humans.
Few-shot learning has proven a very good promise in different areas of machine
learning applications, particularly in image classification. As it is a recent
technique, most researchers focus on understanding and solving the issues
related to its concept by focusing only on common image datasets like
Mini-ImageNet and Omniglot. Few-shot learning also opens an opportunity to
address low resource languages like Amharic. In this study, offline handwritten
Amharic character recognition using few-shot learning is addressed.
Particularly, prototypical networks, the popular and simpler type of few-shot
learning, is implemented as a baseline. Using the opportunities explored in the
nature of Amharic alphabet having row-wise and column-wise similarities, a
novel way of augmenting the training episodes is proposed. The experimental
results show that the proposed method outperformed the baseline method. This
study has implemented few-shot learning for Amharic characters for the first
time. More importantly, the findings of the study open new ways of examining
the influence of training episodes in few-shot learning, which is one of the
important issues that needs exploration. The datasets used for this study are
collected from native Amharic language writers using an Android App developed
as a part of this study.",0.07700738,-0.27031052,-0.15969515,C
11959,"As the data for
supervised models is not easily available, an unsupervised approach for cleaning documents
can be a topic of further research.","The same model architecture can be used for solving problems
like image denoising tasks like (S&P) noise, deblurring, de-raining, etc.","Researchers can also explore training the proposed model in
an unsupervised way similar to GANs.",2022-10-03 04:23:25+00:00,EraseNet: A Recurrent Residual Network for Supervised Document Cleaning,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Yashowardhan Shinde'), arxiv.Result.Author('Kishore Kulkarni')]","Document denoising is considered one of the most challenging tasks in
computer vision. There exist millions of documents that are still to be
digitized, but problems like document degradation due to natural and man-made
factors make this task very difficult. This paper introduces a supervised
approach for cleaning dirty documents using a new fully convolutional
auto-encoder architecture. This paper focuses on restoring documents with
discrepancies like deformities caused due to aging of a document, creases left
on the pages that were xeroxed, random black patches, lightly visible text,
etc., and also improving the quality of the image for better optical character
recognition system (OCR) performance. Removing noise from scanned documents is
a very important step before the documents as this noise can severely affect
the performance of an OCR system. The experiments in this paper have shown
promising results as the model is able to learn a variety of ordinary as well
as unusual noises and rectify them efficiently.",-0.057863854,-0.16794933,0.020169409,C
11974,We further study the impact of the three sampling strategies (cf.,Sampling Strategy.,"¬ß3.2) for the
computation of Linter_mask, namely dense sampling (storing all the pixels of each image into the
memory), sparse sampling (randomly sampling a small set, e.g., 0.5k or 1.0k, of pixels from each
image), and instance-balanced sampling (randomly sampling a small Ô¨Åxed-size set, e.g., 10 or 50, of
pixels from each instance).",2022-10-03 13:14:00+00:00,Learning Equivariant Segmentation with Instance-Unique Querying,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('James Liang'), arxiv.Result.Author('Dongfang Liu')]","Prevalent state-of-the-art instance segmentation methods fall into a
query-based scheme, in which instance masks are derived by querying the image
feature using a set of instance-aware embeddings. In this work, we devise a new
training framework that boosts query-based models through discriminative query
embedding learning. It explores two essential properties, namely dataset-level
uniqueness and transformation equivariance, of the relation between queries and
instances. First, our algorithm uses the queries to retrieve the corresponding
instances from the whole training dataset, instead of only searching within
individual scenes. As querying instances across scenes is more challenging, the
segmenters are forced to learn more discriminative queries for effective
instance separation. Second, our algorithm encourages both image (instance)
representations and queries to be equivariant against geometric
transformations, leading to more robust, instance-query matching. On top of
four famous, query-based models ($i.e.,$ CondInst, SOLOv2, SOTR, and
Mask2Former), our training algorithm provides significant performance gains
($e.g.,$ +1.6 - 3.2 AP) on COCO dataset. In addition, our algorithm promotes
the performance of SOLOv2 by 2.7 AP, on LVISv1 dataset.",0.0703616,0.15819417,0.18131468,A
11975,We further study the impact of the three sampling strategies (cf.,Sampling Strategy.,"¬ß3.2) for the
computation of Linter_mask, namely dense sampling (storing all the pixels of each image into the
memory), sparse sampling (randomly sampling a small set, e.g., 0.5k or 1.0k, of pixels from each
image), and instance-balanced sampling (randomly sampling a small Ô¨Åxed-size set, e.g., 10 or 50, of
pixels from each instance).",2022-10-03 13:14:00+00:00,Learning Equivariant Segmentation with Instance-Unique Querying,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wenguan Wang'), arxiv.Result.Author('James Liang'), arxiv.Result.Author('Dongfang Liu')]","Prevalent state-of-the-art instance segmentation methods fall into a
query-based scheme, in which instance masks are derived by querying the image
feature using a set of instance-aware embeddings. In this work, we devise a new
training framework that boosts query-based models through discriminative query
embedding learning. It explores two essential properties, namely dataset-level
uniqueness and transformation equivariance, of the relation between queries and
instances. First, our algorithm uses the queries to retrieve the corresponding
instances from the whole training dataset, instead of only searching within
individual scenes. As querying instances across scenes is more challenging, the
segmenters are forced to learn more discriminative queries for effective
instance separation. Second, our algorithm encourages both image (instance)
representations and queries to be equivariant against geometric
transformations, leading to more robust, instance-query matching. On top of
four famous, query-based models ($i.e.,$ CondInst, SOLOv2, SOTR, and
Mask2Former), our training algorithm provides significant performance gains
($e.g.,$ +1.6 - 3.2 AP) on COCO dataset. In addition, our algorithm promotes
the performance of SOLOv2 by 2.7 AP, on LVISv1 dataset.",0.0703616,0.15819417,0.18131468,A
11976,"Input: guidance scale s

xT ‚àº N (0, I)

for t in T, T ‚àí 1, ..., 1 do

, Œ£, A ‚Üê Model(xt)

M ‚Üê Mask(‚àöA)                  ‚àö
xÀÜ0 ‚Üê (xt ‚àí 1 ‚àí Œ±¬Øt )/ Œ±¬Øt

xÀú0 ‚Üê ‚àöGaussian-b‚àölur(xÀÜ0)
xÀút ‚Üê Œ±¬ØtxÀú0 + 1 ‚àí Œ±¬Øt

x¬Øt ‚Üê (1 ‚àí M ) xt + M xÀút

‚Üê Model(x¬Øt)

Àú ‚Üê + (1 + s) √ó ( ‚àí )

xt‚àí1 ‚àº N ( ‚àö1Œ±¬Øt (xt ‚àí ‚àö11‚àí‚àíŒ±Œ±¬Øtt Àú), Œ£)

end for

return x0

                                   Appendix D. Limitations and Future Works

Our method has some points that need further research.",Algorithm 1: Pseudo-code of self-attention guided sampling.,"First, it needs double feedforward steps to get the directions.",2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Model Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Following generative adversarial networks (GANs), a de facto standard model
for image generation, denoising diffusion models (DDMs) have been actively
researched and attracted strong attention due to their capability to generate
images with high quality and diversity. However, the way the internal
self-attention mechanism works inside the UNet of DDMs is under-explored. To
unveil them, in this paper, we first investigate the self-attention operations
within the black-boxed diffusion models and build hypotheses. Next, we verify
the hypotheses about the self-attention map by conducting frequency analysis
and testing the relationships with the generated objects. In consequence, we
find out that the attention map is closely related to the quality of generated
images. On the other hand, diffusion guidance methods based on additional
information such as labels are proposed to improve the quality of generated
images. Inspired by these methods, we present label-free guidance based on the
intermediate self-attention map that can guide existing pretrained diffusion
models to generate images with higher fidelity. In addition to the enhanced
sample quality when used alone, we show that the results are further improved
by combining our method with classifier guidance on ImageNet 128x128.",0.10590446,-0.010450606,0.02103448,A
11977,"This will be an
interesting subject to conduct further research on.",2022).,,2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Model Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Following generative adversarial networks (GANs), a de facto standard model
for image generation, denoising diffusion models (DDMs) have been actively
researched and attracted strong attention due to their capability to generate
images with high quality and diversity. However, the way the internal
self-attention mechanism works inside the UNet of DDMs is under-explored. To
unveil them, in this paper, we first investigate the self-attention operations
within the black-boxed diffusion models and build hypotheses. Next, we verify
the hypotheses about the self-attention map by conducting frequency analysis
and testing the relationships with the generated objects. In consequence, we
find out that the attention map is closely related to the quality of generated
images. On the other hand, diffusion guidance methods based on additional
information such as labels are proposed to improve the quality of generated
images. Inspired by these methods, we present label-free guidance based on the
intermediate self-attention map that can guide existing pretrained diffusion
models to generate images with higher fidelity. In addition to the enhanced
sample quality when used alone, we show that the results are further improved
by combining our method with classifier guidance on ImageNet 128x128.",0.22657765,0.07366053,-0.19766852,A
11978,"Input: guidance scale s

xT ‚àº N (0, I)

for t in T, T ‚àí 1, ..., 1 do

, Œ£, A ‚Üê Model(xt)

M ‚Üê Mask(‚àöA)                  ‚àö
xÀÜ0 ‚Üê (xt ‚àí 1 ‚àí Œ±¬Øt )/ Œ±¬Øt

xÀú0 ‚Üê ‚àöGaussian-b‚àölur(xÀÜ0)
xÀút ‚Üê Œ±¬ØtxÀú0 + 1 ‚àí Œ±¬Øt

x¬Øt ‚Üê (1 ‚àí M ) xt + M xÀút

‚Üê Model(x¬Øt)

Àú ‚Üê + (1 + s) √ó ( ‚àí )

xt‚àí1 ‚àº N ( ‚àö1Œ±¬Øt (xt ‚àí ‚àö11‚àí‚àíŒ±Œ±¬Øtt Àú), Œ£)

end for

return x0

                                   Appendix D. Limitations and Future Works

Our method has some points that need further research.",Algorithm 1: Pseudo-code of self-attention guided sampling.,"First, it needs double feedforward steps to get the directions.",2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Models Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Following generative adversarial networks (GANs), a de facto standard model
for image generation, denoising diffusion models (DDMs) have been actively
researched and attracted strong attention due to their capability to generate
images with high quality and diversity. However, the way the internal
self-attention mechanism works inside the UNet of DDMs is under-explored. To
unveil them, in this paper, we first investigate the self-attention operations
within the black-boxed diffusion models and build hypotheses. Next, we verify
the hypotheses about the self-attention map by conducting frequency analysis
and testing the relationships with the generated objects. In consequence, we
find out that the attention map is closely related to the quality of generated
images. On the other hand, diffusion guidance methods based on additional
information such as labels are proposed to improve the quality of generated
images. Inspired by these methods, we present label-free guidance based on the
intermediate self-attention map that can guide existing pretrained diffusion
models to generate images with higher fidelity. In addition to the enhanced
sample quality when used alone, we show that the results are further improved
by combining our method with classifier guidance on ImageNet 128x128.",0.10590446,-0.010450606,0.02103448,A
11979,"This will be an
interesting subject to conduct further research on.",2022).,,2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Models Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Following generative adversarial networks (GANs), a de facto standard model
for image generation, denoising diffusion models (DDMs) have been actively
researched and attracted strong attention due to their capability to generate
images with high quality and diversity. However, the way the internal
self-attention mechanism works inside the UNet of DDMs is under-explored. To
unveil them, in this paper, we first investigate the self-attention operations
within the black-boxed diffusion models and build hypotheses. Next, we verify
the hypotheses about the self-attention map by conducting frequency analysis
and testing the relationships with the generated objects. In consequence, we
find out that the attention map is closely related to the quality of generated
images. On the other hand, diffusion guidance methods based on additional
information such as labels are proposed to improve the quality of generated
images. Inspired by these methods, we present label-free guidance based on the
intermediate self-attention map that can guide existing pretrained diffusion
models to generate images with higher fidelity. In addition to the enhanced
sample quality when used alone, we show that the results are further improved
by combining our method with classifier guidance on ImageNet 128x128.",0.22657765,0.07366053,-0.19766852,A
11980,"Despite their remarkable performance, DDMs re-           the results further improve by combining our method with
                                        main black boxes on which further study is necessary to take      the conventional guidance scheme.","With this guidance, we observe ap-
                                           Denoising diffusion models (DDMs) have been drawing            parent improvements in a wide range of diffusion models,
                                        much attention for their appreciable sample quality and di-       e.g., ADM, IDDPM, and Stable Diffusion, and show that
                                        versity.","We provide extensive
                                        a profound step.",2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Models Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Denoising diffusion models (DDMs) have been drawing much attention for their
appreciable sample quality and diversity. Despite their remarkable performance,
DDMs remain black boxes on which further study is necessary to take a profound
step. Motivated by this, we delve into the design of conventional U-shaped
diffusion models. More specifically, we investigate the self-attention modules
within these models through carefully designed experiments and explore their
characteristics. In addition, inspired by the studies that substantiate the
effectiveness of the guidance schemes, we present plug-and-play diffusion
guidance, namely Self-Attention Guidance (SAG), that can drastically boost the
performance of existing diffusion models. Our method, SAG, extracts the
intermediate attention map from a diffusion model at every iteration and
selects tokens above a certain attention score for masking and blurring to
obtain a partially blurred input. Subsequently, we measure the dissimilarity
between the predicted noises obtained from feeding the blurred and original
input to the diffusion model and leverage it as guidance. With this guidance,
we observe apparent improvements in a wide range of diffusion models, e.g.,
ADM, IDDPM, and Stable Diffusion, and show that the results further improve by
combining our method with the conventional guidance scheme. We provide
extensive ablation studies to verify our choices.",0.3359363,0.08061147,0.15641971,A
11981,"However,
                                        SAG, extracts the intermediate attention map from a dif-          the mechanism behind this performance is relatively under-
                                        fusion model at every iteration and selects tokens above a        explored, and DDMs remain black boxes that necessitate
                                        certain attention score for masking and blurring to obtain a      further study to take a profound step.","Our method,             in synthesizing high-quality and diverse images.",partially blurred input.,2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Models Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Denoising diffusion models (DDMs) have been drawing much attention for their
appreciable sample quality and diversity. Despite their remarkable performance,
DDMs remain black boxes on which further study is necessary to take a profound
step. Motivated by this, we delve into the design of conventional U-shaped
diffusion models. More specifically, we investigate the self-attention modules
within these models through carefully designed experiments and explore their
characteristics. In addition, inspired by the studies that substantiate the
effectiveness of the guidance schemes, we present plug-and-play diffusion
guidance, namely Self-Attention Guidance (SAG), that can drastically boost the
performance of existing diffusion models. Our method, SAG, extracts the
intermediate attention map from a diffusion model at every iteration and
selects tokens above a certain attention score for masking and blurring to
obtain a partially blurred input. Subsequently, we measure the dissimilarity
between the predicted noises obtained from feeding the blurred and original
input to the diffusion model and leverage it as guidance. With this guidance,
we observe apparent improvements in a wide range of diffusion models, e.g.,
ADM, IDDPM, and Stable Diffusion, and show that the results further improve by
combining our method with the conventional guidance scheme. We provide
extensive ablation studies to verify our choices.",-0.09007507,-0.099082015,0.1291607,C
11982,"This will be an inspiring subject
to conduct further research on.","In addition, self-attention-based guidance may Ô¨Åt well in the settings of discrete diffusion models [8, 33], which estimate
the direct probability of tokens compared to the continuous counterparts that approximate it.",22,2022-10-03 13:50:58+00:00,Improving Sample Quality of Diffusion Models Using Self-Attention Guidance,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Susung Hong'), arxiv.Result.Author('Gyuseong Lee'), arxiv.Result.Author('Wooseok Jang'), arxiv.Result.Author('Seungryong Kim')]","Denoising diffusion models (DDMs) have been drawing much attention for their
appreciable sample quality and diversity. Despite their remarkable performance,
DDMs remain black boxes on which further study is necessary to take a profound
step. Motivated by this, we delve into the design of conventional U-shaped
diffusion models. More specifically, we investigate the self-attention modules
within these models through carefully designed experiments and explore their
characteristics. In addition, inspired by the studies that substantiate the
effectiveness of the guidance schemes, we present plug-and-play diffusion
guidance, namely Self-Attention Guidance (SAG), that can drastically boost the
performance of existing diffusion models. Our method, SAG, extracts the
intermediate attention map from a diffusion model at every iteration and
selects tokens above a certain attention score for masking and blurring to
obtain a partially blurred input. Subsequently, we measure the dissimilarity
between the predicted noises obtained from feeding the blurred and original
input to the diffusion model and leverage it as guidance. With this guidance,
we observe apparent improvements in a wide range of diffusion models, e.g.,
ADM, IDDPM, and Stable Diffusion, and show that the results further improve by
combining our method with the conventional guidance scheme. We provide
extensive ablation studies to verify our choices.",0.28352278,-0.067331165,-0.0641287,A
11983,"Analysis and Discussion                                                                    S=128  baseline   10.4M    18.2     30.8  86.4    22.0     46.9    39.9
                                                                                                     F =1       0.76M    18.5     30.6  88.9    22.5     47.1    40.5
   With a successful demonstration of the power of prompt                                            F =4       1.30M    18.1     31.5  88.0    23.3     48.2    38.0
tuning for generative transfer learning, we further study                                            F =16      3.39M    17.9     30.8  86.5    22.6     47.4    37.7
to understand prompt representations (Sec.","baseline   1.81M    18.6     34.6  89.1    23.8     50.9    41.7
                                                                                                     F =1       0.68M    18.6     36.1  89.5    25.2     51.9    41.5
                                                                                              S=16   F =4       0.95M    18.6     35.5  88.4    24.4     51.5    41.4
                                                                                                     F =16      2.02M    18.5     35.0  86.8    24.3     50.8    40.4

5.","5.1, Sec.",2022-10-03 14:56:05+00:00,Visual Prompt Tuning for Generative Transfer Learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Kihyuk Sohn'), arxiv.Result.Author('Yuan Hao'), arxiv.Result.Author('Jos√© Lezama'), arxiv.Result.Author('Luisa Polania'), arxiv.Result.Author('Huiwen Chang'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Irfan Essa'), arxiv.Result.Author('Lu Jiang')]","Transferring knowledge from an image synthesis model trained on a large
dataset is a promising direction for learning generative image models from
various domains efficiently. While previous works have studied GAN models, we
present a recipe for learning vision transformers by generative knowledge
transfer. We base our framework on state-of-the-art generative vision
transformers that represent an image as a sequence of visual tokens to the
autoregressive or non-autoregressive transformers. To adapt to a new domain, we
employ prompt tuning, which prepends learnable tokens called prompt to the
image token sequence, and introduce a new prompt design for our task. We study
on a variety of visual domains, including visual task adaptation
benchmark~\cite{zhai2019large}, with varying amount of training images, and
show effectiveness of knowledge transfer and a significantly better image
generation quality over existing works.",0.41865718,-0.2431325,-0.008808574,A
11987,"We hope our work could inspire further research efforts into exploring how to
accelerate large-scale vision transformers for dense prediction tasks without Ô¨Åne-tuning.","In light of the relatively high costs associated
with re-training or Ô¨Åne-tuning large vision transformer models on various dense prediction tasks, our
study provides a very lightweight method for expediting the inference process while requiring no
additional Ô¨Åne-tuning.","Acknowledgement

This work is partially supported by the National Nature Science Foundation of China under Grant
62071013 and 61671027, and National Key R&D Program of China under Grant 2018AAA0100300.",2022-10-03 15:49:48+00:00,Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Weicong Liang'), arxiv.Result.Author('Yuhui Yuan'), arxiv.Result.Author('Henghui Ding'), arxiv.Result.Author('Xiao Luo'), arxiv.Result.Author('Weihong Lin'), arxiv.Result.Author('Ding Jia'), arxiv.Result.Author('Zheng Zhang'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Han Hu')]","Vision transformers have recently achieved competitive results across various
vision tasks but still suffer from heavy computation costs when processing a
large number of tokens. Many advanced approaches have been developed to reduce
the total number of tokens in large-scale vision transformers, especially for
image classification tasks. Typically, they select a small group of essential
tokens according to their relevance with the class token, then fine-tune the
weights of the vision transformer. Such fine-tuning is less practical for dense
prediction due to the much heavier computation and GPU memory cost than image
classification. In this paper, we focus on a more challenging problem, i.e.,
accelerating large-scale vision transformers for dense prediction without any
additional re-training or fine-tuning. In response to the fact that
high-resolution representations are necessary for dense prediction, we present
two non-parametric operators, a token clustering layer to decrease the number
of tokens and a token reconstruction layer to increase the number of tokens.
The following steps are performed to achieve this: (i) we use the token
clustering layer to cluster the neighboring tokens together, resulting in
low-resolution representations that maintain the spatial structures; (ii) we
apply the following transformer layers only to these low-resolution
representations or clustered tokens; and (iii) we use the token reconstruction
layer to re-create the high-resolution representations from the refined
low-resolution representations. The results obtained by our method are
promising on five dense prediction tasks, including object detection, semantic
segmentation, panoptic segmentation, instance segmentation, and depth
estimation.",-0.12313552,-0.0030341316,0.26146102,B
12000,"Code and data will be released to
                                                  facilitate further research.","We demonstrate the
                                                  ability of SinGRAV in generating plausible and diverse variations from a single
                                                  scene, the merits of SinGRAV over state-of-the-art generative neural scene methods,
                                                  as well as the versatility of SinGRAV by its use in a variety of applications, spanning
                                                  3D scene editing, composition, and animation.","1 Introduction

                                       Recently, 3D generative modeling has made great strides via gravitating towards neural scene repre-
                                       sentations, which boast unprecedentedly photo-realism.",2022-10-03 19:38:14+00:00,SinGRAV: Learning a Generative Radiance Volume from a Single Natural Scene,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yujie Wang'), arxiv.Result.Author('Xuelin Chen'), arxiv.Result.Author('Baoquan Chen')]","We present a 3D generative model for general natural scenes. Lacking
necessary volumes of 3D data characterizing the target scene, we propose to
learn from a single scene. Our key insight is that a natural scene often
contains multiple constituents whose geometry, texture, and spatial
arrangements follow some clear patterns, but still exhibit rich variations over
different regions within the same scene. This suggests localizing the learning
of a generative model on substantial local regions. Hence, we exploit a
multi-scale convolutional network, which possesses the spatial locality bias in
nature, to learn from the statistics of local regions at multiple scales within
a single scene. In contrast to existing methods, our learning setup bypasses
the need to collect data from many homogeneous 3D scenes for learning common
features. We coin our method SinGRAV, for learning a Generative RAdiance Volume
from a Single natural scene. We demonstrate the ability of SinGRAV in
generating plausible and diverse variations from a single scene, the merits of
SinGRAV over state-of-the-art generative neural scene methods, as well as the
versatility of SinGRAV by its use in a variety of applications, spanning 3D
scene editing, composition, and animation. Code and data will be released to
facilitate further research.",-0.25829005,-0.010877818,0.091012515,C
12001,"Code and data will be released to
                                                  facilitate further research.","We demonstrate the
                                                  ability of SinGRAV in generating plausible and diverse variations from a single
                                                  scene, the merits of SinGRAV over state-of-the-art generative neural scene methods,
                                                  as well as the versatility of SinGRAV by its use in a variety of applications, spanning
                                                  3D scene editing, composition, and animation.","1 Introduction

                                       Recently, 3D generative modeling has made great strides via gravitating towards neural scene
                                       representations, which boast unprecedentedly photo-realism.",2022-10-03 19:38:14+00:00,SinGRAV: Learning a Generative Radiance Volume from a Single Natural Scene,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yujie Wang'), arxiv.Result.Author('Xuelin Chen'), arxiv.Result.Author('Baoquan Chen')]","We present a 3D generative model for general natural scenes. Lacking
necessary volumes of 3D data characterizing the target scene, we propose to
learn from a single scene. Our key insight is that a natural scene often
contains multiple constituents whose geometry, texture, and spatial
arrangements follow some clear patterns, but still exhibit rich variations over
different regions within the same scene. This suggests localizing the learning
of a generative model on substantial local regions. Hence, we exploit a
multi-scale convolutional network, which possesses the spatial locality bias in
nature, to learn from the statistics of local regions at multiple scales within
a single scene. In contrast to existing methods, our learning setup bypasses
the need to collect data from many homogeneous 3D scenes for learning common
features. We coin our method SinGRAV, for learning a Generative RAdiance Volume
from a Single natural scene. We demonstrate the ability of SinGRAV in
generating plausible and diverse variations from a single scene, the merits of
SinGRAV over state-of-the-art generative neural scene methods, as well as the
versatility of SinGRAV by its use in a variety of applications, spanning 3D
scene editing, composition, and animation. Code and data will be released to
facilitate further research.",-0.28203198,-0.004449655,0.08997059,C
12002,"Code and data will be released to
                                                   facilitate further research.","We demonstrate the
                                                   ability of SinGRAV in generating plausible and diverse variations from a single
                                                   scene, the merits of SinGRAV over state-of-the-art generative neural scene methods,
                                                   as well as the versatility of SinGRAV by its use in a variety of applications, spanning
                                                   3D scene editing, composition, and animation.","1 Introduction

                                        Recently, 3D generative modeling has made great strides via gravitating towards neural scene repre-
                                        sentations, which boast unprecedentedly photo-realism.",2022-10-03 19:38:14+00:00,SinGRAV: Learning a Generative Radiance Volume from a Single Natural Scene,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yujie Wang'), arxiv.Result.Author('Xuelin Chen'), arxiv.Result.Author('Baoquan Chen')]","We present a 3D generative model for general natural scenes. Lacking
necessary volumes of 3D data characterizing the target scene, we propose to
learn from a single scene. Our key insight is that a natural scene often
contains multiple constituents whose geometry, texture, and spatial
arrangements follow some clear patterns, but still exhibit rich variations over
different regions within the same scene. This suggests localizing the learning
of a generative model on substantial local regions. Hence, we exploit a
multi-scale convolutional network, which possesses the spatial locality bias in
nature, to learn from the statistics of local regions at multiple scales within
a single scene. In contrast to existing methods, our learning setup bypasses
the need to collect data from many homogeneous 3D scenes for learning common
features. We coin our method SinGRAV, for learning a Generative RAdiance Volume
from a Single natural scene. We demonstrate the ability of SinGRAV in
generating plausible and diverse variations from a single scene, the merits of
SinGRAV over state-of-the-art generative neural scene methods, as well as the
versatility of SinGRAV by its use in a variety of applications, spanning 3D
scene editing, composition, and animation. Code and data will be released to
facilitate further research.",-0.25829005,-0.010877818,0.091012515,C
12003,This warrants further study that we leave as future work.,"In that case, PDA methods could be
tweaked.","Finally, we have also investigated
a smaller labeled target set of 50 random samples (50-RND) instead of 100 random samples.",2022-10-03 20:01:29+00:00,A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']","[arxiv.Result.Author('Tiago Salvador'), arxiv.Result.Author('Kilian Fatras'), arxiv.Result.Author('Ioannis Mitliagkas'), arxiv.Result.Author('Adam Oberman')]","Unsupervised Domain Adaptation (UDA) aims at classifying unlabeled target
images leveraging source labeled ones. In this work, we consider the Partial
Domain Adaptation (PDA) variant, where we have extra source classes not present
in the target domain. Most successful algorithms use model selection strategies
that rely on target labels to find the best hyper-parameters and/or models
along training. However, these strategies violate the main assumption in PDA:
only unlabeled target domain samples are available. Moreover, there are also
inconsistencies in the experimental settings - architecture, hyper-parameter
tuning, number of runs - yielding unfair comparisons. The main goal of this
work is to provide a realistic evaluation of PDA methods with the different
model selection strategies under a consistent evaluation protocol. We evaluate
7 representative PDA algorithms on 2 different real-world datasets using 7
different model selection strategies. Our two main findings are: (i) without
target labels for model selection, the accuracy of the methods decreases up to
30 percentage points; (ii) only one method and model selection pair performs
well on both datasets. Experiments were performed with our PyTorch framework,
BenchmarkPDA, which we open source.",0.27509215,0.023531431,-0.072451994,A
12007,"We adopt
paired RGBD images in various environments, which paves            the non-parametric approach mentioned above for body
the way for further research on combing mmWave radars              reconstruction.","positions of all the joints and vertices of a parameterized
[9] present a large-scale mmWave human body dataset with           body representation (SMPL-X [17] in this work).","As our focus is reconstruction, we use the
with RGBD cameras for 3D body reconstruction.",2022-10-04 03:30:18+00:00,ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Anjun Chen'), arxiv.Result.Author('Xiangyu Wang'), arxiv.Result.Author('Kun Shi'), arxiv.Result.Author('Shaohao Zhu'), arxiv.Result.Author('Yingfeng Chen'), arxiv.Result.Author('Bin Fang'), arxiv.Result.Author('Jiming Chen'), arxiv.Result.Author('Yuchi Huo'), arxiv.Result.Author('Qi Ye')]","3D human reconstruction from RGB images achieves decent results in good
weather conditions but degrades dramatically in rough weather. Complementary,
mmWave radars have been employed to reconstruct 3D human joints and meshes in
rough weather. However, combining RGB and mmWave signals for robust all-weather
3D human reconstruction is still an open challenge, given the sparse nature of
mmWave and the vulnerability of RGB images. In this paper, we present
ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies
in all weather conditions robustly. Specifically, our ImmFusion consists of
image and point backbones for token feature extraction and a Transformer module
for token fusion. The image and point backbones refine global and local
features from original data, and the Fusion Transformer Module aims for
effective information fusion of two modalities by dynamically selecting
informative tokens. Extensive experiments on a large-scale dataset, mmBody,
captured in various environments demonstrate that ImmFusion can efficiently
utilize the information of two modalities to achieve a robust 3D human body
reconstruction in all weather conditions. In addition, our method's accuracy is
significantly superior to that of state-of-the-art Transformer-based
LiDAR-camera fusion methods.",-0.06652211,0.40241095,0.06968529,B
12008,"ing further research speciÔ¨Åcally targeting uncertainty-aware
                                                                                                  LPR.","Deep Ensembles
be sufÔ¨Åcient for uncertainty-aware LPR, particularly when                                         also consistently outperforms state-of-the-art image retrieval
operating in novel environments, where epistemic uncertainty                                      techniques that estimate only aleatoric uncertainty, motivat-
is more relevant [10], [11].","Future work could also explore how uncertainty-aware
C. Ensemble Ablation                                                                              LPR can be used to infer uncertainty in robot pose estimation
                                                                                                  for loop-closure detection.",2022-10-04 04:06:44+00:00,Uncertainty-Aware Lidar Place Recognition in Novel Environments,cs.CV,['cs.CV'],"[arxiv.Result.Author('Keita Mason'), arxiv.Result.Author('Joshua Knights'), arxiv.Result.Author('Milad Ramezani'), arxiv.Result.Author('Peyman Moghadam'), arxiv.Result.Author('Dimity Miller')]","State-of-the-art approaches to lidar place recognition degrade significantly
when tested on novel environments that are not present in their training
dataset. To improve their reliability, we propose uncertainty-aware lidar place
recognition, where each predicted place match must have an associated
uncertainty that can be used to identify and reject potentially incorrect
matches. We introduce a novel evaluation protocol designed to benchmark
uncertainty-aware lidar place recognition, and present Deep Ensembles as the
first uncertainty-aware approach for this task. Testing across three
large-scale datasets and three state-of-the-art architectures, we show that
Deep Ensembles consistently improves the performance of lidar place recognition
in novel environments. Compared to a standard network, our results show that
Deep Ensembles improves the Recall@1 by more than 5% and AuPR by more than 3%
on average when tested on previously unseen environments. Our code repository
will be made publicly available upon paper acceptance at
https://github.com/csiro-robotics/Uncertainty-LPR.",-0.11201728,0.09893632,-0.07652931,B
12023,"Spatial heatmaps
proposed approach, establishes a high-quality baseline for        allow for interpretable predictions of affordances that are
further research on this problem.","This large-scale dataset, along with our     Affordance prediction as heatmaps.",easy to use in downstream applications.,2022-10-04 17:49:23+00:00,COPILOT: Human Collision Prediction and Localization from Multi-view Egocentric Videos,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Boxiao Pan'), arxiv.Result.Author('Bokui Shen'), arxiv.Result.Author('Davis Rempe'), arxiv.Result.Author('Despoina Paschalidou'), arxiv.Result.Author('Kaichun Mo'), arxiv.Result.Author('Yanchao Yang'), arxiv.Result.Author('Leonidas J. Guibas')]","To produce safe human motions, assistive wearable exoskeletons must be
equipped with a perception system that enables anticipating potential
collisions from egocentric observations. However, previous approaches to
exoskeleton perception greatly simplify the problem to specific types of
environments, limiting their scalability. In this paper, we propose the
challenging and novel problem of predicting human-scene collisions for diverse
environments from multi-view egocentric RGB videos captured from an
exoskeleton. By classifying which body joints will collide with the environment
and predicting a collision region heatmap that localizes potential collisions
in the environment, we aim to develop an exoskeleton perception system that
generalizes to complex real-world scenes and provides actionable outputs for
downstream control. We propose COPILOT, a video transformer-based model that
performs both collision prediction and localization simultaneously, leveraging
multi-view video inputs via a proposed joint space-time-viewpoint attention
operation. To train and evaluate the model, we build a synthetic data
generation framework to simulate virtual humans moving in photo-realistic 3D
environments. This framework is then used to establish a dataset consisting of
8.6M egocentric RGBD frames to enable future work on the problem. Extensive
experiments suggest that our model achieves promising performance and
generalizes to unseen scenes as well as real world. We apply COPILOT to a
downstream collision avoidance task, and successfully reduce collision cases by
29% on unseen scenes using a simple closed-loop control algorithm.",-0.16194394,0.10159198,-0.15707964,B
12024,"We
further study the effect of random label in Appendix A, showing in a nutshell that variances
of 0.1%/0.01% settings are ¬±0.93/¬±0.32 ‚Äì far smaller than the baseline gaps (+4.18/+3.13).","In fact, reporting for the latter only mIoU for ‚Äòclasses in labels‚Äô, SalsaNext
and Ours get respectively, 45.50% and 47.61% which again demonstrates robustness.","5 Conclusion

In this paper, we present a weakly supervised LiDAR point cloud semantic segmentation
framework.",2022-10-04 17:54:53+00:00,COARSE3D: Class-Prototypes for Contrastive Learning in Weakly-Supervised 3D Point Cloud Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Rong Li'), arxiv.Result.Author('Anh-Quan Cao'), arxiv.Result.Author('Raoul de Charette')]","Annotation of large-scale 3D data is notoriously cumbersome and costly. As an
alternative, weakly-supervised learning alleviates such a need by reducing the
annotation by several order of magnitudes. We propose COARSE3D, a novel
architecture-agnostic contrastive learning strategy for 3D segmentation. Since
contrastive learning requires rich and diverse examples as keys and anchors, we
leverage a prototype memory bank capturing class-wise global dataset
information efficiently into a small number of prototypes acting as keys. An
entropy-driven sampling technique then allows us to select good pixels from
predictions as anchors. Experiments on three projection-based backbones show we
outperform baselines on three challenging real-world outdoor datasets, working
with as low as 0.001% annotations.",-0.19408873,0.036118176,-0.012190215,B
12025,"We
further study the effect of random label in Appendix A, showing in a nutshell that variances
of 0.1%/0.01% settings are ¬±0.93/¬±0.32 ‚Äì far smaller than the baseline gaps (+4.18/+3.13).","In fact, reporting for the latter only mIoU for ‚Äòclasses in labels‚Äô, SalsaNext
and Ours get respectively, 45.50% and 47.61% which again demonstrates robustness.","5 Conclusion

In this paper, we present a weakly supervised LiDAR point cloud semantic segmentation
framework.",2022-10-04 17:54:53+00:00,COARSE3D: Class-Prototypes for Contrastive Learning in Weakly-Supervised 3D Point Cloud Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Rong Li'), arxiv.Result.Author('Anh-Quan Cao'), arxiv.Result.Author('Raoul de Charette')]","Annotation of large-scale 3D data is notoriously cumbersome and costly. As an
alternative, weakly-supervised learning alleviates such a need by reducing the
annotation by several order of magnitudes. We propose COARSE3D, a novel
architecture-agnostic contrastive learning strategy for 3D segmentation. Since
contrastive learning requires rich and diverse examples as keys and anchors, we
leverage a prototype memory bank capturing class-wise global dataset
information efficiently into a small number of prototypes acting as keys. An
entropy-driven sampling technique then allows us to select good pixels from
predictions as anchors. Experiments on three projection-based backbones show we
outperform baselines on three challenging real-world outdoor datasets, working
with as low as 0.001% annotations.",-0.19408873,0.036118176,-0.012190215,B
12055,"Comparing
the two variants demonstrates that an appropriately initialized prompt consistently outperforms a
randomly initialized prompt, highlighting the necessity for further research of the prompt space.",Table 5 summarizes this ablation.,Table 5: Prompt initialization.,2022-10-05 17:05:56+00:00,Variational prompt tuning improves generalization of vision-language models,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mohammad Mahdi Derakhshani'), arxiv.Result.Author('Enrique Sanchez'), arxiv.Result.Author('Adrian Bulat'), arxiv.Result.Author('Victor Guilherme Turrisi da Costa'), arxiv.Result.Author('Cees G. M. Snoek'), arxiv.Result.Author('Georgios Tzimiropoulos'), arxiv.Result.Author('Brais Martinez')]","Prompt tuning provides an efficient mechanism to adapt large vision-language
models to downstream tasks by treating part of the input language prompts as
learnable parameters while freezing the rest of the model. Existing works for
prompt tuning are however prone to damaging the generalization capabilities of
the foundation models, because the learned prompts lack the capacity of
covering certain concepts within the language model. To avoid such limitation,
we propose a probabilistic modeling of the underlying distribution of prompts,
allowing prompts within the support of an associated concept to be derived
through stochastic sampling. This results in a more complete and richer
transfer of the information captured by the language model, providing better
generalization capabilities for downstream tasks. The resulting algorithm
relies on a simple yet powerful variational framework that can be directly
integrated with other developments. We show our approach is seamlessly
integrated into both standard and conditional prompt learning frameworks,
improving the performance on both cases considerably, especially with regards
to preserving the generalization capability of the original model. Our method
provides the current state-of-the-art for prompt learning, surpassing CoCoOp by
1.6% average Top-1 accuracy on the standard benchmark. Remarkably, it even
surpasses the original CLIP model in terms of generalization to new classes.
Implementation code will be released.",0.46691692,-0.024511158,-0.2911338,A
12073,"The ease of transfer from dense to sparse networks and the potential gains in runtime will hopefully
incite further research into these promising technologies.","On the other hand, we
found that the runtime performance of the evaluated frameworks cannot yet match dense networks,
and especially the asynchronous framework can at this point only be seen as a proof-of-concept.","We believe the evaluated sparse CNN
frameworks to be limited by code inefficiencies and lacking hardware support, but in theory to be
a viable optimization of CNNs.",2022-10-05 23:51:39+00:00,Transferring dense object detection models to event-based data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Vincenz Mechler'), arxiv.Result.Author('Pavel Rojtberg')]","Event-based image representations are fundamentally different to traditional
dense images. This poses a challenge to apply current state-of-the-art models
for object detection as they are designed for dense images. In this work we
evaluate the YOLO object detection model on event data. To this end we replace
dense-convolution layers by either sparse convolutions or asynchronous sparse
convolutions which enables direct processing of event-based images and compare
the performance and runtime to feeding event-histograms into
dense-convolutions. Here, hyper-parameters are shared across all variants to
isolate the effect sparse-representation has on detection performance.
  At this, we show that current sparse-convolution implementations cannot
translate their theoretical lower computation requirements into an improved
runtime.",-0.03876644,-0.15377152,0.37779886,C
12074,"And there is a demand for implicit transferability in
                                       further research because adversarial training depends on speciÔ¨Åc attack algorithms for augmented
                                       data[9], which makes the defender hard to and appear passive in the arms race.","Nevertheless, previous research has testiÔ¨Åed the widespread
                                       transferability of adversarial examples[6, 7, 8].","On the contrary,
                                       a rational ensemble strategy is an effective defense method in practice[10, 11].",2022-10-06 00:33:19+00:00,Dynamic Stochastic Ensemble with Adversarial Robust Lottery Ticket Subnetworks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qi Peng'), arxiv.Result.Author('Wenlin Liu'), arxiv.Result.Author('Ruoxi Qin'), arxiv.Result.Author('Libin Hou'), arxiv.Result.Author('Bin Yan'), arxiv.Result.Author('Linyuan Wang')]","Adversarial attacks are considered the intrinsic vulnerability of CNNs.
Defense strategies designed for attacks have been stuck in the adversarial
attack-defense arms race, reflecting the imbalance between attack and defense.
Dynamic Defense Framework (DDF) recently changed the passive safety status quo
based on the stochastic ensemble model. The diversity of subnetworks, an
essential concern in the DDF, can be effectively evaluated by the adversarial
transferability between different networks. Inspired by the poor adversarial
transferability between subnetworks of scratch tickets with various remaining
ratios, we propose a method to realize the dynamic stochastic ensemble defense
strategy. We discover the adversarial transferable diversity between robust
lottery ticket subnetworks drawn from different basic structures and sparsity.
The experimental results suggest that our method achieves better robust and
clean recognition accuracy by adversarial transferable diversity, which would
decrease the reliability of attacks.",0.088155866,-0.22848256,0.026282452,C
12086,"A review and comparative
                                                                study on probabilistic object detection in
        Additionally, further research is needed that           autonomous driving.","mayer, K. (2021).","Transactions on Intel-
    focuses on the class-imbalance problem in                   ligent Transportation Systems.",2022-10-06 14:06:36+00:00,A Review of Uncertainty Calibration in Pretrained Object Detectors,cs.CV,"['cs.CV', 'I.4.0; I.5.0']","[arxiv.Result.Author('Denis Huseljic'), arxiv.Result.Author('Marek Herde'), arxiv.Result.Author('Mehmet Muejde'), arxiv.Result.Author('Bernhard Sick')]","In the field of deep learning based computer vision, the development of deep
object detection has led to unique paradigms (e.g., two-stage or set-based) and
architectures (e.g., Faster-RCNN or DETR) which enable outstanding performance
on challenging benchmark datasets. Despite this, the trained object detectors
typically do not reliably assess uncertainty regarding their own knowledge, and
the quality of their probabilistic predictions is usually poor. As these are
often used to make subsequent decisions, such inaccurate probabilistic
predictions must be avoided. In this work, we investigate the uncertainty
calibration properties of different pretrained object detection architectures
in a multi-class setting. We propose a framework to ensure a fair, unbiased,
and repeatable evaluation and conduct detailed analyses assessing the
calibration under distributional changes (e.g., distributional shift and
application to out-of-distribution data). Furthermore, by investigating the
influence of different detector paradigms, post-processing steps, and suitable
choices of metrics, we deliver novel insights into why poor detector
calibration emerges. Based on these insights, we are able to improve the
calibration of a detector by simply finetuning its last layer.",-0.19442773,-0.011976769,-0.08486901,B
12092,"These Ô¨Åndings
                                                  motivate large-scale ambiguous dataset creation and further research focusing on
                                                  noisy visual data.1

                                       1 Introduction

                                       When making decisions, the human brain uses perceptual uncertainty judgments to account for
                                       missing visual information and other noise [22, 2, 26].","Experimental results suggest that existing vision models
                                                  are not sufÔ¨Åciently equipped to provide meaningful outputs for ambiguous images
                                                  and that datasets of this nature can be used to assess and improve such models
                                                  through model training and direct evaluation of model calibration.","For instance, when humans enter a new
                                       environment, they must quickly gauge what events are taking place in it using limited sensory
                                       input [53, 80, 79].",2022-10-06 17:52:20+00:00,Ambiguous Images With Human Judgments for Robust Visual Event Classification,cs.CV,"['cs.CV', 'cs.AI', 'I.2.10; I.4.8; I.2.0']","[arxiv.Result.Author('Kate Sanders'), arxiv.Result.Author('Reno Kriz'), arxiv.Result.Author('Anqi Liu'), arxiv.Result.Author('Benjamin Van Durme')]","Contemporary vision benchmarks predominantly consider tasks on which humans
can achieve near-perfect performance. However, humans are frequently presented
with visual data that they cannot classify with 100% certainty, and models
trained on standard vision benchmarks achieve low performance when evaluated on
this data. To address this issue, we introduce a procedure for creating
datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a
collection of noisy images extracted from videos. All images are annotated with
ground truth values and a test set is annotated with human uncertainty
judgments. We use this dataset to characterize human uncertainty in vision
tasks and evaluate existing visual event classification models. Experimental
results suggest that existing vision models are not sufficiently equipped to
provide meaningful outputs for ambiguous images and that datasets of this
nature can be used to assess and improve such models through model training and
direct evaluation of model calibration. These findings motivate large-scale
ambiguous dataset creation and further research focusing on noisy visual data.",-0.24373907,0.0062396936,-0.14966378,B
12093,"These Ô¨Åndings
                                                   motivate large-scale ambiguous dataset creation and further research focusing on
                                                   noisy visual data.1

                                        1 Introduction

                                        When making decisions, the human brain uses perceptual uncertainty judgments to account for
                                        missing visual information and other noise [22, 2, 26].","Experimental results suggest that existing vision models
                                                   are not sufÔ¨Åciently equipped to provide meaningful outputs for ambiguous images
                                                   and that datasets of this nature can be used to assess and improve such models
                                                   through model training and direct evaluation of model calibration.","For instance, when humans enter a new
                                        environment, they must quickly gauge what events are taking place in it using limited sensory
                                        input [53, 80, 79].",2022-10-06 17:52:20+00:00,Ambiguous Images With Human Judgments for Robust Visual Event Classification,cs.CV,"['cs.CV', 'cs.AI', 'I.2.10; I.4.8; I.2.0']","[arxiv.Result.Author('Kate Sanders'), arxiv.Result.Author('Reno Kriz'), arxiv.Result.Author('Anqi Liu'), arxiv.Result.Author('Benjamin Van Durme')]","Contemporary vision benchmarks predominantly consider tasks on which humans
can achieve near-perfect performance. However, humans are frequently presented
with visual data that they cannot classify with 100% certainty, and models
trained on standard vision benchmarks achieve low performance when evaluated on
this data. To address this issue, we introduce a procedure for creating
datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a
collection of noisy images extracted from videos. All images are annotated with
ground truth values and a test set is annotated with human uncertainty
judgments. We use this dataset to characterize human uncertainty in vision
tasks and evaluate existing visual event classification models. Experimental
results suggest that existing vision models are not sufficiently equipped to
provide meaningful outputs for ambiguous images and that datasets of this
nature can be used to assess and improve such models through model training and
direct evaluation of model calibration. These findings motivate large-scale
ambiguous dataset creation and further research focusing on noisy visual data.",-0.24373907,0.0062396936,-0.14966378,B
12109,"This
suggests that the characteristics of the predictions can be more clearly exhibited in the case of the Vision
Transformer than CNNs, which encourages the further study of Vision Transformers based on interactions.",between correctly classiÔ¨Åed and misclassiÔ¨Åed images have a clearer diÔ¨Äerence than those for ResNet-18.,"6 Additional experiments on adversarial attacks

In Section 5, we observed that adversarial perturbations have signiÔ¨Åcant eÔ¨Äects on the distributions of
interactions in middle and high order.",2022-10-07 06:50:02+00:00,Game-Theoretic Understanding of Misclassification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Kosuke Sumiyasu'), arxiv.Result.Author('Kazuhiko Kawamoto'), arxiv.Result.Author('Hiroshi Kera')]","This paper analyzes various types of image misclassification from a
game-theoretic view. Particularly, we consider the misclassification of clean,
adversarial, and corrupted images and characterize it through the distribution
of multi-order interactions. We discover that the distribution of multi-order
interactions varies across the types of misclassification. For example,
misclassified adversarial images have a higher strength of high-order
interactions than correctly classified clean images, which indicates that
adversarial perturbations create spurious features that arise from complex
cooperation between pixels. By contrast, misclassified corrupted images have a
lower strength of low-order interactions than correctly classified clean
images, which indicates that corruptions break the local cooperation between
pixels. We also provide the first analysis of Vision Transformers using
interactions. We found that Vision Transformers show a different tendency in
the distribution of interactions from that in CNNs, and this implies that they
exploit the features that CNNs do not use for the prediction. Our study
demonstrates that the recent game-theoretic analysis of deep learning models
can be broadened to analyze various malfunctions of deep learning models
including Vision Transformers by using the distribution, order, and sign of
interactions.",-0.17867045,-0.19621243,0.20802096,C
12113,"require further study and is shown here as an example
of how computational imaging tasks could be used also              Ethical approval.",This will            suggestions.,"This research was approved by
                                                                the University of Glasgow ethics approval committee,
                                                                application no.300210003.",2022-10-07 08:40:18+00:00,Computational imaging with the human brain,cs.CV,"['cs.CV', 'cs.HC', 'q-bio.NC']","[arxiv.Result.Author('Gao Wang'), arxiv.Result.Author('Daniele Faccio')]","Brain-computer interfaces (BCIs) are enabling a range of new possibilities
and routes for augmenting human capability. Here, we propose BCIs as a route
towards forms of computation, i.e. computational imaging, that blend the brain
with external silicon processing. We demonstrate ghost imaging of a hidden
scene using the human visual system that is combined with an adaptive
computational imaging scheme. This is achieved through a projection pattern
`carving' technique that relies on real-time feedback from the brain to modify
patterns at the light projector, thus enabling more efficient and higher
resolution imaging. This brain-computer connectivity demonstrates a form of
augmented human computation that could in the future extend the sensing range
of human vision and provide new approaches to the study of the neurophysics of
human perception. As an example, we illustrate a simple experiment whereby
image reconstruction quality is affected by simultaneous conscious processing
and readout of the perceived light intensities.",0.10280377,0.1248187,-0.17992681,A
12116,"There is thus a pressing need for a high quality transparent object
training video dataset to answer this question and to potentially unlock the power of deep
learning trackers, as well as to facilitate in-depth analysis and foster further research.","However, it is crucial to note that the
results were obtained without re-training the state-of-the-art trackers on representative train-
ing sets, which opens the question whether these observations are not just a consequence of
the domain shift rather than an inherent property of shallow and deep modern learning-based
tracking architectures.",Construction of the training dataset presents many challenges.,2022-10-07 10:08:13+00:00,Trans2k: Unlocking the Power of Deep Models for Transparent Object Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Alan Lukezic'), arxiv.Result.Author('Ziga Trojer'), arxiv.Result.Author('Jiri Matas'), arxiv.Result.Author('Matej Kristan')]","Visual object tracking has focused predominantly on opaque objects, while
transparent object tracking received very little attention. Motivated by the
uniqueness of transparent objects in that their appearance is directly affected
by the background, the first dedicated evaluation dataset has emerged recently.
We contribute to this effort by proposing the first transparent object tracking
training dataset Trans2k that consists of over 2k sequences with 104,343 images
overall, annotated by bounding boxes and segmentation masks. Noting that
transparent objects can be realistically rendered by modern renderers, we
quantify domain-specific attributes and render the dataset containing visual
attributes and tracking situations not covered in the existing object training
datasets. We observe a consistent performance boost (up to 16%) across a
diverse set of modern tracking architectures when trained using Trans2k, and
show insights not previously possible due to the lack of appropriate training
sets. The dataset and the rendering engine will be publicly released to unlock
the power of modern learning-based trackers and foster new designs in
transparent object tracking.",-0.34371257,-0.042399462,0.011148168,B
12117,"STARK DiMP SiamBAN D3S TransATOM SiamRPN ATOM

Opaque 0.777 0.706 0.679 0.676 0.662               0.656 0.650

+ Trans2k 0.752 0.696 0.676 0.663 0.650            0.656 0.650

4.5 The role of using opaque objects in training

To further study the impact of the training set content from perspective of the presence of
opaque and transparent objects, the training sets were varied.","Opaque

‚Äì training with only OTD, + Trans2k ‚Äì using the transparent dataset as well.","We selected two well-known
state-of-the-art trackers that performed well in our previous experiments, yet could be trained
sufÔ¨Åciently fast.",2022-10-07 10:08:13+00:00,Trans2k: Unlocking the Power of Deep Models for Transparent Object Tracking,cs.CV,['cs.CV'],"[arxiv.Result.Author('Alan Lukezic'), arxiv.Result.Author('Ziga Trojer'), arxiv.Result.Author('Jiri Matas'), arxiv.Result.Author('Matej Kristan')]","Visual object tracking has focused predominantly on opaque objects, while
transparent object tracking received very little attention. Motivated by the
uniqueness of transparent objects in that their appearance is directly affected
by the background, the first dedicated evaluation dataset has emerged recently.
We contribute to this effort by proposing the first transparent object tracking
training dataset Trans2k that consists of over 2k sequences with 104,343 images
overall, annotated by bounding boxes and segmentation masks. Noting that
transparent objects can be realistically rendered by modern renderers, we
quantify domain-specific attributes and render the dataset containing visual
attributes and tracking situations not covered in the existing object training
datasets. We observe a consistent performance boost (up to 16%) across a
diverse set of modern tracking architectures when trained using Trans2k, and
show insights not previously possible due to the lack of appropriate training
sets. The dataset and the rendering engine will be publicly released to unlock
the power of modern learning-based trackers and foster new designs in
transparent object tracking.",-0.019542344,0.11459086,0.0073138736,B
12131,"We recommend further study
of transformer-based DeepFake classiÔ¨Åers and how to employ attention as an explanation.","Finally, we observe that recent architectures such as MViT signiÔ¨Åcantly outperform any of
the S3D variations in both detection and explanation quality.",Ethical statement.,2022-10-07 16:41:46+00:00,Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Federico Baldassarre'), arxiv.Result.Author('Quentin Debard'), arxiv.Result.Author('Gonzalo Fiz Pontiveros'), arxiv.Result.Author('Tri Kurniawan Wijaya')]","The proliferation of DeepFake technology is a rising challenge in today's
society, owing to more powerful and accessible generation methods. To counter
this, the research community has developed detectors of ever-increasing
accuracy. However, the ability to explain the decisions of such models to users
is lacking behind and is considered an accessory in large-scale benchmarks,
despite being a crucial requirement for the correct deployment of automated
tools for content moderation. We attribute the issue to the reliance on
qualitative comparisons and the lack of established metrics. We describe a
simple set of metrics to evaluate the visual quality and informativeness of
explanations of video DeepFake classifiers from a human-centric perspective.
With these metrics, we compare common approaches to improve explanation quality
and discuss their effect on both classification and explanation performance on
the recent DFDC and DFD datasets.",-0.18269688,-0.0923243,0.09715344,C
12141,"R(p‚àó(v)) R(v‚àó)

   3D Viewpoints              88.79% 98%

   2D Transformations 76.93% 85%

C.6 Comparison to adversarial 2D transformations

We further study the performance of ViewFool compared to adversarial 2D transformations.","œà              Œ∏              œÜ                R(p‚àó(v)) R(v‚àó)

[‚àí180‚ó¶, 180‚ó¶]        0‚ó¶             90‚ó¶        66.46%     77%
       0‚ó¶      [‚àí30‚ó¶, 30‚ó¶]          90‚ó¶        59.64%     64%
       0‚ó¶                       [20‚ó¶, 160‚ó¶]    75.21%     81%
                     0‚ó¶       [72.5‚ó¶, 107.5‚ó¶]  68.57%     79%
 [‚àí45‚ó¶, 45‚ó¶]   [‚àí7.5‚ó¶, 7.5‚ó¶]    [55‚ó¶, 125‚ó¶]    77.27%     91%
 [‚àí90‚ó¶, 90‚ó¶]   [‚àí15‚ó¶, 15‚ó¶]      [20‚ó¶, 160‚ó¶]    84.25%     96%
[‚àí180‚ó¶, 180‚ó¶]  [‚àí30‚ó¶, 30‚ó¶]

Table C.3: Comparison to adversarial 2D transformations.","Note that
2D transformations (including 2D rotation, scaling, cropping) are a subset of 3D viewpoint changes
studied in this paper.",2022-10-08 03:06:49+00:00,ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints,cs.CV,"['cs.CV', 'cs.AI', 'cs.CR', 'cs.LG', 'stat.ML']","[arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Shouwei Ruan'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Caixin Kang'), arxiv.Result.Author('Xingxing Wei'), arxiv.Result.Author('Jun Zhu')]","Recent studies have demonstrated that visual recognition models lack
robustness to distribution shift. However, current work mainly considers model
robustness to 2D image transformations, leaving viewpoint changes in the 3D
world less explored. In general, viewpoint changes are prevalent in various
real-world applications (e.g., autonomous driving), making it imperative to
evaluate viewpoint robustness. In this paper, we propose a novel method called
ViewFool to find adversarial viewpoints that mislead visual recognition models.
By encoding real-world objects as neural radiance fields (NeRF), ViewFool
characterizes a distribution of diverse adversarial viewpoints under an
entropic regularizer, which helps to handle the fluctuations of the real camera
pose and mitigate the reality gap between the real objects and their neural
representations. Experiments validate that the common image classifiers are
extremely vulnerable to the generated adversarial viewpoints, which also
exhibit high cross-model transferability. Based on ViewFool, we introduce
ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint
robustness of image classifiers. Evaluation results on 40 classifiers with
diverse architectures, objective functions, and data augmentations reveal a
significant drop in model performance when tested on ImageNet-V, which provides
a possibility to leverage ViewFool as an effective data augmentation strategy
to improve viewpoint robustness.",-0.19705099,0.0677294,0.12931505,B
12157,"Methods which do not rely on prior       feature mapping function that transforms raw input to the
knowledge deserve further study.","F (¬∑) : X √ó Y ‚Üí Rd is a
by other potential factors.",domain-invariant representation space.,2022-10-09 03:41:02+00:00,Constrained Maximum Cross-Domain Likelihood for Domain Generalization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianxin Lin'), arxiv.Result.Author('Yongqiang Tang'), arxiv.Result.Author('Junping Wang'), arxiv.Result.Author('Wensheng Zhang')]","As a recent noticeable topic, domain generalization aims to learn a
generalizable model on multiple source domains, which is expected to perform
well on unseen test domains. Great efforts have been made to learn
domain-invariant features by aligning distributions across domains. However,
existing works are often designed based on some relaxed conditions which are
generally hard to satisfy and fail to realize the desired joint distribution
alignment. In this paper, we propose a novel domain generalization method,
which originates from an intuitive idea that a domain-invariant classifier can
be learned by minimizing the KL-divergence between posterior distributions from
different domains. To enhance the generalizability of the learned classifier,
we formalize the optimization objective as an expectation computed on the
ground-truth marginal distribution. Nevertheless, it also presents two obvious
deficiencies, one of which is the side-effect of entropy increase in
KL-divergence and the other is the unavailability of ground-truth marginal
distributions. For the former, we introduce a term named maximum in-domain
likelihood to maintain the discrimination of the learned domain-invariant
representation space. For the latter, we approximate the ground-truth marginal
distribution with source domains under a reasonable convex hull assumption.
Finally, a Constrained Maximum Cross-domain Likelihood (CMCL) optimization
problem is deduced, by solving which the joint distributions are naturally
aligned. An alternating optimization strategy is carefully designed to
approximately solve this optimization problem. Extensive experiments on four
standard benchmark datasets, i.e., Digits-DG, PACS, Office-Home and
miniDomainNet, highlight the superior performance of our method.",0.011383211,-0.09632242,-0.01053866,C
12166,We hope our method can motivate further research.,"The proposed CRT method has achieved new state-of-the-art metric learning performance
on benchmark datasets.","One limitation is that
the memory and compute usage will be increased during training for these two embedding branches,
and we shared weights between them to solve this problem in our experiments.",2022-10-09 06:17:31+00:00,Coded Residual Transform for Generalizable Deep Metric Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shichao Kan'), arxiv.Result.Author('Yixiong Liang'), arxiv.Result.Author('Min Li'), arxiv.Result.Author('Yigang Cen'), arxiv.Result.Author('Jianxin Wang'), arxiv.Result.Author('Zhihai He')]","A fundamental challenge in deep metric learning is the generalization
capability of the feature embedding network model since the embedding network
learned on training classes need to be evaluated on new test classes. To
address this challenge, in this paper, we introduce a new method called coded
residual transform (CRT) for deep metric learning to significantly improve its
generalization capability. Specifically, we learn a set of diversified
prototype features, project the feature map onto each prototype, and then
encode its features using their projection residuals weighted by their
correlation coefficients with each prototype. The proposed CRT method has the
following two unique characteristics. First, it represents and encodes the
feature map from a set of complimentary perspectives based on projections onto
diversified prototypes. Second, unlike existing transformer-based feature
representation approaches which encode the original values of features based on
global correlation analysis, the proposed coded residual transform encodes the
relative differences between the original features and their projected
prototypes. Embedding space density and spectral decay analysis show that this
multi-perspective projection onto diversified prototypes and coded residual
representation are able to achieve significantly improved generalization
capability in metric learning. Finally, to further enhance the generalization
performance, we propose to enforce the consistency on their feature similarity
matrices between coded residual transforms with different sizes of projection
prototypes and embedding dimensions. Our extensive experimental results and
ablation studies demonstrate that the proposed CRT method outperform the
state-of-the-art deep metric learning methods by large margins and improving
upon the current best method by up to 4.28% on the CUB dataset.",0.103741124,-0.23326899,0.031815924,C
12208,"Note also that the retrieval
                                                                 accuracy of the different metrics greatly drops when com-
1 B Li log Pi                                                    pared to the synthetic in-distribution noise which prompts
lcont = B ‚àí C L ,                            (7)                 further research to improve the detection of web noise when

i=1  c=1 i,c

The Ô¨Ånal training objective we optimize is:

l = lclassif + lcont.",at retrieving noisy web samples.,"(8)
                   1.",2022-10-10 11:32:28+00:00,Is your noise correction noisy? PLS: Robustness to label noise with two stage detection,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Paul Albert'), arxiv.Result.Author('Eric Arazo'), arxiv.Result.Author('Tarun Kirshna'), arxiv.Result.Author(""Noel E. O'Connor""), arxiv.Result.Author('Kevin McGuinness')]","Designing robust algorithms capable of training accurate neural networks on
uncurated datasets from the web has been the subject of much research as it
reduces the need for time consuming human labor. The focus of many previous
research contributions has been on the detection of different types of label
noise; however, this paper proposes to improve the correction accuracy of noisy
samples once they have been detected. In many state-of-the-art contributions, a
two phase approach is adopted where the noisy samples are detected before
guessing a corrected pseudo-label in a semi-supervised fashion. The guessed
pseudo-labels are then used in the supervised objective without ensuring that
the label guess is likely to be correct. This can lead to confirmation bias,
which reduces the noise robustness. Here we propose the pseudo-loss, a simple
metric that we find to be strongly correlated with pseudo-label correctness on
noisy samples. Using the pseudo-loss, we dynamically down weight
under-confident pseudo-labels throughout training to avoid confirmation bias
and improve the network accuracy. We additionally propose to use a confidence
guided contrastive objective that learns robust representation on an
interpolated objective between class bound (supervised) for confidently
corrected samples and unsupervised representation for under-confident label
corrections. Experiments demonstrate the state-of-the-art performance of our
Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets
including curated data synthetically corrupted with in-distribution and
out-of-distribution noise, and two real world web noise datasets. Our
experiments are fully reproducible [github coming soon]",0.2272726,-0.15957505,0.08708213,A
12209,"Note also that the retrieval
                                                                                                                                          accuracy of the different metrics greatly drops when com-
                                                                                                                                          pared to the synthetic in-distribution noise which prompts
                                                                                                                                          further research to improve the detection of web noise when
                   1.","(8)                                   that the cross-entropy loss (small loss) is the most accurate
                                                                                                                                          at retrieving noisy web samples.","CIFAR-100 40% ID
                  .95
                  .90                                    .98                                                                              CE loss
                  .85                                    .96                                                                              agreement
                  .80                                    .94
                  .75                                                                                                                     disagreement
                  .70
                                                                                                                                          kl
                              0                                                                                                           no pseudo loss
                  .70                                                                                                                     with pseudo loss

                                       25                     50              75           100               125      150                 175          200

                                                                                  CNWL 40% Web

                  .65

                  .60 .70                                                                                                                 CE loss

                                                              .68                                                                         agreement

                  .55                                         .66                                                                         disagreement
                                                                                                                                          kl
                                                              .64                                                                         no pseudo loss
                                                                                                                                          with pseudo loss

                                                              .62

                  .50                  25                     50              75           100               125      150                 175          200
                              0

Figure 2.",2022-10-10 11:32:28+00:00,Is your noise correction noisy? PLS: Robustness to label noise with two stage detection,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Paul Albert'), arxiv.Result.Author('Eric Arazo'), arxiv.Result.Author('Tarun Krishna'), arxiv.Result.Author(""Noel E. O'Connor""), arxiv.Result.Author('Kevin McGuinness')]","Designing robust algorithms capable of training accurate neural networks on
uncurated datasets from the web has been the subject of much research as it
reduces the need for time consuming human labor. The focus of many previous
research contributions has been on the detection of different types of label
noise; however, this paper proposes to improve the correction accuracy of noisy
samples once they have been detected. In many state-of-the-art contributions, a
two phase approach is adopted where the noisy samples are detected before
guessing a corrected pseudo-label in a semi-supervised fashion. The guessed
pseudo-labels are then used in the supervised objective without ensuring that
the label guess is likely to be correct. This can lead to confirmation bias,
which reduces the noise robustness. Here we propose the pseudo-loss, a simple
metric that we find to be strongly correlated with pseudo-label correctness on
noisy samples. Using the pseudo-loss, we dynamically down weight
under-confident pseudo-labels throughout training to avoid confirmation bias
and improve the network accuracy. We additionally propose to use a confidence
guided contrastive objective that learns robust representation on an
interpolated objective between class bound (supervised) for confidently
corrected samples and unsupervised representation for under-confident label
corrections. Experiments demonstrate the state-of-the-art performance of our
Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets
including curated data synthetically corrupted with in-distribution and
out-of-distribution noise, and two real world web noise datasets. Our
experiments are fully reproducible github.com/PaulAlbert31/SNCF",0.33755255,-0.017836824,0.14610833,A
12210,"We believe this motivates further research on the
‚úì  ‚úì lclassif  77.84  69.09  57.76                                   harmful impact OOD noise and incorrect pseudo-labels have
                                                                     when training on a web noisy dataset using a supervised
the learning rate is reduced and confirmation bias increases.","For
‚úì  ‚úì  ‚úó                                                              CIFAR-100 corrupted with 40% OOD and 20% ID noise,
                                                                     the accuracy benefits of training the contrastive objective
‚úì  ‚úì  ‚úì                                                              are negated when compared to our noise correction baseline
                                                                     (row 2).",contrastive objective.,2022-10-10 11:32:28+00:00,Is your noise correction noisy? PLS: Robustness to label noise with two stage detection,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Paul Albert'), arxiv.Result.Author('Eric Arazo'), arxiv.Result.Author('Tarun Krishna'), arxiv.Result.Author(""Noel E. O'Connor""), arxiv.Result.Author('Kevin McGuinness')]","Designing robust algorithms capable of training accurate neural networks on
uncurated datasets from the web has been the subject of much research as it
reduces the need for time consuming human labor. The focus of many previous
research contributions has been on the detection of different types of label
noise; however, this paper proposes to improve the correction accuracy of noisy
samples once they have been detected. In many state-of-the-art contributions, a
two phase approach is adopted where the noisy samples are detected before
guessing a corrected pseudo-label in a semi-supervised fashion. The guessed
pseudo-labels are then used in the supervised objective without ensuring that
the label guess is likely to be correct. This can lead to confirmation bias,
which reduces the noise robustness. Here we propose the pseudo-loss, a simple
metric that we find to be strongly correlated with pseudo-label correctness on
noisy samples. Using the pseudo-loss, we dynamically down weight
under-confident pseudo-labels throughout training to avoid confirmation bias
and improve the network accuracy. We additionally propose to use a confidence
guided contrastive objective that learns robust representation on an
interpolated objective between class bound (supervised) for confidently
corrected samples and unsupervised representation for under-confident label
corrections. Experiments demonstrate the state-of-the-art performance of our
Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets
including curated data synthetically corrupted with in-distribution and
out-of-distribution noise, and two real world web noise datasets. Our
experiments are fully reproducible github.com/PaulAlbert31/SNCF",0.19079065,-0.21428734,0.035707682,A
12229,"A systematic benchmark and analysis are
                                                  provided along with the dataset to inspire further research.","Current release of the
                                                  dataset contains 12,860 HD rainy images and 1,286 corresponding HD ground
                                                  truth images in diversiÔ¨Åed night scenes.","1 Introduction

                                        Rain reÔ¨Çects and refracts light and causes visibility degradation, making it necessary to remove
                                        rain streaks from rainy images.",2022-10-10 14:08:09+00:00,GTAV-NightRain: Photometric Realistic Large-scale Dataset for Night-time Rain Streak Removal,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fan Zhang'), arxiv.Result.Author('Shaodi You'), arxiv.Result.Author('Yu Li'), arxiv.Result.Author('Ying Fu')]","Rain is transparent, which reflects and refracts light in the scene to the
camera. In outdoor vision, rain, especially rain streaks degrade visibility and
therefore need to be removed. In existing rain streak removal datasets,
although density, scale, direction and intensity have been considered,
transparency is not fully taken into account. This problem is particularly
serious in night scenes, where the appearance of rain largely depends on the
interaction with scene illuminations and changes drastically on different
positions within the image. This is problematic, because unrealistic dataset
causes serious domain bias. In this paper, we propose GTAV-NightRain dataset,
which is a large-scale synthetic night-time rain streak removal dataset. Unlike
existing datasets, by using 3D computer graphic platform (namely GTA V), we are
allowed to infer the three dimensional interaction between rain and
illuminations, which insures the photometric realness. Current release of the
dataset contains 12,860 HD rainy images and 1,286 corresponding HD ground truth
images in diversified night scenes. A systematic benchmark and analysis are
provided along with the dataset to inspire further research.",-0.11116562,0.20573789,0.05619501,B
12230,"‚Ä¢ A systematical and comprehensive benchmark is provided using the proposed dataset so as
           to inspire further research.","‚Ä¢ The proposed dataset is large scale with diversiÔ¨Åed urban scenes and high graphic quality,
          which minimizes dataset bias.","2 Related work

In this section, we Ô¨Årst brieÔ¨Çy introduce existing deraining datasets and some SOTA methods which
are evaluated on our dataset.",2022-10-10 14:08:09+00:00,GTAV-NightRain: Photometric Realistic Large-scale Dataset for Night-time Rain Streak Removal,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fan Zhang'), arxiv.Result.Author('Shaodi You'), arxiv.Result.Author('Yu Li'), arxiv.Result.Author('Ying Fu')]","Rain is transparent, which reflects and refracts light in the scene to the
camera. In outdoor vision, rain, especially rain streaks degrade visibility and
therefore need to be removed. In existing rain streak removal datasets,
although density, scale, direction and intensity have been considered,
transparency is not fully taken into account. This problem is particularly
serious in night scenes, where the appearance of rain largely depends on the
interaction with scene illuminations and changes drastically on different
positions within the image. This is problematic, because unrealistic dataset
causes serious domain bias. In this paper, we propose GTAV-NightRain dataset,
which is a large-scale synthetic night-time rain streak removal dataset. Unlike
existing datasets, by using 3D computer graphic platform (namely GTA V), we are
allowed to infer the three dimensional interaction between rain and
illuminations, which insures the photometric realness. Current release of the
dataset contains 12,860 HD rainy images and 1,286 corresponding HD ground truth
images in diversified night scenes. A systematic benchmark and analysis are
provided along with the dataset to inspire further research.",-0.24993564,-0.0022484008,0.008036146,B
12238,"With the growth of data volume, the expensive cost of annotations
                                                   is the major limitation hindering further study.","While fast progresses have been
                                                   made in supervised object detection, its unsupervised counterpart remains largely
                                                   unexplored.","Therefore, discovering objects
                                                  without annotations has great signiÔ¨Åcance.",2022-10-10 16:05:53+00:00,4D Unsupervised Object Discovery,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuqi Wang'), arxiv.Result.Author('Yuntao Chen'), arxiv.Result.Author('Zhaoxiang Zhang')]","Object discovery is a core task in computer vision. While fast progresses
have been made in supervised object detection, its unsupervised counterpart
remains largely unexplored. With the growth of data volume, the expensive cost
of annotations is the major limitation hindering further study. Therefore,
discovering objects without annotations has great significance. However, this
task seems impractical on still-image or point cloud alone due to the lack of
discriminative information. Previous studies underlook the crucial temporal
information and constraints naturally behind multi-modal inputs. In this paper,
we propose 4D unsupervised object discovery, jointly discovering objects from
4D data -- 3D point clouds and 2D RGB images with temporal information. We
present the first practical approach for this task by proposing a ClusterNet on
3D point clouds, which is jointly iteratively optimized with a 2D localization
network. Extensive experiments on the large-scale Waymo Open Dataset suggest
that the localization network and ClusterNet achieve competitive performance on
both class-agnostic 2D object detection and 3D instance segmentation, bridging
the gap between unsupervised methods and full supervised ones. Codes and models
will be made available at https://github.com/Robertwyq/LSMOL.",-0.27537766,-0.14633377,-0.17018464,B
12248,"Punctuation and articles
wise interactions between head‚Äìdependent DAAM            (punct, det; rows 4 and 6) also lack dominance,
maps, augmenting previous sections and helping to        possibly from having little semantic meaning and
form hypotheses for further research.","We characterize pair-       other (e.g., ‚Äúice cream‚Äù).","attending broadly across the image (Figure 5, top
                                                         right).",2022-10-10 17:55:41+00:00,What the DAAM: Interpreting Stable Diffusion Using Cross Attention,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Raphael Tang'), arxiv.Result.Author('Linqing Liu'), arxiv.Result.Author('Akshat Pandey'), arxiv.Result.Author('Zhiying Jiang'), arxiv.Result.Author('Gefei Yang'), arxiv.Result.Author('Karun Kumar'), arxiv.Result.Author('Pontus Stenetorp'), arxiv.Result.Author('Jimmy Lin'), arxiv.Result.Author('Ferhan Ture')]","Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, but they remain poorly understood, lacking
interpretability analyses. In this paper, we perform a text-image attribution
analysis on Stable Diffusion, a recently open-sourced model. To produce
pixel-level attribution maps, we upscale and aggregate cross-attention
word-pixel scores in the denoising subnetwork, naming our method DAAM. We
evaluate its correctness by testing its semantic segmentation ability on nouns,
as well as its generalized attribution quality on all parts of speech, rated by
humans. We then apply DAAM to study the role of syntax in the pixel space,
characterizing head--dependent heat map interaction patterns for ten common
dependency relations. Finally, we study several semantic phenomena using DAAM,
with a focus on feature entanglement, where we find that cohyponyms worsen
generation quality and descriptive adjectives attend too broadly. To our
knowledge, we are the first to interpret large diffusion models from a
visuolinguistic perspective, which enables future lines of research. Our code
is at https://github.com/castorini/daam.",0.17532112,-0.10797618,-0.30391124,A
12249,"5.1 Cohyponym Entanglement                                            To see if DAAM assists in explaining these effects,
                                                                      we compute binarized DAAM maps (œÑ = 0.4, the
To further study the large nconj:and overlap found                    best value from Sec.","Overall, the non-cohyponym set attains
                                                                      a generation accuracy of 61% and the cohyponym
5 Visuosemantic Analyses                                              set 52%, statistically signiÔ¨Åcant at the 99% level ac-
                                                                      cording to the exact test, supporting our hypothesis.","3.1) for both words and quan-
in Section 4, we hypothesize that semantically sim-                   tify the amount of overlap with IoU.",2022-10-10 17:55:41+00:00,What the DAAM: Interpreting Stable Diffusion Using Cross Attention,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Raphael Tang'), arxiv.Result.Author('Linqing Liu'), arxiv.Result.Author('Akshat Pandey'), arxiv.Result.Author('Zhiying Jiang'), arxiv.Result.Author('Gefei Yang'), arxiv.Result.Author('Karun Kumar'), arxiv.Result.Author('Pontus Stenetorp'), arxiv.Result.Author('Jimmy Lin'), arxiv.Result.Author('Ferhan Ture')]","Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, but they remain poorly understood, lacking
interpretability analyses. In this paper, we perform a text-image attribution
analysis on Stable Diffusion, a recently open-sourced model. To produce
pixel-level attribution maps, we upscale and aggregate cross-attention
word-pixel scores in the denoising subnetwork, naming our method DAAM. We
evaluate its correctness by testing its semantic segmentation ability on nouns,
as well as its generalized attribution quality on all parts of speech, rated by
humans. We then apply DAAM to study the role of syntax in the pixel space,
characterizing head--dependent heat map interaction patterns for ten common
dependency relations. Finally, we study several semantic phenomena using DAAM,
with a focus on feature entanglement, where we find that cohyponyms worsen
generation quality and descriptive adjectives attend too broadly. To our
knowledge, we are the first to interpret large diffusion models from a
visuolinguistic perspective, which enables future lines of research. Our code
is at https://github.com/castorini/daam.",0.3158722,0.007935198,-0.034672722,A
12250,"We characterize pair-       (punct, det; rows 4 and 6) also lack dominance,
wise interactions between head‚Äìdependent DAAM            possibly from having little semantic meaning and
maps, augmenting previous sections and helping to        attending broadly across the image (Figure 5, top
form hypotheses for further research.","Punctuation and articles
relates to generated pixels.",right).,2022-10-10 17:55:41+00:00,What the DAAM: Interpreting Stable Diffusion Using Cross Attention,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Raphael Tang'), arxiv.Result.Author('Linqing Liu'), arxiv.Result.Author('Akshat Pandey'), arxiv.Result.Author('Zhiying Jiang'), arxiv.Result.Author('Gefei Yang'), arxiv.Result.Author('Karun Kumar'), arxiv.Result.Author('Pontus Stenetorp'), arxiv.Result.Author('Jimmy Lin'), arxiv.Result.Author('Ferhan Ture')]","Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, but they remain poorly understood, lacking
interpretability analyses. In this paper, we perform a text-image attribution
analysis on Stable Diffusion, a recently open-sourced model. To produce
pixel-level attribution maps, we upscale and aggregate cross-attention
word-pixel scores in the denoising subnetwork, naming our method DAAM. We
evaluate its correctness by testing its semantic segmentation ability on nouns,
as well as its generalized attribution quality on all parts of speech, rated by
humans. We then apply DAAM to study the role of syntax in the pixel space,
characterizing head--dependent heat map interaction patterns for ten common
dependency relations. Finally, we study several semantic phenomena using DAAM,
with a focus on feature entanglement, where we find that cohyponyms worsen
generation quality and descriptive adjectives attend too broadly. To our
knowledge, we are the first to interpret large diffusion models from a
visuolinguistic perspective, which enables future lines of research. Our code
is at https://github.com/castorini/daam.",0.07176264,-0.023698747,-0.14417984,C
12251,"5 Visuosemantic Analyses                                              Figure 8: First row: a DAAM map for ‚Äúrusty‚Äù and three
                                                                      generated images for ‚Äúa <adj> shovel sitting in a clean
5.1 Cohyponym Entanglement                                            shed;‚Äù second row: a map for ‚Äúbumpy‚Äù and images for
                                                                      ‚Äúa <adj> ball rolling down a hill.‚Äù
To further study the large nconj:and overlap found
in Section 4, we hypothesize that semantically sim-                   tify the amount of overlap with IoU.",cohyponymic zebra‚Äìfridge and giraffe‚Äìfridge prompts.,"We Ô¨Ånd that
ilar words in a prompt have worse generation qual-                    the mIoU for cohyponyms and non-cohyponyms
ity, where only one of the words is generated in the                  are 46.7 and 22.9, suggesting entangled attention
image, not all.",2022-10-10 17:55:41+00:00,What the DAAM: Interpreting Stable Diffusion Using Cross Attention,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Raphael Tang'), arxiv.Result.Author('Linqing Liu'), arxiv.Result.Author('Akshat Pandey'), arxiv.Result.Author('Zhiying Jiang'), arxiv.Result.Author('Gefei Yang'), arxiv.Result.Author('Karun Kumar'), arxiv.Result.Author('Pontus Stenetorp'), arxiv.Result.Author('Jimmy Lin'), arxiv.Result.Author('Ferhan Ture')]","Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, but they remain poorly understood, lacking
interpretability analyses. In this paper, we perform a text-image attribution
analysis on Stable Diffusion, a recently open-sourced model. To produce
pixel-level attribution maps, we upscale and aggregate cross-attention
word-pixel scores in the denoising subnetwork, naming our method DAAM. We
evaluate its correctness by testing its semantic segmentation ability on nouns,
as well as its generalized attribution quality on all parts of speech, rated by
humans. We then apply DAAM to study the role of syntax in the pixel space,
characterizing head--dependent heat map interaction patterns for ten common
dependency relations. Finally, we study several semantic phenomena using DAAM,
with a focus on feature entanglement, where we find that cohyponyms worsen
generation quality and descriptive adjectives attend too broadly. To our
knowledge, we are the first to interpret large diffusion models from a
visuolinguistic perspective, which enables future lines of research. Our code
is at https://github.com/castorini/daam.",0.19600925,-0.10384214,-0.22755986,A
12259,"As such, we believe the proposed formulation to hold a great promise in
minimizing labeling efforts and defines new avenues for further research.","Moreover, proce-
dure flow graphs are only needed at the task level, rather than on a per video
bases.","Acknowledgement

We thank Ran Zhang for the help with flow graph creation and processing.",2022-10-10 20:02:58+00:00,Graph2Vid: Flow graph to Video Grounding forWeakly-supervised Multi-Step Localization,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Nikita Dvornik'), arxiv.Result.Author('Isma Hadji'), arxiv.Result.Author('Hai Pham'), arxiv.Result.Author('Dhaivat Bhatt'), arxiv.Result.Author('Brais Martinez'), arxiv.Result.Author('Afsaneh Fazly'), arxiv.Result.Author('Allan D. Jepson')]","In this work, we consider the problem of weakly-supervised multi-step
localization in instructional videos. An established approach to this problem
is to rely on a given list of steps. However, in reality, there is often more
than one way to execute a procedure successfully, by following the set of steps
in slightly varying orders. Thus, for successful localization in a given video,
recent works require the actual order of procedure steps in the video, to be
provided by human annotators at both training and test times. Instead, here, we
only rely on generic procedural text that is not tied to a specific video. We
represent the various ways to complete the procedure by transforming the list
of instructions into a procedure flow graph which captures the partial order of
steps. Using the flow graphs reduces both training and test time annotation
requirements. To this end, we introduce the new problem of flow graph to video
grounding. In this setup, we seek the optimal step ordering consistent with the
procedure flow graph and a given video. To solve this problem, we propose a new
algorithm - Graph2Vid - that infers the actual ordering of steps in the video
and simultaneously localizes them. To show the advantage of our proposed
formulation, we extend the CrossTask dataset with procedure flow graph
information. Our experiments show that Graph2Vid is both more efficient than
the baselines and yields strong step localization results, without the need for
step order annotation.",-0.063462436,-0.043241676,-0.124546915,C
12260,"As such, we believe the proposed formulation to hold a great promise in
minimizing labeling efforts and defines new avenues for further research.","Moreover, proce-
dure flow graphs are only needed at the task level, rather than on a per video
bases.","Acknowledgement

We thank Ran Zhang for the help with flow graph creation and processing.",2022-10-10 20:02:58+00:00,Graph2Vid: Flow graph to Video Grounding for Weakly-supervised Multi-Step Localization,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Nikita Dvornik'), arxiv.Result.Author('Isma Hadji'), arxiv.Result.Author('Hai Pham'), arxiv.Result.Author('Dhaivat Bhatt'), arxiv.Result.Author('Brais Martinez'), arxiv.Result.Author('Afsaneh Fazly'), arxiv.Result.Author('Allan D. Jepson')]","In this work, we consider the problem of weakly-supervised multi-step
localization in instructional videos. An established approach to this problem
is to rely on a given list of steps. However, in reality, there is often more
than one way to execute a procedure successfully, by following the set of steps
in slightly varying orders. Thus, for successful localization in a given video,
recent works require the actual order of procedure steps in the video, to be
provided by human annotators at both training and test times. Instead, here, we
only rely on generic procedural text that is not tied to a specific video. We
represent the various ways to complete the procedure by transforming the list
of instructions into a procedure flow graph which captures the partial order of
steps. Using the flow graphs reduces both training and test time annotation
requirements. To this end, we introduce the new problem of flow graph to video
grounding. In this setup, we seek the optimal step ordering consistent with the
procedure flow graph and a given video. To solve this problem, we propose a new
algorithm - Graph2Vid - that infers the actual ordering of steps in the video
and simultaneously localizes them. To show the advantage of our proposed
formulation, we extend the CrossTask dataset with procedure flow graph
information. Our experiments show that Graph2Vid is both more efficient than
the baselines and yields strong step localization results, without the need for
step order annotation.",-0.063462436,-0.043241676,-0.124546915,C
12265,"[2022] further study the use of
reconstruction as a pretext task, incorporating a decoder module in various self-supervised contrastive
settings.",Wang et al.,"These two methods use shallow convolutional networks for their decoders to preferably
learn additional useful local features in the latent space.",2022-10-11 00:26:59+00:00,Improving Dense Contrastive Learning with Dense Negative Pairs,cs.CV,['cs.CV'],"[arxiv.Result.Author('Berk Iskender'), arxiv.Result.Author('Zhenlin Xu'), arxiv.Result.Author('Simon Kornblith'), arxiv.Result.Author('Enhung Chu'), arxiv.Result.Author('Maryam Khademi')]","Many contrastive representation learning methods learn a single global
representation of an entire image. However, dense contrastive representation
learning methods such as DenseCL [19] can learn better representations for
tasks requiring stronger spatial localization of features, such as multi-label
classification, detection, and segmentation. In this work, we study how to
improve the quality of the representations learned by DenseCL by modifying the
training scheme and objective function, and propose DenseCL++. We also conduct
several ablation studies to better understand the effects of: (i) various
techniques to form dense negative pairs among augmentations of different
images, (ii) cross-view dense negative and positive pairs, and (iii) an
auxiliary reconstruction task. Our results show 3.5% and 4% mAP improvement
over SimCLR [3] and DenseCL in COCO multi-label classification. In COCO and VOC
segmentation tasks, we achieve 1.8% and 0.7% mIoU improvements over SimCLR,
respectively.",-0.17603451,-0.25568795,-0.02111207,C
12272,We further study X-NeRF‚Äôs completion ability.,Completion Ability                                                                 ods.,"The re-                                Acknowledgement
sult in Fig.",2022-10-11 04:29:26+00:00,X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360$^{\circ} $ Insufficient RGB-D Views,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haoyi Zhu'), arxiv.Result.Author('Hao-Shu Fang'), arxiv.Result.Author('Cewu Lu')]","Neural Radiance Fields (NeRFs), despite their outstanding performance on
novel view synthesis, often need dense input views. Many papers train one model
for each scene respectively and few of them explore incorporating multi-modal
data into this problem. In this paper, we focus on a rarely discussed but
important setting: can we train one model that can represent multiple scenes,
with 360$^\circ $ insufficient views and RGB-D images? We refer insufficient
views to few extremely sparse and almost non-overlapping views. To deal with
it, X-NeRF, a fully explicit approach which learns a general scene completion
process instead of a coordinate-based mapping, is proposed. Given a few
insufficient RGB-D input views, X-NeRF first transforms them to a sparse point
cloud tensor and then applies a 3D sparse generative Convolutional Neural
Network (CNN) to complete it to an explicit radiance field whose volumetric
rendering can be conducted fast without running networks during inference. To
avoid overfitting, besides common rendering loss, we apply perceptual loss as
well as view augmentation through random rotation on point clouds. The proposed
methodology significantly out-performs previous implicit methods in our
setting, indicating the great potential of proposed problem and approach. Codes
and data are available at https://github.com/HaoyiZhu/XNeRF.",0.3534572,0.058648486,0.0025169253,A
12291,"Most importantly, we pave a new                                                                                                                                                                                                                                                                                                                                                            el asti c bands                                                                                                                   the tai l                                      your l egs for a stretch                                     the body
                                        path for understanding the instructional videos, performing
                                        detailed analyses on extensive experiments, which ushers in                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Fi nal l y, do i t to stretch the other j oi nts of
                                        further research1.","We have recon-
                                        structed a new dataset named MedVidCQA and benchmarked                        ‚Ä¶                                                                                                                                                                                                                                                                                                              ‚Ä¶
                                        the VCVAL task, where the proposed method achieves state-
                                        of-the-art (SOTA) both in the video corpus retrieval and visual                                                                                                                                         ‚Ä¶                                                                                                                                                                                       ‚Ä¶‚Ä¶‚Ä¶  Let me show you how to stretch wi th                       Improve j oi nt mobi l i ty by stretchi ng                  The use of the el asti c band i s to hol d the ends of  Support your body on both si des, then strai ghten       Fi nal l y, do i t to stretch the other j oi nts of                  ‚Ä¶
                                        answer localization tasks.","the body

                                            Index Terms‚Äî Video collections, visual answer localiza-                      Input text question and video corpus                                                                                                                                                                                                                                                                           Output retrieval videos and visual answer
                                        tion
                                                                                                                      Fig.",2022-10-11 13:04:59+00:00,Learning to Locate Visual Answer in Video Corpus Using Question,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Bin Li'), arxiv.Result.Author('Yixuan Weng'), arxiv.Result.Author('Bin Sun'), arxiv.Result.Author('Shutao Li')]","We introduce a novel task, named video corpus visual answer localization
(VCVAL), which aims to locate the visual answer in a large collection of
untrimmed, unsegmented instructional videos using a natural language question.
This task requires a range of skills - the interaction between vision and
language, video retrieval, passage comprehension, and visual answer
localization. To solve these, we propose a cross-modal contrastive global-span
(CCGS) method for the VCVAL, jointly training the video corpus retrieval and
visual answer localization tasks. More precisely, we enhance the video
question-answer semantic by adding element-wise visual information into the
pre-trained language model, and designing a novel global-span predictor through
fusion information to locate the visual answer point. The Global-span
contrastive learning is adopted to differentiate the span point in the positive
and negative samples with the global-span matrix. We have reconstructed a new
dataset named MedVidCQA and benchmarked the VCVAL task, where the proposed
method achieves state-of-the-art (SOTA) both in the video corpus retrieval and
visual answer localization tasks. Most importantly, we pave a new path for
understanding the instructional videos, performing detailed analyses on
extensive experiments, which ushers in further research.",0.11118892,-0.019181106,-0.21207626,A
12292,"Most importantly, we perform detailed analyses on                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Fi nal l y, do i t to stretch the other j oi nts of
                                        extensive experiments, paving a new path for understanding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          the body
                                        the instructional videos, which ushers in further research1.","Experimental results show that the pro-                                                                                                                                            ‚Ä¶                                                                                                                                                                                       ‚Ä¶‚Ä¶‚Ä¶  Let me show you how to stretch wi th                       Improve j oi nt mobi l i ty by stretchi ng                  The use of the el asti c band i s to hol d the ends of  Support your body on both si des, then strai ghten       Fi nal l y, do i t to stretch the other j oi nts of                  ‚Ä¶
                                        posed method outperforms other competitive methods both                                                                                                                                                                                                                                                                                                                                                               el asti c bands                                                                                                                   the tai l                                      your l egs for a stretch                                     the body
                                        in the video corpus retrieval and visual answer localization
                                        subtasks.","Input text question and video corpus                                                                                                                                                                                                                                                                           Output retrieval videos and visual answer
                                            Index Terms‚Äî Video collections, visual answer localiza-
                                        tion                                                                          Fig.",2022-10-11 13:04:59+00:00,Learning to Locate Visual Answer in Video Corpus Using Question,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Bin Li'), arxiv.Result.Author('Yixuan Weng'), arxiv.Result.Author('Bin Sun'), arxiv.Result.Author('Shutao Li')]","We introduce a new task, named video corpus visual answer localization
(VCVAL), which aims to locate the visual answer in a large collection of
untrimmed, unsegmented instructional videos using a natural language question.
This task requires a range of skills - the interaction between vision and
language, video retrieval, passage comprehension, and visual answer
localization. In this paper, we propose a cross-modal contrastive global-span
(CCGS) method for the VCVAL, jointly training the video corpus retrieval and
visual answer localization subtasks. More precisely, we first enhance the video
question-answer semantic by adding element-wise visual information into the
pre-trained language model, and then design a novel global-span predictor
through fusion information to locate the visual answer point. The global-span
contrastive learning is adopted to sort the span point from the positive and
negative samples with the global-span matrix. We have reconstructed a dataset
named MedVidCQA, on which the VCVAL task is benchmarked. Experimental results
show that the proposed method outperforms other competitive methods both in the
video corpus retrieval and visual answer localization subtasks. Most
importantly, we perform detailed analyses on extensive experiments, paving a
new path for understanding the instructional videos, which ushers in further
research.",0.13972318,-0.05576328,-0.25180364,A
12293,"Most importantly, we perform detailed analyses on                                                                                                                                                                                                                                                                                                  Answer:
                                        extensive experiments, paving a new path for understanding                                                                                                                                                                                                                                                                                                                                                                                                            ‚Ä¶
                                        the instructional videos, which ushers in further research1.","method outperforms other competitive methods both in the
                                        video corpus retrieval and visual answer localization sub-                                                                                                                                                                                                                                                                                                 Visual  0:14
                                        tasks.","‚Ä¶                                                                                                                                                                                                                                                                        ‚Ä¶
                                            Index Terms‚Äî Video collections, visual answer localiza-
                                        tion                                                                                                                                                                                    ‚Ä¶                                                                                                                                                                 ‚Ä¶‚Ä¶‚Ä¶  Let me show you how to stretch with                 Improve joint mobility by stretching               The use of the elastic band is to hold the ends of  Support your body on both sides, then straighten   Finally, do it to stretch the other joints of                ‚Ä¶
                                                                                                                                                                                                                                                                                                                                                                                                                      elastic bands                                                                                                      the tail                                  your legs for a stretch                                the body
                                                              1.",2022-10-11 13:04:59+00:00,Learning to Locate Visual Answer in Video Corpus Using Question,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Bin Li'), arxiv.Result.Author('Yixuan Weng'), arxiv.Result.Author('Bin Sun'), arxiv.Result.Author('Shutao Li')]","We introduce a new task, named video corpus visual answer localization
(VCVAL), which aims to locate the visual answer in a large collection of
untrimmed instructional videos using a natural language question. This task
requires a range of skills - the interaction between vision and language, video
retrieval, passage comprehension, and visual answer localization. In this
paper, we propose a cross-modal contrastive global-span (CCGS) method for the
VCVAL, jointly training the video corpus retrieval and visual answer
localization subtasks with the global-span matrix. We have reconstructed a
dataset named MedVidCQA, on which the VCVAL task is benchmarked. Experimental
results show that the proposed method outperforms other competitive methods
both in the video corpus retrieval and visual answer localization subtasks.
Most importantly, we perform detailed analyses on extensive experiments, paving
a new path for understanding the instructional videos, which ushers in further
research.",0.07638759,0.021983657,-0.31927508,A
12297,"To stimulate further research in map-free relocalization, we present
a new benchmark and dataset.","While we provide evidence that existing methods can solve map-free relocal-
ization with acceptable precision, such results are restricted to a narrow window
of situations.","We have gathered images of 655 places of interest
worldwide where each place can be represented well by a single reference image.",2022-10-11 14:49:49+00:00,Map-free Visual Relocalization: Metric Pose Relative to a Single Image,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eduardo Arnold'), arxiv.Result.Author('Jamie Wynn'), arxiv.Result.Author('Sara Vicente'), arxiv.Result.Author('Guillermo Garcia-Hernando'), arxiv.Result.Author('√Åron Monszpart'), arxiv.Result.Author('Victor Adrian Prisacariu'), arxiv.Result.Author('Daniyar Turmukhambetov'), arxiv.Result.Author('Eric Brachmann')]","Can we relocalize in a scene represented by a single reference image?
Standard visual relocalization requires hundreds of images and scale
calibration to build a scene-specific 3D map. In contrast, we propose Map-free
Relocalization, i.e., using only one photo of a scene to enable instant, metric
scaled relocalization. Existing datasets are not suitable to benchmark map-free
relocalization, due to their focus on large scenes or their limited
variability. Thus, we have constructed a new dataset of 655 small places of
interest, such as sculptures, murals and fountains, collected worldwide. Each
place comes with a reference image to serve as a relocalization anchor, and
dozens of query images with known, metric camera poses. The dataset features
changing conditions, stark viewpoint changes, high variability across places,
and queries with low to no visual overlap with the reference image. We identify
two viable families of existing methods to provide baseline results: relative
pose regression, and feature matching combined with single-image depth
prediction. While these methods show reasonable performance on some favorable
scenes in our dataset, map-free relocalization proves to be a challenge that
requires new, innovative solutions.",-0.19037698,0.23147812,0.049715824,B
12298,"We
     expose the primary problems of current approaches to guide further research.","‚Äì Baseline results for map-free relocalization using relative pose regression
     methods, and feature matching on top of single image-depth prediction.","‚Äì Additional experiments and ablation studies on ScanNet and 7Scenes datasets,
     allowing comparisons to related, previous research on relative pose estima-
     tion and visual relocalization.",2022-10-11 14:49:49+00:00,Map-free Visual Relocalization: Metric Pose Relative to a Single Image,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eduardo Arnold'), arxiv.Result.Author('Jamie Wynn'), arxiv.Result.Author('Sara Vicente'), arxiv.Result.Author('Guillermo Garcia-Hernando'), arxiv.Result.Author('√Åron Monszpart'), arxiv.Result.Author('Victor Adrian Prisacariu'), arxiv.Result.Author('Daniyar Turmukhambetov'), arxiv.Result.Author('Eric Brachmann')]","Can we relocalize in a scene represented by a single reference image?
Standard visual relocalization requires hundreds of images and scale
calibration to build a scene-specific 3D map. In contrast, we propose Map-free
Relocalization, i.e., using only one photo of a scene to enable instant, metric
scaled relocalization. Existing datasets are not suitable to benchmark map-free
relocalization, due to their focus on large scenes or their limited
variability. Thus, we have constructed a new dataset of 655 small places of
interest, such as sculptures, murals and fountains, collected worldwide. Each
place comes with a reference image to serve as a relocalization anchor, and
dozens of query images with known, metric camera poses. The dataset features
changing conditions, stark viewpoint changes, high variability across places,
and queries with low to no visual overlap with the reference image. We identify
two viable families of existing methods to provide baseline results: relative
pose regression, and feature matching combined with single-image depth
prediction. While these methods show reasonable performance on some favorable
scenes in our dataset, map-free relocalization proves to be a challenge that
requires new, innovative solutions.",-0.27649638,0.18939072,0.039390963,B
12299,"To facilitate further research, we have presented the Niantic map-free relocal-
ization dataset and benchmark with a large number of diverse places of interest.","Our results suggest some directions for future research: improve
the scale estimates by improving depth estimation in outdoor scenes; improve
the accuracy of metric RPR methods; and derive a confidence for their estimates.","We define an evaluation protocol to closely match AR use cases, and make the
dataset and an evaluation service publicly available.",2022-10-11 14:49:49+00:00,Map-free Visual Relocalization: Metric Pose Relative to a Single Image,cs.CV,['cs.CV'],"[arxiv.Result.Author('Eduardo Arnold'), arxiv.Result.Author('Jamie Wynn'), arxiv.Result.Author('Sara Vicente'), arxiv.Result.Author('Guillermo Garcia-Hernando'), arxiv.Result.Author('√Åron Monszpart'), arxiv.Result.Author('Victor Adrian Prisacariu'), arxiv.Result.Author('Daniyar Turmukhambetov'), arxiv.Result.Author('Eric Brachmann')]","Can we relocalize in a scene represented by a single reference image?
Standard visual relocalization requires hundreds of images and scale
calibration to build a scene-specific 3D map. In contrast, we propose Map-free
Relocalization, i.e., using only one photo of a scene to enable instant, metric
scaled relocalization. Existing datasets are not suitable to benchmark map-free
relocalization, due to their focus on large scenes or their limited
variability. Thus, we have constructed a new dataset of 655 small places of
interest, such as sculptures, murals and fountains, collected worldwide. Each
place comes with a reference image to serve as a relocalization anchor, and
dozens of query images with known, metric camera poses. The dataset features
changing conditions, stark viewpoint changes, high variability across places,
and queries with low to no visual overlap with the reference image. We identify
two viable families of existing methods to provide baseline results: relative
pose regression, and feature matching combined with single-image depth
prediction. While these methods show reasonable performance on some favorable
scenes in our dataset, map-free relocalization proves to be a challenge that
requires new, innovative solutions.",-0.22656877,0.3274819,0.11375201,B
12301,"To further study the quality of the generated pseudo labels, we count
the proportion and accuracy of pseudo labels on the ScanNet-v2 training set
during training.","The pseudo labels are generated on the train-
ing set to increase supervision during training, so their quality aÔ¨Äects network
training.",The results are listed in Tab.,2022-10-11 15:22:22+00:00,Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Linghua Tang'), arxiv.Result.Author('Le Hui'), arxiv.Result.Author('Jin Xie')]","Due to the few annotated labels of 3D point clouds, how to learn
discriminative features of point clouds to segment object instances is a
challenging problem. In this paper, we propose a simple yet effective 3D
instance segmentation framework that can achieve good performance by annotating
only one point for each instance. Specifically, to tackle extremely few labels
for instance segmentation, we first oversegment the point cloud into
superpoints in an unsupervised manner and extend the point-level annotations to
the superpoint level. Then, based on the superpoint graph, we propose an
inter-superpoint affinity mining module that considers the semantic and spatial
relations to adaptively learn inter-superpoint affinity to generate
high-quality pseudo labels via semantic-aware random walk. Finally, we propose
a volume-aware instance refinement module to segment high-quality instances by
applying volume constraints of objects in clustering on the superpoint graph.
Extensive experiments on the ScanNet-v2 and S3DIS datasets demonstrate that our
method achieves state-of-the-art performance in the weakly supervised point
cloud instance segmentation task, and even outperforms some fully supervised
methods.",0.075442776,-0.18591283,0.09966247,C
12303,"We have provided a formal analysis of the common latent space of stochastic DPMs via the
bounded distance between images (Section 3.3), but it still needs further study.","Besides the interesting results, it is worth noting that this paper raised more questions than provided
answers.","Notably, Khrulkov
& Oseledets (2022) and Su et al.",2022-10-11 15:53:52+00:00,"Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance",cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Chen Henry Wu'), arxiv.Result.Author('Fernando De la Torre')]","Diffusion models have achieved unprecedented performance in generative
modeling. The commonly-adopted formulation of the latent code of diffusion
models is a sequence of gradually denoised samples, as opposed to the simpler
(e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper
provides an alternative, Gaussian formulation of the latent space of various
diffusion models, as well as an invertible DPM-Encoder that maps images into
the latent space. While our formulation is purely based on the definition of
diffusion models, we demonstrate several intriguing consequences. (1)
Empirically, we observe that a common latent space emerges from two diffusion
models trained independently on related domains. In light of this finding, we
propose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image
translation. Furthermore, applying CycleDiffusion to text-to-image diffusion
models, we show that large-scale text-to-image diffusion models can be used as
zero-shot image-to-image editors. (2) One can guide pre-trained diffusion
models and GANs by controlling the latent codes in a unified, plug-and-play
formulation based on energy-based models. Using the CLIP model and a face
recognition model as guidance, we demonstrate that diffusion models have better
coverage of low-density sub-populations and individuals than GANs. The code is
publicly available at https://github.com/ChenWu98/cycle-diffusion.",0.08365552,0.0409887,0.0326396,A
12305,"Reproducibility

   The BodyM dataset is publicly available at https:
//adversarialbodysim.github.io to enable re-
producibility of our method and further research in this area.",A.6.,,2022-10-11 17:58:10+00:00,Human Body Measurement Estimation with Adversarial Augmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Nataniel Ruiz'), arxiv.Result.Author('Miriam Bellver'), arxiv.Result.Author('Timo Bolkart'), arxiv.Result.Author('Ambuj Arora'), arxiv.Result.Author('Ming C. Lin'), arxiv.Result.Author('Javier Romero'), arxiv.Result.Author('Raja Bala')]","We present a Body Measurement network (BMnet) for estimating 3D
anthropomorphic measurements of the human body shape from silhouette images.
Training of BMnet is performed on data from real human subjects, and augmented
with a novel adversarial body simulator (ABS) that finds and synthesizes
challenging body shapes. ABS is based on the skinned multiperson linear (SMPL)
body model, and aims to maximize BMnet measurement prediction error with
respect to latent SMPL shape parameters. ABS is fully differentiable with
respect to these parameters, and trained end-to-end via backpropagation with
BMnet in the loop. Experiments show that ABS effectively discovers adversarial
examples, such as bodies with extreme body mass indices (BMI), consistent with
the rarity of extreme-BMI bodies in BMnet's training set. Thus ABS is able to
reveal gaps in training data and potential failures in predicting
under-represented body shapes. Results show that training BMnet with ABS
improves measurement prediction accuracy on real bodies by up to 10%, when
compared to no augmentation or random body shape sampling. Furthermore, our
method significantly outperforms SOTA measurement estimation methods by as much
as 3x. Finally, we release BodyM, the first challenging, large-scale dataset of
photo silhouettes and body measurements of real human subjects, to further
promote research in this area. Project website:
https://adversarialbodysim.github.io",0.020291567,-0.1631347,-0.03872612,C
12322,"Lastly, we conclude our analysis by describing opportunities
                                        for further research in this Ô¨Åeld.","Then, we provide an analysis
                                        describing aspects and limitations of various components of different capsule networks, followed by an
                                        analysis of augmenting squash functions.","* These authors contributed equally to this work
2 Convolutional Neural Networks (CNNs)

Convolutional neural networks (CNN) are feed-forward neural networks that can extract features from
data in a hierarchical structure.",2022-10-11 23:30:12+00:00,Effectiveness of the Recent Advances in Capsule Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Nidhin Harilal'), arxiv.Result.Author('Rohan Patil')]","Convolutional neural networks (CNNs) have revolutionized the field of deep
neural networks. However, recent research has shown that CNNs fail to
generalize under various conditions and hence the idea of capsules was
introduced in 2011, though the real surge of research started from 2017. In
this paper, we present an overview of the recent advances in capsule
architecture and routing mechanisms. In addition, we find that the relative
focus in recent literature is on modifying routing procedure or architecture as
a whole but the study of other finer components, specifically, squash function
is wanting. Thus, we also present some new insights regarding the effect of
squash functions in performance of the capsule networks. Finally, we conclude
by discussing and proposing possible opportunities in the field of capsule
networks.",-0.11516535,-0.23539507,0.076385245,C
12342,"The results and potential avenues for further research are
discussed in section 4.","This enables us
to compare the reconstruction against the original image.",The paper ends with the conclusion in section 5.,2022-10-07 19:42:09+00:00,Can Artificial Intelligence Reconstruct Ancient Mosaics?,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Fernando Moral-Andr√©s'), arxiv.Result.Author('Elena Merino-G√≥mez'), arxiv.Result.Author('Pedro Reviriego'), arxiv.Result.Author('Fabrizio Lombardi')]","A large number of ancient mosaics have not reached us because they have been
destroyed by erosion, earthquakes, looting or even used as materials in newer
construction. To make things worse, among the small fraction of mosaics that we
have been able to recover, many are damaged or incomplete. Therefore,
restoration and reconstruction of mosaics play a fundamental role to preserve
cultural heritage and to understand the role of mosaics in ancient cultures.
This reconstruction has traditionally been done manually and more recently
using computer graphics programs but always by humans. In the last years,
Artificial Intelligence (AI) has made impressive progress in the generation of
images from text descriptions and reference images. State of the art AI tools
such as DALL-E2 can generate high quality images from text prompts and can take
a reference image to guide the process. In august 2022, DALL-E2 launched a new
feature called outpainting that takes as input an incomplete image and a text
prompt and then generates a complete image filling the missing parts. In this
paper, we explore whether this innovative technology can be used to reconstruct
mosaics with missing parts. Hence a set of ancient mosaics have been used and
reconstructed using DALL-E2; results are promising showing that AI is able to
interpret the key features of the mosaics and is able to produce
reconstructions that capture the essence of the scene. However, in some cases
AI fails to reproduce some details, geometric forms or introduces elements that
are not consistent with the rest of the mosaic. This suggests that as AI image
generation technology matures in the next few years, it could be a valuable
tool for mosaic reconstruction going forward.",0.06738331,0.33427098,0.0048373966,B
12384,"title suggests, we hope that this approach will serve as           1761‚Äì1773, 2018.
a simple yet strong baseline motivating further research
in heterogeneous face recognition.","7, pp.","[5] D. Poster, M. Thielke, R. Nguyen, S. Rajaraman, X.",2022-10-12 18:54:57+00:00,Prepended Domain Transformer: Heterogeneous Face Recognition without Bells and Whistles,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Anjith George'), arxiv.Result.Author('Amir Mohammadi'), arxiv.Result.Author('Sebastien Marcel')]","Heterogeneous Face Recognition (HFR) refers to matching face images captured
in different domains, such as thermal to visible images (VIS), sketches to
visible images, near-infrared to visible, and so on. This is particularly
useful in matching visible spectrum images to images captured from other
modalities. Though highly useful, HFR is challenging because of the domain gap
between the source and target domain. Often, large-scale paired heterogeneous
face image datasets are absent, preventing training models specifically for the
heterogeneous task. In this work, we propose a surprisingly simple, yet, very
effective method for matching face images across different sensing modalities.
The core idea of the proposed approach is to add a novel neural network block
called Prepended Domain Transformer (PDT) in front of a pre-trained face
recognition (FR) model to address the domain gap. Retraining this new block
with few paired samples in a contrastive learning setup was enough to achieve
state-of-the-art performance in many HFR benchmarks. The PDT blocks can be
retrained for several source-target combinations using the proposed general
framework. The proposed approach is architecture agnostic, meaning they can be
added to any pre-trained FR models. Further, the approach is modular and the
new block can be trained with a minimal set of paired samples, making it much
easier for practical deployment. The source code and protocols will be made
available publicly.",0.038799264,0.07312609,-0.081099786,C
12390,"Some further research is to be carried out to
improve both template matching criteria and the shapes of the vehicle blobs.","The main issue of poor condition and structure of vehicle blobs has been related to inefficient
colour band combination and a technique has been developed called colour band selection technique which
has improved the vehicle blobs, as shown in Figure 8 and 10.","References
Ballard, D.H., and Brown, C.M.",2022-10-11 02:03:32+00:00,Automatic Real-time Vehicle Classification by Image Colour Component Based Template Matching,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Ahmet Orun')],"Selection of appropriate template matching algorithms to run effectively on
real-time low-cost systems is always major issue. This is due to unpredictable
changes in image scene which often necessitate more sophisticated real-time
algorithms to retain image consistency. Inefficiency of low cost auxiliary
hardware and time limitations are the major constraints in using these sorts of
algorithms. The real-time system introduced here copes with these problems
utilising a fast running template matching algorithm, which makes use of best
colour band selection. The system uses fast running real-time algorithms to
achieve template matching and vehicle classification at about 4 frames /sec. on
low-cost hardware. The colour image sequences have been taken by a fixed CCTV
camera overlooking a busy multi-lane road",0.09455727,0.2786948,-0.08536333,A
12391,"Some further research is to be carried out to
improve both template matching criteria and the shapes of the vehicle blobs.","The main issue of poor condition and structure of vehicle blobs has been related to inefficient
colour band combination and a technique has been developed called colour band selection technique which
has improved the vehicle blobs, as shown in Figure 8 and 10.","References
Ballard, D.H., and Brown, C.M.",2022-10-11 02:03:32+00:00,Automatic Real-time Vehicle Classification by Image Colour Component Based Template Matching,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Ahmet Orun')],"Selection of appropriate template matching algorithms to run effectively on
real-time low-cost systems is always major issue. This is due to unpredictable
changes in image scene which often necessitate more sophisticated real-time
algorithms to retain image consistency. Inefficiency of low cost auxiliary
hardware and time limitations are the major constraints in using these sorts of
algorithms. The real-time system introduced here copes with these problems
utilising a fast running template matching algorithm, which makes use of best
colour band selection. The system uses fast running real-time algorithms to
achieve template matching and vehicle classification at about 4 frames /sec. on
low-cost hardware. The colour image sequences have been taken by a fixed CCTV
camera overlooking a busy multi-lane road",0.09455727,0.2786948,-0.08536333,A
12397,"UCF-Crime No 0.7 83.84 83.08                                          This is perhaps due to different attribute of dataset and is
                                                                      left for further study.","UCF-Crime No 0.5 84.33 83.80                                             However, on the XD-Violence dataset, the BERT video
                                                                      classiÔ¨Åcation brings surprising performance improvement.","UCF-Crime Yes 0.5 84.12 83.50
                                                                      5.7.",2022-10-13 03:00:22+00:00,Overlooked Video Classification in Weakly Supervised Video Anomaly Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Weijun Tan'), arxiv.Result.Author('Qi Yao'), arxiv.Result.Author('Jingfeng Liu')]","Current weakly supervised video anomaly detection algorithms mostly use
multiple instance learning (MIL) or their varieties. Almost all recent
approaches focus on how to select the correct snippets for training to improve
the performance. They overlook or do not realize the power of video
classification in boosting the performance of anomaly detection. In this paper,
we study explicitly the power of video classification supervision using a BERT
or LSTM. With this BERT or LSTM, CNN features of all snippets of a video can be
aggregated into a single feature which can be used for video classification.
This simple yet powerful video classification supervision, combined into the
MIL framework, brings extraordinary performance improvement on all three major
video anomaly detection datasets. Particularly it improves the mean average
precision (mAP) on the XD-Violence from SOTA 78.84\% to new 82.10\%. The source
code is available at
https://github.com/wjtan99/BERT_Anomaly_Video_Classification.",0.12415882,-0.039437577,-0.13302365,A
12404,"Our code and datasets have been released
activity should be decoded using a combination of not only the           to facilitate further research1.","Therefore, we argue that the recorded brain           and textual features.","4) Our experimental results show
visual semantic features that were in fact presented as clues, but       several interesting conclusions and cognitive insights about the
also a far richer set of linguistic semantic features typically related  human visual system.",2022-10-13 05:49:33+00:00,Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features,cs.CV,"['cs.CV', 'cs.AI', 'cs.MM', 'cs.NE']","[arxiv.Result.Author('Changde Du'), arxiv.Result.Author('Kaicheng Fu'), arxiv.Result.Author('Jinpeng Li'), arxiv.Result.Author('Huiguang He')]","Decoding human visual neural representations is a challenging task with great
scientific significance in revealing vision-processing mechanisms and
developing brain-like intelligent machines. Most existing methods are difficult
to generalize to novel categories that have no corresponding neural data for
training. The two main reasons are 1) the under-exploitation of the multimodal
semantic knowledge underlying the neural data and 2) the small number of paired
(stimuli-responses) training data. To overcome these limitations, this paper
presents a generic neural decoding method called BraVL that uses multimodal
learning of brain-visual-linguistic features. We focus on modeling the
relationships between brain, visual and linguistic features via multimodal deep
generative models. Specifically, we leverage the mixture-of-product-of-experts
formulation to infer a latent code that enables a coherent joint generation of
all three modalities. To learn a more consistent joint representation and
improve the data efficiency in the case of limited brain activity data, we
exploit both intra- and inter-modality mutual information maximization
regularization terms. In particular, our BraVL model can be trained under
various semi-supervised scenarios to incorporate the visual and textual
features obtained from the extra categories. Finally, we construct three
trimodal matching datasets, and the extensive experiments lead to some
interesting conclusions and cognitive insights: 1) decoding novel visual
categories from human brain activity is practically possible with good
accuracy; 2) decoding models using the combination of visual and linguistic
features perform much better than those using either of them alone; 3) visual
perception may be accompanied by linguistic influences to represent the
semantics of visual stimuli. Code and data: https://github.com/ChangdeDu/BraVL.",-0.10413247,-0.19534181,-0.3182007,C
12408,"The data is entirely com-    use of or augmenting the negative classes also poses a
posed of natural images and designed to have various      challenge in further research in open-set methods.","Furthermore, making better
tion of open-set algorithms.",levels of similarities between its partitions.,2022-10-13 07:01:34+00:00,Large-Scale Open-Set Classification Protocols for ImageNet,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jesus Andres Palechor Anacona'), arxiv.Result.Author('Annesha Bhoumik'), arxiv.Result.Author('Manuel G√ºnther')]","Open-Set Classification (OSC) intends to adapt closed-set classification
models to real-world scenarios, where the classifier must correctly label
samples of known classes while rejecting previously unseen unknown samples.
Only recently, research started to investigate on algorithms that are able to
handle these unknown samples correctly. Some of these approaches address OSC by
including into the training set negative samples that a classifier learns to
reject, expecting that these data increase the robustness of the classifier on
unknown classes. Most of these approaches are evaluated on small-scale and
low-resolution image datasets like MNIST, SVHN or CIFAR, which makes it
difficult to assess their applicability to the real world, and to compare them
among each other. We propose three open-set protocols that provide rich
datasets of natural images with different levels of similarity between known
and unknown classes. The protocols consist of subsets of ImageNet classes
selected to provide training and testing data closer to real-world scenarios.
Additionally, we propose a new validation metric that can be employed to assess
whether the training of deep learning models addresses both the classification
of known samples and the rejection of unknown samples. We use the protocols to
compare the performance of two baseline open-set algorithms to the standard
SoftMax baseline and find that the algorithms work well on negative samples
that have been seen during training, and partially on out-of-distribution
detection tasks, but drop performance in the presence of samples from
previously unseen unknown classes.",-0.032186437,-0.011994614,-0.009601482,C
12409,"The data is entirely com-    use of or augmenting the negative classes also poses a
posed of natural images and designed to have various      challenge in further research in open-set methods.","Furthermore, making better
tion of open-set algorithms.",levels of similarities between its partitions.,2022-10-13 07:01:34+00:00,Large-Scale Open-Set Classification Protocols for ImageNet,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Andres Palechor'), arxiv.Result.Author('Annesha Bhoumik'), arxiv.Result.Author('Manuel G√ºnther')]","Open-Set Classification (OSC) intends to adapt closed-set classification
models to real-world scenarios, where the classifier must correctly label
samples of known classes while rejecting previously unseen unknown samples.
Only recently, research started to investigate on algorithms that are able to
handle these unknown samples correctly. Some of these approaches address OSC by
including into the training set negative samples that a classifier learns to
reject, expecting that these data increase the robustness of the classifier on
unknown classes. Most of these approaches are evaluated on small-scale and
low-resolution image datasets like MNIST, SVHN or CIFAR, which makes it
difficult to assess their applicability to the real world, and to compare them
among each other. We propose three open-set protocols that provide rich
datasets of natural images with different levels of similarity between known
and unknown classes. The protocols consist of subsets of ImageNet classes
selected to provide training and testing data closer to real-world scenarios.
Additionally, we propose a new validation metric that can be employed to assess
whether the training of deep learning models addresses both the classification
of known samples and the rejection of unknown samples. We use the protocols to
compare the performance of two baseline open-set algorithms to the standard
SoftMax baseline and find that the algorithms work well on negative samples
that have been seen during training, and partially on out-of-distribution
detection tasks, but drop performance in the presence of samples from
previously unseen unknown classes.",-0.032186437,-0.011994614,-0.009601482,C
12412,"In the future, we will further study such physical affections and
design a discriminative and robust face recognition model.","Extensive
experiments on the FFHQ and CelebA-HQ datasets indicate that the proposed attack method Adv-
Attribute has strong attack transferability across different face recognition models and yields natural
and inconspicuous adversarial faces.","9
Negative Societal Impacts

The proposed method may be used maliciously to hazard the security of existing face recognition
models in real life.",2022-10-13 09:56:36+00:00,Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shuai Jia'), arxiv.Result.Author('Bangjie Yin'), arxiv.Result.Author('Taiping Yao'), arxiv.Result.Author('Shouhong Ding'), arxiv.Result.Author('Chunhua Shen'), arxiv.Result.Author('Xiaokang Yang'), arxiv.Result.Author('Chao Ma')]","Deep learning models have shown their vulnerability when dealing with
adversarial attacks. Existing attacks almost perform on low-level instances,
such as pixels and super-pixels, and rarely exploit semantic clues. For face
recognition attacks, existing methods typically generate the l_p-norm
perturbations on pixels, however, resulting in low attack transferability and
high vulnerability to denoising defense models. In this work, instead of
performing perturbations on the low-level pixels, we propose to generate
attacks through perturbing on the high-level semantics to improve attack
transferability. Specifically, a unified flexible framework, Adversarial
Attributes (Adv-Attribute), is designed to generate inconspicuous and
transferable attacks on face recognition, which crafts the adversarial noise
and adds it into different attributes based on the guidance of the difference
in face recognition features from the target. Moreover, the importance-aware
attribute selection and the multi-objective optimization strategy are
introduced to further ensure the balance of stealthiness and attacking
strength. Extensive experiments on the FFHQ and CelebA-HQ datasets show that
the proposed Adv-Attribute method achieves the state-of-the-art attacking
success rates while maintaining better visual effects against recent attack
methods.",-0.079675995,-0.13447258,-0.057333257,C
12413,"In the future, we will further study such physical affections and
design a discriminative and robust face recognition model.","Extensive
experiments on the FFHQ and CelebA-HQ datasets indicate that the proposed attack method Adv-
Attribute has strong attack transferability across different face recognition models and yields natural
and inconspicuous adversarial faces.","9
Negative Societal Impacts

The proposed method may be used maliciously to hazard the security of existing face recognition
models in real life.",2022-10-13 09:56:36+00:00,Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shuai Jia'), arxiv.Result.Author('Bangjie Yin'), arxiv.Result.Author('Taiping Yao'), arxiv.Result.Author('Shouhong Ding'), arxiv.Result.Author('Chunhua Shen'), arxiv.Result.Author('Xiaokang Yang'), arxiv.Result.Author('Chao Ma')]","Deep learning models have shown their vulnerability when dealing with
adversarial attacks. Existing attacks almost perform on low-level instances,
such as pixels and super-pixels, and rarely exploit semantic clues. For face
recognition attacks, existing methods typically generate the l_p-norm
perturbations on pixels, however, resulting in low attack transferability and
high vulnerability to denoising defense models. In this work, instead of
performing perturbations on the low-level pixels, we propose to generate
attacks through perturbing on the high-level semantics to improve attack
transferability. Specifically, a unified flexible framework, Adversarial
Attributes (Adv-Attribute), is designed to generate inconspicuous and
transferable attacks on face recognition, which crafts the adversarial noise
and adds it into different attributes based on the guidance of the difference
in face recognition features from the target. Moreover, the importance-aware
attribute selection and the multi-objective optimization strategy are
introduced to further ensure the balance of stealthiness and attacking
strength. Extensive experiments on the FFHQ and CelebA-HQ datasets show that
the proposed Adv-Attribute method achieves the state-of-the-art attacking
success rates while maintaining better visual effects against recent attack
methods.",-0.079675995,-0.13447258,-0.057333257,C
12414,"Meanwhile, we have made our dataset
       publicly available, which can promote further research and evaluation.","To the best of our
       knowledge, this is the largest matting dataset with diverse foreground objects, which can
       further improve the robustness of HAttMatting++.",A preliminary version of this work was presented earlier in [29].,2022-10-13 11:16:49+00:00,Hierarchical and Progressive Image Matting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Qiao'), arxiv.Result.Author('Yuhao Liu'), arxiv.Result.Author('Ziqi Wei'), arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Qiang Cai'), arxiv.Result.Author('Guofeng Zhang'), arxiv.Result.Author('Xin Yang')]","Most matting researches resort to advanced semantics to achieve high-quality
alpha mattes, and direct low-level features combination is usually explored to
complement alpha details. However, we argue that appearance-agnostic
integration can only provide biased foreground details and alpha mattes require
different-level feature aggregation for better pixel-wise opacity perception.
In this paper, we propose an end-to-end Hierarchical and Progressive Attention
Matting Network (HAttMatting++), which can better predict the opacity of the
foreground from single RGB images without additional input. Specifically, we
utilize channel-wise attention to distill pyramidal features and employ spatial
attention at different levels to filter appearance cues. This progressive
attention mechanism can estimate alpha mattes from adaptive semantics and
semantics-indicated boundaries. We also introduce a hybrid loss function fusing
Structural SIMilarity (SSIM), Mean Square Error (MSE), Adversarial loss, and
sentry supervision to guide the network to further improve the overall
foreground structure. Besides, we construct a large-scale and challenging image
matting dataset comprised of 59, 600 training images and 1000 test images (a
total of 646 distinct foreground alpha mattes), which can further improve the
robustness of our hierarchical and progressive aggregation model. Extensive
experiments demonstrate that the proposed HAttMatting++ can capture
sophisticated foreground structures and achieve state-of-the-art performance
with single RGB images as input.",-0.2484637,0.20806953,0.035404325,B
12434,"First, further research is needed in the expression subset, where our
performance is not as good as the rest.","On the other hand, results of subsets where our approach is not competitive also bear
some relevant insights.","This is due to the fact that the 3D facial model used to
initialize the cascade is rigid (see Fig.",2022-10-13 17:58:02+00:00,Shape Preserving Facial Landmarks with Graph Attention Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Andr√©s Prados-Torreblanca'), arxiv.Result.Author('Jos√© M. Buenaposada'), arxiv.Result.Author('Luis Baumela')]","Top-performing landmark estimation algorithms are based on exploiting the
excellent ability of large convolutional neural networks (CNNs) to represent
local appearance. However, it is well known that they can only learn weak
spatial relationships. To address this problem, we propose a model based on the
combination of a CNN with a cascade of Graph Attention Network regressors. To
this end, we introduce an encoding that jointly represents the appearance and
location of facial landmarks and an attention mechanism to weigh the
information according to its reliability. This is combined with a multi-task
approach to initialize the location of graph nodes and a coarse-to-fine
landmark description scheme. Our experiments confirm that the proposed model
learns a global representation of the structure of the face, achieving top
performance in popular benchmarks on head pose and landmark estimation. The
improvement provided by our model is most significant in situations involving
large changes in the local appearance of landmarks.",0.15211228,0.04824773,-0.0391765,A
12439,"The code will be made publicly available upon
Joint optimization In Table 4 of the main paper, we pre-           publication to spark further research in Composite Learn-
sented the performance of the baseline multi-task model            ing (CompL).","was adapted to enable for the joint optimization of the ex-
                                                                   isting algorithms with supervised methods (semantic seg-
C. Multi-Task Model (Semseg and Depth)                             mentation, monocular depth estimation, and boundary de-
                                                                   tection).","(Depth + Semseg), and the model trained jointly with
DenseCL (Depth + Semseg + DenseCL).",2022-10-13 17:59:16+00:00,Composite Learning for Robust and Effective Dense Predictions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Menelaos Kanakis'), arxiv.Result.Author('Thomas E. Huang'), arxiv.Result.Author('David Bruggemann'), arxiv.Result.Author('Fisher Yu'), arxiv.Result.Author('Luc Van Gool')]","Multi-task learning promises better model generalization on a target task by
jointly optimizing it with an auxiliary task. However, the current practice
requires additional labeling efforts for the auxiliary task, while not
guaranteeing better model performance. In this paper, we find that jointly
training a dense prediction (target) task with a self-supervised (auxiliary)
task can consistently improve the performance of the target task, while
eliminating the need for labeling auxiliary tasks. We refer to this joint
training as Composite Learning (CompL). Experiments of CompL on monocular depth
estimation, semantic segmentation, and boundary detection show consistent
performance improvements in fully and partially labeled datasets. Further
analysis on depth estimation reveals that joint training with self-supervision
outperforms most labeled auxiliary tasks. We also find that CompL can improve
model robustness when the models are evaluated in new domains. These results
demonstrate the benefits of self-supervision as an auxiliary task, and
establish the design of novel task-specific self-supervised methods as a new
axis of investigation for future multi-task learning research.",-0.033365082,-0.17180067,-0.027895624,C
12442,"7 and      our approach will encourage further research on 3D GAN
Table 2.","We hope that
single-encoder methods and show the results in Fig.","We show that using a hybrid approach consisting       inversion, which will be further utilized with the single view
of learned encoder E and gradient-based optimization is the    3D reconstruction and semantic attribute editings.",2022-10-13 19:06:58+00:00,3D GAN Inversion with Pose Optimization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jaehoon Ko'), arxiv.Result.Author('Kyusun Cho'), arxiv.Result.Author('Daewon Choi'), arxiv.Result.Author('Kwangrok Ryoo'), arxiv.Result.Author('Seungryong Kim')]","With the recent advances in NeRF-based 3D aware GANs quality, projecting an
image into the latent space of these 3D-aware GANs has a natural advantage over
2D GAN inversion: not only does it allow multi-view consistent editing of the
projected image, but it also enables 3D reconstruction and novel view synthesis
when given only a single image. However, the explicit viewpoint control acts as
a main hindrance in the 3D GAN inversion process, as both camera pose and
latent code have to be optimized simultaneously to reconstruct the given image.
Most works that explore the latent space of the 3D-aware GANs rely on
ground-truth camera viewpoint or deformable 3D model, thus limiting their
applicability. In this work, we introduce a generalizable 3D GAN inversion
method that infers camera viewpoint and latent code simultaneously to enable
multi-view consistent semantic image editing. The key to our approach is to
leverage pre-trained estimators for better initialization and utilize the
pixel-wise depth calculated from NeRF parameters to better reconstruct the
given image. We conduct extensive experiments on image reconstruction and
editing both quantitatively and qualitatively, and further compare our results
with 2D GAN-based editing to demonstrate the advantages of utilizing the latent
space of 3D GANs.",-0.14459883,-0.06930091,0.18558304,C
12443,"We hope that                    In CVPR, 2020.
our approach will encourage further research on 3D GAN
inversion, which will be further utilized with the single view   [9] Antonia Creswell and Anil Anthony Bharath.",employing 3D GANs as an editing tool.,"Inverting
3D reconstruction and semantic attribute editings.",2022-10-13 19:06:58+00:00,3D GAN Inversion with Pose Optimization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jaehoon Ko'), arxiv.Result.Author('Kyusun Cho'), arxiv.Result.Author('Daewon Choi'), arxiv.Result.Author('Kwangrok Ryoo'), arxiv.Result.Author('Seungryong Kim')]","With the recent advances in NeRF-based 3D aware GANs quality, projecting an
image into the latent space of these 3D-aware GANs has a natural advantage over
2D GAN inversion: not only does it allow multi-view consistent editing of the
projected image, but it also enables 3D reconstruction and novel view synthesis
when given only a single image. However, the explicit viewpoint control acts as
a main hindrance in the 3D GAN inversion process, as both camera pose and
latent code have to be optimized simultaneously to reconstruct the given image.
Most works that explore the latent space of the 3D-aware GANs rely on
ground-truth camera viewpoint or deformable 3D model, thus limiting their
applicability. In this work, we introduce a generalizable 3D GAN inversion
method that infers camera viewpoint and latent code simultaneously to enable
multi-view consistent semantic image editing. The key to our approach is to
leverage pre-trained estimators for better initialization and utilize the
pixel-wise depth calculated from NeRF parameters to better reconstruct the
given image. We conduct extensive experiments on image reconstruction and
editing both quantitatively and qualitatively, and further compare our results
with 2D GAN-based editing to demonstrate the advantages of utilizing the latent
space of 3D GANs. Additional results and visualizations are available at
https://3dgan-inversion.github.io .",-0.17021236,0.09179709,0.12286888,B
12451,"The use of image style transfer is
real input data into ST using the image-to-image translation,                    of signiÔ¨Åcant value for further research in the Ô¨Åeld of adapting
however, the segmentation score increased from 61.10% to                         the domain of synthetic renders to real images of 3-D printed
75.19% for the model trained solely on synthetic data.",When converting the                         manufacturing deviations.,This                      products.,2022-10-14 02:32:14+00:00,Synthetic-to-real Composite Semantic Segmentation in Additive Manufacturing,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Aliaksei Petsiuk'), arxiv.Result.Author('Harnoor Singh'), arxiv.Result.Author('Himanshu Dadhwal'), arxiv.Result.Author('Joshua M. Pearce')]","The application of computer vision and machine learning methods in the field
of additive manufacturing (AM) for semantic segmentation of the structural
elements of 3-D printed products will improve real-time failure analysis
systems and can potentially reduce the number of defects by enabling in situ
corrections. This work demonstrates the possibilities of using physics-based
rendering for labeled image dataset generation, as well as image-to-image
translation capabilities to improve the accuracy of real image segmentation for
AM systems. Multi-class semantic segmentation experiments were carried out
based on the U-Net model and cycle generative adversarial network. The test
results demonstrated the capacity of detecting such structural elements of 3-D
printed parts as a top layer, infill, shell, and support. A basis for further
segmentation system enhancement by utilizing image-to-image style transfer and
domain adaptation technologies was also developed. The results indicate that
using style transfer as a precursor to domain adaptation can significantly
improve real 3-D printing image segmentation in situations where a model
trained on synthetic data is the only tool available. The mean intersection
over union (mIoU) scores for synthetic test datasets included 94.90% for the
entire 3-D printed part, 73.33% for the top layer, 78.93% for the infill,
55.31% for the shell, and 69.45% for supports.",-0.09728454,0.06543109,0.09763757,C
12488,"ods that utilize partially labeled IMVs for training have great   Subsequently, MSI utilizes branch Œìprop to propagate key-frame
potential and deserve further research.","MSI Ô¨Årst automatically selects key frames
IMV frames for body parsing is costly, semi-supervised meth-      and then employs branch Œìseg to semantically segment them.",segmentation results to other non-key frames.,2022-10-14 18:46:30+00:00,Semi-supervised Body Parsing and Pose Estimation for Enhancing Infant General Movement Assessment,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haomiao Ni'), arxiv.Result.Author('Yuan Xue'), arxiv.Result.Author('Liya Ma'), arxiv.Result.Author('Qian Zhang'), arxiv.Result.Author('Xiaoye Li'), arxiv.Result.Author('Xiaolei Huang')]","General movement assessment (GMA) of infant movement videos (IMVs) is an
effective method for early detection of cerebral palsy (CP) in infants. We
demonstrate in this paper that end-to-end trainable neural networks for image
sequence recognition can be applied to achieve good results in GMA, and more
importantly, augmenting raw video with infant body parsing and pose estimation
information can significantly improve performance. To solve the problem of
efficiently utilizing partially labeled IMVs for body parsing, we propose a
semi-supervised model, termed SiamParseNet (SPN), which consists of two
branches, one for intra-frame body parts segmentation and another for
inter-frame label propagation. During training, the two branches are jointly
trained by alternating between using input pairs of only labeled frames and
input of both labeled and unlabeled frames. We also investigate training data
augmentation by proposing a factorized video generative adversarial network
(FVGAN) to synthesize novel labeled frames for training. When testing, we
employ a multi-source inference mechanism, where the final result for a test
frame is either obtained via the segmentation branch or via propagation from a
nearby key frame. We conduct extensive experiments for body parsing using SPN
on two infant movement video datasets, where SPN coupled with FVGAN achieves
state-of-the-art performance. We further demonstrate that SPN can be easily
adapted to the infant pose estimation task with superior performance. Last but
not least, we explore the clinical application of our method for GMA. We
collected a new clinical IMV dataset with GMA annotations, and our experiments
show that SPN models for body parsing and pose estimation trained on the first
two datasets generalize well to the new clinical dataset and their results can
significantly boost the CRNN-based GMA prediction performance.",-0.12587386,-0.086856976,-0.13748708,C
12514,"5 Ablation studies

We further study the impact of increasing mining budget on our REM approach
(Table 2).","The active learning experiments show that: (1) Both data-centric and model-
centric approaches significantly help to improve performance on the rare subset,
and a combination of the two can further boost the long-tail performance, (2)
While heuristics based mining methods (‚ÄúPredict Size‚Äù) can achieved targeted
improvement for large vehicles, it likely fails to capture other degrees of rareness,
resulting in lower overall performance.","With a small increase of mining budget (6%), we can match the
performance of a fully-supervised model for both common and rare subsets.",2022-10-15 20:52:07+00:00,Improving the Intra-class Long-tail in 3D Detection via Rare Example Mining,cs.CV,"['cs.CV', 'cs.LG', '68T45']","[arxiv.Result.Author('Chiyu Max Jiang'), arxiv.Result.Author('Mahyar Najibi'), arxiv.Result.Author('Charles R. Qi'), arxiv.Result.Author('Yin Zhou'), arxiv.Result.Author('Dragomir Anguelov')]","Continued improvements in deep learning architectures have steadily advanced
the overall performance of 3D object detectors to levels on par with humans for
certain tasks and datasets, where the overall performance is mostly driven by
common examples. However, even the best performing models suffer from the most
naive mistakes when it comes to rare examples that do not appear frequently in
the training data, such as vehicles with irregular geometries. Most studies in
the long-tail literature focus on class-imbalanced classification problems with
known imbalanced label counts per class, but they are not directly applicable
to the intra-class long-tail examples in problems with large intra-class
variations such as 3D object detection, where instances with the same class
label can have drastically varied properties such as shapes and sizes. Other
works propose to mitigate this problem using active learning based on the
criteria of uncertainty, difficulty, or diversity. In this study, we identify a
new conceptual dimension - rareness - to mine new data for improving the
long-tail performance of models. We show that rareness, as opposed to
difficulty, is the key to data-centric improvements for 3D detectors, since
rareness is the result of a lack in data support while difficulty is related to
the fundamental ambiguity in the problem. We propose a general and effective
method to identify the rareness of objects based on density estimation in the
feature space using flow models, and propose a principled cost-aware
formulation for mining rare object tracks, which improves overall model
performance, but more importantly - significantly improves the performance for
rare objects (by 30.97\%",0.24661791,-0.1627451,-0.1445185,A
12515,"To summarize, our main contributions are as follows:

        ‚Ä¢ We Ô¨Årstly propose a new task termed Semantic Video Moments Retrieval at scale (SVMR), which aims at
           simultaneously retrieving videos and re-localizing semantic similar clips from retrieved videos via a query
           clip, to meet real application requirement;

        ‚Ä¢ Two new benchmarks built from ActivityNet-1.3 [1] and HACS [2] are constructed for comprehensive
           evaluation of SVMR and they will be released to the community for further study in the future;

        ‚Ä¢ A novel two-stage framework, which Ô¨Årstly obtains candidate reference videos via similarity search by
           leveraging a two-branch auto-encoder framework and then applies query-reference semantic re-localization
          via our attention-based alignment model, is proposed as a baseline for SVMR and extensive experiments are
           carried out to validate its effectiveness and superiority.","Extensive experiments are carried out on these benchmarks to show that our solution outperforms
several reference solutions.","2 Related Work

2.1 Video Retrieval

Content-based video retrieval [3, 4, 5] has been developed for decades.",2022-10-15 22:46:22+00:00,Semantic Video Moments Retrieval at Scale: A New Task and a Baseline,cs.CV,['cs.CV'],[arxiv.Result.Author('Na Li')],"Motivated by the increasing need of saving search effort by obtaining
relevant video clips instead of whole videos, we propose a new task, named
Semantic Video Moments Retrieval at scale (SVMR), which aims at finding
relevant videos coupled with re-localizing the video clips in them. Instead of
a simple combination of video retrieval and video re-localization, our task is
more challenging because of several essential aspects. In the 1st stage, our
SVMR should take into account the fact that: 1) a positive candidate long video
can contain plenty of irrelevant clips which are also semantically meaningful.
2) a long video can be positive to two totally different query clips if it
contains clips relevant to two queries. The 2nd re-localization stage also
exhibits different assumptions from existing video re-localization tasks, which
hold an assumption that the reference video must contain semantically similar
segments corresponding to the query clip. Instead, in our scenario, the
retrieved long video can be a false positive one due to the inaccuracy of the
first stage. To address these challenges, we propose our two-stage baseline
solution of candidate videos retrieval followed by a novel attention-based
query-reference semantically alignment framework to re-localize target clips
from candidate videos. Furthermore, we build two more appropriate benchmark
datasets from the off-the-shelf ActivityNet-1.3 and HACS for a thorough
evaluation of SVMR models. Extensive experiments are carried out to show that
our solution outperforms several reference solutions.",-0.19244272,-0.16650474,-0.16888264,C
12516,We hope it will inspire further research into this direction.,"Furthermore, we conduct expensive experiments on this problem to show the remarkable
performance compared to baseline methods.","References

 [1] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles, ‚ÄúActivitynet: A large-scale
      video benchmark for human activity understanding,‚Äù in Proceedings of the IEEE Conference on Computer Vision
      and Pattern Recognition (CVPR), 2015, pp.",2022-10-15 22:46:22+00:00,Semantic Video Moments Retrieval at Scale: A New Task and a Baseline,cs.CV,['cs.CV'],[arxiv.Result.Author('Na Li')],"Motivated by the increasing need of saving search effort by obtaining
relevant video clips instead of whole videos, we propose a new task, named
Semantic Video Moments Retrieval at scale (SVMR), which aims at finding
relevant videos coupled with re-localizing the video clips in them. Instead of
a simple combination of video retrieval and video re-localization, our task is
more challenging because of several essential aspects. In the 1st stage, our
SVMR should take into account the fact that: 1) a positive candidate long video
can contain plenty of irrelevant clips which are also semantically meaningful.
2) a long video can be positive to two totally different query clips if it
contains clips relevant to two queries. The 2nd re-localization stage also
exhibits different assumptions from existing video re-localization tasks, which
hold an assumption that the reference video must contain semantically similar
segments corresponding to the query clip. Instead, in our scenario, the
retrieved long video can be a false positive one due to the inaccuracy of the
first stage. To address these challenges, we propose our two-stage baseline
solution of candidate videos retrieval followed by a novel attention-based
query-reference semantically alignment framework to re-localize target clips
from candidate videos. Furthermore, we build two more appropriate benchmark
datasets from the off-the-shelf ActivityNet-1.3 and HACS for a thorough
evaluation of SVMR models. Extensive experiments are carried out to show that
our solution outperforms several reference solutions.",-0.19373754,-0.035381515,-0.15867546,B
12525,"The attained results are expected to pave the path for further research in-
                                        volving deep learning using remote sensing images.","The paper presents a novel
                                        spatial aware encoder and decoder architecture to maintain the contextual in-
                                        formation and structure of sparse ground truth patches present in the images.","The code is available at
                                        https://github.com/sheikhazhanmohammed/SADMA.git
                                        Keywords: Marine Debris Detection, High-Resolution Optical Remote
                                        Sensing (RS) Image, Attention Mechanism, Image Segmentation

                                        1.",2022-10-16 10:59:32+00:00,ResAttUNet: Detecting Marine Debris using an Attention activated Residual UNet,cs.CV,"['cs.CV', 'eess.IV']",[arxiv.Result.Author('Azhan Mohammed')],"Currently, a significant amount of research has been done in field of Remote
Sensing with the use of deep learning techniques. The introduction of Marine
Debris Archive (MARIDA), an open-source dataset with benchmark results, for
marine debris detection opened new pathways to use deep learning techniques for
the task of debris detection and segmentation. This paper introduces a novel
attention based segmentation technique that outperforms the existing
state-of-the-art results introduced with MARIDA. The paper presents a novel
spatial aware encoder and decoder architecture to maintain the contextual
information and structure of sparse ground truth patches present in the images.
The attained results are expected to pave the path for further research
involving deep learning using remote sensing images. The code is available at
https://github.com/sheikhazhanmohammed/SADMA.git",-0.32517326,0.019983487,0.21582282,B
12542,"Crucial to this work is the
behavior of coordinate networks in the presence of noise, which warrants further study both
for supervised and unsupervised regimes.","In the future, we plan to extensively evaluate across diÔ¨Äerent encoding, decoding, and
denoising methods and also assess downstream impact via pixel-level quantitative MRI
metrics and reader studies on various pathological regions.","6
          Scale-Agnostic Super-Resolution in MRI using Coordinate Networks

References

Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, and Richard Zhang.",2022-10-17 00:42:12+00:00,Scale-Agnostic Super-Resolution in MRI using Feature-Based Coordinate Networks,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dave Van Veen'), arxiv.Result.Author('Rogier van der Sluijs'), arxiv.Result.Author('Batu Ozturkler'), arxiv.Result.Author('Arjun Desai'), arxiv.Result.Author('Christian Bluethgen'), arxiv.Result.Author('Robert D. Boutin'), arxiv.Result.Author('Marc H. Willis'), arxiv.Result.Author('Gordon Wetzstein'), arxiv.Result.Author('David Lindell'), arxiv.Result.Author('Shreyas Vasanawala'), arxiv.Result.Author('John Pauly'), arxiv.Result.Author('Akshay S. Chaudhari')]","We propose using a coordinate network decoder for the task of
super-resolution in MRI. The continuous signal representation of coordinate
networks enables this approach to be scale-agnostic, i.e. one can train over a
continuous range of scales and subsequently query at arbitrary resolutions. Due
to the difficulty of performing super-resolution on inherently noisy data, we
analyze network behavior under multiple denoising strategies. Lastly we compare
this method to a standard convolutional decoder using both quantitative metrics
and a radiologist study implemented in Voxel, our newly developed tool for
web-based evaluation of medical images.",0.038311448,0.07724733,0.19995612,C
12543,"Crucial to this work is the
behavior of coordinate networks in the presence of noise, which warrants further study both
for supervised and unsupervised regimes.","In the future, we plan to extensively evaluate across diÔ¨Äerent encoding, decoding, and
denoising methods and also assess downstream impact via pixel-level quantitative MRI
metrics and reader studies on various pathological regions.","6
          Scale-Agnostic Super-Resolution in MRI using Coordinate Networks

References

Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, and Richard Zhang.",2022-10-17 00:42:12+00:00,Scale-Agnostic Super-Resolution in MRI using Feature-Based Coordinate Networks,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Dave Van Veen'), arxiv.Result.Author('Rogier van der Sluijs'), arxiv.Result.Author('Batu Ozturkler'), arxiv.Result.Author('Arjun Desai'), arxiv.Result.Author('Christian Bluethgen'), arxiv.Result.Author('Robert D. Boutin'), arxiv.Result.Author('Marc H. Willis'), arxiv.Result.Author('Gordon Wetzstein'), arxiv.Result.Author('David Lindell'), arxiv.Result.Author('Shreyas Vasanawala'), arxiv.Result.Author('John Pauly'), arxiv.Result.Author('Akshay S. Chaudhari')]","We propose using a coordinate network decoder for the task of
super-resolution in MRI. The continuous signal representation of coordinate
networks enables this approach to be scale-agnostic, i.e. one can train over a
continuous range of scales and subsequently query at arbitrary resolutions. Due
to the difficulty of performing super-resolution on inherently noisy data, we
analyze network behavior under multiple denoising strategies. Lastly we compare
this method to a standard convolutional decoder using both quantitative metrics
and a radiologist study implemented in Voxel, our newly developed tool for
web-based evaluation of medical images.",0.038311448,0.07724733,0.19995612,C
12558,"We hope
alternative to GPT-3 (Brown et al., 2020), which is    that our work bring inspiration for further research
pretrained with a task-agnostic language modeling      in Ô¨Çexible, modular AI systems for solving vision-
                                                       language tasks.","PNP-VQA achieves state-of-the-arts per-
a decoder-only model, a much smaller open-source       formance on multiple VQA benchmarks.","7 Limitations                                                Purohit, Laria Reynolds, Jonathan Tow, Ben Wang,
                                                             and Samuel Weinbach.",2022-10-17 06:29:54+00:00,Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training,cs.CV,['cs.CV'],"[arxiv.Result.Author('Anthony Meng Huat Tiong'), arxiv.Result.Author('Junnan Li'), arxiv.Result.Author('Boyang Li'), arxiv.Result.Author('Silvio Savarese'), arxiv.Result.Author('Steven C. H. Hoi')]","Visual question answering (VQA) is a hallmark of vision and language
reasoning and a challenging task under the zero-shot setting. We propose
Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast
to most existing works, which require substantial adaptation of pretrained
language models (PLMs) for the vision modality, PNP-VQA requires no additional
training of the PLMs. Instead, we propose to use natural language and network
interpretation as an intermediate representation that glues pretrained models
together. We first generate question-guided informative image captions, and
pass the captions to a PLM as context for question answering. Surpassing
end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on
zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter
Flamingo model by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an
improvement of 9.1% on GQA over FewVLM with 740M PLM parameters. Code is
released at https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa",-0.06986807,-0.14562109,-0.0032811854,C
12560,"A
new TDS dataset is needed for further study, as there are no such datasets yet.","As we
are fairly familiar with them and form strong muscle memories about writing them, the TDS contains
rich discriminative personal writing characteristics and is a potential identity-veriÔ¨Åcation medium.","To facilitate related research and inspire future work, we establish the Multimodal Signature and
Digit String (MSDS) dataset, a multimodal online and ofÔ¨Çine handwriting dataset that comprises two
subsets: MSDS-ChS and MSDS-TDS, where the former contains handwritten Chinense signatures
and the latter contains handwritten TDS.",2022-10-17 08:23:12+00:00,MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Peirong Zhang'), arxiv.Result.Author('Jiajia Jiang'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Lianwen Jin')]","Although online handwriting verification has made great progress recently,
the verification performances are still far behind the real usage owing to the
small scale of the datasets as well as the limited biometric mediums.
Therefore, this paper proposes a new handwriting verification benchmark dataset
named Multimodal Signature and Digit String (MSDS), which consists of two
subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings),
contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per
user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to
the best of our knowledge, is the largest publicly available Chinese signature
dataset for handwriting verification, at least eight times larger than existing
online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit
Strings, i.e, the actual phone numbers of users, which have not been explored
yet. Extensive experiments with different baselines are respectively conducted
for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of
state-of-the-art methods on MSDS-TDS are generally better than those on
MSDS-ChS, which indicates that the handwritten Token Digit String could be a
more effective biometric than handwritten Chinese signature. This is a
promising discovery that could inspire us to explore new biometric traits. The
MSDS dataset is available at https://github.com/HCIILAB/MSDS.",0.07384449,-0.1744039,-0.13128778,C
12561,"A
new TDS dataset is needed for further study, as there are no such datasets yet.","As we
are fairly familiar with them and form strong muscle memories about writing them, the TDS contains
rich discriminative personal writing characteristics and is a potential identity-veriÔ¨Åcation medium.","To facilitate related research and inspire future work, we establish the Multimodal Signature and
Digit String (MSDS) dataset, a multimodal online and ofÔ¨Çine handwriting dataset that comprises two
subsets: MSDS-ChS and MSDS-TDS, where the former contains handwritten Chinense signatures
and the latter contains handwritten TDS.",2022-10-17 08:23:12+00:00,MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Peirong Zhang'), arxiv.Result.Author('Jiajia Jiang'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Lianwen Jin')]","Although online handwriting verification has made great progress recently,
the verification performances are still far behind the real usage owing to the
small scale of the datasets as well as the limited biometric mediums.
Therefore, this paper proposes a new handwriting verification benchmark dataset
named Multimodal Signature and Digit String (MSDS), which consists of two
subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings),
contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per
user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to
the best of our knowledge, is the largest publicly available Chinese signature
dataset for handwriting verification, at least eight times larger than existing
online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit
Strings, i.e, the actual phone numbers of users, which have not been explored
yet. Extensive experiments with different baselines are respectively conducted
for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of
state-of-the-art methods on MSDS-TDS are generally better than those on
MSDS-ChS, which indicates that the handwritten Token Digit String could be a
more effective biometric than handwritten Chinese signature. This is a
promising discovery that could inspire us to explore new biometric traits. The
MSDS dataset is available at https://github.com/HCIILAB/MSDS.",0.07384449,-0.1744039,-0.13128778,C
12562,"A
new TDS dataset is needed for further study, as there are no such datasets yet.","As we
are fairly familiar with them and form strong muscle memories about writing them, the TDS contains
rich discriminative personal writing characteristics and is a potential identity-veriÔ¨Åcation medium.","To facilitate related research and inspire future work, we establish the Multimodal Signature and
Digit String (MSDS) dataset, a multimodal online and ofÔ¨Çine handwriting dataset that comprises two
subsets: MSDS-ChS and MSDS-TDS, where the former contains handwritten Chinense signatures
and the latter contains handwritten TDS.",2022-10-17 08:23:12+00:00,MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Peirong Zhang'), arxiv.Result.Author('Jiajia Jiang'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Lianwen Jin')]","Although online handwriting verification has made great progress recently,
the verification performances are still far behind the real usage owing to the
small scale of the datasets as well as the limited biometric mediums.
Therefore, this paper proposes a new handwriting verification benchmark dataset
named Multimodal Signature and Digit String (MSDS), which consists of two
subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings),
contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per
user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to
the best of our knowledge, is the largest publicly available Chinese signature
dataset for handwriting verification, at least eight times larger than existing
online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit
Strings, i.e, the actual phone numbers of users, which have not been explored
yet. Extensive experiments with different baselines are respectively conducted
for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of
state-of-the-art methods on MSDS-TDS are generally better than those on
MSDS-ChS, which indicates that the handwritten Token Digit String could be a
more effective biometric than handwritten Chinese signature. This is a
promising discovery that could inspire us to explore new biometric traits. The
MSDS dataset is available at https://github.com/HCIILAB/MSDS.",0.07384449,-0.1744039,-0.13128778,C
12563,"A
new TDS dataset is needed for further study, as there are no such datasets yet.","As we
are fairly familiar with them and form strong muscle memories about writing them, the TDS contains
rich discriminative personal writing characteristics and is a potential identity-veriÔ¨Åcation medium.","To facilitate related research and inspire future work, we establish the Multimodal Signature and
Digit String (MSDS) dataset, a multimodal online and ofÔ¨Çine handwriting dataset that comprises two
subsets: MSDS-ChS and MSDS-TDS, where the former contains handwritten Chinense signatures
and the latter contains handwritten TDS.",2022-10-17 08:23:12+00:00,MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Peirong Zhang'), arxiv.Result.Author('Jiajia Jiang'), arxiv.Result.Author('Yuliang Liu'), arxiv.Result.Author('Lianwen Jin')]","Although online handwriting verification has made great progress recently,
the verification performances are still far behind the real usage owing to the
small scale of the datasets as well as the limited biometric mediums.
Therefore, this paper proposes a new handwriting verification benchmark dataset
named Multimodal Signature and Digit String (MSDS), which consists of two
subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings),
contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per
user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to
the best of our knowledge, is the largest publicly available Chinese signature
dataset for handwriting verification, at least eight times larger than existing
online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit
Strings, i.e, the actual phone numbers of users, which have not been explored
yet. Extensive experiments with different baselines are respectively conducted
for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of
state-of-the-art methods on MSDS-TDS are generally better than those on
MSDS-ChS, which indicates that the handwritten Token Digit String could be a
more effective biometric than handwritten Chinese signature. This is a
promising discovery that could inspire us to explore new biometric traits. The
MSDS dataset is available at https://github.com/HCIILAB/MSDS.",0.07384449,-0.1744039,-0.13128778,C
12566,"In addition, motivated by the recently developed robot that can change colour in real time according to
the background [22], we further study the performance of adaptive DE DAC (A-DE DAC).","To further improve the performance of our proposed method, under the same number of
faces of the model, we introduce diÔ¨Äerential evolution (DE) algorithm to search for the near-optimal attack
area.","One example of
our proposed method is presented in Figure 1.",2022-10-17 09:07:52+00:00,Differential Evolution based Dual Adversarial Camouflage: Fooling Human Eyes and Object Detectors,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Jialiang Sun')],"Recent studies reveal that deep neural network (DNN) based object detectors
are vulnerable to adversarial attacks in the form of adding the perturbation to
the images, leading to the wrong output of object detectors. Most current
existing works focus on generating perturbed images, also called adversarial
examples, to fool object detectors. Though the generated adversarial examples
themselves can remain a certain naturalness, most of them can still be easily
observed by human eyes, which limits their further application in the real
world. To alleviate this problem, we propose a differential evolution based
dual adversarial camouflage (DE_DAC) method, composed of two stages to fool
human eyes and object detectors simultaneously. Specifically, we try to obtain
the camouflage texture, which can be rendered over the surface of the object.
In the first stage, we optimize the global texture to minimize the discrepancy
between the rendered object and the scene images, making human eyes difficult
to distinguish. In the second stage, we design three loss functions to optimize
the local texture, making object detectors ineffective. In addition, we
introduce the differential evolution algorithm to search for the near-optimal
areas of the object to attack, improving the adversarial performance under
certain attack area limitations. Besides, we also study the performance of
adaptive DE_DAC, which can be adapted to the environment. Experiments show that
our proposed method could obtain a good trade-off between the fooling human
eyes and object detectors under multiple specific scenes and objects.",0.104292445,0.16324742,0.060631774,A
12567,"‚Ä¢ Consider improving the adaptability to the environment, we further study the performance of adaptive
       DE DAC.","‚Ä¢ We introduce the DE algorithm to search for the near-optimal area to be attacked, improving the attack
       success rate with the limitation of areas.","Under the same selected local texture, compared with DE DAC, adaptive DE DAC can further
       improve the camouÔ¨Çage performance.",2022-10-17 09:07:52+00:00,Differential Evolution based Dual Adversarial Camouflage: Fooling Human Eyes and Object Detectors,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Jialiang Sun')],"Recent studies reveal that deep neural network (DNN) based object detectors
are vulnerable to adversarial attacks in the form of adding the perturbation to
the images, leading to the wrong output of object detectors. Most current
existing works focus on generating perturbed images, also called adversarial
examples, to fool object detectors. Though the generated adversarial examples
themselves can remain a certain naturalness, most of them can still be easily
observed by human eyes, which limits their further application in the real
world. To alleviate this problem, we propose a differential evolution based
dual adversarial camouflage (DE_DAC) method, composed of two stages to fool
human eyes and object detectors simultaneously. Specifically, we try to obtain
the camouflage texture, which can be rendered over the surface of the object.
In the first stage, we optimize the global texture to minimize the discrepancy
between the rendered object and the scene images, making human eyes difficult
to distinguish. In the second stage, we design three loss functions to optimize
the local texture, making object detectors ineffective. In addition, we
introduce the differential evolution algorithm to search for the near-optimal
areas of the object to attack, improving the adversarial performance under
certain attack area limitations. Besides, we also study the performance of
adaptive DE_DAC, which can be adapted to the environment. Experiments show that
our proposed method could obtain a good trade-off between the fooling human
eyes and object detectors under multiple specific scenes and objects.",0.09387219,0.2006772,0.16536368,A
12568,"In addition,
motivated by the recently developed robot that can change color in real-time according to the background [21],
we further study the performance of adaptive DE DAC (A-DE DAC).","To further improve the performance of our proposed method, under the same number of faces of the model,
we introduce diÔ¨Äerential evolution (DE) algorithm to search for the near-optimal attack area.","One example of our proposed method
is presented in Figure 1.",2022-10-17 09:07:52+00:00,Differential Evolution based Dual Adversarial Camouflage: Fooling Human Eyes and Object Detectors,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jialiang Sun'), arxiv.Result.Author('Tingsong Jiang'), arxiv.Result.Author('Wen Yao'), arxiv.Result.Author('Donghua Wang'), arxiv.Result.Author('Xiaoqian Chen')]","Recent studies reveal that deep neural network (DNN) based object detectors
are vulnerable to adversarial attacks in the form of adding the perturbation to
the images, leading to the wrong output of object detectors. Most current
existing works focus on generating perturbed images, also called adversarial
examples, to fool object detectors. Though the generated adversarial examples
themselves can remain a certain naturalness, most of them can still be easily
observed by human eyes, which limits their further application in the real
world. To alleviate this problem, we propose a differential evolution based
dual adversarial camouflage (DE_DAC) method, composed of two stages to fool
human eyes and object detectors simultaneously. Specifically, we try to obtain
the camouflage texture, which can be rendered over the surface of the object.
In the first stage, we optimize the global texture to minimize the discrepancy
between the rendered object and the scene images, making human eyes difficult
to distinguish. In the second stage, we design three loss functions to optimize
the local texture, making object detectors ineffective. In addition, we
introduce the differential evolution algorithm to search for the near-optimal
areas of the object to attack, improving the adversarial performance under
certain attack area limitations. Besides, we also study the performance of
adaptive DE_DAC, which can be adapted to the environment. Experiments show that
our proposed method could obtain a good trade-off between the fooling human
eyes and object detectors under multiple specific scenes and objects.",0.08971338,0.16952124,0.068566754,A
12569,"‚Ä¢ Consider improving the adaptability to the environment, we further study the performance of adaptive
       DE DAC.","‚Ä¢ We introduce the DE algorithm to search for the near-optimal area to be attacked, improving the attack
       success rate with the limitation of areas.","Under the same selected local texture, compared with DE DAC, adaptive DE DAC can further
       improve the camouÔ¨Çage performance.",2022-10-17 09:07:52+00:00,Differential Evolution based Dual Adversarial Camouflage: Fooling Human Eyes and Object Detectors,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jialiang Sun'), arxiv.Result.Author('Tingsong Jiang'), arxiv.Result.Author('Wen Yao'), arxiv.Result.Author('Donghua Wang'), arxiv.Result.Author('Xiaoqian Chen')]","Recent studies reveal that deep neural network (DNN) based object detectors
are vulnerable to adversarial attacks in the form of adding the perturbation to
the images, leading to the wrong output of object detectors. Most current
existing works focus on generating perturbed images, also called adversarial
examples, to fool object detectors. Though the generated adversarial examples
themselves can remain a certain naturalness, most of them can still be easily
observed by human eyes, which limits their further application in the real
world. To alleviate this problem, we propose a differential evolution based
dual adversarial camouflage (DE_DAC) method, composed of two stages to fool
human eyes and object detectors simultaneously. Specifically, we try to obtain
the camouflage texture, which can be rendered over the surface of the object.
In the first stage, we optimize the global texture to minimize the discrepancy
between the rendered object and the scene images, making human eyes difficult
to distinguish. In the second stage, we design three loss functions to optimize
the local texture, making object detectors ineffective. In addition, we
introduce the differential evolution algorithm to search for the near-optimal
areas of the object to attack, improving the adversarial performance under
certain attack area limitations. Besides, we also study the performance of
adaptive DE_DAC, which can be adapted to the environment. Experiments show that
our proposed method could obtain a good trade-off between the fooling human
eyes and object detectors under multiple specific scenes and objects.",0.09387219,0.2006772,0.16536368,A
12574,"D.7 Effect of Normal Smoothness Loss

To further study the impact of the normal smoothness loss, we did an ablation study on the loss term.","When the light distribution becomes narrow (small), the shape accuracy will decrease.",Results in Fig.,2022-10-17 11:01:52+00:00,S$^3$-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Wenqi Yang'), arxiv.Result.Author('Guanying Chen'), arxiv.Result.Author('Chaofeng Chen'), arxiv.Result.Author('Zhenfang Chen'), arxiv.Result.Author('Kwan-Yee K. Wong')]","In this paper, we address the ""dual problem"" of multi-view scene
reconstruction in which we utilize single-view images captured under different
point lights to learn a neural scene representation. Different from existing
single-view methods which can only recover a 2.5D scene representation (i.e., a
normal / depth map for the visible surface), our method learns a neural
reflectance field to represent the 3D geometry and BRDFs of a scene. Instead of
relying on multi-view photo-consistency, our method exploits two
information-rich monocular cues, namely shading and shadow, to infer scene
geometry. Experiments on multiple challenging datasets show that our method is
capable of recovering 3D geometry, including both visible and invisible parts,
of a scene from single-view images. Thanks to the neural reflectance field
representation, our method is robust to depth discontinuities. It supports
applications like novel-view synthesis and relighting. Our code and model can
be found at https://ywq.github.io/s3nerf.",0.22988237,0.31009004,0.14107832,A
12581,"To further study the effect of learned queries, we visualize
in Fig.","The experiments on
query-based Slot-Attention prove that both of our design choices are necessary and effective for
the superior performance of BO-QSA.","2 where we set different numbers of iterative updates of Slot-Attention during inference
on the Stanford Dogs dataset.",2022-10-17 12:14:59+00:00,Unsupervised Object-Centric Learning with Bi-Level Optimized Query Slot Attention,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Baoxiong Jia'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Siyuan Huang')]","The ability to decompose complex natural scenes into meaningful
object-centric abstractions lies at the core of human perception and reasoning.
In the recent culmination of unsupervised object-centric learning, the
Slot-Attention module has played an important role with its simple yet
effective design and fostered many powerful variants. These methods, however,
have been exceedingly difficult to train without supervision and are ambiguous
in the notion of object, especially for complex natural scenes. In this paper,
we propose to address these issues by (1) initializing Slot-Attention modules
with learnable queries and (2) optimizing the model with bi-level optimization.
With simple code adjustments on the vanilla Slot-Attention, our model, Bi-level
Optimized Query Slot Attention, achieves state-of-the-art results on both
synthetic and complex real-world datasets in unsupervised image segmentation
and reconstruction, outperforming previous baselines by a large margin (~10%).
We provide thorough ablative studies to validate the necessity and
effectiveness of our design. Additionally, our model exhibits excellent
potential for concept binding and zero-shot learning. We hope our effort could
provide a single home for the design and learning of slot-based models and pave
the way for more challenging tasks in object-centric learning. Our
implementation is publicly available at
https://github.com/Wall-Facer-liuyu/BO-QSA.",0.096142255,-0.24523798,-0.20681322,C
12612,"To mitigate this, further research on
all tested random seeds) in a few cases.","When the edit is not applied strongly enough,            generative models for image editing, such techniques might
increasing Œ∑ usually achieves the desired result, but it some-       be used by malicious parties for synthesizing fake imagery
times leads to a signiÔ¨Åcant loss of original image details (for      to mislead viewers.","As for zoom and             identiÔ¨Åcation of synthetically edited or generated content is
camera angle changes, these usually occur before the de-             needed.",2022-10-17 17:27:32+00:00,Imagic: Text-Based Real Image Editing with Diffusion Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bahjat Kawar'), arxiv.Result.Author('Shiran Zada'), arxiv.Result.Author('Oran Lang'), arxiv.Result.Author('Omer Tov'), arxiv.Result.Author('Huiwen Chang'), arxiv.Result.Author('Tali Dekel'), arxiv.Result.Author('Inbar Mosseri'), arxiv.Result.Author('Michal Irani')]","Text-conditioned image editing has recently attracted considerable interest.
However, most methods are currently either limited to specific editing types
(e.g., object overlay, style transfer), or apply to synthetically generated
images, or require multiple input images of a common object. In this paper we
demonstrate, for the very first time, the ability to apply complex (e.g.,
non-rigid) text-guided semantic edits to a single real image. For example, we
can change the posture and composition of one or multiple objects inside an
image, while preserving its original characteristics. Our method can make a
standing dog sit down or jump, cause a bird to spread its wings, etc. -- each
within its single high-resolution natural image provided by the user. Contrary
to previous work, our proposed method requires only a single input image and a
target text (the desired edit). It operates on real images, and does not
require any additional inputs (such as image masks or additional views of the
object). Our method, which we call ""Imagic"", leverages a pre-trained
text-to-image diffusion model for this task. It produces a text embedding that
aligns with both the input image and the target text, while fine-tuning the
diffusion model to capture the image-specific appearance. We demonstrate the
quality and versatility of our method on numerous inputs from various domains,
showcasing a plethora of high quality complex semantic image edits, all within
a single unified framework.",-0.0056583136,0.053603355,0.06934349,C
12613,"To mitigate this,
model, it inherits the model‚Äôs generative limitations and bi-        further research on the identiÔ¨Åcation of synthetically edited
ases.","Also, since        techniques might be used by malicious parties for synthe-
our method relies on a pre-trained text-to-image diffusion           sizing fake imagery to mislead viewers.","Therefore, unwanted artifacts are produced when the            or generated content is needed.",2022-10-17 17:27:32+00:00,Imagic: Text-Based Real Image Editing with Diffusion Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bahjat Kawar'), arxiv.Result.Author('Shiran Zada'), arxiv.Result.Author('Oran Lang'), arxiv.Result.Author('Omer Tov'), arxiv.Result.Author('Huiwen Chang'), arxiv.Result.Author('Tali Dekel'), arxiv.Result.Author('Inbar Mosseri'), arxiv.Result.Author('Michal Irani')]","Text-conditioned image editing has recently attracted considerable interest.
However, most methods are currently either limited to specific editing types
(e.g., object overlay, style transfer), or apply to synthetically generated
images, or require multiple input images of a common object. In this paper we
demonstrate, for the very first time, the ability to apply complex (e.g.,
non-rigid) text-guided semantic edits to a single real image. For example, we
can change the posture and composition of one or multiple objects inside an
image, while preserving its original characteristics. Our method can make a
standing dog sit down or jump, cause a bird to spread its wings, etc. -- each
within its single high-resolution natural image provided by the user. Contrary
to previous work, our proposed method requires only a single input image and a
target text (the desired edit). It operates on real images, and does not
require any additional inputs (such as image masks or additional views of the
object). Our method, which we call ""Imagic"", leverages a pre-trained
text-to-image diffusion model for this task. It produces a text embedding that
aligns with both the input image and the target text, while fine-tuning the
diffusion model to capture the image-specific appearance. We demonstrate the
quality and versatility of our method on numerous inputs from various domains,
showcasing a plethora of high quality complex semantic image edits, all within
a single unified framework.",0.033024326,-0.08463029,0.08464509,C
12618,We further study whether two        learning yet poorly in zero-shot transfer.,"We draw on the idea of prototypes [9, 18] by           Empirical evidence reveals that the non-contrastive objec-
introducing bottleneck and l2-norm layers, while they do          tive induces models to perform favorably in representation
not yield a performance gain.","Observing the dis-
projections can share intermediate computation in Tab.",2022-10-17 17:57:46+00:00,Non-Contrastive Learning Meets Language-Image Pre-Training,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinghao Zhou'), arxiv.Result.Author('Li Dong'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Lijuan Wang'), arxiv.Result.Author('Furu Wei')]","Contrastive language-image pre-training (CLIP) serves as a de-facto standard
to align images and texts. Nonetheless, the loose correlation between images
and texts of web-crawled data renders the contrastive objective data
inefficient and craving for a large training batch size. In this work, we
explore the validity of non-contrastive language-image pre-training (nCLIP),
and study whether nice properties exhibited in visual self-supervised models
can emerge. We empirically observe that the non-contrastive objective nourishes
representation learning while sufficiently underperforming under zero-shot
recognition. Based on the above study, we further introduce xCLIP, a
multi-tasking framework combining CLIP and nCLIP, and show that nCLIP aids CLIP
in enhancing feature semantics. The synergy between two objectives lets xCLIP
enjoy the best of both worlds: superior performance in both zero-shot transfer
and representation learning. Systematic evaluation is conducted spanning a wide
variety of downstream tasks including zero-shot classification, out-of-domain
classification, retrieval, visual representation learning, and textual
representation learning, showcasing a consistent performance gain and
validating the effectiveness of xCLIP.",-0.058419,-0.23761111,0.16575819,C
12619,"We further study whether it‚Äôs             tial future work, we may continue to scale up the data size
the best route for two objectives to be optimized in sepa-        (e.g., LAION400M [83]) as well as the model size (e.g.,
rate latent spaces via multi-tasking.","As poten-
tween LCLIP and LnCLIP.","SpeciÔ¨Åcally, we con-        ViT-L [33]) to verify whether the scaling law applies and
                                                                  the performance improvement endures.",2022-10-17 17:57:46+00:00,Non-Contrastive Learning Meets Language-Image Pre-Training,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinghao Zhou'), arxiv.Result.Author('Li Dong'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Lijuan Wang'), arxiv.Result.Author('Furu Wei')]","Contrastive language-image pre-training (CLIP) serves as a de-facto standard
to align images and texts. Nonetheless, the loose correlation between images
and texts of web-crawled data renders the contrastive objective data
inefficient and craving for a large training batch size. In this work, we
explore the validity of non-contrastive language-image pre-training (nCLIP),
and study whether nice properties exhibited in visual self-supervised models
can emerge. We empirically observe that the non-contrastive objective nourishes
representation learning while sufficiently underperforming under zero-shot
recognition. Based on the above study, we further introduce xCLIP, a
multi-tasking framework combining CLIP and nCLIP, and show that nCLIP aids CLIP
in enhancing feature semantics. The synergy between two objectives lets xCLIP
enjoy the best of both worlds: superior performance in both zero-shot transfer
and representation learning. Systematic evaluation is conducted spanning a wide
variety of downstream tasks including zero-shot classification, out-of-domain
classification, retrieval, visual representation learning, and textual
representation learning, showcasing a consistent performance gain and
validating the effectiveness of xCLIP.",0.2872834,-0.0744427,0.13281976,A
12630,"By better understanding the role that a
model‚Äôs latent features play in its decision process, we aim to further research
that improves a CNN‚Äôs ability to generalize with respect to minority classes.","In this paper, we take steps toward demystifying a neural network‚Äôs decision
process for under-represented classes.","Declarations

D. Dablain performed work on the paper during an internship with the OÔ¨Éce
of Naval Research.",2022-10-17 22:40:06+00:00,Understanding CNN Fragility When Learning With Imbalanced Data,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Damien Dablain'), arxiv.Result.Author('Kristen N. Jacobson'), arxiv.Result.Author('Colin Bellinger'), arxiv.Result.Author('Mark Roberts'), arxiv.Result.Author('Nitesh Chawla')]","Convolutional neural networks (CNNs) have achieved impressive results on
imbalanced image data, but they still have difficulty generalizing to minority
classes and their decisions are difficult to interpret. These problems are
related because the method by which CNNs generalize to minority classes, which
requires improvement, is wrapped in a blackbox. To demystify CNN decisions on
imbalanced data, we focus on their latent features. Although CNNs embed the
pattern knowledge learned from a training set in model parameters, the effect
of this knowledge is contained in feature and classification embeddings (FE and
CE). These embeddings can be extracted from a trained model and their global,
class properties (e.g., frequency, magnitude and identity) can be analyzed. We
find that important information regarding the ability of a neural network to
generalize to minority classes resides in the class top-K CE and FE. We show
that a CNN learns a limited number of class top-K CE per category, and that
their number and magnitudes vary based on whether the same class is balanced or
imbalanced. This calls into question whether a CNN has learned intrinsic class
features, or merely frequently occurring ones that happen to exist in the
sampled class distribution. We also hypothesize that latent class diversity is
as important as the number of class examples, which has important implications
for re-sampling and cost-sensitive methods. These methods generally focus on
rebalancing model weights, class numbers and margins; instead of diversifying
class latent features through augmentation. We also demonstrate that a CNN has
difficulty generalizing to test data if the magnitude of its top-K latent
features do not match the training set. We use three popular image datasets and
two cost-sensitive algorithms commonly employed in imbalanced learning for our
experiments.",0.022742134,-0.42043892,0.06190995,C
12632,"That said, there is a lot of room
for further research, including how to automatically adjust the Ô¨Ådelity-expressiveness knobs, how
to increase the chances of a good result, and how to improve generation speed.","We showed that Ô¨Åne-tuning a diffusion model on a single image
is a promising method to bias its output distribution towards that image, and that, surprisingly, editing
capabilities are preserved when using the right sampling methods.","We believe that
Ô¨Åne-tuned text-to-image diffusion models with the sampling mechanisms we propose are a good
starting point for followup research.",2022-10-17 23:46:05+00:00,UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dani Valevski'), arxiv.Result.Author('Matan Kalman'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Yaniv Leviathan')]","We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",0.047739588,-0.072948135,0.111911945,C
12633,"We encourage future research to help mitigate and measure the negative impact of generative models,
and believe thoughtful consideration and further research in all of these matters is necessary prior to
determining how such technologies can be made broadly available.","While we did
not see this effect in our qualitative experiments, more research into bias evaluation methods, both
for image editing and generation will help address this concern.","Acknowledgments

We would like to extend our gratitude to Eyal Molad, Eyal Segalis, Mohammad Norouzi, Yael Pritch,
and Valerie Nygaard, many teams around Google Research for supporting this work, most notably to
the Imagen team, and to our families and friends who volunteered their photos to the paper.",2022-10-17 23:46:05+00:00,UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dani Valevski'), arxiv.Result.Author('Matan Kalman'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Yaniv Leviathan')]","We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",-0.03934668,-0.14570852,-0.092589855,C
12634,"That said, there is a lot of room
for further research, including how to automatically adjust the Ô¨Ådelity-expressiveness knobs, how
to increase the chances of a good result, and how to improve generation speed.","We showed that Ô¨Åne-tuning a diffusion model on a single image
is a promising method to bias its output distribution towards that image, and that, surprisingly, editing
capabilities are preserved when using the right sampling methods.","We believe that
Ô¨Åne-tuned text-to-image diffusion models with the sampling mechanisms we propose are a good
starting point for followup research.",2022-10-17 23:46:05+00:00,UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dani Valevski'), arxiv.Result.Author('Matan Kalman'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Yaniv Leviathan')]","We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",0.047739588,-0.072948135,0.111911945,C
12635,"We encourage future research to help mitigate and measure the potential negative impact of generative
models if misused, and believe thoughtful consideration and further research in all of these matters is
necessary prior to determining how such technologies can and should be made broadly available.","While we did not see this effect in our qualitative experiments, more research into bias evaluation
methods, both for image editing and generation will help address this concern.","Acknowledgments

We would like to extend our gratitude to Eyal Molad, Eyal Segalis, Mohammad Norouzi, William
Chan, Yael Pritch, Valerie Nygaard, many teams around Google Research for supporting this work,
most notably to the Imagen team, and to our families and friends who volunteered their photos to the
paper.",2022-10-17 23:46:05+00:00,UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dani Valevski'), arxiv.Result.Author('Matan Kalman'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Yaniv Leviathan')]","We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",-0.03209975,-0.15658817,-0.060945205,C
12636,"That said, there is a lot of room
for further research, including how to automatically adjust the Ô¨Ådelity-expressiveness knobs, how
to increase the chances of a good result, and how to improve generation speed.","We showed that Ô¨Åne-tuning a diffusion model on a single image
is a promising method to bias its output distribution towards that image, and that, surprisingly, editing
capabilities are preserved when using the right sampling methods.","We believe that
Ô¨Åne-tuned text-to-image diffusion models with the sampling mechanisms we propose are a good
starting point for followup research.",2022-10-17 23:46:05+00:00,UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dani Valevski'), arxiv.Result.Author('Matan Kalman'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Yaniv Leviathan')]","We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",0.047739588,-0.072948135,0.111911945,C
12637,"We encourage future research to help mitigate and measure the potential negative impact of generative
models if misused, and believe thoughtful consideration and further research in all of these matters is
necessary prior to determining how such technologies can be made broadly available.","While we did not see this effect in our qualitative experiments, more research into bias evaluation
methods, both for image editing and generation will help address this concern.","Acknowledgments

We would like to extend our gratitude to Eyal Molad, Eyal Segalis, Mohammad Norouzi, William
Chan, Yael Pritch, Valerie Nygaard, many teams around Google Research for supporting this work,
most notably to the Imagen team, and to our families and friends who volunteered their photos to the
paper.",2022-10-17 23:46:05+00:00,UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dani Valevski'), arxiv.Result.Author('Matan Kalman'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Yaniv Leviathan')]","We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",-0.036180094,-0.1504997,-0.0768416,C
12663,"We Ô¨Ånd that the second task                                                     only for identiÔ¨Åcation
is also challenging and worthy of further study by the face
recognition community.","Both tasks
follow the protocols of LFW.","6k                41.3

    Table 1 summarizes the testing sets used in this paper        extracted from E by two ResBlocks [1], each of which is
for AIFR and Fig.",2022-10-17 07:04:19+00:00,When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework and A New Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhizhong Huang'), arxiv.Result.Author('Junping Zhang'), arxiv.Result.Author('Hongming Shan')]","To minimize the impact of age variation on face recognition, age-invariant
face recognition (AIFR) extracts identity-related discriminative features by
minimizing the correlation between identity- and age-related features while
face age synthesis (FAS) eliminates age variation by converting the faces in
different age groups to the same group. However, AIFR lacks visual results for
model interpretation and FAS compromises downstream recognition due to
artifacts. Therefore, we propose a unified, multi-task framework to jointly
handle these two tasks, termed MTLFace, which can learn the age-invariant
identity-related representation for face recognition while achieving pleasing
face synthesis for model interpretation. Specifically, we propose an
attention-based feature decomposition to decompose the mixed face features into
two uncorrelated components -- identity- and age-related features -- in a
spatially constrained way. Unlike the conventional one-hot encoding that
achieves group-level FAS, we propose a novel identity conditional module to
achieve identity-level FAS, which can improve the age smoothness of synthesized
faces through a weight-sharing strategy. Benefiting from the proposed
multi-task framework, we then leverage those high-quality synthesized faces
from FAS to further boost AIFR via a novel selective fine-tuning strategy.
Furthermore, to advance both AIFR and FAS, we collect and release a large
cross-age face dataset with age and gender annotations, and a new benchmark
specifically designed for tracing long-missing children. Extensive experimental
results on five benchmark cross-age datasets demonstrate that MTLFace yields
superior performance for both AIFR and FAS. We further validate MTLFace on two
popular general face recognition datasets, obtaining competitive performance on
face recognition in the wild. Code is available at
http://hzzone.github.io/MTLFace.",-0.01452158,0.022382123,-0.09477212,C
12664,"We Ô¨Ånd that the second task                                                     only for identiÔ¨Åcation
is also challenging and worthy of further study by the face
recognition community.","Both tasks
follow the protocols of LFW.","6k                41.3

    Table 1 summarizes the testing sets used in this paper        extracted from E by two ResBlocks [1], each of which is
for AIFR and Fig.",2022-10-17 07:04:19+00:00,When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework and A New Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhizhong Huang'), arxiv.Result.Author('Junping Zhang'), arxiv.Result.Author('Hongming Shan')]","To minimize the impact of age variation on face recognition, age-invariant
face recognition (AIFR) extracts identity-related discriminative features by
minimizing the correlation between identity- and age-related features while
face age synthesis (FAS) eliminates age variation by converting the faces in
different age groups to the same group. However, AIFR lacks visual results for
model interpretation and FAS compromises downstream recognition due to
artifacts. Therefore, we propose a unified, multi-task framework to jointly
handle these two tasks, termed MTLFace, which can learn the age-invariant
identity-related representation for face recognition while achieving pleasing
face synthesis for model interpretation. Specifically, we propose an
attention-based feature decomposition to decompose the mixed face features into
two uncorrelated components -- identity- and age-related features -- in a
spatially constrained way. Unlike the conventional one-hot encoding that
achieves group-level FAS, we propose a novel identity conditional module to
achieve identity-level FAS, which can improve the age smoothness of synthesized
faces through a weight-sharing strategy. Benefiting from the proposed
multi-task framework, we then leverage those high-quality synthesized faces
from FAS to further boost AIFR via a novel selective fine-tuning strategy.
Furthermore, to advance both AIFR and FAS, we collect and release a large
cross-age face dataset with age and gender annotations, and a new benchmark
specifically designed for tracing long-missing children. Extensive experimental
results on five benchmark cross-age datasets demonstrate that MTLFace yields
superior performance for both AIFR and FAS. We further validate MTLFace on two
popular general face recognition datasets, obtaining competitive performance on
face recognition in the wild. Code is available at
http://hzzone.github.io/MTLFace.",-0.01452158,0.022382123,-0.09477212,C
12665,"We further study the invariance of the network to
geometric transformations, and show that in natural images, rotation invariance
hurts performance and learning covariant representations across multiple rotated
views leads to improved results.","As shown in Fig.4, this leads to a similar convergence rate as
RotNet [20], while also resulting in better representations from the instance-
similarity based objective.","We demonstrate significant gains in performance
across multiple datasets - CIFAR-10, CIFAR-100 [34] and ImageNet-100 [50, 12],
and the scalability of the proposed approach to ImageNet-1k [12] as well.",2022-10-18 13:55:25+00:00,Towards Efficient and Effective Self-Supervised Learning of Visual Representations,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sravanti Addepalli'), arxiv.Result.Author('Kaushal Bhogale'), arxiv.Result.Author('Priyam Dey'), arxiv.Result.Author('R. Venkatesh Babu')]","Self-supervision has emerged as a propitious method for visual representation
learning after the recent paradigm shift from handcrafted pretext tasks to
instance-similarity based approaches. Most state-of-the-art methods enforce
similarity between various augmentations of a given image, while some methods
additionally use contrastive approaches to explicitly ensure diverse
representations. While these approaches have indeed shown promising direction,
they require a significantly larger number of training iterations when compared
to the supervised counterparts. In this work, we explore reasons for the slow
convergence of these methods, and further propose to strengthen them using
well-posed auxiliary tasks that converge significantly faster, and are also
useful for representation learning. The proposed method utilizes the task of
rotation prediction to improve the efficiency of existing state-of-the-art
methods. We demonstrate significant gains in performance using the proposed
method on multiple datasets, specifically for lower training epochs.",-0.2858847,-0.14728045,0.17387411,C
12668,"‚Ä¢ We demonstrate the limitations of using uncertainty thresholding for OOD detection in
          histopathology; we discuss caveats and areas for further research in applying uncertainty
          estimation in histopathology.","Our paper makes the following contributions:

        ‚Ä¢ We evaluate the predictive performance and calibration of a baseline (MSP), two commonly
          used (MC Dropout and deep ensembles), and one distance-aware uncertainty estimation
          method (SNGP) on CIFAR-10 and compare to two datasets of clinical histopathology data.","2 Methods

2.1 Datasets

First, we evaluate the uncertainty estimation methods on CIFAR-10, to investigate whether their
performance on a standard ML dataset transfers to clinical histopathology data.",2022-10-18 14:49:44+00:00,Uncertainty estimation for out-of-distribution detection in computational histopathology,cs.CV,"['cs.CV', 'cs.LG']",[arxiv.Result.Author('Lea Goetz')],"In computational histopathology algorithms now outperform humans on a range
of tasks, but to date none are employed for automated diagnoses in the clinic.
Before algorithms can be involved in such high-stakes decisions they need to
""know when they don't know"", i.e., they need to estimate their predictive
uncertainty. This allows them to defer potentially erroneous predictions to a
human pathologist, thus increasing their safety. Here, we evaluate the
predictive performance and calibration of several uncertainty estimation
methods on clinical histopathology data. We show that a distance-aware
uncertainty estimation method outperforms commonly used approaches, such as
Monte Carlo dropout and deep ensembles. However, we observe a drop in
predictive performance and calibration on novel samples across all uncertainty
estimation methods tested. We also investigate the use of uncertainty
thresholding to reject out-of-distribution samples for selective prediction. We
demonstrate the limitations of this approach and suggest areas for future
research.",0.06254756,0.03282671,-3.3087097e-05,A
12673,"This might be due to the optimized architectures learning representations that
are intrinsically fairer than those of standard architectures, but it requires further study to test this
hypothesis and determine in precisely which characteristics these architectures differ.",studied in Section 3.,"5 CONCLUSION, FUTURE WORK AND LIMITATIONS

We conducted the Ô¨Årst large-scale analysis of the relationship among hyperparameters and architec-
tural properties, and accuracy, bias, and disparity in predictions.",2022-10-18 15:46:05+00:00,On the Importance of Architectures and Hyperparameters for Fairness in Face Recognition,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Rhea Sukthanker'), arxiv.Result.Author('Samuel Dooley'), arxiv.Result.Author('John P. Dickerson'), arxiv.Result.Author('Colin White'), arxiv.Result.Author('Frank Hutter'), arxiv.Result.Author('Micah Goldblum')]","Face recognition systems are deployed across the world by government agencies
and contractors for sensitive and impactful tasks, such as surveillance and
database matching. Despite their widespread use, these systems are known to
exhibit bias across a range of sociodemographic dimensions, such as gender and
race. Nonetheless, an array of works proposing pre-processing, training, and
post-processing methods have failed to close these gaps. Here, we take a very
different approach to this problem, identifying that both architectures and
hyperparameters of neural networks are instrumental in reducing bias. We first
run a large-scale analysis of the impact of architectures and training
hyperparameters on several common fairness metrics and show that the implicit
convention of choosing high-accuracy architectures may be suboptimal for
fairness. Motivated by our findings, we run the first neural architecture
search for fairness, jointly with a search for hyperparameters. We output a
suite of models which Pareto-dominate all other competitive architectures in
terms of accuracy and fairness. Furthermore, we show that these models transfer
well to other face recognition datasets with similar and distinct protected
attributes. We release our code and raw result files so that researchers and
practitioners can replace our fairness metrics with a bias measure of their
choice.",0.18800427,-0.26015428,0.15246278,A
12674,"In Table 2, we further study the effectiveness of individual
components in our model, including the transformer decoder (T), pre-trained
backbone weights (PBW), and the prototype dropout training strategy (PD).",Ablation study.,"The results show that: i) directly adding a transformer module to the point cloud
segmentation model and training them together greatly harms the performance
by 13.44%; ii) using a pre-trained backbone makes the model easy to train and
boost the result by 1.37%; and iii) the prototype dropout strategy can further
promote the model performance by 0.95%.",2022-10-18 15:57:20+00:00,Number-Adaptive Prototype Learning for 3D Point Cloud Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yangheng Zhao'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Xiaolong Li'), arxiv.Result.Author('Yue Hu'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Yanfeng Wang'), arxiv.Result.Author('Siheng Chen')]","3D point cloud semantic segmentation is one of the fundamental tasks for 3D
scene understanding and has been widely used in the metaverse applications.
Many recent 3D semantic segmentation methods learn a single prototype
(classifier weights) for each semantic class, and classify 3D points according
to their nearest prototype. However, learning only one prototype for each class
limits the model's ability to describe the high variance patterns within a
class. Instead of learning a single prototype for each class, in this paper, we
propose to use an adaptive number of prototypes to dynamically describe the
different point patterns within a semantic class. With the powerful capability
of vision transformer, we design a Number-Adaptive Prototype Learning (NAPL)
model for point cloud semantic segmentation. To train our NAPL model, we
propose a simple yet effective prototype dropout training strategy, which
enables our model to adaptively produce prototypes for each class. The
experimental results on SemanticKITTI dataset demonstrate that our method
achieves 2.3% mIoU improvement over the baseline model based on the point-wise
classification paradigm.",0.0081111435,-0.06624905,0.22159843,C
12675,"In order to facilitate further research on
                                        this task, we release a new benchmark dataset called Reside-Œ≤ Night
                                        dataset, consisting of 4122 nighttime hazed images from 2061 scenes (a)
                                        and 2061 ground truth images.","Our task fundamentally
                                        differs from nighttime dehazing ‚Äì our goal is to jointly dehaze and
                                        enhance scenes, while nighttime dehazing aims to dehaze scenes
                                        under a nighttime setting.","Moreover, we also propose a new
                                        network called NDENet (Nighttime Dehaze-Enhancement Network),
                                        which jointly performs dehazing and low-light enhancement in an
                                        end-to-end manner.",2022-10-18 16:19:25+00:00,Nighttime Dehaze-Enhancement,cs.CV,['cs.CV'],"[arxiv.Result.Author('Harshan Baskar'), arxiv.Result.Author('Anirudh S Chakravarthy'), arxiv.Result.Author('Prateek Garg'), arxiv.Result.Author('Divyam Goel'), arxiv.Result.Author('Abhijith S Raj'), arxiv.Result.Author('Kshitij Kumar'), arxiv.Result.Author('Lakshya'), arxiv.Result.Author('Ravichandra Parvatham'), arxiv.Result.Author('V Sushant'), arxiv.Result.Author('Bijay Kumar Rout')]","In this paper, we introduce a new computer vision task called nighttime
dehaze-enhancement. This task aims to jointly perform dehazing and lightness
enhancement. Our task fundamentally differs from nighttime dehazing -- our goal
is to jointly dehaze and enhance scenes, while nighttime dehazing aims to
dehaze scenes under a nighttime setting. In order to facilitate further
research on this task, we release a new benchmark dataset called Reside-$\beta$
Night dataset, consisting of 4122 nighttime hazed images from 2061 scenes and
2061 ground truth images. Moreover, we also propose a new network called NDENet
(Nighttime Dehaze-Enhancement Network), which jointly performs dehazing and
low-light enhancement in an end-to-end manner. We evaluate our method on the
proposed benchmark and achieve SSIM of 0.8962 and PSNR of 26.25. We also
compare our network with other baseline networks on our benchmark to
demonstrate the effectiveness of our approach. We believe that nighttime
dehaze-enhancement is an essential task particularly for autonomous navigation
applications, and hope that our work will open up new frontiers in research.
Our dataset and code will be made publicly available upon acceptance of our
paper.",-0.17291832,0.15236819,0.18534254,B
12676,"Additionally, to motivate further research on this task, we
                                           However, dehazing is much more challenging when there is       introduce a new-large scale dataset called Reside-Œ≤ Night
                                        insufÔ¨Åcient lighting during image capture.",low-light image enhancement and nighttime image dehazing.,Such a low-light       dataset.,2022-10-18 16:19:25+00:00,Nighttime Dehaze-Enhancement,cs.CV,['cs.CV'],"[arxiv.Result.Author('Harshan Baskar'), arxiv.Result.Author('Anirudh S Chakravarthy'), arxiv.Result.Author('Prateek Garg'), arxiv.Result.Author('Divyam Goel'), arxiv.Result.Author('Abhijith S Raj'), arxiv.Result.Author('Kshitij Kumar'), arxiv.Result.Author('Lakshya'), arxiv.Result.Author('Ravichandra Parvatham'), arxiv.Result.Author('V Sushant'), arxiv.Result.Author('Bijay Kumar Rout')]","In this paper, we introduce a new computer vision task called nighttime
dehaze-enhancement. This task aims to jointly perform dehazing and lightness
enhancement. Our task fundamentally differs from nighttime dehazing -- our goal
is to jointly dehaze and enhance scenes, while nighttime dehazing aims to
dehaze scenes under a nighttime setting. In order to facilitate further
research on this task, we release a new benchmark dataset called Reside-$\beta$
Night dataset, consisting of 4122 nighttime hazed images from 2061 scenes and
2061 ground truth images. Moreover, we also propose a new network called NDENet
(Nighttime Dehaze-Enhancement Network), which jointly performs dehazing and
low-light enhancement in an end-to-end manner. We evaluate our method on the
proposed benchmark and achieve SSIM of 0.8962 and PSNR of 26.25. We also
compare our network with other baseline networks on our benchmark to
demonstrate the effectiveness of our approach. We believe that nighttime
dehaze-enhancement is an essential task particularly for autonomous navigation
applications, and hope that our work will open up new frontiers in research.
Our dataset and code will be made publicly available upon acceptance of our
paper.",-0.122872666,0.2452114,0.12146622,B
12680,"We hope our datasets can help
foster further research into the important problem of understanding human emotions and wellbeing.","Our datasets and
experiment code can be found at github.com/hendrycks/emodiversity.","2 Related Work

Video Understanding With DNNs.",2022-10-18 17:58:25+00:00,How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios,cs.CV,"['cs.CV', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Mantas Mazeika'), arxiv.Result.Author('Eric Tang'), arxiv.Result.Author('Andy Zou'), arxiv.Result.Author('Steven Basart'), arxiv.Result.Author('Jun Shern Chan'), arxiv.Result.Author('Dawn Song'), arxiv.Result.Author('David Forsyth'), arxiv.Result.Author('Jacob Steinhardt'), arxiv.Result.Author('Dan Hendrycks')]","In recent years, deep neural networks have demonstrated increasingly strong
abilities to recognize objects and activities in videos. However, as video
understanding becomes widely used in real-world applications, a key
consideration is developing human-centric systems that understand not only the
content of the video but also how it would affect the wellbeing and emotional
state of viewers. To facilitate research in this setting, we introduce two
large-scale datasets with over 60,000 videos manually annotated for emotional
response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset
contains annotations for distributions of fine-grained emotional responses,
allowing models to gain a detailed understanding of affective states. The Video
to Valence (V2V) dataset contains annotations of relative pleasantness between
videos, which enables predicting a continuous spectrum of wellbeing. In
experiments, we show how video models that are primarily trained to recognize
actions and find contours of objects can be repurposed to understand human
preferences and the emotional content of videos. Although there is room for
improvement, predicting wellbeing and emotional response is on the horizon for
state-of-the-art models. We hope our datasets can help foster further advances
at the intersection of commonsense video understanding and human preference
learning.",-0.07331247,-0.2185841,-0.2560994,C
12699,"We believe that this paves the way for
further research into place recognition-speciÔ¨Åc architectures and loss functions.","Capitalizing on that, we showed that metric learning loss function can
improve performance of VPR techniques when accurate labels are provided.","Finally, we introduced Conv-AP, a fully
convolutional aggregation method that signiÔ¨Åcantly outperforms existing techniques.",2022-10-19 01:39:29+00:00,GSV-Cities: Toward Appropriate Supervised Visual Place Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Amar Ali-bey'), arxiv.Result.Author('Brahim Chaib-draa'), arxiv.Result.Author('Philippe Gigu√®re')]","This paper aims to investigate representation learning for large scale visual
place recognition, which consists of determining the location depicted in a
query image by referring to a database of reference images. This is a
challenging task due to the large-scale environmental changes that can occur
over time (i.e., weather, illumination, season, traffic, occlusion). Progress
is currently challenged by the lack of large databases with accurate ground
truth. To address this challenge, we introduce GSV-Cities, a new image dataset
providing the widest geographic coverage to date with highly accurate ground
truth, covering more than 40 cities across all continents over a 14-year
period. We subsequently explore the full potential of recent advances in deep
metric learning to train networks specifically for place recognition, and
evaluate how different loss functions influence performance. In addition, we
show that performance of existing methods substantially improves when trained
on GSV-Cities. Finally, we introduce a new fully convolutional aggregation
layer that outperforms existing techniques, including GeM, NetVLAD and
CosPlace, and establish a new state-of-the-art on large-scale benchmarks, such
as Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are
available for research purposes at https://github.com/amaralibey/gsv-cities.",-0.24257138,-0.01796184,0.015594719,B
12700,"With the limitations of this paper also discussed in the end,
the scope for further research is vast, which can use this paper as a base in aims of producing better results.","Relatively all the models have excellent performance across both datasets, except for logistic regression and
AdaBoost which perform poorly in the HAR one.","Keywords: Pose Estimation, Activity recommendation, Classification, Ensemble Learning

1 Introduction

In the era of AI, there has been a stark increase in the popularity of the interdisciplinary scientific field known
as computer vision.",2022-10-19 02:07:43+00:00,Performance of different machine learning methods on activity recognition and pose estimation datasets,cs.CV,"['cs.CV', 'physics.soc-ph']","[arxiv.Result.Author('Love Trivedi'), arxiv.Result.Author('Raviit Vij')]","With advancements in computer vision taking place day by day, recently a lot
of light is being shed on activity recognition. With the range for real-world
applications utilizing this field of study increasing across a multitude of
industries such as security and healthcare, it becomes crucial for businesses
to distinguish which machine learning methods perform better than others in the
area. This paper strives to aid in this predicament i.e. building upon previous
related work, it employs both classical and ensemble approaches on rich pose
estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to
evaluate the performance for each model, the results show that overall, random
forest yields the highest accuracy in classifying ADLs. Relatively all the
models have excellent performance across both datasets, except for logistic
regression and AdaBoost perform poorly in the HAR one. With the limitations of
this paper also discussed in the end, the scope for further research is vast,
which can use this paper as a base in aims of producing better results.",-0.22368094,-0.04345282,-0.21728456,B
12701,"This paper strives to express that, and although
limited in its scope, can function as a base for further research and testing.","The range stretches from sports and fitness to surveillance and security, depicting the growing
importance of activity recognition in real-world applications.","Acknowledgements:

‚àóContact email: raviit.vij@gmail.com.",2022-10-19 02:07:43+00:00,Performance of different machine learning methods on activity recognition and pose estimation datasets,cs.CV,"['cs.CV', 'physics.soc-ph']","[arxiv.Result.Author('Love Trivedi'), arxiv.Result.Author('Raviit Vij')]","With advancements in computer vision taking place day by day, recently a lot
of light is being shed on activity recognition. With the range for real-world
applications utilizing this field of study increasing across a multitude of
industries such as security and healthcare, it becomes crucial for businesses
to distinguish which machine learning methods perform better than others in the
area. This paper strives to aid in this predicament i.e. building upon previous
related work, it employs both classical and ensemble approaches on rich pose
estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to
evaluate the performance for each model, the results show that overall, random
forest yields the highest accuracy in classifying ADLs. Relatively all the
models have excellent performance across both datasets, except for logistic
regression and AdaBoost perform poorly in the HAR one. With the limitations of
this paper also discussed in the end, the scope for further research is vast,
which can use this paper as a base in aims of producing better results.",-0.21245578,0.10708429,-0.32519463,B
12734,"We believe that Vi-              IAST ST 33.68 43.64 37.03 45.16 59.61 72.08 74.72 61.77 53.46
sion Transformers with small-sized data [27] or limited la-
bels [21] is an interesting topic that requires further study.","Therefore, ViTs require more
training data than CNNs [27, 15].","SegFormer-based
Moreover, data augmentation, regularization, and tuning of
hyper-parameters still need to be explored when training on        Oracle ‚Äî 43.14 53.02 51.50 61.13 68.06 81.89 81.38 79.81 64.99
limited training data [44].",2022-10-19 17:20:16+00:00,OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Junshi Xia'), arxiv.Result.Author('Naoto Yokoya'), arxiv.Result.Author('Bruno Adriano'), arxiv.Result.Author('Clifford Broni-Bediako')]","We introduce OpenEarthMap, a benchmark dataset, for global high-resolution
land cover mapping. OpenEarthMap consists of 2.2 million segments of 5000
aerial and satellite images covering 97 regions from 44 countries across 6
continents, with manually annotated 8-class land cover labels at a 0.25--0.5m
ground sampling distance. Semantic segmentation models trained on the
OpenEarthMap generalize worldwide and can be used as off-the-shelf models in a
variety of applications. We evaluate the performance of state-of-the-art
methods for unsupervised domain adaptation and present challenging problem
settings suitable for further technical development. We also investigate
lightweight models using automated neural architecture search for limited
computational resources and fast mapping. The dataset is available at
https://open-earth-map.org.",0.15278742,-0.17321363,0.24079022,A
12740,"We will share the annotations
for further research on our project page.","After the annotations are done, for each event i and role k in a video, we create a dictionary of
annotations Gik with keys as frame number of all the frames that has the role k annotated in it and
values as the coordinates of the bounding box corresponding to them.",Compensation.,2022-10-19 18:38:10+00:00,Grounded Video Situation Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zeeshan Khan'), arxiv.Result.Author('C. V. Jawahar'), arxiv.Result.Author('Makarand Tapaswi')]","Dense video understanding requires answering several questions such as who is
doing what to whom, with what, how, why, and where. Recently, Video Situation
Recognition (VidSitu) is framed as a task for structured prediction of multiple
events, their relationships, and actions and various verb-role pairs attached
to descriptive entities. This task poses several challenges in identifying,
disambiguating, and co-referencing entities across multiple verb-role pairs,
but also faces some challenges of evaluation. In this work, we propose the
addition of spatio-temporal grounding as an essential component of the
structured prediction task in a weakly supervised setting, and present a novel
three stage Transformer model, VideoWhisperer, that is empowered to make joint
predictions. In stage one, we learn contextualised embeddings for video
features in parallel with key objects that appear in the video clips to enable
fine-grained spatio-temporal reasoning. The second stage sees verb-role queries
attend and pool information from object embeddings, localising answers to
questions posed about the action. The final stage generates these answers as
captions to describe each verb-role pair present in the video. Our model
operates on a group of events (clips) simultaneously and predicts verbs,
verb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on
a grounding-augmented version of the VidSitu dataset, we observe a large
improvement in entity captioning accuracy, as well as the ability to localize
verb-roles without grounding annotations at training time.",-0.09149088,0.11202391,-0.28226596,B
12763,"SOVNET does not have such                     ‚óè Given the fact that RobustCaps outperforms strong
limitations; nonetheless, its layers are not optimised for                    convolutional baselines such as equivariant residual
achieving state-of-the-art performance on transformation-robust               networks, our work can lead to further research on
classification on complex data.","accurate on data that has significant information that is not
easily captured by groups.",capsule networks.,2022-10-20 08:42:33+00:00,Robustcaps: a transformation-robust capsule network for image classification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sai Raam Venkataraman'), arxiv.Result.Author('S. Balasubramanian'), arxiv.Result.Author('R. Raghunatha Sarma')]","Geometric transformations of the training data as well as the test data
present challenges to the use of deep neural networks to vision-based learning
tasks. In order to address this issue, we present a deep neural network model
that exhibits the desirable property of transformation-robustness. Our model,
termed RobustCaps, uses group-equivariant convolutions in an improved capsule
network model. RobustCaps uses a global context-normalised procedure in its
routing algorithm to learn transformation-invariant part-whole relationships
within image data. This learning of such relationships allows our model to
outperform both capsule and convolutional neural network baselines on
transformation-robust classification tasks. Specifically, RobustCaps achieves
state-of-the-art accuracies on CIFAR-10, FashionMNIST, and CIFAR-100 when the
images in these datasets are subjected to train and test-time rotations and
translations.",-0.18715978,-0.20264974,0.11686327,C
12779,We leave it for further study.,"Dropout We keep all dropout terms in GPT-2
model and increase ‚Äúhidden dropout prob‚Äù and ‚Äúatten-              After designing and implementing the model pipeline,
tion probs dropout prob‚Äù in ViT model from 0.1 to 0.2.         we realized that a Ô¨Ålling generation, such as ‚ÄùPerson
                                                               {relation} Pizza‚Äù and ‚ÄùTrain driving on {object}‚Äù, explic-
   Data Augmentation At the beginning of the project, we       itly provides subject and object information in text features,
tried to use RandomHorizontalFlip and ColorJitter to en-       which might help the model better understand semantic re-
hance the training data, but later we realised that ColorJit-  lation concepts.","ter would cause the colour of the highlighted markers to
change.",2022-10-19 16:15:19+00:00,Image Semantic Relation Generation,cs.CV,"['cs.CV', 'cs.CL']",[arxiv.Result.Author('Mingzhe Du')],"Scene graphs provide structured semantic understanding beyond images. For
downstream tasks, such as image retrieval, visual question answering, visual
relationship detection, and even autonomous vehicle technology, scene graphs
can not only distil complex image information but also correct the bias of
visual models using semantic-level relations, which has broad application
prospects. However, the heavy labour cost of constructing graph annotations may
hinder the application of PSG in practical scenarios. Inspired by the
observation that people usually identify the subject and object first and then
determine the relationship between them, we proposed to decouple the scene
graphs generation task into two sub-tasks: 1) an image segmentation task to
pick up the qualified objects. 2) a restricted auto-regressive text generation
task to generate the relation between given objects. Therefore, in this work,
we introduce image semantic relation generation (ISRG), a simple but effective
image-to-text model, which achieved 31 points on the OpenPSG dataset and
outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points
(CLIP).",0.06445419,-0.28635097,-0.05703894,C
12847,"For these cases,
we further study the more Ô¨Åne-grained spatial-
temporal dependencies using the Grad-CAM (Sel-
Pooling Methods  œÑ          MSRVTT-7k                               DiDeMo          MSVD-QA

                    R1‚Üë R5‚Üë R10‚Üë MdR‚Üì R1‚Üë R5‚Üë R10‚Üë MdR‚Üì                                   Acc.‚Üë

                 0.01 8.2 24.7 35.1 23 6.7 23.5 34.2 32                                   27.4

                 0.1 44.3 69.7 79.0                      2 52.1 80.0 86.9   1             46.1

Text-dependent pooling 1.0 44.4 69.4 79.5                2 52.8 80.3 87.0   1             47.1

                 2.0 43.9 69.1 79.0                      2 52.3 79.8 86.1   1             46.3

                 5.0 44.1 69.3 78.7                      2 50.7 79.6 86.3   1             46.4

Table 7: Effect of different temperatures (œÑ ) in the text-dependent pooling on LiteVLS.","On the contrary, as can be seen from the last two
examples in Figure 4, when the different frames
only differ in minor changes and each frame is
similarly close to the caption, the learned weights
for each frame are also similar.","Methods          MSRVTT-7k                               MSRVTT-9k          DiDeMo

         R1‚Üë R5‚Üë R10‚Üë MdR‚Üì R1‚Üë R5‚Üë R10‚Üë MdR‚Üì R1‚Üë R5‚Üë R10‚Üë MdR‚Üì

LiteVLS 44.5 70.3 80.2 2 46.7 71.8 81.7 2 53.7 79.6 87.0 1
   w/ top-k 44.3 69.9 80.1 2 46.4 72.0 81.1 2 53.5 79.5 87.1 1

LiteVLL 48.9 74.5 83.6 2 50.8 76.3 84.4 1 53.4 80.7 87.0 1
   w/ top-k 48.4 75.4 82.5 2 50.5 76.5 84.1 1 52.3 79.2 86.8 1

Table 8: Effect of using svtc to Ô¨Ålter top-k (k=100) candidates and calculate their svtm score for ranking.",2022-10-21 13:03:49+00:00,LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Dongsheng Chen'), arxiv.Result.Author('Chaofan Tao'), arxiv.Result.Author('Lu Hou'), arxiv.Result.Author('Lifeng Shang'), arxiv.Result.Author('Xin Jiang'), arxiv.Result.Author('Qun Liu')]","Recent large-scale video-language pre-trained models have shown appealing
performance on various downstream tasks. However, the pre-training process is
computationally expensive due to the requirement of millions of video-text
pairs and the redundant data structure of each video. To mitigate these
problems, we propose LiteVL, which adapts a pre-trained image-language model
BLIP into a video-text model directly on downstream tasks, without heavy
pre-training. To enhance the temporal modeling lacking in the image-language
model, we propose to add temporal attention modules in the image encoder of
BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also
propose a non-parametric pooling mechanism to adaptively reweight the
fine-grained video embedding conditioned on the text. Experimental results on
text-video retrieval and video question answering show that the proposed LiteVL
even outperforms previous video-language pre-trained models by a clear margin,
though without any video-language pre-training.",0.2932779,0.03915791,-0.06877144,A
12851,"We further study the influence of the 4k im-
                                                                              age resolution in Figure 9.",4.3.3 4K Supervision.,"Again, one can see that more details in the
                                                                              image can be preserved when 4k images are used for training, i.e.,
                                                                              the black stripes on the shoe are sharper.",2022-10-21 14:42:11+00:00,HDHumans: A Hybrid Approach for High-fidelity Digital Humans,cs.CV,['cs.CV'],"[arxiv.Result.Author('Marc Habermann'), arxiv.Result.Author('Lingjie Liu'), arxiv.Result.Author('Weipeng Xu'), arxiv.Result.Author('Gerard Pons-Moll'), arxiv.Result.Author('Michael Zollhoefer'), arxiv.Result.Author('Christian Theobalt')]","Photo-real digital human avatars are of enormous importance in graphics, as
they enable immersive communication over the globe, improve gaming and
entertainment experiences, and can be particularly beneficial for AR and VR
settings. However, current avatar generation approaches either fall short in
high-fidelity novel view synthesis, generalization to novel motions,
reproduction of loose clothing, or they cannot render characters at the high
resolution offered by modern displays. To this end, we propose HDHumans, which
is the first method for HD human character synthesis that jointly produces an
accurate and temporally coherent 3D deforming surface and highly
photo-realistic images of arbitrary novel views and of motions not seen at
training time. At the technical core, our method tightly integrates a classical
deforming character template with neural radiance fields (NeRF). Our method is
carefully designed to achieve a synergy between classical surface deformation
and NeRF. First, the template guides the NeRF, which allows synthesizing novel
views of a highly dynamic and articulated character and even enables the
synthesis of novel motions. Second, we also leverage the dense pointclouds
resulting from NeRF to further improve the deforming surface via 3D-to-3D
supervision. We outperform the state of the art quantitatively and
qualitatively in terms of synthesis quality and resolution, as well as the
quality of 3D surface reconstruction.",0.0010926854,0.03823725,-0.05787356,C
12855,"The evaluation results were promising, although there is
                                                                 still much room for further research.","The average PA-MPJPE was recorded for all test cases
and is reported in table 1.","In [14] it was found that
  Model      3DPW‚Üì   BlanketGen-3DPW‚Üì                            RNNs had a particularly large performance boost from blan-
Pre-trained   44.97           52.70                              ket occlusion augmentations and it was hypothesized that this
Fine-tuned    50.95           52.46                              was due to the temporal information in the blanket, therefore
                                                                 it is reasonable to expect DL HPE systems that make use of
Table 1: PA-MPJPE in mm.",2022-10-21 15:27:58+00:00,BlanketGen -- A synthetic blanket occlusion augmentation pipeline for MoCap datasets,cs.CV,"['cs.CV', 'I.2.10; I.4.0']","[arxiv.Result.Author('Jo√£o Carmona'), arxiv.Result.Author('Tam√°s Kar√°csony'), arxiv.Result.Author('Jo√£o Paulo Silva Cunha')]","Human motion analysis has seen drastic improvements recently, however, due to
the lack of representative datasets, in clinical in-bed scenarios, it is still
lagging behind for clinical applications. To address this issue, we implemented
BlanketGen, a pipeline that augments videos with synthetic blanket occlusions.
With this pipeline, we generated an augmented version of 3DPW called
BlanketGen-3DPW (code and further information available at
https://gitlab.inesctec.pt/brain-lab/brain-lab-public/blanket-gen-releases ).
We then used this new dataset to fine-tune HybrIK model to improve its
performance in these scenarios with promising results.",0.31191936,0.16068196,0.15738344,A
12864,"We further study the impact of our tuned covariance matrix in
Table 5b.","We show the impact segmentation networks
have for different datasets Table 5a.",Results with post-processing applied.,2022-10-21 17:57:05+00:00,Unsupervised Multi-object Segmentation by Predicting Probable Motion Patterns,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Laurynas Karazija'), arxiv.Result.Author('Subhabrata Choudhury'), arxiv.Result.Author('Iro Laina'), arxiv.Result.Author('Christian Rupprecht'), arxiv.Result.Author('Andrea Vedaldi')]","We propose a new approach to learn to segment multiple image objects without
manual supervision. The method can extract objects form still images, but uses
videos for supervision. While prior works have considered motion for
segmentation, a key insight is that, while motion can be used to identify
objects, not all objects are necessarily in motion: the absence of motion does
not imply the absence of objects. Hence, our model learns to predict image
regions that are likely to contain motion patterns characteristic of objects
moving rigidly. It does not predict specific motion, which cannot be done
unambiguously from a still image, but a distribution of possible motions, which
includes the possibility that an object does not move at all. We demonstrate
the advantage of this approach over its deterministic counterpart and show
state-of-the-art unsupervised object segmentation performance on simulated and
real-world benchmarks, surpassing methods that use motion even at test time. As
our approach is applicable to variety of network architectures that segment the
scenes, we also apply it to existing image reconstruction-based models showing
drastic improvement. Project page and code:
https://www.robots.ox.ac.uk/~vgg/research/ppmp .",0.17155528,-0.013863289,9.65856e-05,A
12893,"The result indicates that our proposed method still has some
                                                                                                                    shortcomings and needs further research and improvement.","When the object in the RS image is salient
can learn more discriminative visual representations of RS im-                                                      and the attributes described by the expression are clear and
ages and effectively fuse and align visual features and textual                                                     unambiguous, the model cannot complete the RSVG correctly.",VI.,2022-10-23 07:08:22+00:00,RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yang Zhan'), arxiv.Result.Author('Zhitong Xiong'), arxiv.Result.Author('Yuan Yuan')]","In this paper, we introduce the task of visual grounding for remote sensing
data (RSVG). RSVG aims to localize the referred objects in remote sensing (RS)
images with the guidance of natural language. To retrieve rich information from
RS imagery using natural language, many research tasks, like RS image visual
question answering, RS image captioning, and RS image-text retrieval have been
investigated a lot. However, the object-level visual grounding on RS images is
still under-explored. Thus, in this work, we propose to construct the dataset
and explore deep learning models for the RSVG task. Specifically, our
contributions can be summarized as follows. 1) We build the new large-scale
benchmark dataset of RSVG, termed RSVGD, to fully advance the research of RSVG.
This new dataset includes image/expression/box triplets for training and
evaluating visual grounding models. 2) We benchmark extensive state-of-the-art
(SOTA) natural image visual grounding methods on the constructed RSVGD dataset,
and some insightful analyses are provided based on the results. 3) A novel
transformer-based Multi-Level Cross-Modal feature learning (MLCM) module is
proposed. Remotely-sensed images are usually with large scale variations and
cluttered backgrounds. To deal with the scale-variation problem, the MLCM
module takes advantage of multi-scale visual features and multi-granularity
textual embeddings to learn more discriminative representations. To cope with
the cluttered background problem, MLCM adaptively filters irrelevant noise and
enhances salient features. In this way, our proposed model can incorporate more
effective multi-level and multi-modal features to boost performance.
Furthermore, this work also provides useful insights for developing better RSVG
models. The dataset and code will be publicly available at
https://github.com/ZhanYang-nwpu/RSVG-pytorch.",-0.15159912,-0.059060372,-0.07491549,C
12903,"Along with the         area to improve the state of datasets available and towards
annotations, we also provide extra unlabelled raw data from      increasing the volumes of high-quality and well annotated
the sensors to facilitate further research, especially into      datasets.","There have been several efforts over the years in this
we use for model training and evaluation.",self- and unsupervised learning over such traffic scenes.,2022-10-23 23:03:17+00:00,IDD-3D: Indian Driving Dataset for 3D Unstructured Road Scenes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shubham Dokania'), arxiv.Result.Author('A. H. Abdul Hafez'), arxiv.Result.Author('Anbumani Subramanian'), arxiv.Result.Author('Manmohan Chandraker'), arxiv.Result.Author('C. V. Jawahar')]","Autonomous driving and assistance systems rely on annotated data from traffic
and road scenarios to model and learn the various object relations in complex
real-world scenarios. Preparation and training of deploy-able deep learning
architectures require the models to be suited to different traffic scenarios
and adapt to different situations. Currently, existing datasets, while
large-scale, lack such diversities and are geographically biased towards mainly
developed cities. An unstructured and complex driving layout found in several
developing countries such as India poses a challenge to these models due to the
sheer degree of variations in the object types, densities, and locations. To
facilitate better research toward accommodating such scenarios, we build a new
dataset, IDD-3D, which consists of multi-modal data from multiple cameras and
LiDAR sensors with 12k annotated driving LiDAR frames across various traffic
scenarios. We discuss the need for this dataset through statistical comparisons
with existing datasets and highlight benchmarks on standard 3D object detection
and tracking tasks in complex layouts. Code and data available at
https://github.com/shubham1810/idd3d_kit.git",-0.28187713,-0.14237222,-0.052336343,B
12910,"Along
this line, we provide some preliminary evidence on how sparse modeling enables interpretability in
the Appendix, and leave further study to future work.","Looking forward, other fundamental principles, algorithm and techniques in sparse
modeling may be introduced to further enhance the capability of our presented framework.","Acknowledgments and Disclosure of Funding

Zhihui Zhu acknowledges support from NSF grants CCF-2008460.",2022-10-24 04:29:21+00:00,Revisiting Sparse Convolutional Model for Visual Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xili Dai'), arxiv.Result.Author('Mingyang Li'), arxiv.Result.Author('Pengyuan Zhai'), arxiv.Result.Author('Shengbang Tong'), arxiv.Result.Author('Xingjian Gao'), arxiv.Result.Author('Shao-Lun Huang'), arxiv.Result.Author('Zhihui Zhu'), arxiv.Result.Author('Chong You'), arxiv.Result.Author('Yi Ma')]","Despite strong empirical performance for image classification, deep neural
networks are often regarded as ``black boxes'' and they are difficult to
interpret. On the other hand, sparse convolutional models, which assume that a
signal can be expressed by a linear combination of a few elements from a
convolutional dictionary, are powerful tools for analyzing natural images with
good theoretical interpretability and biological plausibility. However, such
principled models have not demonstrated competitive performance when compared
with empirically designed deep networks. This paper revisits the sparse
convolutional modeling for image classification and bridges the gap between
good empirical performance (of deep learning) and good interpretability (of
sparse convolutional models). Our method uses differentiable optimization
layers that are defined from convolutional sparse coding as drop-in
replacements of standard convolutional layers in conventional deep neural
networks. We show that such models have equally strong empirical performance on
CIFAR-10, CIFAR-100, and ImageNet datasets when compared to conventional neural
networks. By leveraging stable recovery property of sparse modeling, we further
show that such models can be much more robust to input corruptions as well as
adversarial perturbations in testing through a simple proper trade-off between
sparse regularization and data reconstruction terms. Source code can be found
at https://github.com/Delay-Xili/SDNet.",0.15626872,-0.087023936,-0.0027940413,A
12952,"1029‚Äì
   Our results constitute an interesting departure point for                      1032.
further research.","IGARSS, 2019, pp.","Although the nnU-Nets allowed us to obtain
high-quality cloud detection, these models are of large capacity             [4] S. Mahajan and B. Fataniya, ‚ÄúCloud detection methodologies: variants
(with up to 30M of trainable parameters for the investigated                      and development‚Äîa review,‚Äù Complex & Intelligent Systems, vol.",2022-10-24 23:39:58+00:00,Self-Configuring nnU-Nets Detect Clouds in Satellite Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Bartosz Grabowski'), arxiv.Result.Author('Maciej Ziaja'), arxiv.Result.Author('Michal Kawulok'), arxiv.Result.Author('Nicolas Long√©p√©'), arxiv.Result.Author('Bertrand Le Saux'), arxiv.Result.Author('Jakub Nalepa')]","Cloud detection is a pivotal satellite image pre-processing step that can be
performed both on the ground and on board a satellite to tag useful images. In
the latter case, it can help to reduce the amount of data to downlink by
pruning the cloudy areas, or to make a satellite more autonomous through
data-driven acquisition re-scheduling of the cloudy areas. We approach this
important task with nnU-Nets, a self-reconfigurable framework able to perform
meta-learning of a segmentation network over various datasets. Our experiments,
performed over Sentinel-2 and Landsat-8 multispectral images revealed that
nnU-Nets deliver state-of-the-art cloud segmentation performance without any
manual design. Our approach was ranked within the top 7% best solutions (across
847 participating teams) in the On Cloud N: Cloud Cover Detection Challenge,
where we reached the Jaccard index of 0.882 over more than 10k unseen
Sentinel-2 image patches (the winners obtained 0.897, whereas the baseline
U-Net with the ResNet-34 backbone used as an encoder: 0.817, and the classic
Sentinel-2 image thresholding: 0.652).",0.0028076342,6.565079e-05,0.1322515,B
12962,"‚Ä¢ By leveraging the state-of-the-art instance segmentation
   Moreover, there are no standardized benchmarks to pro-          models and well-deÔ¨Åned evaluation metrics, we build
vide a fair comparison between different stroke extraction         standardized benchmarks to facilitate further research.",ducing transferable features to beneÔ¨Åt downstream tasks.,"methods, which is of great importance to guide and facilitate
further research.",2022-10-25 08:09:14+00:00,"Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks",cs.CV,['cs.CV'],"[arxiv.Result.Author('Lizhao Liu'), arxiv.Result.Author('Kunyang Lin'), arxiv.Result.Author('Shangxin Huang'), arxiv.Result.Author('Zhongli Li'), arxiv.Result.Author('Chao Li'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Qingyu Zhou')]","Stroke is the basic element of Chinese character and stroke extraction has
been an important and long-standing endeavor. Existing stroke extraction
methods are often handcrafted and highly depend on domain expertise due to the
limited training data. Moreover, there are no standardized benchmarks to
provide a fair comparison between different stroke extraction methods, which,
we believe, is a major impediment to the development of Chinese character
stroke understanding and related tasks. In this work, we present the first
public available Chinese Character Stroke Extraction (CCSE) benchmark, with two
new large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).
With the large-scale datasets, we hope to leverage the representation power of
deep models such as CNNs to solve the stroke extraction task, which, however,
remains an open question. To this end, we turn the stroke extraction problem
into a stroke instance segmentation problem. Using the proposed datasets to
train a stroke instance segmentation model, we surpass previous methods by a
large margin. Moreover, the models trained with the proposed datasets benefit
the downstream font generation and handwritten aesthetic assessment tasks. We
hope these benchmark results can facilitate further research. The source code
and datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.",-0.10121247,-0.10737607,-0.083249584,C
12963,"methods, which is of great importance to guide and facilitate
further research.","‚Ä¢ By leveraging the state-of-the-art instance segmentation
   Moreover, there are no standardized benchmarks to pro-          models and well-deÔ¨Åned evaluation metrics, we build
vide a fair comparison between different stroke extraction         standardized benchmarks to facilitate further research.","And the lack of publicly available datasets                        Related Work
leads to inconsistent evaluation protocols.",2022-10-25 08:09:14+00:00,"Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks",cs.CV,['cs.CV'],"[arxiv.Result.Author('Lizhao Liu'), arxiv.Result.Author('Kunyang Lin'), arxiv.Result.Author('Shangxin Huang'), arxiv.Result.Author('Zhongli Li'), arxiv.Result.Author('Chao Li'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Qingyu Zhou')]","Stroke is the basic element of Chinese character and stroke extraction has
been an important and long-standing endeavor. Existing stroke extraction
methods are often handcrafted and highly depend on domain expertise due to the
limited training data. Moreover, there are no standardized benchmarks to
provide a fair comparison between different stroke extraction methods, which,
we believe, is a major impediment to the development of Chinese character
stroke understanding and related tasks. In this work, we present the first
public available Chinese Character Stroke Extraction (CCSE) benchmark, with two
new large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).
With the large-scale datasets, we hope to leverage the representation power of
deep models such as CNNs to solve the stroke extraction task, which, however,
remains an open question. To this end, we turn the stroke extraction problem
into a stroke instance segmentation problem. Using the proposed datasets to
train a stroke instance segmentation model, we surpass previous methods by a
large margin. Moreover, the models trained with the proposed datasets benefit
the downstream font generation and handwritten aesthetic assessment tasks. We
hope these benchmark results can facilitate further research. The source code
and datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.",-0.06370032,-0.0700571,-0.11114837,C
12964,"2017), one of the most
tation models to produce benchmark results that facilitate      important milestones in computer vision, the segmentation
further research.",In Mask R-CNN (He et al.,"Compared to previous methods of stroke        head is applied to the detected instances from the Faster
extraction, our approach does not require reference images      R-CNN (Ren et al.",2022-10-25 08:09:14+00:00,"Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks",cs.CV,['cs.CV'],"[arxiv.Result.Author('Lizhao Liu'), arxiv.Result.Author('Kunyang Lin'), arxiv.Result.Author('Shangxin Huang'), arxiv.Result.Author('Zhongli Li'), arxiv.Result.Author('Chao Li'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Qingyu Zhou')]","Stroke is the basic element of Chinese character and stroke extraction has
been an important and long-standing endeavor. Existing stroke extraction
methods are often handcrafted and highly depend on domain expertise due to the
limited training data. Moreover, there are no standardized benchmarks to
provide a fair comparison between different stroke extraction methods, which,
we believe, is a major impediment to the development of Chinese character
stroke understanding and related tasks. In this work, we present the first
public available Chinese Character Stroke Extraction (CCSE) benchmark, with two
new large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).
With the large-scale datasets, we hope to leverage the representation power of
deep models such as CNNs to solve the stroke extraction task, which, however,
remains an open question. To this end, we turn the stroke extraction problem
into a stroke instance segmentation problem. Using the proposed datasets to
train a stroke instance segmentation model, we surpass previous methods by a
large margin. Moreover, the models trained with the proposed datasets benefit
the downstream font generation and handwritten aesthetic assessment tasks. We
hope these benchmark results can facilitate further research. The source code
and datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.",-0.25959194,-0.028961837,0.06953181,B
12965,stroke extraction task and facilitate further research.,"Improving the         In this work, we propose the Ô¨Årst large-scale Chinese Char-
stroke instance segmentation performance under a high IoU          acter Stroke Extraction (CCSE) benchmark to improve
threshold is a challenging mission to solve.","To this
Qualitative Results We provide qualitative comparisons             end, we effortlessly harvest a large number of Chinese char-
between the traditional approach (Xu et al.",2022-10-25 08:09:14+00:00,"Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks",cs.CV,['cs.CV'],"[arxiv.Result.Author('Lizhao Liu'), arxiv.Result.Author('Kunyang Lin'), arxiv.Result.Author('Shangxin Huang'), arxiv.Result.Author('Zhongli Li'), arxiv.Result.Author('Chao Li'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Qingyu Zhou')]","Stroke is the basic element of Chinese character and stroke extraction has
been an important and long-standing endeavor. Existing stroke extraction
methods are often handcrafted and highly depend on domain expertise due to the
limited training data. Moreover, there are no standardized benchmarks to
provide a fair comparison between different stroke extraction methods, which,
we believe, is a major impediment to the development of Chinese character
stroke understanding and related tasks. In this work, we present the first
public available Chinese Character Stroke Extraction (CCSE) benchmark, with two
new large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).
With the large-scale datasets, we hope to leverage the representation power of
deep models such as CNNs to solve the stroke extraction task, which, however,
remains an open question. To this end, we turn the stroke extraction problem
into a stroke instance segmentation problem. Using the proposed datasets to
train a stroke instance segmentation model, we surpass previous methods by a
large margin. Moreover, the models trained with the proposed datasets benefit
the downstream font generation and handwritten aesthetic assessment tasks. We
hope these benchmark results can facilitate further research. The source code
and datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.",0.014304706,-0.039461706,-0.079600096,C
12977,"We make our code and models publicly available to encour-
                                        age further research on this topic: github.com/tba.","Ad-
                                        ditionally, our Ô¨Åxed-length embeddings can be matched or-
                                        ders of magnitude faster than the commercial system (2.5
                                        million matches/second compared to 50K matches/second).","Figure 2: An example illustrating minutiae correspondences     larity score aggregated from corresponding points is more
between a pair of synthetic Ô¨Ångerprints [1] of the same        than a speciÔ¨Åed threshold, the Ô¨Ångerprint pair is determined
Ô¨Ånger.",2022-10-25 13:08:32+00:00,Minutiae-Guided Fingerprint Embeddings via Vision Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Steven A. Grosz'), arxiv.Result.Author('Joshua J. Engelsma'), arxiv.Result.Author('Rajeev Ranjan'), arxiv.Result.Author('Naveen Ramakrishnan'), arxiv.Result.Author('Manoj Aggarwal'), arxiv.Result.Author('Gerard G. Medioni'), arxiv.Result.Author('Anil K. Jain')]","Minutiae matching has long dominated the field of fingerprint recognition.
However, deep networks can be used to extract fixed-length embeddings from
fingerprints. To date, the few studies that have explored the use of CNN
architectures to extract such embeddings have shown extreme promise. Inspired
by these early works, we propose the first use of a Vision Transformer (ViT) to
learn a discriminative fixed-length fingerprint embedding. We further
demonstrate that by guiding the ViT to focus in on local, minutiae related
features, we can boost the recognition performance. Finally, we show that by
fusing embeddings learned by CNNs and ViTs we can reach near parity with a
commercial state-of-the-art (SOTA) matcher. In particular, we obtain a
TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a
SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our
fixed-length embeddings can be matched orders of magnitude faster than the
commercial system (2.5 million matches/second compared to 50K matches/second).
We make our code and models publicly available to encourage further research on
this topic: https://github.com/tba.",0.21568815,0.015467275,-0.053515032,A
12978,"pletely discarding the minutiae template, we lean on this                     ‚Ä¢ We will release our training and inference code upon
domain knowledge to learn more discriminative and gener-                         acceptance for further research github.com/tba.","Rather than com-

                                                                   2We use the term templates, representations, and embeddings through-
                                                               out to denote a set of features extracted from a Ô¨Ångerprint image.",alizable deep Ô¨Ångerprint embeddings.,2022-10-25 13:08:32+00:00,Minutiae-Guided Fingerprint Embeddings via Vision Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Steven A. Grosz'), arxiv.Result.Author('Joshua J. Engelsma'), arxiv.Result.Author('Rajeev Ranjan'), arxiv.Result.Author('Naveen Ramakrishnan'), arxiv.Result.Author('Manoj Aggarwal'), arxiv.Result.Author('Gerard G. Medioni'), arxiv.Result.Author('Anil K. Jain')]","Minutiae matching has long dominated the field of fingerprint recognition.
However, deep networks can be used to extract fixed-length embeddings from
fingerprints. To date, the few studies that have explored the use of CNN
architectures to extract such embeddings have shown extreme promise. Inspired
by these early works, we propose the first use of a Vision Transformer (ViT) to
learn a discriminative fixed-length fingerprint embedding. We further
demonstrate that by guiding the ViT to focus in on local, minutiae related
features, we can boost the recognition performance. Finally, we show that by
fusing embeddings learned by CNNs and ViTs we can reach near parity with a
commercial state-of-the-art (SOTA) matcher. In particular, we obtain a
TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a
SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our
fixed-length embeddings can be matched orders of magnitude faster than the
commercial system (2.5 million matches/second compared to 50K matches/second).
We make our code and models publicly available to encourage further research on
this topic: https://github.com/tba.",-0.0715946,-0.23995832,-0.0149708465,C
12979,"proprietary-fingerprint-template-pft-iii
                                        We make our code and models publicly available to encour-
                                        age further research on this topic: github.com/tba.","Ad-                           time) and unique (different for every person, even differ-
                                        ditionally, our Ô¨Åxed-length embeddings can be matched or-
                                        ders of magnitude faster than the commercial system (2.5                       1https://www.nist.gov/itl/iad/image-group/
                                        million matches/second compared to 50K matches/second).",1.,2022-10-25 13:08:32+00:00,Minutiae-Guided Fingerprint Embeddings via Vision Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Steven A. Grosz'), arxiv.Result.Author('Joshua J. Engelsma'), arxiv.Result.Author('Rajeev Ranjan'), arxiv.Result.Author('Naveen Ramakrishnan'), arxiv.Result.Author('Manoj Aggarwal'), arxiv.Result.Author('Gerard G. Medioni'), arxiv.Result.Author('Anil K. Jain')]","Minutiae matching has long dominated the field of fingerprint recognition.
However, deep networks can be used to extract fixed-length embeddings from
fingerprints. To date, the few studies that have explored the use of CNN
architectures to extract such embeddings have shown extreme promise. Inspired
by these early works, we propose the first use of a Vision Transformer (ViT) to
learn a discriminative fixed-length fingerprint embedding. We further
demonstrate that by guiding the ViT to focus in on local, minutiae related
features, we can boost the recognition performance. Finally, we show that by
fusing embeddings learned by CNNs and ViTs we can reach near parity with a
commercial state-of-the-art (SOTA) matcher. In particular, we obtain a
TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a
SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our
fixed-length embeddings can be matched orders of magnitude faster than the
commercial system (2.5 million matches/second compared to 50K matches/second).
We make our code and models publicly available to encourage further research on
this topic: https://github.com/tba.",0.11596361,0.17526892,-0.008398131,A
12980,"‚Ä¢ We will release our training and inference code upon                     In this work, we most closely follow the framework es-
      acceptance for further research github.com/tba.",multi-task learning framework.,"tablished in [17] to use a single network to learn a Ô¨Ånger-
                                                                           print embedding, but to guide that network to extract minu-
2.",2022-10-25 13:08:32+00:00,Minutiae-Guided Fingerprint Embeddings via Vision Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Steven A. Grosz'), arxiv.Result.Author('Joshua J. Engelsma'), arxiv.Result.Author('Rajeev Ranjan'), arxiv.Result.Author('Naveen Ramakrishnan'), arxiv.Result.Author('Manoj Aggarwal'), arxiv.Result.Author('Gerard G. Medioni'), arxiv.Result.Author('Anil K. Jain')]","Minutiae matching has long dominated the field of fingerprint recognition.
However, deep networks can be used to extract fixed-length embeddings from
fingerprints. To date, the few studies that have explored the use of CNN
architectures to extract such embeddings have shown extreme promise. Inspired
by these early works, we propose the first use of a Vision Transformer (ViT) to
learn a discriminative fixed-length fingerprint embedding. We further
demonstrate that by guiding the ViT to focus in on local, minutiae related
features, we can boost the recognition performance. Finally, we show that by
fusing embeddings learned by CNNs and ViTs we can reach near parity with a
commercial state-of-the-art (SOTA) matcher. In particular, we obtain a
TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a
SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our
fixed-length embeddings can be matched orders of magnitude faster than the
commercial system (2.5 million matches/second compared to 50K matches/second).
We make our code and models publicly available to encourage further research on
this topic: https://github.com/tba.",0.021498453,-0.3197441,-0.005509028,C
13004,"Thus we expect that further research on how to take an ideal cover enhances our
Atlas Ô¨Çow model.","However,
even though a cover from Mapper has a big advantage in that it gives topological information
of the manifold, it mainly depends on the lens function and cannot guarantee that it is good
enough.","14  TAEJIN PAIK, JAEMIN PARK, AND JUNG HO PARK

                                                  References

 [1] Charles FeÔ¨Äerman, Sanjoy Mitter, and Hariharan Narayanan.",2022-10-24 07:19:48+00:00,Atlas flow : compatible local structures on the manifold,cs.CV,"['cs.CV', 'cs.LG', '62R40', 'I.2.6; I.2.10']","[arxiv.Result.Author('Taejin Paik'), arxiv.Result.Author('Jaemin Park'), arxiv.Result.Author('Jung Ho Park')]","In this paper, we focus on the intersections of a manifold's local structures
to analyze the global structure of a manifold. We obtain local regions on data
manifolds such as the latent space of StyleGAN2, using Mapper, a tool from
topological data analysis. We impose gluing compatibility conditions on
overlapping local regions, which guarantee that the local structures can be
glued together to the global structure of a manifold. We propose a novel
generative flow model called Atlas flow that uses compatibility to reattach the
local regions. Our model shows that the generating processes perform well on
synthetic dataset samples of well-known manifolds with noise. Furthermore, we
investigate the style vector manifold of StyleGAN2 using our model.",0.020541206,0.17332661,0.020658858,B
13017,We further study how resizing inÔ¨Çuences accuracy.,"4.3 Ablation Study

In this section, we check how each component of the proposed approach affects the performance,
including the channel selection strategy, attention module on the input frequency domain, and resizing.","4.3.1 All Components

This experiment is based on the Swin Transformer backbone on the Tiny ImageNet dataset.",2022-10-25 20:24:53+00:00,Explicitly Increasing Input Information Density for Vision Transformers on Small Datasets,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiangyu Chen'), arxiv.Result.Author('Ying Qin'), arxiv.Result.Author('Wenju Xu'), arxiv.Result.Author('Andr√©s M. Bur'), arxiv.Result.Author('Cuncong Zhong'), arxiv.Result.Author('Guanghui Wang')]","Vision Transformers have attracted a lot of attention recently since the
successful implementation of Vision Transformer (ViT) on vision tasks. With
vision Transformers, specifically the multi-head self-attention modules,
networks can capture long-term dependencies inherently. However, these
attention modules normally need to be trained on large datasets, and vision
Transformers show inferior performance on small datasets when training from
scratch compared with widely dominant backbones like ResNets. Note that the
Transformer model was first proposed for natural language processing, which
carries denser information than natural images. To boost the performance of
vision Transformers on small datasets, this paper proposes to explicitly
increase the input information density in the frequency domain. Specifically,
we introduce selecting channels by calculating the channel-wise heatmaps in the
frequency domain using Discrete Cosine Transform (DCT), reducing the size of
input while keeping most information and hence increasing the information
density. As a result, 25% fewer channels are kept while better performance is
achieved compared with previous work. Extensive experiments demonstrate the
effectiveness of the proposed approach on five small-scale datasets, including
CIFAR-10/100, SVHN, Flowers-102, and Tiny ImageNet. The accuracy has been
boosted up to 17.05% with Swin and Focal Transformers. Codes are available at
https://github.com/xiangyu8/DenseVT.",0.12449239,0.0025700415,0.24259523,C
13020,"One exemplary
50 meters           30 meters                               10 meters              Our Ô¨Åndings open up questions for further research: Is

Fig.","D. Attention during a track‚Äôs lifetime

   Observing many tracked objects, we Ô¨Ånd that the attention
spread of a tracked object changes over time.",6.,2022-10-26 00:05:16+00:00,Can Transformer Attention Spread Give Insights Into Uncertainty of Detected and Tracked Objects?,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Felicia Ruppel'), arxiv.Result.Author('Florian Faion'), arxiv.Result.Author('Claudius Gl√§ser'), arxiv.Result.Author('Klaus Dietmayer')]","Transformers have recently been utilized to perform object detection and
tracking in the context of autonomous driving. One unique characteristic of
these models is that attention weights are computed in each forward pass,
giving insights into the model's interior, in particular, which part of the
input data it deemed interesting for the given task. Such an attention matrix
with the input grid is available for each detected (or tracked) object in every
transformer decoder layer. In this work, we investigate the distribution of
these attention weights: How do they change through the decoder layers and
through the lifetime of a track? Can they be used to infer additional
information about an object, such as a detection uncertainty? Especially in
unstructured environments, or environments that were not common during
training, a reliable measure of detection uncertainty is crucial to decide
whether the system can still be trusted or not.",0.054720413,0.20268343,-0.28419745,B
13021,"0     2.5                                              2.5         0
                                                                                   We conclude that the attention matrices available in
        Track age                                           Remaining time
                                                                                transformer models have the potential to give insights into
        in seconds                                          in seconds
                                                                                detection and tracking uncertainty and that further research
Fig.","Attention spread102                                    102                      Can the model design be improved or the model behavior
                                     Attention spread
                                                                                be better understood based on the knowledge about attention
101                                                    101
                                                                                spread per layer?",7.,2022-10-26 00:05:16+00:00,Can Transformer Attention Spread Give Insights Into Uncertainty of Detected and Tracked Objects?,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Felicia Ruppel'), arxiv.Result.Author('Florian Faion'), arxiv.Result.Author('Claudius Gl√§ser'), arxiv.Result.Author('Klaus Dietmayer')]","Transformers have recently been utilized to perform object detection and
tracking in the context of autonomous driving. One unique characteristic of
these models is that attention weights are computed in each forward pass,
giving insights into the model's interior, in particular, which part of the
input data it deemed interesting for the given task. Such an attention matrix
with the input grid is available for each detected (or tracked) object in every
transformer decoder layer. In this work, we investigate the distribution of
these attention weights: How do they change through the decoder layers and
through the lifetime of a track? Can they be used to infer additional
information about an object, such as a detection uncertainty? Especially in
unstructured environments, or environments that were not common during
training, a reliable measure of detection uncertainty is crucial to decide
whether the system can still be trusted or not.",0.067405574,0.029766263,-0.053012684,A
13031,"We believe this work provides the foundation
                                                  and starting point for further research to detect DM deepfakes effectively.","Most importantly, we demonstrate that GANs and DMs produce
                                                  images with different characteristics, which requires adaptation of existing clas-
                                                  siÔ¨Åers to ensure reliable detection.","1 INTRODUCTION

                                        In the recent past, diffusion models (DMs) have shown a lot of promise as a method for synthesizing
                                        images.",2022-10-26 09:01:19+00:00,Towards the Detection of Diffusion Model Deepfakes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jonas Ricker'), arxiv.Result.Author('Simon Damm'), arxiv.Result.Author('Thorsten Holz'), arxiv.Result.Author('Asja Fischer')]","Diffusion models (DMs) have recently emerged as a promising method in image
synthesis. They have surpassed generative adversarial networks (GANs) in both
diversity and quality, and have achieved impressive results in text-to-image
and image-to-image modeling. However, to date, only little attention has been
paid to the detection of DM-generated images, which is critical to prevent
adverse impacts on our society. Although prior work has shown that
GAN-generated images can be reliably detected using automated methods, it is
unclear whether the same methods are effective against DMs. In this work, we
address this challenge and take a first look at detecting DM-generated images.
We approach the problem from two different angles: First, we evaluate the
performance of state-of-the-art detectors on a variety of DMs. Second, we
analyze DM-generated images in the frequency domain and study different factors
that influence the spectral properties of these images. Most importantly, we
demonstrate that GANs and DMs produce images with different characteristics,
which requires adaptation of existing classifiers to ensure reliable detection.
We believe this work provides the foundation and starting point for further
research to detect DM deepfakes effectively.",-0.15660092,-0.092993185,0.3177721,C
13032,"We believe that our results provide the
foundation for further research on the effective detection of deepfakes generated by DMs.","Further analysis suggests that too little weight is given to these frequencies
during training due to the choice of the training objective.","2 RELATED WORK

Universal Fake Image Detection While in recent years a variety of successful methods to detect
artiÔ¨Åcially generated images has been proposed (Verdoliva, 2020), generalization to unseen data
remains a challenging task (Cozzolino et al., 2019).",2022-10-26 09:01:19+00:00,Towards the Detection of Diffusion Model Deepfakes,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jonas Ricker'), arxiv.Result.Author('Simon Damm'), arxiv.Result.Author('Thorsten Holz'), arxiv.Result.Author('Asja Fischer')]","Diffusion models (DMs) have recently emerged as a promising method in image
synthesis. They have surpassed generative adversarial networks (GANs) in both
diversity and quality, and have achieved impressive results in text-to-image
and image-to-image modeling. However, to date, only little attention has been
paid to the detection of DM-generated images, which is critical to prevent
adverse impacts on our society. Although prior work has shown that
GAN-generated images can be reliably detected using automated methods, it is
unclear whether the same methods are effective against DMs. In this work, we
address this challenge and take a first look at detecting DM-generated images.
We approach the problem from two different angles: First, we evaluate the
performance of state-of-the-art detectors on a variety of DMs. Second, we
analyze DM-generated images in the frequency domain and study different factors
that influence the spectral properties of these images. Most importantly, we
demonstrate that GANs and DMs produce images with different characteristics,
which requires adaptation of existing classifiers to ensure reliable detection.
We believe this work provides the foundation and starting point for further
research to detect DM deepfakes effectively.",-0.18403186,-0.13827091,0.16719532,C
13041,"Prior to the rise of deep learning, methods [20] have been pro-
posed to model an input signal as a composition of epitomes, which contain information
about shape and appearance of objects in an input image, and further research also tried to
represent objects and scenes as hierarchical graphs composed of primitives and their rela-
tionships [6, 48, 49, 50].",Unsupervised methods.,"Several methods [10, 13, 18, 23, 28, 38] try to perform decomposition as routing in
an embedding space.",2022-10-25 15:55:24+00:00,Search for Concepts: Discovering Visual Concepts Using Direct Optimization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pradyumna Reddy'), arxiv.Result.Author('Paul Guerrero'), arxiv.Result.Author('Niloy J. Mitra')]","Finding an unsupervised decomposition of an image into individual objects is
a key step to leverage compositionality and to perform symbolic reasoning.
Traditionally, this problem is solved using amortized inference, which does not
generalize beyond the scope of the training data, may sometimes miss correct
decompositions, and requires large amounts of training data. We propose finding
a decomposition using direct, unamortized optimization, via a combination of a
gradient-based optimization for differentiable object properties and global
search for non-differentiable properties. We show that using direct
optimization is more generalizable, misses fewer correct decompositions, and
typically requires less data than methods based on amortized inference. This
highlights a weakness of the current prevalent practice of using amortized
inference that can potentially be improved by integrating more direct
optimization elements.",-0.22746488,-0.1141704,0.12340011,C
13057,"personal, subjective form of self-expression, and
so further research could focus on individualizing       Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei,
results to particular users.","However, style is a           HLT).","Xiaoyong Wei, Minlong Lu, and Xiaodan Liang.",2022-10-26 21:01:19+00:00,FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Suvir Mirchandani'), arxiv.Result.Author('Licheng Yu'), arxiv.Result.Author('Mengjiao Wang'), arxiv.Result.Author('Animesh Sinha'), arxiv.Result.Author('Wenwen Jiang'), arxiv.Result.Author('Tao Xiang'), arxiv.Result.Author('Ning Zhang')]","Multimodal tasks in the fashion domain have significant potential for
e-commerce, but involve challenging vision-and-language learning problems -
e.g., retrieving a fashion item given a reference image plus text feedback from
a user. Prior works on multimodal fashion tasks have either been limited by the
data in individual benchmarks, or have leveraged generic vision-and-language
pre-training but have not taken advantage of the characteristics of fashion
data. Additionally, these works have mainly been restricted to multimodal
understanding tasks. To address these gaps, we make two key contributions.
First, we propose a novel fashion-specific pre-training framework based on
weakly-supervised triplets constructed from fashion image-text pairs. We show
the triplet-based tasks are an effective addition to standard multimodal
pre-training tasks. Second, we propose a flexible decoder-based model
architecture capable of both fashion retrieval and captioning tasks. Together,
our model design and pre-training approach are competitive on a diverse set of
fashion tasks, including cross-modal retrieval, image retrieval with text
feedback, image captioning, relative image captioning, and multimodal
categorization.",0.26390135,-0.09520817,-0.29400343,A
13075,"To promote further study,
ViLG-300 will be open to the community soon.",See Appendix A.1 for more details about the construction process.,"With ViLG-300, we can make convincing comparisons between ERNIE-ViLG 2.0 and DALL-E 24,
Stable Diffusion5.",2022-10-27 08:21:35+00:00,ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zhida Feng'), arxiv.Result.Author('Zhenyu Zhang'), arxiv.Result.Author('Xintong Yu'), arxiv.Result.Author('Yewei Fang'), arxiv.Result.Author('Lanxin Li'), arxiv.Result.Author('Xuyi Chen'), arxiv.Result.Author('Yuxiang Lu'), arxiv.Result.Author('Jiaxiang Liu'), arxiv.Result.Author('Weichong Yin'), arxiv.Result.Author('Shikun Feng'), arxiv.Result.Author('Yu Sun'), arxiv.Result.Author('Hao Tian'), arxiv.Result.Author('Hua Wu'), arxiv.Result.Author('Haifeng Wang')]","Recent progress in diffusion models has revolutionized the popular technology
of text-to-image generation. While existing approaches could produce
photorealistic high-resolution images with text conditions, there are still
several open problems to be solved, which limits the further improvement of
image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a
large-scale Chinese text-to-image diffusion model, which progressively upgrades
the quality of generated images~by: (1) incorporating fine-grained textual and
visual knowledge of key elements in the scene, and (2) utilizing different
denoising experts at different denoising stages. With the proposed mechanisms,
ERNIE-ViLG 2.0 not only achieves the state-of-the-art on MS-COCO with zero-shot
FID score of 6.75, but also significantly outperforms recent models in terms of
image fidelity and image-text alignment, with side-by-side human evaluation on
the bilingual prompt set ViLG-300.",0.38089734,0.13668816,0.12457527,A
13087,"The proposed     emphasis module re-calibrates channel-wise feature maps and
scheme is concluded in Section VI, which includes compre-         selectively emphasises the informative features, resulting in
hensive Ô¨Åndings and recommendations for further research.","Secondly, the detail
detailed comparative analysis of the experiment.",more accurate and sharper depth prediction.,2022-10-27 12:34:41+00:00,2T-UNET: A Two-Tower UNet with Depth Clues for Robust Stereo Depth Estimation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Rohit Choudhary'), arxiv.Result.Author('Mansi Sharma'), arxiv.Result.Author('Rithvik Anil')]","Stereo correspondence matching is an essential part of the multi-step stereo
depth estimation process. This paper revisits the depth estimation problem,
avoiding the explicit stereo matching step using a simple two-tower
convolutional neural network. The proposed algorithm is entitled as 2T-UNet.
The idea behind 2T-UNet is to replace cost volume construction with twin
convolution towers. These towers have an allowance for different weights
between them. Additionally, the input for twin encoders in 2T-UNet are
different compared to the existing stereo methods. Generally, a stereo network
takes a right and left image pair as input to determine the scene geometry.
However, in the 2T-UNet model, the right stereo image is taken as one input and
the left stereo image along with its monocular depth clue information, is taken
as the other input. Depth clues provide complementary suggestions that help
enhance the quality of predicted scene geometry. The 2T-UNet surpasses
state-of-the-art monocular and stereo depth estimation methods on the
challenging Scene flow dataset, both quantitatively and qualitatively. The
architecture performs incredibly well on complex natural scenes, highlighting
its usefulness for various real-time applications. Pretrained weights and code
will be made readily available.",0.17183915,0.10709338,0.016865836,A
13097,In a further study             problem.,"This task is named the few-shot MAD (FS-MAD)
                                        tools are highly vulnerable to such attacks.","Unlike existing MAD research, few-shot MAF (FS-
                                        [4], the authors showed that the images of morphed faces are                MAF) aims at learning general discriminative features, which
                                        realistic enough to fool human examiners.",2022-10-27 14:46:53+00:00,Fusion-based Few-Shot Morphing Attack Detection and Fingerprinting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Na Zhang'), arxiv.Result.Author('Shan Jia'), arxiv.Result.Author('Siwei Lyu'), arxiv.Result.Author('Xin Li')]","The vulnerability of face recognition systems to morphing attacks has posed a
serious security threat due to the wide adoption of face biometrics in the real
world. Most existing morphing attack detection (MAD) methods require a large
amount of training data and have only been tested on a few predefined attack
models. The lack of good generalization properties, especially in view of the
growing interest in developing novel morphing attacks, is a critical limitation
with existing MAD research. To address this issue, we propose to extend MAD
from supervised learning to few-shot learning and from binary detection to
multiclass fingerprinting in this paper. Our technical contributions include:
1) We propose a fusion-based few-shot learning (FSL) method to learn
discriminative features that can generalize to unseen morphing attack types
from predefined presentation attacks; 2) The proposed FSL based on the fusion
of the PRNU model and Noiseprint network is extended from binary MAD to
multiclass morphing attack fingerprinting (MAF). 3) We have collected a
large-scale database, which contains five face datasets and eight different
morphing algorithms, to benchmark the proposed few-shot MAF (FS-MAF) method.
Extensive experimental results show the outstanding performance of our
fusion-based FS-MAF. The code and data will be publicly available at
https://github.com/nz0001na/mad maf.",-0.09249334,-0.10123317,-0.09712423,C
13101,"In order to follow this same line to improve robot navigation, we propose
different branches for further research:

 ‚Äì Own object detection: We construct our auxiliary task based on avail-
     able metadata of different environments (Matterport3DMeta).","We then demonstrate that the cre-
ation of semantically richer instructions that include explicit visual information
allows the agent to better learn to navigate.","If we want
     to expand to new environments where this metadata is not available, we
     must detect objects on our own.",2022-10-27 15:58:07+00:00,Bridging the visual gap in VLN via semantically richer instructions,cs.CV,['cs.CV'],"[arxiv.Result.Author('Joaquin Ossand√≥n'), arxiv.Result.Author('Benjamin Earle'), arxiv.Result.Author('√Ålvaro Soto')]","The Visual-and-Language Navigation (VLN) task requires understanding a
textual instruction to navigate a natural indoor environment using only visual
information. While this is a trivial task for most humans, it is still an open
problem for AI models. In this work, we hypothesize that poor use of the visual
information available is at the core of the low performance of current models.
To support this hypothesis, we provide experimental evidence showing that
state-of-the-art models are not severely affected when they receive just
limited or even no visual data, indicating a strong overfitting to the textual
instructions. To encourage a more suitable use of the visual information, we
propose a new data augmentation method that fosters the inclusion of more
explicit visual information in the generation of textual navigational
instructions. Our main intuition is that current VLN datasets include textual
instructions that are intended to inform an expert navigator, such as a human,
but not a beginner visual navigational agent, such as a randomly initialized DL
model. Specifically, to bridge the visual semantic gap of current VLN datasets,
we take advantage of metadata available for the Matterport3D dataset that,
among others, includes information about object labels that are present in the
scenes. Training a state-of-the-art model with the new set of instructions
increase its performance by 8% in terms of success rate on unseen environments,
demonstrating the advantages of the proposed data augmentation method.",-0.2822341,0.054093625,-0.18531051,B
13127,"To further study the dependence of HTP‚Äôs model com-
ponents on the task performance and its generalization to increasing number of
object goals we conduct several ablations, results for which are available in the
supplementary.","Further, OracleMap-Waypoints
performs better than OracleMap (Occ+Obj) confirming the benefits of subgoals
in long-horizon settings.","Overall, this large effect of small increments in the spatial task scale at hard
and harder levels (Tab 1) shows how Long-HOT stress-tests planning, explo-
ration and reasoning over long spatial and temporal horizons.",2022-10-28 05:30:49+00:00,Long-HOT: A Modular Hierarchical Approach for Long-Horizon Object Transport,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Sriram Narayanan'), arxiv.Result.Author('Dinesh Jayaraman'), arxiv.Result.Author('Manmohan Chandraker')]","We address key challenges in long-horizon embodied exploration and navigation
by proposing a new object transport task and a novel modular framework for
temporally extended navigation. Our first contribution is the design of a novel
Long-HOT environment focused on deep exploration and long-horizon planning
where the agent is required to efficiently find and pick up target objects to
be carried and dropped at a goal location, with load constraints and optional
access to a container if it finds one. Further, we propose a modular
hierarchical transport policy (HTP) that builds a topological graph of the
scene to perform exploration with the help of weighted frontiers. Our
hierarchical approach uses a combination of motion planning algorithms to reach
point goals within explored locations and object navigation policies for moving
towards semantic targets at unknown locations. Experiments on both our proposed
Habitat transport task and on MultiOn benchmarks show that our method
significantly outperforms baselines and prior works. Further, we validate the
effectiveness of our modular approach for long-horizon transport by
demonstrating meaningful generalization to much harder transport scenes with
training only on simpler versions of the task.",0.23527566,0.06468192,-0.22650325,A
13131,"SpeciÔ¨Åcally, to cope with
                                        which is hoped to pave the way for further study in the related    the performance degradation caused by distributed learning, a
                                        research Ô¨Åeld.","This work is the very Ô¨Årst attempt to en-       VMR), to enable high-efÔ¨Åciency and effective VMR model-
                                        able safe and efÔ¨Åcient VMR training in decentralized scene,        ing training with distributed data.","new strategy called grouped sequential federated learning
                                                                                                           is proposed.",2022-10-28 08:17:57+00:00,FedVMR: A New Federated Learning method for Video Moment Retrieval,cs.CV,"['cs.CV', 'cs.IR', 'cs.MM']","[arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Xin Luo'), arxiv.Result.Author('Zhen-Duo Chen'), arxiv.Result.Author('Peng-Fei Zhang'), arxiv.Result.Author('Meng Liu'), arxiv.Result.Author('Xin-Shun Xu')]","Despite the great success achieved, existing video moment retrieval (VMR)
methods are developed under the assumption that data are centralizedly stored.
However, in real-world applications, due to the inherent nature of data
generation and privacy concerns, data are often distributed on different silos,
bringing huge challenges to effective large-scale training. In this work, we
try to overcome above limitation by leveraging the recent success of federated
learning. As the first that is explored in VMR field, the new task is defined
as video moment retrieval with distributed data. Then, a novel federated
learning method named FedVMR is proposed to facilitate large-scale and secure
training of VMR models in decentralized environment. Experiments on benchmark
datasets demonstrate its effectiveness. This work is the very first attempt to
enable safe and efficient VMR training in decentralized scene, which is hoped
to pave the way for further study in the related research field.",0.09009632,-0.18634696,0.061014175,C
13132,"The main reason is that WA hurts the novel-       situation as a limitation for the dummy class-based method,
class performance through the poor weight integration in low     which needs to be further researched in the future.",classes mAPN .,Ô¨Åne-tuning iterations.,2022-10-28 09:02:32+00:00,Towards Few-Shot Open-Set Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Binyi Su'), arxiv.Result.Author('Hua Zhang'), arxiv.Result.Author('Jingzhi Li'), arxiv.Result.Author('Zhong Zhou')]","Open-set object detection (OSOD) aims to detect the known categories and
identify unknown objects in a dynamic world, which has achieved significant
attentions. However, previous approaches only consider this problem in
data-abundant conditions, while neglecting the few-shot scenes. In this paper,
we seek a solution for the few-shot open-set object detection (FSOSOD), which
aims to quickly train a detector based on few samples while detecting all known
classes and identifying unknown classes. The main challenge for this task is
that few training samples induce the model to overfit on the known classes,
resulting in a poor open-set performance. We propose a new FSOSOD algorithm to
tackle this issue, named Few-shOt Open-set Detector (FOOD), which contains a
novel class weight sparsification classifier (CWSC) and a novel unknown
decoupling learner (UDL). To prevent over-fitting, CWSC randomly sparses parts
of the normalized weights for the logit prediction of all classes, and then
decreases the co-adaptability between the class and its neighbors. Alongside,
UDL decouples training the unknown class and enables the model to form a
compact unknown decision boundary. Thus, the unknown objects can be identified
with a confidence probability without any pseudo-unknown samples for training.
We compare our method with several state-of-the-art OSOD methods in few-shot
scenes and observe that our method improves the recall of unknown classes by
5%-9% across all shots in VOC-COCO dataset setting.",0.37681818,-0.07681376,0.05645501,A
13138,Most              ated with this work to accelerate further research.,1(b)).,"SAR missions require the vision-based system to be able
to detect and track from a large distance, {SeaDronesSim            ‚Ä¢ We proposed a pipeline for autonomously generating
is able to vary its resolution for the RGB footage captured.",2022-10-26 21:50:50+00:00,SeaDroneSim: Simulation of Aerial Images for Detection of Objects Above Water,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Xiaomin Lin'), arxiv.Result.Author('Cheng Liu'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Yiannis Aloimonous')]","Unmanned Aerial Vehicles (UAVs) are known for their fast and versatile
applicability. With UAVs' growth in availability and applications, they are now
of vital importance in serving as technological support in
search-and-rescue(SAR) operations in marine environments. High-resolution
cameras and GPUs can be equipped on the UAVs to provide effective and efficient
aid to emergency rescue operations. With modern computer vision algorithms, we
can detect objects for aiming such rescue missions. However, these modern
computer vision algorithms are dependent on numerous amounts of training data
from UAVs, which is time-consuming and labor-intensive for maritime
environments. To this end, we present a new benchmark suite,
\textit{\textbf{SeaDroneSim}}, that can be used to create photo-realistic
aerial image datasets with the ground truth for segmentation masks of any given
object. Utilizing only the synthetic data generated from
\textit{\textbf{SeaDroneSim}}, we obtain 71 mAP on real aerial images for
detecting BlueROV as a feasibility study. This result from the new simulation
suit also serves as a baseline for the detection of BlueROV.",-0.2409961,0.3621974,0.07084718,B
13139,Most              ated with this work to accelerate further research.,1(b)).,"SAR missions require the vision-based system to be able
to detect and track from a large distance, {SeaDronesSim            ‚Ä¢ We proposed a pipeline for autonomously generating
is able to vary its resolution for the RGB footage captured.",2022-10-26 21:50:50+00:00,SeaDroneSim: Simulation of Aerial Images for Detection of Objects Above Water,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Xiaomin Lin'), arxiv.Result.Author('Cheng Liu'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Yiannis Aloimonous')]","Unmanned Aerial Vehicles (UAVs) are known for their fast and versatile
applicability. With UAVs' growth in availability and applications, they are now
of vital importance in serving as technological support in
search-and-rescue(SAR) operations in marine environments. High-resolution
cameras and GPUs can be equipped on the UAVs to provide effective and efficient
aid to emergency rescue operations. With modern computer vision algorithms, we
can detect objects for aiming such rescue missions. However, these modern
computer vision algorithms are dependent on numerous amounts of training data
from UAVs, which is time-consuming and labor-intensive for maritime
environments. To this end, we present a new benchmark suite, SeaDroneSim, that
can be used to create photo-realistic aerial image datasets with the ground
truth for segmentation masks of any given object. Utilizing only the synthetic
data generated from SeaDroneSim, we obtain 71 mAP on real aerial images for
detecting BlueROV as a feasibility study. This result from the new simulation
suit also serves as a baseline for the detection of BlueROV.",-0.2409961,0.3621974,0.07084718,B
13140,3               ciated with this work to accelerate further research.,"We are capable of generating videos and              ‚Ä¢ We open-source 1our SeaDronesSim and dataset asso-
images of any object of interest with a 3D model Fig.","in open water with various lighting conditions, image al-
titudes, viewing angles, and background water colors (see          ‚Ä¢ We proposed a pipeline for autonomously generating
Fig.",2022-10-26 21:50:50+00:00,SeaDroneSim: Simulation of Aerial Images for Detection of Objects Above Water,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Xiaomin Lin'), arxiv.Result.Author('Cheng Liu'), arxiv.Result.Author('Allen Pattillo'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Yiannis Aloimonous')]","Unmanned Aerial Vehicles (UAVs) are known for their fast and versatile
applicability. With UAVs' growth in availability and applications, they are now
of vital importance in serving as technological support in
search-and-rescue(SAR) operations in marine environments. High-resolution
cameras and GPUs can be equipped on the UAVs to provide effective and efficient
aid to emergency rescue operations. With modern computer vision algorithms, we
can detect objects for aiming such rescue missions. However, these modern
computer vision algorithms are dependent on numerous amounts of training data
from UAVs, which is time-consuming and labor-intensive for maritime
environments. To this end, we present a new benchmark suite, SeaDroneSim, that
can be used to create photo-realistic aerial image datasets with the ground
truth for segmentation masks of any given object. Utilizing only the synthetic
data generated from SeaDroneSim, we obtain 71 mAP on real aerial images for
detecting BlueROV as a feasibility study. This result from the new simulation
suit also serves as a baseline for the detection of BlueROV.",-0.28487483,0.27402422,-0.05142545,B
13144,"We re-
                                                 lease Vident-lab at https://doi.org/10.34808/1jby-ay90, the Ô¨Årst dataset
                                                 of dental videos with multi-task labels to facilitate further research in
                                                 relevant video processing applications.","Our experiments on videos of natural teeth in
                                                 phantom scenes demonstrate that the proposed network achieves state-
                                                 of-the-art results in multiple tasks with near real-time processing.","Keywords: Multi-task learning ¬∑ Dental Interventions ¬∑ Video restora-
                                                 tion ¬∑ Motion estimation

                                        1 Introduction

                                        Computer-aided dental intervention is an emerging Ô¨Åeld [33,10,20].",2022-10-25 13:17:59+00:00,Multi-task Video Enhancement for Dental Interventions,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Efklidis Katsaros'), arxiv.Result.Author('Piotr K. Ostrowski'), arxiv.Result.Author('Krzysztof W≈Ç√≥darczak'), arxiv.Result.Author('Emilia Lewandowska'), arxiv.Result.Author('Jacek Ruminski'), arxiv.Result.Author('Damian Siupka-Mr√≥z'), arxiv.Result.Author('≈Åukasz Lassmann'), arxiv.Result.Author('Anna Jezierska'), arxiv.Result.Author('Daniel Wƒôsierski')]","A microcamera firmly attached to a dental handpiece allows dentists to
continuously monitor the progress of conservative dental procedures. Video
enhancement in video-assisted dental interventions alleviates low-light, noise,
blur, and camera handshakes that collectively degrade visual comfort. To this
end, we introduce a novel deep network for multi-task video enhancement that
enables macro-visualization of dental scenes. In particular, the proposed
network jointly leverages video restoration and temporal alignment in a
multi-scale manner for effective video enhancement. Our experiments on videos
of natural teeth in phantom scenes demonstrate that the proposed network
achieves state-of-the-art results in multiple tasks with near real-time
processing. We release Vident-lab at https://doi.org/10.34808/1jby-ay90, the
first dataset of dental videos with multi-task labels to facilitate further
research in relevant video processing applications.",-0.28611076,-0.023135759,-0.096033074,C
13172,We hope this work can inspire others to do further research in this area.,"Moreover, SL3D
generates good clusters of pseudo labels, and its pretrained weights can improve supervised learning
via transfer learning.","References

 [1] Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scannet: Richly-
      annotated 3d reconstructions of indoor scenes.",2022-10-30 11:08:25+00:00,SL3D: Self-supervised-Self-labeled 3D Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fernando Julio Cendra'), arxiv.Result.Author('Lan Ma'), arxiv.Result.Author('Jiajun Shen'), arxiv.Result.Author('Xiaojuan Qi')]","There are a lot of promising results in 3D recognition, including
classification, object detection, and semantic segmentation. However, many of
these results rely on manually collecting densely annotated real-world 3D data,
which is highly time-consuming and expensive to obtain, limiting the
scalability of 3D recognition tasks. Thus in this paper, we study unsupervised
3D recognition and propose a Self-supervised-Self-Labeled 3D Recognition (SL3D)
framework. SL3D simultaneously solves two coupled objectives, i.e., clustering
and learning feature representation to generate pseudo labeled data for
unsupervised 3D recognition. SL3D is a generic framework and can be applied to
solve different 3D recognition tasks, including classification, object
detection, and semantic segmentation. Extensive experiments demonstrate its
effectiveness. Code is available at https://github.com/fcendra/sl3d.",-0.32372135,-0.044978525,0.080972284,B
13173,We hope this work can inspire others to do further research in this area.,"Moreover, SL3D
generates good clusters of pseudo labels, and its pretrained weights can improve supervised learning
via transfer learning.","References

 [1] Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scannet: Richly-
      annotated 3d reconstructions of indoor scenes.",2022-10-30 11:08:25+00:00,SL3D: Self-supervised-Self-labeled 3D Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fernando Julio Cendra'), arxiv.Result.Author('Lan Ma'), arxiv.Result.Author('Jiajun Shen'), arxiv.Result.Author('Xiaojuan Qi')]","There are a lot of promising results in 3D recognition, including
classification, object detection, and semantic segmentation. However, many of
these results rely on manually collecting densely annotated real-world 3D data,
which is highly time-consuming and expensive to obtain, limiting the
scalability of 3D recognition tasks. Thus in this paper, we study unsupervised
3D recognition and propose a Self-supervised-Self-Labeled 3D Recognition (SL3D)
framework. SL3D simultaneously solves two coupled objectives, i.e., clustering
and learning feature representation to generate pseudo labeled data for
unsupervised 3D recognition. SL3D is a generic framework and can be applied to
solve different 3D recognition tasks, including classification, object
detection, and semantic segmentation. Extensive experiments demonstrate its
effectiveness. Code is available at https://github.com/fcendra/sl3d.",-0.32372135,-0.044978525,0.080972284,B
13174,We hope this work can inspire others to do further research in this area.,"Moreover, SL3D
generates good clusters of pseudo labels, and its pretrained weights can improve supervised learning
via transfer learning.","References

 [1] Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scannet: Richly-
      annotated 3d reconstructions of indoor scenes.",2022-10-30 11:08:25+00:00,SL3D: Self-supervised-Self-labeled 3D Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Fernando Julio Cendra'), arxiv.Result.Author('Lan Ma'), arxiv.Result.Author('Jiajun Shen'), arxiv.Result.Author('Xiaojuan Qi')]","Deep learning has attained remarkable success in many 3D visual recognition
tasks, including shape classification, object detection, and semantic
segmentation. However, many of these results rely on manually collecting
densely annotated real-world 3D data, which is highly time-consuming and
expensive to obtain, limiting the scalability of 3D recognition tasks. Thus, we
study unsupervised 3D recognition and propose a Self-supervised-Self-Labeled 3D
Recognition (SL3D) framework. SL3D simultaneously solves two coupled
objectives, i.e., clustering and learning feature representation to generate
pseudo-labeled data for unsupervised 3D recognition. SL3D is a generic
framework and can be applied to solve different 3D recognition tasks, including
classification, object detection, and semantic segmentation. Extensive
experiments demonstrate its effectiveness. Code is available at
https://github.com/fcendra/sl3d.",-0.32372135,-0.044978525,0.080972284,B
13176,"The relatively poorer linear probe performance of MAE on JFT-300M highlights the non-triviality
of scaling from IN-1K to larger datasets and suggests that while MAE is scalable for model size,
scalability to larger datasets requires further study.",(2022).,"Figure 4 gives the corresponding Ô¨Ånetuning
results.",2022-10-30 16:21:22+00:00,"A simple, efficient and scalable contrastive masked autoencoder for learning visual representations",cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Shlok Mishra'), arxiv.Result.Author('Joshua Robinson'), arxiv.Result.Author('Huiwen Chang'), arxiv.Result.Author('David Jacobs'), arxiv.Result.Author('Aaron Sarna'), arxiv.Result.Author('Aaron Maschinot'), arxiv.Result.Author('Dilip Krishnan')]","We introduce CAN, a simple, efficient and scalable method for self-supervised
learning of visual representations. Our framework is a minimal and conceptually
clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N)
the noise prediction approach used in diffusion models. The learning mechanisms
are complementary to one another: contrastive learning shapes the embedding
space across a batch of image samples; masked autoencoders focus on
reconstruction of the low-frequency spatial correlations in a single image
sample; and noise prediction encourages the reconstruction of the
high-frequency components of an image. The combined approach results in a
robust, scalable and simple-to-implement algorithm. The training process is
symmetric, with 50% of patches in both views being masked at random, yielding a
considerable efficiency improvement over prior contrastive learning methods.
Extensive empirical studies demonstrate that CAN achieves strong downstream
performance under both linear and finetuning evaluations on transfer learning
and robustness tasks. CAN outperforms MAE and SimCLR when pre-training on
ImageNet, but is especially useful for pre-training on larger uncurated
datasets such as JFT-300M: for linear probe on ImageNet, CAN achieves 75.4%
compared to 73.4% for SimCLR and 64.1% for MAE. The finetuned performance on
ImageNet of our ViT-L model is 86.1%, compared to 85.5% for SimCLR, and 85.4%
for MAE. The overall FLOPs load of SimCLR is 70% higher than CAN for ViT-L
models.",0.31483096,0.039646275,0.20277682,A
13179,"We hope this
                                                                  work could provide insights to the biomedical imaging and
In order to show the interpretability of the proposed ViTASD,     signal processing community for ASD research and inspire
we visualize the attention maps during inference on the test      further study on exploring the potential of applying explainable
set in Fig.",Visualization and interpretability                           consistent with distinguishable ASD features.,3.,2022-10-30 20:38:56+00:00,ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Xu Cao'), arxiv.Result.Author('Wenqian Ye'), arxiv.Result.Author('Elena Sizikova'), arxiv.Result.Author('Xue Bai'), arxiv.Result.Author('Megan Coffee'), arxiv.Result.Author('Hongwu Zeng'), arxiv.Result.Author('Jianguo Cao')]","Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with
very high prevalence around the world. Research progress in the field of ASD
facial analysis in pediatric patients has been hindered due to a lack of
well-established baselines. In this paper, we propose the use of the Vision
Transformer (ViT) for the computational analysis of pediatric ASD. The
presented model, known as ViTASD, distills knowledge from large facial
expression datasets and offers model structure transferability. Specifically,
ViTASD employs a vanilla ViT to extract features from patients' face images and
adopts a lightweight decoder with a Gaussian Process layer to enhance the
robustness for ASD analysis. Extensive experiments conducted on standard ASD
facial analysis benchmarks show that our method outperforms all of the
representative approaches in ASD facial analysis, while the ViTASD-L achieves a
new state-of-the-art. Our code and pretrained models are available at
https://github.com/IrohXu/ViTASD.",-0.016001381,-0.023405407,-0.15584221,C
13218,stimulate further research in these fields.,11 in part (b).,"The proposed method
                                                                        of representation and computation can be generalized and used
                                                                        for other unitary transforms used in image and signal
                                                                        processing, including the Hadamard transforms [37].",2022-10-31 17:13:27+00:00,Quantum-Inspired Edge Detection Algorithms Implementation using New Dynamic Visual Data Representation and Short-Length Convolution Computation,cs.CV,"['cs.CV', 'math.QA', '68Q12', 'F.2; I.4.0']","[arxiv.Result.Author('Artyom M. Grigoryan'), arxiv.Result.Author('Sos S. Agaian'), arxiv.Result.Author('Karen Panetta')]","As the availability of imagery data continues to swell, so do the demands on
transmission, storage and processing power. Processing requirements to handle
this plethora of data is quickly outpacing the utility of conventional
processing techniques. Transitioning to quantum processing and algorithms that
offer promising efficiencies over conventional methods can address some of
these issues. However, to make this transformation possible, fundamental issues
of implementing real time Quantum algorithms must be overcome for crucial
processes needed for intelligent analysis applications. For example, consider
edge detection tasks which require time-consuming acquisition processes and are
further hindered by the complexity of the devices used thus limiting
feasibility for implementation in real-time applications. Convolution is
another example of an operation that is essential for signal and image
processing applications, where the mathematical operations consist of an
intelligent mixture of multiplication and addition that require considerable
computational resources. This paper studies a new paired transform-based
quantum representation and computation of one-dimensional and 2-D signals
convolutions and gradients. A new visual data representation is defined to
simplify convolution calculations making it feasible to parallelize convolution
and gradient operations for more efficient performance. The new data
representation is demonstrated on multiple illustrative examples for quantum
edge detection, gradients, and convolution. Furthermore, the efficiency of the
proposed approach is shown on real-world images.",0.06389841,0.14193079,0.1317197,C
13219,"We
hypothesize the answer is ‚Äúyes‚Äù, but requires further research.",paired transform in the proposed method of convolution?,"[11] V. Argyriou, T. Vlachos, R. Piroddi, ‚ÄúGradient-adaptive
                                                                              normalized convolution,‚Äù IEEE Signal Processing Letters, vol.",2022-10-31 17:13:27+00:00,Quantum-Inspired Edge Detection Algorithms Implementation using New Dynamic Visual Data Representation and Short-Length Convolution Computation,cs.CV,"['cs.CV', 'math.QA', '68Q12', 'F.2; I.4.0']","[arxiv.Result.Author('Artyom M. Grigoryan'), arxiv.Result.Author('Sos S. Agaian'), arxiv.Result.Author('Karen Panetta')]","As the availability of imagery data continues to swell, so do the demands on
transmission, storage and processing power. Processing requirements to handle
this plethora of data is quickly outpacing the utility of conventional
processing techniques. Transitioning to quantum processing and algorithms that
offer promising efficiencies over conventional methods can address some of
these issues. However, to make this transformation possible, fundamental issues
of implementing real time Quantum algorithms must be overcome for crucial
processes needed for intelligent analysis applications. For example, consider
edge detection tasks which require time-consuming acquisition processes and are
further hindered by the complexity of the devices used thus limiting
feasibility for implementation in real-time applications. Convolution is
another example of an operation that is essential for signal and image
processing applications, where the mathematical operations consist of an
intelligent mixture of multiplication and addition that require considerable
computational resources. This paper studies a new paired transform-based
quantum representation and computation of one-dimensional and 2-D signals
convolutions and gradients. A new visual data representation is defined to
simplify convolution calculations making it feasible to parallelize convolution
and gradient operations for more efficient performance. The new data
representation is demonstrated on multiple illustrative examples for quantum
edge detection, gradients, and convolution. Furthermore, the efficiency of the
proposed approach is shown on real-world images.",0.0143101495,0.07784276,0.21399412,C
13286,"For
to the dataset without annotations in case of further research.",We decide to attach these images separately              every damaged component is charged separately.,"instance, if the car front door and back door are both
We manually pick out the six classes of damages from the                  scratched (as shown in Fig.",2022-11-02 08:09:03+00:00,CarDD: A New Dataset for Vision-based Car Damage Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xinkuang Wang'), arxiv.Result.Author('Wenjing Li'), arxiv.Result.Author('Zhongcheng Wu')]","Automatic car damage detection has attracted significant attention in the car
insurance business. However, due to the lack of high-quality and publicly
available datasets, we can hardly learn a feasible model for car damage
detection. To this end, we contribute with the Car Damage Detection (CarDD),
the first public large-scale dataset designed for vision-based car damage
detection and segmentation. Our CarDD contains 4,000 high-resolution car damage
images with over 9,000 wellannotated instances of six damage categories
(examples are shown in Fig. 1). We detail the image collection, selection, and
annotation processes, and present a statistical dataset analysis. Furthermore,
we conduct extensive experiments on CarDD with state-of-theart deep methods for
different tasks and provide comprehensive analysis to highlight the specialty
of car damage detection.",-0.03788466,0.049183905,-0.1158821,C
13338,"Along with these insights, we release
                                               a toolkit based on ImageNet-X to spur further study into the mistakes the image recognition systems
                                               make: https://facebookresearch.github.io/imagenetx/site/home.","Together, these insights
                                               suggests that to advance the robustness of modern vision models, future research should focus on collecting
                                               additional diverse data and understanding data augmentation schemes.","1 Introduction

                                       Despite deep learning surpassing human performance on ImageNet [23, 15], even today‚Äôs best vision systems
                                       can fail in spectacular ways.",2022-11-03 14:56:32+00:00,ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Badr Youbi Idrissi'), arxiv.Result.Author('Diane Bouchacourt'), arxiv.Result.Author('Randall Balestriero'), arxiv.Result.Author('Ivan Evtimov'), arxiv.Result.Author('Caner Hazirbas'), arxiv.Result.Author('Nicolas Ballas'), arxiv.Result.Author('Pascal Vincent'), arxiv.Result.Author('Michal Drozdzal'), arxiv.Result.Author('David Lopez-Paz'), arxiv.Result.Author('Mark Ibrahim')]","Deep learning vision systems are widely deployed across applications where
reliability is critical. However, even today's best models can fail to
recognize an object when its pose, lighting, or background varies. While
existing benchmarks surface examples challenging for models, they do not
explain why such mistakes arise. To address this need, we introduce ImageNet-X,
a set of sixteen human annotations of factors such as pose, background, or
lighting the entire ImageNet-1k validation set as well as a random subset of
12k training images. Equipped with ImageNet-X, we investigate 2,200 current
recognition models and study the types of mistakes as a function of model's (1)
architecture, e.g. transformer vs. convolutional, (2) learning paradigm, e.g.
supervised vs. self-supervised, and (3) training procedures, e.g., data
augmentation. Regardless of these choices, we find models have consistent
failure modes across ImageNet-X categories. We also find that while data
augmentation can improve robustness to certain factors, they induce spill-over
effects to other factors. For example, strong random cropping hurts robustness
on smaller objects. Together, these insights suggest to advance the robustness
of modern vision models, future research should focus on collecting additional
data and understanding data augmentation schemes. Along with these insights, we
release a toolkit based on ImageNet-X to spur further study into the mistakes
image recognition systems make.",-0.4003489,-0.20034188,0.1750862,C
13351,"We hope our work will
inspire further research in this promising direction of freezing pretrained image models.","Thanks to the efÔ¨Åciency of this frozen setting, competitive performance are achieved on different
model sizes with much less trainable parameters, as indicated in Figure 1.","The proposed
approach can serve as a simple baseline and guide the evaluation of future work.",2022-11-03 17:57:10+00:00,Could Giant Pretrained Image Models Extract Universal Representations?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yutong Lin'), arxiv.Result.Author('Ze Liu'), arxiv.Result.Author('Zheng Zhang'), arxiv.Result.Author('Han Hu'), arxiv.Result.Author('Nanning Zheng'), arxiv.Result.Author('Stephen Lin'), arxiv.Result.Author('Yue Cao')]","Frozen pretrained models have become a viable alternative to the
pretraining-then-finetuning paradigm for transfer learning. However, with
frozen models there are relatively few parameters available for adapting to
downstream tasks, which is problematic in computer vision where tasks vary
significantly in input/output format and the type of information that is of
value. In this paper, we present a study of frozen pretrained models when
applied to diverse and representative computer vision tasks, including object
detection, semantic segmentation and video action recognition. From this
empirical analysis, our work answers the questions of what pretraining task
fits best with this frozen setting, how to make the frozen setting more
flexible to various downstream tasks, and the effect of larger model sizes. We
additionally examine the upper bound of performance using a giant frozen
pretrained model with 3 billion parameters (SwinV2-G) and find that it reaches
competitive performance on a varied set of major benchmarks with only one
shared frozen base network: 60.0 box mAP and 52.2 mask mAP on COCO object
detection test-dev, 57.6 val mIoU on ADE20K semantic segmentation, and 81.7
top-1 accuracy on Kinetics-400 action recognition. With this work, we hope to
bring greater attention to this promising path of freezing pretrained image
models.",0.028360244,0.029459625,0.192429,C
13393,"To further study the effect of the number
ImageNet-testbed (Taori et al.","In particular, we use the  appropriate views.","2020) to test the feature ex-   of views, we weaken LMA to only favor Ô¨Ånite-view gener-
tractor plus linear classiÔ¨Åer head that are trained on Ima-    ation.",2022-11-05 02:00:13+00:00,Local Manifold Augmentation for Multiview Semantic Consistency,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Yang'), arxiv.Result.Author('Wing Yin Cheung'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Xiangyang Ji')]","Multiview self-supervised representation learning roots in exploring semantic
consistency across data of complex intra-class variation. Such variation is not
directly accessible and therefore simulated by data augmentations. However,
commonly adopted augmentations are handcrafted and limited to simple
geometrical and color changes, which are unable to cover the abundant
intra-class variation. In this paper, we propose to extract the underlying data
variation from datasets and construct a novel augmentation operator, named
local manifold augmentation (LMA). LMA is achieved by training an
instance-conditioned generator to fit the distribution on the local manifold of
data and sampling multiview data using it. LMA shows the ability to create an
infinite number of data views, preserve semantics, and simulate complicated
variations in object pose, viewpoint, lighting condition, background etc.
Experiments show that with LMA integrated, self-supervised learning methods
such as MoCov2 and SimSiam gain consistent improvement on prevalent benchmarks
including CIFAR10, CIFAR100, STL10, ImageNet100, and ImageNet. Furthermore, LMA
leads to representations that obtain more significant invariance to the
viewpoint, object pose, and illumination changes and stronger robustness to
various real distribution shifts reflected by ImageNet-V2, ImageNet-R, ImageNet
Sketch etc.",-0.10117476,-0.11963459,0.015681634,C
13406,We hope to further study this problem in the future.,"This suggests that generation quality may not be the only factor
determining the representation transfer ability.","15
GAN architectures        Type           Sources             Generation   Squeeze
                                                                FID     Top-1 Acc
StyleGAN2-ADA            Unconditional  GitHub repo, model      2.92
AutoGAN                  Unconditional  GitHub repo, model     12.42      87.67
StyleGAN2-ADA            Conditional    GitHub repo, model      2.42      76.28
StyleGAN-XL (StyleGAN3)  Conditional    GitHub repo, model      1.85      88.90
BigGAN-DiffAugment-cr    Conditional    GitHub repo, model      8.49      84.97
                                                                          86.41

Table 13: Ablation study with respect to GAN architecture.",2022-11-06 01:10:28+00:00,Distilling Representations from GAN Generator via Squeeze and Span,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Yang'), arxiv.Result.Author('Xiaotian Cheng'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Hakan Bilen'), arxiv.Result.Author('Xiangyang Ji')]","In recent years, generative adversarial networks (GANs) have been an actively
studied topic and shown to successfully produce high-quality realistic images
in various domains. The controllable synthesis ability of GAN generators
suggests that they maintain informative, disentangled, and explainable image
representations, but leveraging and transferring their representations to
downstream tasks is largely unexplored. In this paper, we propose to distill
knowledge from GAN generators by squeezing and spanning their representations.
We squeeze the generator features into representations that are invariant to
semantic-preserving transformations through a network before they are distilled
into the student network. We span the distilled representation of the synthetic
domain to the real domain by also using real training data to remedy the mode
collapse of GANs and boost the student network performance in a real domain.
Experiments justify the efficacy of our method and reveal its great
significance in self-supervised representation learning. Code is available at
https://github.com/yangyu12/squeeze-and-span.",0.20740747,-0.110592775,0.2197544,C
13407,How to address this issue requires further research in the future.,"In
contrast, supervised learning becomes stronger under a large-scale setting since more labels are
available.","Method                         unlabeled data               # labels  FG-mIoU (%)
Supervised learning                    ‚Äì                    29,000         84.32
Ours                                                         1,500         77.90
Ours                      CelebA-train (29,000)             29,000         76.03
Ours                      CelebA-train (29,000)             29,000         75.00
                     CelebA-train + FFHQ (99,000)

Table C.11: Experiments on human face part segmentation under large-scale setting.",2022-11-06 01:29:22+00:00,Learning to Annotate Part Segmentation with Gradient Matching,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yu Yang'), arxiv.Result.Author('Xiaotian Cheng'), arxiv.Result.Author('Hakan Bilen'), arxiv.Result.Author('Xiangyang Ji')]","The success of state-of-the-art deep neural networks heavily relies on the
presence of large-scale labelled datasets, which are extremely expensive and
time-consuming to annotate. This paper focuses on tackling semi-supervised part
segmentation tasks by generating high-quality images with a pre-trained GAN and
labelling the generated images with an automatic annotator. In particular, we
formulate the annotator learning as a learning-to-learn problem. Given a
pre-trained GAN, the annotator learns to label object parts in a set of
randomly generated images such that a part segmentation model trained on these
synthetic images with their predicted labels obtains low segmentation error on
a small validation set of manually labelled images. We further reduce this
nested-loop optimization problem to a simple gradient matching problem and
efficiently solve it with an iterative algorithm. We show that our method can
learn annotators from a broad range of labelled images including real images,
generated images, and even analytically rendered images. Our method is
evaluated with semi-supervised part segmentation tasks and significantly
outperforms other semi-supervised competitors when the amount of labelled
examples is extremely limited.",-0.062980264,-0.18303843,-0.05340483,C
13416,"We hope our work helps the community and inspires further research on uncertainty-aware
learning of the model as well as making more reliable models for medical diagnosis.","We believe there is potential for this model
architecture to be incorporated using Bayesian methods for various tasks involving classiÔ¨Åcation and
regression.","Broader impact statement

The approach introduced in this paper aims at tackling the issue of over-conÔ¨Ådent predictions during
model outcomes for the diagnosis of Diabetic Retinopathy.",2022-11-06 15:22:06+00:00,UATTA-ENS: Uncertainty Aware Test Time Augmented Ensemble for PIRC Diabetic Retinopathy Detection,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Pratinav Seth'), arxiv.Result.Author('Adil Khan'), arxiv.Result.Author('Ananya Gupta'), arxiv.Result.Author('Saurabh Kumar Mishra'), arxiv.Result.Author('Akshat Bhandhari')]","Deep Ensemble Convolutional Neural Networks has become a methodology of
choice for analyzing medical images with a diagnostic performance comparable to
a physician, including the diagnosis of Diabetic Retinopathy. However, commonly
used techniques are deterministic and are therefore unable to provide any
estimate of predictive uncertainty. Quantifying model uncertainty is crucial
for reducing the risk of misdiagnosis. A reliable architecture should be
well-calibrated to avoid over-confident predictions. To address this, we
propose a UATTA-ENS: Uncertainty-Aware Test-Time Augmented Ensemble Technique
for 5 Class PIRC Diabetic Retinopathy Classification to produce reliable and
well-calibrated predictions.",0.16732459,-0.022682723,-0.19676396,A
13417,"We hope our work helps the community and inspires further research on uncertainty-aware
learning of the model as well as making more reliable models for medical diagnosis.","We believe there is potential for this model
architecture to be incorporated using Bayesian methods for various tasks involving classiÔ¨Åcation and
regression.","Broader impact statement

The approach introduced in this paper aims at tackling the issue of over-conÔ¨Ådent predictions during
model outcomes for the diagnosis of Diabetic Retinopathy.",2022-11-06 15:22:06+00:00,UATTA-ENS: Uncertainty Aware Test Time Augmented Ensemble for PIRC Diabetic Retinopathy Detection,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Pratinav Seth'), arxiv.Result.Author('Adil Khan'), arxiv.Result.Author('Ananya Gupta'), arxiv.Result.Author('Saurabh Kumar Mishra'), arxiv.Result.Author('Akshat Bhandari')]","Deep Ensemble Convolutional Neural Networks has become a methodology of
choice for analyzing medical images with a diagnostic performance comparable to
a physician, including the diagnosis of Diabetic Retinopathy. However, commonly
used techniques are deterministic and are therefore unable to provide any
estimate of predictive uncertainty. Quantifying model uncertainty is crucial
for reducing the risk of misdiagnosis. A reliable architecture should be
well-calibrated to avoid over-confident predictions. To address this, we
propose a UATTA-ENS: Uncertainty-Aware Test-Time Augmented Ensemble Technique
for 5 Class PIRC Diabetic Retinopathy Classification to produce reliable and
well-calibrated predictions.",0.16732459,-0.022682723,-0.19676396,A
13449,"We
       also remark the linear separability of the feature space induced by the NTK (Section 4.2);

    ‚Ä¢ we numerically study the theorems and their limitations in simple cases where the network solves the task
       perfectly, and use the feature space and the NTK to gain understanding about the network‚Äôs predictions
       (Section 5);

    ‚Ä¢ we further study this theorem with an application where we teach a network to learn connectivity of planar
       shapes (Section 6) and Ô¨Ånd limitations in interpolation capabilities and interpretability.","Notably, we propose an
       alternative formulation of Domingos‚Äô result that is perhaps more self-explanatory;

    ‚Ä¢ we extend Domingos‚Äô proof to the discrete setting (Section 3.1) and the multi-dimensional case (Section 3.2);

    ‚Ä¢ we mathematically study the implication of these theorems in the case of a linear regression (Section 4.1).","2 Neural networks and kernel machines

2.1 Neural Networks

A neural network N can be described as a family of functions (fi)i‚àà[|1,D|] and a vector of weights w ‚àà Rd such that
the network is deÔ¨Åned as

                                          N (x; w) = fD(fD‚àí1(.",2022-11-07 13:59:28+00:00,Can neural networks extrapolate? Discussion of a theorem by Pedro Domingos,cs.CV,['cs.CV'],"[arxiv.Result.Author('Adrien Courtois'), arxiv.Result.Author('Jean-Michel Morel'), arxiv.Result.Author('Pablo Arias')]","Neural networks trained on large datasets by minimizing a loss have become
the state-of-the-art approach for resolving data science problems, particularly
in computer vision, image processing and natural language processing. In spite
of their striking results, our theoretical understanding about how neural
networks operate is limited. In particular, what are the interpolation
capabilities of trained neural networks? In this paper we discuss a theorem of
Domingos stating that ""every machine learned by continuous gradient descent is
approximately a kernel machine"". According to Domingos, this fact leads to
conclude that all machines trained on data are mere kernel machines. We first
extend Domingo's result in the discrete case and to networks with vector-valued
output. We then study its relevance and significance on simple examples. We
find that in simple cases, the ""neural tangent kernel"" arising in Domingos'
theorem does provide understanding of the networks' predictions. Furthermore,
when the task given to the network grows in complexity, the interpolation
capability of the network can be effectively explained by Domingos' theorem,
and therefore is limited. We illustrate this fact on a classic perception
theory problem: recovering a shape from its boundary.",0.04752771,-0.16097535,0.18422414,C
13468,Because local appearance is not necessarily discrim-           encourage further research on part-based methods.,"Our BPBreID codebase has been released to
ure 1.","inative, standard ReID losses used for learning global rep-
resentations do not scale well to local representation learn-   2.",2022-11-07 16:48:41+00:00,Body Part-Based Representation Learning for Occluded Person Re-Identification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Vladimir Somers'), arxiv.Result.Author('Christophe De Vleeschouwer'), arxiv.Result.Author('Alexandre Alahi')]","Occluded person re-identification (ReID) is a person retrieval task which
aims at matching occluded person images with holistic ones. For addressing
occluded ReID, part-based methods have been shown beneficial as they offer
fine-grained information and are well suited to represent partially visible
human bodies. However, training a part-based model is a challenging task for
two reasons. Firstly, individual body part appearance is not as discriminative
as global appearance (two distinct IDs might have the same local appearance),
this means standard ReID training objectives using identity labels are not
adapted to local feature learning. Secondly, ReID datasets are not provided
with human topographical annotations. In this work, we propose BPBreID, a body
part-based ReID model for solving the above issues. We first design two modules
for predicting body part attention maps and producing body part-based features
of the ReID target. We then propose GiLt, a novel training scheme for learning
part-based representations that is robust to occlusions and non-discriminative
local appearance. Extensive experiments on popular holistic and occluded
datasets show the effectiveness of our proposed method, which outperforms
state-of-the-art methods by 0.7% mAP and 5.6% rank-1 accuracy on the
challenging Occluded-Duke dataset. Our code is available at
https://github.com/VlSomers/bpbreid.",-0.0064933775,-0.16519503,-0.050879247,C
13469,"CelebA [185], CASIA-WebFace, LFW [143], CALFW [186], with synthetically generated
masked face images to enable further research on masked face recognition [43, 187, 188, 154, 189, 190].","Encouraged by these initiatives, many studies have attempted to enrich existing datasets containing faces
without masks, e.g.","For
instance, Wang et al.",2022-11-07 17:20:39+00:00,A Survey on Computer Vision based Human Analysis in the COVID-19 Era,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Fevziye Irem Eyiokur'), arxiv.Result.Author('Alperen Kantarcƒ±'), arxiv.Result.Author('Mustafa Ekrem Erakƒ±n'), arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('Ferda Ofli'), arxiv.Result.Author('Muhammad Imran'), arxiv.Result.Author('Janez Kri≈æaj'), arxiv.Result.Author('Albert Ali Salah'), arxiv.Result.Author('Alexander Waibel'), arxiv.Result.Author('Vitomir ≈†truc'), arxiv.Result.Author('Hazƒ±m Kemal Ekenel')]","The emergence of COVID-19 has had a global and profound impact, not only on
society as a whole, but also on the lives of individuals. Various prevention
measures were introduced around the world to limit the transmission of the
disease, including face masks, mandates for social distancing and regular
disinfection in public spaces, and the use of screening applications. These
developments also triggered the need for novel and improved computer vision
techniques capable of (i) providing support to the prevention measures through
an automated analysis of visual data, on the one hand, and (ii) facilitating
normal operation of existing vision-based services, such as biometric
authentication schemes, on the other. Especially important here, are computer
vision techniques that focus on the analysis of people and faces in visual data
and have been affected the most by the partial occlusions introduced by the
mandates for facial masks. Such computer vision based human analysis techniques
include face and face-mask detection approaches, face recognition techniques,
crowd counting solutions, age and expression estimation procedures, models for
detecting face-hand interactions and many others, and have seen considerable
attention over recent years. The goal of this survey is to provide an
introduction to the problems induced by COVID-19 into such research and to
present a comprehensive review of the work done in the computer vision based
human analysis field. Particular attention is paid to the impact of facial
masks on the performance of various methods and recent solutions to mitigate
this problem. Additionally, a detailed review of existing datasets useful for
the development and evaluation of methods for COVID-19 related applications is
also provided. Finally, to help advance the field further, a discussion on the
main open challenges and future research direction is given.",-0.056346,-0.05534617,-0.052409258,C
13470,"The consolidated
information presented in the survey is expected to help researchers working on similar problems to quickly
get an overview of the work already done and the main challenges that require further research.","SpeciÔ¨Åcally, we discussed the main challenges introduced to CVHA problems by the pandemic,

    9https://www.nytimes.com/2019/07/26/technology/hong-kong-protests-facial-recognition-surveillance.html
  10Martin Pollard, ‚ÄúEven mask-wearers can be ID‚Äôd, China facial recognition Ô¨Årm says,‚Äù Reuters, 9 March 2020, retrieved
from https://reut.rs/2TAwMux
  11‚ÄúBefore Clearview Became a Police Tool, It Was a Secret Plaything of the Rich,‚Äù The New York Times, March 2020,
https://www.nytimes.com/2020/03/05/technology/clearview-investors.html
  12Accessible from: https://gdpr-info.eu/

                                                                     31
presented a high-level taxonomy of existing methods, elaborated on relevant datasets and described, what
we feel, are the most important open issues that need to be addressed in the future.","Acknowledgements

    This research was supported in parts by the ARRS Research Programme P2‚Äì0250 (B) ‚ÄúMetrology and
Biometric Systems‚Äù and the additional funding provided for COVID-19 related research as well as the
bilateral ARRS-TUBITAK funded project: Low Resolution Face Recognition (FaceLQ), with TUBITAK
project number 120N011.",2022-11-07 17:20:39+00:00,A Survey on Computer Vision based Human Analysis in the COVID-19 Era,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Fevziye Irem Eyiokur'), arxiv.Result.Author('Alperen Kantarcƒ±'), arxiv.Result.Author('Mustafa Ekrem Erakƒ±n'), arxiv.Result.Author('Naser Damer'), arxiv.Result.Author('Ferda Ofli'), arxiv.Result.Author('Muhammad Imran'), arxiv.Result.Author('Janez Kri≈æaj'), arxiv.Result.Author('Albert Ali Salah'), arxiv.Result.Author('Alexander Waibel'), arxiv.Result.Author('Vitomir ≈†truc'), arxiv.Result.Author('Hazƒ±m Kemal Ekenel')]","The emergence of COVID-19 has had a global and profound impact, not only on
society as a whole, but also on the lives of individuals. Various prevention
measures were introduced around the world to limit the transmission of the
disease, including face masks, mandates for social distancing and regular
disinfection in public spaces, and the use of screening applications. These
developments also triggered the need for novel and improved computer vision
techniques capable of (i) providing support to the prevention measures through
an automated analysis of visual data, on the one hand, and (ii) facilitating
normal operation of existing vision-based services, such as biometric
authentication schemes, on the other. Especially important here, are computer
vision techniques that focus on the analysis of people and faces in visual data
and have been affected the most by the partial occlusions introduced by the
mandates for facial masks. Such computer vision based human analysis techniques
include face and face-mask detection approaches, face recognition techniques,
crowd counting solutions, age and expression estimation procedures, models for
detecting face-hand interactions and many others, and have seen considerable
attention over recent years. The goal of this survey is to provide an
introduction to the problems induced by COVID-19 into such research and to
present a comprehensive review of the work done in the computer vision based
human analysis field. Particular attention is paid to the impact of facial
masks on the performance of various methods and recent solutions to mitigate
this problem. Additionally, a detailed review of existing datasets useful for
the development and evaluation of methods for COVID-19 related applications is
also provided. Finally, to help advance the field further, a discussion on the
main open challenges and future research direction is given.",-0.013855243,0.04176059,-0.21012318,C
13480,"We will discuss lower
ysis in section 5.1 and also further study how partial mixup                                                                                                                                                                                  performance cases in the following section.",We present performance anal-                                                                                                                                                                                      performance than 10% partial mixup.,responds to adversarial attacks in section 5.3.,2022-11-08 01:43:14+00:00,Understanding the Role of Mixup in Knowledge Distillation: \\An Empirical Study,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hongjun Choi'), arxiv.Result.Author('Eun Som Jeon'), arxiv.Result.Author('Ankita Shukla'), arxiv.Result.Author('Pavan Turaga')]","Mixup is a popular data augmentation technique based on creating new samples
by linear interpolation between two given data samples, to improve both the
generalization and robustness of the trained model. Knowledge distillation
(KD), on the other hand, is widely used for model compression and transfer
learning, which involves using a larger network's implicit knowledge to guide
the learning of a smaller network. At first glance, these two techniques seem
very different, however, we found that ``smoothness"" is the connecting link
between the two and is also a crucial attribute in understanding KD's interplay
with mixup. Although many mixup variants and distillation methods have been
proposed, much remains to be understood regarding the role of a mixup in
knowledge distillation. In this paper, we present a detailed empirical study on
various important dimensions of compatibility between mixup and knowledge
distillation. We also scrutinize the behavior of the networks trained with a
mixup in the light of knowledge distillation through extensive analysis,
visualizations, and comprehensive experiments on image classification. Finally,
based on our findings, we suggest improved strategies to guide the student
network to enhance its effectiveness. Additionally, the findings of this study
provide insightful suggestions to researchers and practitioners that commonly
use techniques from KD. Our code is available at
https://github.com/hchoi71/MIX-KD.",0.21610431,-0.20240381,0.11464678,A
13481,"We will discuss lower
ysis in section 5.1 and also further study how partial mixup                                                                                                                                                                                  performance cases in the following section.",We present performance anal-                                                                                                                                                                                      performance than 10% partial mixup.,responds to adversarial attacks in section 5.3.,2022-11-08 01:43:14+00:00,Understanding the Role of Mixup in Knowledge Distillation: An Empirical Study,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hongjun Choi'), arxiv.Result.Author('Eun Som Jeon'), arxiv.Result.Author('Ankita Shukla'), arxiv.Result.Author('Pavan Turaga')]","Mixup is a popular data augmentation technique based on creating new samples
by linear interpolation between two given data samples, to improve both the
generalization and robustness of the trained model. Knowledge distillation
(KD), on the other hand, is widely used for model compression and transfer
learning, which involves using a larger network's implicit knowledge to guide
the learning of a smaller network. At first glance, these two techniques seem
very different, however, we found that ""smoothness"" is the connecting link
between the two and is also a crucial attribute in understanding KD's interplay
with mixup. Although many mixup variants and distillation methods have been
proposed, much remains to be understood regarding the role of a mixup in
knowledge distillation. In this paper, we present a detailed empirical study on
various important dimensions of compatibility between mixup and knowledge
distillation. We also scrutinize the behavior of the networks trained with a
mixup in the light of knowledge distillation through extensive analysis,
visualizations, and comprehensive experiments on image classification. Finally,
based on our findings, we suggest improved strategies to guide the student
network to enhance its effectiveness. Additionally, the findings of this study
provide insightful suggestions to researchers and practitioners that commonly
use techniques from KD. Our code is available at
https://github.com/hchoi71/MIX-KD.",0.21610431,-0.20240381,0.11464678,A
13522,for further research.,"Therefore, video person search is desired                                  States Air Force.","A naive attempt is to combine the
state-of-the-art multi-object tracking (MOT) methods (e.g.,
FairMOT [31], and ByteMOT [30]) and video-based person
ReID networks.",2022-11-09 03:07:31+00:00,MEVID: Multi-view Extended Videos with Identities for Video Person Re-Identification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Daniel Davila'), arxiv.Result.Author('Dawei Du'), arxiv.Result.Author('Bryon Lewis'), arxiv.Result.Author('Christopher Funk'), arxiv.Result.Author('Joseph Van Pelt'), arxiv.Result.Author('Roderick Collins'), arxiv.Result.Author('Kellie Corona'), arxiv.Result.Author('Matt Brown'), arxiv.Result.Author('Scott McCloskey'), arxiv.Result.Author('Anthony Hoogs'), arxiv.Result.Author('Brian Clipp')]","In this paper, we present the Multi-view Extended Videos with Identities
(MEVID) dataset for large-scale, video person re-identification (ReID) in the
wild. To our knowledge, MEVID represents the most-varied video person ReID
dataset, spanning an extensive indoor and outdoor environment across nine
unique dates in a 73-day window, various camera viewpoints, and entity clothing
changes. Specifically, we label the identities of 158 unique people wearing 598
outfits taken from 8, 092 tracklets, average length of about 590 frames, seen
in 33 camera views from the very large-scale MEVA person activities dataset.
While other datasets have more unique identities, MEVID emphasizes a richer set
of information about each individual, such as: 4 outfits/identity vs. 2
outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5
simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID.
Being based on the MEVA video dataset, we also inherit data that is
intentionally demographically balanced to the continental United States. To
accelerate the annotation process, we developed a semi-automatic annotation
framework and GUI that combines state-of-the-art real-time models for object
detection, pose estimation, person ReID, and multi-object tracking. We evaluate
several state-of-the-art methods on MEVID challenge problems and
comprehensively quantify their robustness in terms of changes of outfit, scale,
and background location. Our quantitative analysis on the realistic, unique
aspects of MEVID shows that there are significant remaining challenges in video
person ReID and indicates important directions for future research.",-0.18967381,0.1009385,-0.11869991,B
13523,for further research.,"Therefore, video person search is desired                                  States Air Force.","A naive attempt is to combine the
state-of-the-art multi-object tracking (MOT) methods (e.g.,
FairMOT [31], and ByteMOT [30]) and video-based person
ReID networks.",2022-11-09 03:07:31+00:00,MEVID: Multi-view Extended Videos with Identities for Video Person Re-Identification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Daniel Davila'), arxiv.Result.Author('Dawei Du'), arxiv.Result.Author('Bryon Lewis'), arxiv.Result.Author('Christopher Funk'), arxiv.Result.Author('Joseph Van Pelt'), arxiv.Result.Author('Roderick Collins'), arxiv.Result.Author('Kellie Corona'), arxiv.Result.Author('Matt Brown'), arxiv.Result.Author('Scott McCloskey'), arxiv.Result.Author('Anthony Hoogs'), arxiv.Result.Author('Brian Clipp')]","In this paper, we present the Multi-view Extended Videos with Identities
(MEVID) dataset for large-scale, video person re-identification (ReID) in the
wild. To our knowledge, MEVID represents the most-varied video person ReID
dataset, spanning an extensive indoor and outdoor environment across nine
unique dates in a 73-day window, various camera viewpoints, and entity clothing
changes. Specifically, we label the identities of 158 unique people wearing 598
outfits taken from 8, 092 tracklets, average length of about 590 frames, seen
in 33 camera views from the very large-scale MEVA person activities dataset.
While other datasets have more unique identities, MEVID emphasizes a richer set
of information about each individual, such as: 4 outfits/identity vs. 2
outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5
simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID.
Being based on the MEVA video dataset, we also inherit data that is
intentionally demographically balanced to the continental United States. To
accelerate the annotation process, we developed a semi-automatic annotation
framework and GUI that combines state-of-the-art real-time models for object
detection, pose estimation, person ReID, and multi-object tracking. We evaluate
several state-of-the-art methods on MEVID challenge problems and
comprehensively quantify their robustness in terms of changes of outfit, scale,
and background location. Our quantitative analysis on the realistic, unique
aspects of MEVID shows that there are significant remaining challenges in video
person ReID and indicates important directions for future research.",-0.18967381,0.1009385,-0.11869991,B
13529,"Nevertheless, this industry has been         paper can motivate further research in this Ô¨Åeld by laying out a
                                       widely criticized for its large waste generation [3], because        series of arguments backed up by concrete results, while also
                                       of phenomena like overproduction and product returns [4].","We hope that this
                                       in the following years.","acting as a short survey on these particular tasks that can be
                                       The main reason for this can be tied back to customer                considered ‚Äùedge-cases‚Äù of forecasting in fashion.",2022-11-09 10:44:51+00:00,On the use of learning-based forecasting methods for ameliorating fashion business processes: A position paper,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Geri Skenderi'), arxiv.Result.Author('Christian Joppi'), arxiv.Result.Author('Matteo Denitto'), arxiv.Result.Author('Marco Cristani')]","The fashion industry is one of the most active and competitive markets in the
world, manufacturing millions of products and reaching large audiences every
year. A plethora of business processes are involved in this large-scale
industry, but due to the generally short life-cycle of clothing items,
supply-chain management and retailing strategies are crucial for good market
performance. Correctly understanding the wants and needs of clients, managing
logistic issues and marketing the correct products are high-level problems with
a lot of uncertainty associated to them given the number of influencing
factors, but most importantly due to the unpredictability often associated with
the future. It is therefore straightforward that forecasting methods, which
generate predictions of the future, are indispensable in order to ameliorate
all the various business processes that deal with the true purpose and meaning
of fashion: having a lot of people wear a particular product or style,
rendering these items, people and consequently brands fashionable. In this
paper, we provide an overview of three concrete forecasting tasks that any
fashion company can apply in order to improve their industrial and market
impact. We underline advances and issues in all three tasks and argue about
their importance and the impact they can have at an industrial level. Finally,
we highlight issues and directions of future work, reflecting on how
learning-based forecasting methods can further aid the fashion industry.",0.27399236,-0.068736464,-0.08174596,A
13530,"This is
   In [15], a variety of machine learning models are taken           an important aspect as it promotes further research on the
into consideration such as boosting algorithms (XGBoost,             methodological aspect of this challenging task, by relying on a
Gradient Boosted Trees) and Neural Networks (MLP, LSTM).","Finally, the
                                                                     authors render their dataset, named Visuelle, public.","dataset that is actually based on the real sales of a fast-fashion
The authors propose to simulate the expert judgment by               company.",2022-11-09 10:44:51+00:00,On the use of learning-based forecasting methods for ameliorating fashion business processes: A position paper,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Geri Skenderi'), arxiv.Result.Author('Christian Joppi'), arxiv.Result.Author('Matteo Denitto'), arxiv.Result.Author('Marco Cristani')]","The fashion industry is one of the most active and competitive markets in the
world, manufacturing millions of products and reaching large audiences every
year. A plethora of business processes are involved in this large-scale
industry, but due to the generally short life-cycle of clothing items,
supply-chain management and retailing strategies are crucial for good market
performance. Correctly understanding the wants and needs of clients, managing
logistic issues and marketing the correct products are high-level problems with
a lot of uncertainty associated to them given the number of influencing
factors, but most importantly due to the unpredictability often associated with
the future. It is therefore straightforward that forecasting methods, which
generate predictions of the future, are indispensable in order to ameliorate
all the various business processes that deal with the true purpose and meaning
of fashion: having a lot of people wear a particular product or style,
rendering these items, people and consequently brands fashionable. In this
paper, we provide an overview of three concrete forecasting tasks that any
fashion company can apply in order to improve their industrial and market
impact. We underline advances and issues in all three tasks and argue about
their importance and the impact they can have at an industrial level. Finally,
we highlight issues and directions of future work, reflecting on how
learning-based forecasting methods can further aid the fashion industry.",0.0108007165,-0.18030229,-0.063512,C
13538,"Experimental results show that            procedure of the human-annotated dataset, which
                                                              covers a wide variety of Wikipedia person entities
                                                              for further research.","Then we introduce the complete building
tions and entities.","Finally, an in-depth data anal-
                                                              ysis will be elaborated on in detail.",2022-11-09 13:27:50+00:00,Visual Named Entity Linking: A New Dataset and A Baseline,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Wenxiang Sun'), arxiv.Result.Author('Yixing Fan'), arxiv.Result.Author('Jiafeng Guo'), arxiv.Result.Author('Ruqing Zhang'), arxiv.Result.Author('Xueqi Cheng')]","Visual Entity Linking (VEL) is a task to link regions of images with their
corresponding entities in Knowledge Bases (KBs), which is beneficial for many
computer vision tasks such as image retrieval, image caption, and visual
question answering. While existing tasks in VEL either rely on textual data to
complement a multi-modal linking or only link objects with general entities,
which fails to perform named entity linking on large amounts of image data. In
this paper, we consider a purely Visual-based Named Entity Linking (VNEL) task,
where the input only consists of an image. The task is to identify objects of
interest (i.e., visual entity mentions) in images and link them to
corresponding named entities in KBs. Since each entity often contains rich
visual and textual information in KBs, we thus propose three different
sub-tasks, i.e., visual to visual entity linking (V2VEL), visual to textual
entity linking (V2TEL), and visual to visual-textual entity linking (V2VTEL).
In addition, we present a high-quality human-annotated visual person linking
dataset, named WIKIPerson. Based on WIKIPerson, we establish a series of
baseline algorithms for the solution of each sub-task, and conduct experiments
to verify the quality of proposed datasets and the effectiveness of baseline
methods. We envision this work to be helpful for soliciting more works
regarding VNEL in the future. The codes and datasets are publicly available at
https://github.com/ict-bigdatalab/VNEL.",0.049326405,-0.20658898,-0.37210774,C
13555,"However,
the promising performance demonstrated in other domains could encourage further research
and development in the SR arena.",Such self-supervised methods have seen limited use in the SR domain thus far.,"The Degradation-Aware SR (DASR) network [23] was one of the Ô¨Årst SR networks to
use contrastive learning for blind SR.",2022-11-09 16:49:35+00:00,The Best of Both Worlds: a Framework for Combining Degradation Prediction with High Performance Super-Resolution Networks,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Matthew Aquilina'), arxiv.Result.Author('Keith George Ciantar'), arxiv.Result.Author('Christian Galea'), arxiv.Result.Author('Kenneth P. Camilleri'), arxiv.Result.Author('Reuben A. Farrugia'), arxiv.Result.Author('John Abela')]","To date, the best-performing blind super-resolution (SR) techniques follow
one of two paradigms: A) generate and train a standard SR network on synthetic
low-resolution - high-resolution (LR - HR) pairs or B) attempt to predict the
degradations an LR image has suffered and use these to inform a customised SR
network. Despite significant progress, subscribers to the former miss out on
useful degradation information that could be used to improve the SR process. On
the other hand, followers of the latter rely on weaker SR networks, which are
significantly outperformed by the latest architectural advancements. In this
work, we present a framework for combining any blind SR prediction mechanism
with any deep SR network, using a metadata insertion block to insert prediction
vectors into SR network feature maps. Through comprehensive testing, we prove
that state-of-the-art contrastive and iterative prediction schemes can be
successfully combined with high-performance SR networks such as RCAN and HAN
within our framework. We show that our hybrid models consistently achieve
stronger SR performance than both their non-blind and blind counterparts.
Furthermore, we demonstrate our framework's robustness by predicting
degradations and super-resolving images from a complex pipeline of blurring,
noise and compression.",0.00021782145,-0.19716491,0.08564366,C
13569,"[5] already motivated that additional                 we state the conclusions and provide insights about possible
                                       inductive biases should be added to deep learning meth-                           improvements and further research directions.",Goyal et al.,"ods in order to go from a reasonably good in-distribution
                                       generalization in highly supervised learning tasks to strong                                               II.",2022-11-09 21:14:08+00:00,Affordance detection with Dynamic-Tree Capsule Networks,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Antonio Rodr√≠guez-S√°nchez'), arxiv.Result.Author('Simon Haller-Seeber'), arxiv.Result.Author('David Peer'), arxiv.Result.Author('Chris Engelhardt'), arxiv.Result.Author('Jakob Mittelberger'), arxiv.Result.Author('Matteo Saveriano')]","Affordance detection from visual input is a fundamental step in autonomous
robotic manipulation. Existing solutions to the problem of affordance detection
rely on convolutional neural networks. However, these networks do not consider
the spatial arrangement of the input data and miss parts-to-whole
relationships. Therefore, they fall short when confronted with novel,
previously unseen object instances or new viewpoints. One solution to overcome
such limitations can be to resort to capsule networks. In this paper, we
introduce the first affordance detection network based on dynamic
tree-structured capsules for sparse 3D point clouds. We show that our
capsule-based network outperforms current state-of-the-art models on viewpoint
invariance and parts-segmentation of new object instances through a novel
dataset we only used for evaluation and it is publicly available from
github.com/gipfelen/DTCG-Net. In the experimental evaluation we will show that
our algorithm is superior to current affordance detection methods when faced
with grasping previously unseen objects thanks to our Capsule Network enforcing
a parts-to-whole representation.",0.009761843,-0.3950448,0.17622182,C
13572,"The cross-category tasks are challenging,

   We showcase the usefulness of part pose by performing           and further research is still needed for better generalizabil-
cross-category part-based object manipulation on four basic        ity.",tion                                                            Limitations.,"For object manipulation, our heuristics method is de-
tasks.",2022-11-10 00:30:22+00:00,GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haoran Geng'), arxiv.Result.Author('Helin Xu'), arxiv.Result.Author('Chengyang Zhao'), arxiv.Result.Author('Chao Xu'), arxiv.Result.Author('Li Yi'), arxiv.Result.Author('Siyuan Huang'), arxiv.Result.Author('He Wang')]","Perceiving and manipulating objects in a generalizable way has been actively
studied by the computer vision and robotics communities, where cross-category
generalizable manipulation skills are highly desired yet underexplored. In this
work, we propose to learn such generalizable perception and manipulation via
Generalizable and Actionable Parts (GAParts). By identifying and defining 9
GAPart classes (e.g. buttons, handles, etc), we show that our part-centric
approach allows our method to learn object perception and manipulation skills
from seen object categories and directly generalize to unseen categories.
Following the GAPart definition, we construct a large-scale part-centric
interactive dataset, GAPartNet, where rich, part-level annotations (semantics,
poses) are provided for 1166 objects and 8489 part instances. Based on
GAPartNet, we investigate three cross-category tasks: part segmentation, part
pose estimation, and part-based object manipulation. Given the large domain
gaps between seen and unseen object categories, we propose a strong 3D
segmentation method from the perspective of domain generalization by
integrating adversarial learning techniques. Our method outperforms all
existing methods by a large margin, no matter on seen or unseen categories.
Furthermore, with part segmentation and pose estimation results, we leverage
the GAPart pose definition to design part-based manipulation heuristics that
can generalize well to unseen object categories in both simulation and real
world. The dataset and code will be released.",-0.13866456,0.06132716,-0.19132318,B
13575,"3.2 Semantically Distinct Augmentation

The performance diÔ¨Äerence versus training stages caused by the aforementioned
randomness between SCR and SCL motivates us to conduct further research on
the impact of the random transformations upon the representation learning.","Please refer to Appendix A.3.2 for
more details.","As
found by the work of SimCLR [Chen et al.",2022-11-10 05:29:43+00:00,Mitigating Forgetting in Online Continual Learning via Contrasting Semantically Distinct Augmentations,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sheng-Feng Yu'), arxiv.Result.Author('Wei-Chen Chiu')]","Online continual learning (OCL) aims to enable model learning from a
non-stationary data stream to continuously acquire new knowledge as well as
retain the learnt one, under the constraints of having limited system size and
computational cost, in which the main challenge comes from the ""catastrophic
forgetting"" issue -- the inability to well remember the learnt knowledge while
learning the new ones. With the specific focus on the class-incremental OCL
scenario, i.e. OCL for classification, the recent advance incorporates the
contrastive learning technique for learning more generalised feature
representation to achieve the state-of-the-art performance but is still unable
to fully resolve the catastrophic forgetting. In this paper, we follow the
strategy of adopting contrastive learning but further introduce the
semantically distinct augmentation technique, in which it leverages strong
augmentation to generate more data samples, and we show that considering these
samples semantically different from their original classes (thus being related
to the out-of-distribution samples) in the contrastive learning mechanism
contributes to alleviate forgetting and facilitate model stability. Moreover,
in addition to contrastive learning, the typical classification mechanism and
objective (i.e. softmax classifier and cross-entropy loss) are included in our
model design for faster convergence and utilising the label information, but
particularly equipped with a sampling strategy to tackle the tendency of
favouring the new classes (i.e. model bias towards the recently learnt
classes). Upon conducting extensive experiments on CIFAR-10, CIFAR-100, and
Mini-Imagenet datasets, our proposed method is shown to achieve superior
performance against various baselines.",0.21978293,-0.20973457,-0.044528082,A
13600,"two variants to the most related work and showing an im-
We will make our implementation publicly available to fa-                                                                             provement in fps of up to 300% and a reduction in latency
cilitate further research.","Moreover,                                                                               marizes our main results on the possible trade-offs between
our preliminary attempt of using manual locks instead of                                                                              accuracy vs. latency and accuracy vs. fps, comparing our
queues was more complex without improving performance.",of 70%.,2022-11-10 18:58:00+00:00,Scaling Neural Face Synthesis to High FPS and Low Latency by Neural Caching,cs.CV,['cs.CV'],"[arxiv.Result.Author('Frank Yu'), arxiv.Result.Author('Sid Fels'), arxiv.Result.Author('Helge Rhodin')]","Recent neural rendering approaches greatly improve image quality, reaching
near photorealism. However, the underlying neural networks have high runtime,
precluding telepresence and virtual reality applications that require high
resolution at low latency. The sequential dependency of layers in deep networks
makes their optimization difficult. We break this dependency by caching
information from the previous frame to speed up the processing of the current
one with an implicit warp. The warping with a shallow network reduces latency
and the caching operations can further be parallelized to improve the frame
rate. In contrast to existing temporal neural networks, ours is tailored for
the task of rendering novel views of faces by conditioning on the change of the
underlying surface mesh. We test the approach on view-dependent rendering of 3D
portrait avatars, as needed for telepresence, on established benchmark
sequences. Warping reduces latency by 70$\%$ (from 49.4ms to 14.9ms on
commodity GPUs) and scales frame rates accordingly over multiple GPUs while
reducing image quality by only 1$\%$, making it suitable as part of end-to-end
view-dependent 3D teleconferencing applications. Our project page can be found
at: https://yu-frank.github.io/lowlatency/.",0.27644104,0.15724215,0.0040869517,A
13601,"We believe that our work may serve as a fruitful
           GMDepth        0.050    -    0.491    -                basis for further research in this area.","Besides, to
Scenes11   DeMoN [37]     0.556  3.402  2.603  0.391              train such a uniÔ¨Åed model, one could also explore recent
           DeepMVS [104]  0.210  0.373  0.891  0.270              unsupervised pretraining approaches (e.g., masked autoen-
           DPSNet [63]    0.056  0.144  0.714  0.140              coders [105]) to learn general feature representations for
           IIB [29]       0.056         0.523                     matching.","0.069         0.106
                                                                  6 CONCLUSION
TABLE 17: Depth performance on RGBD-SLAM, SUN3D
and Scenes11 test datasets.",2022-11-10 18:59:54+00:00,"Unifying Flow, Stereo and Depth Estimation",cs.CV,['cs.CV'],"[arxiv.Result.Author('Haofei Xu'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Jianfei Cai'), arxiv.Result.Author('Hamid Rezatofighi'), arxiv.Result.Author('Fisher Yu'), arxiv.Result.Author('Dacheng Tao'), arxiv.Result.Author('Andreas Geiger')]","We present a unified formulation and model for three motion and 3D perception
tasks: optical flow, rectified stereo matching and unrectified stereo depth
estimation from posed images. Unlike previous specialized architectures for
each specific task, we formulate all three tasks as a unified dense
correspondence matching problem, which can be solved with a single model by
directly comparing feature similarities. Such a formulation calls for
discriminative feature representations, which we achieve using a Transformer,
in particular the cross-attention mechanism. We demonstrate that
cross-attention enables integration of knowledge from another image via
cross-view interactions, which greatly improves the quality of the extracted
features. Our unified model naturally enables cross-task transfer since the
model architecture and parameters are shared across tasks. We outperform RAFT
with our unified model on the challenging Sintel dataset, and our final model
that uses a few additional task-specific refinement steps outperforms or
compares favorably to recent state-of-the-art methods on 10 popular flow,
stereo and depth datasets, while being simpler and more efficient in terms of
model design and inference speed.",-0.19116914,-0.11971609,0.18439394,C
13627,"A further study on pseudo-labels can
be found in Supplementary Section D.

5 Conclusion

In this paper, we have presented a novel active learning strategy for 3D LiDAR
semantic segmentation, named LiDAL.","We speculate that adding pseudo-labels with a reasonable number
will improve the network performance but superfluous pseudo-labels may bring
unhelpful training biases and label noises.","Aiming at exploiting the inter-frame con-
straints embedded in LiDAR sequences, we propose two uncertainty measures
estimating the inconsistencies of network predictions among frames.",2022-11-11 04:47:33+00:00,LiDAL: Inter-frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zeyu Hu'), arxiv.Result.Author('Xuyang Bai'), arxiv.Result.Author('Runze Zhang'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Guangyuan Sun'), arxiv.Result.Author('Hongbo Fu'), arxiv.Result.Author('Chiew-Lan Tai')]","We propose LiDAL, a novel active learning method for 3D LiDAR semantic
segmentation by exploiting inter-frame uncertainty among LiDAR frames. Our core
idea is that a well-trained model should generate robust results irrespective
of viewpoints for scene scanning and thus the inconsistencies in model
predictions across frames provide a very reliable measure of uncertainty for
active sample selection. To implement this uncertainty measure, we introduce
new inter-frame divergence and entropy formulations, which serve as the metrics
for active selection. Moreover, we demonstrate additional performance gains by
predicting and incorporating pseudo-labels, which are also selected using the
proposed inter-frame uncertainty measure. Experimental results validate the
effectiveness of LiDAL: we achieve 95% of the performance of fully supervised
learning with less than 5% of annotations on the SemanticKITTI and nuScenes
datasets, outperforming state-of-the-art active learning methods. Code release:
https://github.com/hzykent/LiDAL.",-0.21568713,0.00081473123,0.046258766,B
13631,"We
I3D [3] features4                                                       hope that our results spark further research on efÔ¨Åcient and
                                                                        effective methods of using the pretrained snippet encoder for
   Table 4 describes the result of ablation studies on our              long-form video understanding.","However, to ensure our SoLa strategy‚Äôs general                 gains in downstream tasks, outperforming most of the re-
applicability, we also conducted experiments on RGB only                cent works that involve retraining the snippet encoder.",SoLa strategy.,2022-11-11 06:27:22+00:00,Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hyolim Kang'), arxiv.Result.Author('Hanjung Kim'), arxiv.Result.Author('Joungbin An'), arxiv.Result.Author('Minsu Cho'), arxiv.Result.Author('Seon Joo Kim')]","Temporal Action Localization (TAL) methods typically operate on top of
feature sequences from a frozen snippet encoder that is pretrained with the
Trimmed Action Classification (TAC) tasks, resulting in a task discrepancy
problem. While existing TAL methods mitigate this issue either by retraining
the encoder with a pretext task or by end-to-end fine-tuning, they commonly
require an overload of high memory and computation. In this work, we introduce
Soft-Landing (SoLa) strategy, an efficient yet effective framework to bridge
the transferability gap between the pretrained encoder and the downstream tasks
by incorporating a light-weight neural network, i.e., a SoLa module, on top of
the frozen encoder. We also propose an unsupervised training scheme for the
SoLa module; it learns with inter-frame Similarity Matching that uses the frame
interval as its supervisory signal, eliminating the need for temporal
annotations. Experimental evaluation on various benchmarks for downstream TAL
tasks shows that our method effectively alleviates the task discrepancy problem
with remarkable computational efficiency.",-0.018141255,0.009257996,0.04474236,C
13643,"To that             port further research, we open-source our code and models
                                        end, we propose OneFormer, a universal image segmenta-               at https://github.com/SHI-Labs/OneFormer.","To sup-
                                        mance across all three image segmentation tasks.","tion framework that uniÔ¨Åes segmentation with a multi-task
                                        train-once design.",2022-11-10 18:56:04+00:00,OneFormer: One Transformer to Rule Universal Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jitesh Jain'), arxiv.Result.Author('Jiachen Li'), arxiv.Result.Author('MangTik Chiu'), arxiv.Result.Author('Ali Hassani'), arxiv.Result.Author('Nikita Orlov'), arxiv.Result.Author('Humphrey Shi')]","Universal Image Segmentation is not a new concept. Past attempts to unify
image segmentation in the last decades include scene parsing, panoptic
segmentation, and, more recently, new panoptic architectures. However, such
panoptic architectures do not truly unify image segmentation because they need
to be trained individually on the semantic, instance, or panoptic segmentation
to achieve the best performance. Ideally, a truly universal framework should be
trained only once and achieve SOTA performance across all three image
segmentation tasks. To that end, we propose OneFormer, a universal image
segmentation framework that unifies segmentation with a multi-task train-once
design. We first propose a task-conditioned joint training strategy that
enables training on ground truths of each domain (semantic, instance, and
panoptic segmentation) within a single multi-task training process. Secondly,
we introduce a task token to condition our model on the task at hand, making
our model task-dynamic to support multi-task training and inference. Thirdly,
we propose using a query-text contrastive loss during training to establish
better inter-task and inter-class distinctions. Notably, our single OneFormer
model outperforms specialized Mask2Former models across all three segmentation
tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each
of the three tasks individually with three times the resources. With new
ConvNeXt and DiNAT backbones, we observe even more performance improvement. We
believe OneFormer is a significant step towards making image segmentation more
universal and accessible. To support further research, we open-source our code
and models at https://github.com/SHI-Labs/OneFormer",-0.26321036,0.014420116,0.058304522,B
13644,"We believe OneFormer is a signiÔ¨Åcant step        [10] Bowen Cheng, Liang-Chieh Chen, Yunchao Wei,
towards making image segmentation more universal and ac-                Yukun Zhu, Zilong Huang, Jinjun Xiong, Thomas S
cessible and will support further research in this direction            Huang, Wen-Mei Hwu, and Honghui Shi.","14, 15
requirements down to a third, making image segmentation
more accessible.","Spgnet:
by open-sourcing our codes and models.",2022-11-10 18:56:04+00:00,OneFormer: One Transformer to Rule Universal Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jitesh Jain'), arxiv.Result.Author('Jiachen Li'), arxiv.Result.Author('MangTik Chiu'), arxiv.Result.Author('Ali Hassani'), arxiv.Result.Author('Nikita Orlov'), arxiv.Result.Author('Humphrey Shi')]","Universal Image Segmentation is not a new concept. Past attempts to unify
image segmentation in the last decades include scene parsing, panoptic
segmentation, and, more recently, new panoptic architectures. However, such
panoptic architectures do not truly unify image segmentation because they need
to be trained individually on the semantic, instance, or panoptic segmentation
to achieve the best performance. Ideally, a truly universal framework should be
trained only once and achieve SOTA performance across all three image
segmentation tasks. To that end, we propose OneFormer, a universal image
segmentation framework that unifies segmentation with a multi-task train-once
design. We first propose a task-conditioned joint training strategy that
enables training on ground truths of each domain (semantic, instance, and
panoptic segmentation) within a single multi-task training process. Secondly,
we introduce a task token to condition our model on the task at hand, making
our model task-dynamic to support multi-task training and inference. Thirdly,
we propose using a query-text contrastive loss during training to establish
better inter-task and inter-class distinctions. Notably, our single OneFormer
model outperforms specialized Mask2Former models across all three segmentation
tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each
of the three tasks individually with three times the resources. With new
ConvNeXt and DiNAT backbones, we observe even more performance improvement. We
believe OneFormer is a significant step towards making image segmentation more
universal and accessible. To support further research, we open-source our code
and models at https://github.com/SHI-Labs/OneFormer",-0.08179872,0.07313755,0.042799067,C
13658,the method in a distributed environment seeks further study.,"However, the effectiveness of        sonalization.","With extensive experiments, we evaluate the effectiveness
   Applying contrastive learning in a distributed system with     of FedStyle in terms of generalization and personalization
stylized local data poses two core challenges.",2022-11-11 20:14:26+00:00,More Generalized and Personalized Unsupervised Representation Learning In A Distributed System,cs.CV,"['cs.CV', 'cs.DC']","[arxiv.Result.Author('Yuewei Yang'), arxiv.Result.Author('Jingwei Sun'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Hai Li'), arxiv.Result.Author('Yiran Chen')]","Discriminative unsupervised learning methods such as contrastive learning
have demonstrated the ability to learn generalized visual representations on
centralized data. It is nonetheless challenging to adapt such methods to a
distributed system with unlabeled, private, and heterogeneous client data due
to user styles and preferences. Federated learning enables multiple clients to
collectively learn a global model without provoking any privacy breach between
local clients. On the other hand, another direction of federated learning
studies personalized methods to address the local heterogeneity. However, work
on solving both generalization and personalization without labels in a
decentralized setting remains unfamiliar. In this work, we propose a novel
method, FedStyle, to learn a more generalized global model by infusing local
style information with local content information for contrastive learning, and
to learn more personalized local models by inducing local style information for
downstream tasks. The style information is extracted by contrasting original
local data with strongly augmented local data (Sobel filtered images). Through
extensive experiments with linear evaluations in both IID and non-IID settings,
we demonstrate that FedStyle outperforms both the generalization baseline
methods and personalization baseline methods in a stylized decentralized
setting. Through comprehensive ablations, we demonstrate our design of style
infusion and stylized personalization improve performance significantly.",0.06346575,-0.20983912,-0.00028709148,C
13661,Since the ablative experiments involved by                                       a new baseline for further research.,"No bells or
The ablation study is considered the primary measure that                                      whistles, the obtained model should be structurally simple,
can declare the effectiveness of each component in the pro-                                    experimentally powerful, and empirically robust to serve as
posed method.","To this end, we build a
prior works are mostly only conducted on CASIA-B [36],                                         silhouette-based model, i.e., GaitBase.",2022-11-12 07:24:29+00:00,OpenGait: Revisiting Gait Recognition Toward Better Practicality,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Fan'), arxiv.Result.Author('Junhao Liang'), arxiv.Result.Author('Chuanfu Shen'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Yongzhen Huang'), arxiv.Result.Author('Shiqi Yu')]","Gait recognition is one of the most important long-distance identification
technologies and increasingly gains popularity in both research and industry
communities. Although significant progress has been made in indoor datasets,
much evidence shows that gait recognition techniques perform poorly in the
wild. More importantly, we also find that many conclusions from prior works
change with the evaluation datasets. Therefore, the more critical goal of this
paper is to present a comprehensive benchmark study for better practicality
rather than only a particular model for better performance. To this end, we
first develop a flexible and efficient gait recognition codebase named
OpenGait. Based on OpenGait, we deeply revisit the recent development of gait
recognition by re-conducting the ablative experiments. Encouragingly, we find
many hidden troubles of prior works and new insights for future research.
Inspired by these discoveries, we develop a structurally simple, empirically
powerful and practically robust baseline model, GaitBase. Experimentally, we
comprehensively compare GaitBase with many current gait recognition methods on
multiple public datasets, and the results reflect that GaitBase achieves
significantly strong performance in most cases regardless of indoor or outdoor
situations. The source code is available at
\url{https://github.com/ShiqiYu/OpenGait}.",0.32779914,0.2166053,-0.21652839,A
13662,Since the ablative experiments involved by                                       a new baseline for further research.,"No bells or
The ablation study is considered the primary measure that                                      whistles, the obtained model should be structurally simple,
can declare the effectiveness of each component in the pro-                                    experimentally powerful, and empirically robust to serve as
posed method.","To this end, we build a
prior works are mostly only conducted on CASIA-B [37],                                         silhouette-based model, i.e., GaitBase.",2022-11-12 07:24:29+00:00,OpenGait: Revisiting Gait Recognition Toward Better Practicality,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chao Fan'), arxiv.Result.Author('Junhao Liang'), arxiv.Result.Author('Chuanfu Shen'), arxiv.Result.Author('Saihui Hou'), arxiv.Result.Author('Yongzhen Huang'), arxiv.Result.Author('Shiqi Yu')]","Gait recognition is one of the most important long-distance identification
technologies and increasingly gains popularity in both research and industry
communities. Although significant progress has been made in indoor datasets,
much evidence shows that gait recognition techniques perform poorly in the
wild. More importantly, we also find that many conclusions from prior works
change with the evaluation datasets. Therefore, the more critical goal of this
paper is to present a comprehensive benchmark study for better practicality
rather than only a particular model for better performance. To this end, we
first develop a flexible and efficient gait recognition codebase named
OpenGait. Based on OpenGait, we deeply revisit the recent development of gait
recognition by re-conducting the ablative experiments. Encouragingly, we find
many hidden troubles of prior works and new insights for future research.
Inspired by these discoveries, we develop a structurally simple, empirically
powerful and practically robust baseline model, GaitBase. Experimentally, we
comprehensively compare GaitBase with many current gait recognition methods on
multiple public datasets, and the results reflect that GaitBase achieves
significantly strong performance in most cases regardless of indoor or outdoor
situations. The source code is available at
\url{https://github.com/ShiqiYu/OpenGait}.",0.3277771,0.21748924,-0.21743679,A
13672,Discussion                                                          developed with further research on this aspect.,"However, we believe that this is still
                                                                         not the optimal solution, and a more principled strategy can be
4.6.","The experimental results presented in the paper demonstrate          References
the efÔ¨Åcacy of MixBin.",2022-11-12 20:30:38+00:00,MixBin: Towards Budgeted Binarization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Udbhav Bamba'), arxiv.Result.Author('Neeraj Anand'), arxiv.Result.Author('Dilip K. Prasad'), arxiv.Result.Author('Deepak K. Gupta')]","Binarization has proven to be amongst the most effective ways of neural
network compression, reducing the FLOPs of the original model by a large
extent. However, such levels of compression are often accompanied by a
significant drop in the performance. There exist some approaches that reduce
this performance drop by facilitating partial binarization of the network,
however, a systematic approach to mix binary and full-precision parameters in a
single network is still missing. In this paper, we propose a paradigm to
perform partial binarization of neural networks in a controlled sense, thereby
constructing budgeted binary neural network (B2NN). We present MixBin, an
iterative search-based strategy that constructs B2NN through optimized mixing
of the binary and full-precision components. MixBin allows to explicitly choose
the approximate fraction of the network to be kept as binary, thereby
presenting the flexibility to adapt the inference cost at a prescribed budget.
We demonstrate through experiments that B2NNs obtained from our MixBin strategy
are significantly better than those obtained from random selection of the
network layers. To perform partial binarization in an effective manner, it is
important that both the full-precision as well as the binary components of the
B2NN are appropriately optimized. We also demonstrate that the choice of the
activation function can have a significant effect on this process, and to
circumvent this issue, we present BinReLU, that can be used as an effective
activation function for the full-precision as well as the binary components of
any B2NN. Experimental investigations reveal that BinReLU outperforms the other
activation functions in all possible scenarios of B2NN: zero-, partial- as well
as full binarization. Finally, we demonstrate the efficacy of MixBin on the
tasks of classification and object tracking using benchmark datasets.",0.35925436,0.18468294,0.04840943,A
13675,"We further study the impact of
our LRG, AMA and DKD modules in Table 6.","                                                                                      66.14
   Effect of components.","We see                                                                                                                                      66.88
that when the modules are individually added to synthesize
the fake images, accuracies are always increased.",2022-11-13 04:43:52+00:00,Long-Range Zero-Shot Generative Deep Network Quantization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yan Luo'), arxiv.Result.Author('Yangcheng Gao'), arxiv.Result.Author('Zhao Zhang'), arxiv.Result.Author('Haijun Zhang'), arxiv.Result.Author('Mingliang Xu'), arxiv.Result.Author('Meng Wang')]","Quantization approximates a deep network model with floating-point numbers by
the one with low bit width numbers, in order to accelerate inference and reduce
computation. Quantizing a model without access to the original data, zero-shot
quantization can be accomplished by fitting the real data distribution by data
synthesis. However, zero-shot quantization achieves inferior performance
compared to the post-training quantization with real data. We find it is
because: 1) a normal generator is hard to obtain high diversity of synthetic
data, since it lacks long-range information to allocate attention to global
features; 2) the synthetic images aim to simulate the statistics of real data,
which leads to weak intra-class heterogeneity and limited feature richness. To
overcome these problems, we propose a novel deep network quantizer, dubbed
Long-Range Zero-Shot Generative Deep Network Quantization (LRQ). Technically,
we propose a long-range generator to learn long-range information instead of
simple local features. In order for the synthetic data to contain more global
features, long-range attention using large kernel convolution is incorporated
into the generator. In addition, we also present an Adversarial Margin Add
(AMA) module to force intra-class angular enlargement between feature vector
and class center. As AMA increases the convergence difficulty of the loss
function, which is opposite to the training objective of the original loss
function, it forms an adversarial process. Furthermore, in order to transfer
knowledge from the full-precision network, we also utilize a decoupled
knowledge distillation. Extensive experiments demonstrate that LRQ obtains
better performance than other competitors.",0.25122815,0.03829761,0.106396206,A
13714,"ABLATION STUDY RESULTS ON INDIAN PINES DATASET
                                                                   In the future, further research will be implemented on the
Class  BLGCN without GCN  BLGCN without GAN     BLGCN           various application Ô¨Åeld of Bayesian layer, including image
                                                                classiÔ¨Åcation and graph node prediction.","tion, ablation studies are arranged to verify the contribution of
                                                                different modules including graph convolution operation, data
                                              TABLE IX:         generation module and dynamic control strategy.","Moreover, we will
   1          0.00¬±0.00          0.00¬±0.00   92.30¬±3.63         continue to improve the theoretical basis of Bayesian layer
   2         72.82¬±18.21        94.05¬±7.43   96.80¬±1.19         and make it available to various neural networks.",2022-11-14 12:56:56+00:00,Bayesian Layer Graph Convolutioanl Network for Hyperspetral Image Classification,cs.CV,"['cs.CV', 'cs.NI']","[arxiv.Result.Author('Mingyang Zhang'), arxiv.Result.Author('Ziqi Di'), arxiv.Result.Author('Maoguo Gong'), arxiv.Result.Author('Yue Wu'), arxiv.Result.Author('Hao Li'), arxiv.Result.Author('Xiangming Jiang')]","In recent years, research on hyperspectral image (HSI) classification has
continuous progress on introducing deep network models, and recently the graph
convolutional network (GCN) based models have shown impressive performance.
However, these deep learning frameworks based on point estimation suffer from
low generalization and inability to quantify the classification results
uncertainty. On the other hand, simply applying the Bayesian Neural Network
(BNN) based on distribution estimation to classify the HSI is unable to achieve
high classification accuracy due to the large amount of parameters. In this
paper, we design a Bayesian layer with Bayesian idea as an insertion layer into
point estimation based neural networks, and propose a Bayesian Layer Graph
Convolutional Network (BLGCN) model by combining graph convolution operations,
which can effectively extract graph information and estimate the uncertainty of
classification results. Moreover, a Generative Adversarial Network (GAN) is
built to solve the sample imbalance problem of HSI dataset. Finally, we design
a dynamic control training strategy based on the confidence interval of the
classification results, which will terminate the training early when the
confidence interval reaches the preseted threshold. The experimental results
show that our model achieves a balance between high classification accuracy and
strong generalization. In addition, it can quantifies the uncertainty of the
classification results.",0.14902502,0.007798068,0.13644272,A
13744,"Furthermore, our novel approach and its
                                                                                                      ease-of-use nature would encourage further research toward
                                                                                                      effective text-guided shape generation.","Additionally, similar to most works that em-                                            a compact and effective latent representation, and advance
                                                                                                      the use of neural models that operate in latent space rather
                                                                                                      than pixel space.",6.1.,2022-11-14 18:25:24+00:00,Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Gal Metzer'), arxiv.Result.Author('Elad Richardson'), arxiv.Result.Author('Or Patashnik'), arxiv.Result.Author('Raja Giryes'), arxiv.Result.Author('Daniel Cohen-Or')]","Text-guided image generation has progressed rapidly in recent years,
inspiring major breakthroughs in text-guided shape generation. Recently, it has
been shown that using score distillation, one can successfully text-guide a
NeRF model to generate a 3D object. We adapt the score distillation to the
publicly available, and computationally efficient, Latent Diffusion Models,
which apply the entire diffusion process in a compact latent space of a
pretrained autoencoder. As NeRFs operate in image space, a naive solution for
guiding them with latent score distillation would require encoding to the
latent space at each guidance step. Instead, we propose to bring the NeRF to
the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we
show that while Text-to-3D models can generate impressive results, they are
inherently unconstrained and may lack the ability to guide or enforce a
specific 3D structure. To assist and direct the 3D generation, we propose to
guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines
the coarse structure of the desired object. Then, we present means to integrate
such a constraint directly into a Latent-NeRF. This unique combination of text
and shape guidance allows for increased control over the generation process. We
also show that latent score distillation can be successfully applied directly
on 3D meshes. This allows for generating high-quality textures on a given
geometry. Our experiments validate the power of our different forms of guidance
and the efficiency of using latent rendering. Implementation is available at
https://github.com/eladrich/latent-nerf",-0.10599765,-0.12169659,-0.04441766,C
13755,"2.7 where the expectation of the Dirichlet distribution relates to
aleatoric uncertainty and its criterion concerning its dispersion can measure the the amount of evidence
in a prediction, hence epistemic uncertainty

Training Objective The ENN training is formulated as a variational approximation to minimize the
KL divergence between the distribution qŒ∏(œÄ|x) and the true posterior distribution p(œÄ|x, y):

                               Lvar(Œ∏; D) = E(x,y)‚àºP (X,Y )[Ô∏ÅKL(Ô∏ÅqŒ∏(œÄ|x) ‚à• p(œÄ|x, y))Ô∏Å]Ô∏Å                               (2.17)

The training objective and its derivation are further study in Chapter 5.","Evidential models provide a second-order uncertainty
representation as shown in Fig.","2.2.4 Uncertainty measures

In regression, while the predictive distribution p(y|x, D) remains intractable (Eq.",2022-11-14 22:07:11+00:00,Robust Deep Learning for Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']",[arxiv.Result.Author('Charles Corbi√®re')],"The last decade's research in artificial intelligence had a significant
impact on the advance of autonomous driving. Yet, safety remains a major
concern when it comes to deploying such systems in high-risk environments. The
objective of this thesis is to develop methodological tools which provide
reliable uncertainty estimates for deep neural networks. First, we introduce a
new criterion to reliably estimate model confidence: the true class probability
(TCP). We show that TCP offers better properties for failure prediction than
current uncertainty measures. Since the true class is by essence unknown at
test time, we propose to learn TCP criterion from data with an auxiliary model,
introducing a specific learning scheme adapted to this context. The relevance
of the proposed approach is validated on image classification and semantic
segmentation datasets. Then, we extend our learned confidence approach to the
task of domain adaptation where it improves the selection of pseudo-labels in
self-training methods. Finally, we tackle the challenge of jointly detecting
misclassification and out-of-distributions samples by introducing a new
uncertainty measure based on evidential models and defined on the simplex.",0.17421499,-0.07339465,0.0046914686,A
13756,"Along with results on Cityscapes, we further study domain adaptation
on another target dataset, namely Mapillary Vistas.",SYNTHIA ‚ñ∑ Mapillary.,"Table 4.4 presents semantic segmentation
performance using SYNTHIA as source dataset.",2022-11-14 22:07:11+00:00,Robust Deep Learning for Autonomous Driving,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']",[arxiv.Result.Author('Charles Corbi√®re')],"The last decade's research in artificial intelligence had a significant
impact on the advance of autonomous driving. Yet, safety remains a major
concern when it comes to deploying such systems in high-risk environments. The
objective of this thesis is to develop methodological tools which provide
reliable uncertainty estimates for deep neural networks. First, we introduce a
new criterion to reliably estimate model confidence: the true class probability
(TCP). We show that TCP offers better properties for failure prediction than
current uncertainty measures. Since the true class is by essence unknown at
test time, we propose to learn TCP criterion from data with an auxiliary model,
introducing a specific learning scheme adapted to this context. The relevance
of the proposed approach is validated on image classification and semantic
segmentation datasets. Then, we extend our learned confidence approach to the
task of domain adaptation where it improves the selection of pseudo-labels in
self-training methods. Finally, we tackle the challenge of jointly detecting
misclassification and out-of-distributions samples by introducing a new
uncertainty measure based on evidential models and defined on the simplex.",-0.17841335,-0.091417074,-0.004493068,C
13762,set will be released for further research.,Best viewed in color.,Pascal VOC 2007          labels to create the setting of partial positive label learning.,2022-11-15 02:11:20+00:00,Category-Adaptive Label Discovery and Noise Rejection for Multi-label Image Recognition with Partial Positive Labels,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tao Pu'), arxiv.Result.Author('Qianru Lao'), arxiv.Result.Author('Hefeng Wu'), arxiv.Result.Author('Tianshui Chen'), arxiv.Result.Author('Liang Lin')]","As a promising solution of reducing annotation cost, training multi-label
models with partial positive labels (MLR-PPL), in which merely few positive
labels are known while other are missing, attracts increasing attention. Due to
the absence of any negative labels, previous works regard unknown labels as
negative and adopt traditional MLR algorithms. To reject noisy labels, recent
works regard large loss samples as noise but ignore the semantic correlation
different multi-label images. In this work, we propose to explore semantic
correlation among different images to facilitate the MLR-PPL task.
Specifically, we design a unified framework, Category-Adaptive Label Discovery
and Noise Rejection, that discovers unknown labels and rejects noisy labels for
each category in an adaptive manner. The framework consists of two
complementary modules: (1) Category-Adaptive Label Discovery module first
measures the semantic similarity between positive samples and then complement
unknown labels with high similarities; (2) Category-Adaptive Noise Rejection
module first computes the sample weights based on semantic similarities from
different samples and then discards noisy labels with low weights. Besides, we
propose a novel category-adaptive threshold updating that adaptively adjusts
the threshold, to avoid the time-consuming manual tuning process. Extensive
experiments demonstrate that our proposed method consistently outperforms
current leading algorithms.",0.10122113,-0.091462575,-0.065609396,A
13763,"Many psychophysical experiments             networks and remain open to further study [40, 47, 58].","Psychophysical measurements of human              Other research has also suggested that Vision Transformers‚Äô
behavior represent a richer source of information for super-        learned representations are diÔ¨Äerent than convolutional neural
vised machine learning.","collect human perceptual responses through carefully de-
signed experiments in response to varying stimuli.",2022-11-15 04:18:43+00:00,Using Human Perception to Regularize Transfer Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Justin Dulay'), arxiv.Result.Author('Walter J. Scheirer')]","Recent trends in the machine learning community show that models with
fidelity toward human perceptual measurements perform strongly on vision tasks.
Likewise, human behavioral measurements have been used to regularize model
performance. But can we transfer latent knowledge gained from this across
different learning objectives? In this work, we introduce PERCEP-TL (Perceptual
Transfer Learning), a methodology for improving transfer learning with the
regularization power of psychophysical labels in models. We demonstrate which
models are affected the most by perceptual transfer learning and find that
models with high behavioral fidelity -- including vision transformers --
improve the most from this regularization by as much as 1.9\% Top@1 accuracy
points. These findings suggest that biologically inspired learning agents can
benefit from human behavioral measurements as regularizers and psychophysical
learned representations can be transferred to independent evaluation tasks.",-0.08074347,-0.15987094,0.06755213,C
13764,"PERCEP-TL performs better than tra-
                                                                            ditional transfer learning, and we hope this work inspires
   Brain-Score is an eÔ¨Äective metric for computing corre-                   further research into psychophysical research in the computer
lations among activations in biologically-inspired neural                   vision community.",Brain-Score Evaluation                                                 learned models.,"models by comparing them to neural activations in biological
units, but it does not account for behavioral diÔ¨Äerences.",2022-11-15 04:18:43+00:00,Using Human Perception to Regularize Transfer Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Justin Dulay'), arxiv.Result.Author('Walter J. Scheirer')]","Recent trends in the machine learning community show that models with
fidelity toward human perceptual measurements perform strongly on vision tasks.
Likewise, human behavioral measurements have been used to regularize model
performance. But can we transfer latent knowledge gained from this across
different learning objectives? In this work, we introduce PERCEP-TL (Perceptual
Transfer Learning), a methodology for improving transfer learning with the
regularization power of psychophysical labels in models. We demonstrate which
models are affected the most by perceptual transfer learning and find that
models with high behavioral fidelity -- including vision transformers --
improve the most from this regularization by as much as 1.9\% Top@1 accuracy
points. These findings suggest that biologically inspired learning agents can
benefit from human behavioral measurements as regularizers and psychophysical
learned representations can be transferred to independent evaluation tasks.",0.026933962,-0.18944433,-0.03581483,C
13765,"These aspects warrant further research and consider-
ation.","In addi-
tion, the proposed model requires less computation resources which is likely to
be environmental friendly.","B Patch-Text Supervision

Figure 9 shows the predicted bounding box (blue) and ground truth box (red) of
a same image across different epochs.",2022-11-15 05:34:40+00:00,YORO -- Lightweight End to End Visual Grounding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chih-Hui Ho'), arxiv.Result.Author('Srikar Appalaraju'), arxiv.Result.Author('Bhavan Jasani'), arxiv.Result.Author('R. Manmatha'), arxiv.Result.Author('Nuno Vasconcelos')]","We present YORO - a multi-modal transformer encoder-only architecture for the
Visual Grounding (VG) task. This task involves localizing, in an image, an
object referred via natural language. Unlike the recent trend in the literature
of using multi-stage approaches that sacrifice speed for accuracy, YORO seeks a
better trade-off between speed an accuracy by embracing a single-stage design,
without CNN backbone. YORO consumes natural language queries, image patches,
and learnable detection tokens and predicts coordinates of the referred object,
using a single transformer encoder. To assist the alignment between text and
visual objects, a novel patch-text alignment loss is proposed. Extensive
experiments are conducted on 5 different datasets with ablations on
architecture design choices. YORO is shown to support real-time inference and
outperform all approaches in this class (single-stage methods) by large
margins. It is also the fastest VG model and achieves the best speed/accuracy
trade-off in the literature.",0.06734216,0.014144575,0.11907335,C
13766,"These traffic lights    state of the art RetinaNet for each frame of INDRA as auxiliary data
use audible tones, verbal messages and/or vibrating surfaces to          for the community to use in further research.","We also release vehicle bounding boxes detected by
been the use of acoustic pedestrian lights [10].","notify the pedestrians about useful information such as the color of
traffic light and road crossing safety.",2022-11-15 06:04:30+00:00,A Dataset and Model for Crossing Indian Roads,cs.CV,['cs.CV'],[arxiv.Result.Author('Siddhi Brahmbhatt')],"Roads in medium-sized Indian towns often have lots of traffic but no (or
disregarded) traffic stops. This makes it hard for the blind to cross roads
safely, because vision is crucial to determine when crossing is safe. Automatic
and reliable image-based safety classifiers thus have the potential to help the
blind to cross Indian roads. Yet, we currently lack datasets collected on
Indian roads from the pedestrian point-of-view, labelled with road crossing
safety information. Existing classifiers from other countries are often
intended for crossroads, and hence rely on the detection and presence of
traffic lights, which is not applicable in Indian conditions. We introduce
INDRA (INdian Dataset for RoAd crossing), the first dataset capturing videos of
Indian roads from the pedestrian point-of-view. INDRA contains 104 videos
comprising of 26k 1080p frames, each annotated with a binary road crossing
safety label and vehicle bounding boxes. We train various classifiers to
predict road crossing safety on this data, ranging from SVMs to convolutional
neural networks (CNNs). The best performing model DilatedRoadCrossNet is a
novel single-image architecture tailored for deployment on the Nvidia Jetson
Nano. It achieves 79% recall at 90% precision on unseen images. Lastly, we
present a wearable road crossing assistant running DilatedRoadCrossNet, which
can help the blind cross Indian roads in real-time. The project webpage is
http://roadcross-assistant.github.io/Website/.",-0.11467933,0.21426588,-0.21349046,B
13771,"Another important implication of this paper is further research on the detection methods of generated images, in
parallel with the development of generative models.","We also suggest that researchers who develop image datasets collected from the
Internet should Ô¨Ålter out or mark generated images, which may affect Ô¨Ånal downstream performance, because adding
generated images may degenerate performance as shown in Section 5.2.","As experimented in Section 6.1, generated images of the latest
generative methods cannot be detected by simple methods that once had been effective.",2022-11-15 12:25:33+00:00,Will Large-scale Generative Models Corrupt Future Datasets?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ryuichiro Hataya'), arxiv.Result.Author('Han Bao'), arxiv.Result.Author('Hiromi Arai')]","Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: ""will such generated images impact the quality of future datasets and
the performance of computer vision models positively or negatively?"" This paper
empirically answers this question by simulating contamination. Namely, we
generate ImageNet-scale and COCO-scale datasets using a state-of-the-art
generative model and evaluate models trained on ``contaminated'' datasets on
various tasks including image classification and image generation. Throughout
experiments, we conclude that generated images negatively affect downstream
performance, while the significance depends on tasks and the amount of
generated images. The generated datasets are available via
https://github.com/moskomule/dataset-contamination.",-0.099534005,-0.08890705,0.059211157,C
13775,"Therefore, further research should be done on self-
      the model shows irregular Ô¨Çuctuations as the pre-training      supervised learning feature transfer methods.","However, for two object detection        and consequently weaken the generalization performance of
      tasks, as shown in Figure 11 (b), the performance of           the model.","For example,
      data size increases.",2022-11-15 13:32:22+00:00,"Self-supervised remote sensing feature learning: Learning Paradigms, Challenges, and Future Works",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Chao Tao'), arxiv.Result.Author('Ji Qi'), arxiv.Result.Author('Mingning Guo'), arxiv.Result.Author('Qing Zhu'), arxiv.Result.Author('Haifeng Li')]","Deep learning has achieved great success in learning features from massive
remote sensing images (RSIs). To better understand the connection between
feature learning paradigms (e.g., unsupervised feature learning (USFL),
supervised feature learning (SFL), and self-supervised feature learning
(SSFL)), this paper analyzes and compares them from the perspective of feature
learning signals, and gives a unified feature learning framework. Under this
unified framework, we analyze the advantages of SSFL over the other two
learning paradigms in RSIs understanding tasks and give a comprehensive review
of the existing SSFL work in RS, including the pre-training dataset,
self-supervised feature learning signals, and the evaluation methods. We
further analyze the effect of SSFL signals and pre-training data on the learned
features to provide insights for improving the RSI feature learning. Finally,
we briefly discuss some open problems and possible research directions.",-0.1803265,-0.19085482,-0.00912118,C
13780,"We
                                                                                         further study the 2D labels projected from 3D ground truth
Ablation 3: effectiveness of the temporal 2D supervision.",Ablation 4: is the 3D prior affect the performance?,whether triggering 3D information leakage for training.,2022-11-15 16:40:11+00:00,Towards 3D Object Detection with 2D Supervision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinrong Yang'), arxiv.Result.Author('Tiancai Wang'), arxiv.Result.Author('Zheng Ge'), arxiv.Result.Author('Weixin Mao'), arxiv.Result.Author('Xiaoping Li'), arxiv.Result.Author('Xiangyu Zhang')]","The great progress of 3D object detectors relies on large-scale data and 3D
annotations. The annotation cost for 3D bounding boxes is extremely expensive
while the 2D ones are easier and cheaper to collect. In this paper, we
introduce a hybrid training framework, enabling us to learn a visual 3D object
detector with massive 2D (pseudo) labels, even without 3D annotations. To break
through the information bottleneck of 2D clues, we explore a new perspective:
Temporal 2D Supervision. We propose a temporal 2D transformation to bridge the
3D predictions with temporal 2D labels. Two steps, including homography wraping
and 2D box deduction, are taken to transform the 3D predictions into 2D ones
for supervision. Experiments conducted on the nuScenes dataset show strong
results (nearly 90% of its fully-supervised performance) with only 25% 3D
annotations. We hope our findings can provide new insights for using a large
number of 2D annotations for 3D perception.",0.002679199,0.09577466,-0.109164685,B
13781,"We further study employing priors on the                    One nonnegligible limitation of our work is the motion
instance division.","Could the split priors on motion state, distance, and size
work better?",Fig.,2022-11-15 16:40:11+00:00,Towards 3D Object Detection with 2D Supervision,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jinrong Yang'), arxiv.Result.Author('Tiancai Wang'), arxiv.Result.Author('Zheng Ge'), arxiv.Result.Author('Weixin Mao'), arxiv.Result.Author('Xiaoping Li'), arxiv.Result.Author('Xiangyu Zhang')]","The great progress of 3D object detectors relies on large-scale data and 3D
annotations. The annotation cost for 3D bounding boxes is extremely expensive
while the 2D ones are easier and cheaper to collect. In this paper, we
introduce a hybrid training framework, enabling us to learn a visual 3D object
detector with massive 2D (pseudo) labels, even without 3D annotations. To break
through the information bottleneck of 2D clues, we explore a new perspective:
Temporal 2D Supervision. We propose a temporal 2D transformation to bridge the
3D predictions with temporal 2D labels. Two steps, including homography wraping
and 2D box deduction, are taken to transform the 3D predictions into 2D ones
for supervision. Experiments conducted on the nuScenes dataset show strong
results (nearly 90% of its fully-supervised performance) with only 25% 3D
annotations. We hope our findings can provide new insights for using a large
number of 2D annotations for 3D perception.",0.08042881,0.15936929,-0.055819206,A
13886,"We hope our simple and effective method could         temporal consistency of instance embeddings between low
                                        beneÔ¨Åt further research.","To improve the
                                        challenge.","sampling rate frames, we propose to pre-train the model on
                                                                                                         COCO [11] with pseudo-labels, which further improves the
                                        1.",2022-11-18 01:40:59+00:00,The Runner-up Solution for YouTube-VIS Long Video Challenge 2022,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junfeng Wu'), arxiv.Result.Author('Yi Jiang'), arxiv.Result.Author('Qihao Liu'), arxiv.Result.Author('Xiang Bai'), arxiv.Result.Author('Song Bai')]","This technical report describes our 2nd-place solution for the ECCV 2022
YouTube-VIS Long Video Challenge. We adopt the previously proposed online video
instance segmentation method IDOL for this challenge. In addition, we use
pseudo labels to further help contrastive learning, so as to obtain more
temporally consistent instance embedding to improve tracking performance
between frames. The proposed method obtains 40.2 AP on the YouTube-VIS 2022
long video dataset and was ranked second place in this challenge. We hope our
simple and effective method could benefit further research.",-0.18738386,-0.23355058,-0.07762186,C
13887,"We believe the simplicity and effectiveness of our
                                        wide applications in video understanding, video editing,         method shall beneÔ¨Åt further research.","It has attracted considerable attention since Ô¨Årst        AP on the public validation set, and 40.2 AP on the private
                                        deÔ¨Åned [18] in 2019 due to the huge challenge and the            test set.","autonomous driving, augmented reality, etc.",2022-11-18 01:40:59+00:00,The Runner-up Solution for YouTube-VIS Long Video Challenge 2022,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junfeng Wu'), arxiv.Result.Author('Yi Jiang'), arxiv.Result.Author('Qihao Liu'), arxiv.Result.Author('Xiang Bai'), arxiv.Result.Author('Song Bai')]","This technical report describes our 2nd-place solution for the ECCV 2022
YouTube-VIS Long Video Challenge. We adopt the previously proposed online video
instance segmentation method IDOL for this challenge. In addition, we use
pseudo labels to further help contrastive learning, so as to obtain more
temporally consistent instance embedding to improve tracking performance
between frames. The proposed method obtains 40.2 AP on the YouTube-VIS 2022
long video dataset and was ranked second place in this challenge. We hope our
simple and effective method could benefit further research.",-0.17599916,0.15605599,-0.17075118,B
13888,ness of our method shall beneÔ¨Åt further research.,"We believe the simplicity and effective-
GPUs, with 4 pairs of frames per GPU.",3.2.,2022-11-18 01:40:59+00:00,The Runner-up Solution for YouTube-VIS Long Video Challenge 2022,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junfeng Wu'), arxiv.Result.Author('Yi Jiang'), arxiv.Result.Author('Qihao Liu'), arxiv.Result.Author('Xiang Bai'), arxiv.Result.Author('Song Bai')]","This technical report describes our 2nd-place solution for the ECCV 2022
YouTube-VIS Long Video Challenge. We adopt the previously proposed online video
instance segmentation method IDOL for this challenge. In addition, we use
pseudo labels to further help contrastive learning, so as to obtain more
temporally consistent instance embedding to improve tracking performance
between frames. The proposed method obtains 40.2 AP on the YouTube-VIS 2022
long video dataset and was ranked second place in this challenge. We hope our
simple and effective method could benefit further research.",0.18507513,0.32998586,0.16676879,A
13903,"However, our focus is on the speciÔ¨Åc task of image de-
blurring, and therefore, further research is needed to draw more general conclusions
on the beneÔ¨Åts of synthetic data.","While the positive impact of synthetic data is well-
known in deep learning research [35], we have demonstrated it in the context of
solving inverse problems.","Furthermore, the stability of our algorithmic pipeline remains to be investigated.",2022-11-18 09:06:56+00:00,Let's Enhance: A Deep Learning Approach to Extreme Deblurring of Text Images,cs.CV,"['cs.CV', 'cs.LG', 'cs.NA', 'math.NA', '94A08, 68T07, 68T20']","[arxiv.Result.Author('Theophil Trippe'), arxiv.Result.Author('Martin Genzel'), arxiv.Result.Author('Jan Macdonald'), arxiv.Result.Author('Maximilian M√§rz')]","This work presents a novel deep-learning-based pipeline for the inverse
problem of image deblurring, leveraging augmentation and pre-training with
synthetic data. Our results build on our winning submission to the recent
Helsinki Deblur Challenge 2021, whose goal was to explore the limits of
state-of-the-art deblurring algorithms in a real-world data setting. The task
of the challenge was to deblur out-of-focus images of random text, thereby in a
downstream task, maximizing an optical-character-recognition-based score
function. A key step of our solution is the data-driven estimation of the
physical forward model describing the blur process. This enables a stream of
synthetic data, generating pairs of ground-truth and blurry images on-the-fly,
which is used for an extensive augmentation of the small amount of challenge
data provided. The actual deblurring pipeline consists of an approximate
inversion of the radial lens distortion (determined by the estimated forward
model) and a U-Net architecture, which is trained end-to-end. Our algorithm was
the only one passing the hardest challenge level, achieving over 70% character
recognition accuracy. Our findings are well in line with the paradigm of
data-centric machine learning, and we demonstrate its effectiveness in the
context of inverse problems. Apart from a detailed presentation of our
methodology, we also analyze the importance of several design choices in a
series of ablation studies. The code of our challenge submission is available
under https://github.com/theophil-trippe/HDC_TUBerlin_version_1.",-0.18209013,-0.05064046,0.3368392,C
13905,"800        N = 5
                                                                                                                                    600        N = 7
   We further study the performance variation of the LPTC                                                                                      N = 9
model with respect to different values of the order n5, where                                                                                  Baseline
n5 is tuned within {5, 10, 15, 20, 25} while Œ≤ and œÑ5 are Ô¨Åxed
to 4 and 30, respectively.","Furthermore, the Weber contrast sensitivity tuning curve                                                                      1000        N = 3
remains unchanged at different settings of Œ≤.",As shown in Fig.,2022-11-18 10:10:48+00:00,Spatio-Temporal Feedback Control of Small Target Motion Detection Visual System,cs.CV,['cs.CV'],"[arxiv.Result.Author('Hongxin Wang'), arxiv.Result.Author('Zhiyan Zhong'), arxiv.Result.Author('Fang Lei'), arxiv.Result.Author('Xiaohua Jing'), arxiv.Result.Author('Jigen Peng'), arxiv.Result.Author('Shigang Yue')]","Feedback is crucial to motion perception in animals' visual systems where its
spatial and temporal dynamics are often shaped by movement patterns of
surrounding environments. However, such spatio-temporal feedback has not been
deeply explored in designing neural networks to detect small moving targets
that cover only one or a few pixels in image while presenting extremely limited
visual features. In this paper, we address small target motion detection
problem by developing a visual system with spatio-temporal feedback loop, and
further reveal its important roles in suppressing false positive background
movement while enhancing network responses to small targets. Specifically, the
proposed visual system is composed of two complementary subnetworks. The first
subnetwork is designed to extract spatial and temporal motion patterns of
cluttered backgrounds by neuronal ensemble coding. The second subnetwork is
developed to capture small target motion information and integrate the
spatio-temporal feedback signal from the first subnetwork to inhibit background
false positives. Experimental results demonstrate that the proposed
spatio-temporal feedback visual system is more competitive than existing
methods in discriminating small moving targets from complex dynamic
environment.",0.4283169,0.12992305,0.16684814,A
13911,"IEEE transactions on pattern analysis and ma-
ods and point promising directions for further study.","Global contrast based salient region
the baseline model, we analyze the weakness of prior meth-                detection.","We                  chine intelligence, 37(3):569‚Äì582, 2014.",2022-11-18 11:59:37+00:00,LVOS: A Benchmark for Long-term Video Object Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lingyi Hong'), arxiv.Result.Author('Wenchao Chen'), arxiv.Result.Author('Zhongying Liu'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Pinxue Guo'), arxiv.Result.Author('Zhaoyu Chen'), arxiv.Result.Author('Wenqiang Zhang')]","Existing video object segmentation (VOS) benchmarks focus on short-term
videos which just last about 3-5 seconds and where objects are visible most of
the time. These videos are poorly representative of practical applications, and
the absence of long-term datasets restricts further investigation of VOS on the
application in realistic scenarios. So, in this paper, we present a new
benchmark dataset and evaluation methodology named LVOS, which consists of 220
videos with a total duration of 421 minutes. To the best of our knowledge, LVOS
is the first densely annotated long-term VOS dataset. The videos in our LVOS
last 1.59 minutes on average, which is 20 times longer than videos in existing
VOS datasets. Each video includes various attributes, especially challenges
deriving from the wild, such as long-term reappearing and cross-temporal
similar objeccts. Moreover, we provide additional language descriptions to
encourage the exploration of integrating linguistic and visual features for
video object segmentation. Based on LVOS, we assess existing video object
segmentation algorithms and propose a Diverse Dynamic Memory network (DDMemory)
that consists of three complementary memory banks to exploit temporal
information adequately. The experiment results demonstrate the strength and
weaknesses of prior methods, pointing promising directions for further study.
Our objective is to provide the community with a large and varied benchmark to
boost the advancement of long-term VOS. Data and code are available at
\url{https://lingyihongfd.github.io/lvos.github.io/}.",0.0736729,0.074356765,-0.02313118,A
13931,"Note that due to the different   larger UCF-HMDBfull [27] dataset is introduced to facilitate
backbones and training techniques applied by the different      further research on VUDA.","IV, V, VI VII, and VIII.","The UCF-HMDBfull dataset is also
methods, direct comparison of their performance may not be      built across the UCF50 and the HMDB51 datasets, but with
fair and only serves as an intuitive reference towards the      more than doubled number of classes compared to the UCF-
comparison of each method.",2022-11-17 05:05:42+00:00,Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuecong Xu'), arxiv.Result.Author('Haozhi Cao'), arxiv.Result.Author('Zhenghua Chen'), arxiv.Result.Author('Xiaoli Li'), arxiv.Result.Author('Lihua Xie'), arxiv.Result.Author('Jianfei Yan')]","Video analysis tasks such as action recognition have received increasing
research interest with growing applications in fields such as smart healthcare,
thanks to the introduction of large-scale datasets and deep learning-based
representations. However, video models trained on existing datasets suffer from
significant performance degradation when deployed directly to real-world
applications due to domain shifts between the training public video datasets
(source video domains) and real-world videos (target video domains). Further,
with the high cost of video annotation, it is more practical to use unlabeled
videos for training. To tackle performance degradation and address concerns in
high video annotation cost uniformly, the video unsupervised domain adaptation
(VUDA) is introduced to adapt video models from the labeled source domain to
the unlabeled target domain by alleviating video domain shift, improving the
generalizability and portability of video models. This paper surveys recent
progress in VUDA with deep learning. We begin with the motivation of VUDA,
followed by its definition, and recent progress of methods for both closed-set
VUDA and VUDA under different scenarios, and current benchmark datasets for
VUDA research. Eventually, future directions are provided to promote further
VUDA research.",0.105328485,-0.1973045,0.055649415,A
13932,"Note that due to the different   larger UCF-HMDBfull [27] dataset is introduced to facilitate
backbones and training techniques applied by the different      further research on VUDA.","IV, V, VI VII, and VIII.","The UCF-HMDBfull dataset is also
methods, direct comparison of their performance may not be      built across the UCF50 and the HMDB51 datasets, but with
fair and only serves as an intuitive reference towards the      more than doubled number of classes compared to the UCF-
comparison of each method.",2022-11-17 05:05:42+00:00,Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuecong Xu'), arxiv.Result.Author('Haozhi Cao'), arxiv.Result.Author('Zhenghua Chen'), arxiv.Result.Author('Xiaoli Li'), arxiv.Result.Author('Lihua Xie'), arxiv.Result.Author('Jianfei Yang')]","Video analysis tasks such as action recognition have received increasing
research interest with growing applications in fields such as smart healthcare,
thanks to the introduction of large-scale datasets and deep learning-based
representations. However, video models trained on existing datasets suffer from
significant performance degradation when deployed directly to real-world
applications due to domain shifts between the training public video datasets
(source video domains) and real-world videos (target video domains). Further,
with the high cost of video annotation, it is more practical to use unlabeled
videos for training. To tackle performance degradation and address concerns in
high video annotation cost uniformly, the video unsupervised domain adaptation
(VUDA) is introduced to adapt video models from the labeled source domain to
the unlabeled target domain by alleviating video domain shift, improving the
generalizability and portability of video models. This paper surveys recent
progress in VUDA with deep learning. We begin with the motivation of VUDA,
followed by its definition, and recent progress of methods for both closed-set
VUDA and VUDA under different scenarios, and current benchmark datasets for
VUDA research. Eventually, future directions are provided to promote further
VUDA research.",0.105328485,-0.1973045,0.055649415,A
13957,"To further study the sensitivity of                  query formulation, which provides a uniÔ¨Åed way to en-
the model to the line location, before Ô¨Åne-tuning, we ran-                     code requisite text location and semantic clues for simul-
domly shift the center line to the top or bottom boundary                      taneous detection and recognition.","We propose a novel explicit point
SPTS [40] for evaluation.","With a single decoder
                                                                               and several simple prediction heads, we devise a much
                                                                               simpler spotter compared with the previous ones.",2022-11-19 19:06:22+00:00,DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maoyuan Ye'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Shanshan Zhao'), arxiv.Result.Author('Juhua Liu'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Dacheng Tao')]","End-to-end text spotting aims to integrate scene text detection and
recognition into a unified framework. Dealing with the relationship between the
two sub-tasks plays a pivotal role in designing effective spotters. Although
transformer-based methods eliminate the heuristic post-processing, they still
suffer from the synergy issue between the sub-tasks and low training
efficiency. In this paper, we present DeepSolo, a simple detection transformer
baseline that lets a single Decoder with Explicit Points Solo for text
detection and recognition simultaneously. Technically, for each text instance,
we represent the character sequence as ordered points and model them with
learnable explicit point queries. After passing a single decoder, the point
queries have encoded requisite text semantics and locations and thus can be
further decoded to the center line, boundary, script, and confidence of text
via very simple prediction heads in parallel, solving the sub-tasks in text
spotting in a unified framework. Besides, we also introduce a text-matching
criterion to deliver more accurate supervisory signals, thus enabling more
efficient training. Quantitative experiments on public benchmarks demonstrate
that DeepSolo outperforms previous state-of-the-art methods and achieves better
training efficiency. In addition, DeepSolo is also compatible with line
annotations, which require much less annotation cost than polygons. The code
will be released.",-0.026355889,0.0028308504,-0.18417545,B
13958,"To further study the sensitivity of                  query formulation, which provides a uniÔ¨Åed way to en-
the model to the line location, before Ô¨Åne-tuning, we ran-                     code requisite text location and semantic clues for simul-
domly shift the center line to the top or bottom boundary                      taneous detection and recognition.","We propose a novel explicit point
SPTS [40] for evaluation.","With a single decoder
                                                                               and several simple prediction heads, we devise a much
                                                                               simpler spotter compared with the previous ones.",2022-11-19 19:06:22+00:00,DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maoyuan Ye'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Shanshan Zhao'), arxiv.Result.Author('Juhua Liu'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Dacheng Tao')]","End-to-end text spotting aims to integrate scene text detection and
recognition into a unified framework. Dealing with the relationship between the
two sub-tasks plays a pivotal role in designing effective spotters. Although
transformer-based methods eliminate the heuristic post-processing, they still
suffer from the synergy issue between the sub-tasks and low training
efficiency. In this paper, we present DeepSolo, a simple detection transformer
baseline that lets a single Decoder with Explicit Points Solo for text
detection and recognition simultaneously. Technically, for each text instance,
we represent the character sequence as ordered points and model them with
learnable explicit point queries. After passing a single decoder, the point
queries have encoded requisite text semantics and locations and thus can be
further decoded to the center line, boundary, script, and confidence of text
via very simple prediction heads in parallel, solving the sub-tasks in text
spotting in a unified framework. Besides, we also introduce a text-matching
criterion to deliver more accurate supervisory signals, thus enabling more
efficient training. Quantitative experiments on public benchmarks demonstrate
that DeepSolo outperforms previous state-of-the-art methods and achieves better
training efficiency. In addition, DeepSolo is also compatible with line
annotations, which require much less annotation cost than polygons. The code
will be released.",-0.026355889,0.0028308504,-0.18417545,B
13967,"2020 IEEE/CVF Conference on Com-
and 3D semantic segmentation and we will do further research on it.","We believe that CA-aug is applicable to other           PointAugment: An Auto-Augmentation Framework for Point
tasks in the Ô¨Åeld of autonomous driving, such as 3D object tracking           Cloud ClassiÔ¨Åcation.","puter Vision and Pattern Recognition (CVPR), pages 6377‚Äì
                                                                              6386, 2020.",2022-11-20 02:45:18+00:00,Context-Aware Data Augmentation for LIDAR 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xuzhong Hu'), arxiv.Result.Author('Zaipeng Duan'), arxiv.Result.Author('Jie Ma')]","For 3D object detection, labeling lidar point cloud is difficult, so data
augmentation is an important module to make full use of precious annotated
data. As a widely used data augmentation method, GT-sample effectively improves
detection performance by inserting groundtruths into the lidar frame during
training. However, these samples are often placed in unreasonable areas, which
misleads model to learn the wrong context information between targets and
backgrounds. To address this problem, in this paper, we propose a context-aware
data augmentation method (CA-aug) , which ensures the reasonable placement of
inserted objects by calculating the ""Validspace"" of the lidar point cloud.
CA-aug is lightweight and compatible with other augmentation methods. Compared
with the GT-sample and the similar method in Lidar-aug(SOTA), it brings higher
accuracy to the existing detectors. We also present an in-depth study of
augmentation methods for the range-view-based(RV-based) models and find that
CA-aug can fully exploit the potential of RV-based networks. The experiment on
KITTI val split shows that CA-aug can improve the mAP of the test model by 8%.",-0.3387596,0.25970823,-0.07029332,B
13968,"Several techniques were recently presented                   [10] J. Xie, Z. Zheng, R. Gao, W. Wang, Z. Song-Chun, and Y. N. Wu,
to obtain a substantial speed-up [45], [46], [47], [48], and                          ‚ÄúLearning descriptor networks for 3d shape synthesis and analysis,‚Äù
further research could be invested into integrating them within                       in The IEEE Conference on Computer Vision and Pattern Recognition
our pipeline.",the one we employ.,"Being the Ô¨Årst work of its kind, IC3D focuses                           (CVPR), 2018.",2022-11-20 04:21:42+00:00,IC3D: Image-Conditioned 3D Diffusion for Shape Generation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Cristian Sbrolli'), arxiv.Result.Author('Paolo Cudrano'), arxiv.Result.Author('Matteo Frosi'), arxiv.Result.Author('Matteo Matteucci')]","In the last years, Denoising Diffusion Probabilistic Models (DDPMs) obtained
state-of-the-art results in many generative tasks, outperforming GANs and other
classes of generative models. In particular, they reached impressive results in
various image generation sub-tasks, among which conditional generation tasks
such as text-guided image synthesis. Given the success of DDPMs in 2D
generation, they have more recently been applied to 3D shape generation,
outperforming previous approaches and reaching state-of-the-art results.
However, 3D data pose additional challenges, such as the choice of the 3D
representation, which impacts design choices and model efficiency. While
reaching state-of-the-art results in generation quality, existing 3D DDPM works
make little or no use of guidance, mainly being unconditional or
class-conditional. In this paper, we present IC3D, the first Image-Conditioned
3D Diffusion model that generates 3D shapes by image guidance. It is also the
first 3D DDPM model that adopts voxels as a 3D representation. To guide our
DDPM, we present and leverage CISP (Contrastive Image-Shape Pre-training), a
model jointly embedding images and shapes by contrastive pre-training, inspired
by text-to-image DDPM works. Our generative diffusion model outperforms the
state-of-the-art in 3D generation quality and diversity. Furthermore, we show
that our generated shapes are preferred by human evaluators to a SoTA
single-view 3D reconstruction model in terms of quality and coherence to the
query image by running a side-by-side human evaluation.",-0.28392428,0.114013895,0.08642733,B
13969,We Ô¨Ånd that further studying the interaction            in the local region before the feature aggregation.,"AE2IL models the interaction between               teresting work, PointWeb [73], develops the Adaptive
eij and eik, but overlooks the reverse edge eji and the           Feature Adjustment (AFA) module to update all points
edge ejk.","As
between eji and ejk could exploit the local structure             a result, the point-to-point relation in PointWeb can
better.",2022-11-20 07:10:14+00:00,Adaptive Edge-to-Edge Interaction Learning for Point Cloud Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shanshan Zhao'), arxiv.Result.Author('Mingming Gong'), arxiv.Result.Author('Xi Li'), arxiv.Result.Author('Dacheng Tao')]","Recent years have witnessed the great success of deep learning on various
point cloud analysis tasks, e.g., classification and semantic segmentation.
Since point cloud data is sparse and irregularly distributed, one key issue for
point cloud data processing is extracting useful information from local
regions. To achieve this, previous works mainly extract the points' features
from local regions by learning the relation between each pair of adjacent
points. However, these works ignore the relation between edges in local
regions, which encodes the local shape information. Associating the
neighbouring edges could potentially make the point-to-point relation more
aware of the local structure and more robust. To explore the role of the
relation between edges, this paper proposes a novel Adaptive Edge-to-Edge
Interaction Learning module, which aims to enhance the point-to-point relation
through modelling the edge-to-edge interaction in the local region adaptively.
We further extend the module to a symmetric version to capture the local
structure more thoroughly. Taking advantage of the proposed modules, we develop
two networks for segmentation and shape classification tasks, respectively.
Various experiments on several public point cloud datasets demonstrate the
effectiveness of our method for point cloud analysis.",0.085853785,0.033647764,0.046201583,A
13973,"Many scholars further study
                                           pared with the classical models or the generalization improve-  other training techniques to improve model generalization,
                                           ment methods, such as Dropout, Mixup, Cutout, and CutMix,       such as classical regularization, label smoothing, and weight
                                           Feature Weaken shows good compatibility and performance.","Com-        Ba, Kiros, and Hinton 2016).",decay.,2022-11-20 11:00:23+00:00,Feature Weaken: Vicinal Data Augmentation for Classification,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Songhao Jiang'), arxiv.Result.Author('Yan Chu'), arxiv.Result.Author('Tianxing Ma'), arxiv.Result.Author('Tianning Zang')]","Deep learning usually relies on training large-scale data samples to achieve
better performance. However, over-fitting based on training data always remains
a problem. Scholars have proposed various strategies, such as feature dropping
and feature mixing, to improve the generalization continuously. For the same
purpose, we subversively propose a novel training method, Feature Weaken, which
can be regarded as a data augmentation method. Feature Weaken constructs the
vicinal data distribution with the same cosine similarity for model training by
weakening features of the original samples. In especially, Feature Weaken
changes the spatial distribution of samples, adjusts sample boundaries, and
reduces the gradient optimization value of back-propagation. This work can not
only improve the classification performance and generalization of the model,
but also stabilize the model training and accelerate the model convergence. We
conduct extensive experiments on classical deep convolution neural models with
five common image classification datasets and the Bert model with four common
text classification datasets. Compared with the classical models or the
generalization improvement methods, such as Dropout, Mixup, Cutout, and CutMix,
Feature Weaken shows good compatibility and performance. We also use
adversarial samples to perform the robustness experiments, and the results show
that Feature Weaken is effective in improving the robustness of the model.",0.1758956,-0.28588626,0.08714885,A
13977,"To help further research and solve the deÔ¨Åciency                    of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34,
of current benchmarks, we propose a large-scale and high-                    pages 10534‚Äì10541, 2020.
quality dataset COESOT for the color-event tracking com-
munity, which presents color frames, event frames, event               [10] Haosheng Chen, Qiangqiang Wu, Yanjie Liang, Xinbo Gao,
voxels, source Ô¨Åles, and dense annotations, etc.","In Proceedings
tracker.",We also                     and Hanzi Wang.,2022-11-20 16:01:31+00:00,"Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric",cs.CV,"['cs.CV', 'cs.AI', 'cs.NE']","[arxiv.Result.Author('Chuanming Tang'), arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Ju Huang'), arxiv.Result.Author('Bo Jiang'), arxiv.Result.Author('Lin Zhu'), arxiv.Result.Author('Jianlin Zhang'), arxiv.Result.Author('Yaowei Wang'), arxiv.Result.Author('Yonghong Tian')]","Combining the Color and Event cameras (also called Dynamic Vision Sensors,
DVS) for robust object tracking is a newly emerging research topic in recent
years. Existing color-event tracking framework usually contains multiple
scattered modules which may lead to low efficiency and high computational
complexity, including feature extraction, fusion, matching, interactive
learning, etc. In this paper, we propose a single-stage backbone network for
Color-Event Unified Tracking (CEUTrack), which achieves the above functions
simultaneously. Given the event points and RGB frames, we first transform the
points into voxels and crop the template and search regions for both
modalities, respectively. Then, these regions are projected into tokens and
parallelly fed into the unified Transformer backbone network. The output
features will be fed into a tracking head for target object localization. Our
proposed CEUTrack is simple, effective, and efficient, which achieves over 75
FPS and new SOTA performance. To better validate the effectiveness of our model
and address the data deficiency of this task, we also propose a generic and
large-scale benchmark dataset for color-event tracking, termed COESOT, which
contains 90 categories and 1354 video sequences. Additionally, a new evaluation
metric named BOC is proposed in our evaluation toolkit to evaluate the
prominence with respect to the baseline methods. We hope the newly proposed
method, dataset, and evaluation metric provide a better platform for
color-event-based tracking. The dataset, toolkit, and source code will be
released on: \url{https://github.com/Event-AHU/COESOT}.",-0.052719265,-0.015780997,-0.17702796,B
13987,"Traversability             trained features to facilitate further research in visual repre-
prediction does not help much on Val-Unseen but is bene-            sentations for VLN.","For example, on top of instance       pre-trained features are orthogonal to other VLN works, and
classification, 3D jigsaw helps the agent perform even bet-         we will release the collected dataset, source code, and pre-
ter on Val-Unseen (#6 compared with #4).",ficial on Val-Seen (#7 compared with #4.),2022-11-20 23:04:39+00:00,Structure-Encoding Auxiliary Tasks for Improved Visual Representation in Vision-and-Language Navigation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Chia-Wen Kuo'), arxiv.Result.Author('Chih-Yao Ma'), arxiv.Result.Author('Judy Hoffman'), arxiv.Result.Author('Zsolt Kira')]","In Vision-and-Language Navigation (VLN), researchers typically take an image
encoder pre-trained on ImageNet without fine-tuning on the environments that
the agent will be trained or tested on. However, the distribution shift between
the training images from ImageNet and the views in the navigation environments
may render the ImageNet pre-trained image encoder suboptimal. Therefore, in
this paper, we design a set of structure-encoding auxiliary tasks (SEA) that
leverage the data in the navigation environments to pre-train and improve the
image encoder. Specifically, we design and customize (1) 3D jigsaw, (2)
traversability prediction, and (3) instance classification to pre-train the
image encoder. Through rigorous ablations, our SEA pre-trained features are
shown to better encode structural information of the scenes, which ImageNet
pre-trained features fail to properly encode but is crucial for the target
navigation task. The SEA pre-trained features can be easily plugged into
existing VLN agents without any tuning. For example, on Test-Unseen
environments, the VLN agents combined with our SEA pre-trained features achieve
absolute success rate improvement of 12% for Speaker-Follower, 5% for
Env-Dropout, and 4% for AuxRN.",-0.13749567,-0.16632384,0.00739629,C
13988,"Primary motivation
                                                includes support for further research in smaller, more eÔ¨Écient neural network architectures such
                                                that can not only accomplish complex tasks, such as steering angle predictions, but also produce
                                                less carbon emissions, or, more succinctly, neural networks that are more environmentally friendly.","This
                                                work provides preliminary evidence that Inception architectures can perform as well or better than
                                                ResNet architectures with less complexity for the autonomous driving task.",We look at various sizes of ResNet and InceptionNet models to compare results.,2022-11-21 00:38:59+00:00,Towards Greener Solutions for Steering Angle Prediction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jeremy C. Hagler'), arxiv.Result.Author('David J. Lamb'), arxiv.Result.Author('Qing Tian')]","In this paper, we investigate the two most popular families of deep neural
architectures (i.e., ResNets and Inception nets) for the autonomous driving
task of steering angle prediction. This work provides preliminary evidence that
Inception architectures can perform as well or better than ResNet architectures
with less complexity for the autonomous driving task. Primary motivation
includes support for further research in smaller, more efficient neural network
architectures such that can not only accomplish complex tasks, such as steering
angle predictions, but also produce less carbon emissions, or, more succinctly,
neural networks that are more environmentally friendly. We look at various
sizes of ResNet and InceptionNet models to compare results. Our derived models
can achieve state-of-the-art results in terms of steering angle MSE.",-0.20271969,-0.19352336,0.22971764,C
13989,"The reduction of computational
complexity will hopefully promote further research into using compact Inception models for complex
tasks which will, in turn, reduce carbon emissions from the training and deployment of artiÔ¨Åcial
intelligence models.","While we do analyze the outcomes of all of the ResNet models, our main focus is to
outperform ResNet models with a comparable Inception architecture.","2 Related Work

    The new focus on lower carbon emissions has put neural network research in a unique position.",2022-11-21 00:38:59+00:00,Towards Greener Solutions for Steering Angle Prediction,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jeremy C. Hagler'), arxiv.Result.Author('David J. Lamb'), arxiv.Result.Author('Qing Tian')]","In this paper, we investigate the two most popular families of deep neural
architectures (i.e., ResNets and Inception nets) for the autonomous driving
task of steering angle prediction. This work provides preliminary evidence that
Inception architectures can perform as well or better than ResNet architectures
with less complexity for the autonomous driving task. Primary motivation
includes support for further research in smaller, more efficient neural network
architectures such that can not only accomplish complex tasks, such as steering
angle predictions, but also produce less carbon emissions, or, more succinctly,
neural networks that are more environmentally friendly. We look at various
sizes of ResNet and InceptionNet models to compare results. Our derived models
can achieve state-of-the-art results in terms of steering angle MSE.",-0.13542342,-0.30118895,0.24551433,C
13991,"A further study on RFID transmission power levels showed that
location tracking is more precise with reduced reader transmission power, but the recognition
rate also decreases [82].","The study
suggested that using multiple sensors including proprioceptive sensors such as an IMU, tilt or
odometer, as well as exteroceptive sensors such as laser scanners or cameras, together with
RFID, improves sensor positioning and mapping while making the solution robust to data
ghosting or corruption.","Beyond these, studies investigated LEDs as an alternative to RFID, as
recognition of RFID is affected by radio or electromagnetic interference and has a longer
response time and low precision.",2022-11-21 04:56:02+00:00,A review of laser scanning for geological and geotechnical applications in underground mining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sarvesh Kumar Singh'), arxiv.Result.Author('Bikram Pratap Banerjee'), arxiv.Result.Author('Simit Raval')]","Laser scanning can provide timely assessments of mine sites despite adverse
challenges in the operational environment. Although there are several published
articles on laser scanning, there is a need to review them in the context of
underground mining applications. To this end, a holistic review of laser
scanning is presented including progress in 3D scanning systems, data
capture/processing techniques and primary applications in underground mines.
Laser scanning technology has advanced significantly in terms of mobility and
mapping, but there are constraints in coherent and consistent data collection
at certain mines due to feature deficiency, dynamics, and environmental
influences such as dust and water. Studies suggest that laser scanning has
matured over the years for change detection, clearance measurements and
structure mapping applications. However, there is scope for improvements in
lithology identification, surface parameter measurements, logistic tracking and
autonomous navigation. Laser scanning has the potential to provide real-time
solutions but the lack of infrastructure in underground mines for data
transfer, geodetic networking and processing capacity remain limiting factors.
Nevertheless, laser scanners are becoming an integral part of mine automation
thanks to their affordability, accuracy and mobility, which should support
their widespread usage in years to come.",0.023678413,0.3009752,-0.2039456,B
13992,"Looking ahead, further research using
laser scanning for lithological classification could improve the rock mass characterisation
process in underground mines with suboptimal lighting.","To date, however, in underground mines, limited-to-no studies employed laser scanning to
identify the lithological composition of the rock mass.","Given the recent advances in multi-
spectral LiDAR, further improvements in identifying the lithology are also expected [101,102].",2022-11-21 04:56:02+00:00,A review of laser scanning for geological and geotechnical applications in underground mining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Sarvesh Kumar Singh'), arxiv.Result.Author('Bikram Pratap Banerjee'), arxiv.Result.Author('Simit Raval')]","Laser scanning can provide timely assessments of mine sites despite adverse
challenges in the operational environment. Although there are several published
articles on laser scanning, there is a need to review them in the context of
underground mining applications. To this end, a holistic review of laser
scanning is presented including progress in 3D scanning systems, data
capture/processing techniques and primary applications in underground mines.
Laser scanning technology has advanced significantly in terms of mobility and
mapping, but there are constraints in coherent and consistent data collection
at certain mines due to feature deficiency, dynamics, and environmental
influences such as dust and water. Studies suggest that laser scanning has
matured over the years for change detection, clearance measurements and
structure mapping applications. However, there is scope for improvements in
lithology identification, surface parameter measurements, logistic tracking and
autonomous navigation. Laser scanning has the potential to provide real-time
solutions but the lack of infrastructure in underground mines for data
transfer, geodetic networking and processing capacity remain limiting factors.
Nevertheless, laser scanners are becoming an integral part of mine automation
thanks to their affordability, accuracy and mobility, which should support
their widespread usage in years to come.",0.044071864,0.29711908,-0.0244415,B
14000,The optimization of the success rates on the M and K grid deserves further research.,"Indeed, the
success rate increases from ~0.765 for M = 16 to ~0.79 for M = 80 (Table 1, upper panel).",The Tree-3 architecture (Fig.,2022-11-21 11:48:30+00:00,Learning on tree architectures outperforms a convolutional feedforward network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuval Meir'), arxiv.Result.Author('Itamar Ben-Noam'), arxiv.Result.Author('Yarden Tzach'), arxiv.Result.Author('Shiri Hodassman'), arxiv.Result.Author('Ido Kanter')]","Advanced deep learning architectures consist of tens of fully connected and
convolutional hidden layers, which are already extended to hundreds, and are
far from their biological realization. Their implausible biological dynamics is
based on changing a weight in a non-local manner, as the number of routes
between an output unit and a weight is typically large, using the
backpropagation technique. Here, offline and online CIFAR-10 database learning
on 3-layer tree architectures, inspired by experimental-based dendritic tree
adaptations, outperforms the achievable success rates of the 5-layer
convolutional LeNet. Its highly pruning tree backpropagation procedure, where a
single route connects an output unit and a weight, represents an efficient
dendritic deep learning.",0.47898248,0.14312197,0.08340371,A
14001,"This
possible advantage of shallow tree architectures in minimizing the effect of vanishing or
exploding gradients, calls for their quantitative examination in further research.","Even if the gradient of each route is small, the summation can be
large and lead to weight explosion, a reality which is excluded in tree architectures.","The introduction of parallel branches instead of the second convolutional layer in LeNet-
5 enhances success rates while preserving the tree structure.",2022-11-21 11:48:30+00:00,Learning on tree architectures outperforms a convolutional feedforward network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuval Meir'), arxiv.Result.Author('Itamar Ben-Noam'), arxiv.Result.Author('Yarden Tzach'), arxiv.Result.Author('Shiri Hodassman'), arxiv.Result.Author('Ido Kanter')]","Advanced deep learning architectures consist of tens of fully connected and
convolutional hidden layers, which are already extended to hundreds, and are
far from their biological realization. Their implausible biological dynamics is
based on changing a weight in a non-local manner, as the number of routes
between an output unit and a weight is typically large, using the
backpropagation technique. Here, offline and online CIFAR-10 database learning
on 3-layer tree architectures, inspired by experimental-based dendritic tree
adaptations, outperforms the achievable success rates of the 5-layer
convolutional LeNet. Its highly pruning tree backpropagation procedure, where a
single route connects an output unit and a weight, represents an efficient
dendritic deep learning.",0.12938793,-0.023709953,0.31050447,A
14002,"The possibility that large-scale and deeper tree architectures, with an
extended number of branches and filters, can compete with state-of-the-art CIFAR-10
success rates deserves further research.","Hence, it is unnecessary to equalize the filters among the branches after
each BP step.","The first step is exemplified with respect to
LeNet-5 and demonstrates the importance of the concept of dendritic learning and its
powerful computation.",2022-11-21 11:48:30+00:00,Learning on tree architectures outperforms a convolutional feedforward network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuval Meir'), arxiv.Result.Author('Itamar Ben-Noam'), arxiv.Result.Author('Yarden Tzach'), arxiv.Result.Author('Shiri Hodassman'), arxiv.Result.Author('Ido Kanter')]","Advanced deep learning architectures consist of tens of fully connected and
convolutional hidden layers, which are already extended to hundreds, and are
far from their biological realization. Their implausible biological dynamics is
based on changing a weight in a non-local manner, as the number of routes
between an output unit and a weight is typically large, using the
backpropagation technique. Here, offline and online CIFAR-10 database learning
on 3-layer tree architectures, inspired by experimental-based dendritic tree
adaptations, outperforms the achievable success rates of the 5-layer
convolutional LeNet. Its highly pruning tree backpropagation procedure, where a
single route connects an output unit and a weight, represents an efficient
dendritic deep learning.",0.203121,-0.15728417,0.13218744,A
14003,The optimization of the success rates on the M and K grid deserves further research.,"Indeed, the
success rate increases from ~0.765 for M = 16 to ~0.79 for M = 80 (Table 1, upper panel).",The Tree-3 architecture (Fig.,2022-11-21 11:48:30+00:00,Learning on tree architectures outperforms a convolutional feedforward network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuval Meir'), arxiv.Result.Author('Itamar Ben-Noam'), arxiv.Result.Author('Yarden Tzach'), arxiv.Result.Author('Shiri Hodassman'), arxiv.Result.Author('Ido Kanter')]","Advanced deep learning architectures consist of tens of fully connected and
convolutional hidden layers, which are already extended to hundreds, and are
far from their biological realization. Their implausible biological dynamics is
based on changing a weight in a non-local manner, as the number of routes
between an output unit and a weight is typically large, using the
backpropagation technique. Here, offline and online CIFAR-10 database learning
on 3-layer tree architectures, inspired by experimental-based dendritic tree
adaptations, outperforms the achievable success rates of the 5-layer
convolutional LeNet. Its highly pruning tree backpropagation procedure, where a
single route connects an output unit and a weight, represents an efficient
dendritic deep learning.",0.47898248,0.14312197,0.08340371,A
14004,"This
possible advantage of shallow tree architectures in minimizing the effect of vanishing or
exploding gradients, calls for their quantitative examination in further research.","Even if the gradient of each route is small, the summation can be
large and lead to weight explosion, a reality which is excluded in tree architectures.","The introduction of parallel branches instead of the second convolutional layer in LeNet-
5 enhances success rates while preserving the tree structure.",2022-11-21 11:48:30+00:00,Learning on tree architectures outperforms a convolutional feedforward network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuval Meir'), arxiv.Result.Author('Itamar Ben-Noam'), arxiv.Result.Author('Yarden Tzach'), arxiv.Result.Author('Shiri Hodassman'), arxiv.Result.Author('Ido Kanter')]","Advanced deep learning architectures consist of tens of fully connected and
convolutional hidden layers, which are already extended to hundreds, and are
far from their biological realization. Their implausible biological dynamics is
based on changing a weight in a non-local manner, as the number of routes
between an output unit and a weight is typically large, using the
backpropagation technique. Here, offline and online CIFAR-10 database learning
on 3-layer tree architectures, inspired by experimental-based dendritic tree
adaptations, outperforms the achievable success rates of the 5-layer
convolutional LeNet. Its highly pruning tree backpropagation procedure, where a
single route connects an output unit and a weight, represents an efficient
dendritic deep learning.",0.12938793,-0.023709953,0.31050447,A
14005,"The possibility that large-scale and deeper tree architectures, with an
extended number of branches and filters, can compete with state-of-the-art CIFAR-10
success rates deserves further research.","Hence, it is unnecessary to equalize the filters among the branches after
each BP step.","The first step is exemplified with respect to
LeNet-5 and demonstrates the importance of the concept of dendritic learning and its
powerful computation.",2022-11-21 11:48:30+00:00,Learning on tree architectures outperforms a convolutional feedforward network,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yuval Meir'), arxiv.Result.Author('Itamar Ben-Noam'), arxiv.Result.Author('Yarden Tzach'), arxiv.Result.Author('Shiri Hodassman'), arxiv.Result.Author('Ido Kanter')]","Advanced deep learning architectures consist of tens of fully connected and
convolutional hidden layers, which are already extended to hundreds, and are
far from their biological realization. Their implausible biological dynamics is
based on changing a weight in a non-local manner, as the number of routes
between an output unit and a weight is typically large, using the
backpropagation technique. Here, offline and online CIFAR-10 database learning
on 3-layer tree architectures, inspired by experimental-based dendritic tree
adaptations, outperforms the achievable success rates of the 5-layer
convolutional LeNet. Its highly pruning tree backpropagation procedure, where a
single route connects an output unit and a weight, represents an efficient
dendritic deep learning.",0.203121,-0.15728417,0.13218744,A
14015,"We further study the
of OpenDet benchmark [4], we notice that VOC-COCO-               design choices of PLN related parameters.",From the construction               Prototype learning network(PLN).,"SpeciÔ¨Åcally, PLN
20000 contains 4952 close-set images from PASCAL VOC
[11] and 20000 open-set images from COCO [12].",2022-11-21 15:00:04+00:00,Open-Set Object Detection Using Classification-free Object Proposal and Instance-level Contrastive Learning with Appendix,cs.CV,"['cs.CV', 'cs.RO']","[arxiv.Result.Author('Zhongxiang Zhou'), arxiv.Result.Author('Yifei Yang'), arxiv.Result.Author('Yue Wang'), arxiv.Result.Author('Rong Xiong')]","Detecting both known and unknown objects is a fundamental skill for robot
manipulation in unstructured environments. Open-set object detection (OSOD) is
a promising direction to handle the problem consisting of two subtasks: objects
and background separation, and open-set object classification. In this paper,
we present Openset RCNN to address the challenging OSOD. To disambiguate
unknown objects and background in the first subtask, we propose to use
classification-free region proposal network (CF-RPN) which estimates the
objectness score of each region purely using cues from object's location and
shape preventing overfitting to the training categories. To identify unknown
objects in the second subtask, we propose to represent them using the
complementary region of known categories in a latent space which is
accomplished by a prototype learning network (PLN). PLN performs instance-level
contrastive learning to encode proposals to a latent space and builds a compact
region centering with a prototype for each known category. Further, we note
that the detection performance of unknown objects can not be unbiasedly
evaluated on the situation that commonly used object detection datasets are not
fully annotated. Thus, a new benchmark is introduced by reorganizing
GraspNet-1billion, a robotic grasp pose detection dataset with complete
annotation. Extensive experiments demonstrate the merits of our method. We
finally show that our Openset RCNN can endow the robot with an open-set
perception ability to support robotic rearrangement tasks in cluttered
environments. More details can be found in
https://sites.google.com/view/openest-rcnn/",-0.07751341,-0.1584063,0.1994699,C
14017,"For
tween two groups, which could include details such as pos-              further research, removing the requirement of neutral face
ture, gaze direction, tears, etc.",The network learns the differences be-                 its expression transfer quality is comparable to them.,"labels and testing the network on more diverse situations,
                                                                        such as videos can be explored.",2022-11-18 11:10:44+00:00,2CET-GAN: Pixel-Level GAN Model for Human Facial Expression Transfer,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaohang Hu'), arxiv.Result.Author('Nuha Aldausari'), arxiv.Result.Author('Gelareh Mohammadi')]","Recent studies have used GAN to transfer expressions between human faces.
However, existing models have many flaws: relying on emotion labels, lacking
continuous expressions, and failing to capture the expression details. To
address these limitations, we propose a novel CycleGAN- and InfoGAN-based
network called 2 Cycles Expression Transfer GAN (2CET-GAN), which can learn
continuous expression transfer without using emotion labels. The experiment
shows our network can generate diverse and high-quality expressions and can
generalize to unknown identities. To the best of our knowledge, we are among
the first to successfully use an unsupervised approach to disentangle
expression representation from identities at the pixel level.",0.08126839,-0.24529064,-0.16524932,C
14019,"At a conceptual level, we hope
                               4                    8 Upsampling factor16                         32       that our work motivates further research about the combi-
                                                                                                           nation of classical optimization-based computer vision and
                                  (b) RMSE (cm) of learning-free methods                                   modern, deep learning-based image representations.","This could potentially make it possible to
                                                                                                           back-propagate through the entire diffusion process with the
                      2.5                                                                                  adjoint sensitivity method [32], rather than unrolling a lim-
                                                                                                           ited number of iterations.","In line
                                                                                                           with others [5,33], our work suggests that such hybrid meth-
Figure 7.",2022-11-21 15:48:13+00:00,Guided Depth Super-Resolution by Deep Anisotropic Diffusion,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Nando Metzger'), arxiv.Result.Author('Rodrigo Caye Daudt'), arxiv.Result.Author('Konrad Schindler')]","Performing super-resolution of a depth image using the guidance from an RGB
image is a problem that concerns several fields, such as robotics, medical
imaging, and remote sensing. While deep learning methods have achieved good
results in this problem, recent work highlighted the value of combining modern
methods with more formal frameworks. In this work, we propose a novel approach
which combines guided anisotropic diffusion with a deep convolutional network
and advances the state of the art for guided depth super-resolution. The edge
transferring/enhancing properties of the diffusion are boosted by the
contextual reasoning capabilities of modern networks, and a strict adjustment
step guarantees perfect adherence to the source image. We achieve unprecedented
results in three commonly used benchmarks for guided depth super-resolution.
The performance gain compared to other methods is the largest at larger scales,
such as x32 scaling. Code for the proposed method will be made available to
promote reproducibility of our results.",-0.07226212,-0.092748046,0.3068202,C
14020,"At a conceptual level, we hope
                               4                    8 Upsampling factor16                         32       that our work motivates further research about the combi-
                                                                                                           nation of classical optimization-based computer vision and
                                  (b) RMSE (cm) of learning-free methods                                   modern, deep learning-based image representations.","This could potentially make it possible to
                                                                                                           back-propagate through the entire diffusion process with the
                      2.5                                                                                  adjoint sensitivity method [32], rather than unrolling a lim-
                                                                                                           ited number of iterations.","In line
                                                                                                           with others [5,33], our work suggests that such hybrid meth-
Figure 7.",2022-11-21 15:48:13+00:00,Guided Depth Super-Resolution by Deep Anisotropic Diffusion,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Nando Metzger'), arxiv.Result.Author('Rodrigo Caye Daudt'), arxiv.Result.Author('Konrad Schindler')]","Performing super-resolution of a depth image using the guidance from an RGB
image is a problem that concerns several fields, such as robotics, medical
imaging, and remote sensing. While deep learning methods have achieved good
results in this problem, recent work highlighted the value of combining modern
methods with more formal frameworks. In this work, we propose a novel approach
which combines guided anisotropic diffusion with a deep convolutional network
and advances the state of the art for guided depth super-resolution. The edge
transferring/enhancing properties of the diffusion are boosted by the
contextual reasoning capabilities of modern networks, and a strict adjustment
step guarantees perfect adherence to the source image. We achieve unprecedented
results in three commonly used benchmarks for guided depth super-resolution.
The performance gain compared to other methods is the largest at larger scales,
such as x32 scaling. Code for the proposed method will be made available to
promote reproducibility of our results.",-0.07226212,-0.092748046,0.3068202,C
14023,"guide further research in this area, we provide a list of ideas
we explored but did not work out as expected.",To           to Pix2NeRF [4] on input and random views at 128√ó128.,"‚Ä¢ We initially experimented with various NeRF representa-
  tions, including MLP-based, voxel-based, and triplanar-
  based.",2022-11-21 17:42:42+00:00,"Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion",cs.CV,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dario Pavllo'), arxiv.Result.Author('David Joseph Tan'), arxiv.Result.Author('Marie-Julie Rakotosaona'), arxiv.Result.Author('Federico Tombari')]","Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.",0.018592786,0.16073537,0.10484193,B
14042,"To address this, we propose a method for generating
                                        synthetic image data that are labelled for semantic segmen-
                                        tation, generalizable to other tasks, and provide a prototype
                                        synthetic image dataset consisting of 2D monocular images of
                                        unmanned spacecraft, in order to enable further research in the
                                        area of autonomous spacecraft rendezvous.","However, recent studies
                                        show that these strong results in broad and common domains
                                        may generalize poorly even to speciÔ¨Åc industrial applications on
                                        earth.","We also present a
                                        strong benchmark result (S√∏rensen-Dice coefÔ¨Åcient 0.8723) on
                                        these synthetic data, suggesting that it is feasible to train well-
                                        performing image segmentation models for this task, especially
                                        if the target spacecraft and its conÔ¨Åguration are known.",2022-11-22 01:30:40+00:00,Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('William S. Armstrong'), arxiv.Result.Author('Spencer Drakontaidis'), arxiv.Result.Author('Nicholas Lui')]","Images of spacecraft photographed from other spacecraft operating in outer
space are difficult to come by, especially at a scale typically required for
deep learning tasks. Semantic image segmentation, object detection and
localization, and pose estimation are well researched areas with powerful
results for many applications, and would be very useful in autonomous
spacecraft operation and rendezvous. However, recent studies show that these
strong results in broad and common domains may generalize poorly even to
specific industrial applications on earth. To address this, we propose a method
for generating synthetic image data that are labelled for semantic
segmentation, generalizable to other tasks, and provide a prototype synthetic
image dataset consisting of 2D monocular images of unmanned spacecraft, in
order to enable further research in the area of autonomous spacecraft
rendezvous. We also present a strong benchmark result (S{\o}rensen-Dice
coefficient 0.8723) on these synthetic data, suggesting that it is feasible to
train well-performing image segmentation models for this task, especially if
the target spacecraft and its configuration are known.",-0.30973825,0.052654356,0.026087064,B
14043,"[1] express concerns about the availability of
                                        same, with the ultimate goal of enabling further research in                                         training data at scale for domain-speciÔ¨Åc object recognition
                                        the area of autonomous spacecraft rendezvous.","of unmanned spacecraft, and are endeavouring to train a
                                        performant deep learning image segmentation model using the                                          Wong et al.","tasks, and propose a method of procedurally generating large
                                                                                                                                             image datasets from 3D models based on a small number of
                                        978-1-6654-9032-0/23/$31.00 ¬©2023 IEEE                                                               physical examples, labelled for image classiÔ¨Åcation.",2022-11-22 01:30:40+00:00,Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('William S. Armstrong'), arxiv.Result.Author('Spencer Drakontaidis'), arxiv.Result.Author('Nicholas Lui')]","Images of spacecraft photographed from other spacecraft operating in outer
space are difficult to come by, especially at a scale typically required for
deep learning tasks. Semantic image segmentation, object detection and
localization, and pose estimation are well researched areas with powerful
results for many applications, and would be very useful in autonomous
spacecraft operation and rendezvous. However, recent studies show that these
strong results in broad and common domains may generalize poorly even to
specific industrial applications on earth. To address this, we propose a method
for generating synthetic image data that are labelled for semantic
segmentation, generalizable to other tasks, and provide a prototype synthetic
image dataset consisting of 2D monocular images of unmanned spacecraft, in
order to enable further research in the area of autonomous spacecraft
rendezvous. We also present a strong benchmark result (S{\o}rensen-Dice
coefficient 0.8723) on these synthetic data, suggesting that it is feasible to
train well-performing image segmentation models for this task, especially if
the target spacecraft and its configuration are known.",-0.33774728,-0.034480095,0.07475355,B
14046,"To further study the eÔ¨Äectiveness of WSP, we compare the results of
WSP and PoseNet [11] on MuPoTS-3D datasets with more strict thresholds
(100mm, 110mm, 120mm, 130mm, 140mm, and 150mm) since 150mm is a
broad threshold.","As shown in Table 6,
WSP outperforms state-of-the-art methods in 3DPCK with a threshold of
150mm.",The results are shown in Table 7.,2022-11-22 03:35:15+00:00,Weakly-supervised Pre-training for 3D Human Pose Estimation via Perspective Knowledge,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhongwei Qiu'), arxiv.Result.Author('Kai Qiu'), arxiv.Result.Author('Jianlong Fu'), arxiv.Result.Author('Dongmei Fu')]","Modern deep learning-based 3D pose estimation approaches require plenty of 3D
pose annotations. However, existing 3D datasets lack diversity, which limits
the performance of current methods and their generalization ability. Although
existing methods utilize 2D pose annotations to help 3D pose estimation, they
mainly focus on extracting 2D structural constraints from 2D poses, ignoring
the 3D information hidden in the images. In this paper, we propose a novel
method to extract weak 3D information directly from 2D images without 3D pose
supervision. Firstly, we utilize 2D pose annotations and perspective prior
knowledge to generate the relationship of that keypoint is closer or farther
from the camera, called relative depth. We collect a 2D pose dataset (MCPC) and
generate relative depth labels. Based on MCPC, we propose a weakly-supervised
pre-training (WSP) strategy to distinguish the depth relationship between two
points in an image. WSP enables the learning of the relative depth of two
keypoints on lots of in-the-wild images, which is more capable of predicting
depth and generalization ability for 3D human pose estimation. After
fine-tuning on 3D pose datasets, WSP achieves state-of-the-art results on two
widely-used benchmarks.",-0.107917376,0.18868011,0.20231678,B
14070,"We further study the effects of the stochas-
images.","the scores are computed between the generated LDR images
(i.e.,, output of the camera model) and the LDR training                                                         Stochastic CRF.",We also compute the dynamic range (DR) for each                                                          tic CRF sampling process in our model.,2022-11-22 15:42:08+00:00,GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Chao Wang'), arxiv.Result.Author('Ana Serrano'), arxiv.Result.Author('Xingang Pan'), arxiv.Result.Author('Bin Chen'), arxiv.Result.Author('Hans-Peter Seidel'), arxiv.Result.Author('Christian Theobalt'), arxiv.Result.Author('Karol Myszkowski'), arxiv.Result.Author('Thomas Leimkuehler')]","Most in-the-wild images are stored in Low Dynamic Range (LDR) form, serving
as a partial observation of the High Dynamic Range (HDR) visual world. Despite
limited dynamic range, these LDR images are often captured with different
exposures, implicitly containing information about the underlying HDR image
distribution. Inspired by this intuition, in this work we present, to the best
of our knowledge, the first method for learning a generative model of HDR
images from in-the-wild LDR image collections in a fully unsupervised manner.
The key idea is to train a generative adversarial network (GAN) to generate HDR
images which, when projected to LDR under various exposures, are
indistinguishable from real LDR images. The projection from HDR to LDR is
achieved via a camera model that captures the stochasticity in exposure and
camera response function. Experiments show that our method GlowGAN can
synthesize photorealistic HDR images in many challenging cases such as
landscapes, lightning, or windows, where previous supervised generative models
produce overexposed images. We further demonstrate the new application of
unsupervised inverse tone mapping (ITM) enabled by GlowGAN. Our ITM method does
not need HDR images or paired multi-exposure images for training, yet it
reconstructs more plausible information for overexposed regions than
state-of-the-art supervised learning models trained on such data.",0.03573876,0.18938461,0.05123865,B
14071,"We further study the effects of the stochas-
images.","the scores are computed between the generated LDR images
(i.e.,, output of the camera model) and the LDR training                                                         Stochastic CRF.",We also compute the dynamic range (DR) for each                                                          tic CRF sampling process in our model.,2022-11-22 15:42:08+00:00,GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Chao Wang'), arxiv.Result.Author('Ana Serrano'), arxiv.Result.Author('Xingang Pan'), arxiv.Result.Author('Bin Chen'), arxiv.Result.Author('Hans-Peter Seidel'), arxiv.Result.Author('Christian Theobalt'), arxiv.Result.Author('Karol Myszkowski'), arxiv.Result.Author('Thomas Leimkuehler')]","Most in-the-wild images are stored in Low Dynamic Range (LDR) form, serving
as a partial observation of the High Dynamic Range (HDR) visual world. Despite
limited dynamic range, these LDR images are often captured with different
exposures, implicitly containing information about the underlying HDR image
distribution. Inspired by this intuition, in this work we present, to the best
of our knowledge, the first method for learning a generative model of HDR
images from in-the-wild LDR image collections in a fully unsupervised manner.
The key idea is to train a generative adversarial network (GAN) to generate HDR
images which, when projected to LDR under various exposures, are
indistinguishable from real LDR images. The projection from HDR to LDR is
achieved via a camera model that captures the stochasticity in exposure and
camera response function. Experiments show that our method GlowGAN can
synthesize photorealistic HDR images in many challenging cases such as
landscapes, lightning, or windows, where previous supervised generative models
produce overexposed images. We further demonstrate the new application of
unsupervised inverse tone mapping (ITM) enabled by GlowGAN. Our ITM method does
not need HDR images or paired multi-exposure images for training, yet it
reconstructs more plausible information for overexposed regions than
state-of-the-art supervised learning models trained on such data.",0.03573876,0.18938461,0.05123865,B
14076,"However,
provides diverse representations, including 2D/3D pose and           they all project 3D point cloud into image representation for
3D Mesh & SMPL, in order to promote further research.","Some works [1, 6] explore view-robust gait recog-
from 2D silhouettes and 3D point clouds, our dataset also            nition frameworks based on LiDAR point cloud.","feature extraction, which loses the original 3D geometry in-
                                                                     formation and gets limited performance.",2022-11-22 16:05:58+00:00,LiCamGait: Gait Recognition in the Wild by Using LiDAR and Camera Multi-modal Visual Sensors,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiao Han'), arxiv.Result.Author('Peishan Cong'), arxiv.Result.Author('Lan Xu'), arxiv.Result.Author('Jingya Wang'), arxiv.Result.Author('Jingyi Yu'), arxiv.Result.Author('Yuexin Ma')]","LiDAR can capture accurate depth information in large-scale scenarios without
the effect of light conditions, and the captured point cloud contains
gait-related 3D geometric properties and dynamic motion characteristics. We
make the first attempt to leverage LiDAR to remedy the limitation of
view-dependent and light-sensitive camera for more robust and accurate gait
recognition. In this paper, we propose a LiDAR-camera-based gait recognition
method with an effective multi-modal feature fusion strategy, which fully
exploits advantages of both point clouds and images. In particular, we propose
a new in-the-wild gait dataset, LiCamGait, involving multi-modal visual data
and diverse 2D/3D representations. Our method achieves state-of-the-art
performance on the new dataset. Code and dataset will be released when this
paper is published.",-0.2649802,0.33398172,-0.046793718,B
14104,"Effective debiased self-
StillMix is quite different from ActorCutMix or FAME, i.e.,           supervised pretraining deserves further research attention.","Model Augmentation   HMDB51          HMDB51-SCUB (‚Üë)                                HMDB51-SCUF (‚Üì)

                       70.39  Place365 VQGAN-CLIP Sinusoid                   Place365 VQGAN-CLIP Sinusoid
                       72.00
 TSM    No             70.72  45.09                            42.16  26.84  23.26  20.03            14.40
Swin-T  Mixup          71.22  46.25                            44.07  28.96  22.60  19.92            14.71
        VideoMix       70.52  42.68                            41.46  23.00  20.98  18.99            12.46
        BE             70.39  45.39                            42.81  27.25  23.42  20.52            14.40
        ActorCutMix    72.15  45.81                            42.32  27.08  23.29  22.43            15.12
        FAME                  51.25                            53.21  36.33  26.04  23.34            17.60
        StillMix       73.92  53.92                            52.21  38.22  11.89  8.44             5.56
                       74.58
        No             73.31  47.61                            42.77  41.41  20.68  17.90            22.80
        Mixup          74.31  46.70                            42.49  40.12  21.25  18.47            23.78
        VideoMix       74.05  41.33                            38.18  38.67  19.64  18.82            22.85
        BE             73.79  47.36                            42.94  40.39  20.91  17.55            21.41
        ActorCutMix    75.25  50.13                            46.51  43.73  22.16  20.26            23.80
        FAME                  54.71                            53.67  45.81  27.10  27.26            26.40
        StillMix              53.82                            52.49  50.02  13.04  13.13            14.97

ments on SCUF show that the improvements on SCUB of                   pretraining at mitigating static bias.","StillMix focuses on motion features instead of foreground
static features.",2022-11-23 11:40:02+00:00,Evaluating and Mitigating Static Bias of Action Representations in the Background and the Foreground,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haoxin Li'), arxiv.Result.Author('Yue Wu'), arxiv.Result.Author('Yuan Liu'), arxiv.Result.Author('Hanwang Zhang'), arxiv.Result.Author('Boyang Li')]","Deep neural networks for video action recognition easily learn to utilize
shortcut static features, such as background and objects instead of motion
features. This results in poor generalization to atypical videos such as soccer
playing on concrete surfaces (instead of soccer fields). However, due to the
rarity of out-of-distribution (OOD) data, quantitative evaluation of static
bias remains a difficult task. In this paper, we synthesize new sets of
benchmarks to evaluate static bias of action representations, including SCUB
for static cues in the background, and SCUF for static cues in the foreground.
Further, we propose a simple yet effective video data augmentation technique,
StillMix, that automatically identifies bias-inducing video frames; unlike
similar augmentation techniques, StillMix does not need to enumerate or
precisely segment biased content. With extensive experiments, we quantitatively
compare and analyze existing action recognition models on the created
benchmarks to reveal their characteristics. We validate the effectiveness of
StillMix and show that it improves TSM (Lin, Gan, and Han 2021) and Video Swin
Transformer (Liu et al. 2021) by more than 10% of accuracy on SCUB for OOD
action recognition.",0.2523038,-0.06304146,-0.071321234,A
14105,"Imagenet: A large-scale hierarchi-
objects revealing a direction for further research.","We found that the performance of these mod-          [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
els on attributes stays clearly behind their performance on             Li, and Li Fei-Fei.",cal image database.,2022-11-23 12:34:43+00:00,Open-vocabulary Attribute Detection,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Mar√≠a A. Bravo'), arxiv.Result.Author('Sudhanshu Mittal'), arxiv.Result.Author('Simon Ging'), arxiv.Result.Author('Thomas Brox')]","Vision-language modeling has enabled open-vocabulary tasks where predictions
can be queried using any text prompt in a zero-shot manner. Existing
open-vocabulary tasks focus on object classes, whereas research on object
attributes is limited due to the lack of a reliable attribute-focused
evaluation benchmark. This paper introduces the Open-Vocabulary Attribute
Detection (OVAD) task and the corresponding OVAD benchmark. The objective of
the novel task and benchmark is to probe object-level attribute information
learned by vision-language models. To this end, we created a clean and densely
annotated test set covering 117 attribute classes on the 80 object classes of
MS COCO. It includes positive and negative annotations, which enables
open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million
annotations. For reference, we provide a first baseline method for
open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's
value by studying the attribute detection performance of several foundation
models. Project page https://ovad-benchmark.github.io/",-0.1476119,-0.08929252,0.034249745,C
14114,"However, like most existing 3D detectors, it may produce errors in some edge
cases, due to the limited data, so further research is still needed to improve its robustness.","Our proposed framework can provide better 3D object detection performance for
autonomous vehicles.",Acknowledgements.,2022-11-23 16:01:06+00:00,Sparse2Dense: Learning to Densify 3D Features for 3D Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Tianyu Wang'), arxiv.Result.Author('Xiaowei Hu'), arxiv.Result.Author('Zhengzhe Liu'), arxiv.Result.Author('Chi-Wing Fu')]","LiDAR-produced point clouds are the major source for most state-of-the-art 3D
object detectors. Yet, small, distant, and incomplete objects with sparse or
few points are often hard to detect. We present Sparse2Dense, a new framework
to efficiently boost 3D detection performance by learning to densify point
clouds in latent space. Specifically, we first train a dense point 3D detector
(DDet) with a dense point cloud as input and design a sparse point 3D detector
(SDet) with a regular point cloud as input. Importantly, we formulate the
lightweight plug-in S2D module and the point cloud reconstruction module in
SDet to densify 3D features and train SDet to produce 3D features, following
the dense 3D features in DDet. So, in inference, SDet can simulate dense 3D
features from regular (sparse) point cloud inputs without requiring dense
inputs. We evaluate our method on the large-scale Waymo Open Dataset and the
Waymo Domain Adaptation Dataset, showing its high performance and efficiency
over the state of the arts.",-0.37015635,0.32301337,0.017494034,B
14120,"MIDI files were          representations are both designed to be fed into deep neural
converted to a simple text file format using the MSQ tool          networks for end-to-end composer identification and to serve
[28, 30], where each note is represented using a text symbol,      as a reference for further research.","These two
by using trained Feedforward Neural Networks, and analyses         sets, the original images of sheet music and the bootleg
the DTV‚Äôs ability to identify a composer.","Half of the scores were
which preserves information about its duration, onset time,        collected from IMSLP, one third score images were created by
pitch, and velocity.",2022-11-23 20:16:45+00:00,Proceedings of the 4th International Workshop on Reading Music Systems,cs.CV,"['cs.CV', 'cs.IR', 'cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Jorge Calvo-Zaragoza'), arxiv.Result.Author('Alexander Pacha'), arxiv.Result.Author('Elona Shatri')]","The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 4th International Workshop on Reading Music
Systems, held online on Nov. 18th 2022.",0.036782272,-0.21067616,-0.026315596,C
14121,"represents a starting point for further research in this important
stage of the OMR.",We hope that this work                   system.,III.,2022-11-23 20:16:45+00:00,Proceedings of the 4th International Workshop on Reading Music Systems,cs.CV,"['cs.CV', 'cs.IR', 'cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Jorge Calvo-Zaragoza'), arxiv.Result.Author('Alexander Pacha'), arxiv.Result.Author('Elona Shatri')]","The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 4th International Workshop on Reading Music
Systems, held online on Nov. 18th 2022.",0.33163583,0.12268126,-0.22097364,A
14130,"In OSDI,
ducted a further study to analyse two other computer            volume 16, pages 265‚Äì283, 2016.
vision technologies, applied for recognition in utility
meter readings:                                            [2] Mart¬¥ƒ±n Abadi, Michael Isard, and Derek G Mur-
                                                                ray.","For this reasons, we con-               tem for large-scale machine learning.","A computational model for tensorÔ¨Çow: an
 ‚Ä¢ an open-source TensorÔ¨Çow technique (FRCNN,                   introduction.",2022-11-24 09:05:52+00:00,Towards computer vision technologies: Semi-automated reading of automated utility meters,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maria Spichkova'), arxiv.Result.Author('Johan van Zyl')]","In this report we analysed a possibility of using computer vision techniques
for automated reading of utility meters. In our study, we focused on two
computer vision techniques: an open-source solution Tensorflow Object Detection
(Tensorflow) and a commercial solution Anyline. This report extends our
previous publication: We start with presentation of a structured analysis of
related approaches. After that we provide a detailed comparison of two computer
vision technologies, Tensorflow Object Detection (Tensorflow) and Anyline,
applied to semi-automated reading of utility meters. In this paper, we discuss
limitations and benefits of each solution applied to utility meters reading,
especially focusing on aspects such as accuracy and inference time. Our goal
was to determine the solution that is the most suitable for this particular
application area, where there are several specific challenges.",-0.06763589,-0.10820985,-0.0019007102,C
14141,"6 Conclusion and Future Work

This work conducts further research on explainable face veriÔ¨Åcation and proposes a novel strategy to generate three
different explanation maps and a conÔ¨Ådence score for a face veriÔ¨Åcation model‚Äôs prediction.","3) The platform presents
results for a small portion of existing face recognition networks.","With our eXplanable face veriÔ¨Åcation platform, we contribute a tool to further investigate the behavior of state-of-the-art
face recognition networks and demonstrate the interpretability and accuracy of our approach.",2022-11-24 17:52:47+00:00,Explainable Model-Agnostic Similarity and Confidence in Face Verification,cs.CV,['cs.CV'],"[arxiv.Result.Author('Martin Knoche'), arxiv.Result.Author('Torben Teepe'), arxiv.Result.Author('Stefan H√∂rmann'), arxiv.Result.Author('Gerhard Rigoll')]","Recently, face recognition systems have demonstrated remarkable performances
and thus gained a vital role in our daily life. They already surpass human face
verification accountability in many scenarios. However, they lack explanations
for their predictions. Compared to human operators, typical face recognition
network system generate only binary decisions without further explanation and
insights into those decisions. This work focuses on explanations for face
recognition systems, vital for developers and operators. First, we introduce a
confidence score for those systems based on facial feature distances between
two input images and the distribution of distances across a dataset. Secondly,
we establish a novel visualization approach to obtain more meaningful
predictions from a face recognition system, which maps the distance deviation
based on a systematic occlusion of images. The result is blended with the
original images and highlights similar and dissimilar facial regions. Lastly,
we calculate confidence scores and explanation maps for several
state-of-the-art face verification datasets and release the results on a web
platform. We optimize the platform for a user-friendly interaction and hope to
further improve the understanding of machine learning decisions. The source
code is available on GitHub, and the web platform is publicly available at
http://explainable-face-verification.ey.r.appspot.com.",-0.14620885,-0.12600933,-0.14811754,C
14144,"the stand-alone method needs 1750 ms.

D. Dynamic model selection with distributed inference                                                      We further study the effect of transmission latency on the
                                                                                                        failure probability of the task.","Distributed inference reaches
method without task partitioning can achieve an accuracy of                                             its maximum accuracy at 1100 ms when using VGG-16, while
65% by running a model with higher prunning rate.","The image size of the arriving
    Time-critical IoT applications using CNN are required to                                            tasks follows a Gaussian distribution with a mean of 300 KB
complete inference within latency constraints, otherwise, the                                           and a variance of 50 KB.",2022-11-24 19:48:30+00:00,Design and Prototyping Distributed CNN Inference Acceleration in Edge Computing,cs.CV,"['cs.CV', 'cs.DC']","[arxiv.Result.Author('hongtian Dong'), arxiv.Result.Author('Nan Li'), arxiv.Result.Author('Alexandros Iosifidis'), arxiv.Result.Author('Qi Zhang')]","For time-critical IoT applications using deep learning, inference
acceleration through distributed computing is a promising approach to meet a
stringent deadline. In this paper, we implement a working prototype of a new
distributed inference acceleration method HALP using three raspberry Pi 4. HALP
accelerates inference by designing a seamless collaboration among edge devices
(EDs) in Edge Computing. We maximize the parallelization between communication
and computation among the collaborative EDs by optimizing the task partitioning
ratio based on the segment-based partitioning. Experimental results show that
the distributed inference HALP achieves 1.7x inference acceleration for VGG-16.
Then, we combine distributed inference with conventional neural network model
compression by setting up different shrinking hyperparameters for MobileNet-V1.
In this way, we can further accelerate inference but at the cost of inference
accuracy loss. To strike a balance between latency and accuracy, we propose
dynamic model selection to select a model which provides the highest accuracy
within the latency constraint. It is shown that the model selection with
distributed inference HALP can significantly improve service reliability
compared to the conventional stand-alone computation.",0.037363954,-0.08387138,0.16538277,C
14145,"the stand-alone method needs 1750 ms.

D. Dynamic model selection with distributed inference                                                      We further study the effect of transmission latency on the
                                                                                                        failure probability of the task.","Distributed inference reaches
method without task partitioning can achieve an accuracy of                                             its maximum accuracy at 1100 ms when using VGG-16, while
65% by running a model with higher prunning rate.","The image size of the arriving
    Time-critical IoT applications using CNN are required to                                            tasks follows a Gaussian distribution with a mean of 300 KB
complete inference within latency constraints, otherwise, the                                           and a variance of 50 KB.",2022-11-24 19:48:30+00:00,Design and Prototyping Distributed CNN Inference Acceleration in Edge Computing,cs.CV,"['cs.CV', 'cs.DC']","[arxiv.Result.Author('Zhongtian Dong'), arxiv.Result.Author('Nan Li'), arxiv.Result.Author('Alexandros Iosifidis'), arxiv.Result.Author('Qi Zhang')]","For time-critical IoT applications using deep learning, inference
acceleration through distributed computing is a promising approach to meet a
stringent deadline. In this paper, we implement a working prototype of a new
distributed inference acceleration method HALP using three raspberry Pi 4. HALP
accelerates inference by designing a seamless collaboration among edge devices
(EDs) in Edge Computing. We maximize the parallelization between communication
and computation among the collaborative EDs by optimizing the task partitioning
ratio based on the segment-based partitioning. Experimental results show that
the distributed inference HALP achieves 1.7x inference acceleration for VGG-16.
Then, we combine distributed inference with conventional neural network model
compression by setting up different shrinking hyperparameters for MobileNet-V1.
In this way, we can further accelerate inference but at the cost of inference
accuracy loss. To strike a balance between latency and accuracy, we propose
dynamic model selection to select a model which provides the highest accuracy
within the latency constraint. It is shown that the model selection with
distributed inference HALP can significantly improve service reliability
compared to the conventional stand-alone computation.",0.037363954,-0.08387138,0.16538277,C
14146,"This
                                                                     design is based on the fact that each DCT coefÔ¨Åcient makes
   ‚Ä¢ We further study the performance of our proposed Edge           an additive contribution to the received image quality.",state and allow ES to decide when to start inference.,"Intelligence framework using semantic communication
      under the latency and transmission rate constraints.",2022-11-24 20:13:17+00:00,Semantic Communication Enabling Robust Edge Intelligence for Time-Critical IoT Applications,cs.CV,"['cs.CV', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Andrea Cavagn'), arxiv.Result.Author('Nan Li'), arxiv.Result.Author('Alexandros Iosifidis'), arxiv.Result.Author('Qi Zhang')]","This paper aims to design robust Edge Intelligence using semantic
communication for time-critical IoT applications. We systematically analyze the
effect of image DCT coefficients on inference accuracy and propose the
channel-agnostic effectiveness encoding for offloading by transmitting the most
meaningful task data first. This scheme can well utilize all available
communication resource and strike a balance between transmission latency and
inference accuracy. Then, we design an effectiveness decoding by implementing a
novel image augmentation process for convolutional neural network (CNN)
training, through which an original CNN model is transformed into a Robust CNN
model. We use the proposed training method to generate Robust MobileNet-v2 and
Robust ResNet-50. The proposed Edge Intelligence framework consists of the
proposed effectiveness encoding and effectiveness decoding. The experimental
results show that the effectiveness decoding using the Robust CNN models
perform consistently better under various image distortions caused by channel
errors or limited communication resource. The proposed Edge Intelligence
framework using semantic communication significantly outperforms the
conventional approach under latency and data rate constraints, in particular,
under ultra stringent deadlines and low data rate.",-0.0446874,-0.0077196863,0.1939493,C
14147,"This
                                                                     design is based on the fact that each DCT coefÔ¨Åcient makes
   ‚Ä¢ We further study the performance of our proposed Edge           an additive contribution to the received image quality.",state and allow ES to decide when to start inference.,"Intelligence framework using semantic communication
      under the latency and transmission rate constraints.",2022-11-24 20:13:17+00:00,Semantic Communication Enabling Robust Edge Intelligence for Time-Critical IoT Applications,cs.CV,"['cs.CV', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Andrea Cavagna'), arxiv.Result.Author('Nan Li'), arxiv.Result.Author('Alexandros Iosifidis'), arxiv.Result.Author('Qi Zhang')]","This paper aims to design robust Edge Intelligence using semantic
communication for time-critical IoT applications. We systematically analyze the
effect of image DCT coefficients on inference accuracy and propose the
channel-agnostic effectiveness encoding for offloading by transmitting the most
meaningful task data first. This scheme can well utilize all available
communication resource and strike a balance between transmission latency and
inference accuracy. Then, we design an effectiveness decoding by implementing a
novel image augmentation process for convolutional neural network (CNN)
training, through which an original CNN model is transformed into a Robust CNN
model. We use the proposed training method to generate Robust MobileNet-v2 and
Robust ResNet-50. The proposed Edge Intelligence framework consists of the
proposed effectiveness encoding and effectiveness decoding. The experimental
results show that the effectiveness decoding using the Robust CNN models
perform consistently better under various image distortions caused by channel
errors or limited communication resource. The proposed Edge Intelligence
framework using semantic communication significantly outperforms the
conventional approach under latency and data rate constraints, in particular,
under ultra stringent deadlines and low data rate.",-0.0446874,-0.0077196863,0.1939493,C
14150,"In Ap-
pendix E, we provide more details about the database and               A typical face-detection model produces a conÔ¨Ådence
how it can be used for further research and to reproduce our        score that a face exists in a given image.","Setting Hyper-Parameters
designated database published alongside the dataset.","Additionally, co-
results.",2022-11-24 21:41:52+00:00,ReFace: Improving Clothes-Changing Re-Identification With Face Features,cs.CV,['cs.CV'],"[arxiv.Result.Author('Daniel Arkushin'), arxiv.Result.Author('Bar Cohen'), arxiv.Result.Author('Shmuel Peleg'), arxiv.Result.Author('Ohad Fried')]","Person re-identification (ReID) has been an active research field for many
years. Despite that, models addressing this problem tend to perform poorly when
the task is to re-identify the same people over a prolonged time, due to
appearance changes such as different clothes and hairstyles. In this work, we
introduce a new method that takes full advantage of the ability of existing
ReID models to extract appearance-related features and combines it with a face
feature extraction model to achieve new state-of-the-art results, both on
image-based and video-based benchmarks. Moreover, we show how our method could
be used for an application in which multiple people of interest, under
clothes-changing settings, should be re-identified given an unseen video and a
limited amount of labeled data. We claim that current ReID benchmarks do not
represent such real-world scenarios, and publish a new dataset, 42Street, based
on a theater play as an example of such an application. We show that our
proposed method outperforms existing models also on this dataset while using
only pre-trained modules and without any further training.",-0.07366628,0.049824286,-0.09534639,C
14155,"(i) First, we replace Lda with L1da, where instead
further study the reason behind this, we visualize the dis-             of minimizing the MMD loss among teachers and among
tributions of audio and video modalities using Kinetics400.",To              ment.,"students, we aim to minimize the loss between the teach-
Interestingly, as shown in Figure 2, we observe that the dis-           ers and students, as L1da = Lmmd(fta, fsv)+Lmmd(ftv, fsa).",2022-11-25 06:51:35+00:00,XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pritam Sarkar'), arxiv.Result.Author('Ali Etemad')]","We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled video clips. XKD is trained with two pseudo
tasks. First, masked data reconstruction is performed to learn
modality-specific representations. Next, self-supervised cross-modal knowledge
distillation is performed between the two modalities through teacher-student
setups to learn complementary information. To identify the most effective
information to transfer and also to tackle the domain gap between audio and
visual modalities which could hinder knowledge transfer, we introduce a domain
alignment strategy for effective cross-modal distillation. Lastly, to develop a
general-purpose solution capable of handling both audio and visual streams, a
modality-agnostic variant of our proposed framework is introduced, which uses
the same backbone for both audio and visual modalities. Our proposed
cross-modal knowledge distillation improves linear evaluation top-1 accuracy of
video action classification by 8.4% on UCF101, 8.1% on HMDB51, 13.8% on
Kinetics-Sound, and 14.2% on Kinetics400. Additionally, our modality-agnostic
variant shows promising results in developing a general-purpose network capable
of handling different data streams. The code is released on the project
website.",0.29151377,0.006147099,-0.007498743,A
14156,"We Ô¨Ånd that further research
VideoMAE [53] K400 V    ViT-B   32√ó 1122 4√ó 162    392   75.7 92.2                       needs to be done to tackle such challenges, which is out-
                        ViT-B   32√ó 1122 4√ó 162    392   75.9 92.1                       of-scope for this work and we plan to address in the future.","This leads to a degenerated
                        ViT-B   32√ó 1122 4√ó 162    392   77.6 92.9                       solution (results not shown).","XKD          K400 VA                                                                     Lastly, when comparing the performance between the teach-
                        ViT-B   32√ó 2242 4√ó 162    1568  79.6 94.9                       ers and students, teachers show slightly better performance
XKD-MATS K400 VA        ViT-M   32√ó 2242 4√ó 162    1568  81.1 95.6                       on all the benchmarks and in all the setups.",2022-11-25 06:51:35+00:00,XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pritam Sarkar'), arxiv.Result.Author('Ali Etemad')]","We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled video clips. XKD is trained with two pseudo
tasks. First, masked data reconstruction is performed to learn
modality-specific representations. Next, self-supervised cross-modal knowledge
distillation is performed between the two modalities through teacher-student
setups to learn complementary information. To identify the most effective
information to transfer and also to tackle the domain gap between audio and
visual modalities which could hinder knowledge transfer, we introduce a domain
alignment strategy for effective cross-modal distillation. Lastly, to develop a
general-purpose solution capable of handling both audio and visual streams, a
modality-agnostic variant of our proposed framework is introduced, which uses
the same backbone for both audio and visual modalities. Our proposed
cross-modal knowledge distillation improves linear evaluation top-1 accuracy of
video action classification by 8.4% on UCF101, 8.1% on HMDB51, 13.8% on
Kinetics-Sound, and 14.2% on Kinetics400. Additionally, our modality-agnostic
variant shows promising results in developing a general-purpose network capable
of handling different data streams. The code is released on the project
website.",0.2628462,0.12432086,0.051173564,A
14157,"(i) First, we replace Lda with L1da, where instead
further study the reason behind this, we visualize the dis-             of minimizing the MMD loss among teachers and among
tributions of audio and video modalities using Kinetics400.",To              ment.,"students, we aim to minimize the loss between the teach-
Interestingly, as shown in Figure 2, we observe that the dis-           ers and students, as L1da = Lmmd(fta, fsv)+Lmmd(ftv, fsa).",2022-11-25 06:51:35+00:00,XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pritam Sarkar'), arxiv.Result.Author('Ali Etemad')]","We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled video clips. XKD is trained with two pseudo
tasks. First, masked data reconstruction is performed to learn
modality-specific representations. Next, self-supervised cross-modal knowledge
distillation is performed between the two modalities through teacher-student
setups to learn complementary information. To identify the most effective
information to transfer and also to tackle the domain gap between audio and
visual modalities which could hinder knowledge transfer, we introduce a domain
alignment strategy for effective cross-modal distillation. Lastly, to develop a
general-purpose solution capable of handling both audio and visual streams, a
modality-agnostic variant of our proposed framework is introduced, which uses
the same backbone for both audio and visual modalities. Our proposed
cross-modal knowledge distillation improves linear evaluation top-1 accuracy of
video action classification by 8.4% on UCF101, 8.1% on HMDB51, 13.8% on
Kinetics-Sound, and 14.2% on Kinetics400. Additionally, our modality-agnostic
variant shows promising results in developing a general-purpose network capable
of handling different data streams. The code is released on the project
website.",0.29151377,0.006147099,-0.007498743,A
14158,"We Ô¨Ånd that further research
VideoMAE [53] K400 V    ViT-B   32√ó 1122 4√ó 162    392   75.7 92.2                       needs to be done to tackle such challenges, which is out-
                        ViT-B   32√ó 1122 4√ó 162    392   75.9 92.1                       of-scope for this work and we plan to address in the future.","This leads to a degenerated
                        ViT-B   32√ó 1122 4√ó 162    392   77.6 92.9                       solution (results not shown).","XKD          K400 VA                                                                     Lastly, when comparing the performance between the teach-
                        ViT-B   32√ó 2242 4√ó 162    1568  79.6 94.9                       ers and students, teachers show slightly better performance
XKD-MATS K400 VA        ViT-M   32√ó 2242 4√ó 162    1568  81.1 95.6                       on all the benchmarks and in all the setups.",2022-11-25 06:51:35+00:00,XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pritam Sarkar'), arxiv.Result.Author('Ali Etemad')]","We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled video clips. XKD is trained with two pseudo
tasks. First, masked data reconstruction is performed to learn
modality-specific representations. Next, self-supervised cross-modal knowledge
distillation is performed between the two modalities through teacher-student
setups to learn complementary information. To identify the most effective
information to transfer and also to tackle the domain gap between audio and
visual modalities which could hinder knowledge transfer, we introduce a domain
alignment strategy for effective cross-modal distillation. Lastly, to develop a
general-purpose solution capable of handling both audio and visual streams, a
modality-agnostic variant of our proposed framework is introduced, which uses
the same backbone for both audio and visual modalities. Our proposed
cross-modal knowledge distillation improves linear evaluation top-1 accuracy of
video action classification by 8.4% on UCF101, 8.1% on HMDB51, 13.8% on
Kinetics-Sound, and 14.2% on Kinetics400. Additionally, our modality-agnostic
variant shows promising results in developing a general-purpose network capable
of handling different data streams. The code is released on the project
website.",0.2628462,0.12432086,0.051173564,A
14159,"(i) First, we replace Lda with L1da, where instead
further study the reason behind this, we visualize the dis-             of minimizing the MMD loss among teachers and among
tributions of audio and video modalities using Kinetics400.",To              ment.,"students, we aim to minimize the loss between the teach-
Interestingly, as shown in Figure 2, we observe that the dis-           ers and students, as L1da = Lmmd(fta, fsv)+Lmmd(ftv, fsa).",2022-11-25 06:51:35+00:00,XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pritam Sarkar'), arxiv.Result.Author('Ali Etemad')]","We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled video clips. XKD is trained with two pseudo
tasks. First, masked data reconstruction is performed to learn
modality-specific representations. Next, self-supervised cross-modal knowledge
distillation is performed between the two modalities through teacher-student
setups to learn complementary information. To identify the most effective
information to transfer and also to tackle the domain gap between audio and
visual modalities which could hinder knowledge transfer, we introduce a domain
alignment strategy for effective cross-modal distillation. Lastly, to develop a
general-purpose solution capable of handling both audio and visual streams, a
modality-agnostic variant of our proposed framework is introduced, which uses
the same backbone for both audio and visual modalities. Our proposed
cross-modal knowledge distillation improves linear evaluation top-1 accuracy of
video action classification by 8.4% on UCF101, 8.1% on HMDB51, 13.8% on
Kinetics-Sound, and 14.2% on Kinetics400. Additionally, our modality-agnostic
variant shows promising results in developing a general-purpose network capable
of handling different data streams. The code is released on the project
website.",0.29151377,0.006147099,-0.007498743,A
14160,"We Ô¨Ånd that further research
VideoMAE [53] K400 V    ViT-B   32√ó 1122 4√ó 162    392   75.7 92.2                       needs to be done to tackle such challenges, which is out-
                        ViT-B   32√ó 1122 4√ó 162    392   75.9 92.1                       of-scope for this work and we plan to address in the future.","This leads to a degenerated
                        ViT-B   32√ó 1122 4√ó 162    392   77.6 92.9                       solution (results not shown).","XKD          K400 VA                                                                     Lastly, when comparing the performance between the teach-
                        ViT-B   32√ó 2242 4√ó 162    1568  79.6 94.9                       ers and students, teachers show slightly better performance
XKD-MATS K400 VA        ViT-M   32√ó 2242 4√ó 162    1568  81.1 95.6                       on all the benchmarks and in all the setups.",2022-11-25 06:51:35+00:00,XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pritam Sarkar'), arxiv.Result.Author('Ali Etemad')]","We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled video clips. XKD is trained with two pseudo
tasks. First, masked data reconstruction is performed to learn
modality-specific representations. Next, self-supervised cross-modal knowledge
distillation is performed between the two modalities through teacher-student
setups to learn complementary information. To identify the most effective
information to transfer and also to tackle the domain gap between audio and
visual modalities which could hinder knowledge transfer, we introduce a domain
alignment strategy for effective cross-modal distillation. Lastly, to develop a
general-purpose solution capable of handling both audio and visual streams, a
modality-agnostic variant of our proposed framework is introduced, which uses
the same backbone for both audio and visual modalities. Our proposed
cross-modal knowledge distillation improves linear evaluation top-1 accuracy of
video action classification by 8.4% on UCF101, 8.1% on HMDB51, 13.8% on
Kinetics-Sound, and 14.2% on Kinetics400. Additionally, our modality-agnostic
variant shows promising results in developing a general-purpose network capable
of handling different data streams. The code is released on the project
website.",0.2628462,0.12432086,0.051173564,A
14169,"However, further research is required to prove this does not
3.1.5 Miscellaneous                                             impact the performance of the machine learning models.","or noise, allowing for an additional speedup factor of 10.","To conclude, there are some miscellaneous dataset settings      3.3.",2022-11-25 12:17:35+00:00,CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Steven Moonen'), arxiv.Result.Author('Bram Vanherle'), arxiv.Result.Author('Joris de Hoog'), arxiv.Result.Author('Taoufik Bourgana'), arxiv.Result.Author('Abdellatif Bey-Temsamani'), arxiv.Result.Author('Nick Michiels')]","The use of computer vision for product and assembly quality control is
becoming ubiquitous in the manufacturing industry. Lately, it is apparent that
machine learning based solutions are outperforming classical computer vision
algorithms in terms of performance and robustness. However, a main drawback is
that they require sufficiently large and labeled training datasets, which are
often not available or too tedious and too time consuming to acquire. This is
especially true for low-volume and high-variance manufacturing. Fortunately, in
this industry, CAD models of the manufactured or assembled products are
available. This paper introduces CAD2Render, a GPU-accelerated synthetic data
generator based on the Unity High Definition Render Pipeline (HDRP). CAD2Render
is designed to add variations in a modular fashion, making it possible for high
customizable data generation, tailored to the needs of the industrial use case
at hand. Although CAD2Render is specifically designed for manufacturing use
cases, it can be used for other domains as well. We validate CAD2Render by
demonstrating state of the art performance in two industrial relevant setups.
We demonstrate that the data generated by our approach can be used to train
object detection and pose estimation models with a high enough accuracy to
direct a robot. The code for CAD2Render is available at
https://github.com/EDM-Research/CAD2Render.",0.25548455,-0.025100878,0.11218667,A
14181,"This does not require any correspondence
and actions is complementary and believe this will facilitate                                                  between 3D pose data and 2D video, and we can thus train
further research in 3D pose forecasting.","be valid in 3D with an adversarial loss against an arbitrary
We demonstrate that simultaneously predicting 3D poses                                                         set of 3D poses.",with a database of unrelated 3D poses.,2022-11-25 18:59:53+00:00,Forecasting Actions and Characteristic 3D Poses,cs.CV,"['cs.CV', 'cs.LG', 'I.2.10; I.4.8; I.5.1; I.5.4']","[arxiv.Result.Author('Christian Diller'), arxiv.Result.Author('Thomas Funkhouser'), arxiv.Result.Author('Angela Dai')]","We propose to model longer-term future human behavior by jointly predicting
action labels and 3D characteristic poses (3D poses representative of the
associated actions). While previous work has considered action and 3D pose
forecasting separately, we observe that the nature of the two tasks is coupled,
and thus we predict them together. Starting from an input 2D video observation,
we jointly predict a future sequence of actions along with 3D poses
characterizing these actions. Since coupled action labels and 3D pose
annotations are difficult and expensive to acquire for videos of complex action
sequences, we train our approach with action labels and 2D pose supervision
from two existing action video datasets, in tandem with an adversarial loss
that encourages likely 3D predicted poses. Our experiments demonstrate the
complementary nature of joint action and characteristic 3D pose prediction: our
joint approach outperforms each task treated individually, enables robust
longer-term sequence prediction, and outperforms alternative approaches to
forecast actions and characteristic 3D poses.",-0.30692238,0.0627553,-0.053911194,B
14187,"Another interesting procedure with further research potential being the Test Time Sum
Augmentation technique.","If any positive result is reached, an analysis would be
required to determine if the increase in resistance to attacks is due only to the improved test
data generalization (already reflected in the accuracy) or if these models obtain an additional
intrinsic robustness property.","Having the configurable Œª parameter, one could experiment with
its value as being a trade-off between performance and robustness.",2022-11-25 22:31:11+00:00,Deep Learning Training Procedure Augmentations,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Cristian Simionescu')],"Recent advances in Deep Learning have greatly improved performance on various
tasks such as object detection, image segmentation, sentiment analysis. The
focus of most research directions up until very recently has been on beating
state-of-the-art results. This has materialized in the utilization of bigger
and bigger models and techniques which help the training procedure to extract
more predictive power out of a given dataset. While this has lead to great
results, many of which with real-world applications, other relevant aspects of
deep learning have remained neglected and unknown. In this work, we will
present several novel deep learning training techniques which, while capable of
offering significant performance gains they also reveal several interesting
analysis results regarding convergence speed, optimization landscape
smoothness, and adversarial robustness. The methods presented in this work are
the following:
  $\bullet$ Perfect Ordering Approximation; a generalized model agnostic
curriculum learning approach. The results show the effectiveness of the
technique for improving training time as well as offer some new insight into
the training process of deep networks.
  $\bullet$ Cascading Sum Augmentation; an extension of mixup capable of
utilizing more data points for linear interpolation by leveraging a smoother
optimization landscape. This can be used for computer vision tasks in order to
improve both prediction performance as well as improve passive model
robustness.",0.36299595,0.046340235,0.10660009,A
14191,"Downstream IVF applications
  TarD [33] 6.84 45.63 8.68 1.86 1.52 0.53 0.32 0.88
   DeF [32] 6.95 38.41 8.21 1.78 1.64 0.60 0.41 0.96                      To further study fusion performance in high-level MM
   ReC [18] 7.10 44.85 8.73 1.78 1.70 0.57 0.39 0.88                   computer vision tasks, this section applies the infrared, visi-
  CDDFuse 7.12 46.00 13.15 2.19 1.76 0.77 0.54 1.03                    ble and fusion images of SOTA methods in Sec.","SDN [68] 6.64 32.66 12.05 1.52 1.49 0.56 0.44 1.00
  RFN [62] 6.83 34.50 15.71 1.20 1.67 0.51 0.39 0.92                   4.4.","4.2 to MM
                                                                       object detection and semantic segmentation, and investigate
          Dataset: RoadScene Infrared-Visible Fusion Dataset [61]      information fusion‚Äôs beneÔ¨Åt for the downstream tasks.",2022-11-26 02:40:28+00:00,CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zixiang Zhao'), arxiv.Result.Author('Haowen Bai'), arxiv.Result.Author('Jiangshe Zhang'), arxiv.Result.Author('Yulun Zhang'), arxiv.Result.Author('Shuang Xu'), arxiv.Result.Author('Zudi Lin'), arxiv.Result.Author('Radu Timofte'), arxiv.Result.Author('Luc Van Gool')]","Multi-modality (MM) image fusion aims to render fused images that maintain
the merits of different modalities, e.g., functional highlight and detailed
textures. To tackle the challenge in modeling cross-modality features and
decomposing desirable modality-specific and modality-shared features, we
propose a novel Correlation-Driven feature Decomposition Fusion (CDDFuse)
network for end-to-end MM feature decomposition and image fusion. In the first
stage of the two-stage architectures, CDDFuse uses Restormer blocks to extract
cross-modality shallow features. We then introduce a dual-branch
Transformer-CNN feature extractor with Lite Transformer (LT) blocks leveraging
long-range attention to handle low-frequency global features and Invertible
Neural Networks (INN) blocks focusing on extracting high-frequency local
information. Upon the embedded semantic information, the low-frequency features
should be correlated while the high-frequency features should be uncorrelated.
Thus, we propose a correlation-driven loss for better feature decomposition. In
the second stage, the LT-based global fusion and INN-based local fusion layers
output the fused image. Extensive experiments demonstrate that our CDDFuse
achieves promising results in multiple fusion tasks, including infrared-visible
image fusion and medical image fusion. We also show that CDDFuse can boost the
performance in downstream infrared-visible semantic segmentation and object
detection in a unified benchmark.",-0.019673822,0.17706275,0.14087468,B
14192,PosPool [12] and PointNeXt [18] take a          further research.,"1), and can act as a baseline for
mentation details.",step toward this goal.,2022-11-26 02:53:40+00:00,Meta Architecure for Point Cloud Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haojia Lin'), arxiv.Result.Author('Xiawu Zheng'), arxiv.Result.Author('Lijiang Li'), arxiv.Result.Author('Fei Chao'), arxiv.Result.Author('Shanshan Wang'), arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Yonghong Tian'), arxiv.Result.Author('Rongrong Ji')]","Recent advances in 3D point cloud analysis bring a diverse set of network
architectures to the field. However, the lack of a unified framework to
interpret those networks makes any systematic comparison, contrast, or analysis
challenging, and practically limits healthy development of the field. In this
paper, we take the initiative to explore and propose a unified framework called
PointMeta, to which the popular 3D point cloud analysis approaches could fit.
This brings three benefits. First, it allows us to compare different approaches
in a fair manner, and use quick experiments to verify any empirical
observations or assumptions summarized from the comparison. Second, the big
picture brought by PointMeta enables us to think across different components,
and revisit common beliefs and key design decisions made by the popular
approaches. Third, based on the learnings from the previous two analyses, by
doing simple tweaks on the existing approaches, we are able to derive a basic
building block, termed PointMetaBase. It shows very strong performance in
efficiency and effectiveness through extensive experiments on challenging
benchmarks, and thus verifies the necessity and benefits of high-level
interpretation, contrast, and comparison like PointMeta. In particular,
PointMetaBase surpasses the previous state-of-the-art method by 0.7%/1.4/%2.1%
mIoU with only 2%/11%/13% of the computation cost on the S3DIS datasets.",0.19200708,0.2537402,-0.3265217,A
14196,"This large-scale dataset
      This new benchmark will be beneÔ¨Åcial to further research     is very signiÔ¨Åcant as it makes up an important shortfall in the
      on DR diagnosis with two-Ô¨Åeld fundus images.",multiple practical screening scenarios.,"current research community and catalyzes future research in
                                                                   two-Ô¨Åeld DR diagnosis.",2022-11-26 12:39:57+00:00,Cross-Field Transformer for Diabetic Retinopathy Grading on Two-feld Fundus Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junlin Hou'), arxiv.Result.Author('Jilan Xu'), arxiv.Result.Author('Fan Xiao'), arxiv.Result.Author('Rui-Wei Zhao'), arxiv.Result.Author('Yuejie Zhang'), arxiv.Result.Author('Haidong Zou'), arxiv.Result.Author('Lina Lu'), arxiv.Result.Author('Wenwen Xue'), arxiv.Result.Author('Rui Feng')]","Automatic diabetic retinopathy (DR) grading based on fundus photography has
been widely explored to benefit the routine screening and early treatment.
Existing researches generally focus on single-feld fundus images, which have
limited field of view for precise eye examinations. In clinical applications,
ophthalmologists adopt two-feld fundus photography as the dominating tool,
where the information from each feld (i.e.,macula-centric and optic
disc-centric) is highly correlated and complementary, and benefits
comprehensive decisions. However, automatic DR grading based on two-feld fundus
photography remains a challenging task due to the lack of publicly available
datasets and effective fusion strategies. In this work, we first construct a
new benchmark dataset (DRTiD) for DR grading, consisting of 3,100 two-feld
fundus images. To the best of our knowledge, it is the largest public DR
dataset with diverse and high-quality two-feld images. Then, we propose a novel
DR grading approach, namely Cross-Field Transformer (CrossFiT), to capture the
correspondence between two felds as well as the long-range spatial correlations
within each feld. Considering the inherent two-feld geometric constraints, we
particularly define aligned position embeddings to preserve relative consistent
position in fundus. Besides, we perform masked cross-field attention during
interaction to flter the noisy relations between fields. Extensive experiments
on our DRTiD dataset and a public DeepDRiD dataset demonstrate the
effectiveness of our CrossFiT network. The new dataset and the source code of
CrossFiT will be publicly available at https://github.com/FDU-VTS/DRTiD.",0.052815016,0.06733575,-0.099017605,A
14197,"This large-scale dataset
      This new benchmark will be beneÔ¨Åcial to further research     is very signiÔ¨Åcant as it makes up an important shortfall in the
      on DR diagnosis with two-Ô¨Åeld fundus images.",multiple practical screening scenarios.,"current research community and catalyzes future research in
                                                                   two-Ô¨Åeld DR diagnosis.",2022-11-26 12:39:57+00:00,Cross-Field Transformer for Diabetic Retinopathy Grading on Two-field Fundus Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junlin Hou'), arxiv.Result.Author('Jilan Xu'), arxiv.Result.Author('Fan Xiao'), arxiv.Result.Author('Rui-Wei Zhao'), arxiv.Result.Author('Yuejie Zhang'), arxiv.Result.Author('Haidong Zou'), arxiv.Result.Author('Lina Lu'), arxiv.Result.Author('Wenwen Xue'), arxiv.Result.Author('Rui Feng')]","Automatic diabetic retinopathy (DR) grading based on fundus photography has
been widely explored to benefit the routine screening and early treatment.
Existing researches generally focus on single-field fundus images, which have
limited field of view for precise eye examinations. In clinical applications,
ophthalmologists adopt two-field fundus photography as the dominating tool,
where the information from each field (i.e.,macula-centric and optic
disc-centric) is highly correlated and complementary, and benefits
comprehensive decisions. However, automatic DR grading based on two-field
fundus photography remains a challenging task due to the lack of publicly
available datasets and effective fusion strategies. In this work, we first
construct a new benchmark dataset (DRTiD) for DR grading, consisting of 3,100
two-field fundus images. To the best of our knowledge, it is the largest public
DR dataset with diverse and high-quality two-field images. Then, we propose a
novel DR grading approach, namely Cross-Field Transformer (CrossFiT), to
capture the correspondence between two fields as well as the long-range spatial
correlations within each field. Considering the inherent two-field geometric
constraints, we particularly define aligned position embeddings to preserve
relative consistent position in fundus. Besides, we perform masked cross-field
attention during interaction to flter the noisy relations between fields.
Extensive experiments on our DRTiD dataset and a public DeepDRiD dataset
demonstrate the effectiveness of our CrossFiT network. The new dataset and the
source code of CrossFiT will be publicly available at
https://github.com/FDU-VTS/DRTiD.",0.052815016,0.06733575,-0.099017605,A
14208,"Cloud segmentation is one of the Ô¨Årst steps for sky/cloud image analysis [31], from which cloud pixels are identiÔ¨Åed
and cloud coverage, cloud shadow projection and various cloud features can be derived for further research.","Existing eÔ¨Äorts include but not limited
to cloud segmentation, cloud classiÔ¨Åcation and cloud motion prediction.","Traditional
methods tend to distinguish cloud and clear sky pixels by applying a threshold, either Ô¨Åxed or adaptive, on features
extracted from the red and blue channels of sky images.",2022-11-27 03:35:58+00:00,"Open-Source Ground-based Sky Image Datasets for Very Short-term Solar Forecasting, Cloud Analysis and Modeling: A Comprehensive Survey",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuhao Nie'), arxiv.Result.Author('Xiatong Li'), arxiv.Result.Author('Quentin Paletta'), arxiv.Result.Author('Max Aragon'), arxiv.Result.Author('Andea Scott'), arxiv.Result.Author('Adam Brandt')]","This study presents a comprehensive survey of open-source ground-based sky
image datasets for very short-term solar forecasting. Related research areas
which could potentially help improve solar forecasting methods, including cloud
segmentation, cloud classification, and cloud motion prediction are also
considered. We first identify 72 open-source sky image datasets that satisfy
the needs of machine/deep learning. Then a database of information about
various aspects of the datasets is constructed. To evaluate each surveyed
datasets, we further develop a multi-criteria ranking system based on 8
dimensions of the datasets which could potentially have important impacts on
usage of the data. Finally, we provide insights on the usage of these datasets
in the open literature. We hope this paper provide an overview for researchers
who are looking for datasets for training deep learning models for very
short-term solar forecasting, cloud analysis, and atmospheric modeling.",-0.1388009,0.27036622,0.07944977,B
14209,"Cloud segmentation is one of the Ô¨Årst steps for sky/cloud image analysis [31], from which cloud pixels are identiÔ¨Åed
and cloud coverage, cloud shadow projection and various cloud features can be derived for further research.","Existing eÔ¨Äorts include but not limited
to cloud segmentation, cloud classiÔ¨Åcation and cloud motion prediction.","Traditional
methods tend to distinguish cloud and clear sky pixels by applying a threshold, either Ô¨Åxed or adaptive, on features
extracted from the red and blue channels of sky images.",2022-11-27 03:35:58+00:00,"Open-Source Ground-based Sky Image Datasets for Very Short-term Solar Forecasting, Cloud Analysis and Modeling: A Comprehensive Survey",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuhao Nie'), arxiv.Result.Author('Xiatong Li'), arxiv.Result.Author('Quentin Paletta'), arxiv.Result.Author('Max Aragon'), arxiv.Result.Author('Andea Scott'), arxiv.Result.Author('Adam Brandt')]","Sky-image-based solar forecasting using deep learning has been recognized as
a promising approach in reducing the uncertainty in solar power generation.
However, one of the biggest challenges is the lack of massive and diversified
sky image samples. In this study, we present a comprehensive survey of
open-source ground-based sky image datasets for very short-term solar
forecasting (i.e., forecasting horizon less than 30 minutes), as well as
related research areas which can potentially help improve solar forecasting
methods, including cloud segmentation, cloud classification and cloud motion
prediction. We first identify 72 open-source sky image datasets that satisfy
the needs of machine/deep learning. Then a database of information about
various aspects of the identified datasets is constructed. To evaluate each
surveyed datasets, we further develop a multi-criteria ranking system based on
8 dimensions of the datasets which could have important impacts on usage of the
data. Finally, we provide insights on the usage of these datasets for different
applications. We hope this paper can provide an overview for researchers who
are looking for datasets for very short-term solar forecasting and related
areas.",-0.1388009,0.27036622,0.07944977,B
14212,"MNER-QG leaves ample
tions from Wikipedia are relatively general, leading to infe-                            scope for further research.","For method 3, deÔ¨Åni-                                  other baselines in different tasks.","For future work, we will explore
rior results on both tasks.",2022-11-27 06:10:03+00:00,MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Meihuizi Jia'), arxiv.Result.Author('Lei Shen'), arxiv.Result.Author('Xin Shen'), arxiv.Result.Author('Lejian Liao'), arxiv.Result.Author('Meng Chen'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Zhendong Chen'), arxiv.Result.Author('Jiaqi Li')]","Multimodal named entity recognition (MNER) is a critical step in information
extraction, which aims to detect entity spans and classify them to
corresponding entity types given a sentence-image pair. Existing methods either
(1) obtain named entities with coarse-grained visual clues from attention
mechanisms, or (2) first detect fine-grained visual regions with toolkits and
then recognize named entities. However, they suffer from improper alignment
between entity types and visual regions or error propagation in the two-stage
manner, which finally imports irrelevant visual information into texts. In this
paper, we propose a novel end-to-end framework named MNER-QG that can
simultaneously perform MRC-based multimodal named entity recognition and query
grounding. Specifically, with the assistance of queries, MNER-QG can provide
prior knowledge of entity types and visual regions, and further enhance
representations of both texts and images. To conduct the query grounding task,
we provide manual annotations and weak supervisions that are obtained via
training a highly flexible visual grounding model with transfer learning. We
conduct extensive experiments on two public MNER datasets, Twitter2015 and
Twitter2017. Experimental results show that MNER-QG outperforms the current
state-of-the-art models on the MNER task, and also improves the query grounding
performance.",0.30882913,-0.018653933,-0.013601806,A
14213,"Comparing results on the MPI-Sintel (Butler         further study the effectiveness of shadow-free loss (LsRf ) and
et al.","For the Ô¨Årst stage, we
Figure 12.",2012) (last row) and ShapeNet intrinsic dataset (Shi    specular-free loss (LhRf ).,2022-11-27 07:26:41+00:00,Estimating Reflectance Layer from A Single Image: Integrating Reflectance Guidance and Shadow/Specular Aware Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yeying Jin'), arxiv.Result.Author('Ruoteng Li'), arxiv.Result.Author('Wenhan Yang'), arxiv.Result.Author('Robby T. Tan')]","Estimating reflectance layer from a single image is a challenging task. It
becomes more challenging when the input image contains shadows or specular
highlights, which often render an inaccurate estimate of the reflectance layer.
Therefore, we propose a two-stage learning method, including reflectance
guidance and a Shadow/Specular-Aware (S-Aware) network to tackle the problem.
In the first stage, an initial reflectance layer free from shadows and
specularities is obtained with the constraint of novel losses that are guided
by prior-based shadow-free and specular-free images. To further enforce the
reflectance layer to be independent from shadows and specularities in the
second-stage refinement, we introduce an S-Aware network that distinguishes the
reflectance image from the input image. Our network employs a classifier to
categorize shadow/shadow-free, specular/specular-free classes, enabling the
activation features to function as attention maps that focus on shadow/specular
regions. Our quantitative and qualitative evaluations show that our method
outperforms the state-of-the-art methods in the reflectance layer estimation
that is free from shadows and specularities.",0.07898292,0.0324962,0.31189463,A
14225,"We plan to make this domain adapted
                                                                                  dataset available for academic purposes and for further research.","Using this domain
                                                                                  adapted dataset and the corresponding ground truth, we train our
                                                                                  restoration framework on it.","Figure 7: We train our network on other synthetic datasets                        4.2 Training details
such as UWCNN [19] and EUVP [7].",2022-11-27 13:24:28+00:00,Towards Realistic Underwater Dataset Generation and Color Restoration,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Neham Jain'), arxiv.Result.Author('Gopi Matta'), arxiv.Result.Author('Kaushik Mitra')]","Recovery of true color from underwater images is an ill-posed problem. This
is because the wide-band attenuation coefficients for the RGB color channels
depend on object range, reflectance, etc. which are difficult to model. Also,
there is backscattering due to suspended particles in water. Thus, most
existing deep-learning based color restoration methods, which are trained on
synthetic underwater datasets, do not perform well on real underwater data.
This can be attributed to the fact that synthetic data cannot accurately
represent real conditions. To address this issue, we use an image to image
translation network to bridge the gap between the synthetic and real domains by
translating images from synthetic underwater domain to real underwater domain.
Using this multimodal domain adaptation technique, we create a dataset that can
capture a diverse array of underwater conditions. We then train a simple but
effective CNN based network on our domain adapted dataset to perform color
restoration. Code and pre-trained models can be accessed at
https://github.com/nehamjain10/TRUDGCR",-0.093282335,-0.22289312,0.15579736,C
14226,"We plan to make this domain adapted
                                                                                  dataset available for academic purposes and for further research.","Using this domain
                                                                                  adapted dataset and the corresponding ground truth, we train our
                                                                                  restoration framework on it.","Figure 7: We train our network on other synthetic datasets                        4.2 Training details
such as UWCNN [19] and EUVP [7].",2022-11-27 13:24:28+00:00,Towards Realistic Underwater Dataset Generation and Color Restoration,cs.CV,"['cs.CV', 'eess.IV']","[arxiv.Result.Author('Neham Jain'), arxiv.Result.Author('Gopi Matta'), arxiv.Result.Author('Kaushik Mitra')]","Recovery of true color from underwater images is an ill-posed problem. This
is because the wide-band attenuation coefficients for the RGB color channels
depend on object range, reflectance, etc. which are difficult to model. Also,
there is backscattering due to suspended particles in water. Thus, most
existing deep-learning based color restoration methods, which are trained on
synthetic underwater datasets, do not perform well on real underwater data.
This can be attributed to the fact that synthetic data cannot accurately
represent real conditions. To address this issue, we use an image to image
translation network to bridge the gap between the synthetic and real domains by
translating images from synthetic underwater domain to real underwater domain.
Using this multimodal domain adaptation technique, we create a dataset that can
capture a diverse array of underwater conditions. We then train a simple but
effective CNN based network on our domain adapted dataset to perform color
restoration. Code and pre-trained models can be accessed at
https://github.com/nehamjain10/TRUDGCR",-0.093282335,-0.22289312,0.15579736,C
14248,"The datasets                  This issue leads to further research directions to be addressed
have ordinal features both in dependent and independent                 in the future.",from the UCI machine learning repository.,"variables, such as Car, Nursery, Hayes-Roth, and Balance
datasets.",2022-11-28 10:18:06+00:00,Angular triangle distance for ordinal metric learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Imam Mustafa Kamal'), arxiv.Result.Author('Hyerim Bae')]","Deep metric learning (DML) aims to automatically construct task-specific
distances or similarities of data, resulting in a low-dimensional
representation. Several significant metric-learning methods have been proposed.
Nonetheless, no approach guarantees the preservation of the ordinal nature of
the original data in a low-dimensional space. Ordinal data are ubiquitous in
real-world problems, such as the severity of symptoms in biomedical cases,
production quality in manufacturing, rating level in businesses, and aging
level in face recognition. This study proposes a novel angular triangle
distance (ATD) and ordinal triplet network (OTD) to obtain an accurate and
meaningful embedding space representation for ordinal data. The ATD projects
the ordinal relation of data in the angular space, whereas the OTD learns its
ordinal projection. We also demonstrated that our new distance measure
satisfies the distance metric properties mathematically. The proposed method
was assessed using real-world data with an ordinal nature, such as biomedical,
facial, and hand-gestured images. Extensive experiments have been conducted,
and the results show that our proposed method not only semantically preserves
the ordinal nature but is also more accurate than existing DML models.
Moreover, we also demonstrate that our proposed method outperforms the
state-of-the-art ordinal metric learning method.",0.1463166,-0.15035495,-0.26875782,A
14256,"Finally,                     are reported to further study the transferability of our
AMT reaches the comparative performance with attention-                      method.","The comparisons on LVIS dataset
mance than random masking and converges faster.","Different from COCO, LVIS has an imbalanced dis-
driven masking.",2022-11-28 14:38:19+00:00,Good helper is around you: Attention-driven Masked Image Modeling,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Jie Gui'), arxiv.Result.Author('Zhengqi Liu'), arxiv.Result.Author('Hao Luo')]","It has been witnessed that masked image modeling (MIM) has shown a huge
potential in self-supervised learning in the past year. Benefiting from the
universal backbone vision transformer, MIM learns self-supervised visual
representations through masking a part of patches of the image while attempting
to recover the missing pixels. Most previous works mask patches of the image
randomly, which underutilizes the semantic information that is beneficial to
visual representation learning. On the other hand, due to the large size of the
backbone, most previous works have to spend much time on pre-training. In this
paper, we propose \textbf{Attention-driven Masking and Throwing Strategy}
(AMT), which could solve both problems above. We first leverage the
self-attention mechanism to obtain the semantic information of the image during
the training process automatically without using any supervised methods.
Masking strategy can be guided by that information to mask areas selectively,
which is helpful for representation learning. Moreover, a redundant patch
throwing strategy is proposed, which makes learning more efficient. As a
plug-and-play module for masked image modeling, AMT improves the linear probing
accuracy of MAE by $2.9\% \sim 5.9\%$ on CIFAR-10/100, STL-10, Tiny ImageNet,
and ImageNet-1K, and obtains an improved performance with respect to
fine-tuning accuracy of MAE and SimMIM. Moreover, this design also achieves
superior performance on downstream detection and segmentation tasks.",0.032620568,-0.18845758,0.065616146,C
14257,"Finally,                     are reported to further study the transferability of our
AMT reaches the comparative performance with attention-                      method.","The comparisons on LVIS dataset
mance than random masking and converges faster.","Different from COCO, LVIS has an imbalanced dis-
driven masking.",2022-11-28 14:38:19+00:00,Good helper is around you: Attention-driven Masked Image Modeling,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Zhengqi Liu'), arxiv.Result.Author('Jie Gui'), arxiv.Result.Author('Hao Luo')]","It has been witnessed that masked image modeling (MIM) has shown a huge
potential in self-supervised learning in the past year. Benefiting from the
universal backbone vision transformer, MIM learns self-supervised visual
representations through masking a part of patches of the image while attempting
to recover the missing pixels. Most previous works mask patches of the image
randomly, which underutilizes the semantic information that is beneficial to
visual representation learning. On the other hand, due to the large size of the
backbone, most previous works have to spend much time on pre-training. In this
paper, we propose \textbf{Attention-driven Masking and Throwing Strategy}
(AMT), which could solve both problems above. We first leverage the
self-attention mechanism to obtain the semantic information of the image during
the training process automatically without using any supervised methods.
Masking strategy can be guided by that information to mask areas selectively,
which is helpful for representation learning. Moreover, a redundant patch
throwing strategy is proposed, which makes learning more efficient. As a
plug-and-play module for masked image modeling, AMT improves the linear probing
accuracy of MAE by $2.9\% \sim 5.9\%$ on CIFAR-10/100, STL-10, Tiny ImageNet,
and ImageNet-1K, and obtains an improved performance with respect to
fine-tuning accuracy of MAE and SimMIM. Moreover, this design also achieves
superior performance on downstream detection and segmentation tasks. Code is
available at https://github.com/guijiejie/AMT.",0.032620568,-0.18845758,0.065616146,C
14258,"This opens several new
shown in this paper will help to put in place models that are  and interesting directions of further research, namely, in-
designed to be understandable to any person.","Therefore, we hope that the results        els (Sanchez and Tsaftaris, 2022).","In addition,      vestigating the behavior of these techniques on such gener-
we believe that models interpretable ‚Äúby design‚Äù might be      ative models, in order to overcome the limitations of VAEs
a key to produce fairer models that do not discriminate in     in terms of image sharpness and level of detail.",2022-11-22 10:53:17+00:00,Clarity: an improved gradient method for producing quality visual counterfactual explanations,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Claire Theobald'), arxiv.Result.Author('Fr√©d√©ric Pennerath'), arxiv.Result.Author('Brieuc Conan-Guez'), arxiv.Result.Author('Miguel Couceiro'), arxiv.Result.Author('Amedeo Napoli')]","Visual counterfactual explanations identify modifications to an image that
would change the prediction of a classifier. We propose a set of techniques
based on generative models (VAE) and a classifier ensemble directly trained in
the latent space, which all together, improve the quality of the gradient
required to compute visual counterfactuals. These improvements lead to a novel
classification model, Clarity, which produces realistic counterfactual
explanations over all images. We also present several experiments that give
insights on why these techniques lead to better quality results than those in
the literature. The explanations produced are competitive with the
state-of-the-art and emphasize the importance of selecting a meaningful input
space for training.",0.059049383,-0.049712118,-0.050754733,C
14265,"To further study the
effects of Ô¨Ånetuning the visual backbones, we provide experiments on two visual representations
learned in a self-supervised fashion: RN-MoCo and ViT-16-MAE.","D Ablation

D.1 Finetuning

In the experiment section, we mainly report results of Ô¨Åxed visual representations, since the evaluation
of general-purpose requires a shared visual representation for various tasks.","During Ô¨Ånetuning, we set the base
learning rate of visual backbones to 10‚àí5 and keep the same warm-up and linear decay schedule.",2022-11-28 15:06:07+00:00,"Perceive, Ground, Reason, and Act: A Benchmark for General-purpose Visual Representation",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Jiangyong Huang'), arxiv.Result.Author('William Yicheng Zhu'), arxiv.Result.Author('Baoxiong Jia'), arxiv.Result.Author('Zan Wang'), arxiv.Result.Author('Xiaojian Ma'), arxiv.Result.Author('Qing Li'), arxiv.Result.Author('Siyuan Huang')]","Current computer vision models, unlike the human visual system, cannot yet
achieve general-purpose visual understanding. Existing efforts to create a
general vision model are limited in the scope of assessed tasks and offer no
overarching framework to perform them holistically. We present a new
comprehensive benchmark, General-purpose Visual Understanding Evaluation
(G-VUE), covering the full spectrum of visual cognitive abilities with four
functional domains $\unicode{x2014}$ Perceive, Ground, Reason, and Act. The
four domains are embodied in 11 carefully curated tasks, from 3D reconstruction
to visual reasoning and manipulation. Along with the benchmark, we provide a
general encoder-decoder framework to allow for the evaluation of arbitrary
visual representation on all 11 tasks. We evaluate various pre-trained visual
representations with our framework and observe that (1) Transformer-based
visual backbone generally outperforms CNN-based backbone on G-VUE, (2) visual
representations from vision-language pre-training are superior to those with
vision-only pre-training across visual tasks. With G-VUE, we provide a holistic
evaluation standard to motivate research toward building general-purpose visual
systems via obtaining more general-purpose visual representations.",-0.09153266,-0.17538989,0.009122277,C
14268,"Unlike the early fusion methods, it has further research in the processing
of fused features after fusion.","Inspired by the above work, this paper proposes a multimodal emotion recognition method based
on face, gesture and text.","Considering combining the internal information in a single modality
and the interaction information between modalities and solving the problems of information
redundancy and high dimensionality, this paper achieves the best result of tri-modal fusion by
combining features followed by a two-dimensional convolution operation and adding attention
weights.",2022-11-20 14:43:36+00:00,"FAF: A novel multimodal emotion recognition approach integrating face, body and text",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zhongyu Fang'), arxiv.Result.Author('Aoyun He'), arxiv.Result.Author('Qihui Yu'), arxiv.Result.Author('Baopeng Gao'), arxiv.Result.Author('Weiping Ding'), arxiv.Result.Author('Tong Zhang'), arxiv.Result.Author('Lei Ma')]","Multimodal emotion analysis performed better in emotion recognition depending
on more comprehensive emotional clues and multimodal emotion dataset. In this
paper, we developed a large multimodal emotion dataset, named ""HED"" dataset, to
facilitate the emotion recognition task, and accordingly propose a multimodal
emotion recognition method. To promote recognition accuracy, ""Feature After
Feature"" framework was used to explore crucial emotional information from the
aligned face, body and text samples. We employ various benchmarks to evaluate
the ""HED"" dataset and compare the performance with our method. The results show
that the five classification accuracy of the proposed multimodal fusion method
is about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62%
respectively compared with that of individual modalities. The complementarity
between each channel is effectively used to improve the performance of emotion
recognition. We had also established a multimodal online emotion prediction
platform, aiming to provide free emotion prediction to more users.",-0.035196453,-0.049240634,-0.15206143,C
14271,"Formally, the mIOU could be deÔ¨Åned as            the number of input layers is further researched in ablation
follow:                                                             study.","Meanwhile, we utilize features of last four layers
[37], we adopt the class mean intersection over union (mIOU)        in backbones as the diverse inputs of our constructed network
as our evaluation metric, which could straightly reÔ¨Çect the         since surface features contain little semantic information and
model performance.","Model is trained by SGD optimizer with momentum of
                                                                    0.9 for 50,000 iterations.",2022-11-20 16:19:47+00:00,Progressively Dual Prior Guided Few-shot Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qinglong Cao'), arxiv.Result.Author('Yuntian Chen'), arxiv.Result.Author('Xiwen Yao'), arxiv.Result.Author('Junwei Han')]","Few-shot semantic segmentation task aims at performing segmentation in query
images with a few annotated support samples. Currently, few-shot segmentation
methods mainly focus on leveraging foreground information without fully
utilizing the rich background information, which could result in wrong
activation of foreground-like background regions with the inadaptability to
dramatic scene changes of support-query image pairs. Meanwhile, the lack of
detail mining mechanism could cause coarse parsing results without some
semantic components or edge areas since prototypes have limited ability to cope
with large object appearance variance. To tackle these problems, we propose a
progressively dual prior guided few-shot semantic segmentation network.
Specifically, a dual prior mask generation (DPMG) module is firstly designed to
suppress the wrong activation in foreground-background comparison manner by
regarding background as assisted refinement information. With dual prior masks
refining the location of foreground area, we further propose a progressive
semantic detail enrichment (PSDE) module which forces the parsing model to
capture the hidden semantic details by iteratively erasing the high-confidence
foreground region and activating details in the rest region with a hierarchical
structure. The collaboration of DPMG and PSDE formulates a novel few-shot
segmentation network that can be learned in an end-to-end manner. Comprehensive
experiments on PASCAL-5i and MS COCO powerfully demonstrate that our proposed
algorithm achieves the great performance.",0.13373527,-0.095268436,0.12768266,A
14272,"As shown in Table II, our model still outperforms previous
state-of-art algorithms of signiÔ¨Åcant advantage in MS COCO                  Moreover, to further study the function of the PSDE module,
dataset.","the right, the outline of the ship could be explicitly observed
                                                                         in the foregound prior mask.","With VGG16 backbone, the proposed method gets                   the training loss curves for the outputs of diverse layers are
0.7% mIOU improvement in 1-shot setting and 2.9% mIOU                    shown in the Figure 7.",2022-11-20 16:19:47+00:00,Progressively Dual Prior Guided Few-shot Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Qinglong Cao'), arxiv.Result.Author('Yuntian Chen'), arxiv.Result.Author('Xiwen Yao'), arxiv.Result.Author('Junwei Han')]","Few-shot semantic segmentation task aims at performing segmentation in query
images with a few annotated support samples. Currently, few-shot segmentation
methods mainly focus on leveraging foreground information without fully
utilizing the rich background information, which could result in wrong
activation of foreground-like background regions with the inadaptability to
dramatic scene changes of support-query image pairs. Meanwhile, the lack of
detail mining mechanism could cause coarse parsing results without some
semantic components or edge areas since prototypes have limited ability to cope
with large object appearance variance. To tackle these problems, we propose a
progressively dual prior guided few-shot semantic segmentation network.
Specifically, a dual prior mask generation (DPMG) module is firstly designed to
suppress the wrong activation in foreground-background comparison manner by
regarding background as assisted refinement information. With dual prior masks
refining the location of foreground area, we further propose a progressive
semantic detail enrichment (PSDE) module which forces the parsing model to
capture the hidden semantic details by iteratively erasing the high-confidence
foreground region and activating details in the rest region with a hierarchical
structure. The collaboration of DPMG and PSDE formulates a novel few-shot
segmentation network that can be learned in an end-to-end manner. Comprehensive
experiments on PASCAL-5i and MS COCO powerfully demonstrate that our proposed
algorithm achieves the great performance.",-0.08034408,-0.09506433,0.18461159,C
14301,"To further study the ef-       a shared backbone to extract image features, which can de-
fectiveness of ESN, we study the effect of our main com-         crease the computational expense.","That
                                                                 is because CAB works as a stage-speciÔ¨Åc decoder and uses
The effect of related components.","DER-like methods have
ponents in Table 5.",2022-11-29 06:57:48+00:00,Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yabin Wang'), arxiv.Result.Author('Zhiheng Ma'), arxiv.Result.Author('Zhiwu Huang'), arxiv.Result.Author('Yaowei Wang'), arxiv.Result.Author('Zhou Su'), arxiv.Result.Author('Xiaopeng Hong')]","This paper focuses on the prevalent performance imbalance in the stages of
incremental learning. To avoid obvious stage learning bottlenecks, we propose a
brand-new stage-isolation based incremental learning framework, which leverages
a series of stage-isolated classifiers to perform the learning task of each
stage without the interference of others. To be concrete, to aggregate multiple
stage classifiers as a uniform one impartially, we first introduce a
temperature-controlled energy metric for indicating the confidence score levels
of the stage classifiers. We then propose an anchor-based energy
self-normalization strategy to ensure the stage classifiers work at the same
energy level. Finally, we design a voting-based inference augmentation strategy
for robust inference. The proposed method is rehearsal free and can work for
almost all continual learning scenarios. We evaluate the proposed method on
four large benchmarks. Extensive results demonstrate the superiority of the
proposed method in setting up new state-of-the-art overall performance.
\emph{Code is available at} \url{https://github.com/iamwangyabin/ESN}.",0.034391493,0.01266551,0.15707248,C
14321,"To fully close the
trends‚Äîsome tasks beneÔ¨Åt (ImageNet-R, DTD) from hav-                    gap to using true few-shot datasets as support sets [26, 77],
ing more support set samples while others do not (Coun-                 further research into exact unsupervised domain matching
try211, Flowers102).","6, we observe two broad                backgrounds, confounding objects etc).",We suggest that this is connected to               of support sets and few-shot datasets is required.,2022-11-28 16:48:41+00:00,SuS-X: Training-Free Name-Only Transfer of Vision-Language Models,cs.CV,"['cs.CV', 'cs.CL', 'cs.MM']","[arxiv.Result.Author('Vishaal Udandarao'), arxiv.Result.Author('Ankush Gupta'), arxiv.Result.Author('Samuel Albanie')]","Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free ""name-only transfer"" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.",-0.18982792,-0.09092526,0.009986002,C
14326,"Our proposed
                                              method and dataset, along with the provided experiments, can be used in further research
                                              in studying effective solutions for universal representations.","In addition, we propose a
                                              dataset on which learning based methods have a hard time to generalize.","Our source code is available
                                              at: github.com/DavidBoja/greedy-grid-search.",2022-11-29 15:36:43+00:00,Challenging the Universal Representation of Deep Models for 3D Point Cloud Registration,cs.CV,['cs.CV'],"[arxiv.Result.Author('David Bojaniƒá'), arxiv.Result.Author('Kristijan Bartol'), arxiv.Result.Author('Josep Forest'), arxiv.Result.Author('Stefan Gumhold'), arxiv.Result.Author('Tomislav Petkoviƒá'), arxiv.Result.Author('Tomislav Pribaniƒá')]","Learning universal representations across different applications domain is an
open research problem. In fact, finding universal architecture within the same
application but across different types of datasets is still unsolved problem
too, especially in applications involving processing 3D point clouds. In this
work we experimentally test several state-of-the-art learning-based methods for
3D point cloud registration against the proposed non-learning baseline
registration method. The proposed method either outperforms or achieves
comparable results w.r.t. learning based methods. In addition, we propose a
dataset on which learning based methods have a hard time to generalize. Our
proposed method and dataset, along with the provided experiments, can be used
in further research in studying effective solutions for universal
representations. Our source code is available at:
github.com/DavidBoja/greedy-grid-search.",0.0371645,-0.2959699,-0.04934516,C
14330,"The intersection visualizes the source of the identity-
expression ambiguity whereas the differences show variance that            We use our method to further study this ambiguity, also
is represented either by identity or expression.",gions less.,"By inspecting the      known as the identity-expression ambiguity, and analyze the
remaining posterior variance, we can demonstrate that, e.g., the        intersection as well as differences between the identity and
variation of the nose is solely explained by the identity model,        expression models of the BFM 2019 [13].",2022-11-29 15:54:34+00:00,Approximating Intersections and Differences Between Statistical Shape Models,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Maximilian Weiherer'), arxiv.Result.Author('Finn Klein'), arxiv.Result.Author('Bernhard Egger')]","To date, the comparison of Statistical Shape Models (SSMs) is often solely
performance-based and carried out by means of simplistic metrics such as
compactness, generalization, or specificity. Any similarities or differences
between the actual shape spaces can neither be visualized nor quantified. In
this paper, we present a first method to compare two SSMs in dense
correspondence by computing approximate intersection spaces and set-theoretic
differences between the affine vector spaces spanned by the models. To this
end, we approximate the distribution of shapes lying in the intersection space
using Markov Chain Monte Carlo, and then apply Principal Component Analysis
(PCA) to its samples. By representing the resulting spaces again as an SSM, our
method enables an easy and intuitive analysis of similarities between two
model's shape spaces. We estimate differences between SSMs in a similar manner;
here, however, the resulting shape spaces are not linear vector spaces anymore
and we do not apply PCA but instead use the posterior samples for
visualization. We showcase the proposed algorithm qualitatively by computing
and analyzing intersection spaces and differences between publicly available
face models focusing on gender-specific male and female as well as identity and
expression models. Our quantitative evaluation based on SSMs built from
synthetic and real-world data sets provides detailed evidence that the
introduced method is able to recover ground-truth intersection spaces and
differences. Finally, we demonstrate that the proposed algorithm can be easily
adapted to also compute intersections and differences between color spaces.",0.19991103,0.024463054,-0.17083855,A
14337,"Moreover,
                                                                               DeepFlash is not an end-to-end method, because its out-
   We further study the impact of adding a squaring and scal-                  put (band-limited velocity Ô¨Åeld) requires an additional PDE
ing module into Fourier-Net.",as such achieves the lowest Dice score (0.597).,"As shown in Table 1, this mod-                    algorithm to compute the Ô¨Ånal deformation.",2022-11-29 16:24:06+00:00,Fourier-Net: Fast Image Registration with Band-limited Deformation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xi Jia'), arxiv.Result.Author('Joseph Bartlett'), arxiv.Result.Author('Wei Chen'), arxiv.Result.Author('Siyang Song'), arxiv.Result.Author('Tianyang Zhang'), arxiv.Result.Author('Xinxing Cheng'), arxiv.Result.Author('Wenqi Lu'), arxiv.Result.Author('Zhaowen Qiu'), arxiv.Result.Author('Jinming Duan')]","Unsupervised image registration commonly adopts U-Net style networks to
predict dense displacement fields in the full-resolution spatial domain. For
high-resolution volumetric image data, this process is however resource
intensive and time-consuming. To tackle this problem, we propose the
Fourier-Net, replacing the expansive path in a U-Net style network with a
parameter-free model-driven decoder. Specifically, instead of our Fourier-Net
learning to output a full-resolution displacement field in the spatial domain,
we learn its low-dimensional representation in a band-limited Fourier domain.
This representation is then decoded by our devised model-driven decoder
(consisting of a zero padding layer and an inverse discrete Fourier transform
layer) to the dense, full-resolution displacement field in the spatial domain.
These changes allow our unsupervised Fourier-Net to contain fewer parameters
and computational operations, resulting in faster inference speeds. Fourier-Net
is then evaluated on two public 3D brain datasets against various
state-of-the-art approaches. For example, when compared to a recent
transformer-based method, i.e., TransMorph, our Fourier-Net, only using
0.22$\%$ of its parameters and 6.66$\%$ of the mult-adds, achieves a 0.6\%
higher Dice score and an 11.48$\times$ faster inference speed. Code is
available at \url{https://github.com/xi-jia/Fourier-Net}.",0.26620567,0.114440255,0.24263766,A
14348,"age extracting other valuable features from the images for
                                                                                                                  further research.","Therefore,
                                                                                                                  we still strongly advocate two-stage approaches and encour-
54                                                                            75 SWheahrrmbaeienteatl.al.","Another issue arising from the intermedi-
PA-MPJPE (mm)                                                                 70                                  ate heatmap representation are completely wrong 2D joint
                                                               PA-MPJPE (mm)                                      detections.",2022-11-29 18:55:13+00:00,DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Karl Holmquist'), arxiv.Result.Author('Bastian Wandt')]","Traditionally, monocular 3D human pose estimation employs a machine learning
model to predict the most likely 3D pose for a given input image. However, a
single image can be highly ambiguous and induces multiple plausible solutions
for the 2D-3D lifting step which results in overly confident 3D pose
predictors. To this end, we propose \emph{DiffPose}, a conditional diffusion
model, that predicts multiple hypotheses for a given input image. In comparison
to similar approaches, our diffusion model is straightforward and avoids
intensive hyperparameter tuning, complex network structures, mode collapse, and
unstable training. Moreover, we tackle a problem of the common two-step
approach that first estimates a distribution of 2D joint locations via
joint-wise heatmaps and consecutively approximates them based on first- or
second-moment statistics. Since such a simplification of the heatmaps removes
valid information about possibly correct, though labeled unlikely, joint
locations, we propose to represent the heatmaps as a set of 2D joint candidate
samples. To extract information about the original distribution from these
samples we introduce our \emph{embedding transformer} that conditions the
diffusion model. Experimentally, we show that DiffPose slightly improves upon
the state of the art for multi-hypothesis pose estimation for simple poses and
outperforms it by a large margin for highly ambiguous poses.",0.03039489,0.2465164,-0.06686859,B
14350,"Therefore, we propose a new diagnostic test set       how to improve the commonsense knowledge of ITR still
in a wider adaptable form, i.e., Image-Text and Text-Image       requires further study.","Though,
VL-models.","CVSE [72] injects commonsense
Retrieval, to achieve a fair evaluation of the pre-trained VL-   knowledge into VL-models for ITR using statistical cor-
models.",2022-11-29 18:59:59+00:00,Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Shuquan Ye'), arxiv.Result.Author('Yujia Xie'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Yichong Xu'), arxiv.Result.Author('Lu Yuan'), arxiv.Result.Author('Chenguang Zhu'), arxiv.Result.Author('Jing Liao')]","This paper focuses on analyzing and improving the commonsense ability of
recent popular vision-language (VL) models. Despite the great success, we
observe that existing VL-models still lack commonsense knowledge/reasoning
ability (e.g., ""Lemons are sour""), which is a vital component towards
artificial general intelligence. Through our analysis, we find one important
reason is that existing large-scale VL datasets do not contain much commonsense
knowledge, which motivates us to improve the commonsense of VL-models from the
data perspective. Rather than collecting a new VL training dataset, we propose
a more scalable strategy, i.e., ""Data Augmentation with kNowledge graph
linearization for CommonsensE capability"" (DANCE). It can be viewed as one type
of data augmentation technique, which can inject commonsense knowledge into
existing VL datasets on the fly during training. More specifically, we leverage
the commonsense knowledge graph (e.g., ConceptNet) and create variants of text
description in VL datasets via bidirectional sub-graph sequentialization. For
better commonsense evaluation, we further propose the first retrieval-based
commonsense diagnostic benchmark. By conducting extensive experiments on some
representative VL-models, we demonstrate that our DANCE technique is able to
significantly improve the commonsense ability while maintaining the performance
on vanilla retrieval tasks. The code and data are available at
https://github.com/pleaseconnectwifi/DANCE",0.015014512,-0.2663073,-0.15306431,C
14361,"To further study the im-
provements, we report the quantitative results in Table I, which
is more detailed.","Our approach
consistently outperforms the DeepLabv3+ model under all
partition protocols, and the gain of the CPCL increases as the
ratio of the labeled data decreases.","The performance increments are shown in
the brackets.",2022-11-30 02:47:25+00:00,Conservative-Progressive Collaborative Learning for Semi-supervised Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siqi Fan'), arxiv.Result.Author('Fenghua Zhu'), arxiv.Result.Author('Zunlei Feng'), arxiv.Result.Author('Yisheng Lv'), arxiv.Result.Author('Mingli Song'), arxiv.Result.Author('Fei-Yue Wang')]","Pseudo supervision is regarded as the core idea in semi-supervised learning
for semantic segmentation, and there is always a tradeoff between utilizing
only the high-quality pseudo labels and leveraging all the pseudo labels.
Addressing that, we propose a novel learning approach, called
Conservative-Progressive Collaborative Learning (CPCL), among which two
predictive networks are trained in parallel, and the pseudo supervision is
implemented based on both the agreement and disagreement of the two
predictions. One network seeks common ground via intersection supervision and
is supervised by the high-quality labels to ensure a more reliable supervision,
while the other network reserves differences via union supervision and is
supervised by all the pseudo labels to keep exploring with curiosity. Thus, the
collaboration of conservative evolution and progressive exploration can be
achieved. To reduce the influences of the suspicious pseudo labels, the loss is
dynamic re-weighted according to the prediction confidence. Extensive
experiments demonstrate that CPCL achieves state-of-the-art performance for
semi-supervised semantic segmentation.",0.092402264,-0.14171937,0.12670557,A
14362,"Thus, the indicator for
                               48.70 55.81 63.01 69.16           the model coupling problem is also worth to be further study.","CutMix-Seg [8]    ResNet-101  39.69 47.58 59.97 65.27           However, the overlap ratio may not be an effective indicator
 PseudoSeg [37]    ResNet-101                                    for model coupling, since we cannot distinguish the model
    CPCL (Ours)    ResNet-50   46.04 54.98 64.71 70.67           coupling problem from the accurate models reaching a high
                                                                 degree of consensus with each other.","55.58 63.20 68.36 69.84                                        REFERENCES

                               57.60 65.50 69.14 72.41            [1] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
                                                                       nenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset for
                               61.88 67.02 72.14 74.25                 semantic urban scene understanding,‚Äù in Proc.",2022-11-30 02:47:25+00:00,Conservative-Progressive Collaborative Learning for Semi-supervised Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Siqi Fan'), arxiv.Result.Author('Fenghua Zhu'), arxiv.Result.Author('Zunlei Feng'), arxiv.Result.Author('Yisheng Lv'), arxiv.Result.Author('Mingli Song'), arxiv.Result.Author('Fei-Yue Wang')]","Pseudo supervision is regarded as the core idea in semi-supervised learning
for semantic segmentation, and there is always a tradeoff between utilizing
only the high-quality pseudo labels and leveraging all the pseudo labels.
Addressing that, we propose a novel learning approach, called
Conservative-Progressive Collaborative Learning (CPCL), among which two
predictive networks are trained in parallel, and the pseudo supervision is
implemented based on both the agreement and disagreement of the two
predictions. One network seeks common ground via intersection supervision and
is supervised by the high-quality labels to ensure a more reliable supervision,
while the other network reserves differences via union supervision and is
supervised by all the pseudo labels to keep exploring with curiosity. Thus, the
collaboration of conservative evolution and progressive exploration can be
achieved. To reduce the influences of the suspicious pseudo labels, the loss is
dynamic re-weighted according to the prediction confidence. Extensive
experiments demonstrate that CPCL achieves state-of-the-art performance for
semi-supervised semantic segmentation.",0.054074354,0.16720751,-0.062357742,B
14365,"However, modules (such as RPN [4]            cover instance masks more precise with a simpler model by
                                        and ROIAlign [1]) restrict the training and inference efÔ¨Åciency      achieving the following two targets: (1) generating a large
                                        deeply, which leads to expensive hardware resources and time         number of points via low design complexity to depict masks
                                        costs for further research.","They formulate the instance segmentation problem as the
                                        combination of previous techniques (object detection and                Considering the issues above, we aim for pursuing to
                                        semantic segmentation).","Some works [5], [6], [7] follow          precisely ; (2) dynamic allocating an appropriate number of
                                        this issue and construct one-stage [8], [9] pipelines, while the     points to smooth and twisty parts of instances, respectively.",2022-11-30 04:50:56+00:00,Growing Instance Mask on Leaf,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Chuang Yang'), arxiv.Result.Author('Haozhao Ma'), arxiv.Result.Author('Qi Wang')]","Contour-based instance segmentation methods include one-stage and multi-stage
schemes. These approaches achieve remarkable performance. However, they have to
define plenty of points to segment precise masks, which leads to high
complexity. We follow this issue and present a single-shot method, called
\textbf{VeinMask}, for achieving competitive performance in low design
complexity. Concretely, we observe that the leaf locates coarse margins via
major veins and grows minor veins to refine twisty parts, which makes it
possible to cover any objects accurately. Meanwhile, major and minor veins
share the same growth mode, which avoids modeling them separately and ensures
model simplicity. Considering the superiorities above, we propose VeinMask to
formulate the instance segmentation problem as the simulation of the vein
growth process and to predict the major and minor veins in polar coordinates.
  Besides, centroidness is introduced for instance segmentation tasks to help
suppress low-quality instances. Furthermore, a surroundings cross-correlation
sensitive (SCCS) module is designed to enhance the feature expression by
utilizing the surroundings of each pixel. Additionally, a Residual IoU (R-IoU)
loss is formulated to supervise the regression tasks of major and minor veins
effectively. Experiments demonstrate that VeinMask performs much better than
other contour-based methods in low design complexity. Particularly, our method
outperforms existing one-stage contour-based methods on the COCO dataset with
almost half the design complexity.",-0.2169031,-0.015276834,0.085653,B
14376,"More-                         more lightweight TFA block to escape from optical Ô¨Çow cal-
over, we also consider the online testing case that the tem-                       culation to achieve higher performance; 2) further research
poral alignment state from backward frames is not available.","Future directions include: 1) to design
from the varying face resolution setting in Table 3.","on the inÔ¨Çuence of other video qualities such as arbitrary
In this case, we design a TFA block with a single forward                          frame rate and compression rate.",2022-11-30 11:50:08+00:00,Learning Motion-Robust Remote Photoplethysmography through Arbitrary Resolution Videos,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Li'), arxiv.Result.Author('Zitong Yu'), arxiv.Result.Author('Jingang Shi')]","Remote photoplethysmography (rPPG) enables non-contact heart rate (HR)
estimation from facial videos which gives significant convenience compared with
traditional contact-based measurements. In the real-world long-term health
monitoring scenario, the distance of the participants and their head movements
usually vary by time, resulting in the inaccurate rPPG measurement due to the
varying face resolution and complex motion artifacts. Different from the
previous rPPG models designed for a constant distance between camera and
participants, in this paper, we propose two plug-and-play blocks (i.e.,
physiological signal feature extraction block (PFE) and temporal face alignment
block (TFA)) to alleviate the degradation of changing distance and head motion.
On one side, guided with representative-area information, PFE adaptively
encodes the arbitrary resolution facial frames to the fixed-resolution facial
structure features. On the other side, leveraging the estimated optical flow,
TFA is able to counteract the rPPG signal confusion caused by the head movement
thus benefit the motion-robust rPPG signal recovery. Besides, we also train the
model with a cross-resolution constraint using a two-stream dual-resolution
framework, which further helps PFE learn resolution-robust facial rPPG
features. Extensive experiments on three benchmark datasets (UBFC-rPPG, COHFACE
and PURE) demonstrate the superior performance of the proposed method. One
highlight is that with PFE and TFA, the off-the-shelf spatio-temporal rPPG
models can predict more robust rPPG signals under both varying face resolution
and severe head movement scenarios. The codes are available at
https://github.com/LJW-GIT/Arbitrary_Resolution_rPPG.",0.14633897,0.2847157,0.05800967,A
14377,"More-                         more lightweight TFA block to escape from optical Ô¨Çow cal-
over, we also consider the online testing case that the tem-                       culation to achieve higher performance; 2) further research
poral alignment state from backward frames is not available.","Future directions include: 1) to design
from the varying face resolution setting in Table 3.","on the inÔ¨Çuence of other video qualities such as arbitrary
In this case, we design a TFA block with a single forward                          frame rate and compression rate.",2022-11-30 11:50:08+00:00,Learning Motion-Robust Remote Photoplethysmography through Arbitrary Resolution Videos,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Li'), arxiv.Result.Author('Zitong Yu'), arxiv.Result.Author('Jingang Shi')]","Remote photoplethysmography (rPPG) enables non-contact heart rate (HR)
estimation from facial videos which gives significant convenience compared with
traditional contact-based measurements. In the real-world long-term health
monitoring scenario, the distance of the participants and their head movements
usually vary by time, resulting in the inaccurate rPPG measurement due to the
varying face resolution and complex motion artifacts. Different from the
previous rPPG models designed for a constant distance between camera and
participants, in this paper, we propose two plug-and-play blocks (i.e.,
physiological signal feature extraction block (PFE) and temporal face alignment
block (TFA)) to alleviate the degradation of changing distance and head motion.
On one side, guided with representative-area information, PFE adaptively
encodes the arbitrary resolution facial frames to the fixed-resolution facial
structure features. On the other side, leveraging the estimated optical flow,
TFA is able to counteract the rPPG signal confusion caused by the head movement
thus benefit the motion-robust rPPG signal recovery. Besides, we also train the
model with a cross-resolution constraint using a two-stream dual-resolution
framework, which further helps PFE learn resolution-robust facial rPPG
features. Extensive experiments on three benchmark datasets (UBFC-rPPG, COHFACE
and PURE) demonstrate the superior performance of the proposed method. One
highlight is that with PFE and TFA, the off-the-shelf spatio-temporal rPPG
models can predict more robust rPPG signals under both varying face resolution
and severe head movement scenarios. The codes are available at
https://github.com/LJW-GIT/Arbitrary_Resolution_rPPG.",0.14633897,0.2847157,0.05800967,A
14378,"More-                         more lightweight TFA block to escape from optical Ô¨Çow cal-
over, we also consider the online testing case that the tem-                       culation to achieve higher performance; 2) further research
poral alignment state from backward frames is not available.","Future directions include: 1) to design
from the varying face resolution setting in Table 3.","on the inÔ¨Çuence of other video qualities such as arbitrary
In this case, we design a TFA block with a single forward                          frame rate and compression rate.",2022-11-30 11:50:08+00:00,Learning Motion-Robust Remote Photoplethysmography through Arbitrary Resolution Videos,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jianwei Li'), arxiv.Result.Author('Zitong Yu'), arxiv.Result.Author('Jingang Shi')]","Remote photoplethysmography (rPPG) enables non-contact heart rate (HR)
estimation from facial videos which gives significant convenience compared with
traditional contact-based measurements. In the real-world long-term health
monitoring scenario, the distance of the participants and their head movements
usually vary by time, resulting in the inaccurate rPPG measurement due to the
varying face resolution and complex motion artifacts. Different from the
previous rPPG models designed for a constant distance between camera and
participants, in this paper, we propose two plug-and-play blocks (i.e.,
physiological signal feature extraction block (PFE) and temporal face alignment
block (TFA)) to alleviate the degradation of changing distance and head motion.
On one side, guided with representative-area information, PFE adaptively
encodes the arbitrary resolution facial frames to the fixed-resolution facial
structure features. On the other side, leveraging the estimated optical flow,
TFA is able to counteract the rPPG signal confusion caused by the head movement
thus benefit the motion-robust rPPG signal recovery. Besides, we also train the
model with a cross-resolution constraint using a two-stream dual-resolution
framework, which further helps PFE learn resolution-robust facial rPPG
features. Extensive experiments on three benchmark datasets (UBFC-rPPG, COHFACE
and PURE) demonstrate the superior performance of the proposed method. One
highlight is that with PFE and TFA, the off-the-shelf spatio-temporal rPPG
models can predict more robust rPPG signals under both varying face resolution
and severe head movement scenarios. The codes are available at
https://github.com/LJW-GIT/Arbitrary_Resolution_rPPG.",0.14633897,0.2847157,0.05800967,A
14385,"[38]
Additionally, to further research in power line segmenta-          adapt context dependencies from both spatial and chan-
tion, we present two datasets - ARPLSyn (synthetic) and            nel views through self-attention and cross-attention mecha-
ARPLReal (real) with high-quality segmentation annota-             nisms.",scale context embedding for semantic segmentation.,"Another line of work leverage cross-attention mech-
tions (refer Figure.",2022-11-29 03:15:27+00:00,QuadFormer: Quadruple Transformer for Unsupervised Domain Adaptation in Power Line Segmentation of Aerial Images,cs.CV,['cs.CV'],"[arxiv.Result.Author('Pratyaksh Prabhav Rao'), arxiv.Result.Author('Feng Qiao'), arxiv.Result.Author('Weide Zhang'), arxiv.Result.Author('Yiliang Xu'), arxiv.Result.Author('Yong Deng'), arxiv.Result.Author('Guangbin Wu'), arxiv.Result.Author('Qiang Zhang')]","Accurate segmentation of power lines in aerial images is essential to ensure
the flight safety of aerial vehicles. Acquiring high-quality ground truth
annotations for training a deep learning model is a laborious process.
Therefore, developing algorithms that can leverage knowledge from labelled
synthetic data to unlabelled real images is highly demanded. This process is
studied in Unsupervised domain adaptation (UDA). Recent approaches to
self-training have achieved remarkable performance in UDA for semantic
segmentation, which trains a model with pseudo labels on the target domain.
However, the pseudo labels are noisy due to a discrepancy in the two data
distributions. We identify that context dependency is important for bridging
this domain gap. Motivated by this, we propose QuadFormer, a novel framework
designed for domain adaptive semantic segmentation. The hierarchical quadruple
transformer combines cross-attention and self-attention mechanisms to adapt
transferable context. Based on cross-attentive and self-attentive feature
representations, we introduce a pseudo label correction scheme to online
denoise the pseudo labels and reduce the domain gap. Additionally, we present
two datasets - ARPLSyn and ARPLReal to further advance research in unsupervised
domain adaptive powerline segmentation. Finally, experimental results indicate
that our method achieves state-of-the-art performance for the domain adaptive
power line segmentation on ARPLSyn$\rightarrow$TTTPLA and
ARPLSyn$\rightarrow$ARPLReal.",-0.18360284,-0.1087417,-0.033300806,C
14406,"In this paper, we describe our Ô¨Åndings while including room
type prediction and destination imagination in the Embodied
Referring Expression Grounding task, but several limitations
still require further study.","In Proceedings of the IEEE conference on com-
           Limitations and future work                           puter vision and pattern recognition, 6077‚Äì6086.","Imagination is not dynamic and it is only conditioned on
the given instruction.",2022-11-30 23:36:17+00:00,Layout-aware Dreamer for Embodied Referring Expression Grounding,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mingxiao Li'), arxiv.Result.Author('Zehao Wang'), arxiv.Result.Author('Tinne Tuytelaars'), arxiv.Result.Author('Marie-Francine Moens')]","In this work, we study the problem of Embodied Referring Expression
Grounding, where an agent needs to navigate in a previously unseen environment
and localize a remote object described by a concise high-level natural language
instruction. When facing such a situation, a human tends to imagine what the
destination may look like and to explore the environment based on prior
knowledge of the environmental layout, such as the fact that a bathroom is more
likely to be found near a bedroom than a kitchen. We have designed an
autonomous agent called Layout-aware Dreamer (LAD), including two novel
modules, that is, the Layout Learner and the Goal Dreamer to mimic this
cognitive decision process. The Layout Learner learns to infer the room
category distribution of neighboring unexplored areas along the path for coarse
layout estimation, which effectively introduces layout common sense of
room-to-room transitions to our agent. To learn an effective exploration of the
environment, the Goal Dreamer imagines the destination beforehand. Our agent
achieves new state-of-the-art performance on the public leaderboard of the
REVERIE dataset in challenging unseen test environments with improvement in
navigation success (SR) by 4.02% and remote grounding success (RGS) by 3.43%
compared to the previous state-of-the-art. The code is released at
https://github.com/zehao-wang/LAD",-0.10213095,-0.08761541,-0.3063379,C
14407,"In this paper, we describe our Ô¨Åndings while including room
type prediction and destination imagination in the Embodied
Referring Expression Grounding task, but several limitations
still require further study.","In Proceedings of the IEEE conference on com-
           Limitations and future work                           puter vision and pattern recognition, 6077‚Äì6086.","Imagination is not dynamic and it is only conditioned on
the given instruction.",2022-11-30 23:36:17+00:00,Layout-aware Dreamer for Embodied Referring Expression Grounding,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mingxiao Li'), arxiv.Result.Author('Zehao Wang'), arxiv.Result.Author('Tinne Tuytelaars'), arxiv.Result.Author('Marie-Francine Moens')]","In this work, we study the problem of Embodied Referring Expression
Grounding, where an agent needs to navigate in a previously unseen environment
and localize a remote object described by a concise high-level natural language
instruction. When facing such a situation, a human tends to imagine what the
destination may look like and to explore the environment based on prior
knowledge of the environmental layout, such as the fact that a bathroom is more
likely to be found near a bedroom than a kitchen. We have designed an
autonomous agent called Layout-aware Dreamer (LAD), including two novel
modules, that is, the Layout Learner and the Goal Dreamer to mimic this
cognitive decision process. The Layout Learner learns to infer the room
category distribution of neighboring unexplored areas along the path for coarse
layout estimation, which effectively introduces layout common sense of
room-to-room transitions to our agent. To learn an effective exploration of the
environment, the Goal Dreamer imagines the destination beforehand. Our agent
achieves new state-of-the-art performance on the public leaderboard of the
REVERIE dataset in challenging unseen test environments with improvement in
navigation success (SR) by 4.02% and remote grounding success (RGS) by 3.43%
compared to the previous state-of-the-art. The code is released at
https://github.com/zehao-wang/LAD",-0.10213095,-0.08761541,-0.3063379,C
14419,"premise, thus opening avenues for further research in combined
OMR-AMT transcription.","similar manner, we assume that for a certain musician it would
The results obtained, while showing a narrow improvement                be, at least, more appealing to play a composition reading a
with respect to the best individual modality, validate our initial      music sheet rather than manually transcribing it.","This work, therefore, aims at exploring, as a proof of
                                                                        concept, whether the transcription results of a multimodal
   Index Terms‚ÄîOptical Music Recognition, Automatic Music               combination of sheet scores and acoustic performances of
Transcription, Multimodal Recognition                                   music pieces improves those of the stand-alone modalities.",2022-12-01 09:18:24+00:00,Proceedings of the 3rd International Workshop on Reading Music Systems,cs.CV,"['cs.CV', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Jorge Calvo-Zaragoza'), arxiv.Result.Author('Alexander Pacha')]","The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 3rd International Workshop on Reading Music
Systems, held in Alicante on the 23rd of July 2021.",0.19139814,-0.023464616,-0.14865221,A
14420,"Despite the positive connotation of the paper, we believe
   This new experiment summarized the behavior of all al-                           that further research is required to maximize the potential
ternatives.","different margins, depending on the complexity of the dataset.","On the one hand, a direct encoding pipeline ‚Äî                           beneÔ¨Åts of this approach.",2022-12-01 09:18:24+00:00,Proceedings of the 3rd International Workshop on Reading Music Systems,cs.CV,"['cs.CV', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Jorge Calvo-Zaragoza'), arxiv.Result.Author('Alexander Pacha')]","The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 3rd International Workshop on Reading Music
Systems, held in Alicante on the 23rd of July 2021.",0.26028422,-0.18555099,-0.012662313,A
14421,"We
   Despite improvements in music notation software, writing               expect this to facilitate further research in end-to-end deep
music on paper or distributing music in print is still very               learning systems in OMR, which are quickly becoming state-
common.",INTRODUCTION                                 bounding boxes and pixel information of each element.,"However, written music needs to be digitised for                  of-the-art.",2022-12-01 09:18:24+00:00,Proceedings of the 3rd International Workshop on Reading Music Systems,cs.CV,"['cs.CV', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Jorge Calvo-Zaragoza'), arxiv.Result.Author('Alexander Pacha')]","The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 3rd International Workshop on Reading Music
Systems, held in Alicante on the 23rd of July 2021.",0.07332234,-0.080092646,0.02011275,C
14428,"13
C LIMITATIONS

There remain many limitations that deserve further study.","We use the ofÔ¨Åcial SwinIR-L (Liang et al., 2021) and
BSRGAN (Zhang et al., 2021) pretrained for SR tasks.","‚Ä¢ Though DDNM brings negligible extra cost on computations, it is still limited by the slow
          inference speed of existing diffusion models.",2022-12-01 13:33:47+00:00,Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yinhuai Wang'), arxiv.Result.Author('Jiwen Yu'), arxiv.Result.Author('Jian Zhang')]","Most existing Image Restoration (IR) models are task-specific, which can not
be generalized to different degradation operators. In this work, we propose the
Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for
arbitrary linear IR problems, including but not limited to image
super-resolution, colorization, inpainting, compressed sensing, and deblurring.
DDNM only needs a pre-trained off-the-shelf diffusion model as the generative
prior, without any extra training or network modifications. By refining only
the null-space contents during the reverse diffusion process, we can yield
diverse results satisfying both data consistency and realness. We further
propose an enhanced and robust version, dubbed DDNM+, to support noisy
restoration and improve restoration quality for hard tasks. Our experiments on
several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot
IR methods. We also demonstrate that DDNM+ can solve complex real-world
applications, e.g., old photo restoration.",0.295497,-0.09998834,0.20039292,A
14429,"13
C LIMITATIONS

There remain many limitations that deserve further study.","We use the ofÔ¨Åcial SwinIR-L (Liang et al., 2021) and
BSRGAN (Zhang et al., 2021) pretrained for SR tasks.","‚Ä¢ Though DDNM brings negligible extra cost on computations, it is still limited by the slow
          inference speed of existing diffusion models.",2022-12-01 13:33:47+00:00,Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yinhuai Wang'), arxiv.Result.Author('Jiwen Yu'), arxiv.Result.Author('Jian Zhang')]","Most existing Image Restoration (IR) models are task-specific, which can not
be generalized to different degradation operators. In this work, we propose the
Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for
arbitrary linear IR problems, including but not limited to image
super-resolution, colorization, inpainting, compressed sensing, and deblurring.
DDNM only needs a pre-trained off-the-shelf diffusion model as the generative
prior, without any extra training or network modifications. By refining only
the null-space contents during the reverse diffusion process, we can yield
diverse results satisfying both data consistency and realness. We further
propose an enhanced and robust version, dubbed DDNM+, to support noisy
restoration and improve restoration quality for hard tasks. Our experiments on
several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot
IR methods. We also demonstrate that DDNM+ can solve complex real-world
applications, e.g., old photo restoration.",0.295497,-0.09998834,0.20039292,A
14432,"To
average CD of the results from I view images of the j-th                     further study the effect of view angles of images on our results,
object.","However, the standard
where J denotes the number of objects, and CDj means the                     deviation of the ‚Äúlamp‚Äù is much larger than those of others.",Table V lists the standard deviation of each category.,2022-12-01 15:11:21+00:00,Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lintai Wu'), arxiv.Result.Author('Qijian Zhang'), arxiv.Result.Author('Junhui Hou'), arxiv.Result.Author('Yong Xu')]","Point clouds captured by scanning devices are often incomplete due to
occlusion. Point cloud completion aims to predict the complete shape based on
its partial input. Existing methods can be classified into supervised and
unsupervised methods. However, both of them require a large number of 3D
complete point clouds, which are difficult to capture. In this paper, we
propose Cross-PCC, an unsupervised point cloud completion method without
requiring any 3D complete point clouds. We only utilize 2D images of the
complete objects, which are easier to capture than 3D complete and clean point
clouds. Specifically, to take advantage of the complementary information from
2D images, we use a single-view RGB image to extract 2D features and design a
fusion module to fuse the 2D and 3D features extracted from the partial point
cloud. To guide the shape of predicted point clouds, we project the predicted
points of the object to the 2D plane and use the foreground pixels of its
silhouette maps to constrain the position of the projected points. To reduce
the outliers of the predicted point clouds, we propose a view calibrator to
move the points projected to the background into the foreground by the
single-view silhouette image. To the best of our knowledge, our approach is the
first point cloud completion method that does not require any 3D supervision.
The experimental results of our method are superior to those of the
state-of-the-art unsupervised methods by a large margin. Moreover, compared to
some supervised methods, our method achieves similar performance. We will make
the source code publicly available at https://github.com/ltwu6/cross-pcc.",0.16385579,0.2129135,-0.10362502,A
14441,"To further study this behav-      and might fail with data that is far from the training distri-
ior, we perform a cycle test on the point encoder, where we        bution.","Like many other data-driven methods, our
injective mapping and the point encoder serves as a noise          method requires a large amount of diverse sstraining data
canceller in the encoding space.","Our model is currently not relightable and can not
add noise n to a certain encoding z and get a noisy version        animate new hairstyles.",2022-12-01 16:09:54+00:00,NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Ziyan Wang'), arxiv.Result.Author('Giljoo Nam'), arxiv.Result.Author('Tuur Stuyck'), arxiv.Result.Author('Stephen Lombardi'), arxiv.Result.Author('Chen Cao'), arxiv.Result.Author('Jason Saragih'), arxiv.Result.Author('Michael Zollhoefer'), arxiv.Result.Author('Jessica Hodgins'), arxiv.Result.Author('Christoph Lassner')]","The capture and animation of human hair are two of the major challenges in
the creation of realistic avatars for the virtual reality. Both problems are
highly challenging, because hair has complex geometry and appearance, as well
as exhibits challenging motion. In this paper, we present a two-stage approach
that models hair independently from the head to address these challenges in a
data-driven manner. The first stage, state compression, learns a
low-dimensional latent space of 3D hair states containing motion and
appearance, via a novel autoencoder-as-a-tracker strategy. To better
disentangle the hair and head in appearance learning, we employ multi-view hair
segmentation masks in combination with a differentiable volumetric renderer.
The second stage learns a novel hair dynamics model that performs temporal hair
transfer based on the discovered latent codes. To enforce higher stability
while driving our dynamics model, we employ the 3D point-cloud autoencoder from
the compression stage for de-noising of the hair state. Our model outperforms
the state of the art in novel view synthesis and is capable of creating novel
hair animations without having to rely on hair observations as a driving
signal.",0.051403187,0.030954462,0.034101486,A
14442,"To further study this behav-      and might fail with data that is far from the training distri-
ior, we perform a cycle test on the point encoder, where we        bution.","Like many other data-driven methods, our
injective mapping and the point encoder serves as a noise          method requires a large amount of diverse sstraining data
canceller in the encoding space.","Our model is currently not relightable and can not
add noise n to a certain encoding z and get a noisy version        animate new hairstyles.",2022-12-01 16:09:54+00:00,NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Ziyan Wang'), arxiv.Result.Author('Giljoo Nam'), arxiv.Result.Author('Tuur Stuyck'), arxiv.Result.Author('Stephen Lombardi'), arxiv.Result.Author('Chen Cao'), arxiv.Result.Author('Jason Saragih'), arxiv.Result.Author('Michael Zollhoefer'), arxiv.Result.Author('Jessica Hodgins'), arxiv.Result.Author('Christoph Lassner')]","The capture and animation of human hair are two of the major challenges in
the creation of realistic avatars for the virtual reality. Both problems are
highly challenging, because hair has complex geometry and appearance, as well
as exhibits challenging motion. In this paper, we present a two-stage approach
that models hair independently from the head to address these challenges in a
data-driven manner. The first stage, state compression, learns a
low-dimensional latent space of 3D hair states containing motion and
appearance, via a novel autoencoder-as-a-tracker strategy. To better
disentangle the hair and head in appearance learning, we employ multi-view hair
segmentation masks in combination with a differentiable volumetric renderer.
The second stage learns a novel hair dynamics model that performs temporal hair
transfer based on the discovered latent codes. To enforce higher stability
while driving our dynamics model, we employ the 3D point-cloud autoencoder from
the compression stage for de-noising of the hair state. Our model outperforms
the state of the art in novel view synthesis and is capable of creating novel
hair animations without having to rely on hair observations as a driving
signal. Project page is here https://ziyanw1.github.io/neuwigs/.",0.051403187,0.030954462,0.034101486,A
14443,"12, 13, our model gener-
                                                                      ates reasonable hair motions with respect to the head motion
   To further study the point encoder E‚Äôs ability on denois-          while preserving multi-view consistency.",As shown in Figs.,"ing the encoding, we tested the encoder with inputs that
containing different level of noise.",2022-12-01 16:09:54+00:00,NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Ziyan Wang'), arxiv.Result.Author('Giljoo Nam'), arxiv.Result.Author('Tuur Stuyck'), arxiv.Result.Author('Stephen Lombardi'), arxiv.Result.Author('Chen Cao'), arxiv.Result.Author('Jason Saragih'), arxiv.Result.Author('Michael Zollhoefer'), arxiv.Result.Author('Jessica Hodgins'), arxiv.Result.Author('Christoph Lassner')]","The capture and animation of human hair are two of the major challenges in
the creation of realistic avatars for the virtual reality. Both problems are
highly challenging, because hair has complex geometry and appearance, as well
as exhibits challenging motion. In this paper, we present a two-stage approach
that models hair independently from the head to address these challenges in a
data-driven manner. The first stage, state compression, learns a
low-dimensional latent space of 3D hair states containing motion and
appearance, via a novel autoencoder-as-a-tracker strategy. To better
disentangle the hair and head in appearance learning, we employ multi-view hair
segmentation masks in combination with a differentiable volumetric renderer.
The second stage learns a novel hair dynamics model that performs temporal hair
transfer based on the discovered latent codes. To enforce higher stability
while driving our dynamics model, we employ the 3D point-cloud autoencoder from
the compression stage for de-noising of the hair state. Our model outperforms
the state of the art in novel view synthesis and is capable of creating novel
hair animations without having to rely on hair observations as a driving
signal. Project page is here https://ziyanw1.github.io/neuwigs/.",0.05336389,0.19355902,0.08033282,A
14453,"(b) Our method uses cheap pseudo labels
                                        publicly available for further research.",Code will be       tated data for training.,Code is available    generated from part-experts for training.,2022-11-30 02:47:18+00:00,FuRPE: Learning Full-body Reconstruction from Part Experts,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhaoxin Fan'), arxiv.Result.Author('Yuqing Pan'), arxiv.Result.Author('Hao Xu'), arxiv.Result.Author('Zhenbo Song'), arxiv.Result.Author('Zhicheng Wang'), arxiv.Result.Author('Kejian Wu'), arxiv.Result.Author('Hongyan Liu'), arxiv.Result.Author('Jun He')]","Full-body reconstruction is a fundamental but challenging task. Owing to the
lack of annotated data, the performances of existing methods are largely
limited. In this paper, we propose a novel method named Full-body
Reconstruction from Part Experts~(FuRPE) to tackle this issue. In FuRPE, the
network is trained using pseudo labels and features generated from
part-experts. An simple yet effective pseudo ground-truth selection scheme is
proposed to extract high-quality pseudo labels. In this way, a large-scale of
existing human body reconstruction datasets can be leveraged and contribute to
the model training. In addition, an exponential moving average training
strategy is introduced to train the network in a self-supervised manner,
further boosting the performance of the model. Extensive experiments on several
widely used datasets demonstrate the effectiveness of our method over the
baseline. Our method achieves the state-of-the-art performance. Code will be
publicly available for further research.",0.11429709,-0.16407393,-0.2088921,A
14463,"While our initial results
it can be tailored to speciÔ¨Åc edge cases that are rare in real     are very promising, further research in synthetic datasets is
data.",One key advantage of synthetic data is that       for 3D human segmentation tasks.,The synthetic data in this work (Sec.,2022-12-01 18:59:21+00:00,3D Segmentation of Humans in Point Clouds with Synthetic Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ay√ßa Takmaz'), arxiv.Result.Author('Jonas Schult'), arxiv.Result.Author('Irem Kaftan'), arxiv.Result.Author('Mertcan Ak√ßay'), arxiv.Result.Author('Robert Sumner'), arxiv.Result.Author('Bastian Leibe'), arxiv.Result.Author('Francis Engelmann'), arxiv.Result.Author('Siyu Tang')]","Segmenting humans in 3D indoor scenes has become increasingly important with
the rise of human-centered robotics and AR/VR applications. In this direction,
we explore the tasks of 3D human semantic-, instance- and multi-human body-part
segmentation. Few works have attempted to directly segment humans in point
clouds (or depth maps), which is largely due to the lack of training data on
humans interacting with 3D scenes. We address this challenge and propose a
framework for synthesizing virtual humans in realistic 3D scenes. Synthetic
point cloud data is attractive since the domain gap between real and synthetic
depth is small compared to images. Our analysis of different training schemes
using a combination of synthetic and realistic data shows that synthetic data
for pre-training improves performance in a wide variety of segmentation tasks
and models. We further propose the first end-to-end model for 3D multi-human
body-part segmentation, called Human3D, that performs all the above
segmentation tasks in a unified manner. Remarkably, Human3D even outperforms
previous task-specific state-of-the-art methods. Finally, we manually annotate
humans in test scenes from EgoBody to compare the proposed training schemes and
segmentation models.",-0.24553156,0.10974446,0.005134017,B
14464,"While our initial results
it can be tailored to speciÔ¨Åc edge cases that are rare in real     are very promising, further research in synthetic datasets is
data.",One key advantage of synthetic data is that       for 3D human segmentation tasks.,The synthetic data in this work (Sec.,2022-12-01 18:59:21+00:00,3D Segmentation of Humans in Point Clouds with Synthetic Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ay√ßa Takmaz'), arxiv.Result.Author('Jonas Schult'), arxiv.Result.Author('Irem Kaftan'), arxiv.Result.Author('Mertcan Ak√ßay'), arxiv.Result.Author('Bastian Leibe'), arxiv.Result.Author('Robert Sumner'), arxiv.Result.Author('Francis Engelmann'), arxiv.Result.Author('Siyu Tang')]","Segmenting humans in 3D indoor scenes has become increasingly important with
the rise of human-centered robotics and AR/VR applications. In this direction,
we explore the tasks of 3D human semantic-, instance- and multi-human body-part
segmentation. Few works have attempted to directly segment humans in point
clouds (or depth maps), which is largely due to the lack of training data on
humans interacting with 3D scenes. We address this challenge and propose a
framework for synthesizing virtual humans in realistic 3D scenes. Synthetic
point cloud data is attractive since the domain gap between real and synthetic
depth is small compared to images. Our analysis of different training schemes
using a combination of synthetic and realistic data shows that synthetic data
for pre-training improves performance in a wide variety of segmentation tasks
and models. We further propose the first end-to-end model for 3D multi-human
body-part segmentation, called Human3D, that performs all the above
segmentation tasks in a unified manner. Remarkably, Human3D even outperforms
previous task-specific state-of-the-art methods. Finally, we manually annotate
humans in test scenes from EgoBody to compare the proposed training schemes and
segmentation models.",-0.24553156,0.10974446,0.005134017,B
14465,"While our initial results
it can be tailored to speciÔ¨Åc edge cases that are rare in real     are very promising, further research in synthetic datasets is
data.",One key advantage of synthetic data is that       for 3D human segmentation tasks.,The synthetic data in this work (Sec.,2022-12-01 18:59:21+00:00,3D Segmentation of Humans in Point Clouds with Synthetic Data,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ay√ßa Takmaz'), arxiv.Result.Author('Jonas Schult'), arxiv.Result.Author('Irem Kaftan'), arxiv.Result.Author('Mertcan Ak√ßay'), arxiv.Result.Author('Bastian Leibe'), arxiv.Result.Author('Robert Sumner'), arxiv.Result.Author('Francis Engelmann'), arxiv.Result.Author('Siyu Tang')]","Segmenting humans in 3D indoor scenes has become increasingly important with
the rise of human-centered robotics and AR/VR applications. In this direction,
we explore the tasks of 3D human semantic-, instance- and multi-human body-part
segmentation. Few works have attempted to directly segment humans in point
clouds (or depth maps), which is largely due to the lack of training data on
humans interacting with 3D scenes. We address this challenge and propose a
framework for synthesizing virtual humans in realistic 3D scenes. Synthetic
point cloud data is attractive since the domain gap between real and synthetic
depth is small compared to images. Our analysis of different training schemes
using a combination of synthetic and realistic data shows that synthetic data
for pre-training improves performance in a wide variety of segmentation tasks
and models. We further propose the first end-to-end model for 3D multi-human
body-part segmentation, called Human3D, that performs all the above
segmentation tasks in a unified manner. Remarkably, Human3D even outperforms
previous task-specific state-of-the-art methods. Finally, we manually annotate
humans in test scenes from EgoBody to compare the proposed training schemes and
segmentation models.",-0.24553156,0.10974446,0.005134017,B
14467,"We Ô¨Ånd that the performance on
We further study key local context selection in a       this subset is much better than on the whole test set,
practical (automatic) setting, where we Ô¨Årst detect     where on average only articles only contain 59%
a set of visual named entities with a multi-modal       of the caption-relevant named entities.",rized in Table 11.,"retrieval pipeline, Ô¨Ånd related non-visual named en-
tities via relation extraction, then use these named       Finally, since we leverage the large-scale pre-
entities to pick relevant news article sentences.",2022-12-01 20:00:27+00:00,Focus! Relevant and Sufficient Context Selection for News Image Captioning,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Mingyang Zhou'), arxiv.Result.Author('Grace Luo'), arxiv.Result.Author('Anna Rohrbach'), arxiv.Result.Author('Zhou Yu')]","News Image Captioning requires describing an image by leveraging additional
context from a news article. Previous works only coarsely leverage the article
to extract the necessary context, which makes it challenging for models to
identify relevant events and named entities. In our paper, we first demonstrate
that by combining more fine-grained context that captures the key named
entities (obtained via an oracle) and the global context that summarizes the
news, we can dramatically improve the model's ability to generate accurate news
captions. This begs the question, how to automatically extract such key
entities from an image? We propose to use the pre-trained vision and language
retrieval model CLIP to localize the visually grounded entities in the news
article and then capture the non-visual entities via an open relation
extraction model. Our experiments demonstrate that by simply selecting a better
context from the article, we can significantly improve the performance of
existing models and achieve new state-of-the-art performance on multiple
benchmarks.",0.007187226,-0.21352038,-0.2453029,C
14490,"Therefore, a DWR module is designed for                  and structure of DRWSeg foster further research in seman-
the high stages of the network; this module is capable of               tic segmentation.","We hope that the idea
speciÔ¨Åc layer.","extracting features with different sizes of receptive Ô¨Åelds
by multibranch dilated convolutions.",2022-12-02 13:55:41+00:00,DWRSeg: Dilation-wise Residual Network for Real-time Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Haoran Wei'), arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Shouchun Xu'), arxiv.Result.Author('Zhongjian Dai'), arxiv.Result.Author('Yaping Dai'), arxiv.Result.Author('Xiangyang Xu')]","Real-time semantic segmentation has played an important role in intelligent
vehicle scenarios. Recently, numerous networks have incorporated information
from multi-size receptive fields to facilitate feature extraction in real-time
semantic segmentation tasks. However, these methods preferentially adopt
massive receptive fields to elicit more contextual information, which may
result in inefficient feature extraction. We believe that the elaborated
receptive fields are crucial, considering the demand for efficient feature
extraction in real-time tasks. Therefore, we propose an effective and efficient
architecture termed Dilation-wise Residual segmentation (DWRSeg), which
possesses different sets of receptive field sizes within different stages. The
architecture involves (i) a Dilation-wise Residual (DWR) module for extracting
features based on different scales of receptive fields in the high level of the
network; (ii) a Simple Inverted Residual (SIR) module that uses an inverted
bottleneck structure to extract features from the low stage; and (iii) a simple
fully convolutional network (FCN)-like decoder for aggregating multiscale
feature maps to generate the prediction. Extensive experiments on the
Cityscapes and CamVid datasets demonstrate the effectiveness of our method by
achieving a state-of-the-art trade-off between accuracy and inference speed, in
addition to being lighter weight. Without using pretraining or resorting to any
training trick, we achieve 72.7% mIoU on the Cityscapes test set at a speed of
319.5 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster
than existing methods. The code and trained models are publicly available.",-0.10724446,0.048372436,0.2222764,C
14500,"space, the corresponding BEV feature representation is sig-
   To further study this motivation, we adjust the height            niÔ¨Åcantly different from [‚àí5, 3] or [‚àí4, 2].","Although the largest
                                                                     range [‚àí6, 4] contains the overall information of the whole
motivation.","Since the height
                                                                     information is viewed as channel dimension, we adopt a
range of BEVDepth [17] and evaluate the 3D object detec-             channel-wise attention [12] to adaptively aggregate the mul-
tion performance of different classes as shown in Tab.",2022-12-02 15:14:48+00:00,BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaowei Chi'), arxiv.Result.Author('Jiaming Liu'), arxiv.Result.Author('Ming Lu'), arxiv.Result.Author('Rongyu Zhang'), arxiv.Result.Author('Zhaoqing Wang'), arxiv.Result.Author('Yandong Guo'), arxiv.Result.Author('Shanghang Zhang')]","Bird's-Eye-View (BEV) 3D Object Detection is a crucial multi-view technique
for autonomous driving systems. Recently, plenty of works are proposed,
following a similar paradigm consisting of three essential components, i.e.,
camera feature extraction, BEV feature construction, and task heads. Among the
three components, BEV feature construction is BEV-specific compared with 2D
tasks. Existing methods aggregate the multi-view camera features to the
flattened grid in order to construct the BEV feature. However, flattening the
BEV space along the height dimension fails to emphasize the informative
features of different heights. For example, the barrier is located at a low
height while the truck is located at a high height. In this paper, we propose a
novel method named BEV Slice Attention Network (BEV-SAN) for exploiting the
intrinsic characteristics of different heights. Instead of flattening the BEV
space, we first sample along the height dimension to build the global and local
BEV slices. Then, the features of BEV slices are aggregated from the camera
features and merged by the attention mechanism. Finally, we fuse the merged
local and global BEV features by a transformer to generate the final feature
map for task heads. The purpose of local BEV slices is to emphasize informative
heights. In order to find them, we further propose a LiDAR-guided sampling
strategy to leverage the statistical distribution of LiDAR to determine the
heights of local slices. Compared with uniform sampling, LiDAR-guided sampling
can determine more informative heights. We conduct detailed experiments to
demonstrate the effectiveness of BEV-SAN. Code will be released.",-0.07335134,0.059245974,0.05945102,B
14515,"The quantitative results are presented in Fig-
The further study in this regard however is out of the scope                                                                                                                ure 8.",formance.,"Very low Œª values lead to a bad MF estimation,
of this work, which remains as our future work.",2022-12-02 17:46:55+00:00,Neural Radiance Fields for Manhattan Scenes with Unknown Manhattan Frame,cs.CV,['cs.CV'],"[arxiv.Result.Author('Nikola Popovic'), arxiv.Result.Author('Danda Pani Paudel'), arxiv.Result.Author('Luc Van Gool')]","Novel view synthesis and 3D modeling using implicit neural field
representation are shown to be very effective for calibrated multi-view
cameras. Such representations are known to benefit from additional geometric
and semantic supervision. Most existing methods that exploit additional
supervision require dense pixel-wise labels or localized scene priors. These
methods cannot benefit from high-level vague scene priors provided in terms of
scenes' descriptions. In this work, we aim to leverage the geometric prior of
Manhattan scenes to improve the implicit neural radiance field representations.
More precisely, we assume that only the knowledge of the scene (under
investigation) being Manhattan is known - with no additional information
whatsoever - with an unknown Manhattan coordinate frame. Such high-level prior
is then used to self-supervise the surface normals derived explicitly in the
implicit neural fields. Our modeling allows us to group the derived normals,
followed by exploiting their orthogonality constraints for self-supervision.
Our exhaustive experiments on datasets of diverse indoor scenes demonstrate the
significant benefit of the proposed method over the established baselines.",0.40946528,0.21331328,0.1001714,A
14519,"We believe our work opens
                                                                 many opportunities for applications and further research.","Moreover, we observe in the tests
                                                                 with our new dataset that the advantages of the proposed
                                                                 approach and monocularized setting can directly be trans-
                                                                 ferred to real-world recordings.","Our supplementary material contains additional de-
                                                                 tails on our new dataset, and in depth comparisons
                                                                 with other techniques.",2022-12-02 18:51:10+00:00,Fast Non-Rigid Radiance Fields from Monocularized Data,cs.CV,"['cs.CV', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Moritz Kappel'), arxiv.Result.Author('Vladislav Golyanik'), arxiv.Result.Author('Susana Castillo'), arxiv.Result.Author('Christian Theobalt'), arxiv.Result.Author('Marcus Magnor')]","3D reconstruction and novel view synthesis of dynamic scenes from collections
of single views recently gained increased attention. Existing work shows
impressive results for synthetic setups and forward-facing real-world data, but
is severely limited in the training speed and angular range for generating
novel views. This paper addresses these limitations and proposes a new method
for full 360{\deg} novel view synthesis of non-rigidly deforming scenes. At the
core of our method are: 1) An efficient deformation module that decouples the
processing of spatial and temporal information for acceleration at training and
inference time; and 2) A static module representing the canonical scene as a
fast hash-encoded neural radiance field. We evaluate the proposed approach on
the established synthetic D-NeRF benchmark, that enables efficient
reconstruction from a single monocular view per time-frame randomly sampled
from a full hemisphere. We refer to this form of inputs as monocularized data.
To prove its practicality for real-world scenarios, we recorded twelve
challenging sequences with human actors by sampling single frames from a
synchronized multi-view rig. In both cases, our method is trained significantly
faster than previous methods (minutes instead of days) while achieving higher
visual accuracy for generated novel views. Our source code and data is
available at our project page
https://graphics.tu-bs.de/publications/kappel2022fast.",-0.10546462,0.08870471,0.018085124,B
14538,some issues deserving further study.,"Although LTrack achieves remarkable perfor-            itation, there are two promising research topics for our future
mance on the cross-domain MOT benchmark, there are still           works.","SpeciÔ¨Åcally, LTrack
does not behave well when many targets with similar ap-               (1) How to distinguish similar targets during tracking is a
pearance and actions occur in the same frame.",2022-12-03 07:57:31+00:00,Generalizing Multiple Object Tracking to Unseen Domains by Introducing Natural Language Representation,cs.CV,['cs.CV'],"[arxiv.Result.Author('En Yu'), arxiv.Result.Author('Songtao Liu'), arxiv.Result.Author('Zhuoling Li'), arxiv.Result.Author('Jinrong Yang'), arxiv.Result.Author('Zeming li'), arxiv.Result.Author('Shoudong Han'), arxiv.Result.Author('Wenbing Tao')]","Although existing multi-object tracking (MOT) algorithms have obtained
competitive performance on various benchmarks, almost all of them train and
validate models on the same domain. The domain generalization problem of MOT is
hardly studied. To bridge this gap, we first draw the observation that the
high-level information contained in natural language is domain invariant to
different tracking domains. Based on this observation, we propose to introduce
natural language representation into visual MOT models for boosting the domain
generalization ability. However, it is infeasible to label every tracking
target with a textual description. To tackle this problem, we design two
modules, namely visual context prompting (VCP) and visual-language mixing
(VLM). Specifically, VCP generates visual prompts based on the input frames.
VLM joints the information in the generated visual prompts and the textual
prompts from a pre-defined Trackbook to obtain instance-level pseudo textual
description, which is domain invariant to different tracking scenes. Through
training models on MOT17 and validating them on MOT20, we observe that the
pseudo textual descriptions generated by our proposed modules improve the
generalization performance of query-based trackers by large margins.",0.071221635,0.16147852,0.0024169162,A
14542,"We further study the selection of the number of multi-
scale deformable attention (MSDeformAtten) layers in pixel decoder to encode the robust mask features.",The Number of MSDeformAtten Layers.,"Table 13: The impact of balance weights Œ≤1 and Œ≤2 in matching cost for the box-level matching
assignment in Box2Mask-T.

Œ≤1 Œ≤2 AP AP50 AP75 APS APM APL

2.0 3.0 39.0 67.6 38.8 7.0 27.8 53.3
2.0 4.0 39.1 66.7 38.8 7.7 27.4 53.7
2.0 6.0 39.4 67.6 39.3 6.8 27.4 53.9
3.0 6.0 39.1 67.3 39.2 6.5 27.2 53.8
1.0 6.0 38.1 66.1 38.0 7.3 26.6 51.8

Table 14: The effectiveness of the number of MSDeformAtten layers in the pixel decoder of
Box2Mask-T.

number AP AP50 AP75 APS APM APL

1  36.8 65.7 36.1 6.1 26.8 50.0

2  38.9 67.7 38.7 6.9 27.8 52.8

4  39.3 67.1 38.9 7.2 28.0 53.6

5  39.4 67.9 39.7 7.8 27.9 53.8

6  39.3 67.3 39.7 6.8 27.6 53.4

The results are reported in Table 14.",2022-12-03 09:32:14+00:00,Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wentong Li'), arxiv.Result.Author('Wenyu Liu'), arxiv.Result.Author('Jianke Zhu'), arxiv.Result.Author('Miaomiao Cui'), arxiv.Result.Author('Risheng Yu'), arxiv.Result.Author('Xiansheng Hua'), arxiv.Result.Author('Lei Zhang')]","In contrast to fully supervised methods using pixel-wise mask labels,
box-supervised instance segmentation takes advantage of simple box annotations,
which has recently attracted increasing research attention. This paper presents
a novel single-shot instance segmentation approach, namely Box2Mask, which
integrates the classical level-set evolution model into deep neural network
learning to achieve accurate mask prediction with only bounding box
supervision. Specifically, both the input image and its deep features are
employed to evolve the level-set curves implicitly, and a local consistency
module based on a pixel affinity kernel is used to mine the local context and
spatial relations. Two types of single-stage frameworks, i.e., CNN-based and
transformer-based frameworks, are developed to empower the level-set evolution
for box-supervised instance segmentation, and each framework consists of three
essential components: instance-aware decoder, box-level matching assignment and
level-set evolution. By minimizing the level-set energy function, the mask map
of each instance can be iteratively optimized within its bounding box
annotation. The experimental results on five challenging testbeds, covering
general scenes, remote sensing, medical and scene text images, demonstrate the
outstanding performance of our proposed Box2Mask approach for box-supervised
instance segmentation. In particular, with the Swin-Transformer large backbone,
our Box2Mask obtains 42.4% mask AP on COCO, which is on par with the recently
developed fully mask-supervised methods. The code is available at:
https://github.com/LiWentomng/boxlevelset.",-0.006218072,0.09151548,0.084251806,C
14544,"To further study the influence of distillation in the pre-
                                                                       training stage, we try to use the pre-trained CLIP model as
   (a) Ablation studies of Temporal Module on Kinetics-Close.","and Kinetics-Fewshot by using different numbers of layers in the
temporal module.","the teacher model to distill the video and language encoder
                                                                       of our model at the feature level, in addition to the logits
Number of layers 0         1  2     4      6      8                    distillation.",2022-12-03 15:46:49+00:00,VLG: General Video Recognition with Web Textual Knowledge,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jintao Lin'), arxiv.Result.Author('Zhaoyang Liu'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Wayne Wu'), arxiv.Result.Author('Limin Wang')]","Video recognition in an open and dynamic world is quite challenging, as we
need to handle different settings such as close-set, long-tail, few-shot and
open-set. By leveraging semantic knowledge from noisy text descriptions crawled
from the Internet, we focus on the general video recognition (GVR) problem of
solving different recognition tasks within a unified framework. The core
contribution of this paper is twofold. First, we build a comprehensive video
recognition benchmark of Kinetics-GVR, including four sub-task datasets to
cover the mentioned settings. To facilitate the research of GVR, we propose to
utilize external textual knowledge from the Internet and provide multi-source
text descriptions for all action classes. Second, inspired by the flexibility
of language representation, we present a unified visual-linguistic framework
(VLG) to solve the problem of GVR by an effective two-stage training paradigm.
Our VLG is first pre-trained on video and language datasets to learn a shared
feature space, and then devises a flexible bi-modal attention head to
collaborate high-level semantic concepts under different settings. Extensive
results show that our VLG obtains the state-of-the-art performance under four
settings. The superior performance demonstrates the effectiveness and
generalization ability of our proposed framework. We hope our work makes a step
towards the general video recognition and could serve as a baseline for future
research. The code and models will be available at
https://github.com/MCG-NJU/VLG.",0.08849043,-0.20457762,0.005222775,C
14547,"In this part, we further study    has potential applications beyond complex scene completion
the performance of the multi-step model without distillation.","Our preliminary work suggests that PSF
steps to one step by default.",and text-guided generation applications.,2022-12-04 06:10:44+00:00,Fast Point Cloud Generation with Straight Flows,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lemeng Wu'), arxiv.Result.Author('Dilin Wang'), arxiv.Result.Author('Chengyue Gong'), arxiv.Result.Author('Xingchao Liu'), arxiv.Result.Author('Yunyang Xiong'), arxiv.Result.Author('Rakesh Ranjan'), arxiv.Result.Author('Raghuraman Krishnamoorthi'), arxiv.Result.Author('Vikas Chandra'), arxiv.Result.Author('Qiang Liu')]","Diffusion models have emerged as a powerful tool for point cloud generation.
A key component that drives the impressive performance for generating
high-quality samples from noise is iteratively denoise for thousands of steps.
While beneficial, the complexity of learning steps has limited its applications
to many 3D real-world. To address this limitation, we propose Point Straight
Flow (PSF), a model that exhibits impressive performance using one step. Our
idea is based on the reformulation of the standard diffusion model, which
optimizes the curvy learning trajectory into a straight path. Further, we
develop a distillation strategy to shorten the straight path into one step
without a performance loss, enabling applications to 3D real-world with latency
constraints. We perform evaluations on multiple 3D tasks and find that our PSF
performs comparably to the standard diffusion model, outperforming other
efficient 3D point cloud generation methods. On real-world applications such as
point cloud completion and training-free text-guided generation in a
low-latency setup, PSF performs favorably.",0.011321302,-0.00076146703,0.01829813,C
14599,"In the future, we will further study the effec-                           ‚ÄúArbitrary-oriented scene text detection via rotation proposals,‚Äù IEEE
tiveness of taking into account the angle information in the                          Transactions on Multimedia, vol.","Extensive experiments on the DOTA-v1.0 and DOTA-v2.0
benchmarks demonstrate the superiority of the proposed MUS-                     [13] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue,
CDB method.","20, no.",2022-12-06 07:50:00+00:00,MUS-CDB: Mixed Uncertainty Sampling with Class Distribution Balancing for Active Annotation in Aerial Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dong Liang'), arxiv.Result.Author('Jing-Wei Zhang'), arxiv.Result.Author('Ying-Peng Tang'), arxiv.Result.Author('Sheng-Jun Hang')]","Recent aerial object detection models rely on a large amount of labeled
training data, which requires unaffordable manual labeling costs in large
aerial scenes with dense objects. Active learning is effective in reducing the
data labeling cost by selectively querying the informative and representative
unlabelled samples. However, existing active learning methods are mainly with
class-balanced setting and image-based querying for generic object detection
tasks, which are less applicable to aerial object detection scenario due to the
long-tailed class distribution and dense small objects in aerial scenes. In
this paper, we propose a novel active learning method for cost-effective aerial
object detection. Specifically, both object-level and image-level
informativeness are considered in the object selection to refrain from
redundant and myopic querying. Besides, an easy-to-use class-balancing
criterion is incorporated to favor the minority objects to alleviate the
long-tailed class distribution problem in model training. To fully utilize the
queried information, we further devise a training loss to mine the latent
knowledge in the undiscovered image regions. Extensive experiments are
conducted on the DOTA-v1.0 and DOTA-v2.0 benchmarks to validate the
effectiveness of the proposed method. The results show that it can save more
than 75% of the labeling cost to reach the same performance compared to the
baselines and state-of-the-art active object detection methods. Code is
available at https://github.com/ZJW700/MUS-CDB",-0.22017938,0.107792646,-0.1482946,B
14600,"In the future, we will further study the effec-                           ‚ÄúArbitrary-oriented scene text detection via rotation proposals,‚Äù IEEE
tiveness of taking into account the angle information in the                          Transactions on Multimedia, vol.","Extensive experiments on the DOTA-v1.0 and DOTA-v2.0
benchmarks demonstrate the superiority of the proposed MUS-                     [13] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue,
CDB method.","20, no.",2022-12-06 07:50:00+00:00,MUS-CDB: Mixed Uncertainty Sampling with Class Distribution Balancing for Active Annotation in Aerial Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dong Liang'), arxiv.Result.Author('Jing-Wei Zhang'), arxiv.Result.Author('Ying-Peng Tang'), arxiv.Result.Author('Sheng-Jun Huang')]","Recent aerial object detection models rely on a large amount of labeled
training data, which requires unaffordable manual labeling costs in large
aerial scenes with dense objects. Active learning is effective in reducing the
data labeling cost by selectively querying the informative and representative
unlabelled samples. However, existing active learning methods are mainly with
class-balanced setting and image-based querying for generic object detection
tasks, which are less applicable to aerial object detection scenario due to the
long-tailed class distribution and dense small objects in aerial scenes. In
this paper, we propose a novel active learning method for cost-effective aerial
object detection. Specifically, both object-level and image-level
informativeness are considered in the object selection to refrain from
redundant and myopic querying. Besides, an easy-to-use class-balancing
criterion is incorporated to favor the minority objects to alleviate the
long-tailed class distribution problem in model training. To fully utilize the
queried information, we further devise a training loss to mine the latent
knowledge in the undiscovered image regions. Extensive experiments are
conducted on the DOTA-v1.0 and DOTA-v2.0 benchmarks to validate the
effectiveness of the proposed method. The results show that it can save more
than 75% of the labeling cost to reach the same performance compared to the
baselines and state-of-the-art active object detection methods. Code is
available at
\href{https://github.com/ZJW700/MUS-CDB}{\textit{https://github.com/ZJW700/MUS-CDB}}.",-0.22017938,0.107792646,-0.1482946,B
14615,"We hope this work
the selective search as additional pseudo ground truths when       can provide good insights and inspire further researches in
these proposals have no overlaps with the pseudo ground            the relatively under-explored but important problem of open
truths generated by the binary classiÔ¨Åer and no overlaps with      world object detection.",We also use the proposals generated by        the effectiveness of our proposed method.,the ground truths of the known classes.,2022-12-06 13:39:30+00:00,Open World DETR: Transformer based Open World Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Na Dong'), arxiv.Result.Author('Yongqiang Zhang'), arxiv.Result.Author('Mingli Ding'), arxiv.Result.Author('Gim Hee Lee')]","Open world object detection aims at detecting objects that are absent in the
object classes of the training data as unknown objects without explicit
supervision. Furthermore, the exact classes of the unknown objects must be
identified without catastrophic forgetting of the previous known classes when
the corresponding annotations of unknown objects are given incrementally. In
this paper, we propose a two-stage training approach named Open World DETR for
open world object detection based on Deformable DETR. In the first stage, we
pre-train a model on the current annotated data to detect objects from the
current known classes, and concurrently train an additional binary classifier
to classify predictions into foreground or background classes. This helps the
model to build an unbiased feature representations that can facilitate the
detection of unknown classes in subsequent process. In the second stage, we
fine-tune the class-specific components of the model with a multi-view
self-labeling strategy and a consistency constraint. Furthermore, we alleviate
catastrophic forgetting when the annotations of the unknown classes becomes
available incrementally by using knowledge distillation and exemplar replay.
Experimental results on PASCAL VOC and MS-COCO show that our proposed method
outperforms other state-of-the-art open world object detection methods by a
large margin.",-0.18351191,-0.036678262,-0.17264375,B
14620,"8
hope our method can serve as baseline and provides inspi-           [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
ration for further research in semantic segmentation.","We
parameters is calculated with the input size of 480 √ó 480.","Future              Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
work may include how to extend the IncepFormer to large-                  Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
scale model with probably 100M+ parameters and how to                     vain Gelly, et al.",2022-12-06 15:08:00+00:00,IncepFormer: Efficient Inception Transformer with Pyramid Pooling for Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lihua Fu'), arxiv.Result.Author('Haoyue Tian'), arxiv.Result.Author('Xiangping Bryce Zhai'), arxiv.Result.Author('Pan Gao'), arxiv.Result.Author('Xiaojiang Peng')]","Semantic segmentation usually benefits from global contexts, fine
localisation information, multi-scale features, etc. To advance
Transformer-based segmenters with these aspects, we present a simple yet
powerful semantic segmentation architecture, termed as IncepFormer. IncepFormer
has two critical contributions as following. First, it introduces a novel
pyramid structured Transformer encoder which harvests global context and fine
localisation features simultaneously. These features are concatenated and fed
into a convolution layer for final per-pixel prediction. Second, IncepFormer
integrates an Inception-like architecture with depth-wise convolutions, and a
light-weight feed-forward module in each self-attention layer, efficiently
obtaining rich local multi-scale object features. Extensive experiments on five
benchmarks show that our IncepFormer is superior to state-of-the-art methods in
both accuracy and speed, e.g., 1) our IncepFormer-S achieves 47.7% mIoU on
ADE20K which outperforms the existing best method by 1% while only costs half
parameters and fewer FLOPs. 2) Our IncepFormer-B finally achieves 82.0% mIoU on
Cityscapes dataset with 39.6M parameters. Code is
available:github.com/shendu0321/IncepFormer.",-0.028296433,-0.06541244,0.012896628,C
14634,"be partially released in public to spur further research in
The sensors have a high temporal resolution and dynamic        event-based visual localization.","As our task is fairly new, we record
cameras that capture the absolute scene brightness, these      new datasets called EvRooms and EvHumans which will
cameras encode brightness changes as a stream of events.","To summarize, our key con-
range, which is beneÔ¨Åcial for robust localization in chal-     tributions are: (i) robust localization in challenging condi-
lenging scenarios such as low lighting or fast camera mo-      tions using event cameras, (ii) sensor level privacy protec-
tion.",2022-12-04 07:22:17+00:00,Privacy-Preserving Visual Localization with Event Cameras,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junho Kim'), arxiv.Result.Author('Young Min Kim'), arxiv.Result.Author('Yicheng Wu'), arxiv.Result.Author('Ramzi Zahreddine'), arxiv.Result.Author('Weston A. Welge'), arxiv.Result.Author('Gurunandan Krishnan'), arxiv.Result.Author('Sizhuo Ma'), arxiv.Result.Author('Jian Wang')]","We present a robust, privacy-preserving visual localization algorithm using
event cameras. While event cameras can potentially make robust localization due
to high dynamic range and small motion blur, the sensors exhibit large domain
gaps making it difficult to directly apply conventional image-based
localization algorithms. To mitigate the gap, we propose applying
event-to-image conversion prior to localization which leads to stable
localization. In the privacy perspective, event cameras capture only a fraction
of visual information compared to normal cameras, and thus can naturally hide
sensitive visual details. To further enhance the privacy protection in our
event-based pipeline, we introduce privacy protection at two levels, namely
sensor and network level. Sensor level protection aims at hiding facial details
with lightweight filtering while network level protection targets hiding the
entire user's view in private scene applications using a novel neural network
inference pipeline. Both levels of protection involve light-weight computation
and incur only a small performance loss. We thus project our method to serve as
a building block for practical location-based services using event cameras. The
code and dataset will be made public through the following link:
https://github.com/82magnolia/event_localization.",-0.24047866,0.26303694,-0.115459174,B
14635,"be partially released in public to spur further research in
The sensors have a high temporal resolution and dynamic        event-based visual localization.","As our task is fairly new, we record
cameras that capture the absolute scene brightness, these      new datasets called EvRooms and EvHumans which will
cameras encode brightness changes as a stream of events.","To summarize, our key con-
range, which is beneÔ¨Åcial for robust localization in chal-     tributions are: (i) robust localization in challenging condi-
lenging scenarios such as low lighting or fast camera mo-      tions using event cameras, (ii) sensor level privacy protec-
tion.",2022-12-04 07:22:17+00:00,Privacy-Preserving Visual Localization with Event Cameras,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junho Kim'), arxiv.Result.Author('Young Min Kim'), arxiv.Result.Author('Yicheng Wu'), arxiv.Result.Author('Ramzi Zahreddine'), arxiv.Result.Author('Weston A. Welge'), arxiv.Result.Author('Gurunandan Krishnan'), arxiv.Result.Author('Sizhuo Ma'), arxiv.Result.Author('Jian Wang')]","We present a robust, privacy-preserving visual localization algorithm using
event cameras. While event cameras can potentially make robust localization due
to high dynamic range and small motion blur, the sensors exhibit large domain
gaps making it difficult to directly apply conventional image-based
localization algorithms. To mitigate the gap, we propose applying
event-to-image conversion prior to localization which leads to stable
localization. In the privacy perspective, event cameras capture only a fraction
of visual information compared to normal cameras, and thus can naturally hide
sensitive visual details. To further enhance the privacy protection in our
event-based pipeline, we introduce privacy protection at two levels, namely
sensor and network level. Sensor level protection aims at hiding facial details
with lightweight filtering while network level protection targets hiding the
entire user's view in private scene applications using a novel neural network
inference pipeline. Both levels of protection involve light-weight computation
and incur only a small performance loss. We thus project our method to serve as
a building block for practical location-based services using event cameras. The
code and dataset will be made public through the following link:
https://github.com/82magnolia/event_localization.",-0.24047866,0.26303694,-0.115459174,B
14636,"From
this observation, a further research direction is the applicability of the static LFI sensor for
medical or well-being applications in everyday devices.","Static LFI Eye Tracking
in the spectra (A-C) can be mapped to different parts (cornea, lens, retina) of the eye.","Especially if the sensor is integrated
into AR glasses, this might allow early detection of biomarkers, which hint at eye disease like a
cataract.",2022-12-06 18:09:25+00:00,Towards Energy Efficient Mobile Eye Tracking for AR Glasses through Optical Sensor Technology,cs.CV,"['cs.CV', 'cs.HC', 'eess.IV']",[arxiv.Result.Author('Johannes Meyer')],"After the introduction of smartphones and smartwatches, AR glasses are
considered the next breakthrough in the field of wearables. While the
transition from smartphones to smartwatches was based mainly on established
display technologies, the display technology of AR glasses presents a
technological challenge. Many display technologies, such as retina projectors,
are based on continuous adaptive control of the display based on the user's
pupil position. Furthermore, head-mounted systems require an adaptation and
extension of established interaction concepts to provide the user with an
immersive experience. Eye-tracking is a crucial technology to help AR glasses
achieve a breakthrough through optimized display technology and gaze-based
interaction concepts. Available eye-tracking technologies, such as VOG, do not
meet the requirements of AR glasses, especially regarding power consumption,
robustness, and integrability. To further overcome these limitations and push
mobile eye-tracking for AR glasses forward, novel laser-based eye-tracking
sensor technologies are researched in this thesis. The thesis contributes to a
significant scientific advancement towards energy-efficient mobile eye-tracking
for AR glasses.",0.06582961,0.28687933,-0.16347893,B
14637,"A.5.8, we conclude our work and
discuss limitations and further research directions.","Finally, in Sec.","A.5.3 Related Work

One of the Ô¨Årst works introducing a stereo camera approach for calibration-free eye-tracking
was published by [96].",2022-12-06 18:09:25+00:00,Towards Energy Efficient Mobile Eye Tracking for AR Glasses through Optical Sensor Technology,cs.CV,"['cs.CV', 'cs.HC', 'eess.IV']",[arxiv.Result.Author('Johannes Meyer')],"After the introduction of smartphones and smartwatches, AR glasses are
considered the next breakthrough in the field of wearables. While the
transition from smartphones to smartwatches was based mainly on established
display technologies, the display technology of AR glasses presents a
technological challenge. Many display technologies, such as retina projectors,
are based on continuous adaptive control of the display based on the user's
pupil position. Furthermore, head-mounted systems require an adaptation and
extension of established interaction concepts to provide the user with an
immersive experience. Eye-tracking is a crucial technology to help AR glasses
achieve a breakthrough through optimized display technology and gaze-based
interaction concepts. Available eye-tracking technologies, such as VOG, do not
meet the requirements of AR glasses, especially regarding power consumption,
robustness, and integrability. To further overcome these limitations and push
mobile eye-tracking for AR glasses forward, novel laser-based eye-tracking
sensor technologies are researched in this thesis. The thesis contributes to a
significant scientific advancement towards energy-efficient mobile eye-tracking
for AR glasses.",-0.1290964,0.43761808,-0.16883254,B
14638,"(iii) Dataset: We collected a comparable large dataset of 20 participants with commercially
available VOG and IMU sensors commonly used in AR glasses to demonstrate the performance
of our system and publish the dataset for further research 2.","(ii) U-HAR: We adapt a CNN (U-HAR) model architecture and add few shot learning to ef-
Ô¨Åciently recognize a rich set of human activities with a high accuracy from eye- and head
movement features.","In the following Section, we provide an overview of related work with a focus on HAR combin-
ing both features from head- and eye-movements.",2022-12-06 18:09:25+00:00,Towards Energy Efficient Mobile Eye Tracking for AR Glasses through Optical Sensor Technology,cs.CV,"['cs.CV', 'cs.HC', 'eess.IV']",[arxiv.Result.Author('Johannes Meyer')],"After the introduction of smartphones and smartwatches, AR glasses are
considered the next breakthrough in the field of wearables. While the
transition from smartphones to smartwatches was based mainly on established
display technologies, the display technology of AR glasses presents a
technological challenge. Many display technologies, such as retina projectors,
are based on continuous adaptive control of the display based on the user's
pupil position. Furthermore, head-mounted systems require an adaptation and
extension of established interaction concepts to provide the user with an
immersive experience. Eye-tracking is a crucial technology to help AR glasses
achieve a breakthrough through optimized display technology and gaze-based
interaction concepts. Available eye-tracking technologies, such as VOG, do not
meet the requirements of AR glasses, especially regarding power consumption,
robustness, and integrability. To further overcome these limitations and push
mobile eye-tracking for AR glasses forward, novel laser-based eye-tracking
sensor technologies are researched in this thesis. The thesis contributes to a
significant scientific advancement towards energy-efficient mobile eye-tracking
for AR glasses.",-0.27857518,-0.05471128,-0.11026509,B
14657,"There are several potential opportunities for further research, encompassing improve-
ments to the optimization and extensions to the analysis.","This effect
appears to be driven by shading interactions.","Alternate formulations might be
able to avoid the limitations caused by the discretized panel configurations in our approach.",2022-12-07 08:46:04+00:00,Site Assessment and Layout Optimization for Rooftop Solar Energy Generation in Worldview-3 Imagery,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zeyad Awwad'), arxiv.Result.Author('Abdulaziz Alharbi'), arxiv.Result.Author('Abdulelah H. Habib'), arxiv.Result.Author('Olivier L. de Weck')]","With the growth of residential rooftop PV adoption in recent decades, the
problem of 1 effective layout design has become increasingly important in
recent years. Although a number 2 of automated methods have been introduced,
these tend to rely on simplifying assumptions and 3 heuristics to improve
computational tractability. We demonstrate a fully automated layout design 4
pipeline that attempts to solve a more general formulation with greater
geometric flexibility that 5 accounts for shading losses. Our approach
generates rooftop areas from satellite imagery and uses 6 MINLP optimization to
select panel positions, azimuth angles and tilt angles on an individual basis 7
rather than imposing any predefined layouts. Our results demonstrate that
although several common 8 heuristics are often effective, they may not be
universally suitable due to complications resulting 9 from geometric
restrictions and shading losses. Finally, we evaluate a few specific heuristics
from the 10 literature and propose a potential new rule of thumb that may help
improve rooftop solar energy 11 potential when shading effects are considered.",0.2585878,0.2796569,0.07136716,A
14669,"Although this efÔ¨Åcient prompting tech-
                                                                   nique proves to adapt CLIP for video tasks, the model strug-
Being a competitive alternative to other methods in-terms          gles to generalize towards unseen classes due to the late fu-
of accuracy, we further study the compute complexity of            sion through the transformer layers.",temporal modeling.,ViFi-CLIP in comparison to other methods.,2022-12-06 18:59:58+00:00,Fine-tuned CLIP Models are Efficient Video Learners,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Hanoona Rasheed'), arxiv.Result.Author('Muhammad Uzair Khattak'), arxiv.Result.Author('Muhammad Maaz'), arxiv.Result.Author('Salman Khan'), arxiv.Result.Author('Fahad Shahbaz Khan')]","Large-scale multi-modal training with image-text pairs imparts strong
generalization to CLIP model. Since training on a similar scale for videos is
infeasible, recent approaches focus on the effective transfer of image-based
CLIP to the video domain. In this pursuit, new parametric modules are added to
learn temporal information and inter-frame relationships which require
meticulous design efforts. Furthermore, when the resulting models are learned
on videos, they tend to overfit on the given task distribution and lack in
generalization aspect. This begs the following question: How to effectively
transfer image-level CLIP representations to videos? In this work, we show that
a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to
bridge the domain gap from images to videos. Our qualitative analysis
illustrates that the frame-level processing from CLIP image-encoder followed by
feature pooling and similarity matching with corresponding text embeddings
helps in implicitly modeling the temporal cues within ViFi-CLIP. Such
fine-tuning helps the model to focus on scene dynamics, moving objects and
inter-object relationships. For low-data regimes where full fine-tuning is not
viable, we propose a `bridge and prompt' approach that first uses fine-tuning
to bridge the domain gap and then learns prompts on language and vision side to
adapt CLIP representations. We extensively evaluate this simple yet strong
baseline on zero-shot, base-to-novel generalization, few-shot and fully
supervised settings across five video benchmarks. Our code is available at
https://github.com/muzairkhattak/ViFi-CLIP.",-0.0104882885,0.027413595,0.15553565,C
14682,"We hope that                     eling visual context is key to augmenting object detection
X-Paste will foster further research on utilizing zero-shot                  datasets.","Mod-
strong baselines on LVIS and COCO datasets.","In Proceedings of the European Conference on
recognition or generative models for various vision tasks.",2022-12-07 18:59:59+00:00,X-Paste: Revisit Copy-Paste at Scale with CLIP and StableDiffusion,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Hanqing Zhao'), arxiv.Result.Author('Dianmo Sheng'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Lu Yuan'), arxiv.Result.Author('Ce Liu'), arxiv.Result.Author('Wenbo Zhou'), arxiv.Result.Author('Qi Chu'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Nenghai Yu')]","Copy-Paste is a simple and effective data augmentation strategy for instance
segmentation. By randomly pasting object instances onto new background images,
it creates new training data for free and significantly boosts the segmentation
performance, especially for rare object categories. Although diverse,
high-quality object instances used in Copy-Paste result in more performance
gain, previous works utilize object instances either from human-annotated
instance segmentation datasets or rendered from 3D object models, and both
approaches are too expensive to scale up to obtain good diversity. In this
paper, we revisit Copy-Paste at scale with the power of newly emerged zero-shot
recognition models (e.g., CLIP) and text2image models (e.g., StableDiffusion).
We demonstrate for the first time that using a text2image model to generate
images or zero-shot recognition model to filter noisily crawled images for
different object categories is a feasible way to make Copy-Paste truly
scalable. To make such success happen, we design a data acquisition and
processing framework, dubbed ""X-Paste"", upon which a systematic study is
conducted. On the LVIS dataset, X-Paste provides impressive improvements over
the strong baseline CenterNet2 with Swin-L as the backbone. Specifically, it
archives +2.6 box AP and +2.1 mask AP gains on all classes and even more
significant gains with +6.8 box AP +6.5 mask AP on long-tail classes.",-0.35963058,-0.14941108,-0.0021497535,C
14701,"Therefore, we hope our work
can provide a baseline and lead to further research         [3] Isola, P., Zhu, J., Zhou, T., Efros, A.A.:
in this realm.","ferent characteristics to train a robust model is               CoRR abs/1505.07818 (2015)
of great signiÔ¨Åcance.","Image-to-image translation with conditional
                                                                adversarial networks.",2022-12-08 12:04:01+00:00,An Empirical Study on Multi-Domain Robust Semantic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yajie Liu'), arxiv.Result.Author('Pu Ge'), arxiv.Result.Author('Qingjie Liu'), arxiv.Result.Author('Shichao Fan'), arxiv.Result.Author('Yunhong Wang')]","How to effectively leverage the plentiful existing datasets to train a robust
and high-performance model is of great significance for many practical
applications. However, a model trained on a naive merge of different datasets
tends to obtain poor performance due to annotation conflicts and domain
divergence.In this paper, we attempt to train a unified model that is expected
to perform well across domains on several popularity segmentation datasets.We
conduct a detailed analysis of the impact on model generalization from three
aspects of data augmentation, training strategies, and model capacity.Based on
the analysis, we propose a robust solution that is able to improve model
generalization across domains.Our solution ranks 2nd on RVC 2022 semantic
segmentation task, with a dataset only 1/3 size of the 1st model used.",-0.1948987,-0.2365417,0.039484885,C
14736,"To ensure our results‚Äô reproducibility and promote further research on holistic
surgical understanding, we will make publicly available the PSI-AVA dataset, an-
4  N. Valderrama et al.","(2) We propose TAPIR, a transformer-based method that leverages the
multi-level annotations of PSI-AVA dataset and establishes a baseline for future
work in our holistic benchmark.","W                                                                 Frame Classification Head

                                   Class Token                             Linear                                                                                     Phases
                                   thw                                   Classifier                                                                                   Steps

                                             (thw + 1) x C           Box Classification Head
                                H  Video
                               c      Feature Extractor     mean                                                                                                      Instruments
                         T                                  pooling
                                                                                                                                          Box Proposals
                                                                                                                                                        Box Features  Atomic
                                                                                                                                                                      Actions
                                                                                                                                                  Linear Classifier
                                   Instrument Detector

Fig.",2022-12-08 22:15:27+00:00,Towards Holistic Surgical Scene Understanding,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Natalia Valderrama'), arxiv.Result.Author('Paola Ruiz Puentes'), arxiv.Result.Author('Isabela Hern√°ndez'), arxiv.Result.Author('Nicol√°s Ayobi'), arxiv.Result.Author('Mathilde Verlyk'), arxiv.Result.Author('Jessica Santander'), arxiv.Result.Author('Juan Caicedo'), arxiv.Result.Author('Nicol√°s Fern√°ndez'), arxiv.Result.Author('Pablo Arbel√°ez')]","Most benchmarks for studying surgical interventions focus on a specific
challenge instead of leveraging the intrinsic complementarity among different
tasks. In this work, we present a new experimental framework towards holistic
surgical scene understanding. First, we introduce the Phase, Step, Instrument,
and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes
annotations for both long-term (Phase and Step recognition) and short-term
reasoning (Instrument detection and novel Atomic Action recognition) in
robot-assisted radical prostatectomy videos. Second, we present Transformers
for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong
baseline for surgical scene understanding. TAPIR leverages our dataset's
multi-level annotations as it benefits from the learned representation on the
instrument detection task to improve its classification capacity. Our
experimental results in both PSI-AVA and other publicly available databases
demonstrate the adequacy of our framework to spur future research on holistic
surgical scene understanding.",-0.09207548,-0.104202725,-0.10219072,C
14737,"To ensure our results‚Äô reproducibility and promote further research on holistic
surgical understanding, we will make publicly available the PSI-AVA dataset, an-
4  N. Valderrama et al.","(2) We propose TAPIR, a transformer-based method that leverages the
multi-level annotations of PSI-AVA dataset and establishes a baseline for future
work in our holistic benchmark.","W                                                                 Frame Classification Head

                                   Class Token                             Linear                                                                                     Phases
                                   thw                                   Classifier                                                                                   Steps

                                             (thw + 1) x C           Box Classification Head
                                H  Video
                               c      Feature Extractor     mean                                                                                                      Instruments
                         T                                  pooling
                                                                                                                                          Box Proposals
                                                                                                                                                        Box Features  Atomic
                                                                                                                                                                      Actions
                                                                                                                                                  Linear Classifier
                                   Instrument Detector

Fig.",2022-12-08 22:15:27+00:00,Towards Holistic Surgical Scene Understanding,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Natalia Valderrama'), arxiv.Result.Author('Paola Ruiz Puentes'), arxiv.Result.Author('Isabela Hern√°ndez'), arxiv.Result.Author('Nicol√°s Ayobi'), arxiv.Result.Author('Mathilde Verlyk'), arxiv.Result.Author('Jessica Santander'), arxiv.Result.Author('Juan Caicedo'), arxiv.Result.Author('Nicol√°s Fern√°ndez'), arxiv.Result.Author('Pablo Arbel√°ez')]","Most benchmarks for studying surgical interventions focus on a specific
challenge instead of leveraging the intrinsic complementarity among different
tasks. In this work, we present a new experimental framework towards holistic
surgical scene understanding. First, we introduce the Phase, Step, Instrument,
and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes
annotations for both long-term (Phase and Step recognition) and short-term
reasoning (Instrument detection and novel Atomic Action recognition) in
robot-assisted radical prostatectomy videos. Second, we present Transformers
for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong
baseline for surgical scene understanding. TAPIR leverages our dataset's
multi-level annotations as it benefits from the learned representation on the
instrument detection task to improve its classification capacity. Our
experimental results in both PSI-AVA and other publicly available databases
demonstrate the adequacy of our framework to spur future research on holistic
surgical scene understanding.",-0.09207548,-0.104202725,-0.10219072,C
14751,"The abovementioned strengths, as well as being an open-source framework and the low cost of augmented
cane, have made this work a foundation for further research.","Another
significant advantage is the use of a white cane as the basis of the system for obstacle avoidance in the event of a system
failure.","However, it still has disadvantages such as being heavy
and not being hands-free.",2022-12-09 09:45:43+00:00,SLAM for Visually Impaired People: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Marziyeh Bamdad'), arxiv.Result.Author('Davide Scaramuzza'), arxiv.Result.Author('Alireza Darvishy')]","In recent decades, several assistive technologies for visually impaired and
blind (VIB) people have been developed to improve their ability to navigate
independently and safely. At the same time, simultaneous localization and
mapping (SLAM) techniques have become sufficiently robust and efficient to be
adopted in the development of assistive technologies. In this paper, we first
report the results of an anonymous survey conducted with VIB people to
understand their experience and needs; we focus on digital assistive
technologies that help them with indoor and outdoor navigation. Then, we
present a literature review of assistive technologies based on SLAM. We discuss
proposed approaches and indicate their pros and cons. We conclude by presenting
future opportunities and challenges in this domain.",0.23758876,0.22742179,-0.3035295,A
14755,"To do so, further research in                 deep CNN models for waste classification.","Comparative analysis of multiple
due to conveyor belts.",In ICAEIC.,2022-12-09 11:43:04+00:00,Synthetic Data for Object Classification in Industrial Applications,cs.CV,['cs.CV'],"[arxiv.Result.Author('August Baaz'), arxiv.Result.Author('Yonan Yonan'), arxiv.Result.Author('Kevin Hernandez-Diaz'), arxiv.Result.Author('Fernando Alonso-Fernandez'), arxiv.Result.Author('Felix Nilsson')]","One of the biggest challenges in machine learning is data collection.
Training data is an important part since it determines how the model will
behave. In object classification, capturing a large number of images per object
and in different conditions is not always possible and can be very
time-consuming and tedious. Accordingly, this work explores the creation of
artificial images using a game engine to cope with limited data in the training
dataset. We combine real and synthetic data to train the object classification
engine, a strategy that has shown to be beneficial to increase confidence in
the decisions made by the classifier, which is often critical in industrial
setups. To combine real and synthetic data, we first train the classifier on a
massive amount of synthetic data, and then we fine-tune it on real images.
Another important result is that the amount of real images needed for
fine-tuning is not very high, reaching top accuracy with just 12 or 24 images
per class. This substantially reduces the requirements of capturing a great
amount of real data.",-0.16012749,-0.108230084,0.14109714,C
14771,"Note that performance on this                                  1CLIP outperforming DINO on semantic coherence indicates that
binary classiÔ¨Åcation task is indicative of semantic coher-                              CLIP‚Äôs vision encoders are good candidates for unsupervised segmentation
ence, as a good classiÔ¨Åer would require patch representa-                               approaches like STEGO [19], but further study of this feature is beyond the
tions corresponding to same labels to have high cosine sim-                             scope of this work.","The target value for binary classiÔ¨Åcation is 1 if
l(x1)i,j = l(x2)p,q, else 0.",ilarity and vice-versa.,2022-12-09 17:23:00+00:00,Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jishnu Mukhoti'), arxiv.Result.Author('Tsung-Yu Lin'), arxiv.Result.Author('Omid Poursaeed'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Ashish Shah'), arxiv.Result.Author('Philip H. S. Torr'), arxiv.Result.Author('Ser-Nam Lim')]","We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets.",-0.0988131,-0.0071260612,-0.08231197,B
14778,Our VIN-          and help advance further research in the VidL domain.,"cal insights and our VidL pretraining recipe will be useful
All models use the same TimeSformer architecture [5].","DLU approach outperforms both the TimeSformer [5] and Om-
niVL [69] baselines by 2.1% and 1.0% respectively.",2022-12-09 18:54:05+00:00,VindLU: A Recipe for Effective Video-and-Language Pretraining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Feng Cheng'), arxiv.Result.Author('Xizi Wang'), arxiv.Result.Author('Jie Lei'), arxiv.Result.Author('David Crandall'), arxiv.Result.Author('Mohit Bansal'), arxiv.Result.Author('Gedas Bertasius')]","The last several years have witnessed remarkable progress in
video-and-language (VidL) understanding. However, most modern VidL approaches
use complex and specialized model architectures and sophisticated pretraining
protocols, making the reproducibility, analysis and comparisons of these
frameworks difficult. Hence, instead of proposing yet another new VidL model,
this paper conducts a thorough empirical study demystifying the most important
factors in the VidL model design. Among the factors that we investigate are (i)
the spatiotemporal architecture design, (ii) the multimodal fusion schemes,
(iii) the pretraining objectives, (iv) the choice of pretraining data, (v)
pretraining and finetuning protocols, and (vi) dataset and model scaling. Our
empirical study reveals that the most important design factors include:
temporal modeling, video-to-text multimodal fusion, masked modeling objectives,
and joint training on images and videos. Using these empirical insights, we
then develop a step-by-step recipe, dubbed VindLU, for effective VidL
pretraining. Our final model trained using our recipe achieves comparable or
better than state-of-the-art results on several VidL tasks without relying on
external CLIP pretraining. In particular, on the text-to-video retrieval task,
our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming
current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains
state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA,
MSRVTT-MC and TVQA. Our code and pretrained models are publicly available at:
https://github.com/klauscc/VindLU.",0.2966227,0.017944794,0.043646112,A
14779,"temporal attention (TA)) that we included in the main draft,
                                                                    we further study Temporal Attention via Prompts (TA-P)
   ‚Ä¢ ActivityNet-QA [84] contains 58K open-ended ques-              and Window Attention (WA).","We evaluate on two open-                  temporal modeling baselines (i.e., mean pooling (MP), late
ended QA datasets ActivityNet-QA, MSRVTT-QA and two                 temporal attention (L-TA), temporal convolution (TC), and
multiple-choice QA dataset MSRVTT-MC, TVQA.","We describe each of these
      tions on 5.8K sampled videos from ActivityNet [29].",2022-12-09 18:54:05+00:00,VindLU: A Recipe for Effective Video-and-Language Pretraining,cs.CV,['cs.CV'],"[arxiv.Result.Author('Feng Cheng'), arxiv.Result.Author('Xizi Wang'), arxiv.Result.Author('Jie Lei'), arxiv.Result.Author('David Crandall'), arxiv.Result.Author('Mohit Bansal'), arxiv.Result.Author('Gedas Bertasius')]","The last several years have witnessed remarkable progress in
video-and-language (VidL) understanding. However, most modern VidL approaches
use complex and specialized model architectures and sophisticated pretraining
protocols, making the reproducibility, analysis and comparisons of these
frameworks difficult. Hence, instead of proposing yet another new VidL model,
this paper conducts a thorough empirical study demystifying the most important
factors in the VidL model design. Among the factors that we investigate are (i)
the spatiotemporal architecture design, (ii) the multimodal fusion schemes,
(iii) the pretraining objectives, (iv) the choice of pretraining data, (v)
pretraining and finetuning protocols, and (vi) dataset and model scaling. Our
empirical study reveals that the most important design factors include:
temporal modeling, video-to-text multimodal fusion, masked modeling objectives,
and joint training on images and videos. Using these empirical insights, we
then develop a step-by-step recipe, dubbed VindLU, for effective VidL
pretraining. Our final model trained using our recipe achieves comparable or
better than state-of-the-art results on several VidL tasks without relying on
external CLIP pretraining. In particular, on the text-to-video retrieval task,
our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming
current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains
state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA,
MSRVTT-MC and TVQA. Our code and pretrained models are publicly available at:
https://github.com/klauscc/VindLU.",-0.07293919,-0.23101169,-0.04908116,C
14805,"6, we apply                                                                                    on dense predicted tasks deserves further study, as these
LaPE to different encoder layers in ViT-Lite [12].",As shown in Tab.,For                                                                                   tasks are more sensitive to position.,2022-12-10 10:38:00+00:00,Position Embedding Needs an Independent Layer Normalization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Runyi Yu'), arxiv.Result.Author('Zhennan Wang'), arxiv.Result.Author('Yinhuai Wang'), arxiv.Result.Author('Kehan Li'), arxiv.Result.Author('Yian Zhao'), arxiv.Result.Author('Jian Zhang'), arxiv.Result.Author('Guoli Song'), arxiv.Result.Author('Jie Chen')]","The Position Embedding (PE) is critical for Vision Transformers (VTs) due to
the permutation-invariance of self-attention operation. By analyzing the input
and output of each encoder layer in VTs using reparameterization and
visualization, we find that the default PE joining method (simply adding the PE
and patch embedding together) operates the same affine transformation to token
embedding and PE, which limits the expressiveness of PE and hence constrains
the performance of VTs. To overcome this limitation, we propose a simple,
effective, and robust method. Specifically, we provide two independent layer
normalizations for token embeddings and PE for each layer, and add them
together as the input of each layer's Muti-Head Self-Attention module. Since
the method allows the model to adaptively adjust the information of PE for
different layers, we name it as Layer-adaptive Position Embedding, abbreviated
as LaPE. Extensive experiments demonstrate that LaPE can improve various VTs
with different types of PE and make VTs robust to PE types. For example, LaPE
improves 0.94% accuracy for ViT-Lite on Cifar10, 0.98% for CCT on Cifar100, and
1.72% for DeiT on ImageNet-1K, which is remarkable considering the negligible
extra parameters, memory and computational cost brought by LaPE. The code is
publicly available at https://github.com/Ingrid725/LaPE.",0.13597484,-0.109827265,0.07047489,A
14806,"6, we apply                                                                                    on dense predicted tasks deserves further study, as these
LaPE to different encoder layers in ViT-Lite [12].",As shown in Tab.,For                                                                                   tasks are more sensitive to position.,2022-12-10 10:38:00+00:00,Position Embedding Needs an Independent Layer Normalization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Runyi Yu'), arxiv.Result.Author('Zhennan Wang'), arxiv.Result.Author('Yinhuai Wang'), arxiv.Result.Author('Kehan Li'), arxiv.Result.Author('Yian Zhao'), arxiv.Result.Author('Jian Zhang'), arxiv.Result.Author('Guoli Song'), arxiv.Result.Author('Jie Chen')]","The Position Embedding (PE) is critical for Vision Transformers (VTs) due to
the permutation-invariance of self-attention operation. By analyzing the input
and output of each encoder layer in VTs using reparameterization and
visualization, we find that the default PE joining method (simply adding the PE
and patch embedding together) operates the same affine transformation to token
embedding and PE, which limits the expressiveness of PE and hence constrains
the performance of VTs. To overcome this limitation, we propose a simple,
effective, and robust method. Specifically, we provide two independent layer
normalizations for token embeddings and PE for each layer, and add them
together as the input of each layer's Muti-Head Self-Attention module. Since
the method allows the model to adaptively adjust the information of PE for
different layers, we name it as Layer-adaptive Position Embedding, abbreviated
as LaPE. Extensive experiments demonstrate that LaPE can improve various VTs
with different types of PE and make VTs robust to PE types. For example, LaPE
improves 0.94% accuracy for ViT-Lite on Cifar10, 0.98% for CCT on Cifar100, and
1.72% for DeiT on ImageNet-1K, which is remarkable considering the negligible
extra parameters, memory and computational cost brought by LaPE. The code is
publicly available at https://github.com/Ingrid725/LaPE.",0.13597484,-0.109827265,0.07047489,A
14844,The     inability to real-time tracking shifts the further research [27].,"Deep Ô¨Çow approaches can improve the tracking
produces object detections, crowd density map, and reID          performance using the powerful neural networks, but the
features, and can perform online and real-time tracking.","counting task and the object detection task can boost each       Using batch detections as input, the MOT methods usually
other by using the same object-count constraint within a given   transform data association as an ofÔ¨Çine energy optimization
area, which makes our approach robust to crowd scenes.",2022-12-12 12:53:58+00:00,"CountingMOT: Joint Counting, Detection and Re-Identification for Multiple Object Tracking",cs.CV,['cs.CV'],"[arxiv.Result.Author('Weihong Ren'), arxiv.Result.Author('Bowen Chen'), arxiv.Result.Author('Yuhang Shi'), arxiv.Result.Author('Weibo Jiang'), arxiv.Result.Author('Honghai Liu')]","The recent trend in multiple object tracking (MOT) is jointly solving
detection and tracking, where object detection and appearance feature (or
motion) are learned simultaneously. Despite competitive performance, in crowded
scenes, joint detection and tracking usually fail to find accurate object
associations due to missed or false detections. In this paper, we jointly model
counting, detection and re-identification in an end-to-end framework, named
CountingMOT, tailored for crowded scenes. By imposing mutual object-count
constraints between detection and counting, the CountingMOT tries to find a
balance between object detection and crowd density map estimation, which can
help it to recover missed detections or reject false detections. Our approach
is an attempt to bridge the gap of object detection, counting, and
re-Identification. This is in contrast to prior MOT methods that either ignore
the crowd density and thus are prone to failure in crowded scenes, or depend on
local correlations to build a graphical relationship for matching targets. The
proposed MOT tracker can perform online and real-time tracking, and achieves
the state-of-the-art results on public benchmarks MOT16 (MOTA of 77.6), MOT17
(MOTA of 78.0%) and MOT20 (MOTA of 70.2%).",-0.35042062,0.007689518,0.03018887,B
14861,"In order to account for different       tangled to some extent, although further research in that di-
warm-up periods gi is deÔ¨Åned as gi = 0 if t < Œ¥i.","If we aim to keep the overall composition of         Œ≥ÀÜt(zt, cp; e) =  i‚ààI  gi  Œ≥  i  (  z  t  ,  c  p  ,  c  e  i  )  (10)
                                                                                                 t

                                                                4
   where gi sums up to one.","However,          rection is necessary to verify our assumption.",2022-12-12 16:21:24+00:00,The Stable Artist: Steering Semantics in Diffusion Latent Space,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Manuel Brack'), arxiv.Result.Author('Patrick Schramowski'), arxiv.Result.Author('Felix Friedrich'), arxiv.Result.Author('Dominik Hintersdorf'), arxiv.Result.Author('Kristian Kersting')]","Large, text-conditioned generative diffusion models have recently gained a
lot of attention for their impressive performance in generating high-fidelity
images from text alone. However, achieving high-quality results is almost
unfeasible in a one-shot fashion. On the contrary, text-guided image generation
involves the user making many slight changes to inputs in order to iteratively
carve out the envisioned image. However, slight changes to the input prompt
often lead to entirely different images being generated, and thus the control
of the artist is limited in its granularity. To provide flexibility, we present
the Stable Artist, an image editing approach enabling fine-grained control of
the image generation process. The main component is semantic guidance (SEGA)
which steers the diffusion process along variable numbers of semantic
directions. This allows for subtle edits to images, changes in composition and
style, as well as optimization of the overall artistic conception. Furthermore,
SEGA enables probing of latent spaces to gain insights into the representation
of concepts learned by the model, even complex ones such as 'carbon emission'.
We demonstrate the Stable Artist on several tasks, showcasing high-quality
image editing and composition.",0.36522362,0.2169952,0.036952786,A
14862,This experiment further        we advocate for further research in this direction.,"Therefore,
more faithful to the original image.","supports an interesting observation of the Stable Diffusion
latent space.",2022-12-12 16:21:24+00:00,The Stable Artist: Steering Semantics in Diffusion Latent Space,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Manuel Brack'), arxiv.Result.Author('Patrick Schramowski'), arxiv.Result.Author('Felix Friedrich'), arxiv.Result.Author('Dominik Hintersdorf'), arxiv.Result.Author('Kristian Kersting')]","Large, text-conditioned generative diffusion models have recently gained a
lot of attention for their impressive performance in generating high-fidelity
images from text alone. However, achieving high-quality results is almost
unfeasible in a one-shot fashion. On the contrary, text-guided image generation
involves the user making many slight changes to inputs in order to iteratively
carve out the envisioned image. However, slight changes to the input prompt
often lead to entirely different images being generated, and thus the control
of the artist is limited in its granularity. To provide flexibility, we present
the Stable Artist, an image editing approach enabling fine-grained control of
the image generation process. The main component is semantic guidance (SEGA)
which steers the diffusion process along variable numbers of semantic
directions. This allows for subtle edits to images, changes in composition and
style, as well as optimization of the overall artistic conception. Furthermore,
SEGA enables probing of latent spaces to gain insights into the representation
of concepts learned by the model, even complex ones such as 'carbon emission'.
We demonstrate the Stable Artist on several tasks, showcasing high-quality
image editing and composition.",0.26215583,0.058548734,0.104275756,A
14863,"Ablation Studies
                                                                 number of heads of self-attention encoding K = {1, 2, 4, 6}
   We further study the influence of three important com-        in Equation (4).",4.2.,"K = 1 implies that our method uses the
ponents of our method: (a) multi-feature representation, (b)     convolutional graph layer instead of self-attention encoding
self-attention encoding, and (c) graph autoencoder.",2022-12-12 16:42:59+00:00,Reconstructing Humpty Dumpty: Multi-feature Graph Autoencoder for Open Set Action Recognition,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dawei Du'), arxiv.Result.Author('Ameya Shringi'), arxiv.Result.Author('Anthony Hoogs'), arxiv.Result.Author('Christopher Funk')]","Most action recognition datasets and algorithms assume a closed world, where
all test samples are instances of the known classes. In open set problems, test
samples may be drawn from either known or unknown classes. Existing open set
action recognition methods are typically based on extending closed set methods
by adding post hoc analysis of classification scores or feature distances and
do not capture the relations among all the video clip elements. Our approach
uses the reconstruction error to determine the novelty of the video since
unknown classes are harder to put back together and thus have a higher
reconstruction error than videos from known classes. We refer to our solution
to the open set action recognition problem as ""Humpty Dumpty"", due to its
reconstruction abilities. Humpty Dumpty is a novel graph-based autoencoder that
accounts for contextual and semantic relations among the clip pieces for
improved reconstruction. A larger reconstruction error leads to an increased
likelihood that the action can not be reconstructed, i.e., can not put Humpty
Dumpty back together again, indicating that the action has never been seen
before and is novel/unknown. Extensive experiments are performed on two
publicly available action recognition datasets including HMDB-51 and UCF-101,
showing the state-of-the-art performance for open set action recognition.",0.12410003,-0.14103134,-0.044234738,C
14864,"need further research about more eÔ¨Écient and compact
                                        Nevertheless, ConvLSTMs suÔ¨Äer from some inherent prob-
                                        lems typical of recurrent neural networks (RNNs), including              visual Transformer, especially for videos.","We still
                                        Short-Term Memory networks (ConvLSTMs) are the core
                                        for almost all the state-of-the-art (SOTA) VFFP models.","Therefore, we
                                        slow training and inference speed, error accumulation during
                                        inference, vanishing gradient, and predicted frame quality               propose a novel eÔ¨Écient Transformer block with smaller
                                        degradation.",2022-12-12 16:46:48+00:00,Video Prediction by Efficient Transformers,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xi Ye'), arxiv.Result.Author('Guillaume-Alexandre Bilodeau')]","Video prediction is a challenging computer vision task that has a wide range
of applications. In this work, we present a new family of Transformer-based
models for video prediction. Firstly, an efficient local spatial-temporal
separation attention mechanism is proposed to reduce the complexity of standard
Transformers. Then, a full autoregressive model, a partial autoregressive model
and a non-autoregressive model are developed based on the new efficient
Transformer. The partial autoregressive model has a similar performance with
the full autoregressive model but a faster inference speed. The
non-autoregressive model not only achieves a faster inference speed but also
mitigates the quality degradation problem of the autoregressive counterparts,
but it requires additional parameters and loss function for learning. Given the
same attention mechanism, we conducted a comprehensive study to compare the
proposed three video prediction variants. Experiments show that the proposed
video prediction models are competitive with more complex state-of-the-art
convolutional-LSTM based models. The source code is available at
https://github.com/XiYe20/VPTR.",-0.09915596,-0.18645343,0.2532832,C
14873,"Our further study with empirical results suggest that a          Recently, inference time defense using contrastive invari-
                                                                    ance [34] and rotation [39] has been shown to improve ad-
better approach is to use dense equivariance constraints.",they did not consider the spatial equivariance in their task.,"versarial robustness without retraining the model on unfore-
Our main hypothesis is that visual representations must be          seen attacks.",2022-12-12 17:52:46+00:00,Robust Perception through Equivariance,cs.CV,['cs.CV'],"[arxiv.Result.Author('Chengzhi Mao'), arxiv.Result.Author('Lingyu Zhang'), arxiv.Result.Author('Abhishek Joshi'), arxiv.Result.Author('Junfeng Yang'), arxiv.Result.Author('Hao Wang'), arxiv.Result.Author('Carl Vondrick')]","Deep networks for computer vision are not reliable when they encounter
adversarial examples. In this paper, we introduce a framework that uses the
dense intrinsic constraints in natural images to robustify inference. By
introducing constraints at inference time, we can shift the burden of
robustness from training to the inference algorithm, thereby allowing the model
to adjust dynamically to each individual image's unique and potentially novel
characteristics at inference time. Among different constraints, we find that
equivariance-based constraints are most effective, because they allow dense
constraints in the feature space without overly constraining the representation
at a fine-grained level. Our theoretical results validate the importance of
having such dense constraints at inference time. Our empirical experiments show
that restoring feature equivariance at inference time defends against
worst-case adversarial perturbations. The method obtains improved adversarial
robustness on four datasets (ImageNet, Cityscapes, PASCAL VOC, and MS-COCO) on
image recognition, semantic segmentation, and instance segmentation tasks.
Project page is available at equi4robust.cs.columbia.edu.",-0.06067971,-0.061103575,-0.037513494,C
14880,"The for-            We further study the inÔ¨Çuence of Random Augmenta-
mer changes the image content with other images (Mixup,
CutMix), or removes part of the content (Random Erase).","kinds of augmentations: content modiÔ¨Åcation augmenta-
tions and content diversiÔ¨Åcation augmentations.",tion in Table 8.,2022-12-12 18:59:59+00:00,CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Ting Zhang'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Shuyang Gu'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Lu Yuan'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Nenghai Yu')]","Recent studies have shown that CLIP has achieved remarkable success in
performing zero-shot inference while its fine-tuning performance is not
satisfactory. In this paper, we identify that fine-tuning performance is
significantly impacted by hyper-parameter choices. We examine various key
hyper-parameters and empirically evaluate their impact in fine-tuning CLIP for
classification tasks through a comprehensive study. We find that the
fine-tuning performance of CLIP is substantially underestimated. Equipped with
hyper-parameter refinement, we demonstrate CLIP itself is better or at least
competitive in fine-tuning compared with large-scale supervised pre-training
approaches or latest works that use CLIP as prediction targets in Masked Image
Modeling. Specifically, CLIP ViT-Base/16 and CLIP ViT-Large/14 can achieve
85.7%,88.0% finetuning Top-1 accuracy on the ImageNet-1K dataset . These
observations challenge the conventional conclusion that CLIP is not suitable
for fine-tuning, and motivate us to rethink recently proposed improvements
based on CLIP. We will release our code publicly at
\url{https://github.com/LightDXY/FT-CLIP}.",0.045578234,-0.0015603881,0.06513986,C
14881,"we further study two other augmentations in Table 13:
3Aug [20] and the AugMix[11].",Here                    inÔ¨Çuence is quite minor.,"The 3Aug also proposes                      weight decay 1e-8 0.01 0.05                      0.1        0.2
using single resize crop (SRC), instead of the default ran-
dom resize crop (RRC).",2022-12-12 18:59:59+00:00,CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Jianmin Bao'), arxiv.Result.Author('Ting Zhang'), arxiv.Result.Author('Dongdong Chen'), arxiv.Result.Author('Shuyang Gu'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Lu Yuan'), arxiv.Result.Author('Dong Chen'), arxiv.Result.Author('Fang Wen'), arxiv.Result.Author('Nenghai Yu')]","Recent studies have shown that CLIP has achieved remarkable success in
performing zero-shot inference while its fine-tuning performance is not
satisfactory. In this paper, we identify that fine-tuning performance is
significantly impacted by hyper-parameter choices. We examine various key
hyper-parameters and empirically evaluate their impact in fine-tuning CLIP for
classification tasks through a comprehensive study. We find that the
fine-tuning performance of CLIP is substantially underestimated. Equipped with
hyper-parameter refinement, we demonstrate CLIP itself is better or at least
competitive in fine-tuning compared with large-scale supervised pre-training
approaches or latest works that use CLIP as prediction targets in Masked Image
Modeling. Specifically, CLIP ViT-Base/16 and CLIP ViT-Large/14 can achieve
85.7%,88.0% finetuning Top-1 accuracy on the ImageNet-1K dataset . These
observations challenge the conventional conclusion that CLIP is not suitable
for fine-tuning, and motivate us to rethink recently proposed improvements
based on CLIP. We will release our code publicly at
\url{https://github.com/LightDXY/FT-CLIP}.",0.19624409,0.13867226,0.27923536,A
14885,"Acknowledgments: This material is based upon work
                                                                  supported by the National Science Foundation (NSF) under
   However, we also observe several limitations, which shows      Award No OIA-1946391, NSF 1920920, NSF FAIN-2223793
some room for further research to improve our PMR.","that our proposed PMR makes an adequate improvement to
the selected SOTA modules.","First,         and NIH 1R01CA277739.",2022-12-12 19:29:07+00:00,Contextual Explainable Video Representation:\\Human Perception-based Understanding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Khoa Vo'), arxiv.Result.Author('Kashu Yamazaki'), arxiv.Result.Author('Phong X. Nguyen'), arxiv.Result.Author('Phat Nguyen'), arxiv.Result.Author('Khoa Luu'), arxiv.Result.Author('Ngan Le')]","Video understanding is a growing field and a subject of intense research,
which includes many interesting tasks to understanding both spatial and
temporal information, e.g., action detection, action recognition, video
captioning, video retrieval. One of the most challenging problems in video
understanding is dealing with feature extraction, i.e. extract contextual
visual representation from given untrimmed video due to the long and
complicated temporal structure of unconstrained videos. Different from existing
approaches, which apply a pre-trained backbone network as a black-box to
extract visual representation, our approach aims to extract the most contextual
information with an explainable mechanism. As we observed, humans typically
perceive a video through the interactions between three main factors, i.e., the
actors, the relevant objects, and the surrounding environment. Therefore, it is
very crucial to design a contextual explainable video representation extraction
that can capture each of such factors and model the relationships between them.
In this paper, we discuss approaches, that incorporate the human perception
process into modeling actors, objects, and the environment. We choose video
paragraph captioning and temporal action detection to illustrate the
effectiveness of human perception based-contextual representation in video
understanding. Source code is publicly available at
https://github.com/UARK-AICV/Video_Representation.",0.34863245,0.12547094,0.07067107,A
14886,"Acknowledgments: This material is based upon work
                                                                  supported by the National Science Foundation (NSF) under
   However, we also observe several limitations, which shows      Award No OIA-1946391, NSF 1920920, NSF FAIN-2223793
some room for further research to improve our PMR.","that our proposed PMR makes an adequate improvement to
the selected SOTA modules.","First,         and NIH 1R01CA277739.",2022-12-12 19:29:07+00:00,Contextual Explainable Video Representation: Human Perception-based Understanding,cs.CV,['cs.CV'],"[arxiv.Result.Author('Khoa Vo'), arxiv.Result.Author('Kashu Yamazaki'), arxiv.Result.Author('Phong X. Nguyen'), arxiv.Result.Author('Phat Nguyen'), arxiv.Result.Author('Khoa Luu'), arxiv.Result.Author('Ngan Le')]","Video understanding is a growing field and a subject of intense research,
which includes many interesting tasks to understanding both spatial and
temporal information, e.g., action detection, action recognition, video
captioning, video retrieval. One of the most challenging problems in video
understanding is dealing with feature extraction, i.e. extract contextual
visual representation from given untrimmed video due to the long and
complicated temporal structure of unconstrained videos. Different from existing
approaches, which apply a pre-trained backbone network as a black-box to
extract visual representation, our approach aims to extract the most contextual
information with an explainable mechanism. As we observed, humans typically
perceive a video through the interactions between three main factors, i.e., the
actors, the relevant objects, and the surrounding environment. Therefore, it is
very crucial to design a contextual explainable video representation extraction
that can capture each of such factors and model the relationships between them.
In this paper, we discuss approaches, that incorporate the human perception
process into modeling actors, objects, and the environment. We choose video
paragraph captioning and temporal action detection to illustrate the
effectiveness of human perception based-contextual representation in video
understanding. Source code is publicly available at
https://github.com/UARK-AICV/Video_Representation.",0.34863245,0.12547094,0.07067107,A
14889,"We hope that this new benchmark            Ô¨Årst author to be people who appear in the age group
dataset will stem further research in this particular Ô¨Åeld.","In this paper, this group is determined by the
age and physical disability.",of 50 or older.,2022-12-12 19:59:47+00:00,Comparison Of Deep Object Detectors On A New Vulnerable Pedestrian Dataset,cs.CV,['cs.CV'],"[arxiv.Result.Author('Devansh Sharma'), arxiv.Result.Author('Tihitina Hade'), arxiv.Result.Author('Qing Tian')]","Pedestrian safety is one primary concern in autonomous driving. The
under-representation of vulnerable groups in today's pedestrian datasets points
to an urgent need for a dataset of vulnerable road users. In this paper, we
first introduce a new vulnerable pedestrian detection dataset, BG Vulnerable
Pedestrian (BGVP) dataset to help train well-rounded models and thus induce
research to increase the efficacy of vulnerable pedestrian detection. The
dataset includes four classes, i.e., Children Without Disability, Elderly
without Disability, With Disability, and Non-Vulnerable. This dataset consists
of images collected from the public domain and manually-annotated bounding
boxes. In addition, on the proposed dataset, we have trained and tested five
state-of-the-art object detection models, i.e., YOLOv4, YOLOv5, YOLOX, Faster
R-CNN, and EfficientDet. Our results indicate that YOLOX and YOLOv4 perform the
best on our dataset, YOLOv4 scoring 0.7999 and YOLOX scoring 0.7779 on the mAP
0.5 metric, while YOLOX outperforms YOLOv4 by 3.8 percent on the mAP 0.5:0.95
metric. Generally speaking, all five detectors do well predicting the With
Disability class and perform poorly in the Elderly Without Disability class.
YOLOX consistently outperforms all other detectors on the mAP (0.5:0.95) per
class metric, obtaining 0.5644, 0.5242, 0.4781, and 0.6796 for Children Without
Disability, Elderly Without Disability, Non-vulnerable, and With Disability,
respectively. Our dataset and codes are available at
https://github.com/devvansh1997/BGVP.",0.20012465,0.01999473,-0.2637908,A
14904,"work encourages further research into the promising use of
Effectiveness of TEUE.","We hope our
by a considerable margin.",To verify the effectiveness of                                               sparse point-level annotation for image understanding.,2022-12-13 11:18:08+00:00,Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhenyu Wu'), arxiv.Result.Author('Lin Wang'), arxiv.Result.Author('Wei Wang'), arxiv.Result.Author('Qing Xia'), arxiv.Result.Author('Chenglizhao Chen'), arxiv.Result.Author('Aimin Hao'), arxiv.Result.Author('Shuo Li')]","Although weakly-supervised techniques can reduce the labeling effort, it is
unclear whether a saliency model trained with weakly-supervised data (e.g.,
point annotation) can achieve the equivalent performance of its
fully-supervised version. This paper attempts to answer this unexplored
question by proving a hypothesis: there is a point-labeled dataset where
saliency models trained on it can achieve equivalent performance when trained
on the densely annotated dataset. To prove this conjecture, we proposed a novel
yet effective adversarial trajectory-ensemble active learning (ATAL). Our
contributions are three-fold: 1) Our proposed adversarial attack triggering
uncertainty can conquer the overconfidence of existing active learning methods
and accurately locate these uncertain pixels. {2)} Our proposed
trajectory-ensemble uncertainty estimation method maintains the advantages of
the ensemble networks while significantly reducing the computational cost. {3)}
Our proposed relationship-aware diversity sampling algorithm can conquer
oversampling while boosting performance. Experimental results show that our
ATAL can find such a point-labeled dataset, where a saliency model trained on
it obtained $97\%$ -- $99\%$ performance of its fully-supervised version with
only ten annotated points per image.",-0.15143025,-0.0101913465,-0.06569277,C
14930,"This will be       better compared to the previous one, of which 0H-
leaved for further research.","One can see in Table V that all performance are
another ILP-P to get instance segmentation.",An illustration can be seen       prob-4000 reaches 80.5% mIoU and 62.8% PQ score.,2022-12-13 17:36:17+00:00,Connectivity-constrained Interactive Panoptic Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ruobing Shen'), arxiv.Result.Author('Bo Tang'), arxiv.Result.Author('Andrea Lodi'), arxiv.Result.Author('Ismail Ben Ayed'), arxiv.Result.Author('Thomas Guthier')]","We address interactive panoptic annotation, where one segment all object and
stuff regions in an image. We investigate two graph-based segmentation
algorithms that both enforce connectivity of each region, with a notable
class-aware Integer Linear Programming (ILP) formulation that ensures global
optimum. Both algorithms can take RGB, or utilize the feature maps from any
DCNN, whether trained on the target dataset or not, as input. We then propose
an interactive, scribble-based annotation framework.",0.2966447,0.104660615,0.03534022,A
14932,This leaves room for further research.,using different epsilons (L‚àû          geNet.,and L2).,2022-12-13 17:51:32+00:00,Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial Detection,cs.CV,"['cs.CV', 'cs.CR']","[arxiv.Result.Author('Peter Lorenz'), arxiv.Result.Author('Margret Keuper'), arxiv.Result.Author('Janis Keuper')]","Convolutional neural networks (CNN) define the state-of-the-art solution on
many perceptual tasks. However, current CNN approaches largely remain
vulnerable against adversarial perturbations of the input that have been
crafted specifically to fool the system while being quasi-imperceptible to the
human eye. In recent years, various approaches have been proposed to defend
CNNs against such attacks, for example by model hardening or by adding explicit
defence mechanisms. Thereby, a small ""detector"" is included in the network and
trained on the binary classification task of distinguishing genuine data from
data containing adversarial perturbations. In this work, we propose a simple
and light-weight detector, which leverages recent findings on the relation
between networks' local intrinsic dimensionality (LID) and adversarial attacks.
Based on a re-interpretation of the LID measure and several simple adaptations,
we surpass the state-of-the-art on adversarial detection by a significant
margin and reach almost perfect results in terms of F1-score for several
networks and datasets. Sources available at:
https://github.com/adverML/multiLID",0.39580458,0.11797884,0.1286282,A
14933,"We further study how the computation cost for high-resolution features changes when the model
size and input scale up by examining FLOP counts.","This highlights
           our technical contribution of efÔ¨Åciently processing high-resolution features with GPViT.","The results are shown in Figure 6 where we
compare GP Block with different group numbers to self-attention and local-attention operations:
Self-attention and GP Block can both exchange global information between image features, but the
computational cost of GP Block grows much slower than self-attention.",2022-12-13 18:26:00+00:00,GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Chenhongyi Yang'), arxiv.Result.Author('Jiarui Xu'), arxiv.Result.Author('Shalini De Mello'), arxiv.Result.Author('Elliot J. Crowley'), arxiv.Result.Author('Xiaolong Wang')]","We present the Group Propagation Vision Transformer (GPViT): a novel
nonhierarchical (i.e. non-pyramidal) transformer model designed for general
visual recognition with high-resolution features. High-resolution features (or
tokens) are a natural fit for tasks that involve perceiving fine-grained
details such as detection and segmentation, but exchanging global information
between these features is expensive in memory and computation because of the
way self-attention scales. We provide a highly efficient alternative Group
Propagation Block (GP Block) to exchange global information. In each GP Block,
features are first grouped together by a fixed number of learnable group
tokens; we then perform Group Propagation where global information is exchanged
between the grouped features; finally, global information in the updated
grouped features is returned back to the image features through a transformer
decoder. We evaluate GPViT on a variety of visual recognition tasks including
image classification, semantic segmentation, object detection, and instance
segmentation. Our method achieves significant performance gains over previous
works across all tasks, especially on tasks that require high-resolution
outputs, for example, our GPViT-L3 outperforms Swin Transformer-B by 2.0 mIoU
on ADE20K semantic segmentation with only half as many parameters. Code and
pre-trained models are available at https://github.com/ChenhongyiYang/GPViT .",-0.00043382868,-0.0021858308,0.19870967,C
14947,"Nevertheless, further research and improvements are still needed.","Deepfake attack detection has been the subject of numerous
studies.","Develop the Online Exam Proctoring Framework We oÔ¨Äered an automated online exam proctoring framework
in this paper which, to the best of our knowledge, is Ô¨Årst to analyse the heart rate of the exam candidate using
presentation attack detection approaches.",2022-12-14 00:30:09+00:00,A Novel Active Solution for Two-Dimensional Face Presentation Attack Detection,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Matineh Pooshideh')],"Identity authentication is the process of verifying one's identity. There are
several identity authentication methods, among which biometric authentication
is of utmost importance. Facial recognition is a sort of biometric
authentication with various applications, such as unlocking mobile phones and
accessing bank accounts. However, presentation attacks pose the greatest threat
to facial recognition. A presentation attack is an attempt to present a
non-live face, such as a photo, video, mask, and makeup, to the camera.
Presentation attack detection is a countermeasure that attempts to identify
between a genuine user and a presentation attack. Several industries, such as
financial services, healthcare, and education, use biometric authentication
services on various devices. This illustrates the significance of presentation
attack detection as the verification step. In this paper, we study
state-of-the-art to cover the challenges and solutions related to presentation
attack detection in a single place. We identify and classify different
presentation attack types and identify the state-of-the-art methods that could
be used to detect each of them. We compare the state-of-the-art literature
regarding attack types, evaluation metrics, accuracy, and datasets and discuss
research and industry challenges of presentation attack detection. Most
presentation attack detection approaches rely on extensive data training and
quality, making them difficult to implement. We introduce an efficient active
presentation attack detection approach that overcomes weaknesses in the
existing literature. The proposed approach does not require training data, is
CPU-light, can process low-quality images, has been tested with users of
various ages and is shown to be user-friendly and highly robust to
2-dimensional presentation attacks.",0.21246596,-0.08460526,-0.15247138,A
15044,"Second, due to the dispersion effect
                                        further research in this Ô¨Åeld.","DeepInfrared toolkit, dataset, and trained models are available                Therefore, segmentation integrity is only a rough approximation
                                        at https://github.com/YimianDai/open-deepinfrared to advance                   of detection accuracy.","of long-distance imaging, it is impossible for humans to
                                                                                                                       label infrared small targets with pixel-level accuracy, and
                                           Index Terms‚ÄîInfrared small target detection, one-stage cas-                 ground-truth annotations include inherent ambiguities.",2022-12-16 13:37:23+00:00,One-Stage Cascade Refinement Networks for Infrared Small Target Detection,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yimian Dai'), arxiv.Result.Author('Xiang Li'), arxiv.Result.Author('Fei Zhou'), arxiv.Result.Author('Yulei Qian'), arxiv.Result.Author('Yaohong Chen'), arxiv.Result.Author('Jian Yang')]","Single-frame InfraRed Small Target (SIRST) detection has been a challenging
task due to a lack of inherent characteristics, imprecise bounding box
regression, a scarcity of real-world datasets, and sensitive localization
evaluation. In this paper, we propose a comprehensive solution to these
challenges. First, we find that the existing anchor-free label assignment
method is prone to mislabeling small targets as background, leading to their
omission by detectors. To overcome this issue, we propose an all-scale
pseudo-box-based label assignment scheme that relaxes the constraints on scale
and decouples the spatial assignment from the size of the ground-truth target.
Second, motivated by the structured prior of feature pyramids, we introduce the
one-stage cascade refinement network (OSCAR), which uses the high-level head as
soft proposals for the low-level refinement head. This allows OSCAR to process
the same target in a cascade coarse-to-fine manner. Finally, we present a new
research benchmark for infrared small target detection, consisting of the
SIRST-V2 dataset of real-world, high-resolution single-frame targets, the
normalized contrast evaluation metric, and the DeepInfrared toolkit for
detection. We conduct extensive ablation studies to evaluate the components of
OSCAR and compare its performance to state-of-the-art model-driven and
data-driven methods on the SIRST-V2 benchmark. Our results demonstrate that a
top-down cascade refinement framework can improve the accuracy of infrared
small target detection without sacrificing efficiency. The DeepInfrared
toolkit, dataset, and trained models are available at
https://github.com/YimianDai/open-deepinfrared to advance further research in
this field.",-0.20322704,0.08284,0.16554645,B
15055,"All of the labeled sub-volumes are available under [15, 12]

We hope that the provided data sets are useful for further research.","Some regions of the dataset simply cannot be clearly annotated due to
the quality of the data and the recording modality.","Author Contributions
RG ‚Äì organized, supervised und participated in the interactive annotation of the XXL data, he also organized the

          annotation tools, deÔ¨Åned, and built all necessary interfaces and integrated it in the segmentation workÔ¨Çow.",2022-12-16 18:31:23+00:00,An annotated instance segmentation XXL-CT dataset from a historic airplane,cs.CV,['cs.CV'],"[arxiv.Result.Author('Roland Gruber'), arxiv.Result.Author('Nils Reims'), arxiv.Result.Author('Andreas Hempfer'), arxiv.Result.Author('Stefan Gerth'), arxiv.Result.Author('Michael Salamon'), arxiv.Result.Author('Thomas Wittenberg')]","The Me 163 was a Second World War fighter airplane and a result of the German
air force secret developments. One of these airplanes is currently owned and
displayed in the historic aircraft exhibition of the Deutsches Museum in
Munich, Germany. To gain insights with respect to its history, design and state
of preservation, a complete CT scan was obtained using an industrial
XXL-computer tomography scanner.
  Using the CT data from the Me 163, all its details can visually be examined
at various levels, ranging from the complete hull down to single sprockets and
rivets. However, while a trained human observer can identify and interpret the
volumetric data with all its parts and connections, a virtual dissection of the
airplane and all its different parts would be quite desirable. Nevertheless,
this means, that an instance segmentation of all components and objects of
interest into disjoint entities from the CT data is necessary.
  As of currently, no adequate computer-assisted tools for automated or
semi-automated segmentation of such XXL-airplane data are available, in a first
step, an interactive data annotation and object labeling process has been
established. So far, seven 512 x 512 x 512 voxel sub-volumes from the Me 163
airplane have been annotated and labeled, whose results can potentially be used
for various new applications in the field of digital heritage, non-destructive
testing, or machine-learning.
  This work describes the data acquisition process of the airplane using an
industrial XXL-CT scanner, outlines the interactive segmentation and labeling
scheme to annotate sub-volumes of the airplane's CT data, describes and
discusses various challenges with respect to interpreting and handling the
annotated and labeled data.",-0.09362843,-0.010025024,-0.20215921,B
15056,"To further study global and local representations, we can
make some design choices applied to z and ŒΩ.","We refer readers to [13] for recent sur-
                                                                  veys.","In ¬ß 3, we           Robustness.",2022-12-16 18:51:10+00:00,Better May Not Be Fairer: Can Data Augmentation Mitigate Subgroup Degradation?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ming-Chang Chiu'), arxiv.Result.Author('Pin-Yu Chen'), arxiv.Result.Author('Xuezhe Ma')]","It is no secret that deep learning models exhibit undesirable behaviors such
as learning spurious correlations instead of learning correct relationships
between input/output pairs. Prior works on robustness study datasets that mix
low-level features to quantify how spurious correlations affect predictions
instead of considering natural semantic factors due to limitations in accessing
realistic datasets for comprehensive evaluation. To bridge this gap, in this
paper we first investigate how natural background colors play a role as
spurious features in image classification tasks by manually splitting the test
sets of CIFAR10 and CIFAR100 into subgroups based on the background color of
each image. We name our datasets CIFAR10-B and CIFAR100-B. We find that while
standard CNNs achieve human-level accuracy, the subgroup performances are not
consistent, and the phenomenon remains even after data augmentation (DA). To
alleviate this issue, we propose FlowAug, a semantic DA method that leverages
the decoupled semantic representations captured by a pre-trained generative
flow. Experimental results show that FlowAug achieves more consistent results
across subgroups than other types of DA methods on CIFAR10 and CIFAR100.
Additionally, it shows better generalization performance. Furthermore, we
propose a generic metric for studying model robustness to spurious
correlations, where we take a macro average on the weighted standard deviations
across different classes. Per our metric, FlowAug demonstrates less reliance on
spurious correlations. Although this metric is proposed to study our curated
datasets, it applies to all datasets that have subgroups or subclasses. Lastly,
aside from less dependence on spurious correlations and better generalization
on in-distribution test sets, we also show superior out-of-distribution results
on CIFAR10.1 and competitive performances on CIFAR10-C and CIFAR100-C.",0.17973626,-0.011660072,0.014136819,A
15057,We encourage joint further study on them with our CIFAR10-CoCo.,"The behavior on each level of severity and each type of
corruption are different.","resnet18                                                             resnet101                                                               preResnet18

                    frost natu1raglaussian_noise                                          frost natu1raglaussian_noise                                             frost natu1raglaussian_noise
                                                                                                                 shot_noise                                                               shot_noise
          saturate                             shot_noise                   saturate                                                                 saturate
                                                                       spatter                            0.75                                  spatter                            0.75
                                   0.75
                                                                                                     0.5 speckle_noise                                                        0.5 speckle_noise
spatter                            0.5 speckle_noise

jpeg_compression                   0.25           impulse_noise jpeg_compression                          0.25  impulse_noise jpeg_compression                          0.25  impulse_noise

pixelate                           0                                                                      0                                                             0

                                                         defocus_blur pixelate                                                  defocus_blur pixelate                                         defocus_blur

elastic_transform                                 gaussian_blur elastic_transform                               gaussian_blur elastic_transform                               gaussian_blur

contrast                                        motion_blur            contrast                                        motion_blur              contrast                             motion_blur
                                         zoom_blur                                                              zoom_blur                                                     zoom_blur
sev=1             brightness  fog  snow                                sev=1      brightness         fog  snow                                  sev=1  brightness  fog  snow
sev=2                                                                  sev=2                                                                    sev=2
sev=3                                                                  sev=3                                                                    sev=3
sev=4                                                                  sev=4                                                                    sev=4
sev=5                                                                  sev=5                                                                    sev=5

     saturate         preResnet101                                          saturate        preVitTiny224                                            saturate        preVitSmall224
spatter                                                                spatter                                                                  spatter
                    frost natu1raglaussian_noise                                          frost natu1raglaussian_noise                                             frost natu1raglaussian_noise
                                           shot_noise                                                            shot_noise                                                               shot_noise

                                    0.75                                                                  0.75                                                                     0.75

                               0.5 speckle_noise                                                     0.5 speckle_noise                                                        0.5 speckle_noise

jpeg_compression                   0.25           impulse_noise        jpeg_compression                   0.25  impulse_noise jpeg_compression                          0.25  impulse_noise

pixelate                           0                                   pixelate                           0                                                             0

                                                         defocus_blur                                                           defocus_blur pixelate                                         defocus_blur

elastic_transform                                   gaussian_blur      elastic_transform                                         gaussian_blur  elastic_transform                              gaussian_blur

contrast                                        motion_blur                     contrast                                     motion_blur               contrast                            motion_blur
                                         zoom_blur                                                                    zoom_blur                                                     zoom_blur
sev=1             brightness  fog  snow                                sev=1             brightness  fog        snow                            sev=1  brightness  fog        snow
sev=2                                                                  sev=2                                                                    sev=2
sev=3                                                                  sev=3                                                                    sev=3
sev=4                                                                  sev=4                                                                    sev=4
sev=5                                                                  sev=5                                                                    sev=5

Figure 14.",2022-12-16 18:51:41+00:00,On Human Visual Contrast Sensitivity and Machine Vision Robustness: A Comparative Study,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ming-Chang Chiu'), arxiv.Result.Author('Yingfei Wang'), arxiv.Result.Author('Derrick Eui Gyu Kim'), arxiv.Result.Author('Pin-Yu Chen'), arxiv.Result.Author('Xuezhe Ma')]","It is well established in neuroscience that color vision plays an essential
part in the human visual perception system. Meanwhile, many novel designs for
computer vision inspired by human vision have achieved success in a wide range
of tasks and applications. Nonetheless, how color differences affect machine
vision has not been well explored. Our work tries to bridge this gap between
the human color vision aspect of visual recognition and that of the machine. To
achieve this, we curate two datasets: CIFAR10-F and CIFAR100-F, which are based
on the foreground colors of the popular CIFAR datasets. Together with CIFAR10-B
and CIFAR100-B, the existing counterpart datasets with information on the
background colors of CIFAR test sets, we assign each image based on its color
contrast level per its foreground and background color labels and use this as a
proxy to study how color contrast affects machine vision. We first conduct a
proof-of-concept study, showing the effect of color difference and validate our
datasets. Furthermore, on a broader level, an important characteristic of human
vision is its robustness against ambient changes; therefore, drawing
inspirations from ophthalmology and the robustness literature, we analogize
contrast sensitivity from the human visual aspect to machine vision and
complement the current robustness study using corrupted images with our
CIFAR-CoCo datasets. In summary, motivated by neuroscience and equipped with
the datasets we curate, we devise a new framework in two dimensions to perform
extensive analyses on the effect of color contrast and corrupted images: (1)
model architecture, (2) model size, to measure the perception ability of
machine vision beyond total accuracy. We also explore how task complexity and
data augmentation play a role in this setup. Our results call attention to new
evaluation approaches for human-like machine perception.",0.090953425,0.1611262,0.17940281,A
15058,We encourage joint further study on them with our CIFAR100-CoCo.,"The behavior on each level of severity and each type
of corruption are different.","C. Training Details

   We emphasize that we do not compete for test set accuracy in this work so generalization performance is not our concern
as long as they are reasonable.",2022-12-16 18:51:41+00:00,On Human Visual Contrast Sensitivity and Machine Vision Robustness: A Comparative Study,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ming-Chang Chiu'), arxiv.Result.Author('Yingfei Wang'), arxiv.Result.Author('Derrick Eui Gyu Kim'), arxiv.Result.Author('Pin-Yu Chen'), arxiv.Result.Author('Xuezhe Ma')]","It is well established in neuroscience that color vision plays an essential
part in the human visual perception system. Meanwhile, many novel designs for
computer vision inspired by human vision have achieved success in a wide range
of tasks and applications. Nonetheless, how color differences affect machine
vision has not been well explored. Our work tries to bridge this gap between
the human color vision aspect of visual recognition and that of the machine. To
achieve this, we curate two datasets: CIFAR10-F and CIFAR100-F, which are based
on the foreground colors of the popular CIFAR datasets. Together with CIFAR10-B
and CIFAR100-B, the existing counterpart datasets with information on the
background colors of CIFAR test sets, we assign each image based on its color
contrast level per its foreground and background color labels and use this as a
proxy to study how color contrast affects machine vision. We first conduct a
proof-of-concept study, showing the effect of color difference and validate our
datasets. Furthermore, on a broader level, an important characteristic of human
vision is its robustness against ambient changes; therefore, drawing
inspirations from ophthalmology and the robustness literature, we analogize
contrast sensitivity from the human visual aspect to machine vision and
complement the current robustness study using corrupted images with our
CIFAR-CoCo datasets. In summary, motivated by neuroscience and equipped with
the datasets we curate, we devise a new framework in two dimensions to perform
extensive analyses on the effect of color contrast and corrupted images: (1)
model architecture, (2) model size, to measure the perception ability of
machine vision beyond total accuracy. We also explore how task complexity and
data augmentation play a role in this setup. Our results call attention to new
evaluation approaches for human-like machine perception.",0.21696338,-0.09996618,-0.06338801,A
15059,"We further study training the augmenta-
views are processed by a deep neural network to extract fea-       tions with more epochs.",Two          Training epochs.,"In Table 2, the performance im-
ture representations.",2022-12-19 18:59:57+00:00,Randomized Quantization for Data Agnostic Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Huimin Wu'), arxiv.Result.Author('Chenyang Lei'), arxiv.Result.Author('Xiao Sun'), arxiv.Result.Author('Peng-Shuai Wang'), arxiv.Result.Author('Qifeng Chen'), arxiv.Result.Author('Kwang-Ting Cheng'), arxiv.Result.Author('Stephen Lin'), arxiv.Result.Author('Zhirong Wu')]","Self-supervised representation learning follows a paradigm of withholding
some part of the data and tasking the network to predict it from the remaining
part. Towards this end, masking has emerged as a generic and powerful tool
where content is withheld along the sequential dimension, e.g., spatial in
images, temporal in audio, and syntactic in language. In this paper, we explore
the orthogonal channel dimension for generic data augmentation. The data for
each channel is quantized through a non-uniform quantizer, with the quantized
value sampled randomly within randomly sampled quantization bins. From another
perspective, quantization is analogous to channel-wise masking, as it removes
the information within each bin, but preserves the information across bins. We
apply the randomized quantization in conjunction with sequential augmentations
on self-supervised contrastive models. This generic approach achieves results
on par with modality-specific augmentation on vision tasks, and
state-of-the-art results on 3D point clouds as well as on audio. We also
demonstrate this method to be applicable for augmenting intermediate embeddings
in a deep neural network on the comprehensive DABS benchmark which is comprised
of various data modalities. Code is availabel at
http://www.github.com/microsoft/random_quantize.",-0.11113006,-0.18248221,0.25503695,C
15060,We further study the capability of augmenting intermedi-               6.,DABS                                                                 naive contrastive learning approach.,"Conclusion
ate features in a neural network, which is less interpretable
than the input data.",2022-12-19 18:59:57+00:00,Randomized Quantization for Data Agnostic Representation Learning,cs.CV,['cs.CV'],"[arxiv.Result.Author('Huimin Wu'), arxiv.Result.Author('Chenyang Lei'), arxiv.Result.Author('Xiao Sun'), arxiv.Result.Author('Peng-Shuai Wang'), arxiv.Result.Author('Qifeng Chen'), arxiv.Result.Author('Kwang-Ting Cheng'), arxiv.Result.Author('Stephen Lin'), arxiv.Result.Author('Zhirong Wu')]","Self-supervised representation learning follows a paradigm of withholding
some part of the data and tasking the network to predict it from the remaining
part. Towards this end, masking has emerged as a generic and powerful tool
where content is withheld along the sequential dimension, e.g., spatial in
images, temporal in audio, and syntactic in language. In this paper, we explore
the orthogonal channel dimension for generic data augmentation. The data for
each channel is quantized through a non-uniform quantizer, with the quantized
value sampled randomly within randomly sampled quantization bins. From another
perspective, quantization is analogous to channel-wise masking, as it removes
the information within each bin, but preserves the information across bins. We
apply the randomized quantization in conjunction with sequential augmentations
on self-supervised contrastive models. This generic approach achieves results
on par with modality-specific augmentation on vision tasks, and
state-of-the-art results on 3D point clouds as well as on audio. We also
demonstrate this method to be applicable for augmenting intermediate embeddings
in a deep neural network on the comprehensive DABS benchmark which is comprised
of various data modalities. Code is availabel at
http://www.github.com/microsoft/random_quantize.",0.07056447,-0.29492587,0.057156786,C
15073,"7 CHALLENGES AND OPPORTUNITIES

Although researchers have paid a lot of attention on human image generation, there are still a lot
of challenges and further research opportunities in this area.","The coarse-to-fine and the texture-rendering generation procedures are generally appropriate for
both the data-augmentation and VITON tasks.",‚Ä¢ Further improve the generation quality.,2022-12-17 15:19:45+00:00,Human Image Generation: A Comprehensive Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Zhen Jia'), arxiv.Result.Author('Zhang Zhang'), arxiv.Result.Author('Liang Wang'), arxiv.Result.Author('Tieniu Tan')]","Image and video synthesis has become a blooming topic in computer vision and
machine learning communities along with the developments of deep generative
models, due to its great academic and application value. Many researchers have
been devoted to synthesizing high-fidelity human images as one of the most
commonly seen object categories in daily lives, where a large number of studies
are performed based on various deep generative models, task settings and
applications. Thus, it is necessary to give a comprehensive overview on these
variant methods on human image generation. In this paper, we divide human image
generation techniques into three paradigms, i.e., data-driven methods,
knowledge-guided methods and hybrid methods. For each route, the most
representative models and the corresponding variants are presented, where the
advantages and characteristics of different methods are summarized in terms of
model architectures and input/output requirements. Besides, the main public
human image datasets and evaluation metrics in the literature are also
summarized. Furthermore, due to the wide application potentials, two typical
downstream usages of synthesized human images are covered, i.e., data
augmentation for person recognition tasks and virtual try-on for fashion
customers. Finally, we discuss the challenges and potential directions of human
image generation to shed light on future research.",-0.14968309,0.09566575,0.068944514,C
15076,"To facilitate further research, we establish baselines for
                                                                                                      camera-based streaming 3D detection, which consistently              Despite the growing research interest in vision-centric
                                                                                                      enhance the streaming performance across various hard-            approaches, the high latency of these methods still prevents
                                                                                                      ware.","and depth estimation [23, 24, 63, 65, 66, 68]).",ASAP project page: https://github.com/                      the practical deployment.,2022-12-17 16:32:15+00:00,Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark,cs.CV,['cs.CV'],"[arxiv.Result.Author('Xiaofeng Wang'), arxiv.Result.Author('Zheng Zhu'), arxiv.Result.Author('Yunpeng Zhang'), arxiv.Result.Author('Guan Huang'), arxiv.Result.Author('Yun Ye'), arxiv.Result.Author('Wenbo Xu'), arxiv.Result.Author('Ziwei Chen'), arxiv.Result.Author('Xingang Wang')]","In recent years, vision-centric perception has flourished in various
autonomous driving tasks, including 3D detection, semantic map construction,
motion forecasting, and depth estimation. Nevertheless, the latency of
vision-centric approaches is too high for practical deployment (e.g., most
camera-based 3D detectors have a runtime greater than 300ms). To bridge the gap
between ideal research and real-world applications, it is necessary to quantify
the trade-off between performance and efficiency. Traditionally,
autonomous-driving perception benchmarks perform the offline evaluation,
neglecting the inference time delay. To mitigate the problem, we propose the
Autonomous-driving StreAming Perception (ASAP) benchmark, which is the first
benchmark to evaluate the online performance of vision-centric perception in
autonomous driving. On the basis of the 2Hz annotated nuScenes dataset, we
first propose an annotation-extending pipeline to generate high-frame-rate
labels for the 12Hz raw images. Referring to the practical deployment, the
Streaming Perception Under constRained-computation (SPUR) evaluation protocol
is further constructed, where the 12Hz inputs are utilized for streaming
evaluation under the constraints of different computational resources. In the
ASAP benchmark, comprehensive experiment results reveal that the model rank
alters under different constraints, suggesting that the model latency and
computation budget should be considered as design choices to optimize the
practical deployment. To facilitate further research, we establish baselines
for camera-based streaming 3D detection, which consistently enhance the
streaming performance across various hardware. ASAP project page:
https://github.com/JeffWang987/ASAP.",-0.33653957,0.3712501,0.06808087,B
15086,"V. Finally, we provide concluding remarks
image enhancement algorithms perform well on underwater           and future directions for further research in Sec.","The detailed discussions of our results are
5] showed that convolutional neural network (CNN) based           presented in Sec.",VI.,2022-12-18 01:07:20+00:00,Adaptive Uncertainty Distribution in Deep Learning for Unsupervised Underwater Image Enhancement,cs.CV,['cs.CV'],"[arxiv.Result.Author('Alzayat Saleh'), arxiv.Result.Author('Marcus Sheaves'), arxiv.Result.Author('Dean Jerry'), arxiv.Result.Author('Mostafa Rahimi Azghadi')]","One of the main challenges in deep learning-based underwater image
enhancement is the limited availability of high-quality training data.
Underwater images are difficult to capture and are often of poor quality due to
the distortion and loss of colour and contrast in water. This makes it
difficult to train supervised deep learning models on large and diverse
datasets, which can limit the model's performance. In this paper, we explore an
alternative approach to supervised underwater image enhancement. Specifically,
we propose a novel unsupervised underwater image enhancement framework that
employs a conditional variational autoencoder (cVAE) to train a deep learning
model with probabilistic adaptive instance normalization (PAdaIN) and
statistically guided multi-colour space stretch that produces realistic
underwater images. The resulting framework is composed of a U-Net as a feature
extractor and a PAdaIN to encode the uncertainty, which we call UDnet. To
improve the visual quality of the images generated by UDnet, we use a
statistically guided multi-colour space stretch module that ensures visual
consistency with the input image and provides an alternative to training using
a ground truth image. The proposed model does not need manual human annotation
and can learn with a limited amount of data and achieves state-of-the-art
results on underwater images. We evaluated our proposed framework on eight
publicly-available datasets. The results show that our proposed framework
yields competitive performance compared to other state-of-the-art approaches in
quantitative as well as qualitative metrics. Code available at
https://github.com/alzayats/UDnet .",-0.14540908,0.095589906,0.25412112,C
15088,"Squat                        16          974           2%
                                                                          Pull ups                     59          704           8%
   ‚Ä¢ We provide the annotated dataset along with detailed
      guideline to continue further research1.","Bouncing on trampoline       534         315           63%
                                                                          Baseball throw               293         256           53%
   ‚Ä¢ We show there is no strong correlation between con-                  Climbing tree                438         390           53%
      Ô¨Ådence value of skeleton sequences and the class-wise               Cutting watermelon           27          723           4%
      accuracy of the model.","2) Kinetics-Skeleton Subset (KSS): Kinetics-skeleton is
                                                                          a benchmark dataset used in HAR research that contains
   The rest of the paper is organized as follows.",2022-12-18 07:36:32+00:00,2D Pose Estimation based Child Action Recognition,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Sanka Mohottala'), arxiv.Result.Author('Sandun Abeygunawardana'), arxiv.Result.Author('Pradeepa Samarasinghe'), arxiv.Result.Author('Dharshana Kasthurirathna'), arxiv.Result.Author('Charith Abhayaratne')]","We present a graph convolutional network with 2D pose estimation for the
first time on child action recognition task achieving on par results with an
RGB modality based model on a novel benchmark dataset containing unconstrained
environment based videos.",0.22444385,0.09228816,-0.1581641,A
15089,"We
frames by the teacher network, SMPLify-X [28], and even        further research several different ratios of feature exchange
better at some splits.","This num-
frame sr to gait sequence g. In addition, we also note from    ber indicates that we use 75% of features from the current
the table that using the distilled feature from CRD is compa-  frame, 12.5% from future frames, and 12.5% from the pre-
rable to the body prior directly extracted from selected RGB   vious frame for the next step‚Äôs convolution operation.","With knowledge distillation, body       in Table 8.",2022-12-18 09:27:00+00:00,Gait Recognition Using 3-D Human Body Shape Inference,cs.CV,['cs.CV'],"[arxiv.Result.Author('Haidong Zhu'), arxiv.Result.Author('Zhaoheng Zheng'), arxiv.Result.Author('Ram Nevatia')]","Gait recognition, which identifies individuals based on their walking
patterns, is an important biometric technique since it can be observed from a
distance and does not require the subject's cooperation. Recognizing a person's
gait is difficult because of the appearance variants in human silhouette
sequences produced by varying viewing angles, carrying objects, and clothing.
Recent research has produced a number of ways for coping with these variants.
In this paper, we present the usage of inferring 3-D body shapes distilled from
limited images, which are, in principle, invariant to the specified variants.
Inference of 3-D shape is a difficult task, especially when only silhouettes
are provided in a dataset. We provide a method for learning 3-D body inference
from silhouettes by transferring knowledge from 3-D shape prior from RGB
photos. We use our method on multiple existing state-of-the-art gait baselines
and obtain consistent improvements for gait identification on two public
datasets, CASIA-B and OUMVLP, on several variants and settings, including a new
setting of novel views not seen during training.",0.0683917,0.050197233,0.09264428,A
15090,"So it also can be used for further research
                                                                                 about these datasets.","Furthermore, the inserted occlusions are either hands, masks,
                                                                                 or microphones.","7

Method         eyes   mouth   occlusion  overall
              0.7041  0.7212   0.3431    0.7353
SEGM          0.7121  0.7345   0.4021    0.7521
SEGM+PPM
SEGM+PPM      0.7445  0.8205   0.6243    0.8052

        +LOM  0.8015  0.8305   0.8523    0.8453
SEGM+PPM      0.8402  0.8815   0.9203    0.9013

        +LOM
        +DOM
Mask FPAN

TABLE I: MIOU of Mask FPAN Variants and Baseline.",2022-12-18 14:51:46+00:00,Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN,cs.CV,['cs.CV'],"[arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Tianfang Zhang'), arxiv.Result.Author('Stefan Oehmcke'), arxiv.Result.Author('Fabian Gieseke'), arxiv.Result.Author('Christian Igel')]","Fine-grained semantic segmentation of a person's face and head, including
facial parts and head components, has progressed a great deal in recent years.
However, it remains a challenging task, whereby considering ambiguous
occlusions and large pose variations are particularly difficult. To overcome
these difficulties, we propose a novel framework termed Mask-FPAN. It uses a
de-occlusion module that learns to parse occluded faces in a semi-supervised
way. In particular, face landmark localization, face occlusionstimations, and
detected head poses are taken into account. A 3D morphable face model combined
with the UV GAN improves the robustness of 2D face parsing. In addition, we
introduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face
paring work. The proposed Mask-FPAN framework addresses the face parsing
problem in the wild and shows significant performance improvements with MIOU
from 0.7353 to 0.9013 compared to the state-of-the-art on challenging face
datasets.",0.13095781,0.19103266,-0.05933764,A
15147,"Recent deep learning-based methods have stimulated
further research interests in texture modelling, and texture       3.","To our best knowledge, GAN inversion has not
                                                                   been applied to texture analysis for synthesis so far.","Method
analysis and synthesis in particular.",2022-12-20 03:57:11+00:00,Texture Representation via Analysis and Synthesis with Generative Adversarial Networks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Jue Lin'), arxiv.Result.Author('Gaurav Sharma'), arxiv.Result.Author('Thrasyvoulos N. Pappas')]","We investigate data-driven texture modeling via analysis and synthesis with
generative adversarial networks. For network training and testing, we have
compiled a diverse set of spatially homogeneous textures, ranging from
stochastic to regular. We adopt StyleGAN3 for synthesis and demonstrate that it
produces diverse textures beyond those represented in the training data. For
texture analysis, we propose GAN inversion using a novel latent domain
reconstruction consistency criterion for synthesized textures, and iterative
refinement with Gramian loss for real textures. We propose perceptual
procedures for evaluating network capabilities, exploring the global and local
behavior of latent space trajectories, and comparing with existing texture
analysis-synthesis techniques.",-0.10949509,-0.094925106,0.32379577,C
15160,"To achieve higher hardware efÔ¨Åciency, it
                                                  is necessary to further study the low-bit quantization of AdderNet.","arXiv:2212.10200v1 [cs.CV] 20 Dec 2022       Redistribution of Weights and Activations for
                                                           AdderNet Quantization

                                              Ying Nie1 Kai Han1,2 Haikang Diao3 Chuanjian Liu1 Enhua Wu2,4 Yunhe Wang1‚àó
                                                                                    1Huawei Noah‚Äôs Ark Lab

                                                                  2State Key Lab of Computer Science, ISCAS & UCAS
                                                        3School of Integrated Circuits, Peking University 4University of Macau

                                                                         {ying.nie, kai.han, yunhe.wang}@huawei.com

                                                                                    Abstract

                                                  Adder Neural Network (AdderNet) provides a new way for developing energy-
                                                  efÔ¨Åcient neural networks by replacing the expensive multiplications in convolution
                                                  with cheaper additions (i.e., 1-norm).","Due to the
                                                  limitation that the commutative law in multiplication does not hold in 1-norm, the
                                                  well-established quantization methods on convolutional networks cannot be applied
                                                  on AdderNets.",2022-12-20 12:24:48+00:00,Redistribution of Weights and Activations for AdderNet Quantization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Ying Nie'), arxiv.Result.Author('Kai Han'), arxiv.Result.Author('Haikang Diao'), arxiv.Result.Author('Chuanjian Liu'), arxiv.Result.Author('Enhua Wu'), arxiv.Result.Author('Yunhe Wang')]","Adder Neural Network (AdderNet) provides a new way for developing
energy-efficient neural networks by replacing the expensive multiplications in
convolution with cheaper additions (i.e.l1-norm). To achieve higher hardware
efficiency, it is necessary to further study the low-bit quantization of
AdderNet. Due to the limitation that the commutative law in multiplication does
not hold in l1-norm, the well-established quantization methods on convolutional
networks cannot be applied on AdderNets. Thus, the existing AdderNet
quantization techniques propose to use only one shared scale to quantize both
the weights and activations simultaneously. Admittedly, such an approach can
keep the commutative law in the l1-norm quantization process, while the
accuracy drop after low-bit quantization cannot be ignored. To this end, we
first thoroughly analyze the difference on distributions of weights and
activations in AdderNet and then propose a new quantization algorithm by
redistributing the weights and the activations. Specifically, the pre-trained
full-precision weights in different kernels are clustered into different
groups, then the intra-group sharing and inter-group independent scales can be
adopted. To further compensate the accuracy drop caused by the distribution
difference, we then develop a lossless range clamp scheme for weights and a
simple yet effective outliers clamp strategy for activations. Thus, the
functionality of full-precision weights and the representation ability of
full-precision activations can be fully preserved. The effectiveness of the
proposed quantization method for AdderNet is well verified on several
benchmarks, e.g., our 4-bit post-training quantized adder ResNet-18 achieves an
66.5% top-1 accuracy on the ImageNet with comparable energy efficiency, which
is about 8.5% higher than that of the previous AdderNet quantization methods.",0.12768537,-0.14998913,0.409024,C
15163,"Except for the source attack
                                                                assessed, we further study the eÔ¨Äectiveness of attacks in three
                                                                settings in this section:

                                                                 ‚Ä¢ Cross-model attack: We obtain the adversarial examples
                                                                    learned from another model to fool the target detectors.","Since there may be no direct access to the target
                                                                detectors, we also study the potential vulnerability of de-
                                                                tectors under black-box attacks.","A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks                13

Table 5: Transferability of adversarial examples for point perturbation attack generated from source models and evaluated on
target models.",2022-12-20 13:09:58+00:00,A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yifan Zhang'), arxiv.Result.Author('Junhui Hou'), arxiv.Result.Author('Yixuan Yuan')]","Deep learning-based 3D object detectors have made significant progress in
recent years and have been deployed in a wide range of applications. It is
crucial to understand the robustness of detectors against adversarial attacks
when employing detectors in security-critical applications. In this paper, we
make the first attempt to conduct a thorough evaluation and analysis of the
robustness of 3D detectors under adversarial attacks. Specifically, we first
extend three kinds of adversarial attacks to the 3D object detection task to
benchmark the robustness of state-of-the-art 3D object detectors against
attacks on KITTI and Waymo datasets, subsequently followed by the analysis of
the relationship between robustness and properties of detectors. Then, we
explore the transferability of cross-model, cross-task, and cross-data attacks.
We finally conduct comprehensive experiments of defense for 3D detectors,
demonstrating that simple transformations like flipping are of little help in
improving robustness when the strategy of transformation imposed on input point
cloud data is exposed to attackers. Our findings will facilitate investigations
in understanding and defending the adversarial attacks against 3D object
detectors to advance this field.",-0.15319777,-0.053531177,0.1288682,C
15164,ingful topic and deserves further research in the future.,"proves the robustness of detectors under point perturbation
How to design an eÔ¨Äective cross-domain attack is a mean-        attack.","8 Evaluation of Defenses                                            Table 11 shows the mAP ratios of detectors under point
                                                                detachment attack when diÔ¨Äerent defense methods are ap-
8.1 Robustness Conferred by Defense                             plied.",2022-12-20 13:09:58+00:00,A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yifan Zhang'), arxiv.Result.Author('Junhui Hou'), arxiv.Result.Author('Yixuan Yuan')]","Deep learning-based 3D object detectors have made significant progress in
recent years and have been deployed in a wide range of applications. It is
crucial to understand the robustness of detectors against adversarial attacks
when employing detectors in security-critical applications. In this paper, we
make the first attempt to conduct a thorough evaluation and analysis of the
robustness of 3D detectors under adversarial attacks. Specifically, we first
extend three kinds of adversarial attacks to the 3D object detection task to
benchmark the robustness of state-of-the-art 3D object detectors against
attacks on KITTI and Waymo datasets, subsequently followed by the analysis of
the relationship between robustness and properties of detectors. Then, we
explore the transferability of cross-model, cross-task, and cross-data attacks.
We finally conduct comprehensive experiments of defense for 3D detectors,
demonstrating that simple transformations like flipping are of little help in
improving robustness when the strategy of transformation imposed on input point
cloud data is exposed to attackers. Our findings will facilitate investigations
in understanding and defending the adversarial attacks against 3D object
detectors to advance this field.",0.16247848,0.11457297,-0.027401503,A
15165,"So in
The results reported in Table 10 show simple transforma-         this section, we further study whether the defense is eÔ¨Äective
tions, including Ô¨Çipping, rotation, scaling, quantiÔ¨Åcation, and  against adaptive attacks.","According to KerckhoÔ¨Ä‚Äôs principle (KerckhoÔ¨Äs,
8.2 Defense Against Adaptive Attacks                             1883), a system should be safe even if all information about
                                                                 it except for the key is publicly available knowledge.","A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks         19

Table 13: Robustness of detectors (mAP ratio) after applying 7 transformation-based defense methods under adaptive point
perturbation attack.",2022-12-20 13:09:58+00:00,A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yifan Zhang'), arxiv.Result.Author('Junhui Hou'), arxiv.Result.Author('Yixuan Yuan')]","Deep learning-based 3D object detectors have made significant progress in
recent years and have been deployed in a wide range of applications. It is
crucial to understand the robustness of detectors against adversarial attacks
when employing detectors in security-critical applications. In this paper, we
make the first attempt to conduct a thorough evaluation and analysis of the
robustness of 3D detectors under adversarial attacks. Specifically, we first
extend three kinds of adversarial attacks to the 3D object detection task to
benchmark the robustness of state-of-the-art 3D object detectors against
attacks on KITTI and Waymo datasets, subsequently followed by the analysis of
the relationship between robustness and properties of detectors. Then, we
explore the transferability of cross-model, cross-task, and cross-data attacks.
We finally conduct comprehensive experiments of defense for 3D detectors,
demonstrating that simple transformations like flipping are of little help in
improving robustness when the strategy of transformation imposed on input point
cloud data is exposed to attackers. Our findings will facilitate investigations
in understanding and defending the adversarial attacks against 3D object
detectors to advance this field.",-0.15049413,0.112314865,0.13054588,B
15166,"Therefore, adversarial training is the promising       2021 IEEE/RSJ International Conference on Intelligent
way to improve robustness of detectors against adversarial          Robots and Systems (IROS), IEEE, pp 2189‚Äì2194
attacks and deserves further study.","In:
point cloud.","Athalye A, Carlini N, Wagner D (2018) Obfuscated gradients
9 Conclusion                                                        give a false sense of security: Circumventing defenses

In this paper, we thoroughly investigated the robustness of
3D detectors under three basic adversarial attacks.",2022-12-20 13:09:58+00:00,A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yifan Zhang'), arxiv.Result.Author('Junhui Hou'), arxiv.Result.Author('Yixuan Yuan')]","Deep learning-based 3D object detectors have made significant progress in
recent years and have been deployed in a wide range of applications. It is
crucial to understand the robustness of detectors against adversarial attacks
when employing detectors in security-critical applications. In this paper, we
make the first attempt to conduct a thorough evaluation and analysis of the
robustness of 3D detectors under adversarial attacks. Specifically, we first
extend three kinds of adversarial attacks to the 3D object detection task to
benchmark the robustness of state-of-the-art 3D object detectors against
attacks on KITTI and Waymo datasets, subsequently followed by the analysis of
the relationship between robustness and properties of detectors. Then, we
explore the transferability of cross-model, cross-task, and cross-data attacks.
We finally conduct comprehensive experiments of defense for 3D detectors,
demonstrating that simple transformations like flipping are of little help in
improving robustness when the strategy of transformation imposed on input point
cloud data is exposed to attackers. Our findings will facilitate investigations
in understanding and defending the adversarial attacks against 3D object
detectors to advance this field.",-0.22775477,0.0017569151,0.13719495,B
15175,The time           left as a topic for further research.,"A more detailed analysis is
onds for one image evaluated on images from CSIQ.","for prediction does not depend on the number of descriptors,
but only on the number of codevectors,                                                          VI.",2022-12-20 15:11:57+00:00,Image quality prediction using synthetic and natural codebooks: comparative results,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maxim Koroteev'), arxiv.Result.Author('Kirill Aistov'), arxiv.Result.Author('Valeriy Berezovskiy'), arxiv.Result.Author('Pavel Frolov')]","We investigate a model for image/video quality assessment based on building a
set of codevectors representing in a sense some basic properties of images,
similar to well-known CORNIA model. We analyze the codebook building method and
propose some modifications for it. Also the algorithm is investigated from the
point of inference time reduction. Both natural and synthetic images are used
for building codebooks and some analysis of synthetic images used for codebooks
is provided. It is demonstrated the results on quality assessment may be
improves with the use if synthetic images for codebook construction. We also
demonstrate regimes of the algorithm in which real time execution on CPU is
possible for sufficiently high correlations with mean opinion score (MOS).
Various pooling strategies are considered as well as the problem of metric
sensitivity to bitrate.",0.09656984,0.11007072,-0.027174115,A
15176,The time           left as a topic for further research.,"A more detailed analysis is
onds for one image evaluated on images from CSIQ.","for prediction does not depend on the number of descriptors,
but only on the number of codevectors,                                                          VI.",2022-12-20 15:11:57+00:00,Image quality prediction using synthetic and natural codebooks: comparative results,cs.CV,['cs.CV'],"[arxiv.Result.Author('Maxim Koroteev'), arxiv.Result.Author('Kirill Aistov'), arxiv.Result.Author('Valeriy Berezovskiy'), arxiv.Result.Author('Pavel Frolov')]","We investigate a model for image/video quality assessment based on building a
set of codevectors representing in a sense some basic properties of images,
similar to well-known CORNIA model. We analyze the codebook building method and
propose some modifications for it. Also the algorithm is investigated from the
point of inference time reduction. Both natural and synthetic images are used
for building codebooks and some analysis of synthetic images used for codebooks
is provided. It is demonstrated the results on quality assessment may be
improves with the use if synthetic images for codebook construction. We also
demonstrate regimes of the algorithm in which real time execution on CPU is
possible for sufficiently high correlations with mean opinion score (MOS).
Various pooling strategies are considered as well as the problem of metric
sensitivity to bitrate.",0.09656984,0.11007072,-0.027174115,A
15194,We hope our study can inspire further research in       3.,"data at different scales and in handling out-of-distribution
samples.","Methodology
the Ô¨Åeld of visual prompt learning.",2022-12-20 18:57:06+00:00,Unleashing the Power of Visual Prompting At the Pixel Level,cs.CV,['cs.CV'],"[arxiv.Result.Author('Junyang Wu'), arxiv.Result.Author('Xianhang Li'), arxiv.Result.Author('Chen Wei'), arxiv.Result.Author('Huiyu Wang'), arxiv.Result.Author('Alan Yuille'), arxiv.Result.Author('Yuyin Zhou'), arxiv.Result.Author('Cihang Xie')]","This paper presents a simple and effective visual prompting method for
adapting pre-trained models to downstream recognition tasks. Our method
includes two key designs. First, rather than directly adding together the
prompt and the image, we treat the prompt as an extra and independent learnable
component. We show that the strategy of reconciling the prompt and the image
matters, and find that warping the prompt around a properly shrinked image
empirically works the best. Second, we re-introduce two ""old tricks"" commonly
used in building transferable adversarial examples, i.e., input diversity and
gradient normalization, into visual prompting. These techniques improve
optimization and enable the prompt to generalize better. We provide extensive
experimental results to demonstrate the effectiveness of our method. Using a
CLIP model, our prompting method sets a new record of 82.8% average accuracy
across 12 popular classification datasets, substantially surpassing the prior
art by +5.6%. It is worth noting that this prompting performance already
outperforms linear probing by +2.1% and can even match fully fine-tuning in
certain datasets. In addition, our prompting method shows competitive
performance across different data scales and against distribution shifts. The
code is publicly available at https://github.com/UCSC-VLAA/EVP.",0.005517899,-0.07274757,-0.26809502,C
15198,"There-
                                                                  fore, we speciÔ¨Åcally study its effect on model selection in
   We further study the sample-level model selection perfor-      Table V. We have the following observations: 1) different GD
mance in Table IV.",hyperparameter of TOD-based model selection method.,"In this experiment, the top-k metric makes     iteration intervals may induce a slight performance difference
less sense as it needs to be averaged over all the test samples.",2022-12-20 19:29:37+00:00,Temporal Output Discrepancy for Loss Estimation-based Active Learning,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Siyu Huang'), arxiv.Result.Author('Tianyang Wang'), arxiv.Result.Author('Haoyi Xiong'), arxiv.Result.Author('Bihan Wen'), arxiv.Result.Author('Jun Huan'), arxiv.Result.Author('Dejing Dou')]","While deep learning succeeds in a wide range of tasks, it highly depends on
the massive collection of annotated data which is expensive and time-consuming.
To lower the cost of data annotation, active learning has been proposed to
interactively query an oracle to annotate a small proportion of informative
samples in an unlabeled dataset. Inspired by the fact that the samples with
higher loss are usually more informative to the model than the samples with
lower loss, in this paper we present a novel deep active learning approach that
queries the oracle for data annotation when the unlabeled sample is believed to
incorporate high loss. The core of our approach is a measurement Temporal
Output Discrepancy (TOD) that estimates the sample loss by evaluating the
discrepancy of outputs given by models at different optimization steps. Our
theoretical investigation shows that TOD lower-bounds the accumulated sample
loss thus it can be used to select informative unlabeled samples. On basis of
TOD, we further develop an effective unlabeled data sampling strategy as well
as an unsupervised learning criterion for active learning. Due to the
simplicity of TOD, our methods are efficient, flexible, and task-agnostic.
Extensive experimental results demonstrate that our approach achieves superior
performances than the state-of-the-art active learning methods on image
classification and semantic segmentation tasks. In addition, we show that TOD
can be utilized to select the best model of potentially the highest testing
accuracy from a pool of candidate models.",0.44185972,0.033192076,-0.036637507,A
15210,"ority of our proposed model which proves the comparable
For a further study, Chen [12] et al.","MAE [10] and BEiT [11] achieve highly competitive results             ‚Ä¢ Qualitative and quantitative analyses illustrate the superi-
through inpainting the images occluded by random masks.","recently explore how                prediction performance and indicative interpretability of
higher random mask rates beneÔ¨Åt the performance.",2022-12-21 02:48:15+00:00,UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Chenlu Zhan'), arxiv.Result.Author('Peng Peng'), arxiv.Result.Author('Hongsen Wang'), arxiv.Result.Author('Tao Chen'), arxiv.Result.Author('Hongwei Wang')]","Medical Visual Question Answering (Medical-VQA) aims to answer clinical
questions regarding radiology images, assisting doctors with decision-making
options. Nevertheless, current Medical-VQA models learn cross-modal
representations through residing vision and texture encoders in dual separate
spaces, which lead to indirect semantic alignment. In this paper, we propose
UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive
Representation Learning with Adversarial Masking. Specifically, to learn an
aligned image-text representation, we first establish a unified dual-stream
pre-training structure with the gradually soft-parameter sharing strategy.
Technically, the proposed strategy learns a constraint for the vision and
texture encoders to be close in a same space, which is gradually loosened as
the higher number of layers. Moreover, for grasping the semantic
representation, we extend the unified Adversarial Masking data augmentation
strategy to the contrastive representation learning of vision and text in a
unified manner, alleviating the meaningless of the commonly used random mask.
Concretely, while the encoder training minimizes the distance between the
original feature and the masking feature, the adversarial masking model keeps
adversarial learning to conversely maximize the distance. Furthermore, we also
intuitively take a further exploration of the unified adversarial masking
strategy, which improves the potential ante-hoc interpretability with
remarkable performance and efficiency. Experimental results on VQA-RAD and
SLAKE public benchmarks demonstrate that UnICLAM outperforms the existing 11
state-of-the-art Medical-VQA models. More importantly, we make an additional
discussion about the performance of UnICLAM in diagnosing heart failure,
verifying that UnICLAM exhibits superior few-shot adaption performance in
practical disease diagnosis.",-0.0035944348,0.10682504,0.109859586,C
15211,"For a further study, Chen et al.","promising performance with the powerful random mask strat-
egy exclusively.","[7] take a      B. UniÔ¨Åed Image and Texture Contrastive representation
detailed discussion which indicates that the higher random        Learning
mask rates beneÔ¨Åt the baseline.",2022-12-21 02:48:15+00:00,UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Chenlu Zhan'), arxiv.Result.Author('Peng Peng'), arxiv.Result.Author('Hongsen Wang'), arxiv.Result.Author('Tao Chen'), arxiv.Result.Author('Hongwei Wang')]","Medical Visual Question Answering (Medical-VQA) aims to to answer clinical
questions regarding radiology images, assisting doctors with decision-making
options. Nevertheless, current Medical-VQA models learn cross-modal
representations through residing vision and texture encoders in dual separate
spaces, which lead to indirect semantic alignment. In this paper, we propose
UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive
Representation Learning with Adversarial Masking. Specifically, to learn an
aligned image-text representation, we first establish a unified dual-stream
pre-training structure with the gradually soft-parameter sharing strategy.
Technically, the proposed strategy learns a constraint for the vision and
texture encoders to be close in a same space, which is gradually loosened as
the higher number of layers. Moreover, for grasping the unified semantic
representation, we extend the adversarial masking data augmentation to the
contrastive representation learning of vision and text in a unified manner.
Concretely, while the encoder training minimizes the distance between original
and masking samples, the adversarial masking module keeps adversarial learning
to conversely maximize the distance. Furthermore, we also intuitively take a
further exploration to the unified adversarial masking augmentation model,
which improves the potential ante-hoc interpretability with remarkable
performance and efficiency. Experimental results on VQA-RAD and SLAKE public
benchmarks demonstrate that UnICLAM outperforms existing 11 state-of-the-art
Medical-VQA models. More importantly, we make an additional discussion about
the performance of UnICLAM in diagnosing heart failure, verifying that UnICLAM
exhibits superior few-shot adaption performance in practical disease diagnosis.",0.038697243,-0.03781337,0.23037678,C
15237,"Therefore, we further study versatile and               across Tokens.","5.1 Modulation Across Tokens (MoTo)

    Motivated by these Ô¨Åndings, we expect our tokenizer to               Therefore, we propose to normalize the input content in a
simultaneously maintain the information accessibility and                spatial-aware manner called MoTo, short for Modulation
feature quality.","The core idea is to modulate the diverse
unexplored design strategies for vision tokenizers, through              semantics in input using regional statistics.",2022-12-21 15:51:43+00:00,What Makes for Good Tokenizers in Vision Transformer?,cs.CV,['cs.CV'],"[arxiv.Result.Author('Shengju Qian'), arxiv.Result.Author('Yi Zhu'), arxiv.Result.Author('Wenbo Li'), arxiv.Result.Author('Mu Li'), arxiv.Result.Author('Jiaya Jia')]","The architecture of transformers, which recently witness booming applications
in vision tasks, has pivoted against the widespread convolutional paradigm.
Relying on the tokenization process that splits inputs into multiple tokens,
transformers are capable of extracting their pairwise relationships using
self-attention. While being the stemming building block of transformers, what
makes for a good tokenizer has not been well understood in computer vision. In
this work, we investigate this uncharted problem from an information trade-off
perspective. In addition to unifying and understanding existing structural
modifications, our derivation leads to better design strategies for vision
tokenizers. The proposed Modulation across Tokens (MoTo) incorporates
inter-token modeling capability through normalization. Furthermore, a
regularization objective TokenProp is embraced in the standard training regime.
Through extensive experiments on various transformer architectures, we observe
both improved performance and intriguing properties of these two plug-and-play
designs with negligible computational overhead. These observations further
indicate the importance of the commonly-omitted designs of tokenizers in vision
transformer.",-0.1045039,-0.039168894,-0.11675432,C
15244,"goal of this benchmark is to provide a comprehensive and            Here, we further study the role of segmentation objectives in
standard evaluation protocol for open-vocabulary segmen-            vision-language understanding.","The           to image segmentation, particularly in the zero-shot setting.","To investigate, we remove
tation on different vocabulary sizes and image domains.",2022-12-21 18:58:41+00:00,"Generalized Decoding for Pixel, Image, and Language",cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Author('Xueyan Zou'), arxiv.Result.Author('Zi-Yi Dou'), arxiv.Result.Author('Jianwei Yang'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Linjie Li'), arxiv.Result.Author('Chunyuan Li'), arxiv.Result.Author('Xiyang Dai'), arxiv.Result.Author('Harkirat Behl'), arxiv.Result.Author('Jianfeng Wang'), arxiv.Result.Author('Lu Yuan'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Lijuan Wang'), arxiv.Result.Author('Yong Jae Lee'), arxiv.Result.Author('Jianfeng Gao')]","We present X-Decoder, a generalized decoding model that can predict
pixel-level segmentation and language tokens seamlessly. X-Decodert takes as
input two types of queries: (i) generic non-semantic queries and (ii) semantic
queries induced from text inputs, to decode different pixel-level and
token-level outputs in the same semantic space. With such a novel design,
X-Decoder is the first work that provides a unified way to support all types of
image segmentation and a variety of vision-language (VL) tasks. Further, our
design enables seamless interactions across tasks at different granularities
and brings mutual benefits by learning a common and rich pixel-level
visual-semantic understanding space, without any pseudo-labeling. After
pretraining on a mixed set of a limited amount of segmentation data and
millions of image-text pairs, X-Decoder exhibits strong transferability to a
wide range of downstream tasks in both zero-shot and finetuning settings.
Notably, it achieves (1) state-of-the-art results on open-vocabulary
segmentation and referring segmentation on eight datasets; (2) better or
competitive finetuned performance to other generalist and specialist models on
segmentation and VL tasks; and (3) flexibility for efficient finetuning and
novel task composition (e.g., referring captioning and image editing). Code,
demo, video, and visualization are available at https://x-decoder-vl.github.io.",-0.15111971,-0.17943215,-0.048257023,C
15255,"Moreover, our results could   CNN (CTS-CNN) to recover areas that were covered by
                                        be helpful for making further research improvements.","For
                                        outcome of this project can be used to develop applications that   cloud removal, they proposed using content-texture-spectral
                                        require cloud-free satellite images.",clouds.,2022-12-21 21:14:35+00:00,MM811 Project Report: Cloud Detection and Removal in Satellite Images,cs.CV,"['cs.CV', 'cs.LG', 'eess.IV']","[arxiv.Result.Author('Dale Chen-Song'), arxiv.Result.Author('Erfan Khalaji'), arxiv.Result.Author('Vaishali Rani')]","For satellite images, the presence of clouds presents a problem as clouds
obscure more than half to two-thirds of the ground information. This problem
causes many issues for reliability in a noise-free environment to communicate
data and other applications that need seamless monitoring. Removing the clouds
from the images while keeping the background pixels intact can help address the
mentioned issues. Recently, deep learning methods have become popular for
researching cloud removal by demonstrating promising results, among which
Generative Adversarial Networks (GAN) have shown considerably better
performance. In this project, we aim to address cloud removal from satellite
images using AttentionGAN and then compare our results by reproducing the
results obtained using traditional GANs and auto-encoders. We use RICE dataset.
The outcome of this project can be used to develop applications that require
cloud-free satellite images. Moreover, our results could be helpful for making
further research improvements.",-0.2044546,0.058814533,0.19921555,C
15269,"We further study the effec-
posed model on three datasets, and we choose mDice, mIoU
and MAE for evaluation.","GSA and LSA rep-
   We further conduct ablation study to demonstrate the         resent global spatial attention module and local spatial at-
necessity and effectiveness of each component of our pro-       tention module respectively.",Effectiveness of SBA and GLSA.,2022-12-21 07:54:02+00:00,DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation,cs.CV,['cs.CV'],"[arxiv.Result.Author('Feilong Tang'), arxiv.Result.Author('Qiming Huang'), arxiv.Result.Author('Jinfeng Wang'), arxiv.Result.Author('Xianxu Hou'), arxiv.Result.Author('Jionglong Su'), arxiv.Result.Author('Jingxin Liu')]","Transformer-based models have been widely demonstrated to be successful in
computer vision tasks by modelling long-range dependencies and capturing global
representations. However, they are often dominated by features of large
patterns leading to the loss of local details (e.g., boundaries and small
objects), which are critical in medical image segmentation. To alleviate this
problem, we propose a Dual-Aggregation Transformer Network called DuAT, which
is characterized by two innovative designs, namely, the Global-to-Local Spatial
Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA
has the ability to aggregate and represent both global and local spatial
features, which are beneficial for locating large and small objects,
respectively. The SBA module is used to aggregate the boundary characteristic
from low-level features and semantic information from high-level features for
better preserving boundary details and locating the re-calibration objects.
Extensive experiments in six benchmark datasets demonstrate that our proposed
model outperforms state-of-the-art methods in the segmentation of skin lesion
images, and polyps in colonoscopy images. In addition, our approach is more
robust than existing methods in various challenging situations such as small
object segmentation and ambiguous object boundaries.",0.11818366,0.111183405,-0.07723134,A
15273,"We further study the effect of AutoAugment (Cubuk et al., 2019).","Pseudo labeling is thus an effective way
for geometry-guided open-world object detector training.","Using it during Phase-II training,
we achieve higher ARN @100 as shown in Figure 5b).",2022-12-22 14:13:33+00:00,GOOD: Exploring Geometric Cues for Detecting Objects in an Open World,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Haiwen Huang'), arxiv.Result.Author('Andreas Geiger'), arxiv.Result.Author('Dan Zhang')]","We address the task of open-world class-agnostic object detection, i.e.,
detecting every object in an image by learning from a limited number of base
object classes. State-of-the-art RGB-based models suffer from overfitting the
training classes and often fail at detecting novel-looking objects. This is
because RGB-based models primarily rely on appearance similarity to detect
novel objects and are also prone to overfitting short-cut cues such as textures
and discriminative parts. To address these shortcomings of RGB-based object
detectors, we propose incorporating geometric cues such as depth and normals,
predicted by general-purpose monocular estimators. Specifically, we use the
geometric cues to train an object proposal network for pseudo-labeling
unannotated novel objects in the training set. Our resulting Geometry-guided
Open-world Object Detector (GOOD) significantly improves detection recall for
novel object categories and already performs well with only a few training
classes. Using a single ""person"" class for training on the COCO dataset, GOOD
surpasses SOTA methods by 5.0% AR@100, a relative improvement of 24%.",-0.30926555,0.03816978,0.00044961087,B
15274,"We further study the effect of AutoAugment (Cubuk et al., 2019).","Pseudo labeling is thus an effective way
for geometry-guided open-world object detector training.","Using it during Phase-II training,
we achieve higher ARN @100 as shown in Figure 5b).",2022-12-22 14:13:33+00:00,GOOD: Exploring Geometric Cues for Detecting Objects in an Open World,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Haiwen Huang'), arxiv.Result.Author('Andreas Geiger'), arxiv.Result.Author('Dan Zhang')]","We address the task of open-world class-agnostic object detection, i.e.,
detecting every object in an image by learning from a limited number of base
object classes. State-of-the-art RGB-based models suffer from overfitting the
training classes and often fail at detecting novel-looking objects. This is
because RGB-based models primarily rely on appearance similarity to detect
novel objects and are also prone to overfitting short-cut cues such as textures
and discriminative parts. To address these shortcomings of RGB-based object
detectors, we propose incorporating geometric cues such as depth and normals,
predicted by general-purpose monocular estimators. Specifically, we use the
geometric cues to train an object proposal network for pseudo-labeling
unannotated novel objects in the training set. Our resulting Geometry-guided
Open-world Object Detector (GOOD) significantly improves detection recall for
novel object categories and already performs well with only a few training
classes. Using a single ""person"" class for training on the COCO dataset, GOOD
surpasses SOTA methods by 5.0% AR@100, a relative improvement of 24%.",-0.30926555,0.03816978,0.00044961087,B
15288,"Though in recent days measures are being taken to
                                        of Jamdani motifs and we believe that our work will open a new         revive this industry from the verge of extinction, the motifs
                                        avenue for further research.","Our experimental results of the pix2pix model on this        income of around BDT 100 only which is very demotivating
                                        dataset show satisfactory outputs of computer-generated images         for them.",which got lost in time can never be restored.,2022-12-22 16:02:44+00:00,Jamdani Motif Generation using Conditional GAN,cs.CV,['cs.CV'],"[arxiv.Result.Author('MD Tanvir Rouf Shawon'), arxiv.Result.Author('Raihan Tanvir'), arxiv.Result.Author('Humaira Ferdous Shifa'), arxiv.Result.Author('Susmoy Kar'), arxiv.Result.Author('Mohammad Imrul Jubair')]","Jamdani is the strikingly patterned textile heritage of Bangladesh. The
exclusive geometric motifs woven on the fabric are the most attractive part of
this craftsmanship having a remarkable influence on textile and fine art. In
this paper, we have developed a technique based on the Generative Adversarial
Network that can learn to generate entirely new Jamdani patterns from a
collection of Jamdani motifs that we assembled, the newly formed motifs can
mimic the appearance of the original designs. Users can input the skeleton of a
desired pattern in terms of rough strokes and our system finalizes the input by
generating the complete motif which follows the geometric structure of real
Jamdani ones. To serve this purpose, we collected and preprocessed a dataset
containing a large number of Jamdani motifs images from authentic sources via
fieldwork and applied a state-of-the-art method called pix2pix to it. To the
best of our knowledge, this dataset is currently the only available dataset of
Jamdani motifs in digital format for computer vision research. Our experimental
results of the pix2pix model on this dataset show satisfactory outputs of
computer-generated images of Jamdani motifs and we believe that our work will
open a new avenue for further research.",0.14572348,0.14188236,-0.029356482,A
15297,"- We provide useful calibration observations as a comprehensive reference for further research on
segmentation model calibration.","We also extend experiments from in-domain to domain-shift data and show that selective
scaling consistently outperforms.","2 RELATED WORK

DNN Calibration.",2022-12-22 22:05:16+00:00,On Calibrating Semantic Segmentation Models: Analysis and An Algorithm,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dongdong Wang'), arxiv.Result.Author('Boqing Gong'), arxiv.Result.Author('Liqiang Wang')]","We study the problem of semantic segmentation calibration. For image
classification, lots of existing solutions are proposed to alleviate model
miscalibration of confidence. However, to date, confidence calibration research
on semantic segmentation is still limited. We provide a systematic study on the
calibration of semantic segmentation models and propose a simple yet effective
approach. First, we find that model capacity, crop size, multi-scale testing,
and prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration, and show
that selective scaling consistently outperforms other methods.",-0.11695525,0.03327754,0.14855614,B
15300,"This primes
future work on this benchmark which is unlike many others and ripe for further research.","The demonstrative baseline predictors indicate that while the learning of these subtasks
together is possible, it is nontrivial to excel at all of the subtasks at the same time.",7.,2022-12-23 04:31:20+00:00,Human Activity Recognition in an Open World,cs.CV,"['cs.CV', 'I.5.4']","[arxiv.Result.Author('Derek S. Prijatelj'), arxiv.Result.Author('Samuel Grieggs'), arxiv.Result.Author('Jin Huang'), arxiv.Result.Author('Dawei Du'), arxiv.Result.Author('Ameya Shringi'), arxiv.Result.Author('Christopher Funk'), arxiv.Result.Author('Adam Kaufman'), arxiv.Result.Author('Eric Robertson'), arxiv.Result.Author('Walter J. Scheirer')]","Managing novelty in perception-based human activity recognition (HAR) is
critical in realistic settings to improve task performance over time and ensure
solution generalization outside of prior seen samples. Novelty manifests in HAR
as unseen samples, activities, objects, environments, and sensor changes, among
other ways. Novelty may be task-relevant, such as a new class or new features,
or task-irrelevant resulting in nuisance novelty, such as never before seen
noise, blur, or distorted video recordings. To perform HAR optimally,
algorithmic solutions must be tolerant to nuisance novelty, and learn over time
in the face of novelty. This paper 1) formalizes the definition of novelty in
HAR building upon the prior definition of novelty in classification tasks, 2)
proposes an incremental open world learning (OWL) protocol and applies it to
the Kinetics datasets to generate a new benchmark KOWL-718, 3) analyzes the
performance of current state-of-the-art HAR models when novelty is introduced
over time, 4) provides a containerized and packaged pipeline for reproducing
the OWL protocol and for modifying for any future updates to Kinetics. The
experimental analysis includes an ablation study of how the different models
perform under various conditions as annotated by Kinetics-AVA. The protocol as
an algorithm for reproducing experiments using the KOWL-718 benchmark will be
publicly released with code and containers at
https://github.com/prijatelj/human-activity-recognition-in-an-open-world. The
code may be used to analyze different annotations and subsets of the Kinetics
datasets in an incremental open world fashion, as well as be extended as
further updates to Kinetics are released.",0.23647812,-0.21620059,-0.061227478,A
15302,"In addition, the released dataset can be used to perform ‚Äòstress‚Äô tests on established de-
                                                                             tection systems and encourages further research toward robust and reliable computer-
                                                                             aided endoscopic image analysis.","Extensive experiments demonstrate a
                                                                             promising false positive suppression in both retrospective and prospective validation.","The dataset and code will be publicly available at
                                                                             http://endoboost.miccai.cloud.",2022-12-23 08:34:36+00:00,EndoBoost: a plug-and-play module for false positive suppression during computer-aided polyp detection in real-world colonoscopy (with dataset),cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Haoran Wang'), arxiv.Result.Author('Yan Zhu'), arxiv.Result.Author('Wenzheng Qin'), arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Pinghong Zhou'), arxiv.Result.Author('Quanlin Li'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Zhijian Song')]","The advance of computer-aided detection systems using deep learning opened a
new scope in endoscopic image analysis. However, the learning-based models
developed on closed datasets are susceptible to unknown anomalies in complex
clinical environments. In particular, the high false positive rate of polyp
detection remains a major challenge in clinical practice. In this work, we
release the FPPD-13 dataset, which provides a taxonomy and real-world cases of
typical false positives during computer-aided polyp detection in real-world
colonoscopy. We further propose a post-hoc module EndoBoost, which can be
plugged into generic polyp detection models to filter out false positive
predictions. This is realized by generative learning of the polyp manifold with
normalizing flows and rejecting false positives through density estimation.
Compared to supervised classification, this anomaly detection paradigm achieves
better data efficiency and robustness in open-world settings. Extensive
experiments demonstrate a promising false positive suppression in both
retrospective and prospective validation. In addition, the released dataset can
be used to perform 'stress' tests on established detection systems and
encourages further research toward robust and reliable computer-aided
endoscopic image analysis. The dataset and code will be publicly available at
http://endoboost.miccai.cloud.",0.03252452,0.17707564,-0.041995108,A
15330,"perspective of weight reinitialization and points out that the
[46, 47, 15, 13, 59, 5, 44] further study the empirical gener-  key premise is the sign of weight values.","[81] studies the lottery ticket hypothesis from the
the model generalization ability in face recognition tasks.",alization ability of hyperspherical learning.,2022-12-24 04:42:15+00:00,Hyperspherical Quantization: Toward Smaller and More Accurate Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dan Liu'), arxiv.Result.Author('Xi Chen'), arxiv.Result.Author('Chen Ma'), arxiv.Result.Author('Xue Liu')]","Model quantization enables the deployment of deep neural networks under
resource-constrained devices. Vector quantization aims at reducing the model
size by indexing model weights with full-precision embeddings, i.e., codewords,
while the index needs to be restored to 32-bit during computation. Binary and
other low-precision quantization methods can reduce the model size up to
32$\times$, however, at the cost of a considerable accuracy drop. In this
paper, we propose an efficient framework for ternary quantization to produce
smaller and more accurate compressed models. By integrating hyperspherical
learning, pruning and reinitialization, our proposed Hyperspherical
Quantization (HQ) method reduces the cosine distance between the full-precision
and ternary weights, thus reducing the bias of the straight-through gradient
estimator during ternary quantization. Compared with existing work at similar
compression levels ($\sim$30$\times$, $\sim$40$\times$), our method
significantly improves the test accuracy and reduces the model size.",0.09503264,-0.19801062,-0.121601775,C
15331,"We further study the impact of different pruning ranges
(r) and step sizes (Œ¥, line 14 of Algorithm 1) on ResNet18.","In Advances in Neural Informa-
                                                                     tion Processing Systems, pages 856‚Äì867, 2017.","[2] Yoshua Bengio, Nicholas Le¬¥onard, and Aaron Courville.",2022-12-24 04:42:15+00:00,Hyperspherical Quantization: Toward Smaller and More Accurate Models,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dan Liu'), arxiv.Result.Author('Xi Chen'), arxiv.Result.Author('Chen Ma'), arxiv.Result.Author('Xue Liu')]","Model quantization enables the deployment of deep neural networks under
resource-constrained devices. Vector quantization aims at reducing the model
size by indexing model weights with full-precision embeddings, i.e., codewords,
while the index needs to be restored to 32-bit during computation. Binary and
other low-precision quantization methods can reduce the model size up to
32$\times$, however, at the cost of a considerable accuracy drop. In this
paper, we propose an efficient framework for ternary quantization to produce
smaller and more accurate compressed models. By integrating hyperspherical
learning, pruning and reinitialization, our proposed Hyperspherical
Quantization (HQ) method reduces the cosine distance between the full-precision
and ternary weights, thus reducing the bias of the straight-through gradient
estimator during ternary quantization. Compared with existing work at similar
compression levels ($\sim$30$\times$, $\sim$40$\times$), our method
significantly improves the test accuracy and reduces the model size.",0.17051657,-0.17807275,0.30082673,A
15342,"Thus, the localization issue in
                                        opment of image editing tools, digital images can be easily                 image forensics needs further study and improvement.","However, with the rapid devel-                image areas were tampered [13].","Com-
                                        tampered and spread on websites.",2022-12-25 02:27:58+00:00,TriPINet: Tripartite Progressive Integration Network for Image Manipulation Localization,cs.CV,['cs.CV'],"[arxiv.Result.Author('Wei-Yun Liang'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Xiao Jin')]","Image manipulation localization aims at distinguishing forged regions from
the whole test image. Although many outstanding prior arts have been proposed
for this task, there are still two issues that need to be further studied: 1)
how to fuse diverse types of features with forgery clues; 2) how to
progressively integrate multistage features for better localization
performance. In this paper, we propose a tripartite progressive integration
network (TriPINet) for end-to-end image manipulation localization. First, we
extract both visual perception information, e.g., RGB input images, and visual
imperceptible features, e.g., frequency and noise traces for forensic feature
learning. Second, we develop a guided cross-modality dual-attention (gCMDA)
module to fuse different types of forged clues. Third, we design a set of
progressive integration squeeze-and-excitation (PI-SE) modules to improve
localization performance by appropriately incorporating multiscale features in
the decoder. Extensive experiments are conducted to compare our method with
state-of-the-art image forensics approaches. The proposed TriPINet obtains
competitive results on several benchmark datasets.",-0.025318772,0.14170626,-0.0665092,C
15357,"On the other                 to solve for these two factors in the black-box setting becomes a
                                        hand, adv-sticker [15] adopts a predeÔ¨Åned meaningful adversarial                  challenging problem for further study.","Thus,
                                        the patch‚Äôs perturbations, and the patch‚Äôs pasting position is Ô¨Åxed               how to design an efÔ¨Åcient simultaneous optimization mechanism
                                        on a location selected based on the prior knowledge.","patch, and uses an evolutionary algorithm to search for a good
                                        patch‚Äôs position to perform the attack, which shows that the patch‚Äôs                  Currently, some works [19], [20], [21] have studied the opti-
                                        position is one of the major parameters on the patch attacks.",2022-12-26 02:48:37+00:00,Simultaneously Optimizing Perturbations and Positions for Black-box Adversarial Patch Attacks,cs.CV,"['cs.CV', 'cs.LG']","[arxiv.Result.Author('Xingxing Wei'), arxiv.Result.Author('Ying Guo'), arxiv.Result.Author('Jie Yu'), arxiv.Result.Author('Bo Zhang')]","Adversarial patch is an important form of real-world adversarial attack that
brings serious risks to the robustness of deep neural networks. Previous
methods generate adversarial patches by either optimizing their perturbation
values while fixing the pasting position or manipulating the position while
fixing the patch's content. This reveals that the positions and perturbations
are both important to the adversarial attack. For that, in this paper, we
propose a novel method to simultaneously optimize the position and perturbation
for an adversarial patch, and thus obtain a high attack success rate in the
black-box setting. Technically, we regard the patch's position, the
pre-designed hyper-parameters to determine the patch's perturbations as the
variables, and utilize the reinforcement learning framework to simultaneously
solve for the optimal solution based on the rewards obtained from the target
model with a small number of queries. Extensive experiments are conducted on
the Face Recognition (FR) task, and results on four representative FR models
show that our method can significantly improve the attack success rate and
query efficiency. Besides, experiments on the commercial FR service and
physical environments confirm its practical application value. We also extend
our method to the traffic sign recognition task to verify its generalization
ability.",0.13236722,0.025258299,0.15052222,A
15389,"To support further research in this area, we have released MVTorch, a PyTorch library for 3D understanding and
                                        generation using multi-view projections.","We also investigate additional aspects of MVTN, such as 2D pretraining and its use
                                        for segmentation.","Index Terms‚ÄîDeep Learning, Multi-view, 3D Point clouds, 3D understanding, 3D shapes, 3D segmentation.",2022-12-27 12:09:16+00:00,MVTN: Learning Multi-View Transformations for 3D Understanding,cs.CV,"['cs.CV', 'cs.GR']","[arxiv.Result.Author('Abdullah Hamdi'), arxiv.Result.Author('Faisal AlZahrani'), arxiv.Result.Author('Silvio Giancola'), arxiv.Result.Author('Bernard Ghanem')]","Multi-view projection techniques have shown themselves to be highly effective
in achieving top-performing results in the recognition of 3D shapes. These
methods involve learning how to combine information from multiple view-points.
However, the camera view-points from which these views are obtained are often
fixed for all shapes. To overcome the static nature of current multi-view
techniques, we propose learning these view-points. Specifically, we introduce
the Multi-View Transformation Network (MVTN), which uses differentiable
rendering to determine optimal view-points for 3D shape recognition. As a
result, MVTN can be trained end-to-end with any multi-view network for 3D shape
classification. We integrate MVTN into a novel adaptive multi-view pipeline
that is capable of rendering both 3D meshes and point clouds. Our approach
demonstrates state-of-the-art performance in 3D classification and shape
retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).
Further analysis indicates that our approach exhibits improved robustness to
occlusion compared to other methods. We also investigate additional aspects of
MVTN, such as 2D pretraining and its use for segmentation. To support further
research in this area, we have released MVTorch, a PyTorch library for 3D
understanding and generation using multi-view projections.",-0.30953038,0.09455833,0.11997664,B
15401,"tuitions and observations, and some of the previous studies
                                                                                                                 could inspire further research.","In particular, we consider two representative Ô¨Åne-                  seek to explain the differences between non-deep learning
                                      grained visual parsing tasks in this paper, i.e., semantic part            and deep models, since these works often share similar in-
                                      segmentation and Ô¨Åne-grained object recognition.","For consolidating these recent
                                          In contrast with coarse-grained object segmentation and                advances, we propose a new taxonomy for Ô¨Åne-grained
                                      base-level classiÔ¨Åcation, Ô¨Åne-grained parsing is meant to                  part segmentation and recognition tasks, and also provide
                                      segment or distinguish visually similar objects that belong                a collection of predominant benchmark datasets following
                                      to different Ô¨Åne-grained concepts, for example, decompos-                  our taxonomy.",2022-12-28 04:20:10+00:00,Parsing Objects at a Finer Granularity: A Survey,cs.CV,['cs.CV'],"[arxiv.Result.Author('Yifan Zhao'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Yonghong Tian')]","Fine-grained visual parsing, including fine-grained part segmentation and
fine-grained object recognition, has attracted considerable critical attention
due to its importance in many real-world applications, e.g., agriculture,
remote sensing, and space technologies. Predominant research efforts tackle
these fine-grained sub-tasks following different paradigms, while the inherent
relations between these tasks are neglected. Moreover, given most of the
research remains fragmented, we conduct an in-depth study of the advanced work
from a new perspective of learning the part relationship. In this perspective,
we first consolidate recent research and benchmark syntheses with new
taxonomies. Based on this consolidation, we revisit the universal challenges in
fine-grained part segmentation and recognition tasks and propose new solutions
by part relationship learning for these important challenges. Furthermore, we
conclude several promising lines of research in fine-grained visual parsing for
future research.",-0.32443735,-0.22856942,-0.027328592,C
15406,"These issues deserve further study and
                                                                              consideration based on this work.","However, the use of a                        evidence that these inferred images can be used to solve
                                                                              clinical problems.","DECLARATION OF COMPETING INTEREST

                                                                                 The authors declare that there is no conÔ¨Çict of interests.",2022-12-28 12:53:44+00:00,Swin MAE: Masked Autoencoders for Small Datasets,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author(""Zi'an Xu""), arxiv.Result.Author('Yin Dai'), arxiv.Result.Author('Fayu Liu'), arxiv.Result.Author('Weibing Chen'), arxiv.Result.Author('Yue Liu'), arxiv.Result.Author('Lifu Shi'), arxiv.Result.Author('Sheng Liu'), arxiv.Result.Author('Yuhang Zhou')]","The development of deep learning models in medical image analysis is majorly
limited by the lack of large-sized and well-annotated datasets. Unsupervised
learning does not require labels and is more suitable for solving medical image
analysis problems. However, most of the current unsupervised learning methods
need to be applied to large datasets. To make unsupervised learning applicable
to small datasets, we proposed Swin MAE, which is a masked autoencoder with
Swin Transformer as its backbone. Even on a dataset of only a few thousand
medical images and without using any pre-trained models, Swin MAE is still able
to learn useful semantic features purely from images. It can equal or even
slightly outperform the supervised model obtained by Swin Transformer trained
on ImageNet in terms of the transfer learning results of downstream tasks. The
code will be publicly available soon.",0.050681435,0.07000363,-0.25732046,C
15410,"In order to further research in this area, we release the
                                                                      dataset created for the single damaged building blend ex-
Figure 5.","No secondary pre-event images for samples
with minor damage were available.",Example of a single post-event building blended into        periment.,2022-12-23 21:01:18+00:00,xFBD: Focused Building Damage Dataset and Analysis,cs.CV,['cs.CV'],"[arxiv.Result.Author('Dennis Melamed'), arxiv.Result.Author('Cameron Johnson'), arxiv.Result.Author('Chen Zhao'), arxiv.Result.Author('Russell Blue'), arxiv.Result.Author('Philip Morrone'), arxiv.Result.Author('Anthony Hoogs'), arxiv.Result.Author('Brian Clipp')]","The xView2 competition and xBD dataset spurred significant advancements in
overhead building damage detection, but the competition's pixel level scoring
can lead to reduced solution performance in areas with tight clusters of
buildings or uninformative context. We seek to advance automatic building
damage assessment for disaster relief by proposing an auxiliary challenge to
the original xView2 competition. This new challenge involves a new dataset and
metrics indicating solution performance when damage is more local and limited
than in xBD. Our challenge measures a network's ability to identify individual
buildings and their damage level without excessive reliance on the buildings'
surroundings. Methods that succeed on this challenge will provide more
fine-grained, precise damage information than original xView2 solutions. The
best-performing xView2 networks' performances dropped noticeably in our new
limited/local damage detection task. The common causes of failure observed are
that (1) building objects and their classifications are not separated well, and
(2) when they are, the classification is strongly biased by surrounding
buildings and other damage context. Thus, we release our augmented version of
the dataset with additional object-level scoring metrics
https://gitlab.kitware.com/dennis.melamed/xfbd to test independence and
separability of building objects, alongside the pixel-level performance metrics
of the original competition. We also experiment with new baseline models which
improve independence and separability of building damage predictions. Our
results indicate that building damage detection is not a fully-solved problem,
and we invite others to use and build on our dataset augmentations and metrics.",-0.009490317,0.21322706,-0.11525674,B
