Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
2217,"To
facilitate further research along this front we make our study data available
at https://gitlab.fri.uni-lj.si/lrk/approximate_video_study/.","Finally, our
work indicates that aside from the viewer‚Äôs physical activity, the content of the
video and the viewer‚Äôs personality, there are also other dimensions that impact
the quality requirements and which must be further explored in order to enable
accurate prediction of the appropriate quality settings for video playback.","References

1.",2022-02-20 10:03:18+00:00,Context-aware adaptation of mobile video decoding resolution,cs.MM,"['cs.MM', 'cs.HC']","[arxiv.Result.Author('Octavian Machidon'), arxiv.Result.Author('Jani Asprov'), arxiv.Result.Author('Tine Fajfar'), arxiv.Result.Author('Veljko Pejovic')]","While the evolution of mobile computing is experiencing a considerable
growth, it is at the same time seriously threatened by the limitations of the
battery technology, which does not keep pace with the evergrowing increase in
energy requirements of mobile applications. A novel approach for reducing the
energy appetite of mobile apps comes from the approximate computing field,
which proposes techniques that in a controlled manner sacrifice computation
accuracy for higher energy savings. Building on this philosophy we propose a
context-aware mobile video quality adaptation that reduces the energy needed
for video playback, while ensuring that a user's quality expectations with
respect to the mobile video are met. We confirm that the decoding resolution
can play a significant role in reducing the overall power consumption of a
mobile device and conduct two user studies to investigate how the context in
which a video is played, its content, and the user's personality, modulate a
user's quality expectations. We discover that a user's physical activity, the
spatial/temporal properties of the video, and the user's personality traits
interact and jointly influence the minimal acceptable playback resolution,
paving the way for context-adaptable approximate mobile computing.",0.37301016,-0.14371943,0.055091687,A
4475,"timodality is, therefore, a suitable avenue of research by
On the other, the audio contains information about the musical     which to further study and exploit the synergies between these
performance, but certain aspects are difÔ¨Åcult to retrieve owing    individual recognition schemes.","Mul-
domain that must be decoded in order to interpret it as music.","to human interpretation or certain unavoidable ambiguities
(such as the clef or the meter).",2022-04-06 20:00:33+00:00,Late multimodal fusion for image and audio music transcription,cs.MM,"['cs.MM', 'cs.CV', 'cs.IR', 'cs.SD', 'eess.AS', 'H.3; H.4; I.4; I.5; J.6']","[arxiv.Result.Author('Mar√≠a Alfaro-Contreras'), arxiv.Result.Author('Jose J. Valero-Mas'), arxiv.Result.Author('Jos√© M. I√±esta'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.",-0.4213348,-0.47151336,-0.28325352,C_centroid
4476,"This         In this work, we aim to further research the aforementioned
                                                                   avenue of multimodal OMR-AMT transcription.","introduced is usually considered together with the Connec-
tionist Temporal ClassiÔ¨Åcation (CTC) loss function [14].","We assume
                                                                   that the score image and the recording of the same piece
JOURNAL OF LATEX CLASS FILES, VOL.",2022-04-06 20:00:33+00:00,Late multimodal fusion for image and audio music transcription,cs.MM,"['cs.MM', 'cs.CV', 'cs.IR', 'cs.SD', 'eess.AS', 'H.3; H.4; I.4; I.5; J.6']","[arxiv.Result.Author('Mar√≠a Alfaro-Contreras'), arxiv.Result.Author('Jose J. Valero-Mas'), arxiv.Result.Author('Jos√© M. I√±esta'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.",-0.40269232,0.5909405,-0.20191705,B_centroid
4477,"timodality is, therefore, a suitable avenue of research by
On the other, the audio contains information about the musical     which to further study and exploit the synergies between these
performance, but certain aspects are difÔ¨Åcult to retrieve owing    individual recognition schemes.","Mul-
domain that must be decoded in order to interpret it as music.","to human interpretation or certain unavoidable ambiguities
(such as the clef or the meter).",2022-04-06 20:00:33+00:00,Late multimodal fusion for image and audio music transcription,cs.MM,"['cs.MM', 'cs.CV', 'cs.IR', 'cs.SD', 'eess.AS', 'H.3; H.4; I.4; I.5; J.6']","[arxiv.Result.Author('Mar√≠a Alfaro-Contreras'), arxiv.Result.Author('Jose J. Valero-Mas'), arxiv.Result.Author('Jos√© M. I√±esta'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.",-0.4213348,-0.47151336,-0.28325352,C
4478,"This         In this work, we aim to further research the aforementioned
                                                                   avenue of multimodal OMR-AMT transcription.","introduced is usually considered together with the Connec-
tionist Temporal ClassiÔ¨Åcation (CTC) loss function [14].","We assume
                                                                   that the score image and the recording of the same piece
JOURNAL OF LATEX CLASS FILES, VOL.",2022-04-06 20:00:33+00:00,Late multimodal fusion for image and audio music transcription,cs.MM,"['cs.MM', 'cs.CV', 'cs.IR', 'cs.SD', 'eess.AS', 'H.3; H.4; I.4; I.5; J.6']","[arxiv.Result.Author('Mar√≠a Alfaro-Contreras'), arxiv.Result.Author('Jose J. Valero-Mas'), arxiv.Result.Author('Jos√© M. I√±esta'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.",-0.40269232,0.5909405,-0.20191705,B
4479,"timodality is, therefore, a suitable avenue of research by
On the other, the audio contains information about the musical     which to further study and exploit the synergies between these
performance, but certain aspects are difÔ¨Åcult to retrieve owing    individual recognition schemes.","Mul-
domain that must be decoded in order to interpret it as music.","to human interpretation or certain unavoidable ambiguities
(such as the clef or the meter).",2022-04-06 20:00:33+00:00,Late multimodal fusion for image and audio music transcription,cs.MM,"['cs.MM', 'cs.CV', 'cs.IR', 'cs.SD', 'eess.AS', 'H.3; H.4; I.4; I.5; J.6']","[arxiv.Result.Author('Mar√≠a Alfaro-Contreras'), arxiv.Result.Author('Jose J. Valero-Mas'), arxiv.Result.Author('Jos√© M. I√±esta'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.",-0.4213348,-0.47151336,-0.28325352,C
4480,"This         In this work, we aim to further research the aforementioned
                                                                   avenue of multimodal OMR-AMT transcription.","introduced is usually considered together with the Connec-
tionist Temporal ClassiÔ¨Åcation (CTC) loss function [14].","We assume
                                                                   that the score image and the recording of the same piece
JOURNAL OF LATEX CLASS FILES, VOL.",2022-04-06 20:00:33+00:00,Late multimodal fusion for image and audio music transcription,cs.MM,"['cs.MM', 'cs.CV', 'cs.IR', 'cs.SD', 'eess.AS', 'H.3; H.4; I.4; I.5; J.6']","[arxiv.Result.Author('Mar√≠a Alfaro-Contreras'), arxiv.Result.Author('Jose J. Valero-Mas'), arxiv.Result.Author('Jos√© M. I√±esta'), arxiv.Result.Author('Jorge Calvo-Zaragoza')]","Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.",-0.40269232,0.5909405,-0.20191705,B
6005,"However, further study is required
icant differences (ùúí2 = 19, ùëù < 0.001).","nificant research attention for omnidirectional videos [5, 40, 56]
Friedman‚Äôs test reveals that there are groups with statistically signif-                                                                                                                       and point clouds [12, 30, 39, 41].",The results of the Wilcoxon                                                                                                                            for live real-time human point cloud reconstructions.,2022-05-10 13:56:41+00:00,Evaluating the Impact of Tiled User-Adaptive Real-Time Point Cloud Streaming on VR Remote Communication,cs.MM,['cs.MM'],"[arxiv.Result.Author('Shishir Subramanyam'), arxiv.Result.Author('Irene Viola'), arxiv.Result.Author('Jack Jansen'), arxiv.Result.Author('Evangelos Alexiou'), arxiv.Result.Author('Alan Hanjalic'), arxiv.Result.Author('Pablo Cesar')]","Remote communication has rapidly become a part of everyday life in both
professional and personal contexts. However, popular video conferencing
applications present limitations in terms of quality of communication,
immersion and social meaning. VR remote communication applications offer a
greater sense of co-presence and mutual sensing of emotions between remote
users. Previous research on these applications has shown that realistic point
cloud user reconstructions offer better immersion and communication as compared
to synthetic user avatars. However, photorealistic point clouds require a large
volume of data per frame and are challenging to transmit over bandwidth-limited
networks. Recent research has demonstrated significant improvements to
perceived quality by optimizing the usage of bandwidth based on the position
and orientation of the user's viewport with user-adaptive streaming. In this
work, we developed a real-time VR communication application with an adaptation
engine that features tiled user-adaptive streaming based on user behaviour. The
application also supports traditional network adaptive streaming. The
contribution of this work is to evaluate the impact of tiled user-adaptive
streaming on quality of communication, visual quality, system performance and
task completion in a functional live VR remote communication system. We perform
a subjective evaluation with 33 users to compare the different streaming
conditions with a neck exercise training task. As a baseline, we use
uncompressed streaming requiring ca. 300Mbps and our solution achieves similar
visual quality with tiled adaptive streaming at 14Mbps. We also demonstrate
statistically significant gains to the quality of interaction and improvements
to system performance and CPU consumption with tiled adaptive streaming as
compared to the more traditional network adaptive streaming.",0.340571,0.20376343,0.06825863,A
7644,"Specifically,        sider this paper an invitation for further study and discus-
in example b) we list a set of music tracks with non-English      sion of the interplay between culture and bias in the con-
vocals, and we use them to retrieve music that is similar in      text of artistic correspondence learning and the challenges
style to a query input, but in a non-English language.",We can also condition on language.,Inter-     it presents.,2022-06-14 20:21:04+00:00,It's Time for Artistic Correspondence in Music and Video,cs.MM,"['cs.MM', 'cs.CV']","[arxiv.Result.Author('Didac Suris'), arxiv.Result.Author('Carl Vondrick'), arxiv.Result.Author('Bryan Russell'), arxiv.Result.Author('Justin Salamon')]","We present an approach for recommending a music track for a given video, and
vice versa, based on both their temporal alignment and their correspondence at
an artistic level. We propose a self-supervised approach that learns this
correspondence directly from data, without any need of human annotations. In
order to capture the high-level concepts that are required to solve the task,
we propose modeling the long-term temporal context of both the video and the
music signals, using Transformer networks for each modality. Experiments show
that this approach strongly outperforms alternatives that do not exploit the
temporal context. The combination of our contributions improve retrieval
accuracy up to 10x over prior state of the art. This strong improvement allows
us to introduce a wide range of analyses and applications. For instance, we can
condition music retrieval based on visually defined attributes.",-0.30183432,-0.2779736,0.10095778,C
9336,"However given one of the strengths of our approach is fully-automated
shaping of the visualisation without extra efforts by a human, other more automated ways to draw the data
will be an important topic of further study.","A manual indication of the important parts by the lecturer at
the time of uploading each video may be a simple solution that could also serve as a way to lead a more
desirable evolution of the contour.","For example, some studies used supervised machine learning

Frontiers   13
Lee et al.",2022-07-26 13:53:15+00:00,Playback-centric visualisations of video usage using weighted interactions to guide where to watch in an educational context,cs.MM,"['cs.MM', 'cs.HC']","[arxiv.Result.Author('Hyowon Lee'), arxiv.Result.Author('Mingming Liu'), arxiv.Result.Author('Michael Scriney'), arxiv.Result.Author('Alan F. Smeaton')]","The increase in use of online educational tools has led to a large amount of
educational video materials made available for students. Finding the right
video content is usually supported by the overarching learning management
system and its interface that organises video items by course, categories and
weeks, and makes them searchable. However, once a video is found, students are
left without further guidance as to what parts in that video they should focus
on. In this article, an additional timeline visualisation to augment the
conventional playback timeline is introduced which employs a novel playback
weighting strategy in which the history of different video interactions
generate scores based on the context of each playback. The resultant scores are
presented on the additional timeline, making it in effect a playback-centric
usage graph nuanced by how each playback was executed. Students can selectively
watch those portions which the contour of the usage visualisation suggests. The
visualisation was implemented and deployed in an undergraduate course at a
university for two full semesters. 270 students used the system throughout both
semesters watching 52 videos, guided by visualisations on what to watch.
Analysis of playback logs revealed students selectively watched corresponding
to the most important portions of the videos as assessed by the instructor who
created the videos. The characteristics of this as a way of guiding students as
to where to watch as well as a complementary tool for playback analysis, are
discussed. Further insights into the potential values of this visualisation and
its underlying playback weighting strategy are also discussed.",0.19149786,0.13482606,0.15939285,A
9793,(9) of the music and the video is a further research area of our work.,"Finally, we calculate the user preference
embedding of batch-level average zu as Eg ¬∑ u‚ààUB uP(u) with
P(u) deÔ¨Åned as 1/|UB| and UB denotes the set of uploaders in
a Batch during training, and then we concatenate it to music

embedding zm to gain deconfounded music embedding zm as:

                zm = zm||zu.","6
Algorithm 1: TeacherN-SGD: Training TeacherN with               posed DebCM, we investigate how do the two components
SGD.",2022-08-07 04:00:43+00:00,Debiased Cross-modal Matching for Content-based Micro-video Background Music Recommendation,cs.MM,"['cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Jinng Yi'), arxiv.Result.Author('Zhenzhong Chen')]","Micro-video background music recommendation is a complicated task where the
matching degree between videos and uploader-selected background music is a
major issue. However, the selection of the user-generated content (UGC) is
biased caused by knowledge limitations and historical preferences among music
of each uploader. In this paper, we propose a Debiased Cross-Modal (DebCM)
matching model to alleviate the influence of such selection bias. Specifically,
we design a teacher-student network to utilize the matching of segments of
music videos, which is professional-generated content (PGC) with specialized
music-matching techniques, to better alleviate the bias caused by insufficient
knowledge of users. The PGC data is captured by a teacher network to guide the
matching of uploader-selected UGC data of the student network by KL-based
knowledge transfer. In addition, uploaders' personal preferences of music
genres are identified as confounders that spuriously correlate music embeddings
and background music selections, resulting in the learned recommender system to
over-recommend music from the majority groups. To resolve such confounders in
the UGC data of the student network, backdoor adjustment is utilized to
deconfound the spurious correlation between music embeddings and prediction
scores. We further utilize Monte Carlo (MC) estimator with batch-level average
as the approximations to avoid integrating the entire confounder space
calculated by the adjustment. Extensive experiments on the TT-150k-genre
dataset demonstrate the effectiveness of the proposed method towards the
selection bias. The code is publicly available on:
\url{https://github.com/jing-1/DebCM}.",-0.17006023,-0.16855581,0.04351279,C
10795,"While recently works are focusing on trying to better
          understand the basic constructs on social VR [89,90], further research is needed for
          considering the volumetric video case.","Future research is needed for the
provision of adequate interaction mechanisms and the seamless inclusion of interactive
14 CHAPTER 15 Volumetric video streaming

          content in the experiences.","Finally, more datasets centered in interactive
          activities are missing [36].",2022-09-05 14:29:56+00:00,Volumetric video streaming: Current approaches and implementations,cs.MM,"['cs.MM', 'eess.IV']","[arxiv.Result.Author('Irene Viola'), arxiv.Result.Author('Pablo Cesar')]","The rise of capturing systems for objects and scenes in 3D with increased
fidelity and immersion has led to the popularity of volumetric video contents
that can be seen from any position and angle in 6 degrees of freedom
navigation. Such contents need large volumes of data to accurately represent
the real world. Thus, novel optimization solutions and delivery systems are
needed to enable volumetric video streaming over bandwidth-limited networks. In
this chapter, we discuss theoretical approaches to volumetric video streaming
optimization, through compression solutions, as well as network and user
adaptation, for high-end and low-powered devices. Moreover, we present an
overview of existing end-to-end systems, and we point to the future of
volumetric video streaming.",0.624587,-0.15152988,-0.11935222,A_centroid
10856,"Our evaluations demonstrate that both CH-SIMS v2.0 and AV-MC framework enable further research for discovering
                                        emotion-bearing acoustic and visual cues and pave the path to interpretable end-to-end HCI applications for real-world scenarios.","Through drawing
                                        unobserved multimodal context along with the text, the model can learn to be aware of different non-verbal contexts for sentiment
                                        prediction.","The
                                        full dataset and code are available for use at https://github.com/thuiar/ch-sims-v2.",2022-08-22 03:31:33+00:00,Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module,cs.MM,"['cs.MM', 'cs.AI', 'cs.CV', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Yihe Liu'), arxiv.Result.Author('Ziqi Yuan'), arxiv.Result.Author('Huisheng Mao'), arxiv.Result.Author('Zhiyun Liang'), arxiv.Result.Author('Wanqiuyue Yang'), arxiv.Result.Author('Yuanzhe Qiu'), arxiv.Result.Author('Tie Cheng'), arxiv.Result.Author('Xiaoteng Li'), arxiv.Result.Author('Hua Xu'), arxiv.Result.Author('Kai Gao')]","Multimodal sentiment analysis (MSA), which supposes to improve text-based
sentiment analysis with associated acoustic and visual modalities, is an
emerging research area due to its potential applications in Human-Computer
Interaction (HCI). However, the existing researches observe that the acoustic
and visual modalities contribute much less than the textual modality, termed as
text-predominant. Under such circumstances, in this work, we emphasize making
non-verbal cues matter for the MSA task. Firstly, from the resource
perspective, we present the CH-SIMS v2.0 dataset, an extension and enhancement
of the CH-SIMS. Compared with the original dataset, the CH-SIMS v2.0 doubles
its size with another 2121 refined video segments with both unimodal and
multimodal annotations and collects 10161 unlabelled raw video segments with
rich acoustic and visual emotion-bearing context to highlight non-verbal cues
for sentiment prediction. Secondly, from the model perspective, benefiting from
the unimodal annotations and the unsupervised data in the CH-SIMS v2.0, the
Acoustic Visual Mixup Consistent (AV-MC) framework is proposed. The designed
modality mixup module can be regarded as an augmentation, which mixes the
acoustic and visual modalities from different videos. Through drawing
unobserved multimodal context along with the text, the model can learn to be
aware of different non-verbal contexts for sentiment prediction. Our
evaluations demonstrate that both CH-SIMS v2.0 and AV-MC framework enables
further research for discovering emotion-bearing acoustic and visual cues and
paves the path to interpretable end-to-end HCI applications for real-world
scenarios.",-0.046843052,0.0038566068,0.15967572,C
10904,"This task will include
the (I) further study and evaluation of the presented approach in a             document model for automated document composition.","Probabilistic
proach in the creative commercial scenario.","In Proceedings
professional scenario, particularly focusing on the exploratory and
co-creativity value of Typesetter module as a tool; (II) further exper-         of the 11th ACM Symposium on Document Engineering, pages 3‚Äì12,
imentation with digital-based media and formats in order to study
the possibility of use this approach to highlight some contents on              New York, September, 2011 2011.",2022-09-07 17:51:21+00:00,ESSYS* Sharing #UC: An Emotion-driven Audiovisual Installation,cs.MM,"['cs.MM', 'cs.HC', 'cs.IR', 'cs.SD', 'eess.AS', 'H.4.m; H.5.1; H.5.5']","[arxiv.Result.Author('S√©rgio M. Rebelo'), arxiv.Result.Author('Mariana Sei√ßa'), arxiv.Result.Author('Pedro Martins'), arxiv.Result.Author('Jo√£o Bicker'), arxiv.Result.Author('Penousal Machado')]","We present ESSYS* Sharing #UC, an audiovisual installation artwork that
reflects upon the emotional context related to the university and the city of
Coimbra, based on the data shared about them on Twitter. The installation was
presented in an urban art gallery of C\'irculo de Artes Pl\'asticas de Coimbra
during the summer and autumn of 2021. In the installation space, one may see a
collection of typographic posters displaying the tweets and listening to an
ever-changing ambient sound. The present audiovisuals are created by an
autonomous computational creative approach, which employs a neural classifier
to recognize the emotional context of a tweet and uses this resulting data as
feedstock for the audiovisual generation. The installation's space is designed
to promote an approach and blend between the online and physical perceptions of
the same location. We applied multiple experiments with the proposed approach
to evaluate the capability and performance. Also, we conduct interview-based
evaluation sessions to understand how the installation elements, especially
poster designs, are experienced by people regarding diversity, expressiveness
and possible employment in other commercial and social scenarios.",-0.091378585,0.09454708,0.4967246,B
11113,A promising solution is to adopt pervasive       which requires further research and state-of-the-art solutions.,"Although
limited resources in MEC networks, computing the AI train-        the use of edge computing in drone-based video applications
ing/inferences in MEC devices may be infeasible, particularly     can bring signiÔ¨Åcant improvement in latency and ofÔ¨Çoading
when the task requires high computational load, e.g., Deep        intensive computation tasks, live video analytics is still the area
Neural Networks.","computing, where different data storages and processing ca-       In particular these issues need further attention.",2022-09-13 06:53:32+00:00,A Survey on Mobile Edge Computing for Video Streaming: Opportunities and Challenges,cs.MM,"['cs.MM', 'cs.NI']","[arxiv.Result.Author('Muhammad Asif Khan'), arxiv.Result.Author('Emna Baccour'), arxiv.Result.Author('Zina Chkirbene'), arxiv.Result.Author('Aiman Erbad'), arxiv.Result.Author('Ridha Hamila'), arxiv.Result.Author('Mounir Hamdi'), arxiv.Result.Author('Moncef Gabbouj')]","5G communication brings substantial improvements in the quality of service
provided to various applications by achieving higher throughput and lower
latency. However, interactive multimedia applications (e.g., ultra high
definition video conferencing, 3D and multiview video streaming, crowd-sourced
video streaming, cloud gaming, virtual and augmented reality) are becoming more
ambitious with high volume and low latency video streams putting strict demands
on the already congested networks. Mobile Edge Computing (MEC) is an emerging
paradigm that extends cloud computing capabilities to the edge of the network
i.e., at the base station level. To meet the latency requirements and avoid the
end-to-end communication with remote cloud data centers, MEC allows to store
and process video content (e.g., caching, transcoding, pre-processing) at the
base stations. Both video on demand and live video streaming can utilize MEC to
improve existing services and develop novel use cases, such as video analytics,
and targeted advertisements. MEC is expected to reshape the future of video
streaming by providing ultra-reliable and low latency streaming (e.g., in
augmented reality, virtual reality, and autonomous vehicles), pervasive
computing (e.g., in real-time video analytics), and blockchain-enabled
architecture for secure live streaming. This paper presents a comprehensive
survey of recent developments in MEC-enabled video streaming bringing
unprecedented improvement to enable novel use cases. A detailed review of the
state-of-the-art is presented covering novel caching schemes, optimal
computation offloading, cooperative caching and offloading and the use of
artificial intelligence (i.e., machine learning, deep learning, and
reinforcement learning) in MEC-assisted video streaming services.",0.29110295,0.22612655,-0.004790779,A
11301,"Indeed, we can make use of the
interesting topic for further research.","In this paper we address both these problems by proposing
Results conÔ¨Årm that multimodal deepfake analysis should           a pipeline to generate multimodal deepfake datasets starting
be preferred and show that audio deepfake attribution is an       from video deepfake ones.","good quality video datasets proposed in the literature and
                                                                  automatize the generation of realistic speech tracks to be
   The rest of paper is structured as follows.",2022-09-16 15:27:35+00:00,TIMIT-TTS: a Text-to-Speech Dataset for Multimodal Synthetic Media Detection,cs.MM,['cs.MM'],"[arxiv.Result.Author('Davide Salvi'), arxiv.Result.Author('Brian Hosler'), arxiv.Result.Author('Paolo Bestagini'), arxiv.Result.Author('Matthew C. Stamm'), arxiv.Result.Author('Stefano Tubaro')]","With the rapid development of deep learning techniques, the generation and
counterfeiting of multimedia material are becoming increasingly straightforward
to perform. At the same time, sharing fake content on the web has become so
simple that malicious users can create unpleasant situations with minimal
effort. Also, forged media are getting more and more complex, with manipulated
videos that are taking the scene over still images. The multimedia forensic
community has addressed the possible threats that this situation could imply by
developing detectors that verify the authenticity of multimedia objects.
However, the vast majority of these tools only analyze one modality at a time.
This was not a problem as long as still images were considered the most widely
edited media, but now, since manipulated videos are becoming customary,
performing monomodal analyses could be reductive. Nonetheless, there is a lack
in the literature regarding multimodal detectors, mainly due to the scarsity of
datasets containing forged multimodal data to train and test the designed
algorithms. In this paper we focus on the generation of an audio-visual
deepfake dataset. First, we present a general pipeline for synthesizing speech
deepfake content from a given real or fake video, facilitating the creation of
counterfeit multimodal material. The proposed method uses Text-to-Speech (TTS)
and Dynamic Time Warping techniques to achieve realistic speech tracks. Then,
we use the pipeline to generate and release TIMIT-TTS, a synthetic speech
dataset containing the most cutting-edge methods in the TTS field. This can be
used as a standalone audio dataset, or combined with other state-of-the-art
sets to perform multimodal research. Finally, we present numerous experiments
to benchmark the proposed dataset in both mono and multimodal conditions,
showing the need for multimodal forensic detectors and more suitable data.",-0.17090929,0.023972712,0.107971914,C
11922,"In addition, that study does not only reflect on the potential
of Social VR for the educational sector but also highlights the need for further research on
determining and exploiting its benefits.","That study suggests that even though Social VR
systems reduce usability (i.e., they are less easy to use, probably due to lack of familiarity), they
increase presence when compared to traditional videoconferencing systems (even when using 2D
screens and avatar-based representations), also concluding that both types of tools seem to be
effective for conducting online lessons.","The study in [Yos21] explored student experiences for remote instruction (University level)
using Social VR, in particular Mozilla Hubs platform, with students attending classes from home
for 7 weeks (Figure 1).",2022-10-01 17:55:43+00:00,"Social VR and multi-party holographic communications: Opportunities, Challenges and Impact in the Education and Training Sectors",cs.MM,['cs.MM'],"[arxiv.Result.Author('Mario Montagud'), arxiv.Result.Author('Gianluca Cernigliaro'), arxiv.Result.Author('Miguel Arevalillo-Herr√°ez'), arxiv.Result.Author('Miguel Garc√≠a-Pineda'), arxiv.Result.Author('Jaume Segura-Garcia'), arxiv.Result.Author('Sergi Fern√°ndez')]","Technological advances can bring many benefits to our daily lives, and this
includes the education and training sectors. In the last years, online
education, teaching and training models are becoming increasingly adopted, in
part influenced by major circumstances like the pandemic. The use of
videoconferencing tools in such sectors has become fundamental, but recent
research has shown their multiple limitations in terms of relevant aspects,
like comfort, interaction quality, situational awareness, (co-)presence, etc.
This study elaborates on a new communication, interaction and collaboration
medium that becomes a promising candidate to overcome such limitations, by
adopting immersive technologies: Social Virtual Reality (VR). First, this
article provides a comprehensive review of studies having provided initial
evidence on (potential) benefits provided by Social VR in relevant use cases
related to education, such as online classes, training and co-design
activities, virtual conferences and interactive visits to virtual spaces, many
of them including comparisons with classical tools like 2D conferencing.
Likewise, the potential benefits of integrating realistic and volumetric users'
representations to enable multi-party holographic communications in Social VR
is also discussed. Next, this article identifies and elaborates on key
limitations of existing studies in this field, including both technological and
methodological aspects. Finally, it discusses key remaining challenges to be
addressed to fully exploit the potential of Social VR in the education sector.",0.7030607,-0.047147516,-0.12409684,A
11923,"First, although various studies have put efforts into comparing 2D desktop and 3D VR viewing
conditions in Social VR, conclusive results are not yet available and further research is needed.","Limited scope and test conditions
Some limitations regarding the test conditions and scope of the state-of-the-art experiments/studies
can also be identified, without meaning to be exhaustive.","In
this line, reported experiences of using VR headsets and controllers for attending virtual
classes/meetings or giving presentations are still very scarce, especially when it comes to
distributed settings in open environments (e.g., participants from their homes) [Rad20].",2022-10-01 17:55:43+00:00,"Social VR and multi-party holographic communications: Opportunities, Challenges and Impact in the Education and Training Sectors",cs.MM,['cs.MM'],"[arxiv.Result.Author('Mario Montagud'), arxiv.Result.Author('Gianluca Cernigliaro'), arxiv.Result.Author('Miguel Arevalillo-Herr√°ez'), arxiv.Result.Author('Miguel Garc√≠a-Pineda'), arxiv.Result.Author('Jaume Segura-Garcia'), arxiv.Result.Author('Sergi Fern√°ndez')]","Technological advances can bring many benefits to our daily lives, and this
includes the education and training sectors. In the last years, online
education, teaching and training models are becoming increasingly adopted, in
part influenced by major circumstances like the pandemic. The use of
videoconferencing tools in such sectors has become fundamental, but recent
research has shown their multiple limitations in terms of relevant aspects,
like comfort, interaction quality, situational awareness, (co-)presence, etc.
This study elaborates on a new communication, interaction and collaboration
medium that becomes a promising candidate to overcome such limitations, by
adopting immersive technologies: Social Virtual Reality (VR). First, this
article provides a comprehensive review of studies having provided initial
evidence on (potential) benefits provided by Social VR in relevant use cases
related to education, such as online classes, training and co-design
activities, virtual conferences and interactive visits to virtual spaces, many
of them including comparisons with classical tools like 2D conferencing.
Likewise, the potential benefits of integrating realistic and volumetric users'
representations to enable multi-party holographic communications in Social VR
is also discussed. Next, this article identifies and elaborates on key
limitations of existing studies in this field, including both technological and
methodological aspects. Finally, it discusses key remaining challenges to be
addressed to fully exploit the potential of Social VR in the education sector.",0.69149023,-0.15656811,-0.2641458,A
12226,"Although this can provide high-deÔ¨Ånition images
                                        oblique photography model, we further study the UAV placement        for users, the transmission rate will be very low when the
                                        problem in the cellular-connected UAV-assisted image acquisition     GT is far away from the BS.",Based on the proposed      on it [6].,"Even worse, once the distance
                                        system, which aims at minimizing the data transmission delay         between the GT and the BS is greater than a certain threshold,
                                        under the condition of satisfying the predetermined image            the communication between the UAV and the BS will be
                                        resolution requirement.",2022-10-10 13:27:19+00:00,UAV Placement for Real-time Video Acquisition: A Tradeoff between Resolution and Delay,cs.MM,['cs.MM'],"[arxiv.Result.Author('Tang Xiao-Wei'), arxiv.Result.Author('Huang Xin-Lin Huang')]","Recently, UAVs endowed with high mobility, low cost, and remote control have
promoted the development of UAV-assisted real-time video/image acquisition
applications, which have a high demand for both transmission rate and image
resolution. However, in conventional vertical photography model, the UAV should
fly to the top of ground targets (GTs) to capture images, thus enlarge the
transmission delay. In this paper, we propose an oblique photography model,
which allows the UAV to capture images of GTs from a far distance while still
satisfying the predetermined resolution requirement. Based on the proposed
oblique photography model, we further study the UAV placement problem in the
cellular-connected UAV-assisted image acquisition system, which aims at
minimizing the data transmission delay under the condition of satisfying the
predetermined image resolution requirement. Firstly, the proposed scheme is
first formulated as an intractable non-convex optimization problem. Then, the
original problem is simplified to obtain a tractable suboptimal solution with
the help of the block coordinate descent and the successive convex
approximation techniques. Finally, the numerical results are presented to show
the effectiveness of the proposed scheme. The numerical results have shown that
the proposed scheme can largely save the transmission time as compared to the
conventional vertical photography model.",0.3527788,0.22978462,-0.21255505,A
12869,"new method proposed in this paper is that it provides much more
                                        information and data about each piece compared to Simonton‚Äôs             2.2 Researcher Assumptions
                                        method; however, further research is required before the robustness
                                        of this novel method is confirmed.",The benefit of this   publications in classical music computational analyses [20].,"Most researchers seem to be making individual assumptions on
                                                                                                                 what they consider ‚Äúfame‚Äù or ‚Äúsuccess‚Äù.",2022-10-21 19:03:29+00:00,A computational analysis on the relationship between melodic originality and thematic fame in classical music from the Romantic period,cs.MM,['cs.MM'],[arxiv.Result.Author('Hudson Griffith')],"In this work, the researcher presents a novel approach to calculating melodic
originality based on the research by Simonton (1994). This novel formula is
then applied to a dataset of 428 classical music pieces from the Romantic
period to analyze the relationship between melodic originality and thematic
fame.",-0.18991898,-0.37915918,0.34823558,C
12870,"If further research is done in this field it is
recommended that the researchers look into a greater length of note    3.1 Calculating Transition Probabilities
sequences.",note sequences‚Äù [13].,"The difficulty behind analyzing longer note sequences
is the amount of data required.",2022-10-21 19:03:29+00:00,A computational analysis on the relationship between melodic originality and thematic fame in classical music from the Romantic period,cs.MM,['cs.MM'],[arxiv.Result.Author('Hudson Griffith')],"In this work, the researcher presents a novel approach to calculating melodic
originality based on the research by Simonton (1994). This novel formula is
then applied to a dataset of 428 classical music pieces from the Romantic
period to analyze the relationship between melodic originality and thematic
fame.",-0.12507354,0.049494825,0.64063084,B
