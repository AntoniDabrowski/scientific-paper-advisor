Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
214,"We hope that wCQ will spur further research in creating better
performing wait-free data structures with F&A.","Though Kogan-Petrank’s method can be used to create wait-free queues with CAS [14], wCQ addresses unique chal-
lenges since it avoids dynamic allocation and uses F&A.","Availability

We provide code for wCQ at https://github.com/rusnikola/wfqueue.",2022-01-06 18:46:53+00:00,wCQ: A Fast Wait-Free Queue with Bounded Memory Usage,cs.DC,['cs.DC'],"[arxiv.Result.Author('Ruslan Nikolaev'), arxiv.Result.Author('Binoy Ravindran')]","The concurrency literature presents a number of approaches for building
non-blocking, FIFO, multiple-producer and multiple-consumer (MPMC) queues.
However, only a fraction of them have high performance. In addition, many queue
designs, such as LCRQ, trade memory usage for better performance. The recently
proposed SCQ design achieves both memory efficiency as well as excellent
performance. Unfortunately, both LCRQ and SCQ are only lock-free. On the other
hand, existing wait-free queues are either not very performant or suffer from
potentially unbounded memory usage. Strictly described, the latter queues, such
as Yang & Mellor-Crummey's (YMC) queue, forfeit wait-freedom as they are
blocking when memory is exhausted.
  We present a wait-free queue, called wCQ. wCQ is based on SCQ and uses its
own variation of fast-path-slow-path methodology to attain wait-freedom and
bound memory usage. Our experimental studies on x86 and PowerPC architectures
validate wCQ's great performance and memory efficiency. They also show that
wCQ's performance is often on par with the best known concurrent queue designs.",-0.020032432,0.083706446,0.11501729,C
215,"We hope that wCQ will
   Though SimQueue [7] uses F&A and outperforms Michael &                 spur further research in creating better performing wait-free data
Scott’s FIFO queue by aggregating concurrent operations with one          structures with F&A.","queues with CAS [18], wCQ addresses unique challenges since it
                                                                          avoids dynamic allocation and uses F&A.","CAS, it is still not that performant.",2022-01-06 18:46:53+00:00,wCQ: A Fast Wait-Free Queue with Bounded Memory Usage,cs.DC,['cs.DC'],"[arxiv.Result.Author('Ruslan Nikolaev'), arxiv.Result.Author('Binoy Ravindran')]","The concurrency literature presents a number of approaches for building
non-blocking, FIFO, multiple-producer and multiple-consumer (MPMC) queues.
However, only a fraction of them have high performance. In addition, many queue
designs, such as LCRQ, trade memory usage for better performance. The recently
proposed SCQ design achieves both memory efficiency as well as excellent
performance. Unfortunately, both LCRQ and SCQ are only lock-free. On the other
hand, existing wait-free queues are either not very performant or suffer from
potentially unbounded memory usage. Strictly described, the latter queues, such
as Yang & Mellor-Crummey's (YMC) queue, forfeit wait-freedom as they are
blocking when memory is exhausted.
  We present a wait-free queue, called wCQ. wCQ is based on SCQ and uses its
own variation of fast-path-slow-path methodology to attain wait-freedom and
bound memory usage. Our experimental studies on x86 and PowerPC architectures
validate wCQ's great performance and memory efficiency. They also show that
wCQ's performance is often on par with the best known concurrent queue designs.",-0.054097146,0.068859816,0.09866668,C
366,"Their
actions requires further research and engineering effort.","Implementing parallel execution of blockchain trans-     order, eliminating the need for distributed commit protocols.","We need        approach is further outlined in several other papers [1, 27, 28].",2022-01-11 02:30:55+00:00,Utilizing Parallelism in Smart Contracts on Decentralized Blockchains by Taming Application-Inherent Conflicts,cs.DC,['cs.DC'],"[arxiv.Result.Author('Péter Garamvölgyi'), arxiv.Result.Author('Yuxi Liu'), arxiv.Result.Author('Dong Zhou'), arxiv.Result.Author('Fan Long'), arxiv.Result.Author('Ming Wu')]","Traditional public blockchain systems typically had very limited transaction
throughput due to the bottleneck of the consensus protocol itself. With recent
advances in consensus technology, the performance limit has been greatly
lifted, typically to thousands of transactions per second. With this,
transaction execution has become a new performance bottleneck. Exploiting
parallelism in transaction execution is a clear and direct way to address this
and further increase transaction throughput. Although some recent literature
introduced concurrency control mechanisms to execute smart contract
transactions in parallel, the reported speedup that they can achieve is far
from ideal. The main reason is that the proposed parallel execution mechanisms
cannot effectively deal with the conflicts inherent in many blockchain
applications.
  In this work, we thoroughly study the historical transaction execution traces
in Ethereum. We observe that application-inherent conflicts are the major
factors that limit the exploitable parallelism during execution. We propose to
use partitioned counters and special commutative instructions to break up the
application conflict chains in order to maximize the potential speedup. During
our evaluations, these techniques doubled the parallel speedup achievable to an
18x overall speedup compared to serial execution, approaching the optimum. We
also propose an OCC scheduler with deterministic aborts, which makes it
suitable for practical integration into public blockchain systems.",0.14772902,0.23433778,0.5310408,B
367,"Our
further research on parallel execution incentives, and the EVM           discussion of the determinism of blockchain transaction execution
needs to be extended with a parallel scheduler.","We need        approach is further outlined in several other papers [1, 27, 28].","""OCC with det.",2022-01-11 02:30:55+00:00,Utilizing Parallelism in Smart Contracts on Decentralized Blockchains by Taming Application-Inherent Conflicts,cs.DC,['cs.DC'],"[arxiv.Result.Author('Péter Garamvölgyi'), arxiv.Result.Author('Yuxi Liu'), arxiv.Result.Author('Dong Zhou'), arxiv.Result.Author('Fan Long'), arxiv.Result.Author('Ming Wu')]","Traditional public blockchain systems typically had very limited transaction
throughput due to the bottleneck of the consensus protocol itself. With recent
advances in consensus technology, the performance limit has been greatly
lifted, typically to thousands of transactions per second. With this,
transaction execution has become a new performance bottleneck. Exploiting
parallelism in transaction execution is a clear and direct way to address this
and further increase transaction throughput. Although some recent literature
introduced concurrency control mechanisms to execute smart contract
transactions in parallel, the reported speedup that they can achieve is far
from ideal. The main reason is that the proposed parallel execution mechanisms
cannot effectively deal with the conflicts inherent in many blockchain
applications.
  In this work, we thoroughly study the historical transaction execution traces
in Ethereum. We observe that application-inherent conflicts are the major
factors that limit the exploitable parallelism during execution. We propose to
use partitioned counters and special commutative instructions to break up the
application conflict chains in order to maximize the potential speedup. During
our evaluations, these techniques doubled the parallel speedup achievable to an
18x overall speedup compared to serial execution, approaching the optimum. We
also propose an OCC scheduler with deterministic aborts, which makes it
suitable for practical integration into public blockchain systems.",0.17533512,0.17974757,0.56546575,A
368,"Their
actions requires further research and engineering effort.","Implementing parallel execution of blockchain trans-     order, eliminating the need for distributed commit protocols.","We need        approach is further outlined in several other papers [1, 29, 30].",2022-01-11 02:30:55+00:00,Utilizing Parallelism in Smart Contracts on Decentralized Blockchains by Taming Application-Inherent Conflicts,cs.DC,['cs.DC'],"[arxiv.Result.Author('Péter Garamvölgyi'), arxiv.Result.Author('Yuxi Liu'), arxiv.Result.Author('Dong Zhou'), arxiv.Result.Author('Fan Long'), arxiv.Result.Author('Ming Wu')]","Traditional public blockchain systems typically had very limited transaction
throughput because of the bottleneck of the consensus protocol itself. With
recent advances in consensus technology, the performance limit has been greatly
lifted, typically to thousands of transactions per second. With this,
transaction execution has become a new performance bottleneck. Exploiting
parallelism in transaction execution is a clear and direct way to address this
and to further increase transaction throughput. Although some recent literature
introduced concurrency control mechanisms to execute smart contract
transactions in parallel, the reported speedup that they can achieve is far
from ideal. The main reason is that the proposed parallel execution mechanisms
cannot effectively deal with the conflicts inherent in many blockchain
applications.
  In this work, we thoroughly study the historical transaction execution traces
in Ethereum. We observe that application-inherent conflicts are the major
factors that limit the exploitable parallelism during execution. We propose to
use partitioned counters and special commutative instructions to break up the
application conflict chains in order to maximize the potential speedup. When we
evaluated the maximum parallel speedup achievable, these techniques doubled
this limit to an 18x overall speedup compared to serial execution, thus
approaching the optimum. We also propose OCC-DA, an optimistic concurrency
control scheduler with deterministic aborts, which makes it possible to use OCC
scheduling in public blockchain settings.",0.14654616,0.23298039,0.53327584,B
369,"Our
further research on parallel execution incentives, and the EVM           discussion of the determinism of blockchain transaction execution
needs to be extended with a parallel scheduler.","We need        approach is further outlined in several other papers [1, 29, 30].",OCC-DA provides          was inspired by these works.,2022-01-11 02:30:55+00:00,Utilizing Parallelism in Smart Contracts on Decentralized Blockchains by Taming Application-Inherent Conflicts,cs.DC,['cs.DC'],"[arxiv.Result.Author('Péter Garamvölgyi'), arxiv.Result.Author('Yuxi Liu'), arxiv.Result.Author('Dong Zhou'), arxiv.Result.Author('Fan Long'), arxiv.Result.Author('Ming Wu')]","Traditional public blockchain systems typically had very limited transaction
throughput because of the bottleneck of the consensus protocol itself. With
recent advances in consensus technology, the performance limit has been greatly
lifted, typically to thousands of transactions per second. With this,
transaction execution has become a new performance bottleneck. Exploiting
parallelism in transaction execution is a clear and direct way to address this
and to further increase transaction throughput. Although some recent literature
introduced concurrency control mechanisms to execute smart contract
transactions in parallel, the reported speedup that they can achieve is far
from ideal. The main reason is that the proposed parallel execution mechanisms
cannot effectively deal with the conflicts inherent in many blockchain
applications.
  In this work, we thoroughly study the historical transaction execution traces
in Ethereum. We observe that application-inherent conflicts are the major
factors that limit the exploitable parallelism during execution. We propose to
use partitioned counters and special commutative instructions to break up the
application conflict chains in order to maximize the potential speedup. When we
evaluated the maximum parallel speedup achievable, these techniques doubled
this limit to an 18x overall speedup compared to serial execution, thus
approaching the optimum. We also propose OCC-DA, an optimistic concurrency
control scheduler with deterministic aborts, which makes it possible to use OCC
scheduling in public blockchain settings.",0.15627109,0.18012346,0.56003696,A
621,"Nevertheless, some open
questions remain that warrant further research.","6 Discussion & Future Work

We have shown how the d-hops placement algorithm for 2D tori can be extended
for QoS-aware resource placement in satellite networks.","6.1 Application for Service Placement

Unlike static compute resource placement, service placement, e.g., placing a
database within a given distance of clients on the satellite network, can leverage
service migration.",2022-01-15 15:32:52+00:00,QoS-Aware Resource Placement for LEO Satellite Edge Computing,cs.DC,"['cs.DC', 'cs.NI']","[arxiv.Result.Author('Tobias Pfandzelter'), arxiv.Result.Author('David Bermbach')]","With the advent of large LEO satellite communication networks to provide
global broadband Internet access, interest in providing edge computing
resources within LEO networks has emerged. The LEO Edge promises low-latency,
high-bandwidth access to compute and storage resources for a global base of
clients and IoT devices regardless of their geographical location.
  Current proposals assume compute resources or service replicas at every LEO
satellite, which requires high upfront investments and can lead to
over-provisioning. To implement and use the LEO Edge efficiently, methods for
server and service placement are required that help select an optimal subset of
satellites as server or service replica locations. In this paper, we show how
the existing research on resource placement on a 2D torus can be applied to
this problem by leveraging the unique topology of LEO satellite networks.
Further, we extend the existing discrete resource placement methods to allow
placement with QoS constraints. In simulation of proposed LEO satellite
communication networks, we show how QoS depends on orbital parameters and that
our proposed method can take these effects into account where the existing
approach cannot.",-0.119534194,-0.07244423,0.05950035,C
622,"In case of the Starlink B shell, the low number          that warrant further research.","Nevertheless, some open questions remain
mean 10ms.",of planes leads to a high distance between planes.,2022-01-15 15:32:52+00:00,QoS-Aware Resource Placement for LEO Satellite Edge Computing,cs.DC,"['cs.DC', 'cs.NI']","[arxiv.Result.Author('Tobias Pfandzelter'), arxiv.Result.Author('David Bermbach')]","With the advent of large LEO satellite communication networks to provide
global broadband Internet access, interest in providing edge computing
resources within LEO networks has emerged. The LEO Edge promises low-latency,
high-bandwidth access to compute and storage resources for a global base of
clients and IoT devices regardless of their geographical location.
  Current proposals assume compute resources or service replicas at every LEO
satellite, which requires high upfront investments and can lead to
over-provisioning. To implement and use the LEO Edge efficiently, methods for
server and service placement are required that help select an optimal subset of
satellites as server or service replica locations. In this paper, we show how
the existing research on resource placement on a 2D torus can be applied to
this problem by leveraging the unique topology of LEO satellite networks.
Further, we extend the existing discrete resource placement methods to allow
placement with QoS constraints. In simulation of proposed LEO satellite
communication networks, we show how QoS depends on orbital parameters and that
our proposed method can take these effects into account where the existing
approach cannot.",0.07812286,0.13243222,-0.24370089,A
943,"How to
With the help of blockchain, the decentralization of resource     design a reasonable reputation mechanism for IIoT systems
                                                                  in a distributed way is a direction worthy of further study.",system can be more comprehensive and complicated.,"In
                                                                  addition, computational workload prediction can help to bring
                                                                  more intelligence to collaborative computation task ofﬂoad-
8                                                                                       IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS

   Average Cost ($/Gcycle)    10                                                        [10] Z. Tian, M. Li, M. Qiu, Y.",2022-01-24 09:12:17+00:00,A Blockchain-Based Distributed Computational Resource Trading System for Industrial Internet of Things Considering Multiple Preferences,cs.DC,['cs.DC'],"[arxiv.Result.Author('Tonghe Wang'), arxiv.Result.Author('Songpu Ai'), arxiv.Result.Author('Zhihong Tian'), arxiv.Result.Author('Brij B. Gupta'), arxiv.Result.Author('Chun Shan')]","Computational task offloading based on edge computing can deal with the
performance bottleneck faced by traditional cloud-based systems for industrial
Internet of things (IIoT). To further optimize computing efficiency and
resource allocation, collaborative offloading has been put forward to enable
the offloading from edge devices to IIoT terminal devices. However, there still
lack incentive mechanisms to encourage participants to take over the tasks from
others. To counter this situation, this paper proposes a distributed
computational resource trading strategy considering multiple preferences of
IIoT users. Unlike most existing works, the objective of our trading strategy
comprehensively considers different satisfaction degrees with task delay,
energy consumption, price, and user reputation of both requesters and
collaborators. Our system uses blockchain to enhance the decentralization,
security, and automation. Compared with the trading method based on classical
double auction matching mechanism, our trading method will have more tasks
offloaded and executed, and the trading results are more friendly to
collaborators with higher reputation scores.",-0.17859858,0.00040746108,0.5109982,B
944,"How to
With the help of blockchain, the decentralization of resource     design a reasonable reputation mechanism for IIoT systems
                                                                  in a distributed way is a direction worthy of further study.",system can be more comprehensive and complicated.,"In
                                                                  addition, computational workload prediction can help to bring
                                                                  more intelligence to collaborative computation task ofﬂoad-
8

   Average Cost ($/Gcycle)    10                                                        [10] Z. Tian, M. Li, M. Qiu, Y.",2022-01-24 09:12:17+00:00,A Blockchain-Based Distributed Computational Resource Trading System for Industrial Internet of Things Considering Multiple Preferences,cs.DC,['cs.DC'],"[arxiv.Result.Author('Tonghe Wang'), arxiv.Result.Author('Songpu Ai'), arxiv.Result.Author('Zhihong Tian'), arxiv.Result.Author('Brij B. Gupta'), arxiv.Result.Author('Chun Shan')]","Computational task offloading based on edge computing can deal with the
performance bottleneck faced by traditional cloud-based systems for industrial
Internet of things (IIoT). To further optimize computing efficiency and
resource allocation, collaborative offloading has been put forward to enable
the offloading from edge devices to IIoT terminal devices. However, there still
lack incentive mechanisms to encourage participants to take over the tasks from
others. To counter this situation, this paper proposes a distributed
computational resource trading strategy considering multiple preferences of
IIoT users. Unlike most existing works, the objective of our trading strategy
comprehensively considers different satisfaction degrees with task delay,
energy consumption, price, and user reputation of both requesters and
collaborators. Our system uses blockchain to enhance the decentralization,
security, and automation. Compared with the trading method based on classical
double auction matching mechanism, our trading method will have more tasks
offloaded and executed, and the trading results are more friendly to
collaborators with higher reputation scores.",-0.17293185,0.008023076,0.52269363,B
1104,The variation in the system resources causes differ-            further research1.,"The same Figure 1a also shows the variation in FC with             • We open source the collected data and developed tool for
cloud providers.","ences in performance between the identical function deployments
for the same FaaS platforms.",2022-01-27 11:44:29+00:00,Estimating the Capacities of Function-as-a-Service Functions,cs.DC,"['cs.DC', 'stat.OT']","[arxiv.Result.Author('Anshul Jindal'), arxiv.Result.Author('Mohak Chadha'), arxiv.Result.Author('Shajulin Benedict'), arxiv.Result.Author('Michael Gerndt')]","Serverless computing is a cloud computing paradigm that allows developers to
focus exclusively on business logic as cloud service providers manage resource
management tasks. Serverless applications follow this model, where the
application is decomposed into a set of fine-grained Function-as-a-Service
(FaaS) functions. However, the obscurities of the underlying system
infrastructure and dependencies between FaaS functions within the application
pose a challenge for estimating the performance of FaaS functions. To
characterize the performance of a FaaS function that is relevant for the user,
we define Function Capacity (FC) as the maximal number of concurrent
invocations the function can serve in a time without violating the
Service-Level Objective (SLO).
  The paper addresses the challenge of quantifying the FC individually for each
FaaS function within a serverless application. This challenge is addressed by
sandboxing a FaaS function and building its performance model. To this end, we
develop FnCapacitor - an end-to-end automated Function Capacity estimation
tool. We demonstrate the functioning of our tool on Google Cloud Functions
(GCF) and AWS Lambda. FnCapacitor estimates the FCs on different deployment
configurations (allocated memory & maximum function instances) by conducting
time-framed load tests and building various models using statistical: linear,
ridge, and polynomial regression, and Deep Neural Network (DNN) methods on the
acquired performance data. Our evaluation of different FaaS functions shows
relatively accurate predictions, with an accuracy greater than 75% using DNN
for both cloud providers.",-0.29540676,-0.26261115,0.010014632,C
1607,"Other
up many opportunities for further research.","For instance,
As presented in Section 5.4, the spot instance ful llment and    we are currently developing data collection for multiple
interruption probability can be better modeled by combining      vendors using the timestamp as a global key, and it helps
the spot instance availability and interruption frequency        to understand temporal characteristics of spot instance
datasets, and providing the historical information can open      availability pattern of di erent cloud providers.","than the timestamp, adding more global keys such as
                                                                 hardware details are bene cial to analyze and compare
    Usefulness of spot instance availability informa-            the spot instance characteristics from various aspects and
tion: By referencing availability information from spot          help to nd optimal spot resources for diverse workloads.",2022-02-07 06:46:53+00:00,SpotLake: Diverse Spot Instance Dataset Archive Service,cs.DC,['cs.DC'],"[arxiv.Result.Author('Sungjae Lee'), arxiv.Result.Author('Jaeil Hwang'), arxiv.Result.Author('Kyungyong Lee')]","Public cloud service vendors provide a surplus of computing resources at a
cheaper price as a spot instance. Despite the cheaper price, the spot instance
can be forced to be shutdown at any moment whenever the surplus resources are
in shortage. To enhance spot instance usage, vendors provide diverse spot
instance datasets. Amon them, the spot price information has been most widely
used so far. However, the tendency toward barely changing spot price weakens
the applicability of the spot price dataset. Besides the price dataset, the
recently introduced spot instance availability and interruption ratio datasets
can help users better utilize spot instances, but they are rarely used in
reality. With a thorough analysis, we could uncover major hurdles when using
the new datasets concerning the lack of historical information, query
constraints, and limited query interfaces. To overcome them, we develop
SpotLake, a spot instance data archive web service that provides historical
information of various spot instance datasets. Novel heuristics to collect
various datasets and a data serving architecture are presented. Through
real-world spot instance availability experiments, we present the applicability
of the proposed system. SpotLake is publicly available as a web service to
speed up cloud system research to improve spot instance usage and availability
while reducing cost.",-0.35638803,-0.17500839,-0.04702294,C
2984,"Literature reported [97, 98, 99] that deep learning approaches for placement robot tasks in fog, however
further research is needed.","IoT deﬁnes each robot as an item capable of interacting with other IoT things and other
robots [98].","In the future, methods and scheduling algorithms for fog computing’s application placement
problem will need to be established depending on the types and categories of request applications, according to research
and evaluations of literature [100].",2022-03-05 16:59:43+00:00,AI for Next Generation Computing: Emerging Trends and Future Directions,cs.DC,['cs.DC'],"[arxiv.Result.Author('Sukhpal Singh Gill'), arxiv.Result.Author('Minxian Xu'), arxiv.Result.Author('Carlo Ottaviani'), arxiv.Result.Author('Panos Patros'), arxiv.Result.Author('Rami Bahsoon'), arxiv.Result.Author('Arash Shaghaghi'), arxiv.Result.Author('Muhammed Golec'), arxiv.Result.Author('Vlado Stankovski'), arxiv.Result.Author('Huaming Wu'), arxiv.Result.Author('Ajith Abraham'), arxiv.Result.Author('Manmeet Singh'), arxiv.Result.Author('Harshit Mehta'), arxiv.Result.Author('Soumya K. Ghosh'), arxiv.Result.Author('Thar Baker'), arxiv.Result.Author('Ajith Kumar Parlikad'), arxiv.Result.Author('Hanan Lutfiyya'), arxiv.Result.Author('Salil S. Kanhere'), arxiv.Result.Author('Rizos Sakellariou'), arxiv.Result.Author('Schahram Dustdar'), arxiv.Result.Author('Omer Rana'), arxiv.Result.Author('Ivona Brandic'), arxiv.Result.Author('Steve Uhlig')]","Autonomic computing investigates how systems can achieve (user) specified
control outcomes on their own, without the intervention of a human operator.
Autonomic computing fundamentals have been substantially influenced by those of
control theory for closed and open-loop systems. In practice, complex systems
may exhibit a number of concurrent and inter-dependent control loops. Despite
research into autonomic models for managing computer resources, ranging from
individual resources (e.g., web servers) to a resource ensemble (e.g., multiple
resources within a data center), research into integrating Artificial
Intelligence (AI) and Machine Learning (ML) to improve resource autonomy and
performance at scale continues to be a fundamental challenge. The integration
of AI/ML to achieve such autonomic and self-management of systems can be
achieved at different levels of granularity, from full to human-in-the-loop
automation. In this article, leading academics, researchers, practitioners,
engineers, and scientists in the fields of cloud computing, AI/ML, and quantum
computing join to discuss current research and potential future directions for
these fields. Further, we discuss challenges and opportunities for leveraging
AI and ML in next generation computing for emerging computing paradigms,
including cloud, fog, edge, serverless and quantum computing environments.",-0.23784845,-0.07987091,-0.027901901,C
3365,"Therefore, further research needs from both legal
and engineering perspectives to explore the regulatory rules in     [2] J. Joshua, “Information bodies: Computational anxiety in neal stephen-
metaverse.","Available:
establish its own legal and ethical framework to regulate users’         http://arxiv.org/abs/2110.05352
behaviors.","son’s snow crash,” Interdisciplinary Literary Studies, vol.",2022-03-16 02:25:39+00:00,Metaverse Native Communication: A Blockchain and Spectrum Prospective,cs.DC,"['cs.DC', 'cs.CY', 'cs.NI']","[arxiv.Result.Author('Hao Xu'), arxiv.Result.Author('Zihao Li'), arxiv.Result.Author('Zongyao Li'), arxiv.Result.Author('Xiaoshuai Zhang'), arxiv.Result.Author('Yao Sun'), arxiv.Result.Author('Lei Zhang')]","Metaverse depicts a vista of constructing a virtual environment parallel to
the real world so people can communicate with others and objects through
digital entities. In the real world, communication relies on identities and
addresses that are recognized by authorities, no matter the link is established
via post, email, mobile phone, or landline. Metaverse, however, is different
from the real world, which requires a single identity belongs to the
individual. This identity can be an encrypted virtual address in the metaverse
but no one can trace or verify it. In order to achieve such addresses to hide
individuals in the metaverse, re-mapping the virtual address to the
individual's identity and a specific spectrum to support the address-based
communication for the metaverse are needed. Therefore, metaverse native or
meta-native communications based on blockchain could be a promising solution to
directly connect entities with their native encrypted addresses that gets rid
of the existing network services based on IP, cellular, HTTP, etc. This paper
proposes a vision of blockchain, encrypted address and address-based access
model for all users, devices, services, etc. to contribute to the metaverse.
Furthermore, the allocation architecture of a designated spectrum for the
metaverse is proposed to remove the barrier to access to the
metaverse/blockchain in response to the initiatives of metaverse and
decentralized Internet.",-0.25472382,0.30480635,0.033966772,B
3936,"The ﬁrst results obtained in this area are en-
couraging, and further research and development should be made into exploring the possi-
bility of a generalized hybrid framework that can combine the strengths of the two parallel
processing models.","To further optimize
resource utilization of certain workloads, some experiments have implemented hybrid mul-
tiprocess/multithreaded processing models.","Computational accelerators (e.g., GPUs, FPGAs) are becoming increasingly prevalent
in most new computing facilities.",2022-03-16 04:25:34+00:00,Evolution of HEP Processing Frameworks,cs.DC,"['cs.DC', 'hep-ex', 'physics.data-an']","[arxiv.Result.Author('Christopher D. Jones'), arxiv.Result.Author('Kyle Knoepfel'), arxiv.Result.Author('Paolo Calafiura'), arxiv.Result.Author('Charles Leggett'), arxiv.Result.Author('Vakhtang Tsulaia')]","HEP data-processing software must support the disparate physics needs of many
experiments. For both collider and neutrino environments, HEP experiments
typically use data-processing frameworks to manage the computational
complexities of their large-scale data processing needs. Data-processing
frameworks are being faced with new challenges this decade. The computing
landscape has changed from the past three decades of homogeneous single-core
x86 batch jobs running on grid sites. Frameworks must now work on a
heterogeneous mixture of different platforms: multi-core machines, different
CPU architectures, and computational accelerators; and different computing
sites: grid, cloud, and high-performance computing. We describe these
challenges in more detail and how frameworks may confront them. Given their
historic success, frameworks will continue to be critical software systems that
enable HEP experiments to meet their computing needs. Frameworks have weathered
computing revolutions in the past; they will do so again with support from the
HEP community",0.15251093,-0.21636558,0.13523492,A
3969,"Therefore,
further research could combine the usage of shared CAVs for people trips and PDO delivery to achieve
better objectives, such as reducing the carbon emission and the total cost.","38
Yang et al (2022)
Moreover, a future crowdsourced delivery system may include connected automated vehicles (CAVs) that
are owned by individuals but rented out to logistics providers during certain portions of the day.","Another research direction involves integrating crowdsourced delivery with other modes of freight and
passenger transport, such as transit vehicles or drones.",2022-03-10 08:07:34+00:00,Tackling the Crowdsourced Delivery Problem at Scale through a Set-Partitioning Formulation and Novel Decomposition Heuristic,cs.DC,"['cs.DC', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Dingtong Yang'), arxiv.Result.Author('Michael F. Hyland'), arxiv.Result.Author('R. Jayakrishnan')]","This paper presents a set-partitioning formulation and a novel decomposition
heuristic (D-H) solution algorithm to solve large-scale instances of the urban
crowdsourced shared-trip package delivery problem. The D-H begins by dividing
the packages between shared personal vehicles (SPVs) and dedicated vehicles
(DVs). For package-assignment to SPVs, this paper enumerates the set of routes
each SPV can traverse and constructs a package-SPV route assignment problem.
For package-assignment to DVs and routing, the paper first obtains DV routes by
solving a conventional vehicle routing problem and then seeks potential
solution improvements by switching packages from SPVs to DVs. The switching
process is cost driven. The D-H significantly outperforms a commercial solver
in terms of computational efficiency, while obtaining near-optimal solutions
for small problem instances. This paper presents a city-scale case study to
analyze the important service design factors that impact the efficiency of
crowdsourced shared-trip delivery. The paper further analyzes the impact of
three important service design factors on system performance, namely (i) the
number of participating SPVs, (ii) the maximum detour willingness of SPVs, and
(iii) the depot locations. The results and findings provide meaningful insights
for industry practice, while the algorithms illustrate promise for large
real-world systems.",-0.33838704,0.096645765,0.16445711,C
4401,"The Arm-based A64FX processor
larger L3 capacity of Milan-X yields up-to 3.4x improvements        from Fujitsu is currently powering Supercomputer Fugaku [5],
over baseline Milan for this memory-bound application, which        which, as of April 2022, leads the TOP500 [30] (for HPL
motivates us to further research 3D-stacked caches.","Figure 1 overviews our           We choose to base our future processor design on the
result, and we see that for a subset of problem sizes, the 3-times  A64FX processor [29].",and HPCG; cf.,2022-04-05 14:16:53+00:00,At the Locus of Performance: A Case Study in Enhancing CPUs with Copious 3D-Stacked Cache,cs.DC,['cs.DC'],"[arxiv.Result.Author('Jens Domke'), arxiv.Result.Author('Emil Vatai'), arxiv.Result.Author('Balazs Gerofi'), arxiv.Result.Author('Yuetsu Kodama'), arxiv.Result.Author('Mohamed Wahib'), arxiv.Result.Author('Artur Podobas'), arxiv.Result.Author('Sparsh Mittal'), arxiv.Result.Author('Miquel Pericàs'), arxiv.Result.Author('Lingqi Zhang'), arxiv.Result.Author('Peng Chen'), arxiv.Result.Author('Aleksandr Drozd'), arxiv.Result.Author('Satoshi Matsuoka')]","Over the last three decades, innovations in the memory subsystem were
primarily targeted at overcoming the data movement bottleneck. In this paper,
we focus on a specific market trend in memory technology: 3D-stacked memory and
caches. We investigate the impact of extending the on-chip memory capabilities
in future HPC-focused processors, particularly by 3D-stacked SRAM. First, we
propose a method oblivious to the memory subsystem to gauge the upper-bound in
performance improvements when data movement costs are eliminated. Then, using
the gem5 simulator, we model two variants of LARC, a processor fabricated in
1.5 nm and enriched with high-capacity 3D-stacked cache. With a volume of
experiments involving a board set of proxy-applications and benchmarks, we aim
to reveal where HPC CPU performance could be circa 2028, and conclude an
average boost of 9.77x for cache-sensitive HPC applications, on a per-chip
basis. Additionally, we exhaustively document our methodological exploration to
motivate HPC centers to drive their own technological agenda through enhanced
co-design.",0.29863435,-0.20679523,-0.015600616,A
4790,"We hope that
                                                                          Celestial, which we have published as open-source, can be of
With its scale and dynamic topology, the LEO edge poses signif-           great use in this further research on and development of LEO edge
icant challenges for applications.","To
                                                                          further improve the accuracy of emulated testbeds, experiences
6.7 Future Work on LEO Edge Systems                                       and data of real LEO edge deployments are needed.",To simplify the deployment of          computing.,2022-04-13 10:27:55+00:00,Celestial: Virtual Software System Testbeds for the LEO Edge,cs.DC,['cs.DC'],"[arxiv.Result.Author('Tobias Pfandzelter'), arxiv.Result.Author('David Bermbach')]","As private space companies such as SpaceX and Telesat are building large LEO
satellite constellations to provide global broadband Internet access,
researchers have proposed to embed compute services within satellite
constellations to provide computing services on the LEO edge. While the LEO
edge is merely theoretical at the moment, providers are expected to rapidly
develop their satellite technologies to keep the upper hand in the new space
race.
  In this paper, we answer the question of how researchers can explore the
possibilities of LEO edge computing and evaluate arbitrary software systems in
an accurate runtime environment and with cost-efficient scalability. To that
end, we present Celestial, a virtual testbed for the LEO edge based on
microVMs. Celestial can efficiently emulate individual satellites and their
movement as well as ground station servers with realistic network conditions
and in an application-agnostic manner, which we show empirically. Additionally,
we explore opportunities and implications of deploying a real-time remote
sensing application on LEO edge infrastructure in a case study on Celestial.",0.044651367,-0.17028558,-0.26421574,C
4791,"We hope that
                                                                          Celestial, which we have published as open-source, can be of
With its scale and dynamic topology, the LEO edge poses signif-           great use in this further research on and development of LEO edge
icant challenges for applications.","To
                                                                          further improve the accuracy of emulated testbeds, experiences
6.7 Future Work on LEO Edge Systems                                       and data of real LEO edge deployments are needed.",To simplify the deployment of          computing.,2022-04-13 10:27:55+00:00,Celestial: Virtual Software System Testbeds for the LEO Edge,cs.DC,['cs.DC'],"[arxiv.Result.Author('Tobias Pfandzelter'), arxiv.Result.Author('David Bermbach')]","As private space companies such as SpaceX and Telesat are building large LEO
satellite constellations to provide global broadband Internet access,
researchers have proposed to embed compute services within satellite
constellations to provide computing services on the LEO edge. While the LEO
edge is merely theoretical at the moment, providers are expected to rapidly
develop their satellite technologies to keep the upper hand in the new space
race.
  In this paper, we answer the question of how researchers can explore the
possibilities of LEO edge computing and evaluate arbitrary software systems in
an accurate runtime environment and with cost-efficient scalability. To that
end, we present Celestial, a virtual testbed for the LEO edge based on
microVMs. Celestial can efficiently emulate individual satellites and their
movement as well as ground station servers with realistic network conditions
and in an application-agnostic manner, which we show empirically. Additionally,
we explore opportunities and implications of deploying a real-time remote
sensing application on LEO edge infrastructure in a case study on Celestial.",0.044651367,-0.17028558,-0.26421574,C
5178,"Our implementations have been open sourced
upper bound on the running time of PVC was improved to                          to enable further research on parallel solutions to the vertex
O(1.2738k + kn) by Chen et al.","Since then, and                       difﬁcult instances of the problem and on graphs with high
after a long series of works [5], [33]–[36], the asymptotic                     average degree.",[4].,2022-04-21 20:44:48+00:00,Parallel Vertex Cover Algorithms on GPUs,cs.DC,['cs.DC'],"[arxiv.Result.Author('Peter Yamout'), arxiv.Result.Author('Karim Barada'), arxiv.Result.Author('Adnan Jaljuli'), arxiv.Result.Author('Amer E. Mouawad'), arxiv.Result.Author('Izzat El Hajj')]","Finding small vertex covers in a graph has applications in numerous domains.
Two common formulations of the problem include: Minimum Vertex Cover, which
finds the smallest vertex cover in a graph, and Parameterized Vertex Cover,
which finds a vertex cover whose size is less than or equal to some parameter
$k$. Algorithms for both formulations traverse a search tree, which grows
exponentially with the size of the graph or the value of $k$.
  Parallelizing the traversal of the vertex cover search tree on GPUs is
challenging for multiple reasons. First, the search tree is a narrow binary
tree which makes it difficult to extract enough sub-trees to process in
parallel to fully utilize the GPU's resources. Second, the search tree is
highly imbalanced which makes load balancing across a massive number of
parallel GPU workers challenging. Third, keeping around all the intermediate
state needed to traverse many sub-trees in parallel puts high pressure on the
GPU's memory resources and may act as a limiting factor to parallelism.
  To address these challenges, we propose an approach to traverse the vertex
cover search tree in parallel using GPUs while handling dynamic load balancing.
Each thread block traverses a different sub-tree using a local stack, however,
we also use a global worklist to balance load. Blocks contribute branches of
their sub-trees to the global worklist on an as-needed basis, while blocks that
finish their sub-trees get new ones from the global worklist. We use degree
arrays to represent intermediate graphs so that the representation is compact
in memory to avoid limiting parallelism, but self-contained which is necessary
for load balancing. Our evaluation shows that compared to prior work, our
hybrid approach of using local stacks and a global worklist substantially
improves performance and reduces load imbalance, especially on difficult
instances of the problem.",0.29088372,-0.038848784,0.11661175,A
5484,"The nature of the problem implies that it does not require
high computational accuracy, so we can use single precision (’ﬂoat’ type) or even
half precision (the last one is the subject of further research).","First, we need to decide on the choice of a relevant data type and
memory structure.","It is worth noting
that mixing ’double’ and ’ﬂoat’ types is a typical mistake of many programmers:
usage of double precision literals and inappropriate mathematical functions can
signiﬁcantly increase execution time.",2022-04-28 18:54:47+00:00,Black-Scholes Option Pricing on Intel CPUs and GPUs: Implementation on SYCL and Optimization Techniques,cs.DC,"['cs.DC', 'cs.MS', 'cs.PF']","[arxiv.Result.Author('Elena Panova'), arxiv.Result.Author('Valentin Volokitin'), arxiv.Result.Author('Anton Gorshkov'), arxiv.Result.Author('Iosif Meyerov')]","The Black-Scholes option pricing problem is one of the widely used financial
benchmarks. We explore the possibility of developing a high-performance
portable code using the SYCL (Data Parallel C++) programming language. We start
from a C++ code parallelized with OpenMP and show optimization techniques that
are beneficial on modern Intel Xeon CPUs. Then, we port the code to SYCL and
consider important optimization aspects on CPUs and GPUs (device-friendly
memory access patterns, relevant data management, employing vector data types).
We show that the developed SYCL code is only 10% inferior to the optimized C++
code when running on CPUs while achieving reasonable performance on Intel GPUs.
We hope that our experience of developing and optimizing the code on SYCL can
be useful to other researchers who plan to port their high-performance C++
codes to SYCL to get all the benefits of single-source programming.",0.23267995,0.0044809207,0.039234914,A
5485,"The nature of the problem implies that it does not require
high computational accuracy, so we can use single precision (’ﬂoat’ type) or even
half precision (the last one is the subject of further research).","First, we need to decide on the choice of a relevant data type and
memory structure.","It is worth noting
that mixing ’double’ and ’ﬂoat’ types is a typical mistake of many programmers:
usage of double precision literals and inappropriate mathematical functions can
signiﬁcantly increase execution time.",2022-04-28 18:54:47+00:00,Black-Scholes Option Pricing on Intel CPUs and GPUs: Implementation on SYCL and Optimization Techniques,cs.DC,"['cs.DC', 'cs.MS', 'cs.PF']","[arxiv.Result.Author('Elena Panova'), arxiv.Result.Author('Valentin Volokitin'), arxiv.Result.Author('Anton Gorshkov'), arxiv.Result.Author('Iosif Meyerov')]","The Black-Scholes option pricing problem is one of the widely used financial
benchmarks. We explore the possibility of developing a high-performance
portable code using the SYCL (Data Parallel C++) programming language. We start
from a C++ code parallelized with OpenMP and show optimization techniques that
are beneficial on modern Intel Xeon CPUs. Then, we port the code to SYCL and
consider important optimization aspects on CPUs and GPUs (device-friendly
memory access patterns, relevant data management, employing vector data types).
We show that the developed SYCL code is only 10% inferior to the optimized C++
code when running on CPUs while achieving reasonable performance on Intel GPUs.
We hope that our experience of developing and optimizing the code on SYCL can
be useful to other researchers who plan to port their high-performance C++
codes to SYCL to get all the benefits of single-source programming.",0.23267995,0.0044809207,0.039234914,A
5813,"Weaknesses: this work still requires further study and analysis of the interplay between the dynamics
   of the two types of optimization approaches (stale gradients and Momentum).","Furthermore, the authors demonstrated that a combination of Momentum (on the worker side)
   and standard defense mechanisms (KRUM[38], MEDIAN[129], BULYAN [45]) could be sufﬁcient
   to defend against the two presented attacks in [157], [158].",2.,2022-05-05 11:12:45+00:00,Byzantine Fault Tolerance in Distributed Machine Learning : a Survey,cs.DC,['cs.DC'],"[arxiv.Result.Author('Djamila Bouhata'), arxiv.Result.Author('Hamouma Moumen'), arxiv.Result.Author('Jocelyn Ahmed Mazari'), arxiv.Result.Author('Ahcène Bounceur')]","Byzantine Fault Tolerance (BFT) is one of the most challenging problems in
Distributed Machine Learning (DML), defined as the resilience of a
fault-tolerant system in the presence of malicious components. Byzantine
failures are still difficult to deal with due to their unrestricted nature,
which results in the possibility of generating arbitrary data. Significant
research efforts are constantly being made to implement BFT in DML. Some recent
studies have considered various BFT approaches in DML. However, some aspects
are limited, such as the few approaches analyzed, and there is no
classification of the techniques used in the studied approaches. In this paper,
we present a survey of recent work surrounding BFT in DML, mainly in
first-order optimization methods, especially Stochastic Gradient Descent(SGD).
We highlight key techniques as well as fundamental approaches. We provide an
illustrative description of the techniques used in BFT in DML, with a proposed
classification of BFT approaches in the context of their fundamental
techniques. This classification is established on specific criteria such as
communication process, optimization method, and topology setting, which
characterize future work methods addressing open challenge",0.14310595,0.1560649,-0.049577277,B
5855,"Figure 6: Evaluation of Parallelization
   Besides the total checking time using ViSearch, we further study
the effects of the optimizations we propose, namely the pruning of            5 RELATED WORK
search space and the parallelization of checking.",testing of CRDT implementations.,"As for the pruning,
we use the number of search states explored as the performance                The fuzzy nature of eventual consistency and its wide adoption
metric.",2022-05-06 11:57:46+00:00,ViSearch: Weak Consistency Measurement for Replicated Data Types,cs.DC,['cs.DC'],"[arxiv.Result.Author('Lintian Shi'), arxiv.Result.Author('Yuqi Zhang'), arxiv.Result.Author('Yu Huang'), arxiv.Result.Author('Hengfeng Wei'), arxiv.Result.Author('Xiaoxing Ma')]","Large-scale replicated data type stores often resort to eventual consistency
to guarantee low latency and high availability. It is widely accepted that
programming over eventually consistent data stores is challenging, since
arbitrary divergence among replicas is allowed. Moreover, pragmatic protocols
actually achieve consistency guarantees stronger than eventual consistency,
which can be and need to be utilized to facilitate the reasoning of and
programming over replicated data types. Toward the challenges above, we propose
the ViSearch framework for precise measurement of eventual consistency
semantics. ViSearch employs the visibility-arbitration specification
methodology in concurrent programming, which extends the linearizability-based
specification methodology with a dynamic visibility relation among operations,
in addition to the standard dynamic happen-before and linearization relations.
The consistency measurement using ViSearch is NP-hard in general. To enable
practical and efficient consistency measurement in replicated data type stores,
the ViSearch framework refactors the existing brute-force checking algorithm to
a generic algorithm skeleton, which further enables efficient pruning of the
search space and effective parallelization. We employ the ViSearch framework
for consistency measurement in two replicated data type stores Riak and
CRDT-Redis. The experimental evaluation shows the usefulness and
cost-effectiveness of consistency measurement based on the ViSearch framework
in realistic scenarios.",0.06944636,-0.045955986,0.099816985,A
5856,"7, except for the ‘small’
   Besides the total checking time using ViSearch, we further study        grade, the pruning can reduce the number of search states approxi-
the effects of the optimizations we propose, namely the pruning of         mately by half.",As shown in Fig.,"Note that, for ‘small’ traces, the checking time is
search space and the parallelization of checking.",2022-05-06 11:57:46+00:00,ViSearch: Weak Consistency Measurement for Replicated Data Types,cs.DC,['cs.DC'],"[arxiv.Result.Author('Lintian Shi'), arxiv.Result.Author('Yuqi Zhang'), arxiv.Result.Author('Yu Huang'), arxiv.Result.Author('Hengfeng Wei'), arxiv.Result.Author('Xiaoxing Ma')]","Large-scale replicated data type stores often resort to eventual consistency
to guarantee low latency and high availability. It is widely accepted that
programming over eventually consistent data stores is challenging, since
arbitrary divergence among replicas is allowed. Moreover, pragmatic protocols
actually achieve consistency guarantees stronger than eventual consistency,
which can be and need to be utilized to facilitate the reasoning of and
programming over replicated data types. Toward the challenges above, we propose
the ViSearch framework for precise measurement of eventual consistency
semantics. ViSearch employs the visibility-arbitration specification
methodology in concurrent programming, which extends the linearizability-based
specification methodology with a dynamic visibility relation among operations,
in addition to the standard dynamic happen-before and linearization relations.
The consistency measurement using ViSearch is NP-hard in general. To enable
practical and efficient consistency measurement in replicated data type stores,
the ViSearch framework refactors the existing brute-force checking algorithm to
a generic algorithm skeleton, which further enables efficient pruning of the
search space and effective parallelization. We employ the ViSearch framework
for consistency measurement in two replicated data type stores Riak and
CRDT-Redis. The experimental evaluation shows the usefulness and
cost-effectiveness of consistency measurement based on the ViSearch framework
in realistic scenarios.",0.25912938,0.04378485,-0.023246415,A
5980,"We further study how to deploy

                                                                   12
                Wang, et al.","Our hybrid learning framework is based
                                                                          on the Greengrass runtime.","/ Future Generation Computer Systems 00 (2022) 1–14                                                  13

stream analytics among edge and cloud resources and improve [5] K. Purandare, An introduction to deepstream sdkAvailable at GStreamer

their accuracies.",2022-05-10 01:56:16+00:00,An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics,cs.DC,"['cs.DC', 'cs.LG', 'cs.NI']","[arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Azim Khan'), arxiv.Result.Author('Jianwu Wang'), arxiv.Result.Author('Aryya Gangopadhyay'), arxiv.Result.Author('Carl E. Busart'), arxiv.Result.Author('Jade Freeman')]","With the popularity of Internet of Things (IoT), edge computing and cloud
computing, more and more stream analytics applications are being developed
including real-time trend prediction and object detection on top of IoT sensing
data. One popular type of stream analytics is the recurrent neural network
(RNN) deep learning model based time series or sequence data prediction and
forecasting. Different from traditional analytics that assumes data to be
processed are available ahead of time and will not change, stream analytics
deals with data that are being generated continuously and data
trend/distribution could change (aka concept drift), which will cause
prediction/forecasting accuracy to drop over time. One other challenge is to
find the best resource provisioning for stream analytics to achieve good
overall latency. In this paper, we study how to best leverage edge and cloud
resources to achieve better accuracy and latency for RNN-based stream
analytics. We propose a novel edge-cloud integrated framework for hybrid stream
analytics that support low latency inference on the edge and high capacity
training on the cloud. We study the flexible deployment of our hybrid learning
framework, namely edge-centric, cloud-centric and edge-cloud integrated.
Further, our hybrid learning framework can dynamically combine inference
results from an RNN model pre-trained based on historical data and another RNN
model re-trained periodically based on the most recent data. Using real-world
and simulated stream datasets, our experiments show the proposed edge-cloud
deployment is the best among all three deployment types in terms of latency.
For accuracy, the experiments show our dynamic learning approach performs the
best among all learning approaches for all three concept drift scenarios.",-0.25043362,-0.26000983,-0.04152926,C
5981,"We further study how to deploy          [3] S. B. Qaisar and M. Usman, “Fog networking for ma-
stream analytics among edge and cloud resources and improve             chine health prognosis: A deep learning perspective,” in
their accuracies.","Our hybrid learning framework is based
on the Greengrass runtime.","Microsoft also provides Azure IoT Edge [37]           International Conference on Computational Science and
service to scale out inference learning by packaging the logic          Its Applications.",2022-05-10 01:56:16+00:00,An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics,cs.DC,"['cs.DC', 'cs.LG', 'cs.NI']","[arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Azim Khan'), arxiv.Result.Author('Jianwu Wang'), arxiv.Result.Author('Aryya Gangopadhyay'), arxiv.Result.Author('Carl E. Busart'), arxiv.Result.Author('Jade Freeman')]","With the popularity of Internet of Things (IoT), edge computing and cloud
computing, more and more stream analytics applications are being developed
including real-time trend prediction and object detection on top of IoT sensing
data. One popular type of stream analytics is the recurrent neural network
(RNN) deep learning model based time series or sequence data prediction and
forecasting. Different from traditional analytics that assumes data to be
processed are available ahead of time and will not change, stream analytics
deals with data that are being generated continuously and data
trend/distribution could change (aka concept drift), which will cause
prediction/forecasting accuracy to drop over time. One other challenge is to
find the best resource provisioning for stream analytics to achieve good
overall latency. In this paper, we study how to best leverage edge and cloud
resources to achieve better accuracy and latency for RNN-based stream
analytics. We propose a novel edge-cloud integrated framework for hybrid stream
analytics that support low latency inference on the edge and high capacity
training on the cloud. We study the flexible deployment of our hybrid learning
framework, namely edge-centric, cloud-centric and edge-cloud integrated.
Further, our hybrid learning framework can dynamically combine inference
results from an RNN model pre-trained based on historical data and another RNN
model re-trained periodically based on the most recent data. Using real-world
and simulated stream datasets, our experiments show the proposed edge-cloud
deployment is the best among all three deployment types in terms of latency.
For accuracy, the experiments show our dynamic learning approach performs the
best among all learning approaches for all three concept drift scenarios.",-0.40440053,-0.18390408,-0.094264716,C
5982,"We further study how to deploy

ferences.","Our hybrid learning framework is based

edge and its representation is sent to the cloud for complex in- on the Greengrass runtime.","Since the edge devices are only used for data pre- stream analytics among edge and cloud resources and improve

processing, not for actual machine learning based inference, their accuracies.",2022-05-10 01:56:16+00:00,An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics,cs.DC,"['cs.DC', 'cs.LG', 'cs.NI']","[arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Azim Khan'), arxiv.Result.Author('Jianwu Wang'), arxiv.Result.Author('Aryya Gangopadhyay'), arxiv.Result.Author('Carl E. Busart'), arxiv.Result.Author('Jade Freeman')]","With the popularity of Internet of Things (IoT), edge computing and cloud
computing, more and more stream analytics applications are being developed
including real-time trend prediction and object detection on top of IoT sensing
data. One popular type of stream analytics is the recurrent neural network
(RNN) deep learning model based time series or sequence data prediction and
forecasting. Different from traditional analytics that assumes data are
available ahead of time and will not change, stream analytics deals with data
that are being generated continuously and data trend/distribution could change
(a.k.a. concept drift), which will cause prediction/forecasting accuracy to
drop over time. One other challenge is to find the best resource provisioning
for stream analytics to achieve good overall latency. In this paper, we study
how to best leverage edge and cloud resources to achieve better accuracy and
latency for stream analytics using a type of RNN model called long short-term
memory (LSTM). We propose a novel edge-cloud integrated framework for hybrid
stream analytics that supports low latency inference on the edge and high
capacity training on the cloud. To achieve flexible deployment, we study
different approaches of deploying our hybrid learning framework including
edge-centric, cloud-centric and edge-cloud integrated. Further, our hybrid
learning framework can dynamically combine inference results from an LSTM model
pre-trained based on historical data and another LSTM model re-trained
periodically based on the most recent data. Using real-world and simulated
stream datasets, our experiments show the proposed edge-cloud deployment is the
best among all three deployment types in terms of latency. For accuracy, the
experiments show our dynamic learning approach performs the best among all
learning approaches for all three concept drift scenarios.",-0.29396546,-0.27294165,-0.062599495,C
6001,"Apart from presenting the partial agreement
problem and new technical results, the signiﬁcance of the article is in exposing open problems
that hopefully will stimulate further research on the partial agreement problem.","Studying the partial set agreement problem
deﬁned in the introduction would be interesting.","References

  1 I. Abraham, D. Malkhi, and A. Spiegelman.",2022-05-10 13:17:39+00:00,Reaching Agreement Among $k$ out of $n$ Processes,cs.DC,['cs.DC'],[arxiv.Result.Author('Gadi Taubenfeld')],"In agreement problems, each process has an input value and must choose the
input of some process (possibly itself) as a decision value. Given $n\geq 2$
processes and $m \geq 2$ possible different input values, we want to design an
agreement algorithm that enables as many processes as possible to decide on the
same value in the presence of $t$ crash failures. Without communication, when
each process simply decides on its input value, at least $\lceil (n-t)/m
\rceil$ of the processes are guaranteed to always decide on the same value. Can
we do better with communication? For some cases, for example when $m=2$, even
in the presence of a single crash failure, the answer is negative in a
deterministic asynchronous system where communication is either by using atomic
read/write registers or by sending and receiving messages. The answer is
positive in other cases.",-0.079895936,0.60211337,0.056359366,B
6002,"Apart from presenting the problem and
the new technical results, the signiﬁcance of the article is in exposing open problems that
hopefully will stimulate further research on the partial agreement problem.","Studying the partial set agreement problem
deﬁned in the introduction would be interesting.","References

  1 I. Abraham, D. Malkhi, and A. Spiegelman.",2022-05-10 13:17:39+00:00,Reaching Agreement Among $k$ out of $n$ Processes,cs.DC,['cs.DC'],[arxiv.Result.Author('Gadi Taubenfeld')],"In agreement problems, each process has an input value and must choose a
decision (output) value. Given $n\geq 2$ processes and $m \geq 2$ possible
different input values, we want to design an agreement algorithm that enables
as many processes as possible to decide on the (same) input value of one of the
processes, in the presence of $t$ crash failures. Without communication, when
each process simply decides on its input value, at least $\lceil (n-t)/m
\rceil$ of the processes are guaranteed to always decide on the same value. Can
we do better with communication? For some cases, for example when $m=2$, even
in the presence of a single crash failure, the answer is negative in a
deterministic asynchronous system where communication is either by using atomic
read/write registers or by sending and receiving messages. The answer is
positive in other cases.",-0.06733324,0.58975273,0.054496005,B
6023,"DISCUSSION AND FUTURE WORK

                                                                         Directions for future work include extending the results
                                                                      to other speciﬁc matrices, e.g., Cauchy matrices and Moore
                                                                      matrices, and to further study algorithms and lower bounds
                                                                      for Vandermonde and Lagrange matrices.",VII.,"Also, incorporating
                                                                      computation and storage constraints of nodes is an interesting
                                                                      direction for future research.",2022-05-10 21:43:49+00:00,All-to-All Encode in Synchronous Systems,cs.DC,"['cs.DC', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Canran Wang'), arxiv.Result.Author('Netanel Raviv')]","We define all-to-all encode, a collective communication operation serving as
a primitive in decentralized computation and storage systems. Consider a
scenario where every processor initially has a data packet and requires a
linear combination of all data packets; the linear combinations are distinct
from one processor to another, and are specified by a generator matrix of an
error correcting code. We use a linear network model, in which processors
transmit linear combinations of their data and previously received packets, and
adopt a standard synchronous system setting to analyze its communication cost.
We provide a universal algorithm which computes any matrix in this model by
only varying intermediate coefficients, and prove its optimality. When the
generator matrix is of the Vandermonde or Lagrange type, we further optimize
the communication efficiency of the proposed algorithm.",0.28130466,0.10922742,0.032221973,A
6024,"to other speciﬁc matrices, e.g., Cauchy matrices and Moore
                                                                      matrices, and to further study algorithms and lower bounds
   Together, the inverse of a Vandermonde matrix can be               for Vandermonde and Lagrange matrices.","In the draw phase, since the Vandermonde matrix V
deﬁned in Equation (15) is invertible, this step can be inverted         Directions for future work include extending the results
by computing the inverse of V using prepare-and-shoot.","Also, incorporating
computed by ﬁrst inversing the loose phase, and computing             computation and storage constraints of processors is an inter-
the inverse of V with prepare-and-shoot.",2022-05-10 21:43:49+00:00,All-to-All Encode in Synchronous Systems,cs.DC,"['cs.DC', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Canran Wang'), arxiv.Result.Author('Netanel Raviv')]","We define all-to-all encode, a collective communication operation serving as
a primitive in decentralized computation and storage systems. Consider a
scenario where every processor initially has a data packet and requires a
linear combination of all data packets; the linear combinations are distinct
from one processor to another, and are specified by a generator matrix of an
error correcting code. We use a linear network model, in which processors
transmit linear combinations of their data and previously received packets, and
adopt a standard synchronous system setting to analyze its communication cost.
We provide a universal algorithm which computes any matrix in this model by
only varying intermediate coefficients, and prove its optimality. When the
generator matrix is of the Vandermonde or Lagrange type, we further optimize
the communication efficiency of the proposed algorithm.",0.33403337,0.12823223,-0.10090098,A
6101,"With this analysis, we argue
                                                   that today’s dependability-centric service mesh design fails at addressing the needs of the different
                                                   types of emerging mobile edge cloud workloads and motivate further research in the directions of
                                                   performance-efﬁcient architectures, stronger QoS guarantees and higher complexity abstractions of
                                                   cloud-native trafﬁc management frameworks.","This paper presents a systematic and qualitative analysis of state-of-the-art service mesh to evaluate
                                                   how suitable its design is for addressing the trafﬁc management needs of performance-demanding
                                                   application workloads hosted in a mobile edge cloud environment.","Keywords service mesh · efﬁcient trafﬁc management · mobile edge cloud · multi-access edge computing ·
                                        performance-demanding applications · software engineering

                                        1 Introduction

                                        The advanced digitalization of industry, enterprise, and society drives increasingly strict and stringent application
                                        performance requirements compared to what is offered by Centralized Cloud (CC) execution environments.",2022-05-12 12:44:52+00:00,A Qualitative Evaluation of Service Mesh-based Traffic Management for Mobile Edge Cloud,cs.DC,['cs.DC'],"[arxiv.Result.Author('Aleksandra Obeso Duque'), arxiv.Result.Author('Cristian Klein'), arxiv.Result.Author('Jinhua Feng'), arxiv.Result.Author('Xuejun Cai'), arxiv.Result.Author('Björn Skubic'), arxiv.Result.Author('Erik Elmroth')]","Service mesh is getting widely adopted as the cloud-native mechanism for
traffic management in microservice-based applications, in particular for
generic IT workloads hosted in more centralized cloud environments.
Performance-demanding applications continue to drive the decentralization of
modern application execution environments, as in the case of mobile edge cloud.
  This paper presents a systematic and qualitative analysis of state-of-the-art
service mesh to evaluate how suitable its design is for addressing the traffic
management needs of performance-demanding application workloads hosted in a
mobile edge cloud environment. With this analysis, we argue that today's
dependability-centric service mesh design fails at addressing the needs of the
different types of emerging mobile edge cloud workloads and motivate further
research in the directions of performance-efficient architectures, stronger QoS
guarantees and higher complexity abstractions of cloud-native traffic
management frameworks.",-0.36256987,-0.26576918,0.10310519,C_centroid
6102,"result of this evaluation, we identify SM strengths, limitations and trade-offs that motivate further research in the area
(see Section 5).","The Service Mesh (SM) is generally split into a
Management Plane (MP), a Control Plane (CP) and a Data Plane (DP).","Our main contributions are two-folded: i. identiﬁcation of strengths, limitations and tradeoffs associated with SM for
MEC, and ii.",2022-05-12 12:44:52+00:00,A Qualitative Evaluation of Service Mesh-based Traffic Management for Mobile Edge Cloud,cs.DC,['cs.DC'],"[arxiv.Result.Author('Aleksandra Obeso Duque'), arxiv.Result.Author('Cristian Klein'), arxiv.Result.Author('Jinhua Feng'), arxiv.Result.Author('Xuejun Cai'), arxiv.Result.Author('Björn Skubic'), arxiv.Result.Author('Erik Elmroth')]","Service mesh is getting widely adopted as the cloud-native mechanism for
traffic management in microservice-based applications, in particular for
generic IT workloads hosted in more centralized cloud environments.
Performance-demanding applications continue to drive the decentralization of
modern application execution environments, as in the case of mobile edge cloud.
  This paper presents a systematic and qualitative analysis of state-of-the-art
service mesh to evaluate how suitable its design is for addressing the traffic
management needs of performance-demanding application workloads hosted in a
mobile edge cloud environment. With this analysis, we argue that today's
dependability-centric service mesh design fails at addressing the needs of the
different types of emerging mobile edge cloud workloads and motivate further
research in the directions of performance-efficient architectures, stronger QoS
guarantees and higher complexity abstractions of cloud-native traffic
management frameworks.",-0.3077859,-0.11858827,-0.048422344,C
6467,"We will further research efﬁcient methods
for sparse training of SE-MoE.","Besides, how to utilize sparse training for large scale
models to obtain better convergence in various tasks is still a seductive topic.","At last, we will enhance our uniﬁed system by collaborating with the resource platform
to perform lower carbon and more environmentally friendly research.",2022-05-20 09:09:27+00:00,SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System,cs.DC,"['cs.DC', 'cs.AI']","[arxiv.Result.Author('Liang Shen'), arxiv.Result.Author('Zhihua Wu'), arxiv.Result.Author('WeiBao Gong'), arxiv.Result.Author('Hongxiang Hao'), arxiv.Result.Author('Yangfan Bai'), arxiv.Result.Author('HuaChao Wu'), arxiv.Result.Author('Xinxuan Wu'), arxiv.Result.Author('Haoyi Xiong'), arxiv.Result.Author('Dianhai Yu'), arxiv.Result.Author('Yanjun Ma')]","With the increasing diversity of ML infrastructures nowadays, distributed
training over heterogeneous computing systems is desired to facilitate the
production of big models. Mixture-of-Experts (MoE) models have been proposed to
lower the cost of training subject to the overall size of models/data through
gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has
made efforts in carrying out large-scale MoE training over heterogeneous
infrastructures, the efficiency of training and inference could be further
improved from several system aspects, including load balancing,
communication/computation efficiency, and memory footprint limits. In this
work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and
Fusion communication over Hierarchical storage, so as to enjoy efficient
parallelisms in various types. For scalable inference in a single node,
especially when the model size is larger than GPU memory, SE-MoE forms the
CPU-GPU memory jointly into a ring of sections to load the model, and executes
the computation tasks across the memory sections in a round-robin manner for
efficient inference. We carried out extensive experiments to evaluate SE-MoE,
where SE-MoE successfully trains a Unified Feature Optimization (UFO) model
with a Sparsely-Gated Mixture-of-Experts model of 12B parameters in 8 days on
48 A100 GPU cards. The comparison against the state-of-the-art shows that
SE-MoE outperformed DeepSpeed with 33% higher throughput (tokens per second) in
training and 13% higher throughput in inference in general. Particularly, under
unbalanced MoE Tasks, e.g., UFO, SE-MoE achieved 64% higher throughput with 18%
lower memory footprints. The code of the framework will be released on:
https://github.com/PaddlePaddle/Paddle.",-0.052014325,-0.03586524,-0.04602039,B
6568,"tures for the runtime prediction as they are often difficult to corre-
They further study which features influence the runtime the most          late with the runtime of real tasks.","Their learning model considers
several types of information, like workflow structure, measured              Lotaru does not include explicit hardware characteristics as fea-
task resource requirements in historical traces, and input file sizes.","Instead, we apply microbench-
and which ones can be omitted.",2022-05-23 10:32:20+00:00,Lotaru: Locally Estimating Runtimes of Scientific Workflow Tasks in Heterogeneous Clusters,cs.DC,['cs.DC'],"[arxiv.Result.Author('Jonathan Bader'), arxiv.Result.Author('Fabian Lehmann'), arxiv.Result.Author('Lauritz Thamsen'), arxiv.Result.Author('Jonathan Will'), arxiv.Result.Author('Ulf Leser'), arxiv.Result.Author('Odej Kao')]","Many scientific workflow scheduling algorithms need to be informed about task
runtimes a-priori to conduct efficient scheduling. In heterogeneous cluster
infrastructures, this problem becomes aggravated because these runtimes are
required for each task-node pair. Using historical data is often not feasible
as logs are typically not retained indefinitely and workloads as well as
infrastructure changes. In contrast, online methods, which predict task
runtimes on specific nodes while the workflow is running, have to cope with the
lack of example runs, especially during the start-up.
  In this paper, we present Lotaru, a novel online method for locally
estimating task runtimes in scientific workflows on heterogeneous clusters.
Lotaru first profiles all nodes of a cluster with a set of short-running and
uniform microbenchmarks. Next, it runs the workflow to be scheduled on the
user's local machine with drastically reduced data to determine important task
characteristics. Based on these measurements, Lotaru learns a Bayesian linear
regression model to predict a task's runtime given the input size and finally
adjusts the predicted runtime specifically for each task-node pair in the
cluster based on the micro-benchmark results. Due to its Bayesian approach,
Lotaru can also compute robust uncertainty estimates and provides them as an
input for advanced scheduling methods.
  Our evaluation with five real-world scientific workflows and different
datasets shows that Lotaru significantly outperforms the baselines in terms of
prediction errors for homogeneous and heterogeneous clusters.",-0.029698042,-0.18265238,-0.0317141,C
7694,POCL [30] implements                     remains as further research.,"Instead, it directly converts the bytecode back              schedulers and hardware schedulers is still a challenge and
to high-level source code (OpenCL).","SPMD-to-MPMD transformation on LLVM IR, but it only
supports OpenCL programs.",2022-06-16 03:14:30+00:00,CuPBoP: CUDA for Parallelized and Broad-range Processors,cs.DC,"['cs.DC', 'cs.AR']","[arxiv.Result.Author('Ruobing Han'), arxiv.Result.Author('Jun Chen'), arxiv.Result.Author('Bhanu Garg'), arxiv.Result.Author('Jeffrey Young'), arxiv.Result.Author('Jaewoong Sim'), arxiv.Result.Author('Hyesoon Kim')]","CUDA is one of the most popular choices for GPU programming, but it can only
be executed on NVIDIA GPUs. Executing CUDA on non-NVIDIA devices not only
benefits the hardware community, but also allows data-parallel computation in
heterogeneous systems. To make CUDA programs portable, some researchers have
proposed using source-to-source translators to translate CUDA to portable
programming languages that can be executed on non-NVIDIA devices. However, most
CUDA translators require additional manual modifications on the translated
code, which imposes a heavy workload on developers. In this paper, CuPBoP is
proposed to execute CUDA on non-NVIDIA devices without relying on any portable
programming languages. Compared with existing work that executes CUDA on
non-NVIDIA devices, CuPBoP does not require manual modification of the CUDA
source code, but it still achieves the highest coverage (69.6%), much higher
than existing frameworks (56.6%) on the Rodinia benchmark. In particular, for
CPU backends, CuPBoP supports several ISAs (e.g., X86, RISC-V, AArch64) and has
close or even higher performance compared with other projects. We also compare
and analyze the performance among CuPBoP, manually optimized OpenMP/MPI
programs, and CUDA programs on the latest Ampere architecture GPU, and show
future directions for supporting CUDA programs on non-NVIDIA devices with high
performance",0.12778795,-0.028775427,0.027441414,A
7977,"The CPU usage and network traffic
increase an average of 0.06%/0.21% and 3.93KB/4.45KB in back-                                   We further study the usability of Deck as compared to a popular hot-
ground/foreground, respectively.","marized in Table 3, Deck incurs negligible standalone overhead: at
most 1.5MB APK size and 1.09/0.64 watts of energy consumption                                   6.5 Usability
in background/foreground.","The overhead is primarily caused                               fix library Tinker [34], which supports dex, library and resources
by the heartbeat messages from devices to Coordinator.",2022-06-23 06:18:57+00:00,Device-centric Federated Analytics At Ease,cs.DC,['cs.DC'],"[arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Junji Qiu'), arxiv.Result.Author('Shangguang Wang'), arxiv.Result.Author('Mengwei Xu')]","Nowadays, high-volume and privacy-sensitive data are generated by mobile
devices, which are better to be preserved on devices and queried on demand.
However, data analysts still lack a uniform way to harness such distributed
on-device data. In this paper, we propose a data querying system, Deck, that
enables flexible device-centric federated analytics. The key idea of Deck is to
bypass the app developers but allow the data analysts to directly submit their
analytics code to run on devices, through a centralized query coordinator
service. Deck provides a list of standard APIs to data analysts and handles
most of the device-specific tasks underneath. Deck further incorporates two key
techniques: (i) a hybrid permission checking mechanism and mandatory
cross-device aggregation to ensure data privacy; (ii) a zero-knowledge
statistical model that judiciously trades off query delay and query resource
expenditure on devices. We fully implement Deck and plug it into 20 popular
Android apps. An in-the-wild deployment on 1,642 volunteers shows that Deck
significantly reduces the query delay by up to 30x compared to baselines. Our
microbenchmarks also demonstrate that the standalone overhead of Deck is
negligible.",-0.079408415,-0.2953555,0.017941799,C
7978,"The CPU usage and network traffic
increase an average of 0.06%/0.21% and 3.93KB/4.45KB in back-                                   We further study the usability of Deck as compared to a popular hot-
ground/foreground, respectively.","marized in Table 3, Deck incurs negligible standalone overhead: at
most 1.5MB APK size and 1.09/0.64 watts of energy consumption                                   6.5 Usability
in background/foreground.","The overhead is primarily caused                               fix library Tinker [34], which supports dex, library and resources
by the heartbeat messages from devices to Coordinator.",2022-06-23 06:18:57+00:00,Device-centric Federated Analytics At Ease,cs.DC,['cs.DC'],"[arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Junji Qiu'), arxiv.Result.Author('Shangguang Wang'), arxiv.Result.Author('Mengwei Xu')]","Nowadays, high-volume and privacy-sensitive data are generated by mobile
devices, which are better to be preserved on devices and queried on demand.
However, data analysts still lack a uniform way to harness such distributed
on-device data. In this paper, we propose a data querying system, Deck, that
enables flexible device-centric federated analytics. The key idea of Deck is to
bypass the app developers but allow the data analysts to directly submit their
analytics code to run on devices, through a centralized query coordinator
service. Deck provides a list of standard APIs to data analysts and handles
most of the device-specific tasks underneath. Deck further incorporates two key
techniques: (i) a hybrid permission checking mechanism and mandatory
cross-device aggregation to ensure data privacy; (ii) a zero-knowledge
statistical model that judiciously trades off query delay and query resource
expenditure on devices. We fully implement Deck and plug it into 20 popular
Android apps. An in-the-wild deployment on 1,642 volunteers shows that Deck
significantly reduces the query delay by up to 30x compared to baselines. Our
microbenchmarks also demonstrate that the standalone overhead of Deck is
negligible.",-0.079408415,-0.2953555,0.017941799,C
8340,Section 5 concludes with further research.,"Section 4 interprets communication pattern logic on simplicial
complexes.","2
2 Structures

Given are a ﬁnite set of agents A and a set of propositional variables P ⊆ P ′ × A, where
P ′ is a countable set.",2022-07-02 12:52:27+00:00,Communication Pattern Logic: Epistemic and Topological Views,cs.DC,"['cs.DC', 'cs.MA']","[arxiv.Result.Author('Armando Castañeda'), arxiv.Result.Author('Hans van Ditmarsch'), arxiv.Result.Author('David A. Rosenblueth'), arxiv.Result.Author('Diego A. Velázquez')]","We propose communication pattern logic. A communication pattern describes how
processes or agents inform each other, independently of the information
content. The full information protocol in distributed computing is the special
case wherein all agents inform each other. We study this protocol in
distributed computing models where communication might fail: an agent is
certain about the messages it receives, but it is uncertain about the messages
other agents have received. In a dynamic epistemic logic with distributed
knowledge and with modalities for communication patterns, the latter are
interpreted by updating Kripke models. We propose an axiomatization of
communication pattern logic, and we show that collective bisimilarity
(comparing models on their distributed knowledge) is preserved when updating
models with communication patterns. We can also interpret communication
patterns by updating simplicial complexes, a well-known topological framework
for distributed computing. We show that the different semantics correspond, and
propose collective bisimulation between simplicial complexes.",-0.0067753815,0.6174927,-0.2360392,B
8341,"5 Conclusion and further research

We presented communication pattern logic to reason about distributed systems wherein
agents communicate to each other all they know.","Having  established  that  (C  ⊘ R, XR)  is  bisimilar  to  (C′   ⊘  R  ,  X  ′  )  ,  we  now  apply  the
                                                                              R

induction hypothesis for ϕ on pointed models (C ⊘R, XR) and (C′ ⊘R, XR′ ) thus obtaining

that (C′ ⊘ R, XR′ ) |= ϕ, so that C′, X′ |= [R, R]ϕ as required.","We provided an original update con-
struction, an axiomatization, gave an alternative semantics on simplicial complexes, and
for both semantics showed preservation after update of collective bisimulation.",2022-07-02 12:52:27+00:00,Communication Pattern Logic: Epistemic and Topological Views,cs.DC,"['cs.DC', 'cs.MA']","[arxiv.Result.Author('Armando Castañeda'), arxiv.Result.Author('Hans van Ditmarsch'), arxiv.Result.Author('David A. Rosenblueth'), arxiv.Result.Author('Diego A. Velázquez')]","We propose communication pattern logic. A communication pattern describes how
processes or agents inform each other, independently of the information
content. The full information protocol in distributed computing is the special
case wherein all agents inform each other. We study this protocol in
distributed computing models where communication might fail: an agent is
certain about the messages it receives, but it is uncertain about the messages
other agents have received. In a dynamic epistemic logic with distributed
knowledge and with modalities for communication patterns, the latter are
interpreted by updating Kripke models. We propose an axiomatization of
communication pattern logic, and we show that collective bisimilarity
(comparing models on their distributed knowledge) is preserved when updating
models with communication patterns. We can also interpret communication
patterns by updating simplicial complexes, a well-known topological framework
for distributed computing. We show that the different semantics correspond, and
propose collective bisimulation between simplicial complexes.",-0.023510259,0.5756311,-0.09704237,B
8342,Let us shortly describe three areas for further research.,"We provided an original update con-
struction, an axiomatization, gave an alternative semantics on simplicial complexes, and
for both semantics showed preservation after update of collective bisimulation.","Update expressity We wish to compare the update expressivity of communication pat-
tern logic and action model logic [5].",2022-07-02 12:52:27+00:00,Communication Pattern Logic: Epistemic and Topological Views,cs.DC,"['cs.DC', 'cs.MA']","[arxiv.Result.Author('Armando Castañeda'), arxiv.Result.Author('Hans van Ditmarsch'), arxiv.Result.Author('David A. Rosenblueth'), arxiv.Result.Author('Diego A. Velázquez')]","We propose communication pattern logic. A communication pattern describes how
processes or agents inform each other, independently of the information
content. The full information protocol in distributed computing is the special
case wherein all agents inform each other. We study this protocol in
distributed computing models where communication might fail: an agent is
certain about the messages it receives, but it is uncertain about the messages
other agents have received. In a dynamic epistemic logic with distributed
knowledge and with modalities for communication patterns, the latter are
interpreted by updating Kripke models. We propose an axiomatization of
communication pattern logic, and we show that collective bisimilarity
(comparing models on their distributed knowledge) is preserved when updating
models with communication patterns. We can also interpret communication
patterns by updating simplicial complexes, a well-known topological framework
for distributed computing. We show that the different semantics correspond, and
propose collective bisimulation between simplicial complexes.",-0.07549676,0.55360955,-0.079362184,B
8484,"We further study the full
for those application domains whose data sizes grow rapidly but           set of HiBench applications and realize that most of them cache a
need to pass over the same data production pipelines [16, 29, 36].",This is particularly important      because only one dataset is cached in svm.,"single dataset, at most.",2022-07-05 20:31:35+00:00,Blink: Lightweight Sample Runs for Cost Optimization of Big Data Applications,cs.DC,"['cs.DC', 'cs.DB']","[arxiv.Result.Author('Hani Al-Sayeh'), arxiv.Result.Author('Muhammad Attahir Jibril'), arxiv.Result.Author('Bunjamin Memishi'), arxiv.Result.Author('Kai-Uwe Sattler')]","Distributed in-memory data processing engines accelerate iterative
applications by caching substantial datasets in memory rather than recomputing
them in each iteration. Selecting a suitable cluster size for caching these
datasets plays an essential role in achieving optimal performance. In practice,
this is a tedious and hard task for end users, who are typically not aware of
cluster specifications, workload semantics and sizes of intermediate data.
  We present Blink, an autonomous sampling-based framework, which predicts
sizes of cached datasets and selects optimal cluster size without relying on
historical runs. We evaluate Blink on a variety of iterative, real-world,
machine learning applications. With an average sample runs cost of 4.6%
compared to the cost of optimal runs, Blink selects the optimal cluster size in
15 out of 16 cases, saving up to 47.4% of execution cost compared to average
costs.",-0.06264513,-0.15324458,0.010449035,C
8666,"output cell                                                output cell
                                                                              sample point                                               sample point
(4) We are releasing our implementation as open-source1
     for further research and use in achieving eﬃcient grid-                                (a) Scatter                      (b) Gather
     ding of astronomical data for current and upcoming
     large single-dish radio telescopes.","as NVIDIA and AMD Radeon Instinct series, enabling
     HEGrid with robust hardware portability.","Figure 2: Input-oriented scatter (left) and output-oriented
                                                                 gather (right) gridding methods.",2022-07-11 02:18:25+00:00,HEGrid: A High Efficient Multi-Channel Radio Astronomical Data Gridding Framework in Heterogeneous Computing Environments,cs.DC,"['cs.DC', 'I.4.9; J.2']","[arxiv.Result.Author('Hao Wang'), arxiv.Result.Author('Ce Yu'), arxiv.Result.Author('Jian Xiao'), arxiv.Result.Author('Shanjiang Tang'), arxiv.Result.Author('Min Long'), arxiv.Result.Author('Ming Zhu')]","The challenge to fully exploit the potential of existing and upcoming
scientific instruments like large single-dish radio telescopes is to process
the collected massive data effectively and efficiently. As a ""quasi 2D stencil
computation"" with the ""Moore neighborhood pattern,"" gridding is the most
computationally intensive step in data reduction pipeline for radio astronomy
studies, enabling astronomers to create correct sky images for further
analysis. However, the existing gridding frameworks can either only run on
multi-core CPU architecture or do not support high-concurrency, multi-channel
data gridding. Their performance is then limited, and there are emerging needs
for innovative gridding frameworks to process data from large single-dish radio
telescopes like the Five-hundred-meter Aperture Spherical Telescope (FAST). To
address those challenges, we developed a High Efficient Gridding framework,
HEGrid, by overcoming the above limitations. Specifically, we propose and
construct the gridding pipeline in heterogeneous computing environments and
achieve multi-pipeline concurrency for high performance multi-channel
processing. Furthermore, we propose pipeline-based co-optimization to alleviate
the potential negative performance impact of possible intra- and inter-pipeline
low computation and I/O utilization, including component share-based redundancy
elimination, thread-level data reuse and overlapping I/O and computation. Our
experiments are based on both simulated datasets and actual FAST observational
datasets. The results show that HEGrid outperforms other state-of-the-art
gridding frameworks by up to 5.5x and has robust hardware portability,
including AMD Radeon Instinct GPU and NVIDIA GPU.",0.19311729,-0.11105877,-0.29503065,A
9535,"For further research, we aim to apply our approach to different distributed problems, e.g.","Additionally, this is the ﬁrst work that uses a
machine learning approach to generate correct and efﬁcient algorithms to solve a speciﬁc distributed problem.","Consensus, and try to
decrease the number of inputs needed, to further decouple our agent from knowledge based on previous works, e.g.",2022-07-31 21:45:20+00:00,Learning to generate Reliable Broadcast Algorithms,cs.DC,"['cs.DC', 'cs.DS', 'cs.LG', 'cs.NI']","[arxiv.Result.Author('Diogo Vaz'), arxiv.Result.Author('David R. Matos'), arxiv.Result.Author('Miguel L. Pardal'), arxiv.Result.Author('Miguel Correia')]","Modern distributed systems are supported by fault-tolerant algorithms, like
Reliable Broadcast and Consensus, that assure the correct operation of the
system even when some of the nodes of the system fail. However, the development
of distributed algorithms is a manual and complex process, resulting in
scientific papers that usually present a single algorithm or variations of
existing ones. To automate the process of developing such algorithms, this work
presents an intelligent agent that uses Reinforcement Learning to generate
correct and efficient fault-tolerant distributed algorithms. We show that our
approach is able to generate correct fault-tolerant Reliable Broadcast
algorithms with the same performance of others available in the literature, in
only 12,000 learning episodes.",-0.14354295,0.23097745,0.21302462,B_centroid
9733,"As pointed out earlier, further research is needed            [6] Usman A Khan, Soummya Kar, Ali Jadbabaie, and Jos´e M.F.","The third simulation provides an example of this capa-
bility.","to more fully understand observer resilience, especially                    Moura.",2022-08-05 00:16:24+00:00,A Hybrid Observer for Estimating the State of a Distributed Linear System,cs.DC,['cs.DC'],"[arxiv.Result.Author('Lili Wang'), arxiv.Result.Author('Ji Liu'), arxiv.Result.Author('A. Stephen Morse')]","A hybrid observer is described for estimating the state of a system of the
form dot x=Ax, y_i=C_ix, i=1,...,m. The system's state x is simultaneously
estimated by m agents assuming agent i senses y_i and receives appropriately
defined data from its neighbors. Neighbor relations are characterized by a
time-varying directed graph N(t). Agent i updates its estimate x_i of x at
event times t_{i1},t_{i2} ... using a local continuous-time linear observer and
a local parameter estimator which iterates q times during each event time
interval [t_{i(s-1)},t_{is}), s>=1, to obtain an estimate of x(t_{is}). Subject
to the assumptions that N(t) is strongly connected, and the system is jointly
observable, it is possible to design parameters so that x_i converges to x with
a pre-assigned rate. This result holds when agents communicate asynchronously
with the assumption that N(t) changes slowly. Exponential convergence is also
assured if the event time sequence of the agents are slightly different,
although only if the system being observed is exponentially stable; this
limitation however, is a robustness issue shared by all open loop state
estimators with small modeling errors. The result also holds facing abrupt
changes in the number of vertices and arcs in the inter-agent communication
graph upon which the algorithm depends.",-0.104873314,0.31686267,-0.3783167,B
9734,"As pointed out earlier, further research is needed            [6] Usman A Khan, Soummya Kar, Ali Jadbabaie, and Jos´e M.F.","The third simulation provides an example of this capa-
bility.","to more fully understand observer resilience, especially                    Moura.",2022-08-05 00:16:24+00:00,A Hybrid Observer for Estimating the State of a Distributed Linear System,cs.DC,['cs.DC'],"[arxiv.Result.Author('Lili Wang'), arxiv.Result.Author('Ji Liu'), arxiv.Result.Author('A. Stephen Morse')]","A hybrid observer is described for estimating the state of a system of the
form dot x=Ax, y_i=C_ix, i=1,...,m. The system's state x is simultaneously
estimated by m agents assuming agent i senses y_i and receives appropriately
defined data from its neighbors. Neighbor relations are characterized by a
time-varying directed graph N(t). Agent i updates its estimate x_i of x at
event times t_{i1},t_{i2} ... using a local continuous-time linear observer and
a local parameter estimator which iterates q times during each event time
interval [t_{i(s-1)},t_{is}), s>=1, to obtain an estimate of x(t_{is}). Subject
to the assumptions that N(t) is strongly connected, and the system is jointly
observable, it is possible to design parameters so that x_i converges to x with
a pre-assigned rate. This result holds when agents communicate asynchronously
with the assumption that N(t) changes slowly. Exponential convergence is also
assured if the event time sequence of the agents are slightly different,
although only if the system being observed is exponentially stable; this
limitation however, is a robustness issue shared by all open loop state
estimators with small modeling errors. The result also holds facing abrupt
changes in the number of vertices and arcs in the inter-agent communication
graph upon which the algorithm depends.",-0.104873314,0.31686267,-0.3783167,B
10119,"While we believe that the modeling framework discussed in this article is a
sound basis for the formal study of digital circuits and even biological systems, it
currently lacks several important features that are left open for further research:

    The ﬁrst one is the choice of a formal language for describing signals and
module speciﬁcations.","We also showed that it may create some
subtle artefacts when composing modules, which need careful consideration and
possibly mitigation.","Whereas our simple signals could of course be described

7 Since we can consider the inertial delay channel to be a basic module here, we need
   not care about implementability at that point.",2022-08-17 08:24:09+00:00,On Specifications and Proofs of Timed Circuits,cs.DC,"['cs.DC', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Matthias Fuegger'), arxiv.Result.Author('Christoph Lenzen'), arxiv.Result.Author('Ulrich Schmid')]","Given a discrete-state continuous-time reactive system, like a digital
circuit, the classical approach is to first model it as a state transition
system and then prove its properties. Our contribution advocates a different
approach: to directly operate on the input-output behavior of such systems,
without identifying states and their transitions in the first place. We discuss
the benefits of this approach at hand of some examples, which demonstrate that
it nicely integrates with concepts of self-stabilization and fault-tolerance.
We also elaborate on some unexpected artefacts of module composition in our
framework, and conclude with some open research questions.",0.028805574,0.32260358,-0.15869686,B
10161,"R. F. Kharal and T. Brown                                                                      21

 8 Conclusions

We hope this work encourages further research into how best to design benchmarks for
concurrent data structures.","A wide range of
    additional capabilities are documented in extensive Jupyter notebook tutorials.","Setbench was carefully designed to mitigate many of the problems
we are aware of, but there are surely more benchmarking pitfalls yet to be discovered in this
area.",2022-08-17 18:15:27+00:00,Performance Anomalies in Concurrent Data Structure Microbenchmarks,cs.DC,['cs.DC'],"[arxiv.Result.Author('Rosina F. Kharal'), arxiv.Result.Author('Trevor Brown')]","Recent decades have witnessed a surge in the development of concurrent data
structures with an increasing interest in data structures implementing
concurrent sets (CSets). Microbenchmarking tools are frequently utilized to
evaluate and compare the performance differences across concurrent data
structures. The underlying structure and design of the microbenchmarks
themselves can play a hidden but influential role in performance results.
However, the impact of microbenchmark design has not been well investigated. In
this work, we illustrate instances where concurrent data structure performance
results reported by a microbenchmark can vary 10-100x depending on the
microbenchmark implementation details. We investigate factors leading to
performance variance across three popular microbenchmarks and outline cases in
which flawed microbenchmark design can lead to an inversion of performance
results between two concurrent data structure implementations. We further
derive a set of recommendations for best practices in the design and usage of
concurrent data structure microbenchmarks and explore advanced features in the
Setbench microbenchmark.",0.14902687,0.014142321,0.13590254,A
10394,"Speedup and Energy Eﬃciency for 3-simplices
      GPU: A100, n = 99328 (62464 for CA2D) 107
                                                                                                                                   In the case of 3-simplices, the RB approach is discarded be-
                                                                       106           Energy Efficiency (EPS/W)                cause it cannot be extended eﬃciently to 3D, as n/2 × n/2 × n
                                                                                                                              orthotope is double the necessary space and the arithmetic oper-
                                                                       105                                                    ations involved would require further research as it will be more
                                                                                                                              complex.",2-Simplex - Energy Efficiency                                                                                      5.3.,"The map λ is also discarded, because by deﬁnition it
                                                                       104                                                    extends to another 3-simplex where i < j < k [24], and not the
                                                                                                                              standard one where i + j + k ≤ n satisﬁes.",2022-08-24 15:39:21+00:00,A Scalable and Energy Efficient GPU Thread Map for Standard m-Simplex Domains,cs.DC,"['cs.DC', 'cs.DM']","[arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Felipe A. Quezada'), arxiv.Result.Author('Benjamin Bustos'), arxiv.Result.Author('Nancy Hitschfeld'), arxiv.Result.Author('Rolando Kindelan')]","This work proposes a new GPU thread map for standard $m$-simplex domains,
that scales its speedup with dimension and is energy efficient compared to
other state of the art approaches. The main contributions of this work are the
formulation of the new block-space map $\mathcal{H}: \mathbb{Z}^m \mapsto
\mathbb{Z}^m$ which is analyzed in terms of resource usage, and its
experimental evaluation in terms of speedup over a bounding box approach and
energy efficiency as elements per second per Watt. Results from the analysis
show that $\mathcal{H}$ has a potential speedup of up to $2\times$ and
$6\times$ for $2$ and $3$-simplices, respectively. Experimental evaluation
shows that $\mathcal{H}$ is competitive for $2$-simplices, reaching $1.2\times
\sim 2.0\times$ of speedup for different tests, which is on par with the
fastest state of the art approaches. For $3$-simplices $\mathcal{H}$ reaches up
to $1.3\times \sim 6.0\times$ of speedup making it the fastest of all. The
extension of $\mathcal{H}$ to higher dimensional $m$-simplices is feasible and
has a potential speedup that scales as $m!$ given a proper selection of
parameters $r, \beta$ which are the scaling and replication factors,
respectively. In terms of energy consumption, although $\mathcal{H}$ is among
the highest in power consumption, it compensates by its short duration, making
it one of the most energy efficient approaches. Lastly, further improvements
with Tensor and Ray Tracing Cores are analyzed, giving insights to leverage
each one of them. The results obtained in this work show that $\mathcal{H}$ is
a scalable and energy efficient map that can contribute to the efficiency of
GPU applications when they need to process standard $m$-simplex domains, such
as Cellular Automata or PDE simulations.",0.57895964,0.0040317755,-0.274141,A
10395,"are the optimal for building a self-similar set of orthotopes for

                                                                            any  m-simplex,  as  well  as  ﬁnd  general  methods  for  packing  S  m
                                                                                                                                                   n

                                                                            into super-orthotope are indeed interesting questions that de-

                                                                            serve further study.","Knowing what parameters
cluster construct, which allows to group several CUDA blocks
and synchronize them to share data, such as matrix fragments.","In terms of power consumption, H showed that although it

                                                                            achieves high power consumption during execution, it compen-

7.2.",2022-08-24 15:39:21+00:00,A Scalable and Energy Efficient GPU Thread Map for Standard m-Simplex Domains,cs.DC,"['cs.DC', 'cs.DM']","[arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Felipe A. Quezada'), arxiv.Result.Author('Benjamin Bustos'), arxiv.Result.Author('Nancy Hitschfeld'), arxiv.Result.Author('Rolando Kindelan')]","This work proposes a new GPU thread map for standard $m$-simplex domains,
that scales its speedup with dimension and is energy efficient compared to
other state of the art approaches. The main contributions of this work are the
formulation of the new block-space map $\mathcal{H}: \mathbb{Z}^m \mapsto
\mathbb{Z}^m$ which is analyzed in terms of resource usage, and its
experimental evaluation in terms of speedup over a bounding box approach and
energy efficiency as elements per second per Watt. Results from the analysis
show that $\mathcal{H}$ has a potential speedup of up to $2\times$ and
$6\times$ for $2$ and $3$-simplices, respectively. Experimental evaluation
shows that $\mathcal{H}$ is competitive for $2$-simplices, reaching $1.2\times
\sim 2.0\times$ of speedup for different tests, which is on par with the
fastest state of the art approaches. For $3$-simplices $\mathcal{H}$ reaches up
to $1.3\times \sim 6.0\times$ of speedup making it the fastest of all. The
extension of $\mathcal{H}$ to higher dimensional $m$-simplices is feasible and
has a potential speedup that scales as $m!$ given a proper selection of
parameters $r, \beta$ which are the scaling and replication factors,
respectively. In terms of energy consumption, although $\mathcal{H}$ is among
the highest in power consumption, it compensates by its short duration, making
it one of the most energy efficient approaches. Lastly, further improvements
with Tensor and Ray Tracing Cores are analyzed, giving insights to leverage
each one of them. The results obtained in this work show that $\mathcal{H}$ is
a scalable and energy efficient map that can contribute to the efficiency of
GPU applications when they need to process standard $m$-simplex domains, such
as Cellular Automata or PDE simulations.",0.43419138,-0.13808669,-0.17036742,A
10396,"Speedup and Energy Eﬃciency for 3-simplices
      GPU: A100, n = 99328 (62464 for CA2D) 107
                                                                                                                                   In the case of 3-simplices, the RB approach is discarded be-
                                                                       106           Energy Efficiency (EPS/W)                cause it cannot be extended eﬃciently to 3D, as n/2 × n/2 × n
                                                                                                                              orthotope is double the necessary space and the arithmetic oper-
                                                                       105                                                    ations involved would require further research as it will be more
                                                                                                                              complex.",2-Simplex - Energy Efficiency                                                                                      5.3.,"The map λ is also discarded, because by deﬁnition it
                                                                       104                                                    extends to another 3-simplex where i < j < k [24], and not the
                                                                                                                              one deﬁned in this work where i + j + k ≤ n satisﬁes.",2022-08-24 15:39:21+00:00,A Scalable and Energy Efficient GPU Thread Map for m-Simplex Domains,cs.DC,"['cs.DC', 'cs.DM']","[arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Felipe A. Quezada'), arxiv.Result.Author('Benjamin Bustos'), arxiv.Result.Author('Nancy Hitschfeld'), arxiv.Result.Author('Rolando Kindelan')]","This work proposes a new GPU thread map for $m$-simplex domains, that scales
its speedup with dimension and is energy efficient compared to other state of
the art approaches. The main contributions of this work are i) the formulation
of the new block-space map $\mathcal{H}: \mathbb{Z}^m \mapsto \mathbb{Z}^m$ for
regular orthogonal simplex domains, which is analyzed in terms of resource
usage, and ii) the experimental evaluation in terms of speedup over a bounding
box approach and energy efficiency as elements per second per Watt. Results
from the analysis show that $\mathcal{H}$ has a potential speedup of up to
$2\times$ and $6\times$ for $2$ and $3$-simplices, respectively. Experimental
evaluation shows that $\mathcal{H}$ is competitive for $2$-simplices, reaching
$1.2\times \sim 2.0\times$ of speedup for different tests, which is on par with
the fastest state of the art approaches. For $3$-simplices $\mathcal{H}$
reaches up to $1.3\times \sim 6.0\times$ of speedup making it the fastest of
all. The extension of $\mathcal{H}$ to higher dimensional $m$-simplices is
feasible and has a potential speedup that scales as $m!$ given a proper
selection of parameters $r, \beta$ which are the scaling and replication
factors, respectively. In terms of energy consumption, although $\mathcal{H}$
is among the highest in power consumption, it compensates by its short
duration, making it one of the most energy efficient approaches. Lastly,
further improvements with Tensor and Ray Tracing Cores are analyzed, giving
insights to leverage each one of them. The results obtained in this work show
that $\mathcal{H}$ is a scalable and energy efficient map that can contribute
to the efficiency of GPU applications when they need to process $m$-simplex
domains, such as Cellular Automata or PDE simulations.",0.57306147,0.0039920853,-0.25306177,A
10397,"are the optimal for building a self-similar set of orthotopes for

                                                                            any  m-simplex,  as  well  as  ﬁnd  general  methods  for  packing  S  m
                                                                                                                                                   n

                                                                            into super-orthotope are indeed interesting questions that de-

                                                                            serve further study.","Knowing what parameters
cluster construct, which allows to group several CUDA blocks
and synchronize them to share data, such as matrix fragments.","In terms of power consumption, H showed that although it

                                                                            achieves high power consumption during execution, it compen-

7.2.",2022-08-24 15:39:21+00:00,A Scalable and Energy Efficient GPU Thread Map for m-Simplex Domains,cs.DC,"['cs.DC', 'cs.DM']","[arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Felipe A. Quezada'), arxiv.Result.Author('Benjamin Bustos'), arxiv.Result.Author('Nancy Hitschfeld'), arxiv.Result.Author('Rolando Kindelan')]","This work proposes a new GPU thread map for $m$-simplex domains, that scales
its speedup with dimension and is energy efficient compared to other state of
the art approaches. The main contributions of this work are i) the formulation
of the new block-space map $\mathcal{H}: \mathbb{Z}^m \mapsto \mathbb{Z}^m$ for
regular orthogonal simplex domains, which is analyzed in terms of resource
usage, and ii) the experimental evaluation in terms of speedup over a bounding
box approach and energy efficiency as elements per second per Watt. Results
from the analysis show that $\mathcal{H}$ has a potential speedup of up to
$2\times$ and $6\times$ for $2$ and $3$-simplices, respectively. Experimental
evaluation shows that $\mathcal{H}$ is competitive for $2$-simplices, reaching
$1.2\times \sim 2.0\times$ of speedup for different tests, which is on par with
the fastest state of the art approaches. For $3$-simplices $\mathcal{H}$
reaches up to $1.3\times \sim 6.0\times$ of speedup making it the fastest of
all. The extension of $\mathcal{H}$ to higher dimensional $m$-simplices is
feasible and has a potential speedup that scales as $m!$ given a proper
selection of parameters $r, \beta$ which are the scaling and replication
factors, respectively. In terms of energy consumption, although $\mathcal{H}$
is among the highest in power consumption, it compensates by its short
duration, making it one of the most energy efficient approaches. Lastly,
further improvements with Tensor and Ray Tracing Cores are analyzed, giving
insights to leverage each one of them. The results obtained in this work show
that $\mathcal{H}$ is a scalable and energy efficient map that can contribute
to the efficiency of GPU applications when they need to process $m$-simplex
domains, such as Cellular Automata or PDE simulations.",0.43419138,-0.13808669,-0.17036742,A
10398,"Speedup and Energy Eﬃciency for 3-simplices

GPU: A100, n = 99328 (62464 for CA2D) 107                                                                                              In the case of 3-simplices, the RB approach is discarded be-
                                                                                                                                  cause it cannot be extended eﬃciently to 3D, as n/2 × n/2 × n
                                                                              106   Energy Efficiency (EPS/W)                     orthotope is double the necessary space and the arithmetic op-
                                                                                                                                  erations involved would require further research as it will be
                                                                              105                                                 more complex.",2-Simplex - Energy Efficiency                                                                                                5.3.,"The map λ is also discarded because it gener-
                                                                                                                                  ates a 3-simplex (i < j < k condition) in which no vertex has all
                                                                              104                                                 of its incident facets orthogonal [24], breaking the x + y + z ≤ n
                                                                                                                                  condition.",2022-08-24 15:39:21+00:00,A Scalable and Energy Efficient GPU Thread Map for m-Simplex Domains,cs.DC,"['cs.DC', 'cs.DM']","[arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Felipe A. Quezada'), arxiv.Result.Author('Benjamin Bustos'), arxiv.Result.Author('Nancy Hitschfeld'), arxiv.Result.Author('Rolando Kindelan')]","This work proposes a new GPU thread map for $m$-simplex domains, that scales
its speedup with dimension and is energy efficient compared to other state of
the art approaches. The main contributions of this work are i) the formulation
of the new block-space map $\mathcal{H}: \mathbb{Z}^m \mapsto \mathbb{Z}^m$ for
regular orthogonal simplex domains, which is analyzed in terms of resource
usage, and ii) the experimental evaluation in terms of speedup over a bounding
box approach and energy efficiency as elements per second per Watt. Results
from the analysis show that $\mathcal{H}$ has a potential speedup of up to
$2\times$ and $6\times$ for $2$ and $3$-simplices, respectively. Experimental
evaluation shows that $\mathcal{H}$ is competitive for $2$-simplices, reaching
$1.2\times \sim 2.0\times$ of speedup for different tests, which is on par with
the fastest state of the art approaches. For $3$-simplices $\mathcal{H}$
reaches up to $1.3\times \sim 6.0\times$ of speedup making it the fastest of
all. The extension of $\mathcal{H}$ to higher dimensional $m$-simplices is
feasible and has a potential speedup that scales as $m!$ given a proper
selection of parameters $r, \beta$ which are the scaling and replication
factors, respectively. In terms of energy consumption, although $\mathcal{H}$
is among the highest in power consumption, it compensates by its short
duration, making it one of the most energy efficient approaches. Lastly,
further improvements with Tensor and Ray Tracing Cores are analyzed, giving
insights to leverage each one of them. The results obtained in this work show
that $\mathcal{H}$ is a scalable and energy efficient map that can contribute
to the efficiency of GPU applications when they need to process $m$-simplex
domains, such as Cellular Automata or PDE simulations.",0.58920527,-0.009611316,-0.27471048,A
10399,"are the optimal for building a self-similar set of orthotopes for

                                                                            any  m-simplex,  as  well  as  ﬁnd  general  methods  for  packing  S  m
                                                                                                                                                   n

                                                                            into super-orthotope are indeed interesting questions that de-

                                                                            serve further study.","Knowing what parameters
cluster construct, which allows to group several CUDA blocks
and synchronize them to share data, such as matrix fragments.","In terms of power consumption, H showed that although it

                                                                            achieves high power consumption during execution, it compen-

7.2.",2022-08-24 15:39:21+00:00,A Scalable and Energy Efficient GPU Thread Map for m-Simplex Domains,cs.DC,"['cs.DC', 'cs.DM']","[arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Felipe A. Quezada'), arxiv.Result.Author('Benjamin Bustos'), arxiv.Result.Author('Nancy Hitschfeld'), arxiv.Result.Author('Rolando Kindelan')]","This work proposes a new GPU thread map for $m$-simplex domains, that scales
its speedup with dimension and is energy efficient compared to other state of
the art approaches. The main contributions of this work are i) the formulation
of the new block-space map $\mathcal{H}: \mathbb{Z}^m \mapsto \mathbb{Z}^m$ for
regular orthogonal simplex domains, which is analyzed in terms of resource
usage, and ii) the experimental evaluation in terms of speedup over a bounding
box approach and energy efficiency as elements per second per Watt. Results
from the analysis show that $\mathcal{H}$ has a potential speedup of up to
$2\times$ and $6\times$ for $2$ and $3$-simplices, respectively. Experimental
evaluation shows that $\mathcal{H}$ is competitive for $2$-simplices, reaching
$1.2\times \sim 2.0\times$ of speedup for different tests, which is on par with
the fastest state of the art approaches. For $3$-simplices $\mathcal{H}$
reaches up to $1.3\times \sim 6.0\times$ of speedup making it the fastest of
all. The extension of $\mathcal{H}$ to higher dimensional $m$-simplices is
feasible and has a potential speedup that scales as $m!$ given a proper
selection of parameters $r, \beta$ which are the scaling and replication
factors, respectively. In terms of energy consumption, although $\mathcal{H}$
is among the highest in power consumption, it compensates by its short
duration, making it one of the most energy efficient approaches. Lastly,
further improvements with Tensor and Ray Tracing Cores are analyzed, giving
insights to leverage each one of them. The results obtained in this work show
that $\mathcal{H}$ is a scalable and energy efficient map that can contribute
to the efficiency of GPU applications when they need to process $m$-simplex
domains, such as Cellular Automata or PDE simulations.",0.43419138,-0.13808669,-0.17036742,A
10621,"Parallel Dynamic Array
                                                                   E. Speciﬁc GPU Dynamic Applications
   Lock-Free Vector (LFVector) [6] was the ﬁrst proposed
parallel dynamic array and the catalyst for further research          When a GPU application requires a dynamic array or similar
on them.",B.,"It proposes and idea similar to doubling arrays by        solution, many times it implements an speciﬁc and hand-
duplicating the size each time more memory is needed.",2022-08-31 20:41:15+00:00,GGArray: A Dynamically Growable GPU Array,cs.DC,['cs.DC'],"[arxiv.Result.Author('Enzo meneses'), arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Héctor Ferrada')]","We present a dynamically Growable GPU array (GGArray) fully implemented in
GPU that does not require synchronization with the host. The idea is to improve
the programming of GPU applications that require dynamic memory, by offering a
structure that does not require pre-allocating GPU VRAM for the worst case
scenario. The GGArray is based on the LFVector, by utilizing an array of them
in order to take advantage of the GPU architecture and the synchronization
offered by thread blocks. This structure is compared to other state of the art
ones such as a pre-allocated static array and a semi-static array that needs to
be resized through communication with the host. Experimental evaluation shows
that the GGArray has a competitive insertion and resize performance, but it is
slower for regular parallel memory accesses. Given the results, the GGArray is
a potentially useful structure for applications with high uncertainty on the
memory usage as well as applications that have phases, such as an insertion
phase followed by a regular GPU phase. In such cases, the GGArray can be used
for the first phase and then data can be flattened for the second phase in
order to allow the classical GPU memory accesses which are faster. These
results constitute a step towards achieving a parallel efficient C++ like
vector for modern GPU architectures.",0.3482126,-0.1464433,0.090265386,A
10622,"General Parallel Dynamic Array                                  E. Speciﬁc GPU Dynamic Applications

   Lock-Free Vector (LFVector) [6] was the ﬁrst proposed              When a GPU application requires a dynamic array or similar
parallel dynamic array and the catalyst for further research on    solution, many times it implements an speciﬁc and hand-
them.",B.,It proposes an idea similar to doubling arrays by dupli-     tailored structure that suits the application needs.,2022-08-31 20:41:15+00:00,GGArray: A Dynamically Growable GPU Array,cs.DC,['cs.DC'],"[arxiv.Result.Author('Enzo Meneses'), arxiv.Result.Author('Cristóbal A. Navarro'), arxiv.Result.Author('Héctor Ferrada')]","We present a dynamically Growable GPU array (GGArray) fully implemented in
GPU that does not require synchronization with the host. The idea is to improve
the programming of GPU applications that require dynamic memory, by offering a
structure that does not require pre-allocating GPU VRAM for the worst case
scenario. The GGArray is based on the LFVector, by utilizing an array of them
in order to take advantage of the GPU architecture and the synchronization
offered by thread blocks. This structure is compared to other state of the art
ones such as a pre-allocated static array and a semi-static array that needs to
be resized through communication with the host. Experimental evaluation shows
that the GGArray has a competitive insertion and resize performance, but it is
slower for regular parallel memory accesses. Given the results, the GGArray is
a potentially useful structure for applications with high uncertainty on the
memory usage as well as applications that have phases, such as an insertion
phase followed by a regular GPU phase. In such cases, the GGArray can be used
for the first phase and then data can be flattened for the second phase in
order to allow the classical GPU memory accesses which are faster. These
results constitute a step towards achieving a parallel efficient C++ like
vector for modern GPU architectures.",0.35311514,-0.134277,0.07024159,A
10626,"In addition, existing systems propose sev-               LOJ will inspire further research not only on dynamic DNN
eral orthogonal concepts that can be seen as complementary to              inference serving systems but other aspects of dynamic DNN
ORLOJ.","namic DNNs, its idea of proactively planning ahead and con-
solidating choices across layers to reduce disturbance still                  While we take a ﬁrst step in this paper, we hope that OR-
applies in ORLOJ.",Clipper’s idea of model selection and INFaaS’s [43]                 lifecycle as well.,2022-08-31 23:58:52+00:00,Orloj: Predictably Serving Unpredictable DNNs,cs.DC,['cs.DC'],"[arxiv.Result.Author('Peifeng Yu'), arxiv.Result.Author('Yuqing Qiu'), arxiv.Result.Author('Xin Jin'), arxiv.Result.Author('Mosharaf Chowdhury')]","Existing DNN serving solutions can provide tight latency SLOs while
maintaining high throughput via careful scheduling of incoming requests, whose
execution times are assumed to be highly predictable and data-independent.
However, inference requests to emerging dynamic DNNs -- e.g., popular natural
language processing (NLP) models and computer vision (CV) models that skip
layers -- are data-dependent. They exhibit poor performance when served using
existing solutions because they experience large variance in request execution
times depending on the input -- the longest request in a batch inflates the
execution times of the smaller ones, causing SLO misses in the absence of
careful batching.
  In this paper, we present Orloj, a dynamic DNN serving system, that captures
this variance in dynamic DNNs using empirical distributions of expected request
execution times, and then efficiently batches and schedules them without
knowing a request's precise execution time. Orloj significantly outperforms
state-of-the-art serving solutions for high variance dynamic DNN workloads by
51--80% in finish rate under tight SLO constraints, and over 100% under more
relaxed SLO settings. For well-studied static DNN workloads, Orloj keeps
comparable performance with the state-of-the-art.",-0.08279287,0.23925923,-0.29224294,B
11059,"To further study the effectiveness
of DUET, we visualize the accuracy and loss in training and in-
ference in Figure 5, the corresponding mean value, median value,
standard derivation, maximum/minimum, and quartiles of real-time
inference are shown in Figure 6.","It is noteworthy
Detail Performance Analysis.","All experiments are repeated five
times.",2022-09-12 13:26:26+00:00,DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization,cs.DC,"['cs.DC', 'cs.AI', 'cs.CV', 'cs.IR']","[arxiv.Result.Author('Zheqi Lv'), arxiv.Result.Author('Wenqiao Zhang'), arxiv.Result.Author('Shengyu Zhang'), arxiv.Result.Author('Kun Kuang'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Yongwei Wang'), arxiv.Result.Author('Zhengyu Chen'), arxiv.Result.Author('Tao Shen'), arxiv.Result.Author('Hongxia Yang'), arxiv.Result.Author('Bengchin Ooi'), arxiv.Result.Author('Fei Wu')]","Device Model Generalization (DMG) is a practical yet under-investigated
research topic for on-device machine learning applications. It aims to improve
the generalization ability of pre-trained models when deployed on
resource-constrained devices, such as improving the performance of pre-trained
cloud models on smart mobiles. While quite a lot of works have investigated the
data distribution shift across clouds and devices, most of them focus on model
fine-tuning on personalized data for individual devices to facilitate DMG.
Despite their promising, these approaches require on-device re-training, which
is practically infeasible due to the overfitting problem and high time delay
when performing gradient calculation on real-time data. In this paper, we argue
that the computational cost brought by fine-tuning can be rather unnecessary.
We consequently present a novel perspective to improving DMG without increasing
computational cost, i.e., device-specific parameter generation which directly
maps data distribution to parameters. Specifically, we propose an efficient
Device-cloUd collaborative parametErs generaTion framework DUET. DUET is
deployed on a powerful cloud server that only requires the low cost of
forwarding propagation and low time delay of data transmission between the
device and the cloud. By doing so, DUET can rehearse the device-specific model
weight realizations conditioned on the personalized real-time data for an
individual device. Importantly, our DUET elegantly connects the cloud and
device as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a
faster and more accurate DMG paradigm. We conduct an extensive experimental
study of DUET on three public datasets, and the experimental results confirm
our framework's effectiveness and generalisability for different DMG tasks.",-0.10287672,-0.03940587,-0.26188302,B
11060,"To further study the effectiveness          that if we upload the embedding of real-time samples, it not only
of DUET, we visualize the accuracy and loss in training and in-          can generate a faster DMG, but also can protect user privacy of
ference in Figure 5, the corresponding mean value, median value,         these samples.","It is noteworthy
Detail Performance Analysis.","The aforementioned observation and analysis verify
standard derivation, maximum/minimum, and quartiles of real-time
inference are shown in Figure 6.",2022-09-12 13:26:26+00:00,DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization,cs.DC,"['cs.DC', 'cs.AI', 'cs.CV', 'cs.IR']","[arxiv.Result.Author('Zheqi Lv'), arxiv.Result.Author('Wenqiao Zhang'), arxiv.Result.Author('Shengyu Zhang'), arxiv.Result.Author('Kun Kuang'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Yongwei Wang'), arxiv.Result.Author('Zhengyu Chen'), arxiv.Result.Author('Tao Shen'), arxiv.Result.Author('Hongxia Yang'), arxiv.Result.Author('Beng chin Ooi'), arxiv.Result.Author('Fei Wu')]","Device Model Generalization (DMG) is a practical yet under-investigated
research topic for on-device machine learning applications. It aims to improve
the generalization ability of pre-trained models when deployed on
resource-constrained devices, such as improving the performance of pre-trained
cloud models on smart mobiles. While quite a lot of works have investigated the
data distribution shift across clouds and devices, most of them focus on model
fine-tuning on personalized data for individual devices to facilitate DMG.
Despite their promising, these approaches require on-device re-training, which
is practically infeasible due to the overfitting problem and high time delay
when performing gradient calculation on real-time data. In this paper, we argue
that the computational cost brought by fine-tuning can be rather unnecessary.
We consequently present a novel perspective to improving DMG without increasing
computational cost, i.e., device-specific parameter generation which directly
maps data distribution to parameters. Specifically, we propose an efficient
Device-cloUd collaborative parametErs generaTion framework DUET. DUET is
deployed on a powerful cloud server that only requires the low cost of
forwarding propagation and low time delay of data transmission between the
device and the cloud. By doing so, DUET can rehearse the device-specific model
weight realizations conditioned on the personalized real-time data for an
individual device. Importantly, our DUET elegantly connects the cloud and
device as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a
faster and more accurate DMG paradigm. We conduct an extensive experimental
study of DUET on three public datasets, and the experimental results confirm
our framework's effectiveness and generalisability for different DMG tasks.",-0.049892306,-0.15353188,-0.1542475,B
11068,"In Section 5, we then further study the relation between existential,
expected, and deterministic mending volumes.","We also explore the landscape of mending volume beyond the case of trees; the results are
summarized in Table 1.","We show that there are LCL problems in which all
three notions coincide, but that there are also problems that separate existential and randomized
mending volumes, as well as problems that separate randomized and deterministic mending
volumes.",2022-09-12 16:17:17+00:00,Mending Partial Solutions with Few Changes,cs.DC,"['cs.DC', 'cs.CC']","[arxiv.Result.Author('Darya Melnyk'), arxiv.Result.Author('Jukka Suomela'), arxiv.Result.Author('Neven Villani')]","In this paper, we study the notion of mending, i.e. given a partial solution
to a graph problem, we investigate how much effort is needed to turn it into a
proper solution. For example, if we have a partial coloring of a graph, how
hard is it to turn it into a proper coloring?
  In prior work (SIROCCO 2022), this question was formalized and studied from
the perspective of mending radius: if there is a hole that we need to patch,
how far do we need to modify the solution? In this work, we investigate a
complementary notion of mending volume: how many nodes need to be modified to
patch a hole?
  We focus on the case of locally checkable labeling problems (LCLs) in trees,
and show that already in this setting there are two infinite hierarchies of
problems: for infinitely many values $0 < \alpha \le 1$, there is an LCL
problem with mending volume $\Theta(n^\alpha)$, and for infinitely many values
$k \ge 1$, there is an LCL problem with mending volume $\Theta(\log^k n)$.
Hence the mendability of LCL problems on trees is a much more fine-grained
question than what one would expect based on the mending radius alone.
  We define three variants of the theme: (1) existential mending volume, i.e.,
how many nodes need to be modified, (2) expected mending volume, i.e., how many
nodes we need to explore to find a patch if we use randomness, and (3)
deterministic mending volume, i.e., how many nodes we need to explore if we use
a deterministic algorithm. We show that all three notions are distinct from
each other, and we analyze the landscape of the complexities of LCL problems
for the respective models.",-0.05777985,0.4173851,-0.049967244,B
11173,"In         addition, updating the auto-tuner to automatically select the
addition, hybrid approaches that combine multiple formats into    optimum format online and not from proﬁling runs remains
a single data structure have been proposed, with the aim to       an avenue for further research.","In
to improve performance by addressing known weaknesses.",better exploit certain sparsity patterns.,2022-09-14 08:19:51+00:00,Exploiting dynamic sparse matrices for performance portable linear algebra operations,cs.DC,['cs.DC'],"[arxiv.Result.Author('Chris Stylianou'), arxiv.Result.Author('Michele Weiland')]","Sparse matrices and linear algebra are at the heart of scientific
simulations. More than 70 sparse matrix storage formats have been developed
over the years, targeting a wide range of hardware architectures and matrix
types. Each format is developed to exploit the particular strengths of an
architecture, or the specific sparsity patterns of matrices, and the choice of
the right format can be crucial in order to achieve optimal performance. The
adoption of dynamic sparse matrices that can change the underlying
data-structure to match the computation at runtime without introducing
prohibitive overheads has the potential of optimizing performance through
dynamic format selection.
  In this paper, we introduce Morpheus, a library that provides an efficient
abstraction for dynamic sparse matrices. The adoption of dynamic matrices aims
to improve the productivity of developers and end-users who do not need to know
and understand the implementation specifics of the different formats available,
but still want to take advantage of the optimization opportunity to improve the
performance of their applications. We demonstrate that by porting HPCG to use
Morpheus, and without further code changes, 1) HPCG can now target
heterogeneous environments and 2) the performance of the SpMV kernel is
improved up to 2.5x and 7x on CPUs and GPUs respectively, through runtime
selection of the best format on each MPI process.",0.09251869,-0.13449115,-0.10858509,A
11604,"directions for further study,

                                           The irregular accesses in many graph analysis algorithms                            II.","These can come from one source, an analyst            brief overview of related work follows in Section V. Finally,
                                        or program trying multiple options, or from a large number of       Section VI summarizes our observations and provides future
                                        sources as in a web-accessible graph database.","LUCATA ARCHITECTURE
                                        also cause problems for current computer architectures.",2022-09-23 23:55:17+00:00,Concurrent Graph Queries on the Lucata Pathfinder,cs.DC,['cs.DC'],"[arxiv.Result.Author('Emory Smith'), arxiv.Result.Author('Shannon Kuntz'), arxiv.Result.Author('Jason Riedy'), arxiv.Result.Author('Martin Deneroff')]","High-performance analysis of unstructured data like graphs now is critical
for applications ranging from business intelligence to genome analysis. Towards
this, data centers hold large graphs in memory to serve multiple concurrent
queries from different users. Even a single analysis often explores multiple
options. Current computing architectures often are not the most time- or
energy-efficient solutions. The novel Lucata Pathfinder architecture tackles
this problem, combining migratory threads for low-latency reading with
memory-side processing for high-performance accumulation. One hundred to 750
concurrent breadth-first searches (BFS) all achieve end-to-end speed-ups of 81%
to 97% over one-at-a-time queries on a graph with 522M edges. Comparing to
RedisGraph running on a large Intel-based server, the Pathfinder achieves a
19$\times$ speed-up running 128 BFS queries concurrently. The Pathfinder also
efficiently supports a mix of concurrent analyses, demonstrated with connected
components and BFS.",0.13806331,-0.03100861,0.011862999,A
11910,"[7] R. Vinayakumar, K. P. Soman, and P. Poornachandran, “Long short-term
   Our results illustrate the potential of machine learning               memory based operation log anomaly detection,” in 2017 International
models in this ﬁeld, and we hope it will elicit further research.",but at a signiﬁcant computation cost.,"Conference on Advances in Computing, Communications and Informat-
Our next step is to reﬁne our prototype, and to test it against           ics (ICACCI), 2017, pp.",2022-09-30 23:11:06+00:00,Towards Implementing ML-Based Failure Detectors,cs.DC,['cs.DC'],"[arxiv.Result.Author('Xiaonan Li'), arxiv.Result.Author('Olivier Marin')]","Most existing failure detection algorithms rely on statistical methods, and
very few use machine learning (ML). This paper explores the viability of ML in
the field of failure detection: is it possible to implement an ML-based
detector that achieves a satisfactory quality of service? We implement a
prototype that uses a basic long short-term memory neural network algorithm,
and study its behavior with real traces. Although ML model has comparatively
longer computing time, our prototype performs well in terms of accuracy and
detection time.",-0.1188411,0.0027511194,-0.043903507,B
11990,"C. Performance of Incremental Simulation

                                                                          We further study the performance difference of incremental
                                                                       simulation between qTask and Qulacs over different numbers
                                                                       of circuit modiﬁers.","However,
of state sizes below our partition size (i.e., 8 qubits with 256       how to efﬁciently extend such sparsity management to an
                                                                       incremental environment remains unknown.","Hereafter, we compare with only Qulacs
                                                                       since Qiskit is much slower.",2022-10-03 16:48:29+00:00,qTask: Task-parallel Quantum Circuit Simulation with Incrementality,cs.DC,['cs.DC'],[arxiv.Result.Author('Tsung-Wei Huang')],"Incremental quantum circuit simulation has emerged as an important tool for
simulation-driven quantum applications, such as circuit synthesis,
verification, and analysis. When a small portion of the circuit is modified,
the simulator must incrementally update state amplitudes for reasonable
turnaround time and productivity. However, this type of incrementality has been
largely ignored by existing research. To fill this gap, we introduce a new
incremental quantum circuit simulator called qTask. qTask leverages a
task-parallel decomposition strategy to explore both inter- and intra-gate
operation parallelisms from partitioned data blocks. Our partitioning strategy
effectively narrows down incremental update to a small set of partitions
affected by circuit modifiers. We have demonstrated the promising performance
of qTask on QASMBench benchmarks. Compared to two state-of-the-art simulators,
Qulacs and Qiskit, qTask is respectively 1.46x and 1.71x faster for full
simulation and 5.77x and 9.76x faster for incremental simulation.",0.13565496,-0.01812272,-0.01294128,A
12014,"However, further research is necessary
centrality function cG, we deﬁne the p-proportion of centrality    to quantitatively determine how well our proposed metric
                                                                   correlates with GPU tuning difﬁculty.",For a          quantify this likelihood.,"x∈Lp(X) cG(x)
                  Cp(G, X) = x∈L(X) cG(x) .",2022-10-04 08:42:12+00:00,Benchmarking optimization algorithms for auto-tuning GPU kernels,cs.DC,"['cs.DC', 'cs.GR', 'cs.PF']","[arxiv.Result.Author('Richard Schoonhoven'), arxiv.Result.Author('Ben van Werkhoven'), arxiv.Result.Author('Kees Joost Batenburg')]","Recent years have witnessed phenomenal growth in the application, and
capabilities of Graphical Processing Units (GPUs) due to their high parallel
computation power at relatively low cost. However, writing a computationally
efficient GPU program (kernel) is challenging, and generally only certain
specific kernel configurations lead to significant increases in performance.
Auto-tuning is the process of automatically optimizing software for
highly-efficient execution on a target hardware platform. Auto-tuning is
particularly useful for GPU programming, as a single kernel requires re-tuning
after code changes, for different input data, and for different architectures.
However, the discrete, and non-convex nature of the search space creates a
challenging optimization problem. In this work, we investigate which algorithm
produces the fastest kernels if the time-budget for the tuning task is varied.
We conduct a survey by performing experiments on 26 different kernel spaces,
from 9 different GPUs, for 16 different evolutionary black-box optimization
algorithms. We then analyze these results and introduce a novel metric based on
the PageRank centrality concept as a tool for gaining insight into the
difficulty of the optimization problem. We demonstrate that our metric
correlates strongly with observed tuning performance.",0.044135593,-0.18567194,0.012741813,A
12215,"While client incentive policies aim to motivate the clients’ participation with fair payoﬀs, there is
an opportunity to further study and propose FL approaches centered around the clients’ interests.","Another potential research direction would focus on the clients’ perspective to deﬁne the client selection
strategy.","In this
sense, we identify several relevant research questions, such as studying the trade-oﬀ between data availability
and performance of the local vs federated learning models; developing models to support the clients’ decisions
as to when to join the federated learning scenario; and approaches that would enable clients to participate in
the federation in a dynamic manner, depending on the current state of the learning model.",2022-09-27 10:08:18+00:00,A Snapshot of the Frontiers of Client Selection in Federated Learning,cs.DC,"['cs.DC', 'cs.AI', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Gergely Dániel Németh'), arxiv.Result.Author('Miguel Ángel Lozano'), arxiv.Result.Author('Novi Quadrianto'), arxiv.Result.Author('Nuria Oliver')]","Federated learning (FL) has been proposed as a privacy-preserving approach in
distributed machine learning. A federated learning architecture consists of a
central server and a number of clients that have access to private, potentially
sensitive data. Clients are able to keep their data in their local machines and
only share their locally trained model's parameters with a central server that
manages the collaborative learning process. FL has delivered promising results
in real-life scenarios, such as healthcare, energy, and finance. However, when
the number of participating clients is large, the overhead of managing the
clients slows down the learning. Thus, client selection has been introduced as
a strategy to limit the number of communicating parties at every step of the
process. Since the early na\""{i}ve random selection of clients, several client
selection methods have been proposed in the literature. Unfortunately, given
that this is an emergent field, there is a lack of a taxonomy of client
selection methods, making it hard to compare approaches. In this paper, we
propose a taxonomy of client selection in Federated Learning that enables us to
shed light on current progress in the field and identify potential areas of
future research in this promising area of machine learning.",-0.36638987,0.24142969,0.16758364,B
12216,"Delta-inling could eliminate the extra-consumed storage
   To further study the target ﬁles of delta-compression, we
collected the number of all involved ﬁle types in all scenarios     issue of delta compression, since most inline area in ﬁle inodes
in Table.",chunks in the free inline area in ﬁle inodes.,1.,2022-10-06 17:19:03+00:00,DeltaFS: Pursuing Zero Update Overhead via Metadata-Enabled Delta Compression for Log-structured File System on Mobile Devices,cs.DC,['cs.DC'],"[arxiv.Result.Author('Chao Wu'), arxiv.Result.Author('Cheng Ji'), arxiv.Result.Author('Geng Yuan'), arxiv.Result.Author('Riwei Pan'), arxiv.Result.Author('Weichao Guo'), arxiv.Result.Author('Chao Yu'), arxiv.Result.Author('Zongwei Zhu'), arxiv.Result.Author('Yanzhi Wang')]","Data compression has been widely adopted to release mobile devices from
intensive write pressure. Delta compression is particularly promising for its
high compression efficacy over conventional compression methods. However, this
method suffers from non-trivial system overheads incurred by delta maintenance
and read penalty, which prevents its applicability on mobile devices. To this
end, this paper proposes DeltaFS, a metadata-enabled Delta compression on
log-structured File System for mobile devices, to achieve utmost compressing
efficiency and zero hardware costs. DeltaFS smartly exploits the out-of-place
updating ability of Log-structured File System (LFS) to alleviate the problems
of write amplification, which is the key bottleneck for delta compression
implementation. Specifically, DeltaFS utilizes the inline area in file inodes
for delta maintenance with zero hardware cost, and integrates an inline area
manage strategy to improve the utilization of constrained inline area.
Moreover, a complimentary delta maintenance strategy is incorporated, which
selectively maintains delta chunks in the main data area to break through the
limitation of constrained inline area. Experimental results show that DeltaFS
substantially reduces write traffics by up to 64.8\%, and improves the I/O
performance by up to 37.3\%.",0.18071859,-3.856793e-05,-0.2158859,A
12312,"For example,      comprehensive understanding of the problem that
a study that facilitates further research with more   is to be solved.","One of the most
to less competitive venues to maximize the impact     critical steps in producing “good” research is a
and timeliness of a research study.","If the problem is not understood
efﬁciency (e.g., via developing a measurement         well enough, it is possible to research something
tool) can be as impactful as studies with ground-     that lies close to a problem and perhaps is an
breaking ideas.",2022-10-11 18:11:07+00:00,Perspectives on Negative Research Results in Pervasive Computing,cs.DC,['cs.DC'],"[arxiv.Result.Author('Ella Peltonen'), arxiv.Result.Author('Nitinder Mohan'), arxiv.Result.Author('Peter Zdankin'), arxiv.Result.Author('Tanya Shreedhar'), arxiv.Result.Author('Tri Nguyen'), arxiv.Result.Author('Suzan Bayhan'), arxiv.Result.Author('Jon Crowcroft'), arxiv.Result.Author('Jussi Kangasharju'), arxiv.Result.Author('Daniela Nicklas')]","Not all research leads to fruitful results; trying new ways or methods may
surpass the state of the art, but sometimes the hypothesis is not proven or the
improvement is insignificant. In a systems discipline like pervasive computing,
there are many sources of errors, from hardware issues over communication
channels to heterogeneous software environments. However, failure to succeed is
not a failure to progress. It is essential to create platforms for sharing
insights, experiences, and lessons learned when conducting research in
pervasive computing so that the same mistakes are not repeated. And sometimes,
a problem is a symptom of discovering new research challenges. Based on the
collective input of the First International Workshop on Negative Results in
Pervasive Computing (PerFail 2022), co-located with the 20th International
Conference on Pervasive Computing and Communications (PerCom 2022), this paper
presents a comprehensive discussion on perspectives on publishing negative
results and lessons learned in pervasive computing.",-0.27191404,0.24224189,-0.025242612,B
12482,"Additionally, cloud fail-
       ure analysis can be used to reduce cloud failures and for further research.",[C2] Analyses of cloud failures prior and during Covid-19 period.,"5
2

Processing Cloud Failure Data

This chapter describes the characteristics of the cloud failure data.",2022-10-15 10:45:34+00:00,Failure Analysis of Big Cloud Service Providers Prior to and During Covid-19 Period,cs.DC,['cs.DC'],[arxiv.Result.Author('Muhammad Ahsan')],"Cloud services are important for societal function such as healthcare,
commerce, entertainment and education. Cloud can provide a variety of features
such as increased collaboration and inexpensive computing. Failures are
unavoidable in cloud services due to the large size and complexity, resulting
in decreased reliability and efficiency. For example, due to bugs, many
high-severity failures have been occurring in cloud infrastructure of popular
providers, causing outages of several hours and the unrecoverable loss of user
data. There are prior studies about cloud failure analyses are limited and use
sources such as news articles. However, a detailed cloud failure focused study
is required that provides analyses for cloud failure data gathered directly
from the vendors. Furthermore, the Covid-19 cloud failures should be studied as
cloud services played a major role throughout the Covid-19 period, as
individuals relied on cloud services for activities such as working from home.
A program can be made for this task. As a result, we will be able to better
understand and mitigate cloud failures to reduce the effect of cloud failures.",-0.4708684,-0.25703636,-0.23498884,C
12483,"For
further study, the tool can be extended to be used for analysis of future cloud failures.","Furthermore, this study provides insight for reasons of cloud failures.","27
6

Self-Reﬂection

6.1 Self-Reﬂection

Through this project I learned about growing demand and importance of cloud services.",2022-10-15 10:45:34+00:00,Failure Analysis of Big Cloud Service Providers Prior to and During Covid-19 Period,cs.DC,['cs.DC'],[arxiv.Result.Author('Muhammad Ahsan')],"Cloud services are important for societal function such as healthcare,
commerce, entertainment and education. Cloud can provide a variety of features
such as increased collaboration and inexpensive computing. Failures are
unavoidable in cloud services due to the large size and complexity, resulting
in decreased reliability and efficiency. For example, due to bugs, many
high-severity failures have been occurring in cloud infrastructure of popular
providers, causing outages of several hours and the unrecoverable loss of user
data. There are prior studies about cloud failure analyses are limited and use
sources such as news articles. However, a detailed cloud failure focused study
is required that provides analyses for cloud failure data gathered directly
from the vendors. Furthermore, the Covid-19 cloud failures should be studied as
cloud services played a major role throughout the Covid-19 period, as
individuals relied on cloud services for activities such as working from home.
A program can be made for this task. As a result, we will be able to better
understand and mitigate cloud failures to reduce the effect of cloud failures.",-0.55522954,-0.22027384,-0.15790533,C
12484,"Additionally, cloud fail-
       ure analysis can be used to reduce cloud failures and for further research.",[C2] Analyses of cloud failures prior and during Covid-19 period.,"5
2

Processing Cloud Failure Data

This chapter describes the characteristics of the cloud failure data.",2022-10-15 10:45:34+00:00,Failure Analysis of Big Cloud Service Providers Prior to and During Covid-19 Period,cs.DC,['cs.DC'],[arxiv.Result.Author('Muhammad Ahsan')],"Cloud services are important for societal function such as healthcare,
commerce, entertainment and education. Cloud can provide a variety of features
such as increased collaboration and inexpensive computing. Failures are
unavoidable in cloud services due to the large size and complexity, resulting
in decreased reliability and efficiency. For example, due to bugs, many
high-severity failures have been occurring in cloud infrastructure of popular
providers, causing outages of several hours and the unrecoverable loss of user
data. There are prior studies about cloud failure analyses are limited and use
sources such as news articles. However, a detailed cloud failure focused study
is required that provides analyses for cloud failure data gathered directly
from the vendors. Furthermore, the Covid-19 cloud failures should be studied as
cloud services played a major role throughout the Covid-19 period, as
individuals relied on cloud services for activities such as working from home.
A program can be made for this task. As a result, we will be able to better
understand and mitigate cloud failures to reduce the effect of cloud failures.",-0.4708684,-0.25703636,-0.23498884,C
12485,"For
further study, the tool can be extended to be used for analysis of future cloud failures.","Furthermore, this study provides insight for reasons of cloud failures.","27
6

Self-Reﬂection

6.1 Self-Reﬂection

Through this project I learned about growing demand and importance of cloud services.",2022-10-15 10:45:34+00:00,Failure Analysis of Big Cloud Service Providers Prior to and During Covid-19 Period,cs.DC,['cs.DC'],[arxiv.Result.Author('Muhammad Ahsan')],"Cloud services are important for societal function such as healthcare,
commerce, entertainment and education. Cloud can provide a variety of features
such as increased collaboration and inexpensive computing. Failures are
unavoidable in cloud services due to the large size and complexity, resulting
in decreased reliability and efficiency. For example, due to bugs, many
high-severity failures have been occurring in cloud infrastructure of popular
providers, causing outages of several hours and the unrecoverable loss of user
data. There are prior studies about cloud failure analyses are limited and use
sources such as news articles. However, a detailed cloud failure focused study
is required that provides analyses for cloud failure data gathered directly
from the vendors. Furthermore, the Covid-19 cloud failures should be studied as
cloud services played a major role throughout the Covid-19 period, as
individuals relied on cloud services for activities such as working from home.
A program can be made for this task. As a result, we will be able to better
understand and mitigate cloud failures to reduce the effect of cloud failures.",-0.55522954,-0.22027384,-0.15790533,C
12909,"Privacy Preservation Sharing a similar sentiment with researchers in Pri-
vateML, we envision the requirements for further research on open problems such
as providing satisfactory privacy guarantees while maintaining the predictive
power of the trained DL strategies, designing measures to ensure communication-
eﬃciency and counter the eﬀect of bloated data caused by encoding and en-
cryption schemes, ensuring fairness as well as robustness while enabling pri-
vacy preservation, etc.","In other words,
we anticipate novel research focused on dynamic resource allocation strategies
that jointly and simultaneously optimise models while allocating resources such
as network bandwidth; computing and storage requirements; energy eﬃciency,
etc through the eﬃcient integration of systems and networks statistics, monitor-
ing data available at the edge [31].",[18].,2022-10-24 04:18:57+00:00,"Deep Edge Intelligence: Architecture, Key Features, Enabling Technologies and Challenges",cs.DC,"['cs.DC', 'cs.AI']","[arxiv.Result.Author('Prabath Abeysekara'), arxiv.Result.Author('Hai Dong'), arxiv.Result.Author('A. K. Qin')]","With the breakthroughs in Deep Learning, recent years have witnessed a
massive surge in Artificial Intelligence applications and services. Meanwhile,
the rapid advances in Mobile Computing and Internet of Things has also given
rise to billions of mobile and smart sensing devices connected to the Internet,
generating zettabytes of data at the network edge. The opportunity to combine
these two domains of technologies to power interconnected devices with
intelligence is likely to pave the way for a new wave of technology
revolutions. Embracing this technology revolution, in this article, we present
a novel computing vision named Deep Edge Intelligence (DEI). DEI employs Deep
Learning, Artificial Intelligence, Cloud and Edge Computing, 5G/6G networks,
Internet of Things, Microservices, etc. aiming to provision reliable and secure
intelligence services to every person and organisation at any place with better
user experience. The vision, system architecture, key layers and features of
DEI are also detailed. Finally, we reveal the key enabling technologies and
research challenges associated with it.",-0.19075957,0.06424249,0.15678349,C
13034,"Hence the system               hki = 1, if the surface-station has cached the data of the task
considered deserves further study.","We have
networks to AUV-aided underwater networks, the research
mentioned above is no longer applicable.","Therefore, in this paper,
a max-proﬁt problem, integrating environment-aware trajec-              Wki and hki = 0 otherwise.",2022-10-26 10:51:41+00:00,Environment-Aware AUV Trajectory Design and Resource Management for Multi-Tier Underwater Computing,cs.DC,['cs.DC'],"[arxiv.Result.Author('Xiangwang Hou'), arxiv.Result.Author('Jingjing Wang'), arxiv.Result.Author('Tong Bai'), arxiv.Result.Author('Yansha Deng'), arxiv.Result.Author('Yong Ren'), arxiv.Result.Author('Lajos Hanzo')]","The Internet of underwater things (IoUT) is envisioned to be an essential
part of maritime activities. Given the IoUT devices' wide-area distribution and
constrained transmit power, autonomous underwater vehicles (AUVs) have been
widely adopted for collecting and forwarding the data sensed by IoUT devices to
the surface-stations. In order to accommodate the diverse requirements of IoUT
applications, it is imperative to conceive a multi-tier underwater computing
(MTUC) framework by carefully harnessing both the computing and the
communications as well as the storage resources of both the surface-station and
of the AUVs as well as of the IoUT devices. Furthermore, to meet the stringent
energy constraints of the IoUT devices and to reduce the operating cost of the
MTUC framework, a joint environment-aware AUV trajectory design and resource
management problem is formulated, which is a high-dimensional NP-hard problem.
To tackle this challenge, we first transform the problem into a Markov decision
process (MDP) and solve it with the aid of the asynchronous advantage
actor-critic (A3C) algorithm. Our simulation results demonstrate the
superiority of our scheme.",-0.09862954,0.067208804,-0.14324024,C
13199,"In the future, we will further study
reach the converged value, suggesting that such conﬁguration                    extended DGPE cases where clients can replicate their vertices
can steer to the local optimum.","19), the achieved system cost can                     GNN-driven applications.","Nevertheless, the gain in                       data across multiple edge servers, and investigate privacy-
system cost improvement comes at a price.",2022-10-31 13:03:16+00:00,GNN at the Edge: Cost-Efficient Graph Neural Network Processing over Distributed Edge Servers,cs.DC,"['cs.DC', 'cs.LG', 'cs.NI', 'cs.SI']","[arxiv.Result.Author('Liekang Zeng'), arxiv.Result.Author('Chongyu Yang'), arxiv.Result.Author('Peng Huang'), arxiv.Result.Author('Zhi Zhou'), arxiv.Result.Author('Shuai Yu'), arxiv.Result.Author('Xu Chen')]","Edge intelligence has arisen as a promising computing paradigm for supporting
miscellaneous smart applications that rely on machine learning techniques.
While the community has extensively investigated multi-tier edge deployment for
traditional deep learning models (e.g. CNNs, RNNs), the emerging Graph Neural
Networks (GNNs) are still under exploration, presenting a stark disparity to
its broad edge adoptions such as traffic flow forecasting and location-based
social recommendation. To bridge this gap, this paper formally studies the cost
optimization for distributed GNN processing over a multi-tier heterogeneous
edge network. We build a comprehensive modeling framework that can capture a
variety of different cost factors, based on which we formulate a cost-efficient
graph layout optimization problem that is proved to be NP-hard. Instead of
trivially applying traditional data placement wisdom, we theoretically reveal
the structural property of quadratic submodularity implicated in GNN's unique
computing pattern, which motivates our design of an efficient iterative
solution exploiting graph cuts. Rigorous analysis shows that it provides
parameterized constant approximation ratio, guaranteed convergence, and exact
feasibility. To tackle potential graph topological evolution in GNN processing,
we further devise an incremental update strategy and an adaptive scheduling
algorithm for lightweight dynamic layout optimization. Evaluations with
real-world datasets and various GNN benchmarks demonstrate that our approach
achieves superior performance over de facto baselines with more than 95.8% cost
eduction in a fast convergence speed.",-0.110076286,-0.07606366,0.20945054,C
13220,"Finally, Section 6 provides conclusive thoughts and delineates
directions for further research.","Section 5 performs an experimental validation
of the proposed approach.",2.,2022-10-31 17:29:41+00:00,Space-fluid Adaptive Sampling by Self-Organisation,cs.DC,"['cs.DC', 'cs.AI', 'cs.MA', 'cs.SY', 'eess.SY', 'I.2.11; D.3.1; D.1.3']","[arxiv.Result.Author('Roberto Casadei'), arxiv.Result.Author('Stefano Mariani'), arxiv.Result.Author('Danilo Pianini'), arxiv.Result.Author('Mirko Viroli'), arxiv.Result.Author('Franco Zambonelli')]","A recurrent task in coordinated systems is managing (estimating, predicting,
or controlling) signals that vary in space, such as distributed sensed data or
computation outcomes. Especially in large-scale settings, the problem can be
addressed through decentralised and situated computing systems: nodes can
locally sense, process, and act upon signals, and coordinate with neighbours to
implement collective strategies. Accordingly, in this work we devise
distributed coordination strategies for the estimation of a spatial phenomenon
through collaborative adaptive sampling. Our design is based on the idea of
dynamically partitioning space into regions that compete and grow/shrink to
provide accurate aggregate sampling. Such regions hence define a sort of
virtualised space that is ""fluid"", since its structure adapts in response to
pressure forces exerted by the underlying phenomenon. We provide an adaptive
sampling algorithm in the field-based coordination framework, and prove it is
self-stabilising and locally optimal. Finally, we verify by simulation that the
proposed algorithm effectively carries out a spatially adaptive sampling while
maintaining a tuneable trade-off between accuracy and efficiency.",-0.10236236,0.22116041,-0.24635231,B
13709,"After that, section 2.2 will mainly focus on optimization methods based on system statis-
tics, which compare and suggest several identiﬁed directions for further research.","It will show the relevance be-
tween the success of federated learning with optimization based on system parameters.","This
section also shows the importance of resource management to assist research optimising
federated learning.",2022-11-14 09:52:48+00:00,Federated Learning Framework in Fogbus2-based Edge Computing Environments,cs.DC,['cs.DC'],[arxiv.Result.Author('Wuji Zhu')],"Federated learning refers to conducting training on multiple distributed
devices and collecting model weights from them to derive a shared
machine-learning model. This allows the model to get benefit from a rich source
of data available from multiple sites. Also, since only model weights are
collected from distributed devices, the privacy of those data is protected. It
is useful in a situation where collaborative training of machine learning
models is necessary while training data are highly sensitive. This study aims
at investigating the implementation of lightweight federated learning to be
deployed on a diverse range of distributed resources, including
resource-constrained edge devices and resourceful cloud servers. As a resource
management framework, the FogBus2 framework, which is a containerized
distributed resource management framework, is selected as the base framework
for the implementation. This research provides an architecture and lightweight
implementation of federated learning in the FogBus2. Moreover, a worker
selection technique is proposed and implemented. The worker selection algorithm
selects an appropriate set of workers to participate in the training to achieve
higher training time efficiency. Besides, this research integrates synchronous
and asynchronous models of federated learning alongside with heuristic-based
worker selection algorithm. It is proven that asynchronous federated learning
is more time efficient compared to synchronous federated learning or sequential
machine learning training. The performance evaluation shows the efficiency of
the federated learning mechanism implemented and integrated with the FogBus2
framework. The worker selection strategy obtains 33.9% less time to reach 80%
accuracy compared to sequential training, while asynchronous further improve
synchronous federated learning training time by 63.3%.",-0.23005137,0.13470961,0.072866164,B
13745,"These microservice deployment strategies can simulated and analysed
to further study microservice performance.","Another intriguing option for deploying microservices is AWS Lambda, a
serverless approach.","34
References

 [1] Apigee | Google Cloud Follow.",2022-11-08 10:52:21+00:00,"The OpenDC Microservice Simulator: Design, Implementation, and Experimentation",cs.DC,"['cs.DC', 'cs.SE']",[arxiv.Result.Author('Muhammad Ahsan')],"Microservices is an architectural style that structures an application as a
collection of loosely coupled services, making it easy for developers to build
and scale their applications. The microservices architecture approach differs
from the traditional monolithic style of treating software development as a
single entity. Microservice architecture is becoming more and more adapted.
However, microservice systems can be complex due to dependencies between the
microservices, resulting in unpredictable performance at a large scale.
Simulation is a cheap and fast way to investigate the performance of
microservices in more detail. This study aims to build a microservices
simulator for evaluating and comparing microservices based applications. The
microservices reference architecture is designed. The architecture is used as
the basis for a simulator. The simulator implementation uses statistical models
to generate the workload. The compelling features added to the simulator
include concurrent execution of microservices, configurable request depth,
three load-balancing policies and four request execution order policies. This
paper contains two experiments to show the simulator usage. The first
experiment covers request execution order policies at the microservice
instance. The second experiment compares load balancing policies across
microservice instances.",-0.30227703,-0.2678818,0.064826645,C
14098,"In natural sciences, such as bioinformatics, material science,
                                        or earth observation, scientiﬁc workﬂows are used to automate         To pave the way for further research in this direction, in this
                                        the analysis of experiments and the collection of results [1]–     paper, we provide an architectural blueprint for monitoring
                                        [8].","INTRODUCTION                          resources efﬁciently in an automated manner [11]–[15] and
                                                                                                           simultaneously enable automated system tuning.",Scientists are faced with systems becoming increasingly       scientiﬁc workﬂows.,2022-11-23 07:09:49+00:00,Towards Advanced Monitoring for Scientific Workflows,cs.DC,['cs.DC'],"[arxiv.Result.Author('Jonathan Bader'), arxiv.Result.Author('Joel Witzke'), arxiv.Result.Author('Soeren Becker'), arxiv.Result.Author('Ansgar Lößer'), arxiv.Result.Author('Fabian Lehmann'), arxiv.Result.Author('Leon Doehler'), arxiv.Result.Author('Anh Duc Vu'), arxiv.Result.Author('Odej Kao')]","Scientific workflows consist of thousands of highly parallelized tasks
executed in a distributed environment involving many components. Automatic
tracing and investigation of the components' and tasks' performance metrics,
traces, and behavior are necessary to support the end user with a level of
abstraction since the large amount of data cannot be analyzed manually. The
execution and monitoring of scientific workflows involves many components, the
cluster infrastructure, its resource manager, the workflow, and the workflow
tasks. All components in such an execution environment access different
monitoring metrics and provide metrics on different abstraction levels. The
combination and analysis of observed metrics from different components and
their interdependencies are still widely unregarded.
  We specify four different monitoring layers that can serve as an
architectural blueprint for the monitoring responsibilities and the
interactions of components in the scientific workflow execution context. We
describe the different monitoring metrics subject to the four layers and how
the layers interact. Finally, we examine five state-of-the-art scientific
workflow management systems (SWMS) in order to assess which steps are needed to
enable our four-layer-based approach.",-0.15690069,0.022422321,-0.074900374,B
14528,"Our code will be open-    dent sub-trees to different thread blocks suffers from high
                                       sourced to enable further research on accelerating MCE.","GPU outperforms the state-of-the-art parallel CPU imple-       The first challenge is that the MCE search tree is substan-
                                       mentation by a geometric mean of 4.9× (up to 16.7×), and       tially more imbalanced, which means that assigning indepen-
                                       scales efficiently to multiple GPUs.",load imbalance.,2022-12-02 22:41:05+00:00,Parallelizing Maximal Clique Enumeration on GPUs,cs.DC,['cs.DC'],"[arxiv.Result.Author('Mohammad Almasri'), arxiv.Result.Author('Yen-Hsiang Chang'), arxiv.Result.Author('Izzat El Hajj'), arxiv.Result.Author('Rakesh Nagi'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Wen-mei Hwu')]","We present a GPU solution for exact maximal clique enumeration (MCE) that
performs a search tree traversal following the Bron-Kerbosch algorithm. Prior
works on parallelizing MCE on GPUs perform a breadth-first traversal of the
tree, which has limited scalability because of the explosion in the number of
tree nodes at deep levels. We propose to parallelize MCE on GPUs by performing
depth-first traversal of independent sub-trees in parallel. Since MCE suffers
from high load imbalance and memory capacity requirements, we propose a worker
list for dynamic load balancing, as well as partial induced sub-graphs and a
compact representation of excluded vertex sets to regulate memory consumption.
Our evaluation shows that our GPU implementation on a single GPU outperforms
the state-of-the-art parallel CPU implementation by a geometric mean of 4.9x
(up to 16.7x), and scales efficiently to multiple GPUs. Our code will be
open-sourced to enable further research on accelerating MCE.",0.32312664,-0.31671655,0.13337898,A
14529,"To further study the effectiveness of the worker
implementation for all graphs.","We observe that our GPU implementation                                          substantially reduces load imbalance compared to not using
consistently and significantly outperforms the parallel CPU                                    a worker list.","The geometric mean speedup                                      list, Table 2 shows the number of donations performed for
of our GPU implementation over the parallel CPU implemen-                                      each graph.",2022-12-02 22:41:05+00:00,Parallelizing Maximal Clique Enumeration on GPUs,cs.DC,['cs.DC'],"[arxiv.Result.Author('Mohammad Almasri'), arxiv.Result.Author('Yen-Hsiang Chang'), arxiv.Result.Author('Izzat El Hajj'), arxiv.Result.Author('Rakesh Nagi'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Wen-mei Hwu')]","We present a GPU solution for exact maximal clique enumeration (MCE) that
performs a search tree traversal following the Bron-Kerbosch algorithm. Prior
works on parallelizing MCE on GPUs perform a breadth-first traversal of the
tree, which has limited scalability because of the explosion in the number of
tree nodes at deep levels. We propose to parallelize MCE on GPUs by performing
depth-first traversal of independent sub-trees in parallel. Since MCE suffers
from high load imbalance and memory capacity requirements, we propose a worker
list for dynamic load balancing, as well as partial induced sub-graphs and a
compact representation of excluded vertex sets to regulate memory consumption.
Our evaluation shows that our GPU implementation on a single GPU outperforms
the state-of-the-art parallel CPU implementation by a geometric mean of 4.9x
(up to 16.7x), and scales efficiently to multiple GPUs. Our code will be
open-sourced to enable further research on accelerating MCE.",0.21906906,-0.3704396,0.2747131,A
14530,"Our code will be open-    pendent subtrees to different thread blocks suffers from high
                                       sourced to enable further research on accelerating MCE.","GPU outperforms the state-of-the-art parallel CPU imple-       The first challenge is that the MCE search tree is substan-
                                       mentation by a geometric mean of 4.9× (up to 16.7×), and       tially more imbalanced, which means that assigning inde-
                                       scales efficiently to multiple GPUs.",load imbalance.,2022-12-02 22:41:05+00:00,Parallelizing Maximal Clique Enumeration on GPUs,cs.DC,['cs.DC'],"[arxiv.Result.Author('Mohammad Almasri'), arxiv.Result.Author('Yen-Hsiang Chang'), arxiv.Result.Author('Izzat El Hajj'), arxiv.Result.Author('Rakesh Nagi'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Wen-mei Hwu')]","We present a GPU solution for exact maximal clique enumeration (MCE) that
performs a search tree traversal following the Bron-Kerbosch algorithm. Prior
works on parallelizing MCE on GPUs perform a breadth-first traversal of the
tree, which has limited scalability because of the explosion in the number of
tree nodes at deep levels. We propose to parallelize MCE on GPUs by performing
depth-first traversal of independent sub-trees in parallel. Since MCE suffers
from high load imbalance and memory capacity requirements, we propose a worker
list for dynamic load balancing, as well as partial induced sub-graphs and a
compact representation of excluded vertex sets to regulate memory consumption.
Our evaluation shows that our GPU implementation on a single GPU outperforms
the state-of-the-art parallel CPU implementation by a geometric mean of 4.9x
(up to 16.7x), and scales efficiently to multiple GPUs. Our code will be
open-sourced to enable further research on accelerating MCE.",0.31203246,-0.32036874,0.12697378,A
14531,"To further study the effectiveness of the worker
implementation for all graphs.","We observe that our GPU implementation                                          substantially reduces load imbalance compared to not using
consistently and significantly outperforms the parallel CPU                                    a worker list.","The geometric mean speedup                                      list, Table 2 shows the number of donations performed for
of our GPU implementation over the parallel CPU implemen-                                      each graph on A100.",2022-12-02 22:41:05+00:00,Parallelizing Maximal Clique Enumeration on GPUs,cs.DC,['cs.DC'],"[arxiv.Result.Author('Mohammad Almasri'), arxiv.Result.Author('Yen-Hsiang Chang'), arxiv.Result.Author('Izzat El Hajj'), arxiv.Result.Author('Rakesh Nagi'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Wen-mei Hwu')]","We present a GPU solution for exact maximal clique enumeration (MCE) that
performs a search tree traversal following the Bron-Kerbosch algorithm. Prior
works on parallelizing MCE on GPUs perform a breadth-first traversal of the
tree, which has limited scalability because of the explosion in the number of
tree nodes at deep levels. We propose to parallelize MCE on GPUs by performing
depth-first traversal of independent sub-trees in parallel. Since MCE suffers
from high load imbalance and memory capacity requirements, we propose a worker
list for dynamic load balancing, as well as partial induced sub-graphs and a
compact representation of excluded vertex sets to regulate memory consumption.
Our evaluation shows that our GPU implementation on a single GPU outperforms
the state-of-the-art parallel CPU implementation by a geometric mean of 4.9x
(up to 16.7x), and scales efficiently to multiple GPUs. Our code will be
open-sourced to enable further research on accelerating MCE.",0.22554669,-0.3703215,0.27099097,A_centroid
14729,"However, it is developed based on a survey of
Swedish public municipalities, and further research needs to be done in other
sectors to increase its applicabilities.","Hence, it can be applied to procure, customize and
adopt cloud-based services.","For example, both the DICM-model and the DRD-method are limited regard-
ing evaluation.",2022-12-06 23:30:27+00:00,Improving the Utilization of Digital Services - Evaluating Contest - Driven Open Data Development and the Adoption of Cloud Services,cs.DC,['cs.DC'],[arxiv.Result.Author('Workneh Yilma Ayele')],"There is a growing interest in utilizing digital services, such as software
apps and cloud-based software services. The utilization of digital services is
increasing more rapidly than any other segment of world trade. The availability
of open data unlocks the possibility of generating market possibilities in the
public and private sectors. Digital service utilization can be improved by
adopting cloud-based software services and open data innovation for service
development. However, open data has no value unless utilized, and little is
known about developing digital services using open data. Evaluation of digital
service development processes to service deployment is indispensable. Despite
this, existing evaluation models are not specifically designed to measure open
data innovation contests. Additionally, existing cloud-based digital service
implications are not used directly to adopt the technology, and empirical
research needs to be included. The research question addressed in this thesis
is: ""How can contest-driven innovation of open data digital services be
evaluated and the adoption of digital services be supported to improve the
utilization of digital services?"" The research approaches used are design
science research, descriptive statistics, and case study. This thesis proposes
Digital Innovation Contest Measurement Model (DICM-model) and Designing and
Refining DICM (DRD-method) for designing and refining DICM-model to provide
more agility. Additionally, a framework of barriers constraining developers of
open data services from developing viable services is also presented. This
framework enables requirement and cloud engineers to prioritize factors
responsible for effective adoption. Future research possibilities are
automation of idea generation, ex-post evaluation of the proposed artifacts,
and expanding cloud-based digital service adoption from suppliers'
perspectives.",-0.45268822,0.012848942,0.052231178,C
14999,"Hence, there is currently a large interest in conducting further research on them [38, 39].","These new methods are insensitive to the quality of the randomness and produce highly accurate results, besides their
simplicity and speed [36, 37].","Speciﬁc recent works applied to GMRES [40] or parallel stencil computations [41] also demonstrate the interest in this
topic.",2022-12-15 12:47:54+00:00,Calculation of the High-Energy Neutron Flux for Anticipating Errors and Recovery Techniques in Exascale Supercomputer Centres,cs.DC,"['cs.DC', 'astro-ph.IM', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Hernán Asorey'), arxiv.Result.Author('Rafael Mayo-García')]","The age of exascale computing has arrived and the risks associated with
neutron and other atmospheric radiation are becoming more critical as the
computing power increases, hence, the expected Mean Time Between Failures will
be reduced because of this radiation. In this work, a new and detailed
calculation of the neutron flux for energies above 50 MeV is presented. This
has been done by using state-of-the-art Monte Carlo astroparticle techniques
and including real atmospheric profiles at each one of the next 23 exascale
supercomputing facilities. Atmospheric impact in the flux and seasonal
variations were observed and characterised, and the barometric coefficient for
high-energy neutrons at each site was obtained. With these coefficients,
potential risks of errors associated with the increase in the flux of energetic
neutrons, such as the occurrence of single event upsets or transients, and the
corresponding failure-in-time rates, can be anticipated just by using the
atmospheric pressure before the assignation of resources to critical tasks at
each exascale facility. For more clarity, examples about how the rate of
failures is affected by the cosmic rays are included, so administrators will
better anticipate which more or less restrictive actions could take for
overcoming errors.",0.40188226,0.023880012,0.039260775,A
15082,"6.1.4 Programming Model for Locality

We believe that along with efﬁcient load-balancing algorithms for irregular problems, there is
also a need for further research in an orthogonal model for better leveraging locality within
these problems.","11     for (auto& [i, j, v] : A) {

12        y[i] += v * x[j];

13     }

14  }

Listing 6.1).","A dynamic tiling approach for grouping similar data for better caching and
a light-weight reordering approach for reordering sparse-matrices are two possible directions
that can be exploited within this programming model.",2022-12-17 21:56:49+00:00,GPU Load Balancing,cs.DC,"['cs.DC', 'cs.DS']",[arxiv.Result.Author('Muhammad Osama')],"Fine-grained workload and resource balancing is the key to high performance
for regular and irregular computations on the GPUs. In this dissertation, we
conduct an extensive survey of existing load-balancing techniques to build an
abstraction that addresses the difficulty of scheduling computations on the
GPU.
  We propose a GPU fine-grained load-balancing abstraction that decouples load
balancing from work processing and aims to support both static and dynamic
schedules with a programmable interface to implement new load-balancing
schedules. Prior to our work, the only way to unleash the GPU's potential on
irregular problems has been to workload-balance through application-specific,
tightly coupled load-balancing techniques. With our open-source framework for
load-balancing, we hope to improve programmers' productivity when developing
irregular-parallel algorithms on the GPU, and also improve the overall
performance characteristics for such applications by allowing a quick path to
experimentation with a variety of existing load-balancing techniques.
  Using our insights from load-balancing irregular workloads, we build
Stream-K, a work-centric parallelization of matrix multiplication (GEMM) and
related computations in dense linear algebra. Whereas contemporary
decompositions are primarily tile-based, our method operates by partitioning an
even share of the aggregate inner loop iterations among physical processing
elements. This provides a near-perfect utilization of computing resources,
regardless of how efficiently the output tiling for any given problem quantizes
across the underlying processing elements. On GPU processors, our Stream-K
parallelization of GEMM produces a peak speedup of up to 14x and 6.7x, and an
average performance response that is both higher and more consistent across 32K
GEMM problem geometries than state-of-the-art math libraries such as CUTLASS
and cuBLAS.",0.26374042,0.009960591,0.0069640083,A
15129,"As datasets become too large for data curators to behold            Note that the output of the above commands and ﬁle con-
at once, further research and work with pseudonymization            tents may provide overlapping, non-conformant, or aggregated
could become part of a “curators’ toolkit” that will help ensure    information.",tmpfs ﬁlesystems.,"Therefore, conclusions drawn from the dataset
that datasets meet increasingly stricter data privacy regulations   should carefully account for these conditions.",2022-12-19 16:54:56+00:00,Pseudonymization at Scale: OLCF's Summit Usage Data Case Study,cs.DC,"['cs.DC', 'cs.DL']","[arxiv.Result.Author('Ketan Maheshwari'), arxiv.Result.Author('Sean R. Wilkinson'), arxiv.Result.Author('Alex May'), arxiv.Result.Author('Tyler Skluzacek'), arxiv.Result.Author('Olga A. Kuchar'), arxiv.Result.Author('Rafael Ferreira da Silva')]","The analysis of vast amounts of data and the processing of complex
computational jobs have traditionally relied upon high performance computing
(HPC) systems. Understanding these analyses' needs is paramount for designing
solutions that can lead to better science, and similarly, understanding the
characteristics of the user behavior on those systems is important for
improving user experiences on HPC systems. A common approach to gathering data
about user behavior is to analyze system log data available only to system
administrators. Recently at Oak Ridge Leadership Computing Facility (OLCF),
however, we unveiled user behavior about the Summit supercomputer by collecting
data from a user's point of view with ordinary Unix commands.
  Here, we discuss the process, challenges, and lessons learned while preparing
this dataset for publication and submission to an open data challenge. The
original dataset contains personal identifiable information (PII) about OLCF
users which needed be masked prior to publication, and we determined that
anonymization, which scrubs PII completely, destroyed too much of the structure
of the data to be interesting for the data challenge. We instead chose to
pseudonymize the dataset to reduce its linkability to users' identities.
Pseudonymization is significantly more computationally expensive than
anonymization, and the size of our dataset, approximately 175 million lines of
raw text, necessitated the development of a parallelized workflow that could be
reused on different HPC machines. We demonstrate the scaling behavior of the
workflow on two leadership class HPC systems at OLCF, and we show that we were
able to bring the overall makespan time from an impractical 20+ hours on a
single node down to around 2 hours. As a result of this work, we release the
entire pseudonymized dataset and make the workflows and source code publicly
available.",-0.13536808,0.13253884,0.08515425,B
15180,"When considering the over-        further research on how to better utilize the overall CEs data,
all CE evolution, one can observe that irrespective of the           to improve the performance on memory failure prediction.","This paper invites
   Table I summarizes the results.","method or the experiment parameters, the results for both
precision and recall are generally improved in comparison to                                      REFERENCES
those not considering it.",2022-11-21 11:34:31+00:00,First CE Matters: On the Importance of Long Term Properties on Memory Failure Prediction,cs.DC,['cs.DC'],"[arxiv.Result.Author('Jasmin Bogatinovski'), arxiv.Result.Author('Qiao Yu'), arxiv.Result.Author('Jorge Cardoso'), arxiv.Result.Author('Odej Kao')]","Dynamic random access memory failures are a threat to the reliability of data
centres as they lead to data loss and system crashes. Timely predictions of
memory failures allow for taking preventive measures such as server migration
and memory replacement. Thereby, memory failure prediction prevents failures
from externalizing, and it is a vital task to improve system reliability. In
this paper, we revisited the problem of memory failure prediction. We analyzed
the correctable errors (CEs) from hardware logs as indicators for a degraded
memory state. As memories do not always work with full occupancy, access to
faulty memory parts is time distributed. Following this intuition, we observed
that important properties for memory failure prediction are distributed through
long time intervals. In contrast, related studies, to fit practical
constraints, frequently only analyze the CEs from the last fixed-size time
interval while ignoring the predating information. Motivated by the observed
discrepancy, we study the impact of including the overall (long-range) CE
evolution and propose novel features that are calculated incrementally to
preserve long-range properties. By coupling the extracted features with machine
learning methods, we learn a predictive model to anticipate upcoming failures
three hours in advance while improving the average relative precision and
recall for 21% and 19% accordingly. We evaluated our methodology on real-world
memory failures from the server fleet of a large cloud provider, justifying its
validity and practicality.",-0.0699165,-0.17448595,-0.24704264,B
15413,opportunity for further research and development.,"Whilst classical volunteer projects,     This area of web-based distributed computing is still in its
driven by user-installable frameworks such as BOINC, have
seen stagnation in recent years, the enhancements made to          infancy and not yet been deployed in the real-world for urgent
web browsers make this technology an ideal mechanism for           workloads, and therefore we believe that there is a signiﬁcant
distributed computing.","One obvious
                                                                   next step is to support kernels written in WebAssembly which
   Leveraging the web for volunteer distributed computing          should increase performance signiﬁcantly.Throttling should
is highly beneﬁcial because there is a zero barrier to entry       also be considered, as currently the assumption is that a task
for end-users, and HTML5 standardisation makes developing          will consume as much CPU on a single core as needed whereas
computational kernels simpler than classical approaches such       in reality this is too simplistic.",2022-12-28 17:56:47+00:00,Web-based volunteer distributed computing for handling time-critical urgent workloads,cs.DC,['cs.DC'],"[arxiv.Result.Author('Nick Brown'), arxiv.Result.Author('Simon Newby')]","Urgent computing workloads are time critical, unpredictable, and highly
dynamic. Whilst efforts are on-going to run these on traditional HPC machines,
another option is to leverage the computing power donated by volunteers.
Volunteer computing, where members of the public donate some of their CPU time
to large scale projects has been popular for many years because it is a
powerful way of delivering compute for specific problems, with the public often
eager to contribute to a good cause with societal benefits. However,
traditional volunteer computing has required user installation of specialist
software which is a barrier to entry, and the development of the software
itself by the projects, even on-top of existing frameworks, is non-trivial.
  As such, the number of users donating CPU time to these volunteer computing
projects has decreased in recent years, and this comes at a time when the
frequency of disasters, often driven by climate change, are rising fast. We
believe that an alternative approach, where visitors to websites donate some of
their CPU time whilst they are browsing, has the potential to address these
issues. However, web-based distributed computing is an immature field and there
are numerous questions that must be answered to fully understand the viability
of leveraging the large scale parallelism that website visitors represent. In
this paper we describe our web-based distributed computing framework, Panther,
and perform in-depth performance experiments for two benchmarks using real
world hardware and real world browsing habits for the first time. By exploring
the performance characteristics of our approach we demonstrate that this is
viable for urgent workloads, but there are numerous caveats, not least the most
appropriate visitor patterns to a website, that must be considered.",-0.15977937,-0.18869953,0.3085357,C
