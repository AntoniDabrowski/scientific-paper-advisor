Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
2109,"Determining the exact reason
will require further study.","One possible reason is that
the parametric dimension in this case is much lower regarding the sensitivity of turbulence model coefficients, for
which 200 prior-based evaluation points can almost meet the accuracy requirements.","a) MSE  b) Violin plots of posterior distributions for Cp at
                        z/c=0.625 (top) and 1.025 (bottom)

    Fig.",2022-02-17 07:44:13+00:00,Adaptive Model Refinement Approach for Bayesian Uncertainty Quantification in Turbulence Model,physics.data-an,"['physics.data-an', 'physics.flu-dyn']","[arxiv.Result.Author('Fanzhi Zeng'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Jinping Li'), arxiv.Result.Author('Tianxin Zhang'), arxiv.Result.Author('Chao Yan')]","The Bayesian uncertainty quantification technique has become well established
in turbulence modeling over the past few years. However, it is computationally
expensive to construct a globally accurate surrogate model for Bayesian
inference in a high-dimensional design space, which limits uncertainty
quantification for complex flow configurations. Borrowing ideas from stratified
sampling and inherited sampling, an adaptive model refinement approach is
proposed in this work, which concentrates on asymptotically improving the local
accuracy of the surrogate model in the high-posterior-density region by
adaptively appending model evaluation points. To achieve this goal, a
modification of inherited Latin hypercube sampling is proposed and then
integrated into the Bayesian framework. The effectiveness and efficiency of the
proposed approach are demonstrated through a two-dimensional heat source
inversion problem and its extension to a high-dimensional design space.
Compared with the prior-based method, the adaptive model refinement approach
has the ability to obtain more reliable inference results using fewer
evaluation points. Finally, the approach is applied to parametric uncertainty
quantification of the Menter shear-stress transport turbulence model for an
axisymmetric transonic bump flow and provides convincing numerical results.",-0.15928125,0.057147585,0.5285922,B
2621,"This could
potentially be attributed to limited training statistics for high-energy neutral particles, which
are correspondingly not well reconstructed by the MLPF algorithm in the current iteration and
require further study.","However, for MET, we observe a misreconstructed
high-MET tail that is most prominent for the QCD sample not used in training.","We report the computational performance of the model on a single stream on a single GPU
in ﬁg.",2022-03-01 10:11:44+00:00,Machine Learning for Particle Flow Reconstruction at CMS,physics.data-an,"['physics.data-an', 'cs.LG', 'hep-ex', 'physics.ins-det', 'stat.ML']","[arxiv.Result.Author('Joosep Pata'), arxiv.Result.Author('Javier Duarte'), arxiv.Result.Author('Farouk Mokhtar'), arxiv.Result.Author('Eric Wulff'), arxiv.Result.Author('Jieun Yoo'), arxiv.Result.Author('Jean-Roch Vlimant'), arxiv.Result.Author('Maurizio Pierini'), arxiv.Result.Author('Maria Girone')]","We provide details on the implementation of a machine-learning based particle
flow algorithm for CMS. The standard particle flow algorithm reconstructs
stable particles based on calorimeter clusters and tracks to provide a global
event reconstruction that exploits the combined information of multiple
detector subsystems, leading to strong improvements for quantities such as jets
and missing transverse energy. We have studied a possible evolution of particle
flow towards heterogeneous computing platforms such as GPUs using a graph
neural network. The machine-learned PF model reconstructs particle candidates
based on the full list of tracks and calorimeter clusters in the event. For
validation, we determine the physics performance directly in the CMS software
framework when the proposed algorithm is interfaced with the offline
reconstruction of jets and missing transverse energy. We also report the
computational performance of the algorithm, which scales approximately linearly
in runtime and memory usage with the input size.",0.21728812,-0.39861667,0.44450325,C
2873,"Third,
                                                                 numerical results presented thus far only entail isostatic sys-
   We analytically demonstrated that the approach never over-    tems, thereby calling for further research on hyperstatic en-
estimates the number of DOFs and yields exact inferences un-     gineering structures, bending-dominated frames, and random
der broad conditions on the quality and quantity of measure-     beam lattices.",ses regarding the applicability of the detection matrix.,ments for linear dynamics without noise.,2022-03-05 23:14:59+00:00,The detection matrix as a model-agnostic tool to estimate the number of degrees of freedom in mechanical systems and engineering structures,physics.data-an,['physics.data-an'],"[arxiv.Result.Author('Paolo Celli'), arxiv.Result.Author('Maurizio Porfiri')]","Estimating the number of degrees of freedom of a mechanical system or an
engineering structure from the time-series of a small set of sensors is a basic
problem in diagnostics, which, however, is often overlooked when monitoring
their health and integrity. In this work, we demonstrate the applicability of
the network-theoretic concept of detection matrix as a tool to solve this
problem. From this estimation, we illustrate the possibility to identify
damage. The detection matrix, recently introduced by Haehne et al. in the
context of network theory, is assembled from the transient response of a few
nodes as a result of non-zero initial conditions: its rank offers an estimate
of the number of nodes in the network itself. The use of the detection matrix
is completely model-agnostic, whereby it does not require any knowledge of the
system dynamics. Here, we show that, with a few modifications, this same
principle applies to discrete systems, such as spring-mass lattices and
trusses. Moreover, we discuss how damage in one or more members causes the
appearance of distinct jumps in the singular values of this matrix, thereby
opening the door to structural health monitoring applications, without the need
for a complete model reconstruction.",-0.1674082,-0.32704586,0.066363,B
3730,"Although further research is needed
free counterparts via the PJSD.","Actually, noise-induced chaos has already
total of 41 values of r) and noise levels σ between 0.0005         been conﬁrmed for some of the bifurcation parameter val-
and 0.002 with step ∆σ = 0.0005 (giving a total of 40              ues our analysis highlights, namely r = 3.63, r = 3.74
values of σ) are ordinally contrasted against their noise-         and r = 3.83 [74].","More precisely, average            to conﬁrm the chaotic nature of the noisy realizations, we
                                                                                                                                                             9

       10-3                                                             10-3                                                      emphasize that the idea of implementing ordinal related
                                                            0.3                                                              0.6  tools as a way of assessing the irreversibility of a time se-
                                                                                                                                  ries is not new and has been recently explored by Zanin et
0.5                                                              0.5                                                              al.",2022-03-23 00:31:19+00:00,Permutation Jensen-Shannon distance: A versatile and fast symbolic tool for complex time series analysis,physics.data-an,['physics.data-an'],"[arxiv.Result.Author('Luciano Zunino'), arxiv.Result.Author('Felipe Olivares'), arxiv.Result.Author('Haroldo V. Ribeiro'), arxiv.Result.Author('Osvaldo A. Rosso')]","The main motivation of this paper is to introduce the permutation
Jensen-Shannon distance, a symbolic tool able to quantify the degree of
similarity between two arbitrary time series. This quantifier results from the
fusion of two concepts, the Jensen-Shannon divergence and the encoding scheme
based on the sequential ordering of the elements in the data series. The
versatility and robustness of this ordinal symbolic distance for characterizing
and discriminating different dynamics are illustrated through several numerical
and experimental applications. Results obtained allow us to be optimistic about
its usefulness in the field of complex time series analysis. Moreover, thanks
to its simplicity, low computational cost, wide applicability and less
susceptibility to outliers and artifacts, this ordinal measure can efficiently
handle large amounts of data and help to tackle the current big data
challenges.",-0.10892533,0.34875327,0.28926468,B
4491,"Sec-          ratio:
tion IV discusses potential objections and limitations,
and section V concludes with a summary and sugges-            t(s) = u(s) · sign(µ(sˆ) − µ(s)),       (3)
tions for further research.","We will use a signed likelihood
gion is known to be free of unknown background.","The appendices prove some
properties of likelihood ratios used in the text (A), dis-    which is positive for excesses (µ(sˆ) > µ(s)) and nega-
cuss computational costs and ways to reduce it (B), and       tive for deﬁcits.",2022-04-07 07:38:21+00:00,Deficit hawks: robust new physics searches with unknown backgrounds,physics.data-an,"['physics.data-an', 'hep-ex', 'stat.AP']",[arxiv.Result.Author('Jelle Aalbers')],"Searches for new physics often face unknown backgrounds, causing false
detections or weakened upper limits. This paper introduces the deficit hawk
technique, which mitigates unknown backgrounds by testing multiple options for
data cuts, such as fiducial volumes or energy thresholds. Combining the power
of likelihood ratios with the robustness of the interval-searching techniques,
deficit hawks could double the physics reach of experiments with partial or
speculative background knowledge. Deficit hawks are well-suited to analyses
that use machine learning or other multidimensional discrimination techniques,
and permit discoveries in regions without unknown background.",0.6199094,0.004896112,-0.10508181,C
4492,"We will use a signed likelihood
and section V concludes with a summary and sugges-           ratio:
tions for further research.","Sec-            If s controls a signal strength, it is useful to distinguish
tion IV discusses potential objections and limitations,      excesses and deﬁcits.","The appendices prove some
properties of likelihood ratios used in the text (A), dis-   t(s) = u(s) · sign(µ(sˆ) − µ(s)),                 (3)
cuss computational costs and ways to reduce it (B), and
show performance tests for additional background sce-        which is positive for excesses (µ(sˆ) > µ(s)) and negative
narios (C).",2022-04-07 07:38:21+00:00,Deficit hawks: robust new physics searches with unknown backgrounds,physics.data-an,"['physics.data-an', 'hep-ex', 'stat.AP']",[arxiv.Result.Author('Jelle Aalbers')],"Searches for new physics often face unknown backgrounds, causing false
detections or weakened upper limits. This paper introduces the deficit hawk
technique, which mitigates unknown backgrounds by testing multiple options for
data cuts, such as fiducial volumes or energy thresholds. Combining the power
of likelihood ratios with the robustness of the interval-searching techniques,
deficit hawks could improve mean upper limits on new physics by a factor two
for experiments with partial or speculative background knowledge. Deficit hawks
are well-suited to analyses that use machine learning or other multidimensional
discrimination techniques, and can be extended to permit discoveries in regions
without unknown background.",0.5246128,-0.1558569,-0.22097121,C_centroid
6511,"As mentioned
in Remarks 11 and 12, its control on the probability of the type II error (not
rejecting when the covariances of the two worlds are actually diﬀerent) need
further study.","The residual consistency test (RCT) proposed by AT99 is a valid test for
checking on the agreement between the covariances (C˜ N vs CN ) of the two
worlds as shown in 2.3, which means that a proper control of the probability
of the type I error (rejecting when the underlying covariance matrix of the null
simulation does match CN of the physical world) can be realized.","Moreover, we outline the connection between the ”optimal ﬁngerprinting”
approach and the Feasible Generalized Least Square, which we hope would
connect the ﬁeld of climate change studies and the statistical approach for
conducting generalized least square regression.",2022-05-21 05:42:12+00:00,A Review on the Optimal Fingerprinting Approach in Climate Change Studies,physics.data-an,"['physics.data-an', 'physics.ao-ph', 'physics.geo-ph']","[arxiv.Result.Author('Hanyue Chen'), arxiv.Result.Author('Song Xi Chen'), arxiv.Result.Author('Mu Mu')]","We provide a review on the ""optimal fingerprinting"" approach as summarized in
Allen and Tett (1999) from a point view of statistical inference in light of
the recent criticism of McKitrick (2021). Our review finds that the ""optimal
fingerprinting"" approach would survive much of McKitrick (2021)'s criticism
under two conditions: (i) the null simulation of the climate model is
independent of the physical observations and (ii) the null simulation provides
consistent estimation of the residual covariance matrix of the physical
observations, both depend on the conduction and the quality of the climate
models. If the latter condition fails, the estimator would be still unbiased
and consistent under routine conditions, but losing the ""optimal"" aspect of the
approach. The residual consistency test suggested by Allen and Tett (1999) is
valid for checking the agreement between the residual covariances of the null
simulation and the physical observations. We further outline the connection
between the ""optimal fingerprinting"" approach and the Feasible Generalized
Least Square.",-0.35765028,0.4120019,0.06613684,A_centroid
6925,"The last Section V contains conclusion and
discussion of the prospects for the further researches.","In the end of this Section, we apply our ap-
proach to deﬁne the information temperature for some                                                   r=1
literary texts.","where we use the concise notation aN1 = a1, a2, ..., aN .",2022-05-18 18:29:17+00:00,Information entropy and temperature of the binary Markov chains,physics.data-an,"['physics.data-an', 'cond-mat.stat-mech', 'physics.class-ph']","[arxiv.Result.Author('O. V. Usatenko'), arxiv.Result.Author('S. S. Melnyk'), arxiv.Result.Author('G. M. Pritula'), arxiv.Result.Author(""V. A. Yampol'skii"")]","We propose two different approaches for introducing the information
temperature of the binary N-th order Markov chains. The first approach is based
on comparing the Markov sequences with the equilibrium Ising chains at given
temperatures. The second approach uses probabilities of finite-length
subsequences of symbols occurring, which determine their entropies. The
derivative of the entropy with respect to the energy gives the information
temperature measured on the scale of introduced energy. For the case of
nearest-neighbor spin/symbol interaction, both approaches provide similar
results. However, the method based on the correspondence of the N-step Markov
and Ising chains appears to be very cumbersome for N>3. We also introduce the
information temperature for the weakly correlated one-parametric Markov chains
and present results for the step-wise and power memory functions. An
application of the developed method to obtain the information temperature of
some literary texts is given.",0.27171817,0.6521637,-0.21200594,A
9167,"Yet, further research into
this direction is not subject of this work.","Hereby, physically reasonable
assumptions could be involved such as that the time scale of the noise process
should not be larger than the time scale of the measured process, which
would simplify the interpretation of the model.","Instead, we discuss Bayesian estimation of a speciﬁc version of the HLE
model.",2022-07-21 17:47:06+00:00,Efficient Bayesian estimation of a non-Markovian Langevin model driven by correlated noise,physics.data-an,['physics.data-an'],"[arxiv.Result.Author('Clemens Willers'), arxiv.Result.Author('Oliver Kamps')]","Data-driven modeling of non-Markovian dynamics is a recent topic of research
with applications in many fields such as climate research, molecular dynamics,
biophysics, or wind power modeling. In the frequently used standard Langevin
equation, memory effects can be implemented through an additional hidden
component which functions as correlated noise, thus resulting in a
non-Markovian model. It can be seen as part of the model class of partially
observed diffusions which are usually adapted to observed data via Bayesian
estimation, whereby the difficulty of the unknown noise values is solved
through a Gibbs sampler. However, when regarding large data sets with a length
of $10^6$ or $10^7$ data points, sampling the distribution of the same amount
of latent variables is unfeasible. For the model discussed in this work, we
solve this issue through a direct derivation of the posterior distribution of
the Euler-Maruyama approximation of the model via analytical marginalization of
the latent variables. Yet, in the case of a nonlinear noise process, the
inverse problem of model estimation proves to be ill-posed and still
numerically expensive. We handle these complications by restricting the noise
to an Ornstein-Uhlenbeck process, which considerably reduces the ambiguity of
the estimation. Further, in this case, the estimation can be performed very
efficiently if the drift and diffusion functions of the observed component are
approximated in a piecewise constant manner. We illustrate the resulting
procedure of efficient Bayesian estimation of the considered non-Markovian
Langevin model by an example from turbulence.",-0.32661742,-0.06673409,-0.35417742,B_centroid
9168,"Another interesting question for further research is whether an eﬃcient
calculation of the posterior of the general HLE model could be possible as
well.",This situation might apply to quite a few applications.,"This would facilitate an improvement of MAP estimation via global
optimization techniques.",2022-07-21 17:47:06+00:00,Efficient Bayesian estimation of a non-Markovian Langevin model driven by correlated noise,physics.data-an,['physics.data-an'],"[arxiv.Result.Author('Clemens Willers'), arxiv.Result.Author('Oliver Kamps')]","Data-driven modeling of non-Markovian dynamics is a recent topic of research
with applications in many fields such as climate research, molecular dynamics,
biophysics, or wind power modeling. In the frequently used standard Langevin
equation, memory effects can be implemented through an additional hidden
component which functions as correlated noise, thus resulting in a
non-Markovian model. It can be seen as part of the model class of partially
observed diffusions which are usually adapted to observed data via Bayesian
estimation, whereby the difficulty of the unknown noise values is solved
through a Gibbs sampler. However, when regarding large data sets with a length
of $10^6$ or $10^7$ data points, sampling the distribution of the same amount
of latent variables is unfeasible. For the model discussed in this work, we
solve this issue through a direct derivation of the posterior distribution of
the Euler-Maruyama approximation of the model via analytical marginalization of
the latent variables. Yet, in the case of a nonlinear noise process, the
inverse problem of model estimation proves to be ill-posed and still
numerically expensive. We handle these complications by restricting the noise
to an Ornstein-Uhlenbeck process, which considerably reduces the ambiguity of
the estimation. Further, in this case, the estimation can be performed very
efficiently if the drift and diffusion functions of the observed component are
approximated in a piecewise constant manner. We illustrate the resulting
procedure of efficient Bayesian estimation of the considered non-Markovian
Langevin model by an example from turbulence.",-0.50751406,-0.20504522,-0.34662223,B
13927,"is still employed and a multimodal signal model on top
                                                                   of this creates a sampling problem that many common
   Rather than further study the properties of derived             numerical sampling tools would struggle with.","(1) being              approach, the multimodal background model built earlier
identiﬁed as the null hypothesis, H0.","intervals on signal models with strong priors – or validat-
ing the properties of frequentist vs. Bayesian discovery              For the purposes of this study a generic Gaussian signal
metrics – a generic anomaly detection task is considered.",2022-11-18 17:48:13+00:00,Hunting for bumps in the margins,physics.data-an,"['physics.data-an', 'hep-ex', 'hep-ph']","[arxiv.Result.Author('David Yallup'), arxiv.Result.Author('Will Handley')]","Data driven modelling is vital to many analyses at collider experiments,
however the derived inference of physical properties becomes subject to details
of the model fitting procedure. This work brings a principled Bayesian picture,
based on the marginal likelihood, of both data modelling and signal extraction
to a common collider physics scenario. First the marginal likelihood based
method is used to propose a more principled construction of the background
process, systematically exploring a variety of candidate shapes. Second the
picture is extended to propose the marginal likelihood as a useful tool for
anomaly detection challenges in particle physics. This proposal offers insight
into both precise background model determination and demonstrates a flexible
method to extend signal determination beyond a simple bump hunt.",-0.0061322404,-0.32166368,-0.15600125,B
