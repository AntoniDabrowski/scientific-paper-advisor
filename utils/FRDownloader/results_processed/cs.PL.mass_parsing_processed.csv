Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract,x,y,z,cluster
345,"Research goals and questions

 The goal of this article is to explore the literature on macroprogramming in breadth, synthe-
 sise the major contributions, and provide a basis for further research.","More
 motivation is given by the urge of the following research questions.","The focus is on the
 programming perspective, rather than e.g.",2022-01-10 17:17:54+00:00,"Macroprogramming: Concepts, State of the Art, and Opportunities of Macroscopic Behaviour Modelling",cs.PL,"['cs.PL', 'cs.DC', 'cs.MA', 'cs.SE', 'cs.SY', 'eess.SY']",[arxiv.Result.Author('Roberto Casadei')],"Macroprogramming refers to the theory and practice of conveniently expressing
the macro(scopic) behaviour of a system using a single program.
Macroprogramming approaches are motivated by the need of effectively capturing
global/system-level aspects and the collective behaviour of a set of
interacting components, while abstracting over low-level details. In the past,
this style of programming has been primarily adopted to describe the
data-processing logic in wireless sensor networks; recently, research forums on
spatial computing, collective adaptive systems, and Internet-of-Things have
provided renewed interest in macro-approaches. However, related contributions
are still fragmented and lacking conceptual consistency. Therefore, to foster
principled research, an integrated view of the field is provided, together with
opportunities and challenges.",-0.1724126,-0.015130027,-0.15207176,A
492,we anticipate the need for further research work.,"This is diﬀerent for Step (4), where
16  Krijnen et al.","This includes more composi-
tional deﬁnitions of the translation relations, such that we can generate at least
part of the decision procedures (semi-)automatically.",2022-01-13 12:26:31+00:00,Translation Certification for Smart Contracts,cs.PL,"['cs.PL', 'D.3.4; F.3.1; D.1.1']","[arxiv.Result.Author('Jacco O. G. Krijnen'), arxiv.Result.Author('Manuel M. T. Chakravarty'), arxiv.Result.Author('Gabriele Keller'), arxiv.Result.Author('Wouter Swierstra')]","Compiler correctness is an old problem, but with the emergence of smart
contracts on blockchains that problem presents itself in a new light. Smart
contracts are self-contained pieces of software that control assets, which are
often of high financial value, in an adversarial environment and, once
committed to the blockchain, they cannot be changed anymore. Smart contracts
are typically developed in a high-level contract language and compiled to
low-level virtual machine code before being committed to the blockchain. For a
smart contract user to trust a given piece of low-level code on the blockchain,
they must convince themselves that (a) they are in possession of the matching
source code and (b) that the compiler faithfully translated the source code's
semantics.
  Classic approaches to compiler correctness tackle the second point. We argue
that translation certification also addresses the first. We describe the proof
architecture of a novel translation certification framework, implemented in
Coq, for a functional smart contract language. We demonstrate that we can model
the compilation pipeline as a sequence of translation relations that facilitate
a modular proof approach and are robust in the face of an evolving compiler
implementation.",0.15133402,0.6569765,-0.032104142,B_centroid
493,"This is diﬀerent for Step (4), where
we anticipate the need for further research work.","The ﬁrst three steps pose a signiﬁcant amount of work, but we do not expect
major new conceptual questions or obstacles.","This includes more composi-
tional deﬁnitions of the translation relations, such that we can generate at least
part of the decision procedures (semi-)automatically.",2022-01-13 12:26:31+00:00,Translation Certification for Smart Contracts,cs.PL,"['cs.PL', 'D.3.4; F.3.1; D.1.1']","[arxiv.Result.Author('Jacco O. G. Krijnen'), arxiv.Result.Author('Manuel M. T. Chakravarty'), arxiv.Result.Author('Gabriele Keller'), arxiv.Result.Author('Wouter Swierstra')]","Compiler correctness is an old problem, but with the emergence of smart
contracts on blockchains that problem presents itself in a new light. Smart
contracts are self-contained pieces of software that control assets, which are
often of high financial value, in an adversarial environment and, once
committed to the blockchain, they cannot be changed anymore. Smart contracts
are typically developed in a high-level contract language and compiled to
low-level virtual machine code before being committed to the blockchain. For a
smart contract user to trust a given piece of low-level code on the blockchain,
they must convince themselves that (a) they are in possession of the matching
source code and (b) that the compiler faithfully translated the source code's
semantics.
  Classic approaches to compiler correctness tackle the second point. We argue
that translation certification also addresses the first. We describe the proof
architecture of a novel translation certification framework, implemented in
Coq, for a functional smart contract language. We demonstrate that we can model
the compilation pipeline as a sequence of translation relations that facilitate
a modular proof approach and are robust in the face of an evolving compiler
implementation.",0.17882821,0.7064126,-0.0314333,B
1040,"2.3 The Birth of Prolog

By 1972, Colmerauer’s aim of creating a human-machine communication system in logic had
led him to further research French language analysis with Pasero (1973), and to numerous ex-
periments with Philippe Roussel and Jean Trudel on automated theorem proving methods.","Another predecessor of Prolog was Ed Elcock’s Aberdeen System, Absys, from 1967, even if
it did not directly inﬂuence the development of Prolog (Elcock, 1990).","Ex-
changes with Kowalski during Kowalski’s visits to Marseille in 1971 and 1972 determined the
choice of SL-resolution for Roussel’s thesis on formal equality in automated theorem-proving
(1972), since it seemed to be the most interesting resolution system to manage procedural calls
and to deal with backtracking a` la Floyd.",2022-01-26 08:43:00+00:00,50 Years of Prolog and Beyond,cs.PL,['cs.PL'],"[arxiv.Result.Author('Philipp Körner'), arxiv.Result.Author('Michael Leuschel'), arxiv.Result.Author('João Barbosa'), arxiv.Result.Author('Vítor Santos Costa'), arxiv.Result.Author('Verónica Dahl'), arxiv.Result.Author('Manuel V. Hermenegildo'), arxiv.Result.Author('Jose F. Morales'), arxiv.Result.Author('Jan Wielemaker'), arxiv.Result.Author('Daniel Diaz'), arxiv.Result.Author('Salvador Abreu'), arxiv.Result.Author('Giovanni Ciatto')]","Both logic programming in general, and Prolog in particular, have a long and
fascinating history, intermingled with that of many disciplines they inherited
from or catalyzed. A large body of research has been gathered over the last 50
years, supported by many Prolog implementations. Many implementations are still
actively developed, while new ones keep appearing. Often, the features added by
different systems were motivated by the interdisciplinary needs of programmers
and implementors, yielding systems that, while sharing the ""classic"" core
language, and, in particular, the main aspects of the ISO-Prolog standard, also
depart from each other in other aspects. This obviously poses challenges for
code portability. The field has also inspired many related, but quite different
languages that have created their own communities.
  This article aims at integrating and applying the main lessons learned in the
process of evolution of Prolog. It is structured into three major parts.
Firstly, we overview the evolution of Prolog systems and the community
approximately up to the ISO standard, considering both the main historic
developments and the motivations behind several Prolog implementations, as well
as other logic programming languages influenced by Prolog. Then, we discuss the
Prolog implementations that are most active after the appearance of the
standard: their visions, goals, commonalities, and incompatibilities. Finally,
we perform a SWOT analysis in order to better identify the potential of Prolog,
and propose future directions along which Prolog might continue to add useful
features, interfaces, libraries, and tools, while at the same time improving
compatibility between implementations.",0.5599381,-0.28387004,-0.07632115,C
1041,"2.3 The Birth of Prolog

Colmerauer’s aim of creating a human-machine communication system in logic had led him to
further research French language analysis with Pasero (1973), and to numerous experiments with
Philippe Roussel and Jean Trudel on automated theorem proving methods.","The semantics of Horn
clauses was explored by Kowalski and van Emden (1976).","Having learned about
SL-resolution, he invited Kowalski to visit Marseille in the summer of 1971.",2022-01-26 08:43:00+00:00,Fifty Years of Prolog and Beyond,cs.PL,['cs.PL'],"[arxiv.Result.Author('Philipp Körner'), arxiv.Result.Author('Michael Leuschel'), arxiv.Result.Author('João Barbosa'), arxiv.Result.Author('Vítor Santos Costa'), arxiv.Result.Author('Verónica Dahl'), arxiv.Result.Author('Manuel V. Hermenegildo'), arxiv.Result.Author('Jose F. Morales'), arxiv.Result.Author('Jan Wielemaker'), arxiv.Result.Author('Daniel Diaz'), arxiv.Result.Author('Salvador Abreu'), arxiv.Result.Author('Giovanni Ciatto')]","Both logic programming in general, and Prolog in particular, have a long and
fascinating history, intermingled with that of many disciplines they inherited
from or catalyzed. A large body of research has been gathered over the last 50
years, supported by many Prolog implementations. Many implementations are still
actively developed, while new ones keep appearing. Often, the features added by
different systems were motivated by the interdisciplinary needs of programmers
and implementors, yielding systems that, while sharing the ""classic"" core
language, and, in particular, the main aspects of the ISO-Prolog standard, also
depart from each other in other aspects. This obviously poses challenges for
code portability. The field has also inspired many related, but quite different
languages that have created their own communities.
  This article aims at integrating and applying the main lessons learned in the
process of evolution of Prolog. It is structured into three major parts.
Firstly, we overview the evolution of Prolog systems and the community
approximately up to the ISO standard, considering both the main historic
developments and the motivations behind several Prolog implementations, as well
as other logic programming languages influenced by Prolog. Then, we discuss the
Prolog implementations that are most active after the appearance of the
standard: their visions, goals, commonalities, and incompatibilities. Finally,
we perform a SWOT analysis in order to better identify the potential of Prolog,
and propose future directions along which Prolog might continue to add useful
features, interfaces, libraries, and tools, while at the same time improving
compatibility between implementations.
  Under consideration in Theory and Practice of Logic Programming (TPLP)",0.6344981,-0.26639578,-0.031785265,C_centroid
1042,"2.3 The Birth of Prolog

Colmerauer’s aim of creating a human-machine communication system in logic had led
him to further research French language analysis with Pasero (1973), and to numer-
ous experiments with Philippe Roussel and Jean Trudel on automated theorem proving
methods.","The
semantics of Horn clauses was explored by Kowalski and van Emden (1976).","Having learned about SL-resolution, he invited Kowalski to visit Marseille in
the summer of 1971.",2022-01-26 08:43:00+00:00,Fifty Years of Prolog and Beyond,cs.PL,['cs.PL'],"[arxiv.Result.Author('Philipp Körner'), arxiv.Result.Author('Michael Leuschel'), arxiv.Result.Author('João Barbosa'), arxiv.Result.Author('Vítor Santos Costa'), arxiv.Result.Author('Verónica Dahl'), arxiv.Result.Author('Manuel V. Hermenegildo'), arxiv.Result.Author('Jose F. Morales'), arxiv.Result.Author('Jan Wielemaker'), arxiv.Result.Author('Daniel Diaz'), arxiv.Result.Author('Salvador Abreu'), arxiv.Result.Author('Giovanni Ciatto')]","Both logic programming in general, and Prolog in particular, have a long and
fascinating history, intermingled with that of many disciplines they inherited
from or catalyzed. A large body of research has been gathered over the last 50
years, supported by many Prolog implementations. Many implementations are still
actively developed, while new ones keep appearing. Often, the features added by
different systems were motivated by the interdisciplinary needs of programmers
and implementors, yielding systems that, while sharing the ""classic"" core
language, and, in particular, the main aspects of the ISO-Prolog standard, also
depart from each other in other aspects. This obviously poses challenges for
code portability. The field has also inspired many related, but quite different
languages that have created their own communities.
  This article aims at integrating and applying the main lessons learned in the
process of evolution of Prolog. It is structured into three major parts.
Firstly, we overview the evolution of Prolog systems and the community
approximately up to the ISO standard, considering both the main historic
developments and the motivations behind several Prolog implementations, as well
as other logic programming languages influenced by Prolog. Then, we discuss the
Prolog implementations that are most active after the appearance of the
standard: their visions, goals, commonalities, and incompatibilities. Finally,
we perform a SWOT analysis in order to better identify the potential of Prolog,
and propose future directions along which Prolog might continue to add useful
features, interfaces, libraries, and tools, while at the same time improving
compatibility between implementations.",0.63030314,-0.2721554,-0.028781675,C
2182,"By studying united monoids, we provide a theoretical foundation
for further research in this area.","By extending algebraic graphs with support for edge labels, we make them suitable for a
much larger class of possible applications.","ACM CCS 2012
    Mathematics of computing → Graph theory;

Keywords algebra, graphs, functional programming

The Art, Science, and Engineering of Programming

Submitted October 1, 2021

Published February 15, 2022

doi  10.22152/programming-journal.org/2022/6/12

     © Andrey Mokhov
     This work is licensed under a “CC BY 4.0” license.",2022-02-18 14:54:57+00:00,United Monoids: Finding Simplicial Sets and Labelled Algebraic Graphs in Trees,cs.PL,['cs.PL'],[arxiv.Result.Author('Andrey Mokhov')],"Graphs and various graph-like combinatorial structures, such as preorders and
hypergraphs, are ubiquitous in programming. This paper focuses on representing
graphs in a purely functional programming language like Haskell. There are
several existing approaches; one of the most recently developed ones is the
""algebraic graphs"" approach (2017). It uses an algebraic data type to represent
graphs and has attracted users, including from industry, due to its emphasis on
equational reasoning and making a common class of bugs impossible by
eliminating internal invariants.
  The previous formulation of algebraic graphs did not support edge labels,
which was a serious practical limitation. In this paper, we redesign the main
algebraic data type and remove this limitation. We follow a fairly standard
approach of parameterising a data structure with a semiring of edge labels. The
new formulation is both more general and simpler: the two operations for
composing graphs used in the previous work can now be obtained from a single
operation by fixing the semiring parameter to zero and one, respectively.
  By instantiating the new data type with different semirings, and working out
laws for interpreting the resulting expression trees, we discover an unusual
algebraic structure, which we call ""united monoids"", that is, a pair of monoids
whose unit elements coincide. We believe that it is worth studying united
monoids in their full generality, going beyond the graphs which prompted their
discovery. To that end, we characterise united monoids with a minimal set of
axioms, prove a few basic theorems, and discuss several notable examples.
  We validate the presented approach by implementing it in the open-source
*algebraic-graphs* library. Our theoretical contributions are supported by
proofs that are included in the paper and have also been machine-checked in
Agda. By extending algebraic graphs with support for edge labels, we make them
suitable for a much larger class of possible applications. By studying united
monoids, we provide a theoretical foundation for further research in this area.",-0.3590852,-0.16628358,0.038000427,A
2504,"It takes strings 𝑎 and 𝑏 with length 8
on directions for further research in this area.",We report interesting case studies which shed light        program gabfeed_1 [46].,"as inputs, denoting the user-entered and correct passwords,
                                                                    respectively.",2022-02-26 13:06:15+00:00,Preventing Timing Side-Channels via Security-Aware Just-In-Time Compilation,cs.PL,"['cs.PL', 'cs.CR']","[arxiv.Result.Author('Qi Qin'), arxiv.Result.Author('JulianAndres JiYang'), arxiv.Result.Author('Fu Song'), arxiv.Result.Author('Taolue Chen'), arxiv.Result.Author('Xinyu Xing')]","Recent work has shown that Just-In-Time (JIT) compilation can introduce
timing side-channels to constant-time programs, which would otherwise be a
principled and effective means to counter timing attacks. In this paper, we
propose a novel approach to eliminate JIT-induced leaks from these programs.
Specifically, we present an operational semantics and a formal definition of
constant-time programs under JIT compilation, laying the foundation for
reasoning about programs with JIT compilation. We then propose to eliminate
JIT-induced leaks via a fine-grained JIT compilation for which we provide an
automated approach to generate policies and a novel type system to show its
soundness. We develop a tool DeJITLeak for Java based on our approach and
implement the fine-grained JIT compilation in HotSpot. Experimental results
show that DeJITLeak can effectively and efficiently eliminate JIT-induced leaks
on three datasets used in side-channel detection",-0.103426725,0.1107992,-0.48307598,B
6588,"However, there are a number of directions for improvement
and further research.","12 CONCLUSIONS & FUTURE WORK

We have presented a sequence of natural optimisation steps that transform a reverse AD algorithm
that can be very rigorously proven correct but is very inefficient, to one that has the correct time
complexity with a good constant factor.","• The sequentially generated integer IDs form a very crude over-approximation of the depen-
       dency structure of the backpropagators, namely to a linear graph.",2022-05-23 14:58:01+00:00,"Dual-Numbers Reverse AD, Efficiently",cs.PL,['cs.PL'],"[arxiv.Result.Author('Tom Smeding'), arxiv.Result.Author('Matthijs Vákár')]","Where dual-numbers forward-mode automatic differentiation (AD) pairs each
scalar value with its tangent derivative, dual-numbers /reverse-mode/ AD
attempts to achieve reverse AD using a similarly simple idea: by pairing each
scalar value with a backpropagator function. Its correctness and efficiency on
higher-order input languages have been analysed by Brunel, Mazza and Pagani,
but this analysis was on a custom operational semantics for which it is unclear
whether it can be implemented efficiently. We take inspiration from their use
of /linear factoring/ to optimise dual-numbers reverse-mode AD to an algorithm
that has the correct complexity and enjoys an efficient implementation in a
standard functional language with resource-linear types, such as Haskell. Aside
from the linear factoring ingredient, our optimisation steps consist of
well-known ideas from the functional programming community. Furthermore, we
observe a connection with classical imperative taping-based reverse AD, as well
as Kmett's 'ad' Haskell library, recently analysed by Krawiec et al. We
demonstrate the practical use of our technique by providing a performant
implementation that differentiates most of Haskell98.",-0.5019446,-0.18799156,-0.3424135,A
7052,"In addition, further research is needed to expand the

reverse function in the longest expression code segment, If the scope of reversible inference.","Mark the return to active record of the
current active record as the current active record, use the
                                                    HAN Ji-Peng, LICHEN Zhi-Hang：

A Constraint and Object Oriented Fifth Generation Programming Language and its Compiler and Runtime System  21

will be called and the return address of the active record created destroys the simplicity, however, machine learning can be

for the function call will be set to the address of the code on the attempted to determine the multi-dimensional weight of a

previous line of the function call code segment calling this function.","start position of function call code segment calling this reverse  In terms of execution performance, during COOL pre-

function coincides with the start position of the code segment of execution, traversal methods are currently used to determine

the longest expression, set the return address to the address of functions with the same structure as expressions, which incur

the next line of code after the end position of the longest considerable performance overhead.",2022-06-02 12:55:03+00:00,A Constraint and Object Oriented Fifth Generation Programming Language and its Compiler and Runtime System,cs.PL,['cs.PL'],"[arxiv.Result.Author('Han Jipeng'), arxiv.Result.Author('Lichen Zhihang')]","Since the advent of LISP, the fifth generation programming language has
developed for decades. However, compared with the fourth generation programming
language, the fifth generation programming language has not been widely used
because of its obscure semantics, rigorous representation of problems, and
limited inference ability. For this reason, COOL (Constraint and Object Ordered
Language), a fifth generation programming language proposed in this paper,
overcomes the problems of intuitive semantics, rigorous restrictions on
handling problem conditions, and improves the inference ability of previous
fifth generation programming languages. Specific improvements are as follows:
First, COOL supports process-oriented and object-oriented for easy application
in production projects; Second, COOL supports expression as function
declaration and function return, which improves language affinity for
mathematical formulas, and supports embedding function parameters into function
name strings to make function naming closer to natural languages. Make
mathematical problems easier to describe; Third, COOL introduces a weighting
mechanism and accelerates the inference process through cumulative weighting.
Fourth, COOL introduces the concepts of forward and reverse functions in
programming so that computers can infer and execute problems with logical
sequential constraints. Fifth, the computer can deduce the reverse solution
process by using the forward solution process through the back-tracking
algorithm and the dynamic programming algorithm, so that the computer can
deduce the problem with time-sequential constraints. Sixth, the pre-execution
step is introduced to separate the inference and function query process of the
program from the execution process, so as to improve the execution speed of the
program.",-0.37341037,-0.116282254,-0.13225386,A
8171,"Furthermore, compilation forking itself provides
opportunities for further research: We plan to investigate the interplay of optimizations
by employing nested forking.","In future work we will shift the focus on improving our machine
learning models further, by employing techniques such as graph neural networks
to better capture graph-based IRs.","Besides that, deoptimization enables us to connect data
generation by compilation forking with using learned or updated ML models in single
runs.",2022-06-28 15:37:33+00:00,Compilation Forking: A Fast and Flexible Way of Generating Data for Compiler-Internal Machine Learning Tasks,cs.PL,['cs.PL'],"[arxiv.Result.Author('Raphael Mosaner'), arxiv.Result.Author('David Leopoldseder'), arxiv.Result.Author('Wolfgang Kisling'), arxiv.Result.Author('Lukas Stadler'), arxiv.Result.Author('Hanspeter Mössenböck')]","Compiler optimization decisions are often based on hand-crafted heuristics
centered around a few established benchmark suites. Alternatively, they can be
learned from feature and performance data produced during compilation. However,
data-driven compiler optimizations based on machine learning models require
large sets of quality data for training in order to match or even outperform
existing human-crafted heuristics. In static compilation setups, related work
has addressed this problem with iterative compilation. However, a dynamic
compiler may produce different data depending on dynamically-chosen compilation
strategies, which aggravates the generation of comparable data. We propose
compilation forking, a technique for generating consistent feature and
performance data from arbitrary, dynamically-compiled programs. Different
versions of program parts with the same profiling and compilation history are
executed within single program runs to minimize noise stemming from dynamic
compilation and the runtime environment. Our approach facilitates large-scale
performance evaluations of compiler optimization decisions. Additionally,
compilation forking supports creating domain-specific compilation strategies
based on machine learning by providing the data for model training. We
implemented compilation forking in the GraalVM compiler in a
programming-language-agnostic way. To assess the quality of the generated data,
we trained several machine learning models to replace compiler heuristics for
loop-related optimizations. The trained models perform equally well to the
highly-tuned compiler heuristics when comparing the geometric means of
benchmark suite performances. Larger impacts on few single benchmarks range
from speedups of 20% to slowdowns of 17%. The presented approach can be
implemented in any dynamic compiler. We believe that it can help to analyze
compilation decisions and leverage the use of machine learning into dynamic
compilation.",-0.44633955,-0.19588391,0.16532287,A_centroid
8788,"Another promising line of work that requires further research is developing a theory of probability
inside Diff.","Furthermore, we conjecture that by allowing non-terminating behavior it might be easier

to get a better categorical understanding of what distributions are, as test functions might simply

be smooth functions, instead of compactly supported ones.",Many modern Bayesian inference engines rely on differentiable programming.,2022-07-13 03:53:27+00:00,Distribution Theoretic Semantics for Non-Smooth Differentiable Programming,cs.PL,['cs.PL'],"[arxiv.Result.Author('Pedro H. Azevedo de Amorim'), arxiv.Result.Author('Christopher Lam')]","With the wide spread of deep learning and gradient descent inspired
optimization algorithms, differentiable programming has gained traction.
Nowadays it has found applications in many different areas as well, such as
scientific computing, robotics, computer graphics and others. One of its
notoriously difficult problems consists in interpreting programs that are not
differentiable everywhere.
  In this work we define $\lambda_\delta$, a core calculus for non-smooth
differentiable programs and define its semantics using concepts from
distribution theory, a well-established area of functional analysis. We also
show how $\lambda_\delta$ presents better equational properties than other
existing semantics and use our semantics to reason about a simplified ray
tracing algorithm. Further, we relate our semantics to existing differentiable
languages by providing translations to and from other existing differentiable
semantic models. Finally, we provide a proof-of-concept implementation in
PyTorch of the novel constructions in this paper.",-0.04552243,0.020963764,0.47196382,A
10778,"Mikhajlov and Sekerinski [4] provide a more general
description of reﬁnement, which is worth exploring and adapting to Elegant
Objects in further research.","We believe that this limited notion should work well in practice for
fragile base class analysis.","Mikhajlov and Sekerinski’s work [4] also considers the original version of a
base class and its reﬁnement as inputs for analysis.",2022-09-05 07:32:15+00:00,Detecting unanticipated mutual recursion using Elegant Objects representation of object-oriented programs,cs.PL,['cs.PL'],"[arxiv.Result.Author('Nikolai Kudasov'), arxiv.Result.Author('Mikhail Olokin'), arxiv.Result.Author('Oleksii Potyomkin'), arxiv.Result.Author('Nikolay Shilov'), arxiv.Result.Author('Maxim Stepanov')]","Elegant Objects (EO) is a variation of the object-oriented programming
paradigm that favors pure objects and decoration. EO programming language is
based on these ideas and has been suggested by Bugayenko as an intermediate
representation for object-oriented programs. This paper provides plausible
representations in EO of some class-based constructions from Java, C++, and
Python. We then reformulate the classical fragile base class problem in the
context of these representations. Finally, we discuss an algorithm for
detecting a subset of fragile base class patterns in EO programs. We show that
using EO as an intermediate language is plausible and discuss possible
improvements to the language to assist in richer static analysis.",-0.15275978,0.008840343,0.6349536,B
