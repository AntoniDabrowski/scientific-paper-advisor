,Unnamed: 0,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract
0,0.0,"It may be that
providing feedback on a ﬁlter-wise basis is too simplistic,
or that some other aspect related to convolution is not con-
ducive to the feedback mechanism.","Further research on the
combination of feedback and convolutional networks may
lead to some conﬁguration that allows for more clear bene-
ﬁts of feedback.","Feedback-Gated Rectiﬁed Linear Units

Zhou, YT and Chellappa, R. Computation of optical ﬂow
In IEEE International Con-
using a neural network.",2023-01-06 17:14:11+00:00,Feedback-Gated Rectified Linear Units,cs.NE,"['cs.NE', 'cs.AI']",[arxiv.Result.Author('Marco Kemmerling')],"Feedback connections play a prominent role in the human brain but have not
received much attention in artificial neural network research. Here, a
biologically inspired feedback mechanism which gates rectified linear units is
proposed. On the MNIST dataset, autoencoders with feedback show faster
convergence, better performance, and more robustness to noise compared to their
counterparts without feedback. Some benefits, although less pronounced and less
consistent, can be observed when networks with feedback are applied on the
CIFAR-10 dataset."
1,1.0,"However, considering that top-1
ImageNet accuracy for a ResNet-18 when trained on the entire dataset is 69.76%3, even for the best
methods the accuracy gap in the continual learning setup is very large.","This suggests that continual
learning, especially in complex scenarios with a large number of classes and high dimensional data,
is far to be solved, and further research should be devoted to this ﬁeld.",3Accuracy taken from the torchvision ofﬁcial page: https://pytorch.org/vision/stable/models.,2023-01-06 11:22:59+00:00,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Vincenzo Lomonaco'), arxiv.Result.Author('Lorenzo Pellegrini'), arxiv.Result.Author('Gabriele Graffieti'), arxiv.Result.Author('Davide Maltoni')]","In recent years we have witnessed a renewed interest in machine learning
methodologies, especially for deep representation learning, that could overcome
basic i.i.d. assumptions and tackle non-stationary environments subject to
various distributional shifts or sample selection biases. Within this context,
several computational approaches based on architectural priors, regularizers
and replay policies have been proposed with different degrees of success
depending on the specific scenario in which they were developed and assessed.
However, designing comprehensive hybrid solutions that can flexibly and
generally be applied with tunable efficiency-effectiveness trade-offs still
seems a distant goal. In this paper, we propose ""Architect, Regularize and
Replay"" (ARR), an hybrid generalization of the renowned AR1 algorithm and its
variants, that can achieve state-of-the-art results in classic scenarios (e.g.
class-incremental learning) but also generalize to arbitrary data streams
generated from real-world datasets such as CIFAR-100, CORe50 and ImageNet-1000."
2,2.0,"Thus, if one is interested in more exotic
equilibrium reﬁnements, for example, EFGs are likely to be a more suitable model.","It is our
hope that further research on MAIDs and causal games will reduce this last diﬀerence.","8.2 Future Work

Our priority is to use causal games to further analyse incentives in multi-agent systems, which
has important applications in ensuring that we build AI systems that are safe and fair.",2023-01-05 22:47:28+00:00,Reasoning about Causality in Games,cs.AI,"['cs.AI', 'cs.GT', 'cs.MA']","[arxiv.Result.Author('Lewis Hammond'), arxiv.Result.Author('James Fox'), arxiv.Result.Author('Tom Everitt'), arxiv.Result.Author('Ryan Carey'), arxiv.Result.Author('Alessandro Abate'), arxiv.Result.Author('Michael Wooldridge')]","Causal reasoning and game-theoretic reasoning are fundamental topics in
artificial intelligence, among many other disciplines: this paper is concerned
with their intersection. Despite their importance, a formal framework that
supports both these forms of reasoning has, until now, been lacking. We offer a
solution in the form of (structural) causal games, which can be seen as
extending Pearl's causal hierarchy to the game-theoretic domain, or as
extending Koller and Milch's multi-agent influence diagrams to the causal
domain. We then consider three key questions: i) How can the (causal)
dependencies in games - either between variables, or between strategies - be
modelled in a uniform, principled manner? ii) How may causal queries be
computed in causal games, and what assumptions does this require? iii) How do
causal games compare to existing formalisms? To address question i), we
introduce mechanised games, which encode dependencies between agents' decision
rules and the distributions governing the game. In response to question ii), we
present definitions of predictions, interventions, and counterfactuals, and
discuss the assumptions required for each. Regarding question iii), we describe
correspondences between causal games and other formalisms, and explain how
causal games can be used to answer queries that other causal or game-theoretic
models do not support. Finally, we highlight possible applications of causal
games, aided by an extensive open-source Python library."
3,3.0,"We might also hope to ex-
tend the framework presented here with: model variations that can more easily capture dynamic
settings, ﬁne-grained subjective beliefs, or optimisation; deﬁnitions capturing other classic equi-
librium reﬁnements such as perfect Bayesian equilibrium [28] or sequential equilibrium [50]; and
methods of causal discovery for games.","Given that we propose this paper and our accompanying codebase as a robust foundation for
reasoning about causality in games, we believe our work presents many other interesting avenues
for further research.","We hope that the advantages causal games confer based on their generality,
explainability, and succinctness (not to mention their compatibility with existing mainstream

24Though it is possible to support said independencies in DAGs via tree-based representations of the CPDs,

which can graphically capture diﬀerent independencies on diﬀerent branches.",2023-01-05 22:47:28+00:00,Reasoning about Causality in Games,cs.AI,"['cs.AI', 'cs.GT', 'cs.MA']","[arxiv.Result.Author('Lewis Hammond'), arxiv.Result.Author('James Fox'), arxiv.Result.Author('Tom Everitt'), arxiv.Result.Author('Ryan Carey'), arxiv.Result.Author('Alessandro Abate'), arxiv.Result.Author('Michael Wooldridge')]","Causal reasoning and game-theoretic reasoning are fundamental topics in
artificial intelligence, among many other disciplines: this paper is concerned
with their intersection. Despite their importance, a formal framework that
supports both these forms of reasoning has, until now, been lacking. We offer a
solution in the form of (structural) causal games, which can be seen as
extending Pearl's causal hierarchy to the game-theoretic domain, or as
extending Koller and Milch's multi-agent influence diagrams to the causal
domain. We then consider three key questions: i) How can the (causal)
dependencies in games - either between variables, or between strategies - be
modelled in a uniform, principled manner? ii) How may causal queries be
computed in causal games, and what assumptions does this require? iii) How do
causal games compare to existing formalisms? To address question i), we
introduce mechanised games, which encode dependencies between agents' decision
rules and the distributions governing the game. In response to question ii), we
present definitions of predictions, interventions, and counterfactuals, and
discuss the assumptions required for each. Regarding question iii), we describe
correspondences between causal games and other formalisms, and explain how
causal games can be used to answer queries that other causal or game-theoretic
models do not support. Finally, we highlight possible applications of causal
games, aided by an extensive open-source Python library."
4,4.0,"The average glance duration is the “mean duration of
all glance durations to an area of interest (or set of related areas of interest) during a condition task, subtask or sub-
subtask)”.","Further research (Horrey and Wickens, 2007) shows that single longer-than-normal glances, especially those
longer than two seconds (Klauer et al., 2006), highly correlate with reduced driving safety.","This is also mentioned in the
“Visual-Manual NHTSA Driver Distraction Guidelines for In-Vehicle Electronic Devices” (NHTSA, 2012).",2023-01-05 13:50:26+00:00,On the Forces of Driver Distraction: Explainable Predictions for the Visual Demand of In-Vehicle Touchscreen Interactions,cs.HC,"['cs.HC', 'cs.AI']","[arxiv.Result.Author('Patrick Ebel'), arxiv.Result.Author('Christoph Lingenfelder'), arxiv.Result.Author('Andreas Vogelsang')]","With modern infotainment systems, drivers are increasingly tempted to engage
in secondary tasks while driving. Since distracted driving is already one of
the main causes of fatal accidents, in-vehicle touchscreen Human-Machine
Interfaces (HMIs) must be as little distracting as possible. To ensure that
these systems are safe to use, they undergo elaborate and expensive empirical
testing, requiring fully functional prototypes. Thus, early-stage methods
informing designers about the implication their design may have on driver
distraction are of great value. This paper presents a machine learning method
that, based on anticipated usage scenarios, predicts the visual demand of
in-vehicle touchscreen interactions and provides local and global explanations
of the factors influencing drivers' visual attention allocation. The approach
is based on large-scale natural driving data continuously collected from
production line vehicles and employs the SHapley Additive exPlanation (SHAP)
method to provide explanations leveraging informed design decisions. Our
approach is more accurate than related work and identifies interactions during
which long glances occur with 68 % accuracy and predicts the total glance
duration with a mean error of 2.4 s. Our explanations replicate the results of
various recent studies and provide fast and easily accessible insights into the
effect of UI elements, driving automation, and vehicle speed on driver
distraction. The system can not only help designers to evaluate current designs
but also help them to better anticipate and understand the implications their
design decisions might have on future designs."
5,5.0,"In addition, statistics data shows that SMILES-mGPT2 model 

gets a slightly lower scores compared with Transformer Reg and MolGPT, which indicates that 

the t-SMILES mGPT2 models could be optimized to get high scores as well.","Such an issue could 

be severed as a starting point for further research.",Figure  5.,2023-01-04 21:41:01+00:00,Fragment-based t-SMILES for de novo molecular generation,cs.LG,"['cs.LG', 'cs.AI', 'q-bio.BM']","[arxiv.Result.Author('Juan-Ni Wu'), arxiv.Result.Author('Tong Wang'), arxiv.Result.Author('Yue Chen'), arxiv.Result.Author('Li-Juan Tang'), arxiv.Result.Author('Hai-Long Wu'), arxiv.Result.Author('Ru-Qin Yu')]","At present, sequence-based and graph-based models are two of popular used
molecular generative models. In this study, we introduce a general-purposed,
fragment-based, hierarchical molecular representation named t-SMILES
(tree-based SMILES) which describes molecules using a SMILES-type string
obtained by doing breadth first search (BFS) on full binary molecular tree
formed from fragmented molecular graph. The proposed t-SMILES combines the
advantages of graph model paying more attention to molecular topology structure
and language model possessing powerful learning ability. Experiments with
feature tree rooted JTVAE and chemical reaction-based BRICS molecular
decomposing algorithms using sequence-based autoregressive generation models on
three popular molecule datasets including Zinc, QM9 and ChEMBL datasets
indicate that t-SMILES based models significantly outperform previously
proposed fragment-based models and being competitive with classical SMILES
based and graph-based approaches. Most importantly, we proposed a new
perspective for fragment based molecular designing. Hence, SOTA powerful
sequence-based solutions could be easily applied for fragment based molecular
tasks."
6,6.0,"To be improved Our experiments show that how to segment, assemble molecular fragments and 

how  to  optimize  molecular  are  also  key  steps  controlling  the  quality  of  generated  molecules.","Therefore, this challenging topic could serve as a starting point for further research.","26 

 
With the rapid development of NLP technology, Transform-based models have been proved to 

enable  text  generation  with  human-like  capabilities  if  trained  on  enough  data.",2023-01-04 21:41:01+00:00,Fragment-based t-SMILES for de novo molecular generation,cs.LG,"['cs.LG', 'cs.AI', 'q-bio.BM']","[arxiv.Result.Author('Juan-Ni Wu'), arxiv.Result.Author('Tong Wang'), arxiv.Result.Author('Yue Chen'), arxiv.Result.Author('Li-Juan Tang'), arxiv.Result.Author('Hai-Long Wu'), arxiv.Result.Author('Ru-Qin Yu')]","At present, sequence-based and graph-based models are two of popular used
molecular generative models. In this study, we introduce a general-purposed,
fragment-based, hierarchical molecular representation named t-SMILES
(tree-based SMILES) which describes molecules using a SMILES-type string
obtained by doing breadth first search (BFS) on full binary molecular tree
formed from fragmented molecular graph. The proposed t-SMILES combines the
advantages of graph model paying more attention to molecular topology structure
and language model possessing powerful learning ability. Experiments with
feature tree rooted JTVAE and chemical reaction-based BRICS molecular
decomposing algorithms using sequence-based autoregressive generation models on
three popular molecule datasets including Zinc, QM9 and ChEMBL datasets
indicate that t-SMILES based models significantly outperform previously
proposed fragment-based models and being competitive with classical SMILES
based and graph-based approaches. Most importantly, we proposed a new
perspective for fragment based molecular designing. Hence, SOTA powerful
sequence-based solutions could be easily applied for fragment based molecular
tasks."
7,7.0,"We can
achieve strong results that mitigate forgetting, namely by modeling the generative CL process and using
sequential Bayesian inference over a few parameters in the class prototype embedding space.","We argue
that modeling the generative CL process is a fruitful direction for further research rather than attempting
sequential Bayesian inference over the weights of a BNN.","8 Discussion & Conclusion

In this paper we have revisited the use of sequential Bayesian inference for CL.",2023-01-04 21:33:13+00:00,On Sequential Bayesian Inference for Continual Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Samuel Kessler'), arxiv.Result.Author('Adam Cobb'), arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Stefan Zohren'), arxiv.Result.Author('Stephen J. Roberts')]","Sequential Bayesian inference can be used for continual learning to prevent
catastrophic forgetting of past tasks and provide an informative prior when
learning new tasks. We revisit sequential Bayesian inference and test whether
having access to the true posterior is guaranteed to prevent catastrophic
forgetting in Bayesian neural networks. To do this we perform sequential
Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as
a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo
samples. We find that this approach fails to prevent catastrophic forgetting
demonstrating the difficulty in performing sequential Bayesian inference in
neural networks. From there we study simple analytical examples of sequential
Bayesian inference and CL and highlight the issue of model misspecification
which can lead to sub-optimal continual learning performance despite exact
inference. Furthermore, we discuss how task data imbalances can cause
forgetting. From these limitations, we argue that we need probabilistic models
of the continual learning generative process rather than relying on sequential
Bayesian inference over Bayesian neural network weights. In this vein, we also
propose a simple baseline called Prototypical Bayesian Continual Learning,
which is competitive with state-of-the-art Bayesian continual learning methods
on class incremental continual learning vision benchmarks."
8,8.0,"References

To the best of our knowledge, this work is the ﬁrst to frame
the problem of federated learning for data streams.","It high-
lights new challenges and—we believe—lays the founda-
tions for further research.","For example, part of our results
are restricted to the important, but still quite speciﬁc, sce-
nario where some clients have static datasets and others
process new samples at each step.",2023-01-04 11:10:48+00:00,Federated Learning for Data Streams,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Othmane Marfoq'), arxiv.Result.Author('Giovanni Neglia'), arxiv.Result.Author('Laetitia Kameni'), arxiv.Result.Author('Richard Vidal')]","Federated learning (FL) is an effective solution to train machine learning
models on the increasing amount of data generated by IoT devices and
smartphones while keeping such data localized. Most previous work on federated
learning assumes that clients operate on static datasets collected before
training starts. This approach may be inefficient because 1) it ignores new
samples clients collect during training, and 2) it may require a potentially
long preparatory phase for clients to collect enough data. Moreover, learning
on static datasets may be simply impossible in scenarios with small aggregate
storage across devices. It is, therefore, necessary to design federated
algorithms able to learn from data streams. In this work, we formulate and
study the problem of \emph{federated learning for data streams}. We propose a
general FL algorithm to learn from data streams through an opportune weighted
empirical risk minimization. Our theoretical analysis provides insights to
configure such an algorithm, and we evaluate its performance on a wide range of
machine learning tasks."
9,9.0,"Many HCI/UX designers lack AI knowledge, so it is still challenging to use 

these tools for prototyping.","Further research is needed on prototyping tools for HCI/UX designers to help quickly 

build prototypes, discover design problems early, and reduce product development risk.","As we look forward, with the emergence of new AI-based design paradigms, HCI/UX design activities 

require the support of new tools.",2023-01-03 07:41:29+00:00,AI in HCI Design and User Experience,cs.HC,"['cs.HC', 'cs.AI']",[arxiv.Result.Author('Wei Xu')],"In this chapter, we review and discuss the transformation of AI technology in
HCI/UX work and assess how AI technology will change how we do the work. We
first discuss how AI can be used to enhance the result of user research and
design evaluation. We then discuss how AI technology can be used to enhance
HCI/UX design. Finally, we discuss how AI-enabled capabilities can improve UX
when users interact with computing systems, applications, and services."
10,10.0,"In Section IV, we introduce

the interplay between emerging applications and UAVs, e.g.,
RIS, semantic communication, VR, etc., and introduce the

application of DL to these new techniques.","Finally, we will
conclude this paper and discuss the challenging open issue
that should be solved and the further research about DL-UAV
swarms in Section V.

II.","DISTRIBUTED MACHINE LEARNING ALGORITHMS

In this section, we present four state-of-the-art DL meth-
ods including federated learning, multi-agent reinforcement
learning, distributed inference, and split learning.",2023-01-03 01:05:18+00:00,"Distributed Machine Learning for UAV Swarms: Computing, Sensing, and Semantics",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yahao Ding'), arxiv.Result.Author('Zhaohui Yang'), arxiv.Result.Author('Quoc-Viet Pham'), arxiv.Result.Author('Zhaoyang Zhang'), arxiv.Result.Author('Mohammad Shikh-Bahaei')]","Unmanned aerial vehicle (UAV) swarms are considered as a promising technique
for next-generation communication networks due to their flexibility, mobility,
low cost, and the ability to collaboratively and autonomously provide services.
Distributed learning (DL) enables UAV swarms to intelligently provide
communication services, multi-directional remote surveillance, and target
tracking. In this survey, we first introduce several popular DL algorithms such
as federated learning (FL), multi-agent Reinforcement Learning (MARL),
distributed inference, and split learning, and present a comprehensive overview
of their applications for UAV swarms, such as trajectory design, power control,
wireless resource allocation, user assignment, perception, and satellite
communications. Then, we present several state-of-the-art applications of UAV
swarms in wireless communication systems, such us reconfigurable intelligent
surface (RIS), virtual reality (VR), semantic communications, and discuss the
problems and challenges that DL-enabled UAV swarms can solve in these
applications. Finally, we describe open problems of using DL in UAV swarms and
future research directions of DL enabled UAV swarms. In summary, this survey
provides a comprehensive survey of various DL applications for UAV swarms in
extensive scenarios."
11,11.0,"The studies conducted in this work can be
adapted to other languages, other biases and other
corpora.","We hope further research can assess the
frequency-based distortion in these settings as well
as the inﬂuence of hyperparameter choices.","References

Osman Aka, Ken Burke, Alex Bauerle, Christina Greer,
and Margaret Mitchell.",2023-01-02 18:27:10+00:00,The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Francisco Valentini'), arxiv.Result.Author('Germán Rosati'), arxiv.Result.Author('Diego Fernandez Slezak'), arxiv.Result.Author('Edgar Altszyler')]","Numerous works use word embedding-based metrics to quantify societal biases
and stereotypes in texts. Recent studies have found that word embeddings can
capture semantic similarity but may be affected by word frequency. In this work
we study the effect of frequency when measuring female vs. male gender bias
with word embedding-based bias quantification methods. We find that Skip-gram
with negative sampling and GloVe tend to detect male bias in high frequency
words, while GloVe tends to return female bias in low frequency words. We show
these behaviors still exist when words are randomly shuffled. This proves that
the frequency-based effect observed in unshuffled corpora stems from properties
of the metric rather than from word associations. The effect is spurious and
problematic since bias metrics should depend exclusively on word co-occurrences
and not individual word frequencies. Finally, we compare these results with the
ones obtained with an alternative metric based on Pointwise Mutual Information.
We find that this metric does not show a clear dependence on frequency, even
though it is slightly skewed towards male bias across all frequencies."
12,12.0,"Speciﬁcally, we illustrate designs
and ideas, such as incorporating digital
twins for holistic
network virtualization, connected AI for network management,
AI slices with training and inference separation, and hybrid
data-model driven methods, throughout this paper.","Last, after
introducing our vision of holistic network virtualization and
pervasive network intelligence, we present open issues and
challenges to inspire further research.","There are a few surveys on virtualization and AI in the liter-
ature [21], [32]–[34].",2023-01-02 04:15:33+00:00,Holistic Network Virtualization and Pervasive Network Intelligence for 6G,cs.NI,"['cs.NI', 'cs.AI']","[arxiv.Result.Author('Xuemin'), arxiv.Result.Author('Shen'), arxiv.Result.Author('Jie Gao'), arxiv.Result.Author('Wen Wu'), arxiv.Result.Author('Mushu Li'), arxiv.Result.Author('Conghao Zhou'), arxiv.Result.Author('Weihua Zhuang')]","In this tutorial paper, we look into the evolution and prospect of network
architecture and propose a novel conceptual architecture for the 6th generation
(6G) networks. The proposed architecture has two key elements, i.e., holistic
network virtualization and pervasive artificial intelligence (AI). The holistic
network virtualization consists of network slicing and digital twin, from the
aspects of service provision and service demand, respectively, to incorporate
service-centric and user-centric networking. The pervasive network intelligence
integrates AI into future networks from the perspectives of networking for AI
and AI for networking, respectively. Building on holistic network
virtualization and pervasive network intelligence, the proposed architecture
can facilitate three types of interplay, i.e., the interplay between digital
twin and network slicing paradigms, between model-driven and data-driven
methods for network management, and between virtualization and AI, to maximize
the flexibility, scalability, adaptivity, and intelligence for 6G networks. We
also identify challenges and open issues related to the proposed architecture.
By providing our vision, we aim to inspire further discussions and developments
on the potential architecture of 6G."
13,13.0,"The word cloud
distinguish the word frequency in these documents in font size and the bigram represents the
co-appearance of two words in sequential order ranking by counts in the tables and in the network
figures.","We also further research the emerging documents of blockchain standard development
and papers discussing blockchain standards by working groups and institutions globally.","6

Sunshine Zhang

Data and Code Availability: We open source the data and code for replication and future

research on the Github: https://github.com/sunshineluyao/design-principle-blockchain.",2023-01-01 21:57:25+00:00,The Design Principle of Blockchain: An Initiative for the SoK of SoKs,cs.CR,"['cs.CR', 'cs.AI', 'cs.CL', 'stat.ML']",[arxiv.Result.Author('Sunshine Zhang')],"Blockchain, also coined as decentralized AI, has the potential to empower AI
to be more trustworthy by creating a decentralized trust of privacy, security,
and audibility. However, systematic studies on the design principle of
Blockchain as a trust engine for an integrated society of
Cyber-Physical-Socia-System (CPSS) are still absent. In this article, we
provide an initiative for seeking the design principle of Blockchain for a
better digital world. Using a hybrid method of qualitative and quantitative
studies, we examine the past origin, the current development, and the future
directions of Blockchain design principles. We have three findings. First, the
answers to whether Blockchain lives up to its original design principle as a
distributed database are controversial. Second, the current development of
Blockchain community reveals a taxonomy of 7 categories, including privacy and
security, scalability, decentralization, applicability, governance and
regulation, system design, and cross-chain interoperability. Both research and
practice are more centered around the first category of privacy and security
and the fourth category of applicability. Future scholars, practitioners, and
policy-makers have vast opportunities in other, much less exploited facets and
the synthesis at the interface of multiple aspects. Finally, in
counter-examples, we conclude that a synthetic solution that crosses discipline
boundaries is necessary to close the gaps between the current design of
Blockchain and the design principle of a trust engine for a truly intelligent
world."
14,14.0,"In task MatchBoard, the trainee needs to pick up a
digit or alphabet block and place it on groove in the cor-
responding position, which further practices hand-eye coor-
dination.","These tasks are speciﬁcally designed and modeled
which can be easily extended and applied for further research
on surgical training and human-involved robot learning.","F. Surgical Skills Learning from Human Demonstrations

Traditional methods of learning from demonstration focus
on exploiting data and knowledge from machine-generated
script-based demonstrations, while they could suffer from
heavy exploration burden, low success rate, even failures
for complex surgical tasks [22].",2023-01-01 18:05:25+00:00,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,cs.RO,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yonghao Long'), arxiv.Result.Author('Wang Wei'), arxiv.Result.Author('Tao Huang'), arxiv.Result.Author('Yuehao Wang'), arxiv.Result.Author('Qi Dou')]","Surgical robot automation has attracted increasing research interest over the
past decade, expecting its huge potential to benefit surgeons, nurses and
patients. Recently, the learning paradigm of embodied AI has demonstrated
promising ability to learn good control policies for various complex tasks,
where embodied AI simulators play an essential role to facilitate relevant
researchers. However, existing open-sourced simulators for surgical robot are
still not sufficiently supporting human interactions through physical input
devices, which further limits effective investigations on how human
demonstrations would affect policy learning. In this paper, we study
human-in-the-loop embodied intelligence with a new interactive simulation
platform for surgical robot learning. Specifically, we establish our platform
based on our previously released SurRoL simulator with several new features
co-developed to allow high-quality human interaction via an input device. With
these, we further propose to collect human demonstrations and imitate the
action patterns to achieve more effective policy learning. We showcase the
improvement of our simulation environment with the designed new features and
tasks, and validate state-of-the-art reinforcement learning algorithms using
the interactive environment. Promising results are obtained, with which we hope
to pave the way for future research on surgical embodied intelligence. Our
platform is released and will be continuously updated in the website:
https://med-air.github.io/SurRoL/"
15,15.0,"(3) Most studies construct target geometric structures in
Euclidean space, which may not be suitable for problems
with non-Euclidean data such as graphs.","Thus, how to
improve training efﬁciency and deal with the large-size
dataset, as well as mining geometry information of non-
Euclidean data deserve further research.","From the application perspective, computation-efﬁcient
approaches are more applicable for pixel-wise semantic
segmentation tasks, which require higher resources com-
pared with classiﬁcation tasks.",2022-12-31 18:44:45+00:00,Source-Free Unsupervised Domain Adaptation: A Survey,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yuqi Fang'), arxiv.Result.Author('Pew-Thian Yap'), arxiv.Result.Author('Weili Lin'), arxiv.Result.Author('Hongtu Zhu'), arxiv.Result.Author('Mingxia Liu')]","Unsupervised domain adaptation (UDA) via deep learning has attracted
appealing attention for tackling domain-shift problems caused by distribution
discrepancy across different domains. Existing UDA approaches highly depend on
the accessibility of source domain data, which is usually limited in practical
scenarios due to privacy protection, data storage and transmission cost, and
computation burden. To tackle this issue, many source-free unsupervised domain
adaptation (SFUDA) methods have been proposed recently, which perform knowledge
transfer from a pre-trained source model to unlabeled target domain with source
data inaccessible. A comprehensive review of these works on SFUDA is of great
significance. In this paper, we provide a timely and systematic literature
review of existing SFUDA approaches from a technical perspective. Specifically,
we categorize current SFUDA studies into two groups, i.e., white-box SFUDA and
black-box SFUDA, and further divide them into finer subcategories based on
different learning strategies they use. We also investigate the challenges of
methods in each subcategory, discuss the advantages/disadvantages of white-box
and black-box SFUDA methods, conclude the commonly used benchmark datasets, and
summarize the popular techniques for improved generalizability of models
learned without using source data. We finally discuss several promising future
directions in this field."
16,16.0,"Then, we organize
and discuss advanced techniques of ICL, in-
cluding training strategies, prompting strate-
gies, and so on.","Finally, we present the chal-
lenges of ICL and provide potential directions
for further research.","We hope our work can en-
courage more research on uncovering how ICL
works and improving ICL in future work.1

1

Introduction

With the scaling of model size and corpus size (De-
vlin et al., 2019; Radford et al., 2019; Brown et al.,
2020; Chowdhery et al., 2022), large language mod-
els demonstrate new abilities that learn from the
demonstration consisting of a few examples in the
context (in-context learning for short).",2022-12-31 15:57:09+00:00,A Survey for In-context Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Damai Dai'), arxiv.Result.Author('Ce Zheng'), arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Xu Sun'), arxiv.Result.Author('Jingjing Xu'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Zhifang Sui')]","With the increasing ability of large language models (LLMs), in-context
learning (ICL) has become a new paradigm for natural language processing (NLP),
where LLMs make predictions only based on contexts augmented with a few
training examples. It has been a new trend exploring ICL to evaluate and
extrapolate the ability of LLMs. In this paper, we aim to survey and summarize
the progress, challenges, and future work in ICL. We first present a formal
definition of ICL and clarify its correlation to related studies. Then, we
organize and discuss advanced techniques of ICL, including training strategies,
prompting strategies, and so on. Finally, we present the challenges of ICL and
provide potential directions for further research. We hope our work can
encourage more research on uncovering how ICL works and improving ICL in future
work."
17,17.0,"We note
that proving a lower bound on the number of linear regions, as done by Hanin and Rolnick [2019a],
for the manifold setting remains open.","Our work opens up avenues for further research that combines
model geometry and data geometry and can lead to empirical research geared towards developing
DNN architectures for high dimensional datasets that lie on a low dimensional manifold.","6 Acknowledgements

This work was funded by L2M (DARPA Lifelong Learning Machines program under grant number
FA8750-18-2-0117), the Penn MURI (ONR under the PERISCOPE MURI Contract N00014- 17-
1-2699), and the ONR Swarm (the ONR under grant number N00014-21-1-2200).",2022-12-29 17:32:05+00:00,Effects of Data Geometry in Early Deep Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Saket Tiwari'), arxiv.Result.Author('George Konidaris')]","Deep neural networks can approximate functions on different types of data,
from images to graphs, with varied underlying structure. This underlying
structure can be viewed as the geometry of the data manifold. By extending
recent advances in the theoretical understanding of neural networks, we study
how a randomly initialized neural network with piece-wise linear activation
splits the data manifold into regions where the neural network behaves as a
linear function. We derive bounds on the density of boundary of linear regions
and the distance to these boundaries on the data manifold. This leads to
insights into the expressivity of randomly initialized deep neural networks on
non-Euclidean data sets. We empirically corroborate our theoretical results
using a toy supervised learning problem. Our experiments demonstrate that
number of linear regions varies across manifolds and the results hold with
changing neural network architectures. We further demonstrate how the
complexity of linear regions is different on the low dimensional manifold of
images as compared to the Euclidean space, using the MetFaces dataset."
18,18.0,"To
remedy the pathology, we proposed the use of non-parametric behavioral reference policies, which
we showed can signiﬁcantly accelerate and improve online learning and yield online policies that
(often signiﬁcantly) outperform current state-of-the-art methods on challenging continuous control
tasks.","We hope that this work will encourage further research into better model classes for deep
reinforcement learning algorithms, including and especially for reinforcement from image inputs.","10

0K100K200K300K400K500KTimesteps050001000015ExpertDemonstrations0K100K200K300K400K500KTimesteps0500010000OneExpertDemonstrationN-PPAC(Ours)EnsembleBehavioralPolicyMC-DropoutBehavioralPolicyNoBehavioralPolicy(SAC)Acknowledgments and Disclosure of Funding

We thank Ashvin Nair for sharing his code and results, as well as for providing helpful insights about
the dexterous hand manipulation suite.",2022-12-28 16:29:09+00:00,On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations,cs.LG,"['cs.LG', 'cs.AI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Cong Lu'), arxiv.Result.Author('Michael A. Osborne'), arxiv.Result.Author('Yarin Gal'), arxiv.Result.Author('Yee Whye Teh')]","KL-regularized reinforcement learning from expert demonstrations has proved
successful in improving the sample efficiency of deep reinforcement learning
algorithms, allowing them to be applied to challenging physical real-world
tasks. However, we show that KL-regularized reinforcement learning with
behavioral reference policies derived from expert demonstrations can suffer
from pathological training dynamics that can lead to slow, unstable, and
suboptimal online learning. We show empirically that the pathology occurs for
commonly chosen behavioral policy classes and demonstrate its impact on sample
efficiency and online policy performance. Finally, we show that the pathology
can be remedied by non-parametric behavioral reference policies and that this
allows KL-regularized reinforcement learning to significantly outperform
state-of-the-art approaches on a variety of challenging locomotion and
dexterous hand manipulation tasks."
19,19.0,8.3.,"Further research

Besides the additional steps towards validating the pro-
posed theoretical contribution to compositional architec-
ture frameworks, further research can be conducted in how
to apply category theory to system and software architec-
tures.","Category theory is a very broad ﬁeld of mathemat-
ics, which is why it is diﬃcult to ﬁnd a starting point to

Relates to
section
3.2

Table 7: Additional suggestions for validation, including hypotheses and actions.",2022-12-27 18:05:02+00:00,A Compositional Approach to Creating Architecture Frameworks with an Application to Distributed AI Systems,cs.SE,"['cs.SE', 'cs.AI', 'D.2.1; D.2.11']","[arxiv.Result.Author('Hans-Martin Heyn'), arxiv.Result.Author('Eric Knauss'), arxiv.Result.Author('Patrizio Pelliccione')]","Artificial intelligence (AI) in its various forms finds more and more its way
into complex distributed systems. For instance, it is used locally, as part of
a sensor system, on the edge for low-latency high-performance inference, or in
the cloud, e.g. for data mining. Modern complex systems, such as connected
vehicles, are often part of an Internet of Things (IoT). To manage complexity,
architectures are described with architecture frameworks, which are composed of
a number of architectural views connected through correspondence rules. Despite
some attempts, the definition of a mathematical foundation for architecture
frameworks that are suitable for the development of distributed AI systems
still requires investigation and study. In this paper, we propose to extend the
state of the art on architecture framework by providing a mathematical model
for system architectures, which is scalable and supports co-evolution of
different aspects for example of an AI system. Based on Design Science
Research, this study starts by identifying the challenges with architectural
frameworks. Then, we derive from the identified challenges four rules and we
formulate them by exploiting concepts from category theory. We show how
compositional thinking can provide rules for the creation and management of
architectural frameworks for complex systems, for example distributed systems
with AI. The aim of the paper is not to provide viewpoints or architecture
models specific to AI systems, but instead to provide guidelines based on a
mathematical formulation on how a consistent framework can be built up with
existing, or newly created, viewpoints. To put in practice and test the
approach, the identified and formulated rules are applied to derive an
architectural framework for the EU Horizon 2020 project ``Very efficient deep
learning in the IoT"" (VEDLIoT) in the form of a case study."
20,20.0,"The open call can
serve to further validate the idea of a compositional archi-
tectural framework and to collect best practices on how to
work with and apply the VEDLIoT architectural frame-
work to new use cases.","Further research is proceeding in
developing software tools that support the ideas of a com-
positional architectural framework.","One such tool could

be built upon the idea of using textual architectural de-
scription and a distributed version control system such as
git, see for example Knauss et al.",2022-12-27 18:05:02+00:00,A Compositional Approach to Creating Architecture Frameworks with an Application to Distributed AI Systems,cs.SE,"['cs.SE', 'cs.AI', 'D.2.1; D.2.11']","[arxiv.Result.Author('Hans-Martin Heyn'), arxiv.Result.Author('Eric Knauss'), arxiv.Result.Author('Patrizio Pelliccione')]","Artificial intelligence (AI) in its various forms finds more and more its way
into complex distributed systems. For instance, it is used locally, as part of
a sensor system, on the edge for low-latency high-performance inference, or in
the cloud, e.g. for data mining. Modern complex systems, such as connected
vehicles, are often part of an Internet of Things (IoT). To manage complexity,
architectures are described with architecture frameworks, which are composed of
a number of architectural views connected through correspondence rules. Despite
some attempts, the definition of a mathematical foundation for architecture
frameworks that are suitable for the development of distributed AI systems
still requires investigation and study. In this paper, we propose to extend the
state of the art on architecture framework by providing a mathematical model
for system architectures, which is scalable and supports co-evolution of
different aspects for example of an AI system. Based on Design Science
Research, this study starts by identifying the challenges with architectural
frameworks. Then, we derive from the identified challenges four rules and we
formulate them by exploiting concepts from category theory. We show how
compositional thinking can provide rules for the creation and management of
architectural frameworks for complex systems, for example distributed systems
with AI. The aim of the paper is not to provide viewpoints or architecture
models specific to AI systems, but instead to provide guidelines based on a
mathematical formulation on how a consistent framework can be built up with
existing, or newly created, viewpoints. To put in practice and test the
approach, the identified and formulated rules are applied to derive an
architectural framework for the EU Horizon 2020 project ``Very efficient deep
learning in the IoT"" (VEDLIoT) in the form of a case study."
21,21.0,"2
1
2
2
:
v
i
X
r
a

Abstract

Table-and-text hybrid question answering (Hy-
bridQA) is a widely used and challenging NLP
task commonly applied in the ﬁnancial and scien-
tiﬁc domain.","The early research focuses on mi-
grating other QA task methods to HybridQA, while
with further research, more and more HybridQA-
speciﬁc methods have been present.","With the rapid
development of HybridQA, the systematic survey
is still under-explored to summarize the main tech-
niques and advance further research.",2022-12-27 12:34:57+00:00,"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions",cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dingzirui Wang'), arxiv.Result.Author('Longxu Dou'), arxiv.Result.Author('Wanxiang Che')]","Table-and-text hybrid question answering (HybridQA) is a widely used and
challenging NLP task commonly applied in the financial and scientific domain.
The early research focuses on migrating other QA task methods to HybridQA,
while with further research, more and more HybridQA-specific methods have been
present. With the rapid development of HybridQA, the systematic survey is still
under-explored to summarize the main techniques and advance further research.
So we present this work to summarize the current HybridQA benchmarks and
methods, then analyze the challenges and future directions of this task. The
contributions of this paper can be summarized in three folds: (1) first survey,
to our best knowledge, including benchmarks, methods and challenges for
HybridQA; (2) systematic investigation with the reasonable comparison of the
existing systems to articulate their advantages and shortcomings; (3) detailed
analysis of challenges in four important dimensions to shed light on future
directions."
22,22.0,"The early research focuses on mi-
grating other QA task methods to HybridQA, while
with further research, more and more HybridQA-
speciﬁc methods have been present.","With the rapid
development of HybridQA, the systematic survey
is still under-explored to summarize the main tech-
niques and advance further research.","So we present
this work to summarize the current HybridQA
benchmarks and methods, then analyze the chal-
lenges and future directions of this task.",2022-12-27 12:34:57+00:00,"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions",cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dingzirui Wang'), arxiv.Result.Author('Longxu Dou'), arxiv.Result.Author('Wanxiang Che')]","Table-and-text hybrid question answering (HybridQA) is a widely used and
challenging NLP task commonly applied in the financial and scientific domain.
The early research focuses on migrating other QA task methods to HybridQA,
while with further research, more and more HybridQA-specific methods have been
present. With the rapid development of HybridQA, the systematic survey is still
under-explored to summarize the main techniques and advance further research.
So we present this work to summarize the current HybridQA benchmarks and
methods, then analyze the challenges and future directions of this task. The
contributions of this paper can be summarized in three folds: (1) first survey,
to our best knowledge, including benchmarks, methods and challenges for
HybridQA; (2) systematic investigation with the reasonable comparison of the
existing systems to articulate their advantages and shortcomings; (3) detailed
analysis of challenges in four important dimensions to shed light on future
directions."
23,23.0,"This will enable
the development of more comprehensive comics processing backbones that can
tackle high-level features of comics.","Overall, there is signiﬁcant potential for further research in the ﬁeld of com-
putational studies of comics.","By addressing the challenges of text detection,
comics component processing, and uniﬁed OCR, we can enable the development
of more eﬀective computational methods for understanding the content of comics
and enable the development of high-level processing methods for tasks such as
narrative understanding and story generation.",2022-12-27 12:05:23+00:00,A Comprehensive Gold Standard and Benchmark for Comics Text Detection and Recognition,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Gürkan Soykan'), arxiv.Result.Author('Deniz Yuret'), arxiv.Result.Author('Tevfik Metin Sezgin')]","This study focuses on improving the optical character recognition (OCR) data
for panels in the COMICS dataset, the largest dataset containing text and
images from comic books. To do this, we developed a pipeline for OCR processing
and labeling of comic books and created the first text detection and
recognition datasets for western comics, called ""COMICS Text+: Detection"" and
""COMICS Text+: Recognition"". We evaluated the performance of state-of-the-art
text detection and recognition models on these datasets and found significant
improvement in word accuracy and normalized edit distance compared to the text
in COMICS. We also created a new dataset called ""COMICS Text+"", which contains
the extracted text from the textboxes in the COMICS dataset. Using the improved
text data of COMICS Text+ in the comics processing model from resulted in
state-of-the-art performance on cloze-style tasks without changing the model
architecture. The COMICS Text+ dataset can be a valuable resource for
researchers working on tasks including text detection, recognition, and
high-level processing of comics, such as narrative understanding, character
relations, and story generation. All the data and inference instructions can be
accessed in https://github.com/gsoykan/comics_text_plus."
24,24.0,"FUTURE DIRECTIONS

This section presents and discusses many prospective re-
search directions in the future.","Although some directions have
been covered in above sections, we believe they are necessary
for FedRS, and need to be further researched.",Decentralized FedRS.,2022-12-27 08:09:45+00:00,A Survey on Federated Recommendation Systems,cs.IR,"['cs.IR', 'cs.AI', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Zehua Sun'), arxiv.Result.Author('Yonghui Xu'), arxiv.Result.Author('Yong Liu'), arxiv.Result.Author('Wei He'), arxiv.Result.Author('Yali Jiang'), arxiv.Result.Author('Fangzhao Wu'), arxiv.Result.Author('Lizhen Cui')]","Federated learning has recently been applied to recommendation systems to
protect user privacy. In federated learning settings, recommendation systems
can train recommendation models only collecting the intermediate parameters
instead of the real user data, which greatly enhances the user privacy. Beside,
federated recommendation systems enable to collaborate with other data
platforms to improve recommended model performance while meeting the regulation
and privacy constraints. However, federated recommendation systems faces many
new challenges such as privacy, security, heterogeneity and communication
costs. While significant research has been conducted in these areas, gaps in
the surveying literature still exist. In this survey, we-(1) summarize some
common privacy mechanisms used in federated recommendation systems and discuss
the advantages and limitations of each mechanism; (2) review some robust
aggregation strategies and several novel attacks against security; (3)
summarize some approaches to address heterogeneity and communication costs
problems; (4)introduce some open source platforms that can be used to build
federated recommendation systems; (5) present some prospective research
directions in the future. This survey can guide researchers and practitioners
understand the research progress in these areas."
25,25.0,"Extensive experiments demonstrate a
promising false positive suppression in both retrospective and prospective validation.","In addition, the released dataset can be used to perform ‘stress’ tests on established de-
tection systems and encourages further research toward robust and reliable computer-
aided endoscopic image analysis.","The dataset and code will be publicly available at
http://endoboost.miccai.cloud.",2022-12-23 08:34:36+00:00,EndoBoost: a plug-and-play module for false positive suppression during computer-aided polyp detection in real-world colonoscopy (with dataset),cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Haoran Wang'), arxiv.Result.Author('Yan Zhu'), arxiv.Result.Author('Wenzheng Qin'), arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Pinghong Zhou'), arxiv.Result.Author('Quanlin Li'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Zhijian Song')]","The advance of computer-aided detection systems using deep learning opened a
new scope in endoscopic image analysis. However, the learning-based models
developed on closed datasets are susceptible to unknown anomalies in complex
clinical environments. In particular, the high false positive rate of polyp
detection remains a major challenge in clinical practice. In this work, we
release the FPPD-13 dataset, which provides a taxonomy and real-world cases of
typical false positives during computer-aided polyp detection in real-world
colonoscopy. We further propose a post-hoc module EndoBoost, which can be
plugged into generic polyp detection models to filter out false positive
predictions. This is realized by generative learning of the polyp manifold with
normalizing flows and rejecting false positives through density estimation.
Compared to supervised classification, this anomaly detection paradigm achieves
better data efficiency and robustness in open-world settings. Extensive
experiments demonstrate a promising false positive suppression in both
retrospective and prospective validation. In addition, the released dataset can
be used to perform 'stress' tests on established detection systems and
encourages further research toward robust and reliable computer-aided
endoscopic image analysis. The dataset and code will be publicly available at
http://endoboost.miccai.cloud."
26,26.0,"We also extend experiments from in-domain to domain-shift data and show that selective
scaling consistently outperforms.","- We provide useful calibration observations as a comprehensive reference for further research on
segmentation model calibration.","2 RELATED WORK

DNN Calibration.",2022-12-22 22:05:16+00:00,On Calibrating Semantic Segmentation Models: Analysis and An Algorithm,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dongdong Wang'), arxiv.Result.Author('Boqing Gong'), arxiv.Result.Author('Liqiang Wang')]","We study the problem of semantic segmentation calibration. For image
classification, lots of existing solutions are proposed to alleviate model
miscalibration of confidence. However, to date, confidence calibration research
on semantic segmentation is still limited. We provide a systematic study on the
calibration of semantic segmentation models and propose a simple yet effective
approach. First, we find that model capacity, crop size, multi-scale testing,
and prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration, and show
that selective scaling consistently outperforms other methods."
27,27.0,"Simulation results using real-world data from the

Italian Volleyball League (SuperLega) are shown, conﬁrming the applicability of the theoretical

ﬁndings.","Further research work could address the development of adjustment rules for the step

size of the algorithm given its impact on the algorithm performance as well as proceed the

stochastic analysis of other extensions and variations of the (original) Elo algorithm.","Declaration of competing interest

The authors declare that they have no known competing ﬁnancial interests or personal

relationships that could have appeared to inﬂuence the work reported in this paper.",2022-12-22 19:50:00+00:00,"A comprehensive analysis of the Elo rating algorithm: Stochastic model, convergence characteristics, design guidelines, and experimental results",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Daniel Gomes de Pinho Zanco'), arxiv.Result.Author('Leszek Szczecinski'), arxiv.Result.Author('Eduardo Vinicius Kuhn'), arxiv.Result.Author('Rui Seara')]","The Elo algorithm, due to its simplicity, is widely used for rating in sports
competitions as well as in other applications where the rating/ranking is a
useful tool for predicting future results. However, despite its widespread use,
a detailed understanding of the convergence properties of the Elo algorithm is
still lacking. Aiming to fill this gap, this paper presents a comprehensive
(stochastic) analysis of the Elo algorithm, considering round-robin
(one-on-one) competitions. Specifically, analytical expressions are derived
characterizing the behavior/evolution of the skills and of important
performance metrics. Then, taking into account the relationship between the
behavior of the algorithm and the step-size value, which is a hyperparameter
that can be controlled, some design guidelines as well as discussions about the
performance of the algorithm are provided. To illustrate the applicability of
the theoretical findings, experimental results are shown, corroborating the
very good match between analytical predictions and those obtained from the
algorithm using real-world data (from the Italian SuperLega, Volleyball
League)."
28,28.0,"We
examine how well the three variants place synonyms together and keep homonyms apart, their abil-
ity to recall synonyms as a function of training set size, and their training efﬁciency.","Finally, we
discuss related work on incremental learning and directions for further research.",1.,2022-12-22 18:16:58+00:00,Efficient Induction of Language Models Via Probabilistic Concept Formation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Christopher J. MacLellan'), arxiv.Result.Author('Peter Matsakis'), arxiv.Result.Author('Pat Langley')]","This paper presents a novel approach to the acquisition of language models
from corpora. The framework builds on Cobweb, an early system for constructing
taxonomic hierarchies of probabilistic concepts that used a tabular,
attribute-value encoding of training cases and concepts, making it unsuitable
for sequential input like language. In response, we explore three new
extensions to Cobweb -- the Word, Leaf, and Path variants. These systems encode
each training case as an anchor word and surrounding context words, and they
store probabilistic descriptions of concepts as distributions over anchor and
context information. As in the original Cobweb, a performance element sorts a
new instance downward through the hierarchy and uses the final node to predict
missing features. Learning is interleaved with performance, updating concept
probabilities and hierarchy structure as classification occurs. Thus, the new
approaches process training cases in an incremental, online manner that it very
different from most methods for statistical language learning. We examine how
well the three variants place synonyms together and keep homonyms apart, their
ability to recall synonyms as a function of training set size, and their
training efficiency. Finally, we discuss related work on incremental learning
and directions for further research."
29,29.0,"The Path variant also offers a novel method for
dynamically updating the underlying representation during incremental learning.","We believe the
current work sets the stage for innovative approaches to statistical language learning that differ con-
siderably from current language inducers and we hope that it inspires further research on human-like
learning.","Just as Word2Vec led, over the past decade, to large language models with impressive abil-
ities, we hope that our contextual extensions to Cobweb will develop into large-scale human-like
language systems that exhibit efﬁcient, incremental, and continual learning.",2022-12-22 18:16:58+00:00,Efficient Induction of Language Models Via Probabilistic Concept Formation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Christopher J. MacLellan'), arxiv.Result.Author('Peter Matsakis'), arxiv.Result.Author('Pat Langley')]","This paper presents a novel approach to the acquisition of language models
from corpora. The framework builds on Cobweb, an early system for constructing
taxonomic hierarchies of probabilistic concepts that used a tabular,
attribute-value encoding of training cases and concepts, making it unsuitable
for sequential input like language. In response, we explore three new
extensions to Cobweb -- the Word, Leaf, and Path variants. These systems encode
each training case as an anchor word and surrounding context words, and they
store probabilistic descriptions of concepts as distributions over anchor and
context information. As in the original Cobweb, a performance element sorts a
new instance downward through the hierarchy and uses the final node to predict
missing features. Learning is interleaved with performance, updating concept
probabilities and hierarchy structure as classification occurs. Thus, the new
approaches process training cases in an incremental, online manner that it very
different from most methods for statistical language learning. We examine how
well the three variants place synonyms together and keep homonyms apart, their
ability to recall synonyms as a function of training set size, and their
training efficiency. Finally, we discuss related work on incremental learning
and directions for further research."
30,30.0,"Perhaps, pretraining with a fully trained policy provides much more additional experience to the
hindsight estimator than a uniform one, because winning games takes much longer episodes covering
various states.","Further research regarding this scenario could test pretraining the hindsight estimator
for a ﬁxed amount of steps instead of episodes, to make the comparison more fair.","4.5.2 Policy Prior

Section 4.4.2 demonstrates the importance of synchronizing the hindsight estimator to the acting policy,
but needs additional resources to do so.",2022-12-22 12:06:37+00:00,Towards Causal Credit Assignment,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Mátyás Schubert')],"Adequately assigning credit to actions for future outcomes based on their
contributions is a long-standing open challenge in Reinforcement Learning. The
assumptions of the most commonly used credit assignment method are
disadvantageous in tasks where the effects of decisions are not immediately
evident. Furthermore, this method can only evaluate actions that have been
selected by the agent, making it highly inefficient. Still, no alternative
methods have been widely adopted in the field. Hindsight Credit Assignment is a
promising, but still unexplored candidate, which aims to solve the problems of
both long-term and counterfactual credit assignment. In this thesis, we
empirically investigate Hindsight Credit Assignment to identify its main
benefits, and key points to improve. Then, we apply it to factored state
representations, and in particular to state representations based on the causal
structure of the environment. In this setting, we propose a variant of
Hindsight Credit Assignment that effectively exploits a given causal structure.
We show that our modification greatly decreases the workload of Hindsight
Credit Assignment, making it more efficient and enabling it to outperform the
baseline credit assignment method on various tasks. This opens the way to other
methods based on given or learned causal structures."
31,31.0,"Increasing the threshold (currently set to 0), or applying top k-winners as in (Wortsman et al., 2020)
for supervised learning, may lead to a meta optimization process where signiﬁcantly smaller masks maintain
acceptable levels of performance.","In summary, while more work is required to improve the memory eﬃciency
of the proposed approaches, the success of the linear combination methods suggests venues of research to
reduce memory consumption, while the performance advantages justify further research of masking methods
in LRL.","8 Conclusion

This work introduces the use of modulating masks for lifelong reinforcement learning problems.",2022-12-21 15:49:20+00:00,Lifelong Reinforcement Learning with Modulating Masks,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Eseoghene Ben-Iwhiwhu'), arxiv.Result.Author('Saptarshi Nath'), arxiv.Result.Author('Praveen K. Pilly'), arxiv.Result.Author('Soheil Kolouri'), arxiv.Result.Author('Andrea Soltoggio')]","Lifelong learning aims to create AI systems that continuously and
incrementally learn during a lifetime, similar to biological learning. Attempts
so far have met problems, including catastrophic forgetting, interference among
tasks, and the inability to exploit previous knowledge. While considerable
research has focused on learning multiple input distributions, typically in
classification, lifelong reinforcement learning (LRL) must also deal with
variations in the state and transition distributions, and in the reward
functions. Modulating masks, recently developed for classification, are
particularly suitable to deal with such a large spectrum of task variations. In
this paper, we adapted modulating masks to work with deep LRL, specifically PPO
and IMPALA agents. The comparison with LRL baselines in both discrete and
continuous RL tasks shows competitive performance. We further investigated the
use of a linear combination of previously learned masks to exploit previous
knowledge when learning new tasks: not only is learning faster, the algorithm
solves tasks that we could not otherwise solve from scratch due to extremely
sparse rewards. The results suggest that RL with modulating masks is a
promising approach to lifelong learning, to the composition of knowledge to
learn increasingly complex tasks, and to knowledge reuse for efficient and
faster learning."
32,32.0,"Carousel shaping ensures each stage receives 

the same training amount.","Since this chapter focuses on the impact of OI, further research on 

incorporating these methods into OI is left open.","𝜖-greedy and softmax are both commonly applied exploration techniques in reinforcement 

learning.",2022-12-21 15:33:01+00:00,On Reinforcement Learning for the Game of 2048,cs.LG,"['cs.LG', 'cs.AI', 'I.2.6; I.2.8']",[arxiv.Result.Author('Hung Guei')],"2048 is a single-player stochastic puzzle game. This intriguing and addictive
game has been popular worldwide and has attracted researchers to develop
game-playing programs. Due to its simplicity and complexity, 2048 has become an
interesting and challenging platform for evaluating the effectiveness of
machine learning methods. This dissertation conducts comprehensive research on
reinforcement learning and computer game algorithms for 2048. First, this
dissertation proposes optimistic temporal difference learning, which
significantly improves the quality of learning by employing optimistic
initialization to encourage exploration for 2048. Furthermore, based on this
approach, a state-of-the-art program for 2048 is developed, which achieves the
highest performance among all learning-based programs, namely an average score
of 625377 points and a rate of 72% for reaching 32768-tiles. Second, this
dissertation investigates several techniques related to 2048, including the
n-tuple network ensemble learning, Monte Carlo tree search, and deep
reinforcement learning. These techniques are promising for further improving
the performance of the current state-of-the-art program. Finally, this
dissertation discusses pedagogical applications related to 2048 by proposing
course designs and summarizing the teaching experience. The proposed course
designs use 2048-like games as materials for beginners to learn reinforcement
learning and computer game algorithms. The courses have been successfully
applied to graduate-level students and received well by student feedback."
33,33.0,"The prerequisite is that the agent has
learned a generally applicable policy that is suitable for the
problem instance.","Based on the hypothesis that conventional
metaheuristics can be improved in this way, further research

9

Appendix A.","Settings and hyperparameters for the numer-

ical experiments

Table A.5: MILP Solver environment

Solver:
OS:
vCPU:
RAM:

IBM CPLEX V12
CentOS 7.5.1804
Intel Xeon E5-2630 V4 2.2 GHz with 28 cores assigned
64 GB DDR4 2133 MHz

Table A.6: GA and GASA hyperparameters

Hyperparameter

Value

Initialization method

First population size
Selection method
Number of survivors
Oﬀspring size
Mutation probability
Combination probability
Generations
SA exploitation probability
SA iterations
SA attempts to restart

Equally distributed resources
Random dispatching rules
50 genomes
Best (survival of the ﬁttest)
8
20
Adaptive (linearly decreasing)
Adaptive (linearly increasing)
25
0% (GA), 5% (GASA, GASA+RL)
100
25

Table A.7: DRL hyperparameters for the training process and for the actor-
crititic PPO network

Hyperparameter

Value

Overall training steps
Network update interval (steps)
Learning rate
Discount factor
Activation function
Network architecture (hidden layers)

Dynamic (30,000 on GBRT03)
Dynamic (10 on GBRT03)
0.0001 (linearly decreasing)
0.999
Rectiﬁed Linear Unit (leaky)
29 neurons (shared)
27, 26 neurons (value net)
26 neurons (policy net)

Appendix B. DRL observation space and intermediate re-

ward function

Table B.8: DRL observation space features (i: Feature index)

i

1

2

3

Description

Relative environment time: Current simulation time in relation to the av-
erage WIP per station.",2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
34,34.0,"This is not a disadvantage of this study,
as we were still able to conﬁrm that DRL can improve the re-
sults of a memetic algorithm.","However, further research should
be undertaken to examine the predictability of appropriate hy-
perparameters for diﬀerent production environments consider-
ing varying numbers of stations, workers or tasks.",6.,2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
35,35.0,"The deployment of the framework can
play a crucial role for a realistic production planning in the
mentioned domain.","However, the proposed framework has
some shortcomings that lead to further research questions.","The
central scientiﬁc question raised by this study is how DRL can
be better generalized to solve varying and complex planning
scenarios.",2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
36,36.0,"Therefore, the success of IoS
critically lingers on exploiting 6G physical layer enabling
technologies, semantic communications and edge intelligence.","How intelligent agents at the edge handle partial observability
for overcoming these challenges also requires further research.","the authors propose new solutions for
semantic communications in the area of machine intelligence,
while the latter also provides a holistic survey.",2022-12-21 03:37:38+00:00,The Internet of Senses: Building on Semantic Communications and Edge Intelligence,cs.AI,"['cs.AI', 'eess.SP']","[arxiv.Result.Author('Roghayeh Joda'), arxiv.Result.Author('Medhat Elsayed'), arxiv.Result.Author('Hatem Abou-zeid'), arxiv.Result.Author('Ramy Atawia'), arxiv.Result.Author('Akram Bin Sediq'), arxiv.Result.Author('Gary Boudreau'), arxiv.Result.Author('Melike Erol-Kantarci'), arxiv.Result.Author('Lajos Hanzo')]","The Internet of Senses (IoS) holds the promise of flawless telepresence-style
communication for all human `receptors' and therefore blurs the difference of
virtual and real environments. We commence by highlighting the compelling use
cases empowered by the IoS and also the key network requirements. We then
elaborate on how the emerging semantic communications and Artificial
Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies
may satisfy the requirements of IoS use cases. On one hand, semantic
communications can be applied for extracting meaningful and significant
information and hence efficiently exploit the resources and for harnessing a
priori information at the receiver to satisfy IoS requirements. On the other
hand, AI/ML facilitates frugal network resource management by making use of the
enormous amount of data generated in IoS edge nodes and devices, as well as by
optimizing the IoS performance via intelligent agents. However, the intelligent
agents deployed at the edge are not completely aware of each others' decisions
and the environments of each other, hence they operate in a partially rather
than fully observable environment. Therefore, we present a case study of
Partially Observable Markov Decision Processes (POMDP) for improving the User
Equipment (UE) throughput and energy consumption, as they are imperative for
IoS use cases, using Reinforcement Learning for astutely activating and
deactivating the component carriers in carrier aggregation. Finally, we outline
the challenges and open issues of IoS implementations and employing semantic
communications, edge intelligence as well as learning under partial
observability in the IoS context."
37,37.0,"HTC will play an
important role in IoS, as it creates realistic telepresence
environments.","Naturally,
the multiplicity of angular views
are highly correlated, hence they lend themselves to high
compression holographic encoding, which is in its infancy and
requiring further research.","HTC is naturally related to human vision, but when further
augmented with data from other senses, it can facilitate the
creation of a digital twin of an environment as highlighted in
the following section.",2022-12-21 03:37:38+00:00,The Internet of Senses: Building on Semantic Communications and Edge Intelligence,cs.AI,"['cs.AI', 'eess.SP']","[arxiv.Result.Author('Roghayeh Joda'), arxiv.Result.Author('Medhat Elsayed'), arxiv.Result.Author('Hatem Abou-zeid'), arxiv.Result.Author('Ramy Atawia'), arxiv.Result.Author('Akram Bin Sediq'), arxiv.Result.Author('Gary Boudreau'), arxiv.Result.Author('Melike Erol-Kantarci'), arxiv.Result.Author('Lajos Hanzo')]","The Internet of Senses (IoS) holds the promise of flawless telepresence-style
communication for all human `receptors' and therefore blurs the difference of
virtual and real environments. We commence by highlighting the compelling use
cases empowered by the IoS and also the key network requirements. We then
elaborate on how the emerging semantic communications and Artificial
Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies
may satisfy the requirements of IoS use cases. On one hand, semantic
communications can be applied for extracting meaningful and significant
information and hence efficiently exploit the resources and for harnessing a
priori information at the receiver to satisfy IoS requirements. On the other
hand, AI/ML facilitates frugal network resource management by making use of the
enormous amount of data generated in IoS edge nodes and devices, as well as by
optimizing the IoS performance via intelligent agents. However, the intelligent
agents deployed at the edge are not completely aware of each others' decisions
and the environments of each other, hence they operate in a partially rather
than fully observable environment. Therefore, we present a case study of
Partially Observable Markov Decision Processes (POMDP) for improving the User
Equipment (UE) throughput and energy consumption, as they are imperative for
IoS use cases, using Reinforcement Learning for astutely activating and
deactivating the component carriers in carrier aggregation. Finally, we outline
the challenges and open issues of IoS implementations and employing semantic
communications, edge intelligence as well as learning under partial
observability in the IoS context."
38,38.0,"A central aim of the competition was to design a problem in
which both forecasting and optimization are important tasks to
perform well.","While this objective has been achieved to some
degree, and all shortlisted participants but one had competitive
forecasting methodologies (indicated by a MASE less than 1),
it is a difﬁcult task and the interplay between forecasting and
optimization in such an integrated system still has ample room
for further research.","While the number of time series was, to the best of our
knowledge, larger than in any other similar undertaking before,
it is still a relatively small amount if series, and drawing very
ﬁne-grained conclusions seems inadequate.",2022-12-21 02:34:12+00:00,Comparison and Evaluation of Methods for a Predict+Optimize Problem in Renewable Energy,cs.AI,['cs.AI'],"[arxiv.Result.Author('Christoph Bergmeir'), arxiv.Result.Author('Frits de Nijs'), arxiv.Result.Author('Abishek Sriramulu'), arxiv.Result.Author('Mahdi Abolghasemi'), arxiv.Result.Author('Richard Bean'), arxiv.Result.Author('John Betts'), arxiv.Result.Author('Quang Bui'), arxiv.Result.Author('Nam Trong Dinh'), arxiv.Result.Author('Nils Einecke'), arxiv.Result.Author('Rasul Esmaeilbeigi'), arxiv.Result.Author('Scott Ferraro'), arxiv.Result.Author('Priya Galketiya'), arxiv.Result.Author('Evgenii Genov'), arxiv.Result.Author('Robert Glasgow'), arxiv.Result.Author('Rakshitha Godahewa'), arxiv.Result.Author('Yanfei Kang'), arxiv.Result.Author('Steffen Limmer'), arxiv.Result.Author('Luis Magdalena'), arxiv.Result.Author('Pablo Montero-Manso'), arxiv.Result.Author('Daniel Peralta'), arxiv.Result.Author('Yogesh Pipada Sunil Kumar'), arxiv.Result.Author('Alejandro Rosales-Pérez'), arxiv.Result.Author('Julian Ruddick'), arxiv.Result.Author('Akylas Stratigakos'), arxiv.Result.Author('Peter Stuckey'), arxiv.Result.Author('Guido Tack'), arxiv.Result.Author('Isaac Triguero'), arxiv.Result.Author('Rui Yuan')]","Algorithms that involve both forecasting and optimization are at the core of
solutions to many difficult real-world problems, such as in supply chains
(inventory optimization), traffic, and in the transition towards carbon-free
energy generation in battery/load/production scheduling in sustainable energy
systems. Typically, in these scenarios we want to solve an optimization problem
that depends on unknown future values, which therefore need to be forecast. As
both forecasting and optimization are difficult problems in their own right,
relatively few research has been done in this area. This paper presents the
findings of the ``IEEE-CIS Technical Challenge on Predict+Optimize for
Renewable Energy Scheduling,"" held in 2021. We present a comparison and
evaluation of the seven highest-ranked solutions in the competition, to provide
researchers with a benchmark problem and to establish the state of the art for
this benchmark, with the aim to foster and facilitate research in this area.
The competition used data from the Monash Microgrid, as well as weather data
and energy market data. It then focused on two main challenges: forecasting
renewable energy production and demand, and obtaining an optimal schedule for
the activities (lectures) and on-site batteries that lead to the lowest cost of
energy. The most accurate forecasts were obtained by gradient-boosted tree and
random forest models, and optimization was mostly performed using mixed integer
linear and quadratic programming. The winning method predicted different
scenarios and optimized over all scenarios jointly using a sample average
approximation method."
39,39.0,"This shows that as we introduce new functionality
to machine learning systems, we must be aware of novel threats that emerge.","We outline a few
extensions of our approach and interesting directions for further research:

• Our method for generating poison and camouﬂage points was based on the gradient-matching
attack of Geiping et al.",(2021).,2022-12-21 01:52:17+00:00,Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CY']","[arxiv.Result.Author('Jimmy Z. Di'), arxiv.Result.Author('Jack Douglas'), arxiv.Result.Author('Jayadev Acharya'), arxiv.Result.Author('Gautam Kamath'), arxiv.Result.Author('Ayush Sekhari')]","We introduce camouflaged data poisoning attacks, a new attack vector that
arises in the context of machine unlearning and other settings when model
retraining may be induced. An adversary first adds a few carefully crafted
points to the training dataset such that the impact on the model's predictions
is minimal. The adversary subsequently triggers a request to remove a subset of
the introduced points at which point the attack is unleashed and the model's
predictions are negatively affected. In particular, we consider clean-label
targeted attacks (in which the goal is to cause the model to misclassify a
specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.
This attack is realized by constructing camouflage datapoints that mask the
effect of a poisoned dataset."
40,40.0,"7.4 Multi-modal Mathematical Reasoning

In recent years, there has been growing interest
in multi-modal mathematical reasoning, which in-
volves using multiple sources of information, such
as text, tables, natural images, and diagrams, to
solve mathematical problems (Kahou et al., 2017;
Kaﬂe et al., 2018; Lu et al., 2021b, 2022b).","De-
spite this growing interest, there are still many chal-
lenges and opportunities for further research in this
ﬁeld.","Currently available datasets in this domain
tend to be small (Zhao et al., 2022), generated from
templates (Kahou et al., 2017), or focus on speciﬁc
topics (Lu et al., 2021a; Chen et al., 2022a).",2022-12-20 18:46:16+00:00,A Survey of Deep Learning for Mathematical Reasoning,cs.AI,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Pan Lu'), arxiv.Result.Author('Liang Qiu'), arxiv.Result.Author('Wenhao Yu'), arxiv.Result.Author('Sean Welleck'), arxiv.Result.Author('Kai-Wei Chang')]","Mathematical reasoning is a fundamental aspect of human intelligence and is
applicable in various fields, including science, engineering, finance, and
everyday life. The development of artificial intelligence (AI) systems capable
of solving math problems and proving theorems has garnered significant interest
in the fields of machine learning and natural language processing. For example,
mathematics serves as a testbed for aspects of reasoning that are challenging
for powerful deep learning models, driving new algorithmic and modeling
advances. On the other hand, recent advances in large-scale neural language
models have opened up new benchmarks and opportunities to use deep learning for
mathematical reasoning. In this survey paper, we review the key tasks,
datasets, and methods at the intersection of mathematical reasoning and deep
learning over the past decade. We also evaluate existing benchmarks and
methods, and discuss future research directions in this domain."
41,41.0,"This leaves
open the question of whether the models are ac-
tually able to reason in a way that is similar to
human reasoning, or whether they are simply able
to achieve good performance on the tasks through
other means.","Further research is needed to more
formally analyze the reasoning abilities of LLMs.","5 Findings and Implications

In this section, we summarize the important ﬁnd-
ings and implications of studies on reasoning in
large language models.",2022-12-20 16:29:03+00:00,Towards Reasoning in Large Language Models: A Survey,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jie Huang'), arxiv.Result.Author('Kevin Chen-Chuan Chang')]","Reasoning is a fundamental aspect of human intelligence that plays a crucial
role in activities such as problem solving, decision making, and critical
thinking. In recent years, large language models (LLMs) have made significant
progress in natural language processing, and there is observation that these
models may exhibit reasoning abilities when they are sufficiently large.
However, it is not yet clear to what extent LLMs are capable of reasoning. This
paper provides a comprehensive overview of the current state of knowledge on
reasoning in LLMs, including techniques for improving and eliciting reasoning
in these models, methods and benchmarks for evaluating reasoning abilities,
findings and implications of previous research in this field, and suggestions
on future directions. Our aim is to provide a detailed and up-to-date review of
this topic and stimulate meaningful discussion and future work."
42,42.0,"On multi-choice tasks like Commonsense
QA, all three alternatives outperform TopK (left
side of Figure 2(a)).","The frustration of TopK on
multi-choice tasks suggests that applying ICL to
these tasks holds immense prospects, and therefore
remains an interesting ﬁeld of further research.","6.2 Accuracy of ranking method

In our ranking module, we randomly select 10 dif-
ferent permutations for each testing sample and
use MDL to select the best-performing one in an
unsupervised manner.",2022-12-20 15:55:21+00:00,Self-adaptive In-context Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Yaoxiang Wang'), arxiv.Result.Author('Jiacheng Ye'), arxiv.Result.Author('Lingpeng Kong')]","Despite the surprising few-shot performance of in-context learning (ICL), it
is still a common practice to randomly sample examples to serve as context.
This paper advocates a new principle for ICL: self-adaptive in-context
learning. The self-adaption mechanism is introduced to help each sample find an
in-context example permutation (i.e., selection and ordering) that can derive
the correct prediction, thus maximizing performance. To validate the
effectiveness of self-adaptive ICL, we propose a general select-then-rank
framework and instantiate it with new selection and ranking algorithms. Upon
extensive evaluation on eight different NLP datasets, our self-adaptive ICL
method achieves a 40% relative improvement over the common practice setting.
Further analysis reveals the enormous potential of self-adaptive ICL that it
might be able to close the gap between ICL and finetuning given more advanced
algorithms. Our code is released to facilitate future research in this area:
https://github.com/Shark-NLP/self-adaptive-ICL"
43,43.0,"However, NN models have shown a great ability to reconstruct complex EO patterns [Diaconu et al.,
2022].","The problem of how to adapt the MV learning model to missing views still has open points that could
motivate further research.","For example, consider when more than two views are used, the missing views can
be dynamic and a robust model could be suitable.",2022-12-20 15:12:27+00:00,Common Practices and Taxonomy in Deep Multi-view Fusion for Remote Sensing Applications,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Francisco Mena'), arxiv.Result.Author('Diego Arenas'), arxiv.Result.Author('Marlon Nuske'), arxiv.Result.Author('Andreas Dengel')]","The advances in remote sensing technologies have boosted applications for
Earth observation. These technologies provide multiple observations or views
with different levels of information. They might contain static or temporary
views with different levels of resolution, in addition to having different
types and amounts of noise due to sensor calibration or deterioration. A great
variety of deep learning models have been applied to fuse the information from
these multiple views, known as deep multi-view or multi-modal fusion learning.
However, the approaches in the literature vary greatly since different
terminology is used to refer to similar concepts or different illustrations are
given to similar techniques. This article gathers works on multi-view fusion
for Earth observation by focusing on the common practices and approaches used
in the literature. We summarize and structure insights from several different
publications concentrating on unifying points and ideas. In this manuscript, we
provide a harmonized terminology while at the same time mentioning the various
alternative terms that are used in literature. The topics covered by the works
reviewed focus on supervised learning with the use of neural network models. We
hope this review, with a long list of recent references, can support future
research and lead to a unified advance in the area."
44,44.0,"Though DSI models are prone to catastrophic forgetting while we index new docu-
ments, we have shown through extensive experiments that our two proposed solutions to optimize
for the ﬂatter loss basins using SAM and using generative memory alleviate forgetting to a signiﬁ-
cant extent.","With this work, we lay down foundations for further research in this space, so that the
strategies here do not just beneﬁt DSI, but the continual learning community in general.","ETHICS STATEMENT

Training large models is expensive, and has a detrimental impact on the environment (Strubell et al.,
2019).",2022-12-19 18:59:34+00:00,DSI++: Updating Transformer Memory with New Documents,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Sanket Vaibhav Mehta'), arxiv.Result.Author('Jai Gupta'), arxiv.Result.Author('Yi Tay'), arxiv.Result.Author('Mostafa Dehghani'), arxiv.Result.Author('Vinh Q. Tran'), arxiv.Result.Author('Jinfeng Rao'), arxiv.Result.Author('Marc Najork'), arxiv.Result.Author('Emma Strubell'), arxiv.Result.Author('Donald Metzler')]","Differentiable Search Indices (DSIs) encode a corpus of documents in the
parameters of a model and use the same model to map queries directly to
relevant document identifiers. Despite the strong performance of DSI models,
deploying them in situations where the corpus changes over time is
computationally expensive because reindexing the corpus requires re-training
the model. In this work, we introduce DSI++, a continual learning challenge for
DSI to incrementally index new documents while being able to answer queries
related to both previously and newly indexed documents. Across different model
scales and document identifier representations, we show that continual indexing
of new documents leads to considerable forgetting of previously indexed
documents. We also hypothesize and verify that the model experiences forgetting
events during training, leading to unstable learning. To mitigate these issues,
we investigate two approaches. The first focuses on modifying the training
dynamics. Flatter minima implicitly alleviate forgetting, so we optimize for
flatter loss basins and show that the model stably memorizes more documents
(+12\%). Next, we introduce a generative memory to sample pseudo-queries for
documents and supplement them during continual indexing to prevent forgetting
for the retrieval task. Extensive experiments on novel continual indexing
benchmarks based on Natural Questions (NQ) and MS MARCO demonstrate that our
proposed solution mitigates forgetting by a significant margin. Concretely, it
improves the average Hits@10 by $+21.1\%$ over competitive baselines for NQ and
requires $6$ times fewer model updates compared to re-training the DSI model
for incrementally indexing five corpora in a sequence."
45,45.0,"Returning to invigilated and oral
exams could form part of the solution, while using advanced proctoring
techniques and AI-text output detectors may be eﬀective in addressing
this issue, they are not likely to be foolproof solutions.","Further research
is needed to fully understand the implications of large language models
like ChatGPT and to devise strategies for combating the risk of cheating
using these tools.","It is crucial for educators and institutions to be aware
of the possibility of ChatGPT being used for cheating and to investigate
measures to address it in order to maintain the fairness and validity of
online exams for all students.",2022-12-19 08:15:16+00:00,ChatGPT: The End of Online Exam Integrity?,cs.AI,"['cs.AI', 'cs.CL']",[arxiv.Result.Author('Teo Susnjak')],"This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students."
46,46.0,"The authors identify the
limitations and challenges of online examinations, including cheating issues, together
with access to technology, and the lack of standardized approaches.","The study concludes
by calling for further research on online examinations and the importance of designing
online examinations that are fair, valid, and reliable.","In a comprehensive report, Barber et al.",2022-12-19 08:15:16+00:00,ChatGPT: The End of Online Exam Integrity?,cs.AI,"['cs.AI', 'cs.CL']",[arxiv.Result.Author('Teo Susnjak')],"This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students."
47,47.0,"There are some
indications that GPT-text output detectors already in existence5 have some potential
to identify AI-generated text due to an underlying signature in all the text.","However,
these tools need further research.","With respect to the suggestion of using AI to improve the security and reliability of
the online exam platform and to detect and deter cheating may be ineﬀective.",2022-12-19 08:15:16+00:00,ChatGPT: The End of Online Exam Integrity?,cs.AI,"['cs.AI', 'cs.CL']",[arxiv.Result.Author('Teo Susnjak')],"This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students."
48,48.0,"New
AI and machine learning tools capable of detecting text outputs from ChatGPT-like
models need to be researched.","While further research is needed to fully understand the
implications of these large language models and to develop strategies for addressing the
potential for cheating using these tools.","It is important for educators and institutions
to be aware of the potential of this tool to facilitate cheating and to take steps to
combat it, in order to maintain the integrity of online exams and ensure fair and valid
assessments for all students.",2022-12-19 08:15:16+00:00,ChatGPT: The End of Online Exam Integrity?,cs.AI,"['cs.AI', 'cs.CL']",[arxiv.Result.Author('Teo Susnjak')],"This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students."
49,49.0,"Third,

domain-transferability of the model could not be tested due to the lack of suitable datasets that have

concordant labels and concordant data available for training.","8

We hope that our work triggers further research into these areas and that it promotes the clinical

applicability of multimodal deep learning models.","Online Methods

Ethics Statement

All experiments were conducted in accordance with the Declaration of Helsinki and the International

Ethical Guidelines for Biomedical Research Involving Human Subjects by the Council for International

Organizations of Medical Sciences (CIOMS).",2022-12-18 20:43:37+00:00,Medical Diagnosis with Large Scale Multimodal Transformers: Leveraging Diverse Data for More Accurate Diagnosis,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Firas Khader'), arxiv.Result.Author('Gustav Mueller-Franzes'), arxiv.Result.Author('Tianci Wang'), arxiv.Result.Author('Tianyu Han'), arxiv.Result.Author('Soroosh Tayebi Arasteh'), arxiv.Result.Author('Christoph Haarburger'), arxiv.Result.Author('Johannes Stegmaier'), arxiv.Result.Author('Keno Bressem'), arxiv.Result.Author('Christiane Kuhl'), arxiv.Result.Author('Sven Nebelung'), arxiv.Result.Author('Jakob Nikolas Kather'), arxiv.Result.Author('Daniel Truhn')]","Multimodal deep learning has been used to predict clinical endpoints and
diagnoses from clinical routine data. However, these models suffer from scaling
issues: they have to learn pairwise interactions between each piece of
information in each data type, thereby escalating model complexity beyond
manageable scales. This has so far precluded a widespread use of multimodal
deep learning. Here, we present a new technical approach of ""learnable
synergies"", in which the model only selects relevant interactions between data
modalities and keeps an ""internal memory"" of relevant data. Our approach is
easily scalable and naturally adapts to multimodal data inputs from clinical
routine. We demonstrate this approach on three large multimodal datasets from
radiology and ophthalmology and show that it outperforms state-of-the-art
models in clinically relevant diagnosis tasks. Our new approach is transferable
and will allow the application of multimodal deep learning to a broad set of
clinically relevant problems."
50,50.0,"We describe the architecture of the web-based system and
how to use it for the MeSH term suggestion task.","For the Python
library, we describe how the library can be used for advancing
further research and experimentation, and we validate the results of
the methods contained in the library on standard datasets.","Our web-
based prototype system is available at http://ielab-mesh-suggest.",2022-12-18 05:32:19+00:00,MeSH Suggester: A Library and System for MeSH Term Suggestion for Systematic Review Boolean Query Construction,cs.IR,"['cs.IR', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Shuai Wang'), arxiv.Result.Author('Hang Li'), arxiv.Result.Author('Guido Zuccon')]","Boolean query construction is often critical for medical systematic review
literature search. To create an effective Boolean query, systematic review
researchers typically spend weeks coming up with effective query terms and
combinations. One challenge to creating an effective systematic review Boolean
query is the selection of effective MeSH Terms to include in the query. In our
previous work, we created neural MeSH term suggestion methods and compared them
to state-of-the-art MeSH term suggestion methods. We found neural MeSH term
suggestion methods to be highly effective.
  In this demonstration, we build upon our previous work by creating (1) a
Web-based MeSH term suggestion prototype system that allows users to obtain
suggestions from a number of underlying methods and (2) a Python library that
implements ours and others' MeSH term suggestion methods and that is aimed at
researchers who want to further investigate, create or deploy such type of
methods. We describe the architecture of the web-based system and how to use it
for the MeSH term suggestion task. For the Python library, we describe how the
library can be used for advancing further research and experimentation, and we
validate the results of the methods contained in the library on standard
datasets. Our web-based prototype system is available at
http://ielab-mesh-suggest.uqcloud.net, while our Python library is at
https://github.com/ielab/meshsuggestlib."
51,51.0,"[381] use
GNNs to solve, with very little supervision, the decision variant of the
Traveling Salesperson Problem (TSP), a highly relevant NP-Complete
problem.","4.7 Datasets
We summarize most popular datasets in Table 4 considering different
scenarios and expect to promote further researches.","5 FUTURE DIRECTIONS
In this section, we conclude major trends and challenges, which are
also future directions of graph learning.",2022-12-17 22:05:07+00:00,Graph Learning: A Comprehensive Survey and Future Directions,cs.AI,['cs.AI'],"[arxiv.Result.Author('Shaopeng Wei'), arxiv.Result.Author('Yu Zhao')]","Graph learning aims to learn complex relationships among nodes and the
topological structure of graphs, such as social networks, academic networks and
e-commerce networks, which are common in the real world. Those relationships
make graphs special compared with traditional tabular data in which nodes are
dependent on non-Euclidean space and contain rich information to explore. Graph
learning developed from graph theory to graph data mining and now is empowered
with representation learning, making it achieve great performances in various
scenarios, even including text, image, chemistry, and biology. Due to the broad
application prospects in the real world, graph learning has become a popular
and promising area in machine learning. Thousands of works have been proposed
to solve various kinds of problems in graph learning and is appealing more and
more attention in academic community, which makes it pivotal to survey previous
valuable works. Although some of the researchers have noticed this phenomenon
and finished impressive surveys on graph learning. However, they failed to link
related objectives, methods and applications in a more logical way and cover
current ample scenarios as well as challenging problems due to the rapid
expansion of the graph learning."
52,52.0,"There still lacks
a comprehensive survey to conclude previous works in more logic
and organic way.","What’s more, the high-speed development of this
area makes it difﬁcult to cover current ample applications, trends
and future challenges, which are vital to further research on graph
learning.","To remedy this, we collect and analyze the latest works to
provide meaningful suggestions.",2022-12-17 22:05:07+00:00,Graph Learning: A Comprehensive Survey and Future Directions,cs.AI,['cs.AI'],"[arxiv.Result.Author('Shaopeng Wei'), arxiv.Result.Author('Yu Zhao')]","Graph learning aims to learn complex relationships among nodes and the
topological structure of graphs, such as social networks, academic networks and
e-commerce networks, which are common in the real world. Those relationships
make graphs special compared with traditional tabular data in which nodes are
dependent on non-Euclidean space and contain rich information to explore. Graph
learning developed from graph theory to graph data mining and now is empowered
with representation learning, making it achieve great performances in various
scenarios, even including text, image, chemistry, and biology. Due to the broad
application prospects in the real world, graph learning has become a popular
and promising area in machine learning. Thousands of works have been proposed
to solve various kinds of problems in graph learning and is appealing more and
more attention in academic community, which makes it pivotal to survey previous
valuable works. Although some of the researchers have noticed this phenomenon
and finished impressive surveys on graph learning. However, they failed to link
related objectives, methods and applications in a more logical way and cover
current ample scenarios as well as challenging problems due to the rapid
expansion of the graph learning."
53,53.0,"The
modeling phase of Figure 1 depicts the diﬀerent modules employed.","3.1.1 Planning dataset

We generate a PDDL-based dataset as a benchmark to ﬁnetune pretrained CodeT5 and facilitate further research at
the intersection of LLMs and automated planning.","We use the domain model (in PDDL) to generate corresponding

3

Figure 2: Snapshot of one instance of the plan dataset for blocksworld domain.",2022-12-16 19:06:49+00:00,Plansformer: Generating Symbolic Plans using Transformers,cs.AI,['cs.AI'],"[arxiv.Result.Author('Vishal Pallagani'), arxiv.Result.Author('Bharath Muppasani'), arxiv.Result.Author('Keerthiram Murugesan'), arxiv.Result.Author('Francesca Rossi'), arxiv.Result.Author('Lior Horesh'), arxiv.Result.Author('Biplav Srivastava'), arxiv.Result.Author('Francesco Fabiano'), arxiv.Result.Author('Andrea Loreggia')]","Large Language Models (LLMs) have been the subject of active research,
significantly advancing the field of Natural Language Processing (NLP). From
BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural
language tasks such as question answering, summarization, and text generation.
Many ongoing efforts focus on understanding LLMs' capabilities, including their
knowledge of the world, syntax, and semantics. However, extending the textual
prowess of LLMs to symbolic reasoning has been slow and predominantly focused
on tackling problems related to the mathematical field. In this paper, we
explore the use of LLMs for automated planning - a branch of AI concerned with
the realization of action sequences (plans) to achieve a goal, typically
executed by intelligent agents, autonomous robots, and unmanned vehicles. We
introduce Plansformer; an LLM fine-tuned on planning problems and capable of
generating plans with favorable behavior in terms of correctness and length
with reduced knowledge-engineering efforts. We also demonstrate the
adaptability of Plansformer in solving different planning domains with varying
complexities, owing to the transfer learning abilities of LLMs. For one
configuration of Plansformer, we achieve ~97% valid plans, out of which ~95%
are optimal for Towers of Hanoi - a puzzle-solving domain."
54,54.0,"Study how to improve the performance of dialogue systems by 
integrating the MRTM model with the state-of-the-art open-source 
dialogue systems.","Accurate and prompt answering 
framework based on customer 
reviews and question-answer 
pairs [21] 

The diversity of research in the field in the e-commerce market in 
terms of expanding the range of the automatic response system, 
further research is expected, such as studies on comparing various 
items or recommending items that match them.","Generative Feature Language 
Models for Mining Implicit 
Features from Customer 
Reviews [35] 

Customer satisfaction and 
natural language processing [10] 

Understanding customer 
satisfaction via deep learning 
and natural language processing 
[12] 

User interactions with chatbot 
interfaces vs. Menu-based 
interfaces: An empirical study 
[13] 

Automatic update strategy for 
real-time discovery of hidden 
customer intents in chatbot 
systems [28] 

The possibility of evaluating the proposed method using reviews in 
other natural languages would be an interesting direction that 
warrants further investigation.",2022-12-16 18:17:07+00:00,Natural Language Processing in Customer Service: A Systematic Review,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Malak Mashaabi'), arxiv.Result.Author('Areej Alotaibi'), arxiv.Result.Author('Hala Qudaih'), arxiv.Result.Author('Raghad Alnashwan'), arxiv.Result.Author('Hend Al-Khalifa')]","Artificial intelligence and natural language processing (NLP) are
increasingly being used in customer service to interact with users and answer
their questions. The goal of this systematic review is to examine existing
research on the use of NLP technology in customer service, including the
research domain, applications, datasets used, and evaluation methods. The
review also looks at the future direction of the field and any significant
limitations. The review covers the time period from 2015 to 2022 and includes
papers from five major scientific databases. Chatbots and question-answering
systems were found to be used in 10 main fields, with the most common use in
general, social networking, and e-commerce areas. Twitter was the second most
commonly used dataset, with most research also using their own original
datasets. Accuracy, precision, recall, and F1 were the most common evaluation
methods. Future work aims to improve the performance and understanding of user
behavior and emotions, and address limitations such as the volume, diversity,
and quality of datasets. This review includes research on different spoken
languages and models and techniques."
55,55.0,"Number of occupants occupying the front seats of the AV: zero with an unladen mass of 

1247𝑘𝑔, one with a laden mass of 1327𝑘𝑔 and two with a laden mass of 1407𝑘𝑔 

iii.","Number of pedestrians: 0 to 4 

Therefore, based on knowing the above information, along with the mathematical model of the AV 
motion, further research is undertaken to allow the injury severities of the collision outcomes to be 
determined.","For example, given the scenario in Figure 1, how will the above properties affect the peak 
deformation  of  the  AV  and  the  level  of  occupant  injury  severity?",2022-12-16 15:39:44+00:00,Predicting Autonomous Vehicle Collision Injury Severity Levels for Ethical Decision Making and Path Planning,eess.SY,"['eess.SY', 'cs.AI', 'cs.SY']","[arxiv.Result.Author('James E. Pickering'), arxiv.Result.Author('Keith J. Burnham')]","Developments in autonomous vehicles (AVs) are rapidly advancing and will in
the next 20 years become a central part to our society. However, especially in
the early stages of deployment, there is expected to be incidents involving
AVs. In the event of AV incidents, decisions will need to be made that require
ethical decisions, e.g., deciding between colliding into a group of pedestrians
or a rigid barrier. For an AV to undertake such ethical decision making and
path planning, simulation models of the situation will be required that are
used in real-time on-board the AV. These models will enable path planning and
ethical decision making to be undertaken based on predetermined collision
injury severity levels. In this research, models are developed for the path
planning and ethical decision making that predetermine knowledge regarding the
possible collision injury severities, i.e., peak deformation of the AV
colliding into the rigid barrier or the impact velocity of the AV colliding
into a pedestrian. Based on such knowledge and using fuzzy logic, a novel
nonlinear weighted utility cost function for the collision injury severity
levels is developed. This allows the model-based predicted collision outcomes
arising from AV peak deformation and AV-pedestrian impact velocity to be
examined separately via weighted utility cost functions with a common
structure. The general form of the weighted utility cost function exploits a
fuzzy sets approach, thus allowing common utility costs from the two separate
utility cost functions to be meaningfully compared. A decision-making
algorithm, which makes use of a utilitarian ethical approach, ensures that the
AV will always steer onto the path which represents the lowest injury severity
level, hence utility cost to society."
56,56.0,"The second category answers the “how” question
and provides concrete implementation details and techniques
towards enabling a SemCom system.","Belonging to the last
category, this paper focuses on the summary and tutorials for
facilitating further research.","Prior to our work, publicly-available surveys & tutorials
on SemCom have shed some light on summarizing the lat-
est developments and future challenges.",2022-12-16 14:00:37+00:00,Semantics-Empowered Communication: A Tutorial-cum-Survey,cs.HC,"['cs.HC', 'cs.AI', 'cs.NI', 'eess.SP']","[arxiv.Result.Author('Zhilin Lu'), arxiv.Result.Author('Rongpeng Li'), arxiv.Result.Author('Kun Lu'), arxiv.Result.Author('Xianfu Chen'), arxiv.Result.Author('Ekram Hossain'), arxiv.Result.Author('Zhifeng Zhao'), arxiv.Result.Author('Honggang Zhang')]","Along with the springing up of semantics-empowered communication (SemCom)
researches, it is now witnessing an unprecedentedly growing interest towards a
wide range of aspects (e.g., theories, applications, metrics and
implementations) in both academia and industry. In this work, we primarily aim
to provide a comprehensive survey on both the background and research taxonomy,
as well as a detailed technical tutorial. Specifically, we start by reviewing
the literature and answering the ""what"" and ""why"" questions in semantic
transmissions. Afterwards, we present corresponding ecosystems, including
theories, metrics, datasets and toolkits, on top of which the taxonomy for
research directions is presented. Furthermore, we propose to categorize the
critical enabling techniques by explicit and implicit reasoning-based methods,
and elaborate on how they evolve and contribute to modern content \& channel
semantics-empowered communications. Besides reviewing and summarizing the
latest efforts in SemCom, we discuss the relations with other communication
levels (e.g., reliable and goal-oriented communications) from a holistic and
unified viewpoint. Subsequently, in order to facilitate the future developments
and industrial applications, we also highlight advanced practical techniques
for boosting semantic accuracy, robustness, and large-scale scalability, just
to mention a few. Finally, we discuss the technical challenges that shed light
on future research opportunities."
57,57.0,"Experimental results on three widely-used
datasets show a significant improvement while applying
DR4KT to several typical KT methods.","Upon the acceptance
of this paper, we will release the model source code for facil-
itating further research.","2 RELATED WORKS
2.1 Knowledge Tracing
Knowledge tracing originates from [4] and branches out multiple
types of methods.",2022-12-16 13:55:07+00:00,Differentiating Student Feedbacks for Knowledge Tracing,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jiajun Cui'), arxiv.Result.Author('Wei Zhang')]","In computer-aided education and intelligent tutoring systems, knowledge
tracing (KT) raises attention due to the development of data-driven learning
methods, which aims to predict students' future performance given their past
question response sequences to trace their knowledge states. However, current
deep learning approaches only focus on enhancing prediction accuracy, but
neglecting the discrimination imbalance of responses. That is, a considerable
proportion of question responses are weak to discriminate students' knowledge
states, but equally considered compared to other discriminative responses, thus
hurting the ability of tracing students' personalized knowledge states. To
tackle this issue, we propose DR4KT for Knowledge Tracing, which reweights the
contribution of different responses according to their discrimination in
training. For retaining high prediction accuracy on low discriminative
responses after reweighting, DR4KT also introduces a discrimination-aware score
fusion technique to make a proper combination between student knowledge mastery
and the questions themselves. Comprehensive experimental results show that our
DR4KT applied on four mainstream KT methods significantly improves their
performance on three widely-used datasets."
58,58.0,"We ﬁnd that for 84% of the ambiguous questions, the model is able
to generate the correct clarifying question.","Further improving the model performance in generating
relevant clarifying questions is an interesting avenue for further research.",Table 2: Human evaluation of each of the conversational turns.,2022-12-15 12:47:18+00:00,CLAM: Selective Clarification for Ambiguous Questions with Large Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Lorenz Kuhn'), arxiv.Result.Author('Yarin Gal'), arxiv.Result.Author('Sebastian Farquhar')]","State-of-the-art language models are often accurate on many
question-answering benchmarks with well-defined questions. Yet, in real
settings questions are often unanswerable without asking the user for
clarifying information. We show that current SotA models often do not ask the
user for clarification when presented with imprecise questions and instead
provide incorrect answers or ""hallucinate"". To address this, we introduce CLAM,
a framework that first uses the model to detect ambiguous questions, and if an
ambiguous question is detected, prompts the model to ask the user for
clarification. Furthermore, we show how to construct a scalable and
cost-effective automatic evaluation protocol using an oracle language model
with privileged information to provide clarifying information. We show that our
method achieves a 20.15 percentage point accuracy improvement over SotA on a
novel ambiguous question-answering answering data set derived from TriviaQA."
59,59.0,"Second, our ﬁndings are restricted to small Transformer and
simple regression problems.","Further research is needed to mechanistically understand in-context learning in
larger models and language modeling.","Third, we are excited about targeted modiﬁcations to Transformer
architectures, or their training protocols, leading to improved gradient descent based learning algorithms
or allow for alternative in-context learners to be implemented within Transformer weights, augmenting
their functionality.",2022-12-15 09:21:21+00:00,Transformers learn in-context by gradient descent,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Johannes von Oswald'), arxiv.Result.Author('Eyvind Niklasson'), arxiv.Result.Author('Ettore Randazzo'), arxiv.Result.Author('João Sacramento'), arxiv.Result.Author('Alexander Mordvintsev'), arxiv.Result.Author('Andrey Zhmoginov'), arxiv.Result.Author('Max Vladymyrov')]","Transformers have become the state-of-the-art neural network architecture
across numerous domains of machine learning. This is partly due to their
celebrated ability to transfer and to learn in-context based on few examples.
Nevertheless, the mechanisms by which Transformers become in-context learners
are not well understood and remain mostly an intuition. Here, we argue that
training Transformers on auto-regressive tasks can be closely related to
well-known gradient-based meta-learning formulations. We start by providing a
simple weight construction that shows the equivalence of data transformations
induced by 1) a single linear self-attention layer and by 2) gradient-descent
(GD) on a regression loss. Motivated by that construction, we show empirically
that when training self-attention-only Transformers on simple regression tasks
either the models learned by GD and Transformers show great similarity or,
remarkably, the weights found by optimization match the construction. Thus we
show how trained Transformers implement gradient descent in their forward pass.
This allows us, at least in the domain of regression problems, to
mechanistically understand the inner workings of optimized Transformers that
learn in-context. Furthermore, we identify how Transformers surpass plain
gradient descent by an iterative curvature correction and learn linear models
on deep data representations to solve non-linear regression tasks. Finally, we
discuss intriguing parallels to a mechanism identified to be crucial for
in-context learning termed induction-head (Olsson et al., 2022) and show how it
could be understood as a specific case of in-context learning by gradient
descent learning within Transformers."
60,60.0,"In
this paper, we aim to address this gap by proposing a computational approach to studying regret.","Our goal is to shed
light on the role of regret in human emotions and decision-making, and provide a foundation for further research in

 
 
 
 
 
 
ReDDIT/ Balouchzahi et al.",this area.,2022-12-14 23:41:57+00:00,ReDDIT: Regret Detection and Domain Identification from Text,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Fazlourrahman Balouchzahi'), arxiv.Result.Author('Sabur Butt'), arxiv.Result.Author('Grigori Sidorov'), arxiv.Result.Author('Alexander Gelbukh')]","In this paper, we present a study of regret and its expression on social
media platforms. Specifically, we present a novel dataset of Reddit texts that
have been classified into three classes: Regret by Action, Regret by Inaction,
and No Regret. We then use this dataset to investigate the language used to
express regret on Reddit and to identify the domains of text that are most
commonly associated with regret. Our findings show that Reddit users are most
likely to express regret for past actions, particularly in the domain of
relationships. We also found that deep learning models using GloVe embedding
outperformed other models in all experiments, indicating the effectiveness of
GloVe for representing the meaning and context of words in the domain of
regret. Overall, our study provides valuable insights into the nature and
prevalence of regret on social media, as well as the potential of deep learning
and word embeddings for analyzing and understanding emotional language in
online text. These findings have implications for the development of natural
language processing algorithms and the design of social media platforms that
support emotional expression and communication."
61,61.0,"Section 6 discusses the outcomes and discussions of the experimental
results.","Finally, Section 7 concludes the article by summarizing the key contributions and discussing directions
of further research.","2 Related Work

Monte-Carlo tree-search has ﬁrst been developed for algorithms to play board games [9].",2022-12-14 23:01:53+00:00,Monte-Carlo Tree-Search for Leveraging Performance of Blackbox Job-Shop Scheduling Heuristics,cs.AI,"['cs.AI', 'math.OC', 'I.2.8; F.2.2']","[arxiv.Result.Author('Florian Wimmenauer'), arxiv.Result.Author('Matúš Mihalák'), arxiv.Result.Author('Mark H. M. Winands')]","In manufacturing, the production is often done on out-of-the-shelf
manufacturing lines, whose underlying scheduling heuristics are not known due
to the intellectual property. We consider such a setting with a black-box
job-shop system and an unknown scheduling heuristic that, for a given
permutation of jobs, schedules the jobs for the black-box job-shop with the
goal of minimizing the makespan. Here, the jobs need to enter the job-shop in
the given order of the permutation, but may take different paths within the job
shop, which depends on the black-box heuristic. The performance of the
black-box heuristic depends on the order of the jobs, and the natural problem
for the manufacturer is to find an optimum ordering of the jobs.
  Facing a real-world scenario as described above, we engineer the Monte-Carlo
tree-search for finding a close-to-optimum ordering of jobs. To cope with a
large solutions-space in planning scenarios, a hierarchical Monte-Carlo tree
search (H-MCTS) is proposed based on abstraction of jobs. On synthetic and
real-life problems, H-MCTS with integrated abstraction significantly
outperforms pure heuristic-based techniques as well as other Monte-Carlo search
variants. We furthermore show that, by modifying the evaluation metric in
H-MCTS, it is possible to achieve other optimization objectives than what the
scheduling heuristics are designed for -- e.g., minimizing the total completion
time instead of the makespan. Our experimental observations have been also
validated in real-life cases, and our H-MCTS approach has been implemented in a
production plant's controller."
62,62.0,"How-
ever, on the uniformly random generated Demirkol instances, the integrated abstraction did not show perfor-
mance improvement due to the lack of structure in the jobs.","Further research could extend the current work by
handling problems with less structure.","Additionally, further improving the performance of the MCTS for the planning problem at hand could result
in better plans.",2022-12-14 23:01:53+00:00,Monte-Carlo Tree-Search for Leveraging Performance of Blackbox Job-Shop Scheduling Heuristics,cs.AI,"['cs.AI', 'math.OC', 'I.2.8; F.2.2']","[arxiv.Result.Author('Florian Wimmenauer'), arxiv.Result.Author('Matúš Mihalák'), arxiv.Result.Author('Mark H. M. Winands')]","In manufacturing, the production is often done on out-of-the-shelf
manufacturing lines, whose underlying scheduling heuristics are not known due
to the intellectual property. We consider such a setting with a black-box
job-shop system and an unknown scheduling heuristic that, for a given
permutation of jobs, schedules the jobs for the black-box job-shop with the
goal of minimizing the makespan. Here, the jobs need to enter the job-shop in
the given order of the permutation, but may take different paths within the job
shop, which depends on the black-box heuristic. The performance of the
black-box heuristic depends on the order of the jobs, and the natural problem
for the manufacturer is to find an optimum ordering of the jobs.
  Facing a real-world scenario as described above, we engineer the Monte-Carlo
tree-search for finding a close-to-optimum ordering of jobs. To cope with a
large solutions-space in planning scenarios, a hierarchical Monte-Carlo tree
search (H-MCTS) is proposed based on abstraction of jobs. On synthetic and
real-life problems, H-MCTS with integrated abstraction significantly
outperforms pure heuristic-based techniques as well as other Monte-Carlo search
variants. We furthermore show that, by modifying the evaluation metric in
H-MCTS, it is possible to achieve other optimization objectives than what the
scheduling heuristics are designed for -- e.g., minimizing the total completion
time instead of the makespan. Our experimental observations have been also
validated in real-life cases, and our H-MCTS approach has been implemented in a
production plant's controller."
63,63.0,"Deepfake attack detection has been the subject of numerous
studies.","Nevertheless, further research and improvements are still needed.","Develop the Online Exam Proctoring Framework We oﬀered an automated online exam proctoring framework
in this paper which, to the best of our knowledge, is ﬁrst to analyse the heart rate of the exam candidate using

presentation attack detection approaches.",2022-12-14 00:30:09+00:00,A Novel Active Solution for Two-Dimensional Face Presentation Attack Detection,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Matineh Pooshideh')],"Identity authentication is the process of verifying one's identity. There are
several identity authentication methods, among which biometric authentication
is of utmost importance. Facial recognition is a sort of biometric
authentication with various applications, such as unlocking mobile phones and
accessing bank accounts. However, presentation attacks pose the greatest threat
to facial recognition. A presentation attack is an attempt to present a
non-live face, such as a photo, video, mask, and makeup, to the camera.
Presentation attack detection is a countermeasure that attempts to identify
between a genuine user and a presentation attack. Several industries, such as
financial services, healthcare, and education, use biometric authentication
services on various devices. This illustrates the significance of presentation
attack detection as the verification step. In this paper, we study
state-of-the-art to cover the challenges and solutions related to presentation
attack detection in a single place. We identify and classify different
presentation attack types and identify the state-of-the-art methods that could
be used to detect each of them. We compare the state-of-the-art literature
regarding attack types, evaluation metrics, accuracy, and datasets and discuss
research and industry challenges of presentation attack detection. Most
presentation attack detection approaches rely on extensive data training and
quality, making them difficult to implement. We introduce an efficient active
presentation attack detection approach that overcomes weaknesses in the
existing literature. The proposed approach does not require training data, is
CPU-light, can process low-quality images, has been tested with users of
various ages and is shown to be user-friendly and highly robust to
2-dimensional presentation attacks."
64,64.0,"Similar studies could be made for comparing deterministic and nature-inspired methods on problems with general

or hidden constraints.",This will be the subject of our further research.,"Acknowledgements

This work was supported by the Grant Agency of the Czech Republic project no.",2022-12-13 19:44:24+00:00,Are metaheuristics worth it? A computational comparison between nature-inspired and deterministic techniques on black-box optimization problems,cs.NE,"['cs.NE', 'cs.AI', 'math.OC']",[arxiv.Result.Author('Jakub Kudela')],"In the field of derivative-free optimization, both of its main branches, the
deterministic and nature-inspired techniques, experienced in recent years
substantial advancement. In this paper, we provide an extensive computational
comparison of selected methods from each of these branches. The chosen
representatives were either standard and well-utilized methods, or the
best-performing methods from recent numerical comparisons. The computational
comparison was performed on five different benchmark sets and the results were
analyzed in terms of performance, time complexity, and convergence properties
of the selected methods. The results showed that, when dealing with situations
where the objective function evaluations are relatively cheap, the
nature-inspired methods have a significantly better performance than their
deterministic counterparts. However, in situations when the function
evaluations are costly or otherwise prohibited, the deterministic methods might
provide more consistent and overall better results."
65,65.0,"Discussion 

We present a novel deep-learning generative model of entire patient timelines within secondary care across 
mental and physical health, incorporating interoperable disease, procedure, medication and symptom concepts.","Foresight allows A/B testing of historical events, helpful for purposes of further research into historical real-
world data through emulation of virtual trials, real-world risk estimation, analysis of bias, as well as synthetic 
data generation.","Foresight allows simulation of a synthetic patient from single time-steps during a time-constrained inpatient 
episode all the way to a multi-year timeline of chronic conditions.",2022-12-13 19:06:00+00:00,Foresight -- Deep Generative Modelling of Patient Timelines using Electronic Health Records,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zeljko Kraljevic'), arxiv.Result.Author('Dan Bean'), arxiv.Result.Author('Anthony Shek'), arxiv.Result.Author('Rebecca Bendayan'), arxiv.Result.Author('Joshua Au Yeung'), arxiv.Result.Author('Alexander Deng'), arxiv.Result.Author('Alfie Baston'), arxiv.Result.Author('Jack Ross'), arxiv.Result.Author('Esther Idowu'), arxiv.Result.Author('James T Teo'), arxiv.Result.Author('Richard J Dobson')]","Electronic Health Records (EHRs) hold detailed longitudinal information about
each patient's health status and general clinical history, a large portion of
which is stored within the unstructured text. Temporal modelling of this
medical history, which considers the sequence of events, can be used to
forecast and simulate future events, estimate risk, suggest alternative
diagnoses or forecast complications. While most prediction approaches use
mainly structured data or a subset of single-domain forecasts and outcomes, we
processed the entire free-text portion of EHRs for longitudinal modelling. We
present Foresight, a novel GPT3-based pipeline that uses NER+L tools (i.e.
MedCAT) to convert document text into structured, coded concepts, followed by
providing probabilistic forecasts for future medical events such as disorders,
medications, symptoms and interventions. Since large portions of EHR data are
in text form, such an approach benefits from a granular and detailed view of a
patient while introducing modest additional noise. On tests in two large UK
hospitals (King's College Hospital, South London and Maudsley) and the US
MIMIC-III dataset precision@10 of 0.80, 0.81 and 0.91 was achieved for
forecasting the next biomedical concept. Foresight was also validated on 34
synthetic patient timelines by 5 clinicians and achieved relevancy of 97% for
the top forecasted candidate disorder. Foresight can be easily trained and
deployed locally as it only requires free-text data (as a minimum). As a
generative model, it can simulate follow-on disorders, medications and
interventions for as many steps as required. Foresight is a general-purpose
model for biomedical concept modelling that can be used for real-world risk
estimation, virtual trials and clinical research to study the progression of
diseases, simulate interventions and counterfactuals, and for educational
purposes."
66,66.0,theory can be pursued.,"Since this initially theoretical ﬁnding has also been con-
ﬁrmed in our synthetic data example, a next natural step for further research is
applications of our approach to real data situations.","Since for larger applications
also very large linear programs arise when checking the proposed dominance
criterion, it should also be explored to what extent the constraint sets of the lin-
ear programs can still be purged of redundancies (e.g., by explicitly exploiting
transitivity) or to what extent the optimal values can be approximated by less
complex linear programs.",2022-12-13 11:47:02+00:00,Multi-Target Decision Making under Conditions of Severe Uncertainty,cs.AI,"['cs.AI', 'econ.TH', 'stat.ME', '91-10', 'G.3']","[arxiv.Result.Author('Christoph Jansen'), arxiv.Result.Author('Georg Schollmeyer'), arxiv.Result.Author('Thomas Augustin')]","The quality of consequences in a decision making problem under (severe)
uncertainty must often be compared among different targets (goals, objectives)
simultaneously. In addition, the evaluations of a consequence's performance
under the various targets often differ in their scale of measurement,
classically being either purely ordinal or perfectly cardinal. In this paper,
we transfer recent developments from abstract decision theory with incomplete
preferential and probabilistic information to this multi-target setting and
show how -- by exploiting the (potentially) partial cardinal and partial
probabilistic information -- more informative orders for comparing decisions
can be given than the Pareto order. We discuss some interesting properties of
the proposed orders between decision options and show how they can be
concretely computed by linear optimization. We conclude the paper by
demonstrating our framework in an artificial (but quite real-world) example in
the context of comparing algorithms under different performance measures."
67,67.0,"Note that
our method can be generalized to transfer-based few-shot
learners with any distance-based classiﬁer.","Although we
improve upon existing methods, we think that it opens up
interesting new directions for further research.","References

[Bateni et al., 2022] Bateni, P., Barber, J., van de Meent,
J.-W., and Wood, F. (2022).",2022-12-13 10:21:15+00:00,A Statistical Model for Predicting Generalization in Few-Shot Classification,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Yassir Bendou'), arxiv.Result.Author('Vincent Gripon'), arxiv.Result.Author('Bastien Pasdeloup'), arxiv.Result.Author('Lukas Mauch'), arxiv.Result.Author('Stefan Uhlich'), arxiv.Result.Author('Fabien Cardinaux'), arxiv.Result.Author('Ghouthi Boukli Hacene'), arxiv.Result.Author('Javier Alonso Garcia')]","The estimation of the generalization error of classifiers often relies on a
validation set. Such a set is hardly available in few-shot learning scenarios,
a highly disregarded shortcoming in the field. In these scenarios, it is common
to rely on features extracted from pre-trained neural networks combined with
distance-based classifiers such as nearest class mean. In this work, we
introduce a Gaussian model of the feature distribution. By estimating the
parameters of this model, we are able to predict the generalization error on
new classification tasks with few samples. We observe that accurate distance
estimates between class-conditional densities are the key to accurate estimates
of the generalization performance. Therefore, we propose an unbiased estimator
for these distances and integrate it in our numerical analysis. We show that
our approach outperforms alternatives such as the leave-one-out
cross-validation strategy in few-shot settings."
68,68.0,"However, collecting and labeling new data for an existing system
with pre-trained ML/DL models could be both hard and time-consuming.","Therefore, towards addressing these issues and enabling further research on
AI systems, in this paper, we propose an exploratory study of AI system risk
assessment.","As an early attempt, this paper performs analysis and risk assess-
ment of an AI system through characterizing data distribution and uncertainty
inside the system.",2022-12-13 03:34:25+00:00,An Exploratory Study of AI System Risk Assessment from the Lens of Data Distribution and Uncertainty,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Zhijie Wang'), arxiv.Result.Author('Yuheng Huang'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Haruki Yokoyama'), arxiv.Result.Author('Susumu Tokumoto'), arxiv.Result.Author('Kazuki Munakata')]","Deep learning (DL) has become a driving force and has been widely adopted in
many domains and applications with competitive performance. In practice, to
solve the nontrivial and complicated tasks in real-world applications, DL is
often not used standalone, but instead contributes as a piece of gadget of a
larger complex AI system. Although there comes a fast increasing trend to study
the quality issues of deep neural networks (DNNs) at the model level, few
studies have been performed to investigate the quality of DNNs at both the unit
level and the potential impacts on the system level. More importantly, it also
lacks systematic investigation on how to perform the risk assessment for AI
systems from unit level to system level. To bridge this gap, this paper
initiates an early exploratory study of AI system risk assessment from both the
data distribution and uncertainty angles to address these issues. We propose a
general framework with an exploratory study for analyzing AI systems. After
large-scale (700+ experimental configurations and 5000+ GPU hours) experiments
and in-depth investigations, we reached a few key interesting findings that
highlight the practical need and opportunities for more in-depth investigations
into AI systems."
69,69.0,"In the depicted example, editing an image of a king
by guiding it away from the concept ‘male’ and towards the
concept ‘female’ produces a compositionally similar image
but replaces the king with a queen.","This indicates that the
latent space of diffusion models may inherently be disen-

2https://huggingface.co/runwayml/stable-diffusion-v1-5

5

tangled to some extent, although further research in that di-
rection is necessary to verify our assumption.","Furthermore,

the Stable Artist can simultaneously
change multiple aspects of an image with minimal inter-
ference of different concepts, as shown in Fig.",2022-12-12 16:21:24+00:00,The Stable Artist: Steering Semantics in Diffusion Latent Space,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Manuel Brack'), arxiv.Result.Author('Patrick Schramowski'), arxiv.Result.Author('Felix Friedrich'), arxiv.Result.Author('Dominik Hintersdorf'), arxiv.Result.Author('Kristian Kersting')]","Large, text-conditioned generative diffusion models have recently gained a
lot of attention for their impressive performance in generating high-fidelity
images from text alone. However, achieving high-quality results is almost
unfeasible in a one-shot fashion. On the contrary, text-guided image generation
involves the user making many slight changes to inputs in order to iteratively
carve out the envisioned image. However, slight changes to the input prompt
often lead to entirely different images being generated, and thus the control
of the artist is limited in its granularity. To provide flexibility, we present
the Stable Artist, an image editing approach enabling fine-grained control of
the image generation process. The main component is semantic guidance (SEGA)
which steers the diffusion process along variable numbers of semantic
directions. This allows for subtle edits to images, changes in composition and
style, as well as optimization of the overall artistic conception. Furthermore,
SEGA enables probing of latent spaces to gain insights into the representation
of concepts learned by the model, even complex ones such as 'carbon emission'.
We demonstrate the Stable Artist on several tasks, showcasing high-quality
image editing and composition."
70,70.0,"fairness, by
detecting and steering certain related concepts.","Therefore,
we advocate for further research in this direction.","Another frequently voiced point of criticism, is the no-
tion that generative models like stable diffusion are replac-
ing human artists and illustrators.",2022-12-12 16:21:24+00:00,The Stable Artist: Steering Semantics in Diffusion Latent Space,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Manuel Brack'), arxiv.Result.Author('Patrick Schramowski'), arxiv.Result.Author('Felix Friedrich'), arxiv.Result.Author('Dominik Hintersdorf'), arxiv.Result.Author('Kristian Kersting')]","Large, text-conditioned generative diffusion models have recently gained a
lot of attention for their impressive performance in generating high-fidelity
images from text alone. However, achieving high-quality results is almost
unfeasible in a one-shot fashion. On the contrary, text-guided image generation
involves the user making many slight changes to inputs in order to iteratively
carve out the envisioned image. However, slight changes to the input prompt
often lead to entirely different images being generated, and thus the control
of the artist is limited in its granularity. To provide flexibility, we present
the Stable Artist, an image editing approach enabling fine-grained control of
the image generation process. The main component is semantic guidance (SEGA)
which steers the diffusion process along variable numbers of semantic
directions. This allows for subtle edits to images, changes in composition and
style, as well as optimization of the overall artistic conception. Furthermore,
SEGA enables probing of latent spaces to gain insights into the representation
of concepts learned by the model, even complex ones such as 'carbon emission'.
We demonstrate the Stable Artist on several tasks, showcasing high-quality
image editing and composition."
71,71.0,"9 CONCLUSION
9.1 Future Direction

Instrumental Variable has been an attractive research topic
for a long time as it provides an effective way to uncover

causal relationships in real-world problems.","In this section,
we point several lines for further research.",9.1.1 How to ﬁnd a valid IV?,2022-12-12 08:59:04+00:00,Instrumental Variables in Causal Inference and Machine Learning: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'stat.ME']","[arxiv.Result.Author('Anpeng Wu'), arxiv.Result.Author('Kun Kuang'), arxiv.Result.Author('Ruoxuan Xiong'), arxiv.Result.Author('Fei Wu')]","Causal inference is the process of using assumptions, study designs, and
estimation strategies to draw conclusions about the causal relationships
between variables based on data. This allows researchers to better understand
the underlying mechanisms at work in complex systems and make more informed
decisions. In many settings, we may not fully observe all the confounders that
affect both the treatment and outcome variables, complicating the estimation of
causal effects. To address this problem, a growing literature in both causal
inference and machine learning proposes to use Instrumental Variables (IV).
This paper serves as the first effort to systematically and comprehensively
introduce and discuss the IV methods and their applications in both causal
inference and machine learning. First, we provide the formal definition of IVs
and discuss the identification problem of IV regression methods under different
assumptions. Second, we categorize the existing work on IV methods into three
streams according to the focus on the proposed methods, including two-stage
least squares with IVs, control function with IVs, and evaluation of IVs. For
each stream, we present both the classical causal inference methods, and recent
developments in the machine learning literature. Then, we introduce a variety
of applications of IV methods in real-world scenarios and provide a summary of
the available datasets and algorithms. Finally, we summarize the literature,
discuss the open problems and suggest promising future research directions for
IV methods and their applications. We also develop a toolkit of IVs methods
reviewed in this survey at https://github.com/causal-machine-learning-lab/mliv."
72,72.0,"ducted by the ACT on evaluating the feasibility of a neuromorphic approach
for onboard AI applications (Section 5).","We hope that this chapter will
stimulate further research pursuing a neuromorphic approach to spacecraft
onboard computation and sensing.","2 Spiking neural networks

Arguably, the feature which is found most often in modern neuromorphic
algorithms is spike-based communication.",2022-12-10 07:46:29+00:00,Neuromorphic Computing and Sensing in Space,cs.NE,"['cs.NE', 'cs.AI']","[arxiv.Result.Author('Dario Izzo'), arxiv.Result.Author('Alexander Hadjiivanov'), arxiv.Result.Author('Dominik Dold'), arxiv.Result.Author('Gabriele Meoni'), arxiv.Result.Author('Emmanuel Blazquez')]","The term ``neuromorphic'' refers to systems that are closely resembling the
architecture and/or the dynamics of biological neural networks. Typical
examples are novel computer chips designed to mimic the architecture of a
biological brain, or sensors that get inspiration from, e.g., the visual or
olfactory systems in insects and mammals to acquire information about the
environment. This approach is not without ambition as it promises to enable
engineered devices able to reproduce the level of performance observed in
biological organisms -- the main immediate advantage being the efficient use of
scarce resources, which translates into low power requirements. The emphasis on
low power and energy efficiency of neuromorphic devices is a perfect match for
space applications. Spacecraft -- especially miniaturized ones -- have strict
energy constraints as they need to operate in an environment which is scarce
with resources and extremely hostile. In this work we present an overview of
early attempts made to study a neuromorphic approach in a space context at the
European Space Agency's (ESA) Advanced Concepts Team (ACT)."
73,73.0,"Finally, we conclude in Section 7.","To foster further research, benchmarks and experimental data

are released on our website [87].","QVIP: A Formal Verification Approach for QNNs

ASE ’22, October 10–14, 2022, Rochester, MI, USA

outputs of each layer are quantized into integers, depending on the
given quantization configurations.",2022-12-10 03:00:29+00:00,QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks,cs.CR,"['cs.CR', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Yedi Zhang'), arxiv.Result.Author('Zhe Zhao'), arxiv.Result.Author('Fu Song'), arxiv.Result.Author('Min Zhang'), arxiv.Result.Author('Taolue Chen'), arxiv.Result.Author('Jun Sun')]","Deep learning has become a promising programming paradigm in software
development, owing to its surprising performance in solving many challenging
tasks. Deep neural networks (DNNs) are increasingly being deployed in practice,
but are limited on resource-constrained devices owing to their demand for
computational power. Quantization has emerged as a promising technique to
reduce the size of DNNs with comparable accuracy as their floating-point
numbered counterparts. The resulting quantized neural networks (QNNs) can be
implemented energy-efficiently. Similar to their floating-point numbered
counterparts, quality assurance techniques for QNNs, such as testing and formal
verification, are essential but are currently less explored. In this work, we
propose a novel and efficient formal verification approach for QNNs. In
particular, we are the first to propose an encoding that reduces the
verification problem of QNNs into the solving of integer linear constraints,
which can be solved using off-the-shelf solvers. Our encoding is both sound and
complete. We demonstrate the application of our approach on local robustness
verification and maximum robustness radius computation. We implement our
approach in a prototype tool QVIP and conduct a thorough evaluation.
Experimental results on QNNs with different quantization bits confirm the
effectiveness and efficiency of our approach, e.g., two orders of magnitude
faster and able to solve more verification tasks in the same time limit than
the state-of-the-art methods."
74,74.0,"Additionally, there is scope for exploring
several optimizations in the mini-batch sampling strategy and in the
GPU communications.","The use of 2D and 3D partitioning schemes
is another promising avenue for further research.","Acknowledgments
Aparajita is supported by the Feuer International Scholarship in
Artificial Intelligence.",2022-12-09 17:51:13+00:00,Scalable Graph Convolutional Network Training on Distributed-Memory Systems,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Gunduz Vehbi Demirci'), arxiv.Result.Author('Aparajita Haldar'), arxiv.Result.Author('Hakan Ferhatosmanoglu')]","Graph Convolutional Networks (GCNs) are extensively utilized for deep
learning on graphs. The large data sizes of graphs and their vertex features
make scalable training algorithms and distributed memory systems necessary.
Since the convolution operation on graphs induces irregular memory access
patterns, designing a memory- and communication-efficient parallel algorithm
for GCN training poses unique challenges. We propose a highly parallel training
algorithm that scales to large processor counts. In our solution, the large
adjacency and vertex-feature matrices are partitioned among processors. We
exploit the vertex-partitioning of the graph to use non-blocking point-to-point
communication operations between processors for better scalability. To further
minimize the parallelization overheads, we introduce a sparse matrix
partitioning scheme based on a hypergraph partitioning model for full-batch
training. We also propose a novel stochastic hypergraph model to encode the
expected communication volume in mini-batch training. We show the merits of the
hypergraph model, previously unexplored for GCN training, over the standard
graph partitioning model which does not accurately encode the communication
costs. Experiments performed on real-world graph datasets demonstrate that the
proposed algorithms achieve considerable speedups over alternative solutions.
The optimizations achieved on communication costs become even more pronounced
at high scalability with many processors. The performance benefits are
preserved in deeper GCNs having more layers as well as on billion-scale graphs."
75,75.0,"Experiments with multiple biases in
different environments, as well as an analysis of the true human policy (which potentially suffers
from unknown, yet to be characterized suboptimalities), reassuringly show remarkably consistent
results: over and over again, we see that as the human model and the true human behavior are more
and more aligned, the reward error decreases.","Overall, our results convey the optimistic message
that reward learning improves as we obtain better human models, and motivate further research into
improved models.",Limitations and future work.,2022-12-09 08:16:20+00:00,On the Sensitivity of Reward Inference to Misspecified Human Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Joey Hong'), arxiv.Result.Author('Kush Bhatia'), arxiv.Result.Author('Anca Dragan')]","Inferring reward functions from human behavior is at the center of value
alignment - aligning AI objectives with what we, humans, actually want. But
doing so relies on models of how humans behave given their objectives. After
decades of research in cognitive science, neuroscience, and behavioral
economics, obtaining accurate human models remains an open research topic. This
begs the question: how accurate do these models need to be in order for the
reward inference to be accurate? On the one hand, if small errors in the model
can lead to catastrophic error in inference, the entire framework of reward
learning seems ill-fated, as we will never have perfect models of human
behavior. On the other hand, if as our models improve, we can have a guarantee
that reward accuracy also improves, this would show the benefit of more work on
the modeling side. We study this question both theoretically and empirically.
We do show that it is unfortunately possible to construct small adversarial
biases in behavior that lead to arbitrarily large errors in the inferred
reward. However, and arguably more importantly, we are also able to identify
reasonable assumptions under which the reward inference error can be bounded
linearly in the error in the human model. Finally, we verify our theoretical
insights in discrete and continuous control tasks with simulated and human
data."
76,76.0,"Also a direct
combination of two models makes the model architecture
differ signiﬁcantly from the standard ones, rendering the
attack easy to detect [18].","Further research is needed to
understand whether other more effective poisoning attacks
exist to pose a credible threat to our approach.",Future work.,2022-12-09 06:29:43+00:00,"Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Rui Zhu'), arxiv.Result.Author('Di Tang'), arxiv.Result.Author('Siyuan Tang'), arxiv.Result.Author('XiaoFeng Wang'), arxiv.Result.Author('Haixu Tang')]","In this paper, we present a simple yet surprisingly effective technique to
induce ""selective amnesia"" on a backdoored model. Our approach, called SEAM,
has been inspired by the problem of catastrophic forgetting (CF), a long
standing issue in continual learning. Our idea is to retrain a given DNN model
on randomly labeled clean data, to induce a CF on the model, leading to a
sudden forget on both primary and backdoor tasks; then we recover the primary
task by retraining the randomized model on correctly labeled clean data. We
analyzed SEAM by modeling the unlearning process as continual learning and
further approximating a DNN using Neural Tangent Kernel for measuring CF. Our
analysis shows that our random-labeling approach actually maximizes the CF on
an unknown backdoor in the absence of triggered inputs, and also preserves some
feature extraction in the network to enable a fast revival of the primary task.
We further evaluated SEAM on both image processing and Natural Language
Processing tasks, under both data contamination and training manipulation
attacks, over thousands of models either trained on popular image datasets or
provided by the TrojAI competition. Our experiments show that SEAM vastly
outperforms the state-of-the-art unlearning techniques, achieving a high
Fidelity (measuring the gap between the accuracy of the primary task and that
of the backdoor) within a few minutes (about 30 times faster than training a
model from scratch using the MNIST dataset), with only a small amount of clean
data (0.1% of training data for TrojAI models)."
77,77.0,"Essentially, the forgetting step could
be viewed as an attempt to ﬁnd a good initialization point
for learning the primary task.","This implies that a good ini-
tialization may help reduce the ASR of a certain backdoor,
which is an open problem for further research.",8.,2022-12-09 06:29:43+00:00,"Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Rui Zhu'), arxiv.Result.Author('Di Tang'), arxiv.Result.Author('Siyuan Tang'), arxiv.Result.Author('XiaoFeng Wang'), arxiv.Result.Author('Haixu Tang')]","In this paper, we present a simple yet surprisingly effective technique to
induce ""selective amnesia"" on a backdoored model. Our approach, called SEAM,
has been inspired by the problem of catastrophic forgetting (CF), a long
standing issue in continual learning. Our idea is to retrain a given DNN model
on randomly labeled clean data, to induce a CF on the model, leading to a
sudden forget on both primary and backdoor tasks; then we recover the primary
task by retraining the randomized model on correctly labeled clean data. We
analyzed SEAM by modeling the unlearning process as continual learning and
further approximating a DNN using Neural Tangent Kernel for measuring CF. Our
analysis shows that our random-labeling approach actually maximizes the CF on
an unknown backdoor in the absence of triggered inputs, and also preserves some
feature extraction in the network to enable a fast revival of the primary task.
We further evaluated SEAM on both image processing and Natural Language
Processing tasks, under both data contamination and training manipulation
attacks, over thousands of models either trained on popular image datasets or
provided by the TrojAI competition. Our experiments show that SEAM vastly
outperforms the state-of-the-art unlearning techniques, achieving a high
Fidelity (measuring the gap between the accuracy of the primary task and that
of the backdoor) within a few minutes (about 30 times faster than training a
model from scratch using the MNIST dataset), with only a small amount of clean
data (0.1% of training data for TrojAI models)."
78,78.0,"However, the use 
of AI in developing new bioactive compounds is not without challenges and limitations.","Ethical 

 
 
 
 
considerations must be taken into account, and further research is needed to fully understand 
the advantages and limitations of AI in this area7.","Despite these challenges, AI is expected to 
significantly contribute to the development of new medications and therapies in the following 
few years.",2022-12-08 23:23:39+00:00,"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",cs.CL,"['cs.CL', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Alexandre Blanco-Gonzalez'), arxiv.Result.Author('Alfonso Cabezon'), arxiv.Result.Author('Alejandro Seco-Gonzalez'), arxiv.Result.Author('Daniel Conde-Torres'), arxiv.Result.Author('Paula Antelo-Riveiro'), arxiv.Result.Author('Angel Pineiro'), arxiv.Result.Author('Rebeca Garcia-Fandino')]","Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section."
79,79.0,"2 Proposed Methodology

In this section, we begin formally deﬁning integrated information and the
problem of optimizing integrated information in the space of transition
probability matrices.","Then, we show how this problem cannot be solved
with common metaheuristics or techniques such as vanilla Bayesian op-
timization [11] and introduce a random search algorithm to solve the
problem that can be used as a baseline for further research.","2.1 Integrated information

Throughout this paper, we use the deﬁnitions of integrated information
employed in [17].",2022-12-08 22:34:00+00:00,Optimizing Integrated Information with a Prior Guided Random Search Algorithm,cs.AI,['cs.AI'],"[arxiv.Result.Author('Eduardo C. Garrido-Merchán'), arxiv.Result.Author('Javier Sánchez-Cañizares')]","Integrated information theory (IIT) is a theoretical framework that provides
a quantitative measure to estimate when a physical system is conscious, its
degree of consciousness, and the complexity of the qualia space that the system
is experiencing. Formally, IIT rests on the assumption that if a surrogate
physical system can fully embed the phenomenological properties of
consciousness, then the system properties must be constrained by the properties
of the qualia being experienced. Following this assumption, IIT represents the
physical system as a network of interconnected elements that can be thought of
as a probabilistic causal graph, $\mathcal{G}$, where each node has an
input-output function and all the graph is encoded in a transition probability
matrix. Consequently, IIT's quantitative measure of consciousness, $\Phi$, is
computed with respect to the transition probability matrix and the present
state of the graph. In this paper, we provide a random search algorithm that is
able to optimize $\Phi$ in order to investigate, as the number of nodes
increases, the structure of the graphs that have higher $\Phi$. We also provide
arguments that show the difficulties of applying more complex black-box search
algorithms, such as Bayesian optimization or metaheuristics, in this particular
problem. Additionally, we suggest specific research lines for these techniques
to enhance the search algorithm that guarantees maximal $\Phi$."
80,80.0,"In order to better evaluate the security of object detection
models in auto-driving system, this competition simulated
the adversarial attack in autonomous driving environments
for contestants to make experiments.","We aim to attract more
and more people to focus on the security of auto-driving
system and make some meaningful validation experiments
with contestants for further research on adversarial attack in
auto-driving ﬁeld in real world.",3.1.,2022-12-07 02:45:27+00:00,Artificial Intelligence Security Competition (AISC),cs.CR,"['cs.CR', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Peng Chen'), arxiv.Result.Author('Senyou Deng'), arxiv.Result.Author('Lianji L'), arxiv.Result.Author('Yi Sun'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Jiaxing Li'), arxiv.Result.Author('Yunteng Tan'), arxiv.Result.Author('Xinyu Liu'), arxiv.Result.Author('Yangyi Dong'), arxiv.Result.Author('Enhui Xu'), arxiv.Result.Author('Jincai Xu'), arxiv.Result.Author('Shu Xu'), arxiv.Result.Author('Xuelin Fu'), arxiv.Result.Author('Changfeng Sun'), arxiv.Result.Author('Haoliang Han'), arxiv.Result.Author('Xuchong Zhang'), arxiv.Result.Author('Shen Chen'), arxiv.Result.Author('Zhimin Sun'), arxiv.Result.Author('Junyi Cao'), arxiv.Result.Author('Taiping Yao'), arxiv.Result.Author('Shouhong Ding'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Jian Lin'), arxiv.Result.Author('Tianpeng Wu'), arxiv.Result.Author('Ye Wang'), arxiv.Result.Author('Yu Fu'), arxiv.Result.Author('Lin Feng'), arxiv.Result.Author('Kangkang Gao'), arxiv.Result.Author('Zeyu Liu'), arxiv.Result.Author('Yuanzhe Pang'), arxiv.Result.Author('Chengqi Duan'), arxiv.Result.Author('Huipeng Zhou'), arxiv.Result.Author('Yajie Wang'), arxiv.Result.Author('Yuhang Zhao'), arxiv.Result.Author('Shangbo Wu'), arxiv.Result.Author('Haoran Lyu'), arxiv.Result.Author('Zhiyu Lin'), arxiv.Result.Author('Yifei Gao'), arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Haonan Wang'), arxiv.Result.Author('Jitao Sang'), arxiv.Result.Author('Chen Ma'), arxiv.Result.Author('Junhao Zheng'), arxiv.Result.Author('Yijia Li'), arxiv.Result.Author('Chao Shen'), arxiv.Result.Author('Chenhao Lin'), arxiv.Result.Author('Zhichao Cui'), arxiv.Result.Author('Guoshuai Liu'), arxiv.Result.Author('Huafeng Shi'), arxiv.Result.Author('Kun Hu'), arxiv.Result.Author('Mengxin Zhang')]","The security of artificial intelligence (AI) is an important research area
towards safe, reliable, and trustworthy AI systems. To accelerate the research
on AI security, the Artificial Intelligence Security Competition (AISC) was
organized by the Zhongguancun Laboratory, China Industrial Control Systems
Cyber Emergency Response Team, Institute for Artificial Intelligence, Tsinghua
University, and RealAI as part of the Zhongguancun International Frontier
Technology Innovation Competition (https://www.zgc-aisc.com/en). The
competition consists of three tracks, including Deepfake Security Competition,
Autonomous Driving Security Competition, and Face Recognition Security
Competition. This report will introduce the competition rules of these three
tracks and the solutions of top-ranking teams in each track."
81,81.0,"Next, we will present our method in Section
3, then show our experimental results in Section 4.","Finally, we will summarize
our work and discuss some possible further research in Section 5.","2 Background

2.1 Gaussian-Bernoulli Restricted Boltzmann Machine

A Gaussian-Bernoulli restricted Boltzmann Machine (GB-RBM) [18] is deﬁned
on a complete bipartite graph as shown in Figure 1.",2022-12-07 00:01:20+00:00,Learning State Transition Rules from Hidden Layers of Restricted Boltzmann Machines,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Koji Watanabe'), arxiv.Result.Author('Katsumi Inoue')]","Understanding the dynamics of a system is important in many scientific and
engineering domains. This problem can be approached by learning state
transition rules from observations using machine learning techniques. Such
observed time-series data often consist of sequences of many continuous
variables with noise and ambiguity, but we often need rules of dynamics that
can be modeled with a few essential variables. In this work, we propose a
method for extracting a small number of essential hidden variables from
high-dimensional time-series data and for learning state transition rules
between these hidden variables. The proposed method is based on the Restricted
Boltzmann Machine (RBM), which treats observable data in the visible layer and
latent features in the hidden layer. However, real-world data, such as video
and audio, include both discrete and continuous variables, and these variables
have temporal relationships. Therefore, we propose Recurrent Temporal
GaussianBernoulli Restricted Boltzmann Machine (RTGB-RBM), which combines
Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM) to handle continuous
visible variables, and Recurrent Temporal Restricted Boltzmann Machine (RT-RBM)
to capture time dependence between discrete hidden variables. We also propose a
rule-based method that extracts essential information as hidden variables and
represents state transition rules in interpretable form. We conduct experiments
on Bouncing Ball and Moving MNIST datasets to evaluate our proposed method.
Experimental results show that our method can learn the dynamics of those
physical systems as state transition rules between hidden variables and can
predict unobserved future states from observed state transitions."
82,82.0,"trolley problem)
[119], [120], [121], [122].","Given the far-reaching importance
of AI ethical decision making to its social acceptance [123],
further research on this topic is necessary.","Second, due to
the capacity of people the event could hold, the number of
participants in the current study was limited (68, 68 and 65
effective observations in the ﬁrst stage, second stage and
third stage, respectively).",2022-12-06 12:06:34+00:00,Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling,cs.HC,"['cs.HC', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Zhaoning Li'), arxiv.Result.Author('Qiaoli Jiang'), arxiv.Result.Author('Zhengming Wu'), arxiv.Result.Author('Anqi Liu'), arxiv.Result.Author('Haiyan Wu'), arxiv.Result.Author('Miner Huang'), arxiv.Result.Author('Kai Huang'), arxiv.Result.Author('Yixuan Ku')]","Autonomous cars are indispensable when humans go further down the hands-free
route. Although existing literature highlights that the acceptance of the
autonomous car will increase if it drives in a human-like manner, sparse
research offers the naturalistic experience from a passenger's seat perspective
to examine the human likeness of current autonomous cars. The present study
tested whether the AI driver could create a human-like ride experience for
passengers based on 69 participants' feedback in a real-road scenario. We
designed a ride experience-based version of the non-verbal Turing test for
automated driving. Participants rode in autonomous cars (driven by either human
or AI drivers) as a passenger and judged whether the driver was human or AI.
The AI driver failed to pass our test because passengers detected the AI driver
above chance. In contrast, when the human driver drove the car, the passengers'
judgement was around chance. We further investigated how human passengers
ascribe humanness in our test. Based on Lewin's field theory, we advanced a
computational model combining signal detection theory with pre-trained language
models to predict passengers' humanness rating behaviour. We employed affective
transition between pre-study baseline emotions and corresponding post-stage
emotions as the signal strength of our model. Results showed that the
passengers' ascription of humanness would increase with the greater affective
transition. Our study suggested an important role of affective transition in
passengers' ascription of humanness, which might become a future direction for
autonomous driving."
83,83.0,"This probably stems from the
difﬁculty in training a high quality BiGAN to craft natural
samples for these datasets.","Further research is needed to see
if this problem can be solved by using stronger generator
trained on larger datasets.",5.5.,2022-12-06 01:48:27+00:00,On the Discredibility of Membership Inference Attacks,cs.CR,"['cs.CR', 'cs.AI']","[arxiv.Result.Author('Shahbaz Rezaei'), arxiv.Result.Author('Xin Liu')]","With the wide-spread application of machine learning models, it has become
critical to study the potential data leakage of models trained on sensitive
data. Recently, various membership inference (MI) attacks are proposed that
determines if a sample was part of the training set or not. Although the first
generation of MI attacks has been proven to be ineffective in practice, a few
recent studies proposed practical MI attacks that achieve reasonable true
positive rate at low false positive rate. The question is whether these attacks
can be reliably used in practice. We showcase a practical application of
membership inference attacks where it is used by an auditor (investigator) to
prove to a judge/jury that an auditee unlawfully used sensitive data during
training. Then, we show that the auditee can provide a dataset (with
potentially unlimited number of samples) to a judge where MI attacks
catastrophically fail. Hence, the auditee challenges the credibility of the
auditor and can get the case dismissed. More importantly, we show that the
auditee does not need to know anything about the MI attack neither a query
access to it. In other words, all currently SOTA MI attacks in literature
suffer from the same issue. Through comprehensive experimental evaluation, we
show that our algorithms can increase the false positive rate from ten to
thousands times larger than what auditor claim to the judge. Lastly, we argue
that the implication of our algorithms is beyond discredibility: Current
membership inference attacks can identify the memorized subpopulations, but
they cannot reliably identify which exact sample in the subpopulation was used
during training."
84,84.0,"Therefore, we
present a taxonomy highlighting the various facets of deploying synthetic data for advanced analytics
systems.","Furthermore, we identify typical application scenarios for synthetic data to assess the current
state of adoption and thereby unveil missed opportunities to pave the way for further research.","Keywords Synthetic Data · Taxonomy · Advanced Analytics · Deep Learning · Cluster Analysis

1

Introduction

In the last decade, advanced approaches to the analysis and exploitation of large amounts of heterogeneous data (“big
data”) have gained tremendous attention, particularly on the part of corporate decision-makers but also from academic
researchers [1, 2, 3].",2022-12-05 22:13:58+00:00,Towards a Taxonomy for the Use of Synthetic Data in Advanced Analytics,cs.LG,"['cs.LG', 'cs.AI', 'A.1; H.0; H.4; I.2.1']","[arxiv.Result.Author('Peter Kowalczyk'), arxiv.Result.Author('Giacomo Welsch'), arxiv.Result.Author('Frédéric Thiesse')]","The proliferation of deep learning techniques led to a wide range of advanced
analytics applications in important business areas such as predictive
maintenance or product recommendation. However, as the effectiveness of
advanced analytics naturally depends on the availability of sufficient data, an
organization's ability to exploit the benefits might be restricted by limited
data or likewise data access. These challenges could force organizations to
spend substantial amounts of money on data, accept constrained analytics
capacities, or even turn into a showstopper for analytics projects. Against
this backdrop, recent advances in deep learning to generate synthetic data may
help to overcome these barriers. Despite its great potential, however,
synthetic data are rarely employed. Therefore, we present a taxonomy
highlighting the various facets of deploying synthetic data for advanced
analytics systems. Furthermore, we identify typical application scenarios for
synthetic data to assess the current state of adoption and thereby unveil
missed opportunities to pave the way for further research."
85,85.0,"To this end, we consistently scrutinized our results qualitatively to ensure coherence and
applicability.","Nevertheless, the present paper may open avenues for further research regarding the use of synthetic data in advanced
analytics.","First, it outlines the rather unexplored ﬁelds of application for synthetic data which might be very appealing to

12

Symthetic Data in Advanced Analytics

discover (e.g., public service, security, agriculture, commerce, as well as media and graphical).",2022-12-05 22:13:58+00:00,Towards a Taxonomy for the Use of Synthetic Data in Advanced Analytics,cs.LG,"['cs.LG', 'cs.AI', 'A.1; H.0; H.4; I.2.1']","[arxiv.Result.Author('Peter Kowalczyk'), arxiv.Result.Author('Giacomo Welsch'), arxiv.Result.Author('Frédéric Thiesse')]","The proliferation of deep learning techniques led to a wide range of advanced
analytics applications in important business areas such as predictive
maintenance or product recommendation. However, as the effectiveness of
advanced analytics naturally depends on the availability of sufficient data, an
organization's ability to exploit the benefits might be restricted by limited
data or likewise data access. These challenges could force organizations to
spend substantial amounts of money on data, accept constrained analytics
capacities, or even turn into a showstopper for analytics projects. Against
this backdrop, recent advances in deep learning to generate synthetic data may
help to overcome these barriers. Despite its great potential, however,
synthetic data are rarely employed. Therefore, we present a taxonomy
highlighting the various facets of deploying synthetic data for advanced
analytics systems. Furthermore, we identify typical application scenarios for
synthetic data to assess the current state of adoption and thereby unveil
missed opportunities to pave the way for further research."
86,86.0,"Chapter 5 shows the re-
sults of the numerical experiments.","In Chapter 6 discusses the results, the limitations of
the proposed method and gives ideas that can guide further research.","5

2 Theoretic foundations and literature

review

2.1 Graph terminology

Our ﬁrst research question requires causally modeling complex systems.",2022-12-05 17:23:59+00:00,Observational and Interventional Causal Learning for Regret-Minimizing Control,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Christian Reiser')],"We explore how observational and interventional causal discovery methods can
be combined. A state-of-the-art observational causal discovery algorithm for
time series capable of handling latent confounders and contemporaneous effects,
called LPCMCI, is extended to profit from casual constraints found through
randomized control trials. Numerical results show that, given perfect
interventional constraints, the reconstructed structural causal models (SCMs)
of the extended LPCMCI allow 84.6% of the time for the optimal prediction of
the target variable. The implementation of interventional and observational
causal discovery is modular, allowing causal constraints from other sources.
  The second part of this thesis investigates the question of regret minimizing
control by simultaneously learning a causal model and planning actions through
the causal model. The idea is that an agent to optimize a measured variable
first learns the system's mechanics through observational causal discovery. The
agent then intervenes on the most promising variable with randomized values
allowing for the exploitation and generation of new interventional data. The
agent then uses the interventional data to enhance the causal model further,
allowing improved actions the next time.
  The extended LPCMCI can be favorable compared to the original LPCMCI
algorithm. The numerical results show that detecting and using interventional
constraints leads to reconstructed SCMs that allow 60.9% of the time for the
optimal prediction of the target variable in contrast to the baseline of 53.6%
when using the original LPCMCI algorithm. Furthermore, the induced average
regret decreases from 1.2 when using the original LPCMCI algorithm to 1.0 when
using the extended LPCMCI algorithm with interventional discovery."
87,87.0,"47

6 Discussion

6.1 Limitations and future work

The proposed approach to extend LPCMCI to proﬁt from interventional data has limita-
tions.",I explain these limitations and point to possible further research in the following.,"6.1.1 Empirical validation

The empirical validation through numerical experiments is limited and could be extended
to include a full parameter study where all parameter combinations are simulated.",2022-12-05 17:23:59+00:00,Observational and Interventional Causal Learning for Regret-Minimizing Control,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Christian Reiser')],"We explore how observational and interventional causal discovery methods can
be combined. A state-of-the-art observational causal discovery algorithm for
time series capable of handling latent confounders and contemporaneous effects,
called LPCMCI, is extended to profit from casual constraints found through
randomized control trials. Numerical results show that, given perfect
interventional constraints, the reconstructed structural causal models (SCMs)
of the extended LPCMCI allow 84.6% of the time for the optimal prediction of
the target variable. The implementation of interventional and observational
causal discovery is modular, allowing causal constraints from other sources.
  The second part of this thesis investigates the question of regret minimizing
control by simultaneously learning a causal model and planning actions through
the causal model. The idea is that an agent to optimize a measured variable
first learns the system's mechanics through observational causal discovery. The
agent then intervenes on the most promising variable with randomized values
allowing for the exploitation and generation of new interventional data. The
agent then uses the interventional data to enhance the causal model further,
allowing improved actions the next time.
  The extended LPCMCI can be favorable compared to the original LPCMCI
algorithm. The numerical results show that detecting and using interventional
constraints leads to reconstructed SCMs that allow 60.9% of the time for the
optimal prediction of the target variable in contrast to the baseline of 53.6%
when using the original LPCMCI algorithm. Furthermore, the induced average
regret decreases from 1.2 when using the original LPCMCI algorithm to 1.0 when
using the extended LPCMCI algorithm with interventional discovery."
88,88.0,"Furthermore, the ability to invert the LG Fibration for nearly all
elements allows for the development of Machine Learning algorithms that
may avoid the issues of uncertainty and reproducibility that currently
plague contemporary methods.","[7] The primary result of this paper is a
novel method of nearly invertible geometric dimensional reduction from
S2n−1 to Sn, which has applications for further research in both mathe-
matics and Artiﬁcial Intelligence, including but not limited to the ﬁelds
of homotopy groups of spheres, algebraic topology, machine learning, and
algebraic biology.","1

 
 
 
 
 
 
1

Introduction

Since the discovery of Quaternions by Olinde Rodrigues[2] and Sir William

Hamilton[3] independently in 1840 and 1843, the anti-commutative Division
Algebras of H and subsequently O have been the de facto generalization of the
Complex Numbers in mathematical research.",2022-12-05 05:01:28+00:00,The LG Fibration,math.DG,"['math.DG', 'cs.AI']","[arxiv.Result.Author('Daniel Livschitz'), arxiv.Result.Author('Weiqing Gu')]","Deep Learning has significantly impacted the application of data-to-decision
throughout research and industry, however, they lack a rigorous mathematical
foundation, which creates situations where algorithmic results fail to be
practically invertible. In this paper we present a nearly invertible mapping
between $\mathbb{R}^{2^n}$ and $\mathbb{R}^{n+1}$ via a topological connection
between $S^{2^n-1}$ and $S^n$. Throughout the paper we utilize the algebra of
Multicomplex rotation groups and polyspherical coordinates to define two maps:
the first is a contraction from $S^{2^n-1}$ to $\displaystyle \otimes^n_{k=1}
SO(2)$, and the second is a projection from $\displaystyle \otimes^n_{k=1}
SO(2)$ to $S^{n}$. Together these form a composite map that we call the LG
Fibration. In analogy to the generation of Hopf Fibration using Hypercomplex
geometry from $S^{(2n-1)} \mapsto CP^n$, our fibration uses Multicomplex
geometry to project $S^{2^n-1}$ onto $S^n$. We also investigate the algebraic
properties of the LG Fibration, ultimately deriving a distance difference
function to determine which pairs of vectors have an invariant inner product
under the transformation. The LG Fibration has applications to Machine Learning
and AI, in analogy to the current applications of Hopf Fibrations in adaptive
UAV control. Furthermore, the ability to invert the LG Fibration for nearly all
elements allows for the development of Machine Learning algorithms that may
avoid the issues of uncertainty and reproducibility that currently plague
contemporary methods. The primary result of this paper is a novel method of
nearly invertible geometric dimensional reduction from $S^{2^n-1}$ to $S^n$,
which has the capability to extend the research in both mathematics and AI,
including but not limited to the fields of homotopy groups of spheres,
algebraic topology, machine learning, and algebraic biology."
89,89.0,"Tn
This procedure immediately opens up the possibility for two separate areas of
research: Rn+1 rotations using commutative R2n
dimensional subgroups in
pure mathematics, and Dimensionality reduction algorithms for sparse high
dimensional spaces in theoretical computer science.","and

Gn

Large scale implementation of the LG Fibration in industrial Dimensionality
Reduction requires further research into methods of preconditioning data sets
such that the inital data will optimally lie within the deﬁned manifold.","Additional research may also be conducted into determining other suitable low
dimensional manifolds that may be embedded in high dimensional space, for
this would allow alternative approaches to minimize data loss.",2022-12-05 05:01:28+00:00,The LG Fibration,math.DG,"['math.DG', 'cs.AI']","[arxiv.Result.Author('Daniel Livschitz'), arxiv.Result.Author('Weiqing Gu')]","Deep Learning has significantly impacted the application of data-to-decision
throughout research and industry, however, they lack a rigorous mathematical
foundation, which creates situations where algorithmic results fail to be
practically invertible. In this paper we present a nearly invertible mapping
between $\mathbb{R}^{2^n}$ and $\mathbb{R}^{n+1}$ via a topological connection
between $S^{2^n-1}$ and $S^n$. Throughout the paper we utilize the algebra of
Multicomplex rotation groups and polyspherical coordinates to define two maps:
the first is a contraction from $S^{2^n-1}$ to $\displaystyle \otimes^n_{k=1}
SO(2)$, and the second is a projection from $\displaystyle \otimes^n_{k=1}
SO(2)$ to $S^{n}$. Together these form a composite map that we call the LG
Fibration. In analogy to the generation of Hopf Fibration using Hypercomplex
geometry from $S^{(2n-1)} \mapsto CP^n$, our fibration uses Multicomplex
geometry to project $S^{2^n-1}$ onto $S^n$. We also investigate the algebraic
properties of the LG Fibration, ultimately deriving a distance difference
function to determine which pairs of vectors have an invariant inner product
under the transformation. The LG Fibration has applications to Machine Learning
and AI, in analogy to the current applications of Hopf Fibrations in adaptive
UAV control. Furthermore, the ability to invert the LG Fibration for nearly all
elements allows for the development of Machine Learning algorithms that may
avoid the issues of uncertainty and reproducibility that currently plague
contemporary methods. The primary result of this paper is a novel method of
nearly invertible geometric dimensional reduction from $S^{2^n-1}$ to $S^n$,
which has the capability to extend the research in both mathematics and AI,
including but not limited to the fields of homotopy groups of spheres,
algebraic topology, machine learning, and algebraic biology."
90,90.0,"Future publications should
aim to utilize the abelian rotation and hyperbolic groups in Cn for
mathematical physics, such as generalizing how the split complex numbers
may be used to perform a Lorentz boost in 1+1 Minkowski space.","[1]
Discovering the LG Fibration was made possible because of the clear
connection between geometry and algebra in the Multicomplex numbers, and
our hope through this paper is to inspire further research into the applications
of geometric groups embedded in high dimensional spaces.","References

[1] Francesco Catoni et al.",2022-12-05 05:01:28+00:00,The LG Fibration,math.DG,"['math.DG', 'cs.AI']","[arxiv.Result.Author('Daniel Livschitz'), arxiv.Result.Author('Weiqing Gu')]","Deep Learning has significantly impacted the application of data-to-decision
throughout research and industry, however, they lack a rigorous mathematical
foundation, which creates situations where algorithmic results fail to be
practically invertible. In this paper we present a nearly invertible mapping
between $\mathbb{R}^{2^n}$ and $\mathbb{R}^{n+1}$ via a topological connection
between $S^{2^n-1}$ and $S^n$. Throughout the paper we utilize the algebra of
Multicomplex rotation groups and polyspherical coordinates to define two maps:
the first is a contraction from $S^{2^n-1}$ to $\displaystyle \otimes^n_{k=1}
SO(2)$, and the second is a projection from $\displaystyle \otimes^n_{k=1}
SO(2)$ to $S^{n}$. Together these form a composite map that we call the LG
Fibration. In analogy to the generation of Hopf Fibration using Hypercomplex
geometry from $S^{(2n-1)} \mapsto CP^n$, our fibration uses Multicomplex
geometry to project $S^{2^n-1}$ onto $S^n$. We also investigate the algebraic
properties of the LG Fibration, ultimately deriving a distance difference
function to determine which pairs of vectors have an invariant inner product
under the transformation. The LG Fibration has applications to Machine Learning
and AI, in analogy to the current applications of Hopf Fibrations in adaptive
UAV control. Furthermore, the ability to invert the LG Fibration for nearly all
elements allows for the development of Machine Learning algorithms that may
avoid the issues of uncertainty and reproducibility that currently plague
contemporary methods. The primary result of this paper is a novel method of
nearly invertible geometric dimensional reduction from $S^{2^n-1}$ to $S^n$,
which has the capability to extend the research in both mathematics and AI,
including but not limited to the fields of homotopy groups of spheres,
algebraic topology, machine learning, and algebraic biology."
91,91.0,"Simple early fusion
of these multimodal data streams via concatenation is neither
appropriate nor feasible.","Further research is required on how
to handle multimodal personalization data streams.",B.,2022-12-04 18:16:57+00:00,Persona-Based Conversational AI: State of the Art and Challenges,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Junfeng Liu'), arxiv.Result.Author('Christopher Symons'), arxiv.Result.Author('Ranga Raju Vatsavai')]","Conversational AI has become an increasingly prominent and practical
application of machine learning. However, existing conversational AI techniques
still suffer from various limitations. One such limitation is a lack of
well-developed methods for incorporating auxiliary information that could help
a model understand conversational context better. In this paper, we explore how
persona-based information could help improve the quality of response generation
in conversations. First, we provide a literature review focusing on the current
state-of-the-art methods that utilize persona information. We evaluate two
strong baseline methods, the Ranking Profile Memory Network and the
Poly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates
the importance of incorporating persona information into conversational
systems. Additionally, our study highlights several limitations with current
state-of-the-art methods and outlines challenges and future research directions
for advancing personalized conversational AI technology."
92,92.0,"However, such behavioral data can’t be easily obtained
from Turkers (e.g., ConvAI2 dataset) or social media sites
(e.g., Twitter dataset), and may require guidance from highly
skilled behavioral scientists.","Therefore, further research is
needed to address these limitations, in addition to a strong
collaboration with different domain experts, as well as Turkers
and volunteers to guide the process of generating rich persona-
based conversational data.","In summary, existing datasets are not sufﬁcient to build
advanced persona-based conversational agents, in particular, in
domains where more complex and multimodal data is required
to engage users and drive them towards speciﬁc goals.",2022-12-04 18:16:57+00:00,Persona-Based Conversational AI: State of the Art and Challenges,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Junfeng Liu'), arxiv.Result.Author('Christopher Symons'), arxiv.Result.Author('Ranga Raju Vatsavai')]","Conversational AI has become an increasingly prominent and practical
application of machine learning. However, existing conversational AI techniques
still suffer from various limitations. One such limitation is a lack of
well-developed methods for incorporating auxiliary information that could help
a model understand conversational context better. In this paper, we explore how
persona-based information could help improve the quality of response generation
in conversations. First, we provide a literature review focusing on the current
state-of-the-art methods that utilize persona information. We evaluate two
strong baseline methods, the Ranking Profile Memory Network and the
Poly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates
the importance of incorporating persona information into conversational
systems. Additionally, our study highlights several limitations with current
state-of-the-art methods and outlines challenges and future research directions
for advancing personalized conversational AI technology."
93,93.0,"• We experimentally evaluate and analyze the re-
sults of different families of sequence labeling
baseline models on our dataset.","We also summa-
rize difﬁculties in our dataset, and provide ﬁnd-
ings and further research topics on our dataset.","of
potential
CA4P-483 to regulate privacy policies with law
knowledge and program analysis technologies.",2022-12-04 05:59:59+00:00,A Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification,cs.CR,"['cs.CR', 'cs.AI']","[arxiv.Result.Author('Kaifa Zhao'), arxiv.Result.Author('Le Yu'), arxiv.Result.Author('Shiyao Zhou'), arxiv.Result.Author('Jing Li'), arxiv.Result.Author('Xiapu Luo'), arxiv.Result.Author('Yat Fei Aemon Chiu'), arxiv.Result.Author('Yutong Liu')]","Privacy protection raises great attention on both legal levels and user
awareness. To protect user privacy, countries enact laws and regulations
requiring software privacy policies to regulate their behavior. However,
privacy policies are written in natural languages with many legal terms and
software jargon that prevent users from understanding and even reading them. It
is desirable to use NLP techniques to analyze privacy policies for helping
users understand them. Furthermore, existing datasets ignore law requirements
and are limited to English. In this paper, we construct the first Chinese
privacy policy dataset, namely CA4P-483, to facilitate the sequence labeling
tasks and regulation compliance identification between privacy policies and
software. Our dataset includes 483 Chinese Android application privacy
policies, over 11K sentences, and 52K fine-grained annotations. We evaluate
families of robust and representative baseline models on our dataset. Based on
baseline performance, we provide findings and potential research directions on
our dataset. Finally, we investigate the potential applications of CA4P-483
combing regulation requirements and program analysis."
94,94.0,"No Full Coverage refers to the situa-
tion where the output does not cover the full answer
as expressed by all triples mentioned in the given
sub-graph.","We believe that further research on the
reasoning ability and interpretability of the model
can help address this issue.","Unnatural Connec-
tion denotes that a response is not connected to the
dialogue history especially the last utterance natu-
rally.",2022-12-03 10:36:34+00:00,RHO ($ρ$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ziwei Ji'), arxiv.Result.Author('Zihan Liu'), arxiv.Result.Author('Nayeon Lee'), arxiv.Result.Author('Tiezheng Yu'), arxiv.Result.Author('Bryan Wilie'), arxiv.Result.Author('Min Zeng'), arxiv.Result.Author('Pascale Fung')]","Dialogue systems can leverage large pre-trained language models and knowledge
to generate fluent and informative responses. However, these models are still
prone to produce hallucinated responses not supported by the input source,
which greatly hinders their application. The heterogeneity between external
knowledge and dialogue context challenges representation learning and source
integration, and further contributes to unfaithfulness. To handle this
challenge and generate more faithful responses, this paper presents RHO
($\rho$) utilizing the representations of linked entities and relation
predicates from a knowledge graph (KG). We propose (1) local knowledge
grounding to combine textual embeddings with the corresponding KG embeddings;
and (2) global knowledge grounding to equip RHO with multi-hop reasoning
abilities via the attention mechanism. In addition, we devise a response
re-ranking technique based on walks over KG sub-graphs for better
conversational reasoning. Experimental results on OpenDialKG show that our
approach significantly outperforms state-of-the-art methods on both automatic
and human evaluation by a large margin, especially in hallucination reduction
(17.54% in FeQA)."
95,95.0,"Despite
the practical importance of this regime, molecular prop-
erty prediction using ML with limited data instances has
been relatively under-explored, and remains a challeng-
ing task, especially for deep learning models which often
require large amounts of training instances due to large
number of model parameters.","In the low-data setting, understanding a ML model’s
performance is important since predictions inform deci-
sions about further research directions, or, in a sequen-
tial learning setting, promote molecules to be subject to
property measurement.","In particular, we place empha-
sis on 1) the generalizability, the ability of a model to
predict accurately on new chemical data, and 2) uncer-
tainty calibration, the ability of a model to estimate the
conﬁdence of its predictions.",2022-12-03 08:19:06+00:00,Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS,cs.CE,"['cs.CE', 'cs.AI']","[arxiv.Result.Author('Gary Tom'), arxiv.Result.Author('Riley J. Hickman'), arxiv.Result.Author('Aniket Zinzuwadia'), arxiv.Result.Author('Afshan Mohajeri'), arxiv.Result.Author('Benjamin Sanchez-Lengeling'), arxiv.Result.Author('Alan Aspuru-Guzik')]","Deep learning models that leverage large datasets are often the state of the
art for modelling molecular properties. When the datasets are smaller (< 2000
molecules), it is not clear that deep learning approaches are the right
modelling tool. In this work we perform an extensive study of the calibration
and generalizability of probabilistic machine learning models on small chemical
datasets. Using different molecular representations and models, we analyse the
quality of their predictions and uncertainties in a variety of tasks (binary,
regression) and datasets. We also introduce two simulated experiments that
evaluate their performance: (1) Bayesian optimization guided molecular design,
(2) inference on out-of-distribution data via ablated cluster splits. We offer
practical insights into model and feature choice for modelling small chemical
datasets, a common scenario in new chemical experiments. We have packaged our
analysis into the DIONYSUS repository, which is open sourced to aid in
reproducibility and extension to new datasets."
96,96.0,"Additionally, we used a two-phase study design,
starting with a larger phase in which we sought feedback from the participants after using the fully
functional tool for one month, and continuing with a second phase in which we implemented a
within-participant randomized controlled experiment lasting two months.","The results of the user study suggest that the risk awareness paradigm has the potential to improve
online discourse and motivate further research in this direction.","In exit surveys, the majority of
participants report that they found ConvoWizard helpful for identifying tense situations, with
the tool both supplementing their intuitions—catching types of tension that they may not have
known to look for—and activating their existing intuitions—reminding them to be on the lookout
for tension in situations where they may not have been paying attention.",2022-12-02 19:00:03+00:00,Thread With Caution: Proactively Helping Users Assess and Deescalate Tension in Their Online Discussions,cs.HC,"['cs.HC', 'cs.AI', 'cs.CL', 'cs.CY', 'physics.soc-ph']","[arxiv.Result.Author('Jonathan P. Chang'), arxiv.Result.Author('Charlotte Schluger'), arxiv.Result.Author('Cristian Danescu-Niculescu-Mizil')]","Incivility remains a major challenge for online discussion platforms, to such
an extent that even conversations between well-intentioned users can often
derail into uncivil behavior. Traditionally, platforms have relied on
moderators to -- with or without algorithmic assistance -- take corrective
actions such as removing comments or banning users. In this work we propose a
complementary paradigm that directly empowers users by proactively enhancing
their awareness about existing tension in the conversation they are engaging in
and actively guides them as they are drafting their replies to avoid further
escalation.
  As a proof of concept for this paradigm, we design an algorithmic tool that
provides such proactive information directly to users, and conduct a user study
in a popular discussion platform. Through a mixed methods approach combining
surveys with a randomized controlled experiment, we uncover qualitative and
quantitative insights regarding how the participants utilize and react to this
information. Most participants report finding this proactive paradigm valuable,
noting that it helps them to identify tension that they may have otherwise
missed and prompts them to further reflect on their own replies and to revise
them. These effects are corroborated by a comparison of how the participants
draft their reply when our tool warns them that their conversation is at risk
of derailing into uncivil behavior versus in a control condition where the tool
is disabled. These preliminary findings highlight the potential of this
user-centered paradigm and point to concrete directions for future
implementations."
97,97.0,"Zero-shot
and a ﬁne-tuning approach using the labels only
show more inconsistent results across the two years.","The zero-shot with legal reasoning approach shows
the best result for one year only and may be more
prone to overﬁtting to a speciﬁc test set indicating
that further research of those prompting approaches
is needed.","2 Legal Entailment task

The COLIEE competition (Rabelo et al., 2022) has
been carried out since 2014 driving research in
the area of legal retrieval and entailment.",2022-12-02 17:41:22+00:00,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']","[arxiv.Result.Author('Fangyi Yu'), arxiv.Result.Author('Lee Quartey'), arxiv.Result.Author('Frank Schilder')]","Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fare with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431."
98,98.0,"Moreover, our
networks are being lighter weight.","We hope that the idea
and structure of DRWSeg foster further research in seman-
tic segmentation.",Acknowledgment.,2022-12-02 13:55:41+00:00,DWRSeg: Dilation-wise Residual Network for Real-time Semantic Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Haoran Wei'), arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Shouchun Xu'), arxiv.Result.Author('Zhongjian Dai'), arxiv.Result.Author('Yaping Dai'), arxiv.Result.Author('Xiangyang Xu')]","Real-time semantic segmentation has played an important role in intelligent
vehicle scenarios. Recently, numerous networks have incorporated information
from multi-size receptive fields to facilitate feature extraction in real-time
semantic segmentation tasks. However, these methods preferentially adopt
massive receptive fields to elicit more contextual information, which may
result in inefficient feature extraction. We believe that the elaborated
receptive fields are crucial, considering the demand for efficient feature
extraction in real-time tasks. Therefore, we propose an effective and efficient
architecture termed Dilation-wise Residual segmentation (DWRSeg), which
possesses different sets of receptive field sizes within different stages. The
architecture involves (i) a Dilation-wise Residual (DWR) module for extracting
features based on different scales of receptive fields in the high level of the
network; (ii) a Simple Inverted Residual (SIR) module that uses an inverted
bottleneck structure to extract features from the low stage; and (iii) a simple
fully convolutional network (FCN)-like decoder for aggregating multiscale
feature maps to generate the prediction. Extensive experiments on the
Cityscapes and CamVid datasets demonstrate the effectiveness of our method by
achieving a state-of-the-art trade-off between accuracy and inference speed, in
addition to being lighter weight. Without using pretraining or resorting to any
training trick, we achieve 72.7% mIoU on the Cityscapes test set at a speed of
319.5 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster
than existing methods. The code and trained models are publicly available."
99,99.0,"The de-
gree of slowdown will inevitably depend on the
hardware, but we estimate that a generative ap-
proach could be several times slower than a dis-
criminative one.","However, this could also be a mat-
ter for further research on the topic; for example,
non-autoregressive generative models are steadily
narrowing the performance gap (Gu and Tan, 2022)
while mitigating the weaknesses of current autore-
gressive approaches.",Evaluation.,2022-12-02 11:19:16+00:00,Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Simone Conia'), arxiv.Result.Author('Edoardo Barba'), arxiv.Result.Author('Alessandro Scirè'), arxiv.Result.Author('Roberto Navigli')]","One of the common traits of past and present approaches for Semantic Role
Labeling (SRL) is that they rely upon discrete labels drawn from a predefined
linguistic inventory to classify predicate senses and their arguments. However,
we argue this need not be the case. In this paper, we present an approach that
leverages Definition Modeling to introduce a generalized formulation of SRL as
the task of describing predicate-argument structures using natural language
definitions instead of discrete labels. Our novel formulation takes a first
step towards placing interpretability and flexibility foremost, and yet our
experiments and analyses on PropBank-style and FrameNet-style, dependency-based
and span-based SRL also demonstrate that a flexible model with an interpretable
output does not necessarily come at the expense of performance. We release our
software for research purposes at https://github.com/SapienzaNLP/dsrl."
100,100.0,6 (b)(d)).,"The actual reason may need further research to
fully unveil.","For the implementation detail, it is worth men-
tioning that for the case tuning only PLM in the up-
stream stage, parameter-efﬁcient elements(adapters
or prompts) are initialized but remain frozen un-
til entering the downstream stage.",2022-12-02 08:56:53+00:00,General Framework for Self-Supervised Model Priming for Parameter-Efficient Fine-tuning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shih-Cheng Huang'), arxiv.Result.Author('Shih-Heng Wang'), arxiv.Result.Author('Min-Han Shih'), arxiv.Result.Author('Saurav Sahay'), arxiv.Result.Author('Hung-yi Lee')]","Parameter-efficient methods (like Prompt or Adapters) for adapting
pre-trained language models to downstream tasks have been popular recently.
However, hindrances still prevent these methods from reaching their full
potential. For example, two significant challenges are few-shot adaptation and
cross-task generalization ability. To tackle these issues, we propose a general
framework to enhance the few-shot adaptation and cross-domain generalization
ability of parameter-efficient methods. In our framework, we prime the
self-supervised model for parameter-efficient methods to rapidly adapt to
various downstream few-shot tasks. To evaluate the authentic generalization
ability of these parameter-efficient methods, we conduct experiments on a
few-shot cross-domain benchmark containing 160 diverse NLP tasks. The
experiment result reveals that priming by tuning PLM only with extra training
tasks leads to the best performance. Also, we perform a comprehensive analysis
of various parameter-efficient methods under few-shot cross-domain scenarios."
101,101.0,"(2018) also found similar re-
sults with diﬀerent manipulations creating and resolving uncertainty about diﬀerent brands.","Further research into this eﬀect is needed, but we hypothesize that, more generally, learners
may develop increased positive attitudes towards topics associated with the inostensible
concept when curiosity is resolved.","Individual Interest → Curiosity:
Individual interest may direct curiosity by directing
a learner’s attention.",2022-12-01 00:18:56+00:00,Five Properties of Specific Curiosity You Didn't Know Curious Machines Should Have,cs.AI,"['cs.AI', 'cs.LG']","[arxiv.Result.Author('Nadia M. Ady'), arxiv.Result.Author('Roshan Shariff'), arxiv.Result.Author('Johannes Günther'), arxiv.Result.Author('Patrick M. Pilarski')]","Curiosity for machine agents has been a focus of lively research activity.
The study of human and animal curiosity, particularly specific curiosity, has
unearthed several properties that would offer important benefits for machine
learners, but that have not yet been well-explored in machine intelligence. In
this work, we conduct a comprehensive, multidisciplinary survey of the field of
animal and machine curiosity. As a principal contribution of this work, we use
this survey as a foundation to introduce and define what we consider to be five
of the most important properties of specific curiosity: 1) directedness towards
inostensible referents, 2) cessation when satisfied, 3) voluntary exposure, 4)
transience, and 5) coherent long-term learning. As a second main contribution
of this work, we show how these properties may be implemented together in a
proof-of-concept reinforcement learning agent: we demonstrate how the
properties manifest in the behaviour of this agent in a simple non-episodic
grid-world environment that includes curiosity-inducing locations and induced
targets of curiosity. As we would hope, our example of a computational specific
curiosity agent exhibits short-term directed behaviour while updating long-term
preferences to adaptively seek out curiosity-inducing situations. This work,
therefore, presents a landmark synthesis and translation of specific curiosity
to the domain of machine learning and reinforcement learning and provides a
novel view into how specific curiosity operates and in the future might be
integrated into the behaviour of goal-seeking, decision-making computational
agents in complex environments."
102,102.0,"Khanna, Lu, Warrier - 6

Our results show that socioeconomic factors, specifically income and level of educational
attainment, correlate with certain health outcomes through visual analysis and robust predictive
modeling.","Additionally, we found that these disparities are even more correlated with certain
“fixable problems,” which provides a potential pathway for further research to investigate how
the disparities present in the healthcare system today can be addressed.","Acknowledgements

9.",2022-12-01 00:00:40+00:00,The Impact of Socioeconomic Factors on Health Disparities,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', '68U99', 'I.2.6; J.4']","[arxiv.Result.Author('Krish Khanna'), arxiv.Result.Author('Jeffrey Lu'), arxiv.Result.Author('Jay Warrier'), arxiv.Result.Author('Phillip Lo'), arxiv.Result.Author('Adela DePavia'), arxiv.Result.Author('Ray Fregly')]","High-quality healthcare in the US can be cost-prohibitive for certain
socioeconomic groups. In this paper, we examined data from the US Census and
the CDC to determine the degree to which specific socioeconomic factors
correlate with both specific and general health metrics. We employed visual
analysis to find broad trends and predictive modeling to identify more complex
relationships between variables. Our results indicate that certain
socioeconomic factors, like income and educational attainment, are highly
correlated with aggregate measures of health."
103,103.0,"In spite of being strongly motivated, both of our proposed procedures do not obtain consistent
improvements over simply using full-dimensional models, unlike some of the aforementioned more
involved manifold-aware models.","We hope that this surprising result will lead into further research
aiming to understand the interplay between the manifold hypothesis and DGMs.","2 Background

2.1 Likelihood-based DGMs and Tweedie’s formula

Throughout this work we will assume that we have access to samples from a distribution p(x) in RD.",2022-11-30 19:00:00+00:00,Denoising Deep Generative Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Gabriel Loaiza-Ganem'), arxiv.Result.Author('Brendan Leigh Ross'), arxiv.Result.Author('Luhuan Wu'), arxiv.Result.Author('John P. Cunningham'), arxiv.Result.Author('Jesse C. Cresswell'), arxiv.Result.Author('Anthony L. Caterini')]","Likelihood-based deep generative models have recently been shown to exhibit
pathological behaviour under the manifold hypothesis as a consequence of using
high-dimensional densities to model data with low-dimensional structure. In
this paper we propose two methodologies aimed at addressing this problem. Both
are based on adding Gaussian noise to the data to remove the dimensionality
mismatch during training, and both provide a denoising mechanism whose goal is
to sample from the model as though no noise had been added to the data. Our
first approach is based on Tweedie's formula, and the second on models which
take the variance of added noise as a conditional input. We show that
surprisingly, while well motivated, these approaches only sporadically improve
performance over not adding noise, and that other methods of addressing the
dimensionality mismatch are more empirically adequate."
104,104.0,"1. allowing to deﬁne a family of semantics for non-monotonic reasoning with disjunctive informa-

tion,

2. clarifying similarities and diﬀerences between semantics stemming from the use of diﬀerent

operators.","The introduction of disjunctive information in AFT points to a wealth of further research, such
as deﬁning three-valued and well-founded semantics for various disjunctive nonmonotonic formalisms
and studying on the basis of which operators various well-founded semantics for DLP can be repre-
sented in our framework.","For example, our framework can potentially be used for deﬁning three-valued
and well-founded semantics for propositional theories [54], disjunctive logic programs with recursive
aggregates [26], logic programs with aggregates in the head [33], logic programs with forks [1], and
disjunctive default logics [32, 14].",2022-11-30 18:58:32+00:00,Non-Deterministic Approximation Fixpoint Theory and Its Application in Disjunctive Logic Programming,cs.AI,['cs.AI'],"[arxiv.Result.Author('Jesse Heyninck'), arxiv.Result.Author('Ofer Arieli'), arxiv.Result.Author('Bart Bogaerts')]","Approximation fixpoint theory (AFT) is an abstract and general algebraic
framework for studying the semantics of nonmonotonic logics. It provides a
unifying study of the semantics of different formalisms for nonmonotonic
reasoning, such as logic programming, default logic and autoepistemic logic. In
this paper, we extend AFT to dealing with non-deterministic constructs that
allow to handle indefinite information, represented e.g. by disjunctive
formulas. This is done by generalizing the main constructions and corresponding
results of AFT to non-deterministic operators, whose ranges are sets of
elements rather than single elements. The applicability and usefulness of this
generalization is illustrated in the context of disjunctive logic programming."
105,105.0,"They formulate the instance segmentation problem as the
combination of previous techniques (object detection and
semantic segmentation).","However, modules (such as RPN [4]
and ROIAlign [1]) restrict the training and inference efﬁciency
deeply, which leads to expensive hardware resources and time
costs for further research.","Some works [5], [6], [7] follow
this issue and construct one-stage [8], [9] pipelines, while the
hybrid design of detection and segmentation makes it hard to
assemble instance masks.",2022-11-30 04:50:56+00:00,Growing Instance Mask on Leaf,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Chuang Yang'), arxiv.Result.Author('Haozhao Ma'), arxiv.Result.Author('Qi Wang')]","Contour-based instance segmentation methods include one-stage and multi-stage
schemes. These approaches achieve remarkable performance. However, they have to
define plenty of points to segment precise masks, which leads to high
complexity. We follow this issue and present a single-shot method, called
\textbf{VeinMask}, for achieving competitive performance in low design
complexity. Concretely, we observe that the leaf locates coarse margins via
major veins and grows minor veins to refine twisty parts, which makes it
possible to cover any objects accurately. Meanwhile, major and minor veins
share the same growth mode, which avoids modeling them separately and ensures
model simplicity. Considering the superiorities above, we propose VeinMask to
formulate the instance segmentation problem as the simulation of the vein
growth process and to predict the major and minor veins in polar coordinates.
  Besides, centroidness is introduced for instance segmentation tasks to help
suppress low-quality instances. Furthermore, a surroundings cross-correlation
sensitive (SCCS) module is designed to enhance the feature expression by
utilizing the surroundings of each pixel. Additionally, a Residual IoU (R-IoU)
loss is formulated to supervise the regression tasks of major and minor veins
effectively. Experiments demonstrate that VeinMask performs much better than
other contour-based methods in low design complexity. Particularly, our method
outperforms existing one-stage contour-based methods on the COCO dataset with
almost half the design complexity."
106,106.0,"Although the comparisons with the oracle demonstrated comparable modeling
capabilities with respect to modeling with LFI-based methods, future research is still required to
adapt alternative solutions to the proposed meta-learning problem.","Further research should also take
ethical considerations, such as possible privacy issues, into account.","We also recognize that user
modeling could potentially be abused to serve other interests than those of users.",2022-11-29 15:09:18+00:00,Differentiable User Models,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Alex Hämäläinen'), arxiv.Result.Author('Mustafa Mert Çelikok'), arxiv.Result.Author('Samuel Kaski')]","Probabilistic user modeling is essential for building collaborative AI
systems within probabilistic frameworks. However, modern advanced user models,
often designed as cognitive behavior simulators, are computationally
prohibitive for interactive use in cooperative AI assistants. In this extended
abstract, we address this problem by introducing widely-applicable
differentiable surrogates for bypassing this computational bottleneck; the
surrogates enable using modern behavioral models with online computational cost
which is independent of their original computational cost. We show
experimentally that modeling capabilities comparable to likelihood-free
inference methods are achievable, with over eight orders of magnitude reduction
in computational time. Finally, we demonstrate how AI-assistants can
computationally feasibly use cognitive models in a previously studied
menu-search task."
107,107.0,"Across these more than 5,000,000 noisy images
from four commonly used academic datasets: Adience [20], Casual Conversations Dataset [34],
MIAP [65], and UTKFace [82].","Additionally, to allow further research, we make our raw data
available for exploration here: https://dooleys.github.io/robustness/.1

By benchmarking both commercial and academic models, we can understand two important insights:
(1) audit the use-case of a company which takes open-source models to build in-house facial recog-
nition models, and (2) adjudicate corporation’s claims of caring about demographic biases in their
products by measuring the extent to which their models are less biased than academic models which
have no fairness considerations.","As such, we endeavor to answer three research questions:

(RQ1): How robust are commercial and academic face detection models to natural types of noise?",2022-11-29 05:22:47+00:00,Robustness Disparities in Face Detection,cs.CY,"['cs.CY', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Samuel Dooley'), arxiv.Result.Author('George Z. Wei'), arxiv.Result.Author('Tom Goldstein'), arxiv.Result.Author('John P. Dickerson')]","Facial analysis systems have been deployed by large companies and critiqued
by scholars and activists for the past decade. Many existing algorithmic audits
examine the performance of these systems on later stage elements of facial
analysis systems like facial recognition and age, emotion, or perceived gender
prediction; however, a core component to these systems has been vastly
understudied from a fairness perspective: face detection, sometimes called face
localization. Since face detection is a pre-requisite step in facial analysis
systems, the bias we observe in face detection will flow downstream to the
other components like facial recognition and emotion prediction. Additionally,
no prior work has focused on the robustness of these systems under various
perturbations and corruptions, which leaves open the question of how various
people are impacted by these phenomena. We present the first of its kind
detailed benchmark of face detection systems, specifically examining the
robustness to noise of commercial and academic models. We use both standard and
recently released academic facial datasets to quantitatively analyze trends in
face detection robustness. Across all the datasets and systems, we generally
find that photos of individuals who are $\textit{masculine presenting}$,
$\textit{older}$, of $\textit{darker skin type}$, or have $\textit{dim
lighting}$ are more susceptible to errors than their counterparts in other
identities."
108,108.0,"In addition to the
effectiveness of our proposed method, we acknowledge its
limitations, particularly as a triplet representation potentially
requires a high computational cost in a large-scale dataset.","This issue leads to further research directions to be addressed
in the future.","APPENDIX A
ATD IS A DISTANCE METRIC

identity, symmetry, and triangle

k ) ≥ 0

θ

z

lm
i

,z

ln
j

=
θ
z

=

π
lo
k

ln
j

,z
π

= [0, 1],

= [0, 1],

, zln

, zln

j ) =

j ) + DA(zln

Proof.",2022-11-28 10:18:06+00:00,Angular triangle distance for ordinal metric learning,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Imam Mustafa Kamal'), arxiv.Result.Author('Hyerim Bae')]","Deep metric learning (DML) aims to automatically construct task-specific
distances or similarities of data, resulting in a low-dimensional
representation. Several significant metric-learning methods have been proposed.
Nonetheless, no approach guarantees the preservation of the ordinal nature of
the original data in a low-dimensional space. Ordinal data are ubiquitous in
real-world problems, such as the severity of symptoms in biomedical cases,
production quality in manufacturing, rating level in businesses, and aging
level in face recognition. This study proposes a novel angular triangle
distance (ATD) and ordinal triplet network (OTD) to obtain an accurate and
meaningful embedding space representation for ordinal data. The ATD projects
the ordinal relation of data in the angular space, whereas the OTD learns its
ordinal projection. We also demonstrated that our new distance measure
satisfies the distance metric properties mathematically. The proposed method
was assessed using real-world data with an ordinal nature, such as biomedical,
facial, and hand-gestured images. Extensive experiments have been conducted,
and the results show that our proposed method not only semantically preserves
the ordinal nature but is also more accurate than existing DML models.
Moreover, we also demonstrate that our proposed method outperforms the
state-of-the-art ordinal metric learning method."
109,109.0,"Experimental results on two widely-used datasets show that
the joint-training model MNER-QG competes strongly with
other baselines in different tasks.","MNER-QG leaves ample
scope for further research.","For future work, we will explore
more multimodal topics.",2022-11-27 06:10:03+00:00,MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Meihuizi Jia'), arxiv.Result.Author('Lei Shen'), arxiv.Result.Author('Xin Shen'), arxiv.Result.Author('Lejian Liao'), arxiv.Result.Author('Meng Chen'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Zhendong Chen'), arxiv.Result.Author('Jiaqi Li')]","Multimodal named entity recognition (MNER) is a critical step in information
extraction, which aims to detect entity spans and classify them to
corresponding entity types given a sentence-image pair. Existing methods either
(1) obtain named entities with coarse-grained visual clues from attention
mechanisms, or (2) first detect fine-grained visual regions with toolkits and
then recognize named entities. However, they suffer from improper alignment
between entity types and visual regions or error propagation in the two-stage
manner, which finally imports irrelevant visual information into texts. In this
paper, we propose a novel end-to-end framework named MNER-QG that can
simultaneously perform MRC-based multimodal named entity recognition and query
grounding. Specifically, with the assistance of queries, MNER-QG can provide
prior knowledge of entity types and visual regions, and further enhance
representations of both texts and images. To conduct the query grounding task,
we provide manual annotations and weak supervisions that are obtained via
training a highly flexible visual grounding model with transfer learning. We
conduct extensive experiments on two public MNER datasets, Twitter2015 and
Twitter2017. Experimental results show that MNER-QG outperforms the current
state-of-the-art models on the MNER task, and also improves the query grounding
performance."
110,110.0,"Existing eﬀorts include but not limited
to cloud segmentation, cloud classiﬁcation and cloud motion prediction.","Cloud segmentation is one of the ﬁrst steps for sky/cloud image analysis [31], from which cloud pixels are identiﬁed
and cloud coverage, cloud shadow projection and various cloud features can be derived for further research.","Traditional
methods tend to distinguish cloud and clear sky pixels by applying a threshold, either ﬁxed or adaptive, on features
extracted from the red and blue channels of sky images.",2022-11-27 03:35:58+00:00,"Open-Source Ground-based Sky Image Datasets for Very Short-term Solar Forecasting, Cloud Analysis and Modeling: A Comprehensive Survey",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Yuhao Nie'), arxiv.Result.Author('Xiatong Li'), arxiv.Result.Author('Quentin Paletta'), arxiv.Result.Author('Max Aragon'), arxiv.Result.Author('Andea Scott'), arxiv.Result.Author('Adam Brandt')]","Sky-image-based solar forecasting using deep learning has been recognized as
a promising approach in reducing the uncertainty in solar power generation.
However, one of the biggest challenges is the lack of massive and diversified
sky image samples. In this study, we present a comprehensive survey of
open-source ground-based sky image datasets for very short-term solar
forecasting (i.e., forecasting horizon less than 30 minutes), as well as
related research areas which can potentially help improve solar forecasting
methods, including cloud segmentation, cloud classification and cloud motion
prediction. We first identify 72 open-source sky image datasets that satisfy
the needs of machine/deep learning. Then a database of information about
various aspects of the identified datasets is constructed. To evaluate each
surveyed datasets, we further develop a multi-criteria ranking system based on
8 dimensions of the datasets which could have important impacts on usage of the
data. Finally, we provide insights on the usage of these datasets for different
applications. We hope this paper can provide an overview for researchers who
are looking for datasets for very short-term solar forecasting and related
areas."
111,111.0,"Although RL
improves the performance only by a small value, control pol-
icy by RL is adaptable to a wider range of initial condition
and is able to adjust the control on the ﬂy to minimize the
accumulated error.","Further research needs to be conducted
to test the generalizability of RL under different initial con-
ditions, terrain conditions and external disturbance.","We
visualizes the design and the trajectory of the robotic table
for four tasks using the training result by RL.",2022-11-26 20:52:03+00:00,Computational Co-Design for Variable Geometry Truss,cs.AI,"['cs.AI', 'cs.NE', 'cs.RO']","[arxiv.Result.Author('Jianzhe Gu'), arxiv.Result.Author('Lining Yao')]","Living creatures and machines interact with the world through their
morphology and motions. Recent advances in creating bio-inspired morphing
robots and machines have led to the study of variable geometry truss (VGT),
structures that can approximate arbitrary geometries and has large degree of
freedom to deform. However, they are limited to simple geometries and motions
due to the excessively complex control system. While a recent work PneuMesh
solves this challenge with a novel VGT design that introduces a selective
channel connection strategy, it imposes new challenge in identifying effective
channel groupings and control methods.
  Building on top of the hardware concept presented in PneuMesh, we frame the
challenge into a co-design problem and introduce a learning-based model to find
a sub-optimal design. Specifically, given an initial truss structure provided
by a human designer, we first adopt a genetic algorithm (GA) to optimize the
channel grouping, and then couple GA with reinforcement learning (RL) for the
control. The model is tailored to the PneuMesh system with customized
initialization, mutation and selection functions, as well as the customized
translation-invariant state vector for reinforcement learning. The result shows
that our method enables a robotic table-based VGT to achieve various motions
with a limited number of control inputs. The table is trained to move, lower
its body or tilt its tabletop to accommodate multiple use cases such as
benefiting kids and painters to use it in different shape states, allowing
inclusive and adaptive design through morphing trusses."
112,112.0,"To this end, Chia et al.","(2022) propose a chal-
lenging task called zero-shot relation triplet extrac-
tion (ZeroRTE) to overcome the above limitations
and encourage further research on training models
that can generalize to unseen relations.","ZeroRTE
aims to extract relational triplets under the zero-
shot setting.",2022-11-26 04:27:31+00:00,PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yuquan Lan'), arxiv.Result.Author('Dongxu Li'), arxiv.Result.Author('Yunqi Zhang'), arxiv.Result.Author('Hui Zhao'), arxiv.Result.Author('Gang Zhao')]","Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation
triplets from unstructured texts under the zero-shot setting, where the
relation sets at the training and testing stages are disjoint. Previous
state-of-the-art method handles this challenging task by leveraging pretrained
language models to generate data as additional training samples, which
increases the training cost and severely constrains the model performance. To
address the above issues, we propose a novel method named PCRED for ZeroRTE
with Potential Candidate Relation Selection and Entity Boundary Detection. The
remarkable characteristic of PCRED is that it does not rely on additional data
and still achieves promising performance. The model adopts a relation-first
paradigm, recognizing unseen relations through candidate relation selection.
With this approach, the semantics of relations are naturally infused in the
context. Entities are extracted based on the context and the semantics of
relations subsequently. We evaluate our model on two ZeroRTE datasets. The
experiment results show that our method consistently outperforms previous
works. Our code will be available at https://anonymous.4open.science/r/PCRED."
113,113.0,"If any positive result is reached, an analysis would be
required to determine if the increase in resistance to attacks is due only to the improved test
data generalization (already reflected in the accuracy) or if these models obtain an additional
intrinsic robustness property.","Another interesting procedure with further research potential being the Test Time Sum
Augmentation technique.","Having the configurable λ parameter, one could experiment with
its value as being a trade-off between performance and robustness.",2022-11-25 22:31:11+00:00,Deep Learning Training Procedure Augmentations,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Cristian Simionescu')],"Recent advances in Deep Learning have greatly improved performance on various
tasks such as object detection, image segmentation, sentiment analysis. The
focus of most research directions up until very recently has been on beating
state-of-the-art results. This has materialized in the utilization of bigger
and bigger models and techniques which help the training procedure to extract
more predictive power out of a given dataset. While this has lead to great
results, many of which with real-world applications, other relevant aspects of
deep learning have remained neglected and unknown. In this work, we will
present several novel deep learning training techniques which, while capable of
offering significant performance gains they also reveal several interesting
analysis results regarding convergence speed, optimization landscape
smoothness, and adversarial robustness. The methods presented in this work are
the following:
  $\bullet$ Perfect Ordering Approximation; a generalized model agnostic
curriculum learning approach. The results show the effectiveness of the
technique for improving training time as well as offer some new insight into
the training process of deep networks.
  $\bullet$ Cascading Sum Augmentation; an extension of mixup capable of
utilizing more data points for linear interpolation by leveraging a smoother
optimization landscape. This can be used for computer vision tasks in order to
improve both prediction performance as well as improve passive model
robustness."
114,114.0,"Then the obtained message vectors of node and edge are
concatenated with the corresponding current hidden states
to be sent to the communicate function.","For simplicity, we
still use an addition operator as the communicative kernel
(line 5, 9, 20, 22), and more complex calculating operator
can be considered in further research.","Considering that the
molecular graphs are seen as directed graphs, the hidden
state of an edge should not rely on its reverse message which
should be subtracted in each updating stage (line 12).",2022-11-25 11:53:23+00:00,Molecular Joint Representation Learning via Multi-modal Information,cs.LG,"['cs.LG', 'cs.AI', 'q-bio.BM']","[arxiv.Result.Author('Tianyu Wu'), arxiv.Result.Author('Yang Tang'), arxiv.Result.Author('Qiyu Sun'), arxiv.Result.Author('Luolin Xiong')]","In recent years, artificial intelligence has played an important role on
accelerating the whole process of drug discovery. Various of molecular
representation schemes of different modals (e.g. textual sequence or graph) are
developed. By digitally encoding them, different chemical information can be
learned through corresponding network structures. Molecular graphs and
Simplified Molecular Input Line Entry System (SMILES) are popular means for
molecular representation learning in current. Previous works have done attempts
by combining both of them to solve the problem of specific information loss in
single-modal representation on various tasks. To further fusing such
multi-modal imformation, the correspondence between learned chemical feature
from different representation should be considered. To realize this, we propose
a novel framework of molecular joint representation learning via Multi-Modal
information of SMILES and molecular Graphs, called MMSG. We improve the
self-attention mechanism by introducing bond level graph representation as
attention bias in Transformer to reinforce feature correspondence between
multi-modal information. We further propose a Bidirectional Message
Communication Graph Neural Network (BMC GNN) to strengthen the information flow
aggregated from graphs for further combination. Numerous experiments on public
property prediction datasets have demonstrated the effectiveness of our model."
115,115.0,"Meanwhile, students
participating in the WRITING task students enjoyed the educational experience of learning a new
script (e.g.","“I am interested in writing forms and would one day like to learn some unusual scripts.”,
“interesting learning to write another language”), but wished to learn more about the characters’
meaning, motivating further research in making automatically-discovered skills (which may not
necessarily be characters) more interpretable to students (e.g.","“I would like to know what Balinese
characters I’m tracing and their meaning”).",2022-11-25 10:18:29+00:00,Assistive Teaching of Motor Control Tasks to Humans,cs.AI,"['cs.AI', 'cs.HC', 'cs.RO']","[arxiv.Result.Author('Megha Srivastava'), arxiv.Result.Author('Erdem Biyik'), arxiv.Result.Author('Suvir Mirchandani'), arxiv.Result.Author('Noah Goodman'), arxiv.Result.Author('Dorsa Sadigh')]","Recent works on shared autonomy and assistive-AI technologies, such as
assistive robot teleoperation, seek to model and help human users with limited
ability in a fixed task. However, these approaches often fail to account for
humans' ability to adapt and eventually learn how to execute a control task
themselves. Furthermore, in applications where it may be desirable for a human
to intervene, these methods may inhibit their ability to learn how to succeed
with full self-control. In this paper, we focus on the problem of assistive
teaching of motor control tasks such as parking a car or landing an aircraft.
Despite their ubiquitous role in humans' daily activities and occupations,
motor tasks are rarely taught in a uniform way due to their high complexity and
variance. We propose an AI-assisted teaching algorithm that leverages skill
discovery methods from reinforcement learning (RL) to (i) break down any motor
control task into teachable skills, (ii) construct novel drill sequences, and
(iii) individualize curricula to students with different capabilities. Through
an extensive mix of synthetic and user studies on two motor control tasks --
parking a car with a joystick and writing characters from the Balinese alphabet
-- we show that assisted teaching with skills improves student performance by
around 40% compared to practicing full trajectories without skills, and
practicing with individualized drills can result in up to 25% further
improvement. Our source code is available at
https://github.com/Stanford-ILIAD/teaching"
116,116.0,"The implemented context-awareness decision model is a rule-based
reasoning engine.","Further research is required to select and evaluate alterna-
tive decision models suitable for robotic platform deployment, particularly in
settings where sensor noise will perturb normal system operations.","The second
avenue of future work is to study the impact of incorrect context or situation
identiﬁcation.",2022-11-22 20:25:59+00:00,Contextually Aware Intelligent Control Agents for Heterogeneous Swarms,cs.AI,['cs.AI'],"[arxiv.Result.Author('Adam Hepworth'), arxiv.Result.Author('Aya Hussein'), arxiv.Result.Author('Darryn Reid'), arxiv.Result.Author('Hussein Abbass')]","An emerging challenge in swarm shepherding research is to design effective
and efficient artificial intelligence algorithms that maintain a
low-computational ceiling while increasing the swarm's abilities to operate in
diverse contexts. We propose a methodology to design a context-aware
swarm-control intelligent agent. The intelligent control agent (shepherd) first
uses swarm metrics to recognise the type of swarm it interacts with to then
select a suitable parameterisation from its behavioural library for that
particular swarm type. The design principle of our methodology is to increase
the situation awareness (i.e. information contents) of the control agent
without sacrificing the low-computational cost necessary for efficient swarm
control. We demonstrate successful shepherding in both homogeneous and
heterogeneous swarms."
117,117.0,"The proposed simulation model can help policymakers study residential
patterns and address challenges in areas such as urban planning.","Further researchers can
improve this model by exploring different RL algorithms or considering other realistic input
parameters.","References:

1.",2022-11-22 18:40:41+00:00,The impact of moving expenses on social segregation: a simulation with RL and ABM,econ.GN,"['econ.GN', 'cs.AI', 'q-fin.EC']",[arxiv.Result.Author('Xinyu Li')],"Over the past decades, breakthroughs such as Reinforcement Learning (RL) and
Agent-based modeling (ABM) have made simulations of economic models feasible.
Recently, there has been increasing interest in applying ABM to study the
impact of residential preferences on neighborhood segregation in the Schelling
Segregation Model. In this paper, RL is combined with ABM to simulate a
modified Schelling Segregation model, which incorporates moving expenses as an
input parameter. In particular, deep Q network (DQN) is adopted as RL agents'
learning algorithm to simulate the behaviors of households and their
preferences. This paper studies the impact of moving expenses on the overall
segregation pattern and its role in social integration. A more comprehensive
simulation of the segregation model is built for policymakers to forecast the
potential consequences of their policies."
118,118.0,"However, recent studies
show that these strong results in broad and common domains
may generalize poorly even to speciﬁc industrial applications on
earth.","To address this, we propose a method for generating
synthetic image data that are labelled for semantic segmen-
tation, generalizable to other tasks, and provide a prototype
synthetic image dataset consisting of 2D monocular images of
unmanned spacecraft, in order to enable further research in the
area of autonomous spacecraft rendezvous.","We also present a
strong benchmark result (Sørensen-Dice coefﬁcient 0.8723) on
these synthetic data, suggesting that it is feasible to train well-
performing image segmentation models for this task, especially
if the target spacecraft and its conﬁguration are known.",2022-11-22 01:30:40+00:00,Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('William S. Armstrong'), arxiv.Result.Author('Spencer Drakontaidis'), arxiv.Result.Author('Nicholas Lui')]","Images of spacecraft photographed from other spacecraft operating in outer
space are difficult to come by, especially at a scale typically required for
deep learning tasks. Semantic image segmentation, object detection and
localization, and pose estimation are well researched areas with powerful
results for many applications, and would be very useful in autonomous
spacecraft operation and rendezvous. However, recent studies show that these
strong results in broad and common domains may generalize poorly even to
specific industrial applications on earth. To address this, we propose a method
for generating synthetic image data that are labelled for semantic
segmentation, generalizable to other tasks, and provide a prototype synthetic
image dataset consisting of 2D monocular images of unmanned spacecraft, in
order to enable further research in the area of autonomous spacecraft
rendezvous. We also present a strong benchmark result (S{\o}rensen-Dice
coefficient 0.8723) on these synthetic data, suggesting that it is feasible to
train well-performing image segmentation models for this task, especially if
the target spacecraft and its configuration are known."
119,119.0,"[1] note that these strong results in broad and
common domains may generalize poorly even to speciﬁc
industrial applications on earth.","To address this, we generated a prototype synthetic image
dataset labelled for semantic segmentation of 2D images
of unmanned spacecraft, and are endeavouring to train a
performant deep learning image segmentation model using the
same, with the ultimate goal of enabling further research in
the area of autonomous spacecraft rendezvous.","978-1-6654-9032-0/23/$31.00 ©2023 IEEE

Related work

Minaee et al.",2022-11-22 01:30:40+00:00,Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('William S. Armstrong'), arxiv.Result.Author('Spencer Drakontaidis'), arxiv.Result.Author('Nicholas Lui')]","Images of spacecraft photographed from other spacecraft operating in outer
space are difficult to come by, especially at a scale typically required for
deep learning tasks. Semantic image segmentation, object detection and
localization, and pose estimation are well researched areas with powerful
results for many applications, and would be very useful in autonomous
spacecraft operation and rendezvous. However, recent studies show that these
strong results in broad and common domains may generalize poorly even to
specific industrial applications on earth. To address this, we propose a method
for generating synthetic image data that are labelled for semantic
segmentation, generalizable to other tasks, and provide a prototype synthetic
image dataset consisting of 2D monocular images of unmanned spacecraft, in
order to enable further research in the area of autonomous spacecraft
rendezvous. We also present a strong benchmark result (S{\o}rensen-Dice
coefficient 0.8723) on these synthetic data, suggesting that it is feasible to
train well-performing image segmentation models for this task, especially if
the target spacecraft and its configuration are known."
120,120.0,"Negative results

Throughout the development of our method, we exper-
imented with various techniques drawing inspiration from
the literature on GANs and representation learning.","To
guide further research in this area, we provide a list of ideas
we explored but did not work out as expected.","• We initially experimented with various NeRF representa-
tions, including MLP-based, voxel-based, and triplanar-
based.",2022-11-21 17:42:42+00:00,"Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion",cs.CV,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG']","[arxiv.Result.Author('Dario Pavllo'), arxiv.Result.Author('David Joseph Tan'), arxiv.Result.Author('Marie-Julie Rakotosaona'), arxiv.Result.Author('Federico Tombari')]","Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks."
121,121.0,"This could potentially make it possible to
back-propagate through the entire diffusion process with the
adjoint sensitivity method [32], rather than unrolling a lim-
ited number of iterations.","At a conceptual level, we hope
that our work motivates further research about the combi-
nation of classical optimization-based computer vision and
modern, deep learning-based image representations.","In line
with others [5,33], our work suggests that such hybrid meth-
ods hold great potential, not only for super-resolution, but
also for other low-level vision problems, and perhaps be-
yond.",2022-11-21 15:48:13+00:00,Guided Depth Super-Resolution by Deep Anisotropic Diffusion,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Nando Metzger'), arxiv.Result.Author('Rodrigo Caye Daudt'), arxiv.Result.Author('Konrad Schindler')]","Performing super-resolution of a depth image using the guidance from an RGB
image is a problem that concerns several fields, such as robotics, medical
imaging, and remote sensing. While deep learning methods have achieved good
results in this problem, recent work highlighted the value of combining modern
methods with more formal frameworks. In this work, we propose a novel approach
which combines guided anisotropic diffusion with a deep convolutional network
and advances the state of the art for guided depth super-resolution. The edge
transferring/enhancing properties of the diffusion are boosted by the
contextual reasoning capabilities of modern networks, and a strict adjustment
step guarantees perfect adherence to the source image. We achieve unprecedented
results in three commonly used benchmarks for guided depth super-resolution.
The performance gain compared to other methods is the largest at larger scales,
such as x32 scaling. Code for the proposed method will be made available to
promote reproducibility of our results."
122,122.0,"The last graph at the bottom mirrors the algorithm accelerated by ’SuperSAB’ method with the

same initial τ1 as at the last ’Momentum’ (1).","28

Other order and further combination of these techniques and also their adaptive tuning were also

partly analysed, however, their comprehensive testing is outside of the scope of the current paper, more-

over, there is a signiﬁcant, further research potential in this direction as well.","The main aim here was

to make the described tests easier, more stable and quicker, so, the combined algorithm was applied, as

introduced in pseudo code 1.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
123,123.0,"As it is shown in ﬁgure 28, the ﬁnal MSE values of the dynamic τ algorithm did not reach better

results than the ﬁxed one in case of a regression datasets (e.g.","Diabetes), however, for higher τ values its

ranges seems to be similar, so it may have some advantages as well, but, this ﬁeld could be a challenge of

further research analysis.","In this research ﬁeld Heravi & Hodtani (2018a,c,b, 2019) presented excellent research results analysing

noise and other features inherited in the training datasets to predict various properties of the resulted train-

ing models.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
124,124.0,"The proposed, novel dynamic training algorithm serves with various superiority behaviour but it is

important to note that these results are valid only for the classiﬁcation, but not for the regression assign-

ments, according to the various assessments.","Probably this is because the introduced EExpAbs measure

combines CE with MSE and CE has advances typically in classiﬁcation tasks, but this is only an assump-

tion that needs further research.","54

12.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
125,125.0,"R´enyi Entropy) as it was

introduced at the beginning of the paper.","To integrate more measures into a novel one could be a

promising direction for further research.","Ideal would be having a ”super-measure” that integrates

(almost) all of the beneﬁcial measures into one error ”calculus”.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
126,126.0,"Creating this dataset was required as there
exists no such kind of evaluation dataset due to the fact that,
to the best of our knowledge, no existing LP work deals with
unseen relations.","The results obtained with the proposed
model on this challenging dataset are provided which could
be seen as a first attempt to facilitate further research in the
community on the topic of LP with unseen relations.","This paper is organized as follows: Section 2 discusses the related
works in the area of inductive LP.",2022-11-21 12:35:30+00:00,RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs,cs.AI,"['cs.AI', 'cs.CL']","[arxiv.Result.Author('Genet Asefa Gesese'), arxiv.Result.Author('Harald Sack'), arxiv.Result.Author('Mehwish Alam')]","Due to the open world assumption, Knowledge Graphs (KGs) are never complete.
In order to address this issue, various Link Prediction (LP) methods are
proposed so far. Some of these methods are inductive LP models which are
capable of learning representations for entities not seen during training.
However, to the best of our knowledge, none of the existing inductive LP models
focus on learning representations for unseen relations. In this work, a novel
Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion
which learns representations for both unseen entities and unseen relations. In
addition to leveraging textual literals associated with both entities and
relations by employing language models, RAILD also introduces a novel
graph-based approach to generate features for relations. Experiments are
conducted with different existing and newly created challenging benchmark
datasets and the results indicate that RAILD leads to performance improvement
over the state-of-the-art models. Moreover, since there are no existing
inductive LP models which learn representations for unseen relations, we have
created our own baselines and the results obtained with RAILD also outperform
these baselines."
127,127.0,"Moreover,
the WeiDNeR algorithm is applied to the training set, the validation
set, and the test set separately so as to avoid generating features us-
ing unseen graphs for training.","As this is the first work, to the best
of our knowledge, to ever make an attempt to perform LP with un-
seen relations, it would facilitate further research in the community
to redirect the focus to unseen relations as well as entities.",4.4.4 Ablation studies.,2022-11-21 12:35:30+00:00,RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs,cs.AI,"['cs.AI', 'cs.CL']","[arxiv.Result.Author('Genet Asefa Gesese'), arxiv.Result.Author('Harald Sack'), arxiv.Result.Author('Mehwish Alam')]","Due to the open world assumption, Knowledge Graphs (KGs) are never complete.
In order to address this issue, various Link Prediction (LP) methods are
proposed so far. Some of these methods are inductive LP models which are
capable of learning representations for entities not seen during training.
However, to the best of our knowledge, none of the existing inductive LP models
focus on learning representations for unseen relations. In this work, a novel
Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion
which learns representations for both unseen entities and unseen relations. In
addition to leveraging textual literals associated with both entities and
relations by employing language models, RAILD also introduces a novel
graph-based approach to generate features for relations. Experiments are
conducted with different existing and newly created challenging benchmark
datasets and the results indicate that RAILD leads to performance improvement
over the state-of-the-art models. Moreover, since there are no existing
inductive LP models which learn representations for unseen relations, we have
created our own baselines and the results obtained with RAILD also outperform
these baselines."
128,128.0,"Nevertheless, VS is imprecise and prone to producing inaccurate predictions,

much like several other in silico techniques.","Once this occurs, inactive molecules may be classiﬁed

as false positives, wasting time and important resources on further research.","Therefore, increasing

VS’s enrichment rates is still necessary.",2022-11-21 09:15:13+00:00,"Intelligent Computing: The Latest Advances, Challenges and Future",cs.AI,['cs.AI'],"[arxiv.Result.Author('Shiqiang Zhu'), arxiv.Result.Author('Ting Yu'), arxiv.Result.Author('Tao Xu'), arxiv.Result.Author('Hongyang Chen'), arxiv.Result.Author('Schahram Dustdar'), arxiv.Result.Author('Sylvain Gigan'), arxiv.Result.Author('Deniz Gunduz'), arxiv.Result.Author('Ekram Hossain'), arxiv.Result.Author('Yaochu Jin'), arxiv.Result.Author('Feng Lin'), arxiv.Result.Author('Bo Liu'), arxiv.Result.Author('Zhiguo Wan'), arxiv.Result.Author('Ji Zhang'), arxiv.Result.Author('Zhifeng Zhao'), arxiv.Result.Author('Wentao Zhu'), arxiv.Result.Author('Zuoning Chen'), arxiv.Result.Author('Tariq Durrani'), arxiv.Result.Author('Huaimin Wang'), arxiv.Result.Author('Jiangxing Wu'), arxiv.Result.Author('Tongyi Zhang'), arxiv.Result.Author('Yunhe Pan')]","Computing is a critical driving force in the development of human
civilization. In recent years, we have witnessed the emergence of intelligent
computing, a new computing paradigm that is reshaping traditional computing and
promoting digital revolution in the era of big data, artificial intelligence
and internet-of-things with new computing theories, architectures, methods,
systems, and applications. Intelligent computing has greatly broadened the
scope of computing, extending it from traditional computing on data to
increasingly diverse computing paradigms such as perceptual intelligence,
cognitive intelligence, autonomous intelligence, and human-computer fusion
intelligence. Intelligence and computing have undergone paths of different
evolution and development for a long time but have become increasingly
intertwined in recent years: intelligent computing is not only
intelligence-oriented but also intelligence-driven. Such cross-fertilization
has prompted the emergence and rapid advancement of intelligent computing.
Intelligent computing is still in its infancy and an abundance of innovations
in the theories, systems, and applications of intelligent computing are
expected to occur soon. We present the first comprehensive survey of literature
on intelligent computing, covering its theory fundamentals, the technological
fusion of intelligence and computing, important applications, challenges, and
future perspectives. We believe that this survey is highly timely and will
provide a comprehensive reference and cast valuable insights into intelligent
computing for academic and industrial researchers and practitioners."
129,129.0,"The VLN agents combined with our
SEA pre-trained features (without tuning) achieve 12% SR
improvement for Speaker-Follower, 5% for Env-Dropout,
and 4% for AuxRN in test-unseen under the single-run set-
ting.","The contributions of proposed auxiliary tasks and SEA
pre-trained features are orthogonal to other VLN works, and
we will release the collected dataset, source code, and pre-
trained features to facilitate further research in visual repre-
sentations for VLN.","Appendix

A.",2022-11-20 23:04:39+00:00,Structure-Encoding Auxiliary Tasks for Improved Visual Representation in Vision-and-Language Navigation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Chia-Wen Kuo'), arxiv.Result.Author('Chih-Yao Ma'), arxiv.Result.Author('Judy Hoffman'), arxiv.Result.Author('Zsolt Kira')]","In Vision-and-Language Navigation (VLN), researchers typically take an image
encoder pre-trained on ImageNet without fine-tuning on the environments that
the agent will be trained or tested on. However, the distribution shift between
the training images from ImageNet and the views in the navigation environments
may render the ImageNet pre-trained image encoder suboptimal. Therefore, in
this paper, we design a set of structure-encoding auxiliary tasks (SEA) that
leverage the data in the navigation environments to pre-train and improve the
image encoder. Specifically, we design and customize (1) 3D jigsaw, (2)
traversability prediction, and (3) instance classification to pre-train the
image encoder. Through rigorous ablations, our SEA pre-trained features are
shown to better encode structural information of the scenes, which ImageNet
pre-trained features fail to properly encode but is crucial for the target
navigation task. The SEA pre-trained features can be easily plugged into
existing VLN agents without any tuning. For example, on Test-Unseen
environments, the VLN agents combined with our SEA pre-trained features achieve
absolute success rate improvement of 12% for Speaker-Follower, 5% for
Env-Dropout, and 4% for AuxRN."
130,130.0,"Extensive experiments on

8

three datasets verify the effectiveness and efﬁciency of our
tracker.","To help further research and solve the deﬁciency
of current benchmarks, we propose a large-scale and high-
quality dataset COESOT for the color-event tracking com-
munity, which presents color frames, event frames, event
voxels, source ﬁles, and dense annotations, etc.","We also
provide a toolkit for tracking evaluation and comparison
with 28 baseline tracking results on COESOT.",2022-11-20 16:01:31+00:00,"Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric",cs.CV,"['cs.CV', 'cs.AI', 'cs.NE']","[arxiv.Result.Author('Chuanming Tang'), arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Ju Huang'), arxiv.Result.Author('Bo Jiang'), arxiv.Result.Author('Lin Zhu'), arxiv.Result.Author('Jianlin Zhang'), arxiv.Result.Author('Yaowei Wang'), arxiv.Result.Author('Yonghong Tian')]","Combining the Color and Event cameras (also called Dynamic Vision Sensors,
DVS) for robust object tracking is a newly emerging research topic in recent
years. Existing color-event tracking framework usually contains multiple
scattered modules which may lead to low efficiency and high computational
complexity, including feature extraction, fusion, matching, interactive
learning, etc. In this paper, we propose a single-stage backbone network for
Color-Event Unified Tracking (CEUTrack), which achieves the above functions
simultaneously. Given the event points and RGB frames, we first transform the
points into voxels and crop the template and search regions for both
modalities, respectively. Then, these regions are projected into tokens and
parallelly fed into the unified Transformer backbone network. The output
features will be fed into a tracking head for target object localization. Our
proposed CEUTrack is simple, effective, and efficient, which achieves over 75
FPS and new SOTA performance. To better validate the effectiveness of our model
and address the data deficiency of this task, we also propose a generic and
large-scale benchmark dataset for color-event tracking, termed COESOT, which
contains 90 categories and 1354 video sequences. Additionally, a new evaluation
metric named BOC is proposed in our evaluation toolkit to evaluate the
prominence with respect to the baseline methods. We hope the newly proposed
method, dataset, and evaluation metric provide a better platform for
color-event-based tracking. The dataset, toolkit, and source code will be
released on: \url{https://github.com/Event-AHU/COESOT}."
131,131.0,"Inspired by the above work, this paper proposes a multimodal emotion recognition method based 
on face, gesture and text.","Unlike the early fusion methods, it has further research in the processing 
of fused features after fusion.","Considering combining the internal information in a single modality 
and  the  interaction  information  between  modalities  and  solving  the  problems  of  information 
redundancy  and  high  dimensionality,  this  paper  achieves  the  best  result  of  tri-modal  fusion  by 
combining  features  followed  by  a  two-dimensional  convolution  operation  and  adding  attention 

 
weights.",2022-11-20 14:43:36+00:00,"FAF: A novel multimodal emotion recognition approach integrating face, body and text",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zhongyu Fang'), arxiv.Result.Author('Aoyun He'), arxiv.Result.Author('Qihui Yu'), arxiv.Result.Author('Baopeng Gao'), arxiv.Result.Author('Weiping Ding'), arxiv.Result.Author('Tong Zhang'), arxiv.Result.Author('Lei Ma')]","Multimodal emotion analysis performed better in emotion recognition depending
on more comprehensive emotional clues and multimodal emotion dataset. In this
paper, we developed a large multimodal emotion dataset, named ""HED"" dataset, to
facilitate the emotion recognition task, and accordingly propose a multimodal
emotion recognition method. To promote recognition accuracy, ""Feature After
Feature"" framework was used to explore crucial emotional information from the
aligned face, body and text samples. We employ various benchmarks to evaluate
the ""HED"" dataset and compare the performance with our method. The results show
that the five classification accuracy of the proposed multimodal fusion method
is about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62%
respectively compared with that of individual modalities. The complementarity
between each channel is effectively used to improve the performance of emotion
recognition. We had also established a multimodal online emotion prediction
platform, aiming to provide free emotion prediction to more users."
132,132.0,"Background  provided  key  evidence  when  difficult  to  determine  the 
emotional state, which was not sensitive to the real-time emotion recognition.","Further research is 
needed for deep semantic feature mining and emotion recognition in dynamic environment.","Future 
work in emotion recognition needs high quality dataset, recognition models and more advanced 
theoretical models to interpretation of multimodal emotion recognition.",2022-11-20 14:43:36+00:00,"FAF: A novel multimodal emotion recognition approach integrating face, body and text",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Zhongyu Fang'), arxiv.Result.Author('Aoyun He'), arxiv.Result.Author('Qihui Yu'), arxiv.Result.Author('Baopeng Gao'), arxiv.Result.Author('Weiping Ding'), arxiv.Result.Author('Tong Zhang'), arxiv.Result.Author('Lei Ma')]","Multimodal emotion analysis performed better in emotion recognition depending
on more comprehensive emotional clues and multimodal emotion dataset. In this
paper, we developed a large multimodal emotion dataset, named ""HED"" dataset, to
facilitate the emotion recognition task, and accordingly propose a multimodal
emotion recognition method. To promote recognition accuracy, ""Feature After
Feature"" framework was used to explore crucial emotional information from the
aligned face, body and text samples. We employ various benchmarks to evaluate
the ""HED"" dataset and compare the performance with our method. The results show
that the five classification accuracy of the proposed multimodal fusion method
is about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62%
respectively compared with that of individual modalities. The complementarity
between each channel is effectively used to improve the performance of emotion
recognition. We had also established a multimodal online emotion prediction
platform, aiming to provide free emotion prediction to more users."
133,133.0,"Furthermore, the SAT approach is
generic to be employed on most GNN models without sacriﬁcing
classiﬁcation power and training efﬁciency.","Studying the robustness of GNNs is an important problem,
and this work provides essential insights for further research.","In
the future, we plan to study tasks beyond node classiﬁcation and
extend our research to other graph structures such as multiple
and heterogeneous information graphs.",2022-11-20 07:56:55+00:00,Spectral Adversarial Training for Robust Graph Neural Network,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jintang Li'), arxiv.Result.Author('Jiaying Peng'), arxiv.Result.Author('Liang Chen'), arxiv.Result.Author('Zibin Zheng'), arxiv.Result.Author('Tingting Liang'), arxiv.Result.Author('Qing Ling')]","Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency."
134,134.0,"To the best of our knowledge the proposed ACO 

is  the  first  one  proposed  for  the  CARP  providing  high  quality  results  for  large  scale  instances.","However,  the  performance  of  the  proposed  algorithm  does  not  reach  state-of-the-art  performance 

and further researches are required to increase the convergence rate and to reduce the computational 

times.","References 

1.",2022-11-19 10:31:27+00:00,First Competitive Ant Colony Scheme for the CARP,cs.NE,"['cs.NE', 'cs.AI']","[arxiv.Result.Author('Lacomme Philippe'), arxiv.Result.Author('Prins Christian'), arxiv.Result.Author('Tanguy Alain')]","This paper addresses the Capacitated Arc Routing Problem (CARP) using an Ant
Colony Optimization scheme. Ant Colony schemes can compute solutions for medium
scale instances of VRP. The proposed Ant Colony is dedicated to large-scale
instances of CARP with more than 140 nodes and 190 arcs to service. The Ant
Colony scheme is coupled with a local search procedure and provides high
quality solutions. The benchmarks we carried out prove possible to obtain
solutions as profitable as CARPET ones can be obtained using such scheme when a
sufficient number of iterations is devoted to the ants. It competes with the
Genetic Algorithm of Lacomme et al. regarding solution quality but it is more
time consuming on large scale instances. The method has been intensively
benchmarked on the well-known instances of Eglese, DeArmon and the last ones of
Belenguer and Benavent. This research report is a step forward CARP resolution
by Ant Colony proving ant schemes can compete with Taboo search methods and
Genetic Algorithms"
135,135.0,"Different from discrete
hidden layers in the neural networks, Neural ODE uses the
ODE system as a fundamental component, and the output is
calculated through the ODESolver.","The continuous neural dy-
namics of Neural ODE determines that it can handle or incor-
porate time-series data, especially with non-uniform intervals,
which inspires further research greatly [21–23].","Rubanova
et al.",2022-11-19 05:43:10+00:00,Autoregressive GNN-ODE GRU Model for Network Dynamics,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bo Liang'), arxiv.Result.Author('Lin Wang'), arxiv.Result.Author('Xiaofan Wang')]","Revealing the continuous dynamics on the networks is essential for
understanding, predicting, and even controlling complex systems, but it is hard
to learn and model the continuous network dynamics because of complex and
unknown governing equations, high dimensions of complex systems, and
unsatisfactory observations. Moreover, in real cases, observed time-series data
are usually non-uniform and sparse, which also causes serious challenges. In
this paper, we propose an Autoregressive GNN-ODE GRU Model (AGOG) to learn and
capture the continuous network dynamics and realize predictions of node states
at an arbitrary time in a data-driven manner. The GNN module is used to model
complicated and nonlinear network dynamics. The hidden state of node states is
specified by the ODE system, and the augmented ODE system is utilized to map
the GNN into the continuous time domain. The hidden state is updated through
GRUCell by observations. As prior knowledge, the true observations at the same
timestamp are combined with the hidden states for the next prediction. We use
the autoregressive model to make a one-step ahead prediction based on
observation history. The prediction is achieved by solving an initial-value
problem for ODE. To verify the performance of our model, we visualize the
learned dynamics and test them in three tasks: interpolation reconstruction,
extrapolation prediction, and regular sequences prediction. The results
demonstrate that our model can capture the continuous dynamic process of
complex systems accurately and make precise predictions of node states with
minimal error. Our model can consistently outperform other baselines or achieve
comparable performance."
136,136.0,"The experiments are
conducted on an Intel Xeon Gold 5122 CPU equipped with 300 GB RAM.","For
reproducibility and further research, our source code is public.6

4.2 Parameter Tuning

Our simulated annealing approach and our genetic algorithm require parame-
ters which we have to chosen carefully.","We provide recommendations based on

3 https://www.infosun.ﬁm.uni-passau.de/~chris/down/MIP-1202.pdf
4 http://www.graphdrawing.org/data.html
5 http://minisat.se/MiniSat.html
6 https://github.com/domduerr/bipartite

8

D. Dürrschnabel et al.",2022-11-18 15:45:45+00:00,Discovering Locally Maximal Bipartite Subgraphs,cs.AI,"['cs.AI', 'math.CO', '90C27', 'G.2.1; F.2.2']","[arxiv.Result.Author('Dominik Dürrschnabel'), arxiv.Result.Author('Tom Hanika'), arxiv.Result.Author('Gerd Stumme')]","Induced bipartite subgraphs of maximal vertex cardinality are an essential
concept for the analysis of graphs. Yet, discovering them in large graphs is
known to be computationally hard. Therefore, we consider in this work a weaker
notion of this problem, where we discard the maximality constraint in favor of
inclusion maximality. Thus, we aim to discover locally maximal bipartite
subgraphs. For this, we present three heuristic approaches to extract such
subgraphs and compare their results to the solutions of the global problem. For
the latter, we employ the algorithmic strength of fast SAT-solvers. Our three
proposed heuristics are based on a greedy strategy, a simulated annealing
approach, and a genetic algorithm, respectively. We evaluate all four
algorithms with respect to their time requirement and the vertex cardinality of
the discovered bipartite subgraphs on several benchmark datasets"
137,137.0,"Although each NAS component has a distinct role, some operations (sampling and evaluating candi-
dates, for instance) overlap due to shared implementation properties.","HiveNAS was designed with a
modular and reusable pattern to encourage further research; from the dataset to the evaluation strat-
egy, every component in the pipeline can be trivially substituted if they support particular endpoints
(delineated thoroughly in the framework’s documentation1).","Although HiveNAS was developed strictly as a NAS framework, a few numerical benchmarks are
provided to test the integrity of ABC (independent of the NAS components) and to empirically
deduce the optimal conﬁguration parameters for the optimization algorithm.",2022-11-18 14:11:47+00:00,HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization,cs.NE,"['cs.NE', 'cs.AI', 'cs.LG', 'I.2.0; G.1.6; I.2.2; G.3; I.2.11']","[arxiv.Result.Author('Mohamed Shahawy'), arxiv.Result.Author('Elhadj Benkhelifa')]","The traditional Neural Network-development process requires substantial
expert knowledge and relies heavily on intuition and trial-and-error. Neural
Architecture Search (NAS) frameworks were introduced to robustly search for
network topologies, as well as facilitate the automated development of Neural
Networks. While some optimization approaches -- such as Genetic Algorithms --
have been extensively explored in the NAS context, other Metaheuristic
Optimization algorithms have not yet been evaluated. In this paper, we propose
HiveNAS, the first Artificial Bee Colony-based NAS framework."
138,138.0,"The counter
column has not been used in training or testing of the models.",Both decisions are open for further research.,"3) Segmentation: This operation accepts a series of records

and outputs a series divided in blocks of the given size.",2022-11-18 07:27:07+00:00,Intrusion Detection in Internet of Things using Convolutional Neural Networks,cs.CR,"['cs.CR', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Martin Kodys'), arxiv.Result.Author('Zhi Lu'), arxiv.Result.Author('Kar Wai Fok'), arxiv.Result.Author('Vrizlynn L. L. Thing')]","Internet of Things (IoT) has become a popular paradigm to fulfil needs of the
industry such as asset tracking, resource monitoring and automation. As
security mechanisms are often neglected during the deployment of IoT devices,
they are more easily attacked by complicated and large volume intrusion attacks
using advanced techniques. Artificial Intelligence (AI) has been used by the
cyber security community in the past decade to automatically identify such
attacks. However, deep learning methods have yet to be extensively explored for
Intrusion Detection Systems (IDS) specifically for IoT. Most recent works are
based on time sequential models like LSTM and there is short of research in
CNNs as they are not naturally suited for this problem. In this article, we
propose a novel solution to the intrusion attacks against IoT devices using
CNNs. The data is encoded as the convolutional operations to capture the
patterns from the sensors data along time that are useful for attacks detection
by CNNs. The proposed method is integrated with two classical CNNs: ResNet and
EfficientNet, where the detection performance is evaluated. The experimental
results show significant improvement in both true positive rate and false
positive rate compared to the baseline using LSTM."
139,139.0,"Another possible approach could be to modify
LLMs to accept two parameters – instruction (safe) and data (unsafe) – and avoid following any
instructions from the unsafe data parameters [27].","While a solution to these attacks remains open, our ﬁndings demonstrate the difﬁculty of defending
against them and highlight the need for further research and discussion on the subject.","We hope that
our framework support researchers answer these questions, and ultimately reduce AI risks as we
discuss in Appendix A.",2022-11-17 13:43:20+00:00,Ignore Previous Prompt: Attack Techniques For Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Fábio Perez'), arxiv.Result.Author('Ian Ribeiro')]","Transformer-based large language models (LLMs) provide a powerful foundation
for natural language tasks in large-scale customer-facing applications.
However, studies that explore their vulnerabilities emerging from malicious
user interaction are scarce. By proposing PromptInject, a prosaic alignment
framework for mask-based iterative adversarial prompt composition, we examine
how GPT-3, the most widely deployed language model in production, can be easily
misaligned by simple handcrafted inputs. In particular, we investigate two
types of attacks -- goal hijacking and prompt leaking -- and demonstrate that
even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit
GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject
is available at https://github.com/agencyenterprise/PromptInject."
140,140.0,"(2020)
to measure the validity of feature ranking methods.","By
interrogating and measuring the disagreement between ex-
plainability methods, we hope to motivate further research
on explanation methods that evaluate properties such as
those introduced in Agarwal et al.","(2022) (e.g., faithful-
ness, stability, and fairness).",2022-11-16 14:45:16+00:00,Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'physics.ao-ph', 'stat.AP']","[arxiv.Result.Author('Montgomery Flora'), arxiv.Result.Author('Corey Potvin'), arxiv.Result.Author('Amy McGovern'), arxiv.Result.Author('Shawn Handler')]","With increasing interest in explaining machine learning (ML) models, the
first part of this two-part study synthesizes recent research on methods for
explaining global and local aspects of ML models. This study distinguishes
explainability from interpretability, local from global explainability, and
feature importance versus feature relevance. We demonstrate and visualize
different explanation methods, how to interpret them, and provide a complete
Python package (scikit-explain) to allow future researchers to explore these
products. We also highlight the frequent disagreement between explanation
methods for feature rankings and feature effects and provide practical advice
for dealing with these disagreements. We used ML models developed for severe
weather prediction and sub-freezing road surface temperature prediction to
generalize the behavior of the different explanation methods. For feature
rankings, there is substantially more agreement on the set of top features
(e.g., on average, two methods agree on 6 of the top 10 features) than on
specific rankings (on average, two methods only agree on the ranks of 2-3
features in the set of top 10 features). On the other hand, two feature effect
curves from different methods are in high agreement as long as the phase space
is well sampled. Finally, a lesser-known method, tree interpreter, was found
comparable to SHAP for feature effects, and with the widespread use of random
forests in geosciences and computational ease of tree interpreter, we recommend
it be explored in future research."
141,141.0,"Recall that FSP measures the importance of a
feature when all other features are permuted, which breaks
up any feature interactions or correlations.","The similarity
between SAGE and FSP suggests that SAGE may be ig-
noring feature interactions, but further research is required
to test this hypothesis.","Lastly, the TI method had the
greatest rank agreement with Gini importance (GIN, 0.7),
which can poorly assign rank when features are correlated
(Strobl et al.",2022-11-16 14:45:16+00:00,Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'physics.ao-ph', 'stat.AP']","[arxiv.Result.Author('Montgomery Flora'), arxiv.Result.Author('Corey Potvin'), arxiv.Result.Author('Amy McGovern'), arxiv.Result.Author('Shawn Handler')]","With increasing interest in explaining machine learning (ML) models, the
first part of this two-part study synthesizes recent research on methods for
explaining global and local aspects of ML models. This study distinguishes
explainability from interpretability, local from global explainability, and
feature importance versus feature relevance. We demonstrate and visualize
different explanation methods, how to interpret them, and provide a complete
Python package (scikit-explain) to allow future researchers to explore these
products. We also highlight the frequent disagreement between explanation
methods for feature rankings and feature effects and provide practical advice
for dealing with these disagreements. We used ML models developed for severe
weather prediction and sub-freezing road surface temperature prediction to
generalize the behavior of the different explanation methods. For feature
rankings, there is substantially more agreement on the set of top features
(e.g., on average, two methods agree on 6 of the top 10 features) than on
specific rankings (on average, two methods only agree on the ranks of 2-3
features in the set of top 10 features). On the other hand, two feature effect
curves from different methods are in high agreement as long as the phase space
is well sampled. Finally, a lesser-known method, tree interpreter, was found
comparable to SHAP for feature effects, and with the widespread use of random
forests in geosciences and computational ease of tree interpreter, we recommend
it be explored in future research."
142,142.0,"Since the optimal solution to the 0-1 knapsack
problem is always an IMS, the bounds derived in Subsection 3.3 can also be added to the problem without changing
the optimum and this could be interesting to obtain strong relaxations.","We encourage further research to investigate
which relaxations would work best (e.g.","linear relaxations, surrogate relaxations or Lagrangian-based relaxations) and
how these relaxations could be efﬁciently solved.",2022-11-16 12:48:35+00:00,Features for the 0-1 knapsack problem based on inclusionwise maximal solutions,cs.DS,"['cs.DS', 'cs.AI']","[arxiv.Result.Author('Jorik Jooken'), arxiv.Result.Author('Pieter Leyman'), arxiv.Result.Author('Patrick De Causmaecker')]","Decades of research on the 0-1 knapsack problem led to very efficient
algorithms that are able to quickly solve large problem instances to
optimality. This prompted researchers to also investigate whether relatively
small problem instances exist that are hard for existing solvers and
investigate which features characterize their hardness. Previously the authors
proposed a new class of hard 0-1 knapsack problem instances and demonstrated
that the properties of so-called inclusionwise maximal solutions (IMSs) can be
important hardness indicators for this class. In the current paper, we
formulate several new computationally challenging problems related to the IMSs
of arbitrary 0-1 knapsack problem instances. Based on generalizations of
previous work and new structural results about IMSs, we formulate polynomial
and pseudopolynomial time algorithms for solving these problems. From this we
derive a set of 14 computationally expensive features, which we calculate for
two large datasets on a supercomputer in approximately 540 CPU-hours. We show
that the proposed features contain important information related to the
empirical hardness of a problem instance that was missing in earlier features
from the literature by training machine learning models that can accurately
predict the empirical hardness of a wide variety of 0-1 knapsack problem
instances. Using the instance space analysis methodology, we also show that
hard 0-1 knapsack problem instances are clustered together around a relatively
dense region of the instance space and several features behave differently in
the easy and hard parts of the instance space."
143,143.0,"We do not see any immediate ad-
verse effects that our methodology and dataset can
lead to.","On the contrary, further research into CS
from an NLP context can only provide beneﬁts to
people with cognitive disabilities.","Other Considerations Gooding (2022) recently
presented multiple different ethical considerations
for text simpliﬁcation research.",2022-11-16 10:51:03+00:00,Cognitive Simplification Operations Improve Text Simplification,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Eytan Chamovitz'), arxiv.Result.Author('Omri Abend')]","Text Simplification (TS) is the task of converting a text into a form that is
easier to read while maintaining the meaning of the original text. A sub-task
of TS is Cognitive Simplification (CS), converting text to a form that is
readily understood by people with cognitive disabilities without rendering it
childish or simplistic. This sub-task has yet to be explored with neural
methods in NLP, and resources for it are scarcely available. In this paper, we
present a method for incorporating knowledge from the cognitive accessibility
domain into a TS model, by introducing an inductive bias regarding what
simplification operations to use. We show that by adding this inductive bias to
a TS-trained model, it is able to adapt better to CS without ever seeing CS
data, and outperform a baseline model on a traditional TS benchmark. In
addition, we provide a novel test dataset for CS, and analyze the differences
between CS corpora and existing TS corpora, in terms of how simplification
operations are applied."
144,144.0,"Any
one of these systems could be used as well for CS,
and such a comparison is warranted.","The goal of
this paper however is to highlight the need and
possibilities of further research into CS, and pro-
vide initial benchmarks and tools to do so.","We do
not presume that our methodology of adding sim-
pliﬁcation operations is the best methodology for
CS.",2022-11-16 10:51:03+00:00,Cognitive Simplification Operations Improve Text Simplification,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Eytan Chamovitz'), arxiv.Result.Author('Omri Abend')]","Text Simplification (TS) is the task of converting a text into a form that is
easier to read while maintaining the meaning of the original text. A sub-task
of TS is Cognitive Simplification (CS), converting text to a form that is
readily understood by people with cognitive disabilities without rendering it
childish or simplistic. This sub-task has yet to be explored with neural
methods in NLP, and resources for it are scarcely available. In this paper, we
present a method for incorporating knowledge from the cognitive accessibility
domain into a TS model, by introducing an inductive bias regarding what
simplification operations to use. We show that by adding this inductive bias to
a TS-trained model, it is able to adapt better to CS without ever seeing CS
data, and outperform a baseline model on a traditional TS benchmark. In
addition, we provide a novel test dataset for CS, and analyze the differences
between CS corpora and existing TS corpora, in terms of how simplification
operations are applied."
145,145.0,"Additionally, this connection also offers a
new testbed for meta-exploration and meta-RL research.","As discussed in Section 2, while existing
meta-RL benchmarks tend to be either readily accessible or impactful and realistic, automatically
providing feedback simultaneously provides both, and we release code for a meta-RL wrapper of the
Bounce programming assignment to spur further research in this direction.","5 Experiments

In our experiments, we aim to answer ﬁve main questions: (1) How does automated feedback grading
accuracy compare to human grading accuracy?",2022-11-16 10:00:23+00:00,Giving Feedback on Interactive Student Programs with Meta-Exploration,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Evan Zheran Liu'), arxiv.Result.Author('Moritz Stephan'), arxiv.Result.Author('Allen Nie'), arxiv.Result.Author('Chris Piech'), arxiv.Result.Author('Emma Brunskill'), arxiv.Result.Author('Chelsea Finn')]","Developing interactive software, such as websites or games, is a particularly
engaging way to learn computer science. However, teaching and giving feedback
on such software is time-consuming -- standard approaches require instructors
to manually grade student-implemented interactive programs. As a result, online
platforms that serve millions, like Code.org, are unable to provide any
feedback on assignments for implementing interactive programs, which critically
hinders students' ability to learn. One approach toward automatic grading is to
learn an agent that interacts with a student's program and explores states
indicative of errors via reinforcement learning. However, existing work on this
approach only provides binary feedback of whether a program is correct or not,
while students require finer-grained feedback on the specific errors in their
programs to understand their mistakes. In this work, we show that exploring to
discover errors can be cast as a meta-exploration problem. This enables us to
construct a principled objective for discovering errors and an algorithm for
optimizing this objective, which provides fine-grained feedback. We evaluate
our approach on a set of over 700K real anonymized student programs from a
Code.org interactive assignment. Our approach provides feedback with 94.3%
accuracy, improving over existing approaches by 17.7% and coming within 1.5% of
human-level accuracy. Project web page: https://ezliu.github.io/dreamgrader."
146,146.0,"Future work involves investigat-
ing when or how easily a spurious feature could be
exploited, by exploring the relationship among the
minimum description length of a spurious feature
(Voita and Titov, 2020), the proportion of data-
points that contains the spurious feature, the choice
of RL algorithm, and the degree of the reward gam-
ing behavior.","Additionally, further research on anti-
gaming approaches is needed to fulﬁll the potential
of training text generators by learned rewards.","Acknowledgement

We thank Nitish Joshi, Nicholas Lourie, Chen Zhao,
and Sebastian Gehrmann for discussion and feed-
back.",2022-11-16 07:10:02+00:00,Reward Gaming in Conditional Text Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Richard Yuanzhe Pang'), arxiv.Result.Author('Vishakh Padmakumar'), arxiv.Result.Author('Thibault Sellam'), arxiv.Result.Author('Ankur P. Parikh'), arxiv.Result.Author('He He')]","To align conditional text generation model outputs with desired behaviors,
there has been an increasing focus on training the model using reinforcement
learning (RL) with reward functions learned from human annotations. Under this
framework, we identify three common cases where high rewards are incorrectly
assigned to undesirable patterns: noise-induced spurious correlation, naturally
occurring spurious correlation, and covariate shift. We show that even though
learned metrics achieve high performance on the distribution of the data used
to train the reward function, the undesirable patterns may be amplified during
RL training of the text generation model. While there has been discussion about
reward gaming in the RL or safety community, in this short discussion piece, we
would like to highlight reward gaming in the NLG community using concrete
conditional text generation examples and discuss potential fixes and areas for
future work."
147,147.0,"First, our reconstruc-
tion might still miss some small details in target point clouds.","Further research in improving the reconstruction quality is
thus worthwhile.","Second, the latent code of the SP-GAN is
not compact.",2022-11-16 06:29:29+00:00,PointInverter: Point Cloud Reconstruction and Editing via a Generative Model with Shape Priors,cs.CV,"['cs.CV', 'cs.AI', 'cs.GR']","[arxiv.Result.Author('Jaeyeon Kim'), arxiv.Result.Author('Binh-Son Hua'), arxiv.Result.Author('Duc Thanh Nguyen'), arxiv.Result.Author('Sai-Kit Yeung')]","In this paper, we propose a new method for mapping a 3D point cloud to the
latent space of a 3D generative adversarial network. Our generative model for
3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point
cloud generator. We derive an efficient way to encode an input 3D point cloud
to the latent space of the SP-GAN. Our point cloud encoder can resolve the
point ordering issue during inversion, and thus can determine the
correspondences between points in the generated 3D point cloud and those in the
canonical sphere used by the generator. We show that our method outperforms
previous GAN inversion methods for 3D point clouds, achieving state-of-the-art
results both quantitatively and qualitatively. Our code is available at
https://github.com/hkust-vgd/point_inverter."
148,148.0,"Our work is the ﬁrst to explore the combination of attention mechanism and ambiguity by
deep neural networks.","For further research, we plan to provide experimental results to validate
the effectiveness of our models based on the attention-ambiguity mechanism.","In addition, we plan
to extend the GAMMT framework to vision, audio, reinforcement learning and other domains in
the future works.",2022-11-16 06:24:26+00:00,GAMMT: Generative Ambiguity Modeling Using Multiple Transformers,cs.LG,"['cs.LG', 'cs.AI', 'math.PR']",[arxiv.Result.Author('Xingcheng Xu')],"We introduce a new model based on sets of probabilities for sequential data.
We name the model GAMMT, which stands for Generative Ambiguity Models using
Multiple Transformers. We suppose that data generating process of a sequence is
ambiguous and determined by a set of probabilities rather than one as in the
conventional model. We use multiple parallel transformers connected by a
selection mechanism to approximate ambiguous probabilities. The GAMMT allows
for ambiguity modeling in a generative way and multiple representations of the
input tokens and the input sequence. This work explores the combination of
attention mechanism and ambiguity by deep neural networks. We expect that this
framework will facilitate new research into machine learning, improving our
understanding of the attention-ambiguity mechanism."
149,149.0,"Moreover, terms on levels 2 and 3 do not follow logical reasoning that could
make them understandable to any human or machine, as demonstrated by Anthony E Moss in [18].","Further research has proven that most people would use ""simple"" color terms instead, and only a small portion would
use non-basic terms [19].","These limitations are maintained throughout different cultures and languages [12, 20, 21, 22,
23, 24].",2022-11-15 19:26:51+00:00,ABANICCO: A New Color Space for Multi-Label Pixel Classification and Color Segmentation,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Laura Nicolás-Sáenz'), arxiv.Result.Author('Agapito Ledezma'), arxiv.Result.Author('Javier Pascau'), arxiv.Result.Author('Arrate Muñoz-Barrutia')]","In any computer vision task involving color images, a necessary step is
classifying pixels according to color and segmenting the respective areas.
However, the development of methods able to successfully complete this task has
proven challenging, mainly due to the gap between human color perception,
linguistic color terms, and digital representation. In this paper, we propose a
novel method combining geometric analysis of color theory, fuzzy color spaces,
and multi-label systems for the automatic classification of pixels according to
12 standard color categories (Green, Yellow, Light Orange, Deep Orange, Red,
Pink, Purple, Ultramarine, Blue, Teal, Brown, and Neutral). Moreover, we
present a robust, unsupervised, unbiased strategy for color naming based on
statistics and color theory. ABANICCO was tested against the state of the art
in color classification and with the standarized ISCC-NBS color system,
providing accurate classification and a standard, easily understandable
alternative for hue naming recognizable by humans and machines. We expect this
solution to become the base to successfully tackle a myriad of problems in all
fields of computer vision, such as region characterization, histopathology
analysis, fire detection, product quality prediction, object description, and
hyperspectral imaging."
150,150.0,"Thus, using a
uniﬁed feature transfer strategy, either linear probe or ﬁne-
tuning, for the features learned by different SSFL methods
may result in the ineffective transfer or even negative transfer,
and consequently weaken the generalization performance of
the model.","Therefore, further research should be done on self-
supervised learning feature transfer methods.","For example,
1) propose criteria to measure the correlation between self-
supervised learned features and downstream tasks, and then
design feature transfer methods according to their relationship.",2022-11-15 13:32:22+00:00,"Self-supervised remote sensing feature learning: Learning Paradigms, Challenges, and Future Works",cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Chao Tao'), arxiv.Result.Author('Ji Qi'), arxiv.Result.Author('Mingning Guo'), arxiv.Result.Author('Qing Zhu'), arxiv.Result.Author('Haifeng Li')]","Deep learning has achieved great success in learning features from massive
remote sensing images (RSIs). To better understand the connection between
feature learning paradigms (e.g., unsupervised feature learning (USFL),
supervised feature learning (SFL), and self-supervised feature learning
(SSFL)), this paper analyzes and compares them from the perspective of feature
learning signals, and gives a unified feature learning framework. Under this
unified framework, we analyze the advantages of SSFL over the other two
learning paradigms in RSIs understanding tasks and give a comprehensive review
of the existing SSFL work in RS, including the pre-training dataset,
self-supervised feature learning signals, and the evaluation methods. We
further analyze the effect of SSFL signals and pre-training data on the learned
features to provide insights for improving the RSI feature learning. Finally,
we briefly discuss some open problems and possible research directions."
151,151.0,"This can be done analytically or more likely by using
simulation-based methods such as particle ﬁltering.","Testing this approach com-
putationally is an interesting subject for further research, either in the context
of Wordle, or more generally in the context of the adaptive control problem of
Section 4.","An important direction for further research is the use of our methodology
in automated planning.",2022-11-15 03:46:41+00:00,Reinforcement Learning Methods for Wordle: A POMDP/Adaptive Control Approach,cs.AI,['cs.AI'],"[arxiv.Result.Author('Siddhant Bhambri'), arxiv.Result.Author('Amrita Bhattacharjee'), arxiv.Result.Author('Dimitri Bertsekas')]","In this paper we address the solution of the popular Wordle puzzle, using new
reinforcement learning methods, which apply more generally to adaptive control
of dynamic systems and to classes of Partially Observable Markov Decision
Process (POMDP) problems. These methods are based on approximation in value
space and the rollout approach, admit a straightforward implementation, and
provide improved performance over various heuristic approaches. For the Wordle
puzzle, they yield on-line solution strategies that are very close to optimal
at relatively modest computational cost. Our methods are viable for more
complex versions of Wordle and related search problems, for which an optimal
strategy would be impossible to compute. They are also applicable to a wide
range of adaptive sequential decision problems that involve an unknown or
frequently changing environment whose parameters are estimated on-line."
152,152.0,"Testing this approach com-
putationally is an interesting subject for further research, either in the context
of Wordle, or more generally in the context of the adaptive control problem of
Section 4.","An important direction for further research is the use of our methodology
in automated planning.","For example, special types of POMDP involving a
fully observable state component, and a constant partially observable compo-
nent, have been investigated in a number of works on planning.",2022-11-15 03:46:41+00:00,Reinforcement Learning Methods for Wordle: A POMDP/Adaptive Control Approach,cs.AI,['cs.AI'],"[arxiv.Result.Author('Siddhant Bhambri'), arxiv.Result.Author('Amrita Bhattacharjee'), arxiv.Result.Author('Dimitri Bertsekas')]","In this paper we address the solution of the popular Wordle puzzle, using new
reinforcement learning methods, which apply more generally to adaptive control
of dynamic systems and to classes of Partially Observable Markov Decision
Process (POMDP) problems. These methods are based on approximation in value
space and the rollout approach, admit a straightforward implementation, and
provide improved performance over various heuristic approaches. For the Wordle
puzzle, they yield on-line solution strategies that are very close to optimal
at relatively modest computational cost. Our methods are viable for more
complex versions of Wordle and related search problems, for which an optimal
strategy would be impossible to compute. They are also applicable to a wide
range of adaptive sequential decision problems that involve an unknown or
frequently changing environment whose parameters are estimated on-line."
153,153.0,"Multiple participants are invited
to perform this game, and the average accuracy of detect-
ing the repeated images is quantitated as HumanMem score
assigned to the images.","Further research in this line [10,
54, 86] have constructed more datasets [6, 25, 32, 49, 57]
and developed stronger methods to predict HumanMem
scores [12, 26, 45–47, 49, 49, 50, 56, 57, 65, 66].","One of our
goals in this work is to compare machine memory and hu-
man memory, thus we keep the deﬁnition of MachineMem
score identical to HumanMem score.",2022-11-14 18:48:08+00:00,What Images are More Memorable to Machines?,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Junlin Han'), arxiv.Result.Author('Huangying Zhan'), arxiv.Result.Author('Jie Hong'), arxiv.Result.Author('Pengfei Fang'), arxiv.Result.Author('Hongdong Li'), arxiv.Result.Author('Lars Petersson'), arxiv.Result.Author('Ian Reid')]","This paper studies the problem of measuring and predicting how memorable an
image is to pattern recognition machines, as a path to explore machine
intelligence. Firstly, we propose a self-supervised machine memory
quantification pipeline, dubbed ``MachineMem measurer'', to collect machine
memorability scores of images. Similar to humans, machines also tend to
memorize certain kinds of images, whereas the types of images that machines and
humans memorialize are different. Through in-depth analysis and comprehensive
visualizations, we gradually unveil that ""complex"" images are usually more
memorable to machines. We further conduct extensive experiments across 11
different machines (from linear classifiers to modern ViTs) and 9 pre-training
methods to analyze and understand machine memory. This work proposes the
concept of machine memorability and opens a new research direction at the
interface between machine memory and visual data."
154,154.0,"While the system can gen-

erally respond to a simple question based on a single knowledge snippet, it often

makes mistakes given a context that requires sophisticated reasoning [102].","These

issues call for further research in the future.","58

Chapter 5

Improving the computational

eﬃciency of large-scale response

retrieval

Large neural networks have been the state-of-the-art machine learning models in

recent years.",2022-11-14 17:27:07+00:00,From Knowledge Augmentation to Multi-tasking: Towards Human-like Dialogue Systems,cs.AI,"['cs.AI', 'cs.CL', 'cs.LG']",[arxiv.Result.Author('Tom Young')],"The goal of building dialogue agents that can converse with humans naturally
has been a long-standing dream of researchers since the early days of
artificial intelligence. The well-known Turing Test proposed to judge the
ultimate validity of an artificial intelligence agent on the
indistinguishability of its dialogues from humans'. It should come as no
surprise that human-level dialogue systems are very challenging to build. But,
while early effort on rule-based systems found limited success, the emergence
of deep learning enabled great advance on this topic.
  In this thesis, we focus on methods that address the numerous issues that
have been imposing the gap between artificial conversational agents and
human-level interlocutors. These methods were proposed and experimented with in
ways that were inspired by general state-of-the-art AI methodologies. But they
also targeted the characteristics that dialogue systems possess."
155,155.0,"Considering the
high annotation cost, our character relation anno-
tation works are restricted to Harry Potter.","These
concerns warrant further research and considera-
tion when utilizing this work to build intelligent
person-like dialogue systems in the virtual world.","References

Nuo Chen, Chenyu You, and Yuexian Zou.",2022-11-13 10:16:39+00:00,What would Harry say? Building Dialogue Agents for Characters in a Story,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Nuo Chen'), arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Haiyun Jiang'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Ziyang Chen'), arxiv.Result.Author('Longyue Wang'), arxiv.Result.Author('Jia Li')]","We have a Christmas gift for Harry Potter fans all over the world. In this
paper, we present Harry Potter Dialogue (HPD), a dataset that helps train Harry
Potter-like dialogue agents. Such a task is typically viewed as a variant of
personalized dialogue agents, but they differ significantly in three respects:
1) Harry lived in a virtual world of wizards, thus, real-world commonsense may
not apply to Harry's conversations; 2) Harry's behavior is strongly linked to
background information in conversations: the scene, its attributes and its
relationship to other speakers; and 3) Such backgrounds are dynamically altered
as the storyline goes on. The HPD dataset, as the first dataset to facilitate
the study of dialogue agent construction for characters within a story,
provides rich contextual information about each dialogue session such as
scenes, character attributes, and relations. More importantly, all the
background information will change over the course of the story. In addition,
HPD could support both dialogue generation and retrieval tasks. We evaluate
baselines such as Dialog-GPT and BOB to determine the extent to which they can
generate Harry Potter-like responses. The experimental results disappoint us in
that although the generated responses are fluent, they still seem out of
character for Harry. Besides, we validate the current most robust dialogue
agent, ChatGPT, which also can't generate plausible Harry-Potter-like responses
in some cases, either. Our results suggest that there is much scope for future
research."
156,156.0,"Novel technologies such as HE are
accelerated by community contributions.","It is hoped that the codebase released with
this paper will be valuable for further research.","1.2 Approach

The approach of the paper is to create HE implementations of CC fraud detectors and
make latency and storage comparisons with their original plaintext versions.",2022-11-12 14:28:17+00:00,Privacy-Preserving Credit Card Fraud Detection using Homomorphic Encryption,cs.CR,"['cs.CR', 'cs.AI']",[arxiv.Result.Author('David Nugent')],"Credit card fraud is a problem continuously faced by financial institutions
and their customers, which is mitigated by fraud detection systems. However,
these systems require the use of sensitive customer transaction data, which
introduces both a lack of privacy for the customer and a data breach
vulnerability to the card provider. This paper proposes a system for private
fraud detection on encrypted transactions using homomorphic encryption. Two
models, XGBoost and a feedforward classifier neural network, are trained as
fraud detectors on plaintext data. They are then converted to models which use
homomorphic encryption for private inference. Latency, storage, and detection
results are discussed, along with use cases and feasibility of deployment. The
XGBoost model has better performance, with an encrypted inference as low as
6ms, compared to 296ms for the neural network. However, the neural network
implementation may still be preferred, as it is simpler to deploy securely. A
codebase for the system is also provided, for simulation and further
development."
157,157.0,"[129], Rudin and Carlson [174] point out that sometimes we
rely too much on deep learning methods, for some specific tasks simple models can be considered
and can also receive an excellent performance without the intransparency of deep learning models
that may lose the generalizability.","More further research should be conducted to decide how to
maintain the balance of explainability and performance.","7 CONCLUSION
Explainability has attracted increasing attention in the RL community due to practical, safe, and
trustworthy concerns.",2022-11-12 13:52:06+00:00,"A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yunpeng Qing'), arxiv.Result.Author('Shunyu Liu'), arxiv.Result.Author('Jie Song'), arxiv.Result.Author('Huiqiong Wang'), arxiv.Result.Author('Mingli Song')]","Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and performance of agents while this
kind of method is often ignored in XRL field. Some challenges and opportunities
in XRL are discussed. This survey intends to provide a high-level summarization
of XRL and to motivate future research on more effective XRL solutions.
Corresponding open source codes are collected and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning."
158,158.0,4.,"Challenges and Opportunities

To more systematically use capabilities, further research
is needed.","We argue that ML engineering can gener-
ally benefit from software engineering disciplines, with
principles from requirements engineering and software
testing in particular.",2022-11-11 18:50:21+00:00,Capabilities for Better ML Engineering,cs.AI,"['cs.AI', 'cs.SE']","[arxiv.Result.Author('Chenyang Yang'), arxiv.Result.Author('Rachel Brower-Sinning'), arxiv.Result.Author('Grace A. Lewis'), arxiv.Result.Author('Christian Kästner'), arxiv.Result.Author('Tongshuang Wu')]","In spite of machine learning's rapid growth, its engineering support is
scattered in many forms, and tends to favor certain engineering stages,
stakeholders, and evaluation preferences. We envision a capability-based
framework, which uses fine-grained specifications for ML model behaviors to
unite existing efforts towards better ML engineering. We use concrete scenarios
(model design, debugging, and maintenance) to articulate capabilities' broad
applications across various different dimensions, and their impact on building
safer, more generalizable and more trustworthy models that reflect human needs.
Through preliminary experiments, we show capabilities' potential for reflecting
model generalizability, which can provide guidance for ML engineering process.
We discuss challenges and opportunities for capabilities' integration into ML
engineering."
159,159.0,"Third, WER and CER
quantify ASR performance, but identifying
the nature of the errors concerned is crit-
ical to dementia screening and monitoring.","Thus further research is needed to study the
source(s) of errors and their eﬀects on this
task.","Lastly,
it should be noted that all
of the ASR models we tested in this study
were pre-trained on read speech, whose na-
ture is very diﬀerent from the spontaneous
speech (Kiss and Vicsi, 2017; Howell and
Kadi-Haniﬁ, 1991).",2022-11-11 17:06:45+00:00,The Far Side of Failure: Investigating the Impact of Speech Recognition Errors on Subsequent Dementia Classification,eess.AS,"['eess.AS', 'cs.AI', 'cs.CL', 'cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Changye Li'), arxiv.Result.Author('Trevor Cohen'), arxiv.Result.Author('Serguei Pakhomov')]","Linguistic anomalies detectable in spontaneous speech have shown promise for
various clinical applications including screening for dementia and other forms
of cognitive impairment. The feasibility of deploying automated tools that can
classify language samples obtained from speech in large-scale clinical settings
depends on the ability to capture and automatically transcribe the speech for
subsequent analysis. However, the impressive performance of self-supervised
learning (SSL) automatic speech recognition (ASR) models with curated speech
data is not apparent with challenging speech samples from clinical settings.
One of the key questions for successfully applying ASR models for clinical
applications is whether imperfect transcripts they generate provide sufficient
information for downstream tasks to operate at an acceptable level of accuracy.
In this study, we examine the relationship between the errors produced by
several deep learning ASR systems and their impact on the downstream task of
dementia classification. One of our key findings is that, paradoxically, ASR
systems with relatively high error rates can produce transcripts that result in
better downstream classification accuracy than classification based on verbatim
transcripts."
160,160.0,"Our experiments indicated that the
graph representation in which the spatial connectivity and the
UMAP dimensionality reduction are combined produces the
best result.","Further research will focus on studying the impact
of adaptative stain-speciﬁc normalisation techniques and the
size of neighbourhoods in the overall classiﬁcation capabili-
ties of pipelines like the one discussed in this work.",6.,2022-11-10 21:28:53+00:00,Employing Graph Representations for Cell-level Characterization of Melanoma MELC Samples,cs.CV,"['cs.CV', 'cs.AI', 'cs.CE']","[arxiv.Result.Author('Luis Carlos Rivera Monroy'), arxiv.Result.Author('Leonhard Rist'), arxiv.Result.Author('Martin Eberhardt'), arxiv.Result.Author('Christian Ostalecki'), arxiv.Result.Author('Andreas Baur'), arxiv.Result.Author('Julio Vera'), arxiv.Result.Author('Katharina Breininger'), arxiv.Result.Author('Andreas Maier')]","Histopathology imaging is crucial for the diagnosis and treatment of skin
diseases. For this reason, computer-assisted approaches have gained popularity
and shown promising results in tasks such as segmentation and classification of
skin disorders. However, collecting essential data and sufficiently
high-quality annotations is a challenge. This work describes a pipeline that
uses suspected melanoma samples that have been characterized using
Multi-Epitope-Ligand Cartography (MELC). This cellular-level tissue
characterisation is then represented as a graph and used to train a graph
neural network. This imaging technology, combined with the methodology proposed
in this work, achieves a classification accuracy of 87%, outperforming existing
approaches by 10%."
161,161.0,"This functionality could be used in fur-
ther research regarding embodiment and proprioception.","5 Technical challenges & considera-

tions

While implementing our demonstration, we encountered
several technical challenges in need of further research.","Unresolved problems diminish sense of agency of the
player, or the experience of controlling one’s own actions
and their corresponding effects in the environment.",2022-11-10 21:13:04+00:00,Steps towards prompt-based creation of virtual worlds,cs.HC,"['cs.HC', 'cs.AI', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Jasmine Roberts'), arxiv.Result.Author('Andrzej Banburski-Fahey'), arxiv.Result.Author('Jaron Lanier')]","Large language models trained for code generation can be applied to speaking
virtual worlds into existence (creating virtual worlds). In this work we show
that prompt-based methods can both accelerate in-VR level editing, as well as
can become part of gameplay rather than just part of game development. As an
example, we present Codex VR Pong which shows non-deterministic game mechanics
using generative processes to not only create static content but also
non-trivial interactions between 3D objects. This demonstration naturally leads
to an integral discussion on how one would evaluate and benchmark experiences
created by generative models - as there are no qualitative or quantitative
metrics that apply in these scenarios. We conclude by discussing impending
challenges of AI-assisted co-creation in VR."
162,162.0,"We benchmark our topology agent on a 118 substation power grid and demonstrate how
to successfully combine it with traditional congestion management measures.","Finally, we discuss
the practical relevance of such an agent using the examples of day-ahead planning and real-time
remedial action recommendation, and outline promising further research directions.","2 Environment Design and Interaction Workﬂow

This section describes the power grid environment, its observation and action spaces, reward as well
as custom modules.",2022-11-10 14:39:28+00:00,Power Grid Congestion Management via Topology Optimization with AlphaZero,cs.AI,"['cs.AI', 'cs.LG']","[arxiv.Result.Author('Matthias Dorfer'), arxiv.Result.Author('Anton R. Fuxjäger'), arxiv.Result.Author('Kristian Kozak'), arxiv.Result.Author('Patrick M. Blies'), arxiv.Result.Author('Marcel Wasserer')]","The energy sector is facing rapid changes in the transition towards clean
renewable sources. However, the growing share of volatile, fluctuating
renewable generation such as wind or solar energy has already led to an
increase in power grid congestion and network security concerns. Grid operators
mitigate these by modifying either generation or demand (redispatching,
curtailment, flexible loads). Unfortunately, redispatching of fossil generators
leads to excessive grid operation costs and higher emissions, which is in
direct opposition to the decarbonization of the energy sector. In this paper,
we propose an AlphaZero-based grid topology optimization agent as a non-costly,
carbon-free congestion management alternative. Our experimental evaluation
confirms the potential of topology optimization for power grid operation,
achieves a reduction of the average amount of required redispatching by 60%,
and shows the interoperability with traditional congestion management methods.
Our approach also ranked 1st in the WCCI 2022 Learning to Run a Power Network
(L2RPN) competition. Based on our findings, we identify and discuss open
research problems as well as technical challenges for a productive system on a
real power grid."
163,163.0,"We also
analyze how important the phrasing of the verbose
forms are and how many samples are needed to
get good quantitative performance.","We hope that
this work on using sample efﬁcient LLMs serves to
motivate further research in making ToD systems
simpler and quicker to develop.","Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov.",2022-11-10 14:16:00+00:00,Prompt Learning for Domain Adaptation in Task-Oriented Dialogue,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Makesh Narsimhan Sreedhar'), arxiv.Result.Author('Christopher Parisien')]","Conversation designers continue to face significant obstacles when creating
production quality task-oriented dialogue systems. The complexity and cost
involved in schema development and data collection is often a major barrier for
such designers, limiting their ability to create natural, user-friendly
experiences. We frame the classification of user intent as the generation of a
canonical form, a lightweight semantic representation using natural language.
We show that canonical forms offer a promising alternative to traditional
methods for intent classification. By tuning soft prompts for a frozen large
language model, we show that canonical forms generalize very well to new,
unseen domains in a zero- or few-shot setting. The method is also
sample-efficient, reducing the complexity and effort of developing new
task-oriented dialogue domains."
164,164.0,"Conservative estimates suggest that global
datacenter energy consumption between 2010 and 2018 went up by 6%, totaling
205 TWh in 2018.","Further research [7] implies that the datacenter energy con-
sumption is an order of magnitude higher than the estimated 6%, considering

 
 
 
 
 
 
2

Vanamala Venkataswamy, Jake Grigsby, Andrew Grimshaw, and Yanjun Qi

numerous unaccounted small-to-medium scale datacenters and datacenters that
cater to new technologies (e.g., blockchain, cryptocurrency mining).","Datacenters
in the U.S consume 1.8% of the total electricity; electricity predominantly gen-
erated using non-renewable sources emitting an estimated ∼ 230 Million Metric
tons of greenhouse gases every year.",2022-11-10 05:17:14+00:00,RARE: Renewable Energy Aware Resource Management in Datacenters,cs.DC,"['cs.DC', 'cs.AI']","[arxiv.Result.Author('Vanamala Venkataswamy'), arxiv.Result.Author('Jake Grigsby'), arxiv.Result.Author('Andrew Grimshaw'), arxiv.Result.Author('Yanjun Qi')]","The exponential growth in demand for digital services drives massive
datacenter energy consumption and negative environmental impacts. Promoting
sustainable solutions to pressing energy and digital infrastructure challenges
is crucial. Several hyperscale cloud providers have announced plans to power
their datacenters using renewable energy. However, integrating renewables to
power the datacenters is challenging because the power generation is
intermittent, necessitating approaches to tackle power supply variability. Hand
engineering domain-specific heuristics-based schedulers to meet specific
objective functions in such complex dynamic green datacenter environments is
time-consuming, expensive, and requires extensive tuning by domain experts. The
green datacenters need smart systems and system software to employ multiple
renewable energy sources (wind and solar) by intelligently adapting computing
to renewable energy generation. We present RARE (Renewable energy Aware
REsource management), a Deep Reinforcement Learning (DRL) job scheduler that
automatically learns effective job scheduling policies while continually
adapting to datacenters' complex dynamic environment. The resulting DRL
scheduler performs better than heuristic scheduling policies with different
workloads and adapts to the intermittent power supply from renewables. We
demonstrate DRL scheduler system design parameters that, when tuned correctly,
produce better performance. Finally, we demonstrate that the DRL scheduler can
learn from and improve upon existing heuristic policies using Offline Learning."
165,165.0,"All the time we
use early stopping to prevent over-ﬁtting.","We are planning to release all of our codes and data splits
publicly upon the acceptance of the paper for reproducibility
of our results and to stem further research in the area.","1D Conv, 1x3, filters=k,BN, ReLUMax Pool, 1x4Dropouts, rate=0.1ResNet-Style Block,filters = kGlobal Max PoolFeature Vector1D Conv, 1x3, filters=k,BN, ReLU1D Conv, 1x3, filters=k,BN, ReLU1D Conv, 1x3, filters=k+1D Conv, 1x1, filters=k BN, ReLUDense, 2048, BN,ReLUDense, 512, BNProjectorDense, 8196, BN,ReLUDense, 8196, BN,ReLUPredictorDense, 8196, BN,ReLUDense, 4096, BN,ReLUDense, 2048, BN,ReLUDense, 5128

Augmentation

Description

1
2
3
4

5
6
7
8
9
10

Jitter
Random Scaling
Magnitude Warping
Time Warping

Flipping
Data Dropping
Random Sampling
Permutations
Negation
Channel Shufﬂing

Adding random noise to a sample
Scaling each channel of the input with a randomly generated constant
Random element-wise scaling with a smooth transition along time dimension
Stretches the data across time dimension.",2022-10-24 05:56:32+00:00,Non-Contrastive Learning-based Behavioural Biometrics for Smart IoT Devices,cs.CR,"['cs.CR', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Oshan Jayawardana'), arxiv.Result.Author('Fariza Rashid'), arxiv.Result.Author('Suranga Seneviratne')]","Behaviour biometrics are being explored as a viable alternative to overcome
the limitations of traditional authentication methods such as passwords and
static biometrics. Also, they are being considered as a viable authentication
method for IoT devices such as smart headsets with AR/VR capabilities,
wearables, and erables, that do not have a large form factor or the ability to
seamlessly interact with the user. Recent behavioural biometric solutions use
deep learning models that require large amounts of annotated training data.
Collecting such volumes of behaviour biometrics data raises privacy and
usability concerns. To this end, we propose using SimSiam-based non-contrastive
self-supervised learning to improve the label efficiency of behavioural
biometric systems. The key idea is to use large volumes of unlabelled (and
anonymised) data to build good feature extractors that can be subsequently used
in supervised settings. Using two EEG datasets, we show that at lower amounts
of labelled data, non-contrastive learning performs 4%-11% more than
conventional methods such as supervised learning and data augmentation. We also
show that, in general, self-supervised learning methods perform better than
other baselines. Finally, through careful experimentation, we show various
modifications that can be incorporated into the non-contrastive learning
process to archive high performance."
166,166.0,"In other words,
we anticipate novel research focused on dynamic resource allocation strategies
that jointly and simultaneously optimise models while allocating resources such
as network bandwidth; computing and storage requirements; energy eﬃciency,
etc through the eﬃcient integration of systems and networks statistics, monitor-
ing data available at the edge [31].","Privacy Preservation Sharing a similar sentiment with researchers in Pri-
vateML, we envision the requirements for further research on open problems such
as providing satisfactory privacy guarantees while maintaining the predictive
power of the trained DL strategies, designing measures to ensure communication-
eﬃciency and counter the eﬀect of bloated data caused by encoding and en-
cryption schemes, ensuring fairness as well as robustness while enabling pri-
vacy preservation, etc.",[18].,2022-10-24 04:18:57+00:00,"Deep Edge Intelligence: Architecture, Key Features, Enabling Technologies and Challenges",cs.DC,"['cs.DC', 'cs.AI']","[arxiv.Result.Author('Prabath Abeysekara'), arxiv.Result.Author('Hai Dong'), arxiv.Result.Author('A. K. Qin')]","With the breakthroughs in Deep Learning, recent years have witnessed a
massive surge in Artificial Intelligence applications and services. Meanwhile,
the rapid advances in Mobile Computing and Internet of Things has also given
rise to billions of mobile and smart sensing devices connected to the Internet,
generating zettabytes of data at the network edge. The opportunity to combine
these two domains of technologies to power interconnected devices with
intelligence is likely to pave the way for a new wave of technology
revolutions. Embracing this technology revolution, in this article, we present
a novel computing vision named Deep Edge Intelligence (DEI). DEI employs Deep
Learning, Artificial Intelligence, Cloud and Edge Computing, 5G/6G networks,
Internet of Things, Microservices, etc. aiming to provision reliable and secure
intelligence services to every person and organisation at any place with better
user experience. The vision, system architecture, key layers and features of
DEI are also detailed. Finally, we reveal the key enabling technologies and
research challenges associated with it."
167,167.0,"Our solution provides implementation insights to
the MARL in the MAPF research community.","(3) We open-
sourced1our solution and the optimized feature parser for
further research on multi-agent reinforcement learning in
MAPF problems.","Copyright © 2023, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org).",2022-10-24 03:22:20+00:00,Multi-Agent Path Finding via Tree LSTM,cs.AI,"['cs.AI', 'cs.LG', 'cs.MA']","[arxiv.Result.Author('Yuhao Jiang'), arxiv.Result.Author('Kunjie Zhang'), arxiv.Result.Author('Qimai Li'), arxiv.Result.Author('Jiaxin Chen'), arxiv.Result.Author('Xiaolong Zhu')]","In recent years, Multi-Agent Path Finding (MAPF) has attracted attention from
the fields of both Operations Research (OR) and Reinforcement Learning (RL).
However, in the 2021 Flatland3 Challenge, a competition on MAPF, the best RL
method scored only 27.9, far less than the best OR method. This paper proposes
a new RL solution to Flatland3 Challenge, which scores 125.3, several times
higher than the best RL solution before. We creatively apply a novel network
architecture, TreeLSTM, to MAPF in our solution. Together with several other RL
techniques, including reward shaping, multiple-phase training, and centralized
control, our solution is comparable to the top 2-3 OR methods."
168,168.0,"We found
that, in the red-10 game, the relation network may feel the kindness
that an opponent unleashed mistakenly, so unleashing kindness
to the opponent, leads to their cooperation intention increase con-
stantly, some mechanisms can be made there to prevent the wrong
mutual reinforcement.","In the real world, there are some agents
with superior knowledge and the intention to hide their identities
or even deceive; how the relation network identities in the more
complicated circumstance should be further researched.","Except for
the red-10 game environment, we will try to apply IDRL in other
tasks, such as the self-driving automobile task.",2022-10-24 00:54:59+00:00,IDRL: Identifying Identities in Multi-Agent Reinforcement Learning with Ambiguous Identities,cs.AI,['cs.AI'],"[arxiv.Result.Author('Shijie Han'), arxiv.Result.Author('Peng liu'), arxiv.Result.Author('Siyuan Li')]","Multi-agent reinforcement learning(MARL) is a prevalent learning paradigm for
solving stochastic games. In previous studies, agents in a game are defined to
be teammates or enemies beforehand, and the relation of the agents is fixed
throughout the game. Those works can hardly work in the games where the
competitive and collaborative relationships are not public and dynamically
changing, which is decided by the \textit{identities} of the agents. How to
learn a successful policy in such a situation where the identities of agents
are ambiguous is still a problem. Focusing on this problem, in this work, we
develop a novel MARL framework: IDRL, which identifies the identities of the
agents dynamically and then chooses the corresponding policy to perform in the
task. In the IDRL framework, a relation network is constructed to deduce the
identities of the multi-agents through feeling the kindness and hostility
unleashed by other agents; a dangerous network is built to estimate the risk of
the identification. We also propose an intrinsic reward to help train the
relation network and the dangerous network to get a trade-off between the need
to maximize external reward and the accuracy of identification. After
identifying the cooperation-competition pattern among the agents, the proposed
method IDRL applies one of the off-the-shelf MARL methods to learn the policy.
Taking the poker game \textit{red-10} as the experiment environment,
experiments show that the IDRL can achieve superior performance compared to the
other MARL methods. Significantly, the relation network has the par performance
to identify the identities of agents with top human players; the dangerous
network reasonably avoids the risk of imperfect identification."
169,169.0,"Comparing the socio-cognitive versions,
the Bode plots of the caste-based algorithm seems to be the
best ﬁt.","The separated caste algorithm is the worst, perhaps
the learning relations should be further researched.","is easy to see that

In Fig.",2022-10-23 22:21:10+00:00,Socio-cognitive Optimization of Time-delay Control Problems using Evolutionary Metaheuristics,cs.NE,"['cs.NE', 'cs.AI', 'I.2.11; I.2.8']","[arxiv.Result.Author('Piotr Kipinski'), arxiv.Result.Author('Hubert Guzowski'), arxiv.Result.Author('Aleksandra Urbanczyk'), arxiv.Result.Author('Maciej Smolka'), arxiv.Result.Author('Marek Kisiel-Dorohinicki'), arxiv.Result.Author('Aleksander Byrski'), arxiv.Result.Author('Zuzana Kominkova Oplatkova'), arxiv.Result.Author('Roman Senkerik'), arxiv.Result.Author('Libor Pekar'), arxiv.Result.Author('Radek Matusu'), arxiv.Result.Author('Frantisek Gazdos')]","Metaheuristics are universal optimization algorithms which should be used for
solving difficult problems, unsolvable by classic approaches. In this paper we
aim at constructing novel socio-cognitive metaheuristic based on castes, and
apply several versions of this algorithm to optimization of time-delay system
model. Besides giving the background and the details of the proposed algorithms
we apply them to optimization of selected variants of the problem and discuss
the results."
170,170.0,"This as-
sumption is similar in spirit to the step-by-step
“exemplars” manually provided in chain of thought
prompting (Wei et al., 2022).","In line with their
ﬁndings, we believe that the conﬁrmation of our
hypothesis helps to motivate further research in
automating this step.","4.2 Demonstrate

Once we have a diagram with component tasks, we
need to demonstrate them, preferably with some
degree of natural language variation.",2022-10-23 03:22:34+00:00,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Victor S. Bursztyn'), arxiv.Result.Author('David Demeter'), arxiv.Result.Author('Doug Downey'), arxiv.Result.Author('Larry Birnbaum')]","How to usefully encode compositional task structure has long been a core
challenge in AI. Recent work in chain of thought prompting has shown that for
very large neural language models (LMs), explicitly demonstrating the
inferential steps involved in a target task may improve performance over
end-to-end learning that focuses on the target task alone. However, chain of
thought prompting has significant limitations due to its dependency on huge
pretrained LMs. In this work, we present compositional fine-tuning (CFT): an
approach based on explicitly decomposing a target task into component tasks,
and then fine-tuning smaller LMs on a curriculum of such component tasks. We
apply CFT to recommendation tasks in two domains, world travel and local
dining, as well as a previously studied inferential task (sports
understanding). We show that CFT outperforms end-to-end learning even with
equal amounts of data, and gets consistently better as more component tasks are
modeled via fine-tuning. Compared with chain of thought prompting, CFT performs
at least as well using LMs only 7.4% of the size, and is moreover applicable to
task domains for which data are not available during pretraining."
171,171.0,"Although we obtain improvements from three very
different types of component tasks—factual state-
ments, factual comparisons, and negative prefer-
ence interpretations—standard end-to-end learning
schemes tend to overlook the explicit use of com-
positional structure or focus only on factual knowl-
edge.","We hope to encourage further research in
other principled, task-agnostic methods for lever-
aging compositional structure in LM ﬁne-tuning.","Compared to chain of thought prompting, meth-
ods based on ﬁne-tuning have at least two advan-
tages.",2022-10-23 03:22:34+00:00,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Victor S. Bursztyn'), arxiv.Result.Author('David Demeter'), arxiv.Result.Author('Doug Downey'), arxiv.Result.Author('Larry Birnbaum')]","How to usefully encode compositional task structure has long been a core
challenge in AI. Recent work in chain of thought prompting has shown that for
very large neural language models (LMs), explicitly demonstrating the
inferential steps involved in a target task may improve performance over
end-to-end learning that focuses on the target task alone. However, chain of
thought prompting has significant limitations due to its dependency on huge
pretrained LMs. In this work, we present compositional fine-tuning (CFT): an
approach based on explicitly decomposing a target task into component tasks,
and then fine-tuning smaller LMs on a curriculum of such component tasks. We
apply CFT to recommendation tasks in two domains, world travel and local
dining, as well as a previously studied inferential task (sports
understanding). We show that CFT outperforms end-to-end learning even with
equal amounts of data, and gets consistently better as more component tasks are
modeled via fine-tuning. Compared with chain of thought prompting, CFT performs
at least as well using LMs only 7.4% of the size, and is moreover applicable to
task domains for which data are not available during pretraining."
172,172.0,"We show in our results how the perfor-
mance of the planner varies both in ADR and convergence
time as a function of these parameters on the two domains.","Future Work
During the development of B3RTDP we identiﬁed several
areas where it could be improved with further research and
development.","Much of the current running time of the algorithm can
be attributed to the belief update calculation of equation 1.",2022-10-22 21:42:59+00:00,B$^3$RTDP: A Belief Branch and Bound Real-Time Dynamic Programming Approach to Solving POMDPs,cs.AI,"['cs.AI', 'I.2.8; I.2.6']","[arxiv.Result.Author('Sigurdur Orn Adalgeirsson'), arxiv.Result.Author('Cynthia Breazeal')]","Partially Observable Markov Decision Processes (POMDPs) offer a promising
world representation for autonomous agents, as they can model both transitional
and perceptual uncertainties. Calculating the optimal solution to POMDP
problems can be computationally expensive as they require reasoning over the
(possibly infinite) space of beliefs. Several approaches have been proposed to
overcome this difficulty, such as discretizing the belief space, point-based
belief sampling, and Monte Carlo tree search. The Real-Time Dynamic Programming
approach of the RTDP-Bel algorithm approximates the value function by storing
it in a hashtable with discretized belief keys. We propose an extension to the
RTDP-Bel algorithm which we call Belief Branch and Bound RTDP (B$^3$RTDP). Our
algorithm uses a bounded value function representation and takes advantage of
this in two novel ways: a search-bounding technique based on action selection
convergence probabilities, and a method for leveraging early action convergence
called the \textit{Convergence Frontier}. Lastly, we empirically demonstrate
that B$^3$RTDP can achieve greater returns in less time than the
state-of-the-art SARSOP solver on known POMDP problems."
173,173.0,2020.,"Optimizing the

A Code release

To help with further research, we make our code
publicly available using the ViLMedic library (Del-
brouck et al., 2022).","More speciﬁcally, we re-
lease the code of all the factually-oriented met-
rics presented in Section 5.2 in one package.",2022-10-21 18:27:45+00:00,Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jean-Benoit Delbrouck'), arxiv.Result.Author('Pierre Chambon'), arxiv.Result.Author('Christian Bluethgen'), arxiv.Result.Author('Emily Tsai'), arxiv.Result.Author('Omar Almusa'), arxiv.Result.Author('Curtis P. Langlotz')]","Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports."
174,174.0,"We additionally demonstrated that
a frozen feature set is necessary for retaining the full robustness aspect of these kernels, as either
allowing the batchnorm parameters to vary or using SGD as opposed to linearized training in the
second stage results in a drop in robust accuracy.","Our results strongly motivate further research utilizing the adversarial properties of linearized training
we observed as a defense mechanism for adversarial attacks.","Furthermore, we note that linearized
training costs approximately double the cost of benign training, however, adversarial training is
signiﬁcantly more expensive.",2022-10-21 15:21:15+00:00,Evolution of Neural Tangent Kernels under Benign and Adversarial Training,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']","[arxiv.Result.Author('Noel Loo'), arxiv.Result.Author('Ramin Hasani'), arxiv.Result.Author('Alexander Amini'), arxiv.Result.Author('Daniela Rus')]","Two key challenges facing modern deep learning are mitigating deep networks'
vulnerability to adversarial attacks and understanding deep learning's
generalization capabilities. Towards the first issue, many defense strategies
have been developed, with the most common being Adversarial Training (AT).
Towards the second challenge, one of the dominant theories that has emerged is
the Neural Tangent Kernel (NTK) -- a characterization of neural network
behavior in the infinite-width limit. In this limit, the kernel is frozen, and
the underlying feature map is fixed. In finite widths, however, there is
evidence that feature learning happens at the earlier stages of the training
(kernel learning) before a second phase where the kernel remains fixed (lazy
training). While prior work has aimed at studying adversarial vulnerability
through the lens of the frozen infinite-width NTK, there is no work that
studies the adversarial robustness of the empirical/finite NTK during training.
In this work, we perform an empirical study of the evolution of the empirical
NTK under standard and adversarial training, aiming to disambiguate the effect
of adversarial training on kernel learning and lazy training. We find under
adversarial training, the empirical NTK rapidly converges to a different kernel
(and feature map) than standard training. This new kernel provides adversarial
robustness, even when non-robust training is performed on top of it.
Furthermore, we find that adversarial training on top of a fixed kernel can
yield a classifier with $76.1\%$ robust accuracy under PGD attacks with
$\varepsilon = 4/255$ on CIFAR-10."
175,175.0,"Limitations

First, despite the efﬁciency of LittleBird and its
excellent performance in question answering, it is
still unknown whether LittleBird works well for
other NLP tasks.","Further research is needed on
other tasks.","Second, in the encoder-decoder archi-
tecture model that requires cross-attention, since
position information is not injected into each token
at all, it may be difﬁcult for the decoder layer to
ﬁnd appropriate tokens of the encoder to attend.",2022-10-21 10:46:41+00:00,LittleBird: Efficient Faster & Longer Transformer for Question Answering,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Minchul Lee'), arxiv.Result.Author('Kijong Han'), arxiv.Result.Author('Myeong Cheol Shin')]","BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a
limitation dealing with long inputs due to its attention mechanism. Longformer,
ETC and BigBird addressed this issue and effectively solved the quadratic
dependency problem. However we find that these models are not sufficient, and
propose LittleBird, a novel model based on BigBird with improved speed and
memory footprint while maintaining accuracy. In particular, we devise a more
flexible and efficient position representation method based on Attention with
Linear Biases (ALiBi). We also show that replacing the method of global
information represented in the BigBird with pack and unpack attention is more
effective. The proposed model can work on long inputs even after being
pre-trained on short inputs, and can be trained efficiently reusing existing
pre-trained language model for short inputs. This is a significant benefit for
low-resource languages where large amounts of long text data are difficult to
obtain. As a result, our experiments show that LittleBird works very well in a
variety of languages, achieving high performance in question answering tasks,
particularly in KorQuAD2.0, Korean Question Answering Dataset for long
paragraphs."
176,176.0,"For successful comparison and evaluation of counterfactual explanations, future research needs to ensure unified

metrics for counterfactual properties such as proximity and data manifold closeness.","Additionally, for the counterfactual

explanations to reach their full potential as actionable explanations, further research is needed in developing causally-

correct counterfactuals and producing recourse.","6 COUNTERFACTUAL EXPLANATIONS IN REINFORCEMENT LEARNING

In the previous section we provided an overview of the state-of-the-art methods for generating counterfactual expla-

nations in supervised learning tasks.",2022-10-21 09:50:53+00:00,Counterfactual Explanations for Reinforcement Learning,cs.AI,['cs.AI'],"[arxiv.Result.Author('Jasmina Gajcin'), arxiv.Result.Author('Ivana Dusparic')]","While AI algorithms have shown remarkable success in various fields, their
lack of transparency hinders their application to real-life tasks. Although
explanations targeted at non-experts are necessary for user trust and human-AI
collaboration, the majority of explanation methods for AI are focused on
developers and expert users. Counterfactual explanations are local explanations
that offer users advice on what can be changed in the input for the output of
the black-box model to change. Counterfactuals are user-friendly and provide
actionable advice for achieving the desired output from the AI system. While
extensively researched in supervised learning, there are few methods applying
them to reinforcement learning (RL). In this work, we explore the reasons for
the underrepresentation of a powerful explanation method in RL. We start by
reviewing the current work in counterfactual explanations in supervised
learning. Additionally, we explore the differences between counterfactual
explanations in supervised learning and RL and identify the main challenges
that prevent adoption of methods from supervised in reinforcement learning.
Finally, we redefine counterfactuals for RL and propose research directions for
implementing counterfactuals in RL."
177,177.0,"However,
this counterfactual instance is unlikely to occur under the agent’s policy 𝜋 which is already following goal 𝐴 in state 𝑠.","For this reason, further research in understanding how can the search space for counterfactual instances be defined is

needed.",6.4.2 Categorical Variables.,2022-10-21 09:50:53+00:00,Counterfactual Explanations for Reinforcement Learning,cs.AI,['cs.AI'],"[arxiv.Result.Author('Jasmina Gajcin'), arxiv.Result.Author('Ivana Dusparic')]","While AI algorithms have shown remarkable success in various fields, their
lack of transparency hinders their application to real-life tasks. Although
explanations targeted at non-experts are necessary for user trust and human-AI
collaboration, the majority of explanation methods for AI are focused on
developers and expert users. Counterfactual explanations are local explanations
that offer users advice on what can be changed in the input for the output of
the black-box model to change. Counterfactuals are user-friendly and provide
actionable advice for achieving the desired output from the AI system. While
extensively researched in supervised learning, there are few methods applying
them to reinforcement learning (RL). In this work, we explore the reasons for
the underrepresentation of a powerful explanation method in RL. We start by
reviewing the current work in counterfactual explanations in supervised
learning. Additionally, we explore the differences between counterfactual
explanations in supervised learning and RL and identify the main challenges
that prevent adoption of methods from supervised in reinforcement learning.
Finally, we redefine counterfactuals for RL and propose research directions for
implementing counterfactuals in RL."
178,178.0,"For this reason, temporal similarity needs to be considered along

with similarity of features when calculating proximity.","Further research is necessary to define metrics and integrating

temporal similarity in the search for counterfactuals in RL.","6.4.4

Stochasticity.",2022-10-21 09:50:53+00:00,Counterfactual Explanations for Reinforcement Learning,cs.AI,['cs.AI'],"[arxiv.Result.Author('Jasmina Gajcin'), arxiv.Result.Author('Ivana Dusparic')]","While AI algorithms have shown remarkable success in various fields, their
lack of transparency hinders their application to real-life tasks. Although
explanations targeted at non-experts are necessary for user trust and human-AI
collaboration, the majority of explanation methods for AI are focused on
developers and expert users. Counterfactual explanations are local explanations
that offer users advice on what can be changed in the input for the output of
the black-box model to change. Counterfactuals are user-friendly and provide
actionable advice for achieving the desired output from the AI system. While
extensively researched in supervised learning, there are few methods applying
them to reinforcement learning (RL). In this work, we explore the reasons for
the underrepresentation of a powerful explanation method in RL. We start by
reviewing the current work in counterfactual explanations in supervised
learning. Additionally, we explore the differences between counterfactual
explanations in supervised learning and RL and identify the main challenges
that prevent adoption of methods from supervised in reinforcement learning.
Finally, we redefine counterfactuals for RL and propose research directions for
implementing counterfactuals in RL."
179,179.0,"Especially in the fields of

multi-goal and multi-objective RL, diverse counterfactual explanations which capture the wide range of possible causes

are necessary in order to fully comprehend the agent’s decisions.","Further research in integration of different causes

into counterfactual explanations is needed.",Developing metrics for evaluating counterfactual properties in RL is another important research direction necessary.,2022-10-21 09:50:53+00:00,Counterfactual Explanations for Reinforcement Learning,cs.AI,['cs.AI'],"[arxiv.Result.Author('Jasmina Gajcin'), arxiv.Result.Author('Ivana Dusparic')]","While AI algorithms have shown remarkable success in various fields, their
lack of transparency hinders their application to real-life tasks. Although
explanations targeted at non-experts are necessary for user trust and human-AI
collaboration, the majority of explanation methods for AI are focused on
developers and expert users. Counterfactual explanations are local explanations
that offer users advice on what can be changed in the input for the output of
the black-box model to change. Counterfactuals are user-friendly and provide
actionable advice for achieving the desired output from the AI system. While
extensively researched in supervised learning, there are few methods applying
them to reinforcement learning (RL). In this work, we explore the reasons for
the underrepresentation of a powerful explanation method in RL. We start by
reviewing the current work in counterfactual explanations in supervised
learning. Additionally, we explore the differences between counterfactual
explanations in supervised learning and RL and identify the main challenges
that prevent adoption of methods from supervised in reinforcement learning.
Finally, we redefine counterfactuals for RL and propose research directions for
implementing counterfactuals in RL."
180,,"Through extensive experiments in

various translation directions, considering back-
translation and multilingual translation, we ﬁnd
that an encoder-only model can perform as good
as an encoder-decoder model.","We further discuss
implications and subtleties of such models to mo-
tivate further research into more compact models
and more general neural network interfaces.","Acknowledgements

This work was partially supported by the project
HYKIST funded by the German Federal Ministry
of Health on the basis of a decision of the German
Federal Parliament (Bundestag) under funding ID
ZMVI1-2520DAT04A, and by NeuroSys which, as
part of the initiative “Clusters4Future”, is funded
by the Federal Ministry of Education and Research
BMBF (03ZU1106DA).",2022-10-21 08:33:55+00:00,Is Encoder-Decoder Redundant for Neural Machine Translation?,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yingbo Gao'), arxiv.Result.Author('Christian Herold'), arxiv.Result.Author('Zijian Yang'), arxiv.Result.Author('Hermann Ney')]","Encoder-decoder architecture is widely adopted for sequence-to-sequence
modeling tasks. For machine translation, despite the evolution from long
short-term memory networks to Transformer networks, plus the introduction and
development of attention mechanism, encoder-decoder is still the de facto
neural network architecture for state-of-the-art models. While the motivation
for decoding information from some hidden space is straightforward, the strict
separation of the encoding and decoding steps into an encoder and a decoder in
the model architecture is not necessarily a must. Compared to the task of
autoregressive language modeling in the target language, machine translation
simply has an additional source sentence as context. Given the fact that neural
language models nowadays can already handle rather long contexts in the target
language, it is natural to ask whether simply concatenating the source and
target sentences and training a language model to do translation would work. In
this work, we investigate the aforementioned concept for machine translation.
Specifically, we experiment with bilingual translation, translation with
additional target monolingual data, and multilingual translation. In all cases,
this alternative approach performs on par with the baseline encoder-decoder
Transformer, suggesting that an encoder-decoder architecture might be redundant
for neural machine translation."
181,,"The code for the linear
experiments is written in JAX [41].","We expect that the release of bilingual and monolingual models trained on identical conditions will
motivate further research in this area by cognitive scientists doing computational research.","The main
motivation for this paper was a theory from Cognitive Science regarding increased Cognitive Reserve
in bilingual people.",2022-10-20 22:23:27+00:00,Multitasking Models are Robust to Structural Failure: A Neural Model for Bilingual Cognitive Reserve,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Giannis Daras'), arxiv.Result.Author('Negin Raoof'), arxiv.Result.Author('Zoi Gkalitsiou'), arxiv.Result.Author('Alexandros G. Dimakis')]","We find a surprising connection between multitask learning and robustness to
neuron failures. Our experiments show that bilingual language models retain
higher performance under various neuron perturbations, such as random
deletions, magnitude pruning and weight noise compared to equivalent
monolingual ones. We provide a theoretical justification for this robustness by
mathematically analyzing linear representation learning and showing that
multitasking creates more robust representations. Our analysis connects
robustness to spectral properties of the learned representation and proves that
multitasking leads to higher robustness for diverse task vectors. We
open-source our code and models:
https://github.com/giannisdaras/multilingual_robustness"
182,,The results show that users trust different explanations in the proxy task and the actual decision-making task.,"Therefore, we argue that further research is required to uncover the links between current proxy tasks and on-task

performance or to devise new proxy tasks with a verified connection to actual tasks.",Simulated evaluation as a cost-efficient solution?,2022-10-20 20:53:00+00:00,Towards Human-centered Explainable AI: User Studies for Model Explanations,cs.AI,"['cs.AI', 'cs.HC']","[arxiv.Result.Author('Yao Rong'), arxiv.Result.Author('Tobias Leemann'), arxiv.Result.Author('Thai-trang Nguyen'), arxiv.Result.Author('Lisa Fiedler'), arxiv.Result.Author('Peizhu Qian'), arxiv.Result.Author('Vaibhav Unhelkar'), arxiv.Result.Author('Tina Seidel'), arxiv.Result.Author('Gjergji Kasneci'), arxiv.Result.Author('Enkelejda Kasneci')]","Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI
research. A better understanding of the needs of XAI users, as well as
human-centered evaluations of explainable models are both a necessity and a
challenge. In this paper, we explore how HCI and AI researchers conduct user
studies in XAI applications based on a systematic literature review. After
identifying and thoroughly analyzing 85 core papers with human-based XAI
evaluations over the past five years, we categorize them along the measured
characteristics of explanatory methods, namely trust, understanding, fairness,
usability, and human-AI team performance. Our research shows that XAI is
spreading more rapidly in certain application domains, such as recommender
systems than in others, but that user evaluations are still rather sparse and
incorporate hardly any insights from cognitive or social sciences. Based on a
comprehensive discussion of best practices, i.e., common models, design
choices, and measures in user studies, we propose practical guidelines on
designing and conducting user studies for XAI researchers and practitioners.
Lastly, this survey also highlights several open research directions,
particularly linking psychological science and human-centered XAI."
183,,"By contrast, the deepest ResNet uses more than 1000

dimensioniona in its lowest layers.","Given that genetic algorithms go even further by not training one

network, but 100 at a time, and considering our limited time, we had to halt further research.","39

7 Further enhancements

7.1 Optimisation

The single biggest challenge we faced was performance.",2022-10-20 18:41:57+00:00,Combining Neuro-Evolution of Augmenting Topologies with Convolutional Neural Networks,cs.NE,"['cs.NE', 'cs.AI']","[arxiv.Result.Author('Jan Hohenheim'), arxiv.Result.Author('Mathias Fischler'), arxiv.Result.Author('Sara Zarubica'), arxiv.Result.Author('Jeremy Stucki')]","Current deep convolutional networks are fixed in their topology. We explore
the possibilites of making the convolutional topology a parameter itself by
combining NeuroEvolution of Augmenting Topologies (NEAT) with Convolutional
Neural Networks (CNNs) and propose such a system using blocks of Residual
Networks (ResNets). We then explain how our suggested system can only be built
once additional optimizations have been made, as genetic algorithms are way
more demanding than training per backpropagation. On the way there we explain
most of those buzzwords and offer a gentle and brief introduction to the most
important modern areas of machine learning"
184,,"CSHTEST offers a novel architecture, along with a deliberate data
split methodology that can empower practitioners and domain experts to improve causally informed
modeling and deep learning.","There is extensive further research needed to fully realize the utility
of structural causal hypothesis testing in conjugation with deep learning function approximation.","We hope to better differentiate leaky causal models, without constraints on losses, using minimum
entropy properties such as in [14] .",2022-10-20 13:46:15+00:00,Causal Structural Hypothesis Testing and Data Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Jeffrey Jiang'), arxiv.Result.Author('Omead Pooladzandi'), arxiv.Result.Author('Sunay Bhat'), arxiv.Result.Author('Gregory Pottie')]","A vast amount of expert and domain knowledge is captured by causal structural
priors, yet there has been little research on testing such priors for
generalization and data synthesis purposes. We propose a novel model
architecture, Causal Structural Hypothesis Testing, that can use nonparametric,
structural causal knowledge and approximate a causal model's functional
relationships using deep neural networks. We use these architectures for
comparing structural priors, akin to hypothesis testing, using a deliberate
(non-random) split of training and testing data. Extensive simulations
demonstrate the effectiveness of out-of-distribution generalization error as a
proxy for causal structural prior hypothesis testing and offers a statistical
baseline for interpreting results. We show that the variational version of the
architecture, Causal Structural Variational Hypothesis Testing can improve
performance in low SNR regimes. Due to the simplicity and low parameter count
of the models, practitioners can test and compare structural prior hypotheses
on small dataset and use the priors with the best generalization capacity to
synthesize much larger, causally-informed datasets. Finally, we validate our
methods on a synthetic pendulum dataset, and show a use-case on a real-world
trauma surgery ground-level falls dataset."
185,,"We also hope to extend both CSHTEST and CSVHTEST to
more ﬂexible architecture which can combine recent progress with differential causal inference and
binary sampling to better automate full or partial causal discovery.","The results are a promising start
to much further research integrating deep learning causal models with real-world priors and domain
knowledge.","7

References

[1] J. Pearl, “Causal inference in statistics: An overview,” Statistics Surveys, vol.",2022-10-20 13:46:15+00:00,Causal Structural Hypothesis Testing and Data Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Jeffrey Jiang'), arxiv.Result.Author('Omead Pooladzandi'), arxiv.Result.Author('Sunay Bhat'), arxiv.Result.Author('Gregory Pottie')]","A vast amount of expert and domain knowledge is captured by causal structural
priors, yet there has been little research on testing such priors for
generalization and data synthesis purposes. We propose a novel model
architecture, Causal Structural Hypothesis Testing, that can use nonparametric,
structural causal knowledge and approximate a causal model's functional
relationships using deep neural networks. We use these architectures for
comparing structural priors, akin to hypothesis testing, using a deliberate
(non-random) split of training and testing data. Extensive simulations
demonstrate the effectiveness of out-of-distribution generalization error as a
proxy for causal structural prior hypothesis testing and offers a statistical
baseline for interpreting results. We show that the variational version of the
architecture, Causal Structural Variational Hypothesis Testing can improve
performance in low SNR regimes. Due to the simplicity and low parameter count
of the models, practitioners can test and compare structural prior hypotheses
on small dataset and use the priors with the best generalization capacity to
synthesize much larger, causally-informed datasets. Finally, we validate our
methods on a synthetic pendulum dataset, and show a use-case on a real-world
trauma surgery ground-level falls dataset."
186,,"Although PSGD routinely outperform AdaBelief both in mean ﬁnal loss and variance (across
3 iterations per test), there were select conditions and DAGs in which any single optimizer would
underperform or not converge.","We leave it to further research to investigate optimizer performance
and considerations for causally informed deep learning architecture that are constrained in unique
ways than traditional deep learning models.","PSGD had better loss in 171 cases, and lower variance
in 148 of the 176 test cases.",2022-10-20 13:46:15+00:00,Causal Structural Hypothesis Testing and Data Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Jeffrey Jiang'), arxiv.Result.Author('Omead Pooladzandi'), arxiv.Result.Author('Sunay Bhat'), arxiv.Result.Author('Gregory Pottie')]","A vast amount of expert and domain knowledge is captured by causal structural
priors, yet there has been little research on testing such priors for
generalization and data synthesis purposes. We propose a novel model
architecture, Causal Structural Hypothesis Testing, that can use nonparametric,
structural causal knowledge and approximate a causal model's functional
relationships using deep neural networks. We use these architectures for
comparing structural priors, akin to hypothesis testing, using a deliberate
(non-random) split of training and testing data. Extensive simulations
demonstrate the effectiveness of out-of-distribution generalization error as a
proxy for causal structural prior hypothesis testing and offers a statistical
baseline for interpreting results. We show that the variational version of the
architecture, Causal Structural Variational Hypothesis Testing can improve
performance in low SNR regimes. Due to the simplicity and low parameter count
of the models, practitioners can test and compare structural prior hypotheses
on small dataset and use the priors with the best generalization capacity to
synthesize much larger, causally-informed datasets. Finally, we validate our
methods on a synthetic pendulum dataset, and show a use-case on a real-world
trauma surgery ground-level falls dataset."
187,,"We also see that DN NGAE
(GAE) and DN NGAT (GAT ) do not necessarily predict the same image based on
the given context.",We believe that further research is needed w.r.t.,"investigating
how to best incorporate context in combination with image data.",2022-10-20 13:09:00+00:00,Context-driven Visual Object Recognition based on Knowledge Graphs,cs.AI,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.SC']","[arxiv.Result.Author('Sebastian Monka'), arxiv.Result.Author('Lavdim Halilaj'), arxiv.Result.Author('Achim Rettinger')]","Current deep learning methods for object recognition are purely data-driven
and require a large number of training samples to achieve good results. Due to
their sole dependence on image data, these methods tend to fail when confronted
with new environments where even small deviations occur. Human perception,
however, has proven to be significantly more robust to such distribution
shifts. It is assumed that their ability to deal with unknown scenarios is
based on extensive incorporation of contextual knowledge. Context can be based
either on object co-occurrences in a scene or on memory of experience. In
accordance with the human visual cortex which uses context to form different
object representations for a seen image, we propose an approach that enhances
deep learning methods by using external contextual knowledge encoded in a
knowledge graph. Therefore, we extract different contextual views from a
generic knowledge graph, transform the views into vector space and infuse it
into a DNN. We conduct a series of experiments to investigate the impact of
different contextual views on the learned object representations for the same
image dataset. The experimental results provide evidence that the contextual
views influence the image representations in the DNN differently and therefore
lead to different predictions for the same images. We also show that context
helps to strengthen the robustness of object recognition models for
out-of-distribution images, usually occurring in transfer learning tasks or
real-world scenarios."
188,,the results at diﬀerent iterations of model training can be studied.,"In further research, methods for stabilizing

Figure 11: The sums of edge weights belonging to numbered sensors/nodes for diﬀerent 5
training iterations.","The diﬀerences between obtained adjacency matrices also led to the idea
that there cannot be an ideal adjacency matrix.",2022-10-20 11:03:21+00:00,Graph Neural Networks with Trainable Adjacency Matrices for Fault Diagnosis on Multivariate Sensor Data,cs.AI,"['cs.AI', 'cs.LG']","[arxiv.Result.Author('Alexander Kovalenko'), arxiv.Result.Author('Vitaliy Pozdnyakov'), arxiv.Result.Author('Ilya Makarov')]","Timely detected anomalies in the chemical technological processes, as well as
the earliest detection of the cause of the fault, significantly reduce the
production cost in the industrial factories. Data on the state of the
technological process and the operation of production equipment are received by
a large number of different sensors. To better predict the behavior of the
process and equipment, it is necessary not only to consider the behavior of the
signals in each sensor separately, but also to take into account their
correlation and hidden relationships with each other. Graph-based data
representation helps with this. The graph nodes can be represented as data from
the different sensors, and the edges can display the influence of these data on
each other. In this work, the possibility of applying graph neural networks to
the problem of fault diagnosis in a chemical process is studied. It was
proposed to construct a graph during the training of graph neural network. This
allows to train models on data where the dependencies between the sensors are
not known in advance. In this work, several methods for obtaining adjacency
matrices were considered, as well as their quality was studied. It has also
been proposed to use multiple adjacency matrices in one model. We showed
state-of-the-art performance on the fault diagnosis task with the Tennessee
Eastman Process dataset. The proposed graph neural networks outperformed the
results of recurrent neural networks."
189,,[6] Caio Paziani Tomazella and Marcelo Seido Nagano.,"A comprehensive review of branch-and-bound algorithms:
Guidelines and directions for further research on the ﬂowshop scheduling problem.","Expert Systems with Applica-
tions, 158:113556, 2020.",2022-10-20 02:37:01+00:00,Optimal Settings for Cryptocurrency Trading Pairs,q-fin.TR,"['q-fin.TR', 'cs.AI', 'math.OC']","[arxiv.Result.Author('Di Zhang'), arxiv.Result.Author('Qiang Niu'), arxiv.Result.Author('Youzhou Zhou')]","The goal of cryptocurrencies is decentralization. In principle, all
currencies have equal status. Unlike traditional stock markets, there is no
default currency of denomination (fiat), thus the trading pairs can be set
freely. However, it is impractical to set up a trading market between every two
currencies. In order to control management costs and ensure sufficient
liquidity, we must give priority to covering those large-volume trading pairs
and ensure that all coins are reachable. We note that this is an optimization
problem. Its particularity lies in: 1) the trading volume between most (>99.5%)
possible trading pairs cannot be directly observed. 2) It satisfies the
connectivity constraint, that is, all currencies are guaranteed to be tradable.
  To solve this problem, we use a two-stage process: 1) Fill in missing values
based on a regularized, truncated eigenvalue decomposition, where the
regularization term is used to control what extent missing values should be
limited to zero. 2) Search for the optimal trading pairs, based on a branch and
bound process, with heuristic search and pruning strategies.
  The experimental results show that: 1) If the number of denominated coins is
not limited, we will get a more decentralized trading pair settings, which
advocates the establishment of trading pairs directly between large currency
pairs. 2) There is a certain room for optimization in all exchanges. The
setting of inappropriate trading pairs is mainly caused by subjectively setting
small coins to quote, or failing to track emerging big coins in time. 3) Too
few trading pairs will lead to low coverage; too many trading pairs will need
to be adjusted with markets frequently. Exchanges should consider striking an
appropriate balance between them."
190,,Section 6 describe the discussion of the paper.,"Finally, in Section 7,
we conclude with recommendations for further research.","2 Literature Review

To address text classiﬁcation [7], several machine and deep learning-based approaches have been introduced.",2022-10-19 21:53:49+00:00,Machine and Deep Learning Methods with Manual and Automatic Labelling for News Classification in Bangla Language,cs.AI,['cs.AI'],"[arxiv.Result.Author('Istiak Ahmad'), arxiv.Result.Author('Fahad AlQurashi'), arxiv.Result.Author('Rashid Mehmood')]","Research in Natural Language Processing (NLP) has increasingly become
important due to applications such as text classification, text mining,
sentiment analysis, POS tagging, named entity recognition, textual entailment,
and many others. This paper introduces several machine and deep learning
methods with manual and automatic labelling for news classification in the
Bangla language. We implemented several machine (ML) and deep learning (DL)
algorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient
Descent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest
Neighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document
Frequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long
Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit
(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and
FastText word embedding models. We develop automatic labelling methods using
Latent Dirichlet Allocation (LDA) and investigate the performance of
single-label and multi-label article classification methods. To investigate
performance, we developed from scratch Potrika, the largest and the most
extensive dataset for news classification in the Bangla language, comprising
185.51 million words and 12.57 million sentences contained in 664,880 news
articles in eight distinct categories, curated from six popular online news
portals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%
achieve the highest accuracy for manually-labelled data. For the automatic
labelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy
for single-label and multi-label data, respectively. The methods developed in
this paper are expected to advance research in Bangla and other languages."
191,,"Using EX, results in 4.4% of the pre-
dictions on DEV being false positive, while another 0.8% of
the predictions are false negative.","Our studies show that met-
rics and annotation need further research attention on Spider.",7.,2022-10-19 15:35:06+00:00,N-Best Hypotheses Reranking for Text-To-SQL Systems,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Lu Zeng'), arxiv.Result.Author('Sree Hari Krishnan Parthasarathi'), arxiv.Result.Author('Dilek Hakkani-Tur')]","Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task."
192,,"• For Multilingual MMT, we propose an effec-
tive language-aware visual prompt generation
strategy to produce different visual prompts
for different target languages based on the vi-
sion modality and type of the target language.","• We establish two Multilingual MMT bench-
mark datasets to nourish the further research
on Multilingual MMT, and extensive experi-
ments on these datasets demonstrate the effec-
tiveness of our proposed LVP-M3 method.","2 Related Works

Multimodal Machine Translation.",2022-10-19 12:21:39+00:00,LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Hongcheng Guo'), arxiv.Result.Author('Jiaheng Liu'), arxiv.Result.Author('Haoyang Huang'), arxiv.Result.Author('Jian Yang'), arxiv.Result.Author('Zhoujun Li'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('Zheng Cui'), arxiv.Result.Author('Furu Wei')]","Multimodal Machine Translation (MMT) focuses on enhancing text-only
translation with visual features, which has attracted considerable attention
from both natural language processing and computer vision communities. Recent
advances still struggle to train a separate model for each language pair, which
is costly and unaffordable when the number of languages increases in the real
world. In other words, the multilingual multimodal machine translation
(Multilingual MMT) task has not been investigated, which aims to handle the
aforementioned issues by providing a shared semantic space for multiple
languages. Besides, the image modality has no language boundaries, which is
superior to bridging the semantic gap between languages. To this end, we first
propose the Multilingual MMT task by establishing two new Multilingual MMT
benchmark datasets covering seven languages. Then, an effective baseline LVP-M3
using visual prompts is proposed to support translations between different
languages, which includes three stages (token encoding, language-aware visual
prompt generation, and language translation). Extensive experimental results on
our constructed benchmark datasets demonstrate the effectiveness of LVP-M3
method for Multilingual MMT."
193,,"However, for
other task, the token level embedding is very im-
portant (e.g, determining the start and end location
of the answer for question-answering).","Therefore,
layer-wise and token-level (including the [CLS]
and [MASK] token) “correlation” is of interest as
well, and further research is needed.","5 Limitations

References

In our preliminary experiments, we study encoder
LMs similar to BERT.",2022-10-19 04:28:19+00:00,Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', '68T50 (Primary) 68T30, 68T07 (Secondary)', 'I.2.7']",[arxiv.Result.Author('Hao Zhang')],"Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its
variants, have led to significant improvements on various NLP tasks in past
years. However, a theoretical framework for studying their relationships is
still missing. In this paper, we fill this gap by investigating the linear
dependency between pre-trained LMs. The linear dependency of LMs is defined
analogously to the linear dependency of vectors. We propose Language Model
Decomposition (LMD) to represent a LM using a linear combination of other LMs
as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD
similar to the coefficient of determination is defined and used to measure the
linear dependency of a set of LMs. In experiments, we find that BERT and eleven
(11) BERT-like LMs are 91% linearly dependent. This observation suggests that
current state-of-the-art (SOTA) LMs are highly ""correlated"". To further advance
SOTA we need more diverse and novel LMs that are less dependent on existing
LMs."
194,,"This  has  significant  implications  for  the  design 
of machines, as it indicates that modifying a person’s perception of a machine’s 
agency  can  affect  how  that  person  judges  the  machine’s  mistakes.","We  hope 
these findings help stimulate further research on the moral philosophy of humans 
judging machines.","11 

 
 
 
 
Figure 1: A. Schematic Illustration of mind perception models of humans and 
machines.",2022-10-18 18:20:42+00:00,Why do people judge humans differently from machines? The role of agency and experience,cs.CY,"['cs.CY', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Jingling Zhang'), arxiv.Result.Author('Jane Conway'), arxiv.Result.Author('César A. Hidalgo')]","People are known to judge artificial intelligence using a utilitarian moral
philosophy and humans using a moral philosophy emphasizing perceived
intentions. But why do people judge humans and machines differently? Psychology
suggests that people may have different mind perception models for humans and
machines, and thus, will treat human-like robots more similarly to the way they
treat humans. Here we present a randomized experiment where we manipulated
people's perception of machines to explore whether people judge more human-like
machines more similarly to the way they judge humans. We find that people's
judgments of machines become more similar to that of humans when they perceive
machines as having more agency (e.g. ability to plan, act), but not more
experience (e.g. ability to feel). Our findings indicate that people's use of
different moral philosophies to judge humans and machines can be explained by a
progression of mind perception models where the perception of agency plays a
prominent role. These findings add to the body of evidence suggesting that
people's judgment of machines becomes more similar to that of humans motivating
further work on differences in the judgment of human and machine actions."
195,,"Future directions for research include
probing models to provide explanations for why the
unsafe advice will lead to physical harm and quanti-
fying the commonsense knowledge required within

the different scenario/advice pairs.","Further research
can work toward preventing the initial generation
of unsafe text by incorporating external resources
such as comprehensive commonsense knowledge
bases while also training models to detect and ﬂag
unsafe advice after generation.","Additionally, as
physical harm is not uniform and exists on a spec-
trum, this aspect can be further broken down into
various levels of harm.",2022-10-18 17:59:31+00:00,SafeText: A Benchmark for Exploring Physical Safety in Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sharon Levy'), arxiv.Result.Author('Emily Allaway'), arxiv.Result.Author('Melanie Subbiah'), arxiv.Result.Author('Lydia Chilton'), arxiv.Result.Author('Desmond Patton'), arxiv.Result.Author('Kathleen McKeown'), arxiv.Result.Author('William Yang Wang')]","Understanding what constitutes safe text is an important issue in natural
language processing and can often prevent the deployment of models deemed
harmful and unsafe. One such type of safety that has been scarcely studied is
commonsense physical safety, i.e. text that is not explicitly violent and
requires additional commonsense knowledge to comprehend that it leads to
physical harm. We create the first benchmark dataset, SafeText, comprising
real-life scenarios with paired safe and physically unsafe pieces of advice. We
utilize SafeText to empirically study commonsense physical safety across
various models designed for text generation and commonsense reasoning tasks. We
find that state-of-the-art large language models are susceptible to the
generation of unsafe text and have difficulty rejecting unsafe advice. As a
result, we argue for further studies of safety and the assessment of
commonsense physical safety in models before release."
196,,"The  relative  performance  of  RL  compared  to 
previous  continual  learning  studies,  such  as  elastic  weight 
consolidation  [47],  synaptic  intelligence  [48],  and  generative 
replay using a generator [32], remains unknown and needs to 
be investigated.","Another aspect for further research is the usage 
of different data modalities.","RL was validated using EHR data, 
and  the  RL  performance  needs  to  be  tested  using  data  of 
different  modalities.",2022-10-17 19:54:38+00:00,Review Learning: Alleviating Catastrophic Forgetting with Generative Replay without Generator,cs.AI,"['cs.AI', 'cs.LG']","[arxiv.Result.Author('Jaesung Yoo'), arxiv.Result.Author('Sunghyuk Choi'), arxiv.Result.Author('Ye Seul Yang'), arxiv.Result.Author('Suhyeon Kim'), arxiv.Result.Author('Jieun Choi'), arxiv.Result.Author('Dongkyeong Lim'), arxiv.Result.Author('Yaeji Lim'), arxiv.Result.Author('Hyung Joon Joo'), arxiv.Result.Author('Dae Jung Kim'), arxiv.Result.Author('Rae Woong Park'), arxiv.Result.Author('Hyeong-Jin Yoon'), arxiv.Result.Author('Kwangsoo Kim')]","When a deep learning model is sequentially trained on different datasets, it
forgets the knowledge acquired from previous data, a phenomenon known as
catastrophic forgetting. It deteriorates performance of the deep learning model
on diverse datasets, which is critical in privacy-preserving deep learning
(PPDL) applications based on transfer learning (TL). To overcome this, we
propose review learning (RL), a generative-replay-based continual learning
technique that does not require a separate generator. Data samples are
generated from the memory stored within the synaptic weights of the deep
learning model which are used to review knowledge acquired from previous
datasets. The performance of RL was validated through PPDL experiments.
Simulations and real-world medical multi-institutional experiments were
conducted using three types of binary classification electronic health record
data. In the real-world experiments, the global area under the receiver
operating curve was 0.710 for RL and 0.655 for TL. Thus, RL was highly
effective in retaining previously learned knowledge."
197,,"Experiments demonstrate that
answer-only prompting underestimates model capabilities and that CoT prompting enables the most capable
Codex model to outperform the average human-rater baseline on 17 out of 23 tasks in BBH.","We release the data
and prompts used in the work, as well as the outputs from the Codex models, to facilitate further research.","10

If we look at (A), it says that working at a restaurant has made the speaker an amazing chef.",2022-10-17 17:08:26+00:00,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Mirac Suzgun'), arxiv.Result.Author('Nathan Scales'), arxiv.Result.Author('Nathanael Schärli'), arxiv.Result.Author('Sebastian Gehrmann'), arxiv.Result.Author('Yi Tay'), arxiv.Result.Author('Hyung Won Chung'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Quoc V. Le'), arxiv.Result.Author('Ed H. Chi'), arxiv.Result.Author('Denny Zhou'), arxiv.Result.Author('Jason Wei')]","BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that
focuses on tasks believed to be beyond the capabilities of current language
models. Language models have already made good progress on this benchmark, with
the best model in the BIG-Bench paper outperforming average reported
human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But
on what tasks do language models fall short of average human-rater performance,
and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we
call BIG-Bench Hard (BBH). These are the task for which prior language model
evaluations did not outperform the average human-rater. We find that applying
chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the
average human-rater performance on 10 of the 23 tasks, and Codex
(code-davinci-002) to surpass the average human-rater performance on 17 of the
23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot
prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,
2022), substantially underestimates the best performance and capabilities of
language models, which is better captured via CoT prompting. As further
analysis, we explore the interaction between CoT and model scale on BBH,
finding that CoT enables emergent task performance on several BBH tasks with
otherwise flat scaling curves."
198,,"Yet, one might need
clear human concepts to reason over the alignment of the features [17].","Hence, further research on
semantic, concept-based explanations acquired via human computation is needed [18, 91].",Leveraging Literature on Knowledge Acquisition for Identifying Expected Features.,2022-10-17 10:00:51+00:00,A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,cs.AI,['cs.AI'],"[arxiv.Result.Author('Andrea Tocchetti'), arxiv.Result.Author('Lorenzo Corti'), arxiv.Result.Author('Agathe Balayn'), arxiv.Result.Author('Mireia Yurrita'), arxiv.Result.Author('Philip Lippmann'), arxiv.Result.Author('Marco Brambilla'), arxiv.Result.Author('Jie Yang')]","Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future."
199,,"The article 
presents a new design of a self-optimising and self-adaptive 
AutoAI.","5.1   Limitations and further research

The limitation of the proposed design is primarily in the 
area of limited functionality.","While the proposed design can 
work for one AI function, it might not be as successful in all 
functions.",2022-10-17 09:31:51+00:00,Review of the state of the art in autonomous artificial intelligence,cs.AI,"['cs.AI', 'cs.HC', 'cs.LG', 'cs.RO']","[arxiv.Result.Author('Petar Radanliev'), arxiv.Result.Author('David De Roure')]","This article presents a new design for autonomous artificial intelligence
(AI), based on the state-of-the-art algorithms, and describes a new autonomous
AI system called AutoAI. The methodology is used to assemble the design founded
on self-improved algorithms that use new and emerging sources of data (NEFD).
The objective of the article is to conceptualise the design of a novel AutoAI
algorithm. The conceptual approach is used to advance into building new and
improved algorithms. The article integrates and consolidates the findings from
existing literature and advances the AutoAI design into (1) using new and
emerging sources of data for teaching and training AI algorithms and (2)
enabling AI algorithms to use automated tools for training new and improved
algorithms. This approach is going beyond the state-of-the-art in AI algorithms
and suggests a design that enables autonomous algorithms to self-optimise and
self-adapt, and on a higher level, be capable to self-procreate."
200,,"1The dataset

is available at https://github.com/

naver-ai/carecall-memory

3.","We release the ﬁrst Korean long-term dialogue
dataset for further research on memory man-
agement in dialogues.","2 Related Work

Dialogue

Personalized
System Building
human-like open-domain chatbots is one of the
seminal research topics in the ﬁeld of natural
language processing.",2022-10-17 05:06:38+00:00,Keep Me Updated! Memory Management in Long-term Conversations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sanghwan Bae'), arxiv.Result.Author('Donghyun Kwak'), arxiv.Result.Author('Soyoung Kang'), arxiv.Result.Author('Min Young Lee'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Yuin Jeong'), arxiv.Result.Author('Hyeri Kim'), arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Woomyoung Park'), arxiv.Result.Author('Nako Sung')]","Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions."
201,,"We also show that
keeping memory up-to-date in long-term conver-
sations is important for engaging and human-like
dialogues.","We release the newly collected dataset,
looking forward to further research on this promis-
ing direction.","Limitations

For the sake of simplicity and clarity in our current
research study, we only considered remembering
and updating information of a single interlocutor.",2022-10-17 05:06:38+00:00,Keep Me Updated! Memory Management in Long-term Conversations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sanghwan Bae'), arxiv.Result.Author('Donghyun Kwak'), arxiv.Result.Author('Soyoung Kang'), arxiv.Result.Author('Min Young Lee'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Yuin Jeong'), arxiv.Result.Author('Hyeri Kim'), arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Woomyoung Park'), arxiv.Result.Author('Nako Sung')]","Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions."
202,,"In a realistic
scenario, a very accurate prior estimation may be
difﬁcult to obtain.","In addition, the biased distribu-
tion caused by the incomplete labeling of positive
samples is one of the bottlenecks of the current
method, and there is still much left to be improved
for extremely unlabeled scenarios and scenarios
where the gap between the test set and the training
set distribution is too large, which can be a direc-
tion for further research.","However, for now, we
believe that our task is a valuable contribution to
advancing the application of document-level RE
in more realistic scenarios and provides a robust
baseline for this direction.",2022-10-17 02:54:49+00:00,A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ye Wang'), arxiv.Result.Author('Xinxin Liu'), arxiv.Result.Author('Wenxin Hu'), arxiv.Result.Author('Tao Zhang')]","Document-level relation extraction (RE) aims to identify relations between
entities across multiple sentences. Most previous methods focused on
document-level RE under full supervision. However, in real-world scenario, it
is expensive and difficult to completely label all relations in a document
because the number of entity pairs in document-level RE grows quadratically
with the number of entities. To solve the common incomplete labeling problem,
we propose a unified positive-unlabeled learning framework - shift and squared
ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled
(PU) learning on document-level RE for the first time. Considering that labeled
data of a dataset may lead to prior shift of unlabeled data, we introduce a PU
learning under prior shift of training data. Also, using none-class score as an
adaptive threshold, we propose squared ranking loss and prove its Bayesian
consistency with multi-label ranking metrics. Extensive experiments demonstrate
that our method achieves an improvement of about 14 F1 points relative to the
previous baseline with incomplete labeling. In addition, it outperforms
previous state-of-the-art results under both fully supervised and extremely
unlabeled settings as well."
203,,?,the conclusion and further research directions are presented.,"2 Related work

To our knowledge, no other study has been previously done on FDA perfor-
mance for low-dimensional continuous optimization problems.",2022-10-16 16:50:35+00:00,Study of the Fractal decomposition based metaheuristic on low-dimensional Black-Box optimization problems,cs.NE,"['cs.NE', 'cs.AI']","[arxiv.Result.Author('Arcadi Llanza'), arxiv.Result.Author('Nadiya Shvai'), arxiv.Result.Author('Amir Nakib')]","This paper analyzes the performance of the Fractal Decomposition Algorithm
(FDA) metaheuristic applied to low-dimensional continuous optimization
problems. This algorithm was originally developed specifically to deal
efficiently with high-dimensional continuous optimization problems by building
a fractal-based search tree with a branching factor linearly proportional to
the number of dimensions. Here, we aim to answer the question of whether FDA
could be equally effective for low-dimensional problems. For this purpose, we
evaluate the performance of FDA on the Black Box Optimization Benchmark (BBOB)
for dimensions 2, 3, 5, 10, 20, and 40. The experimental results show that
overall the FDA in its current form does not perform well enough. Among
different function groups, FDA shows its best performance on Misc. moderate and
Weak structure functions."
204,,"[2018], in which a
smooth function quantiﬁes the “DAG-ness” of the graph.","Due to its high complexity and NP-hard
characterization, further researches seek to improve on this or relax the constraint Yu et al.","[2019];
Lachapelle et al.",2022-10-15 04:07:39+00:00,GFlowCausal: Generative Flow Networks for Causal Discovery,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Wenqian Li'), arxiv.Result.Author('Yinchuan Li'), arxiv.Result.Author('Shengyu Zhu'), arxiv.Result.Author('Yunfeng Shao'), arxiv.Result.Author('Jianye Hao'), arxiv.Result.Author('Yan Pang')]","Causal discovery aims to uncover causal structure among a set of variables.
Score-based approaches mainly focus on searching for the best Directed Acyclic
Graph (DAG) based on a predefined score function. However, most of them are not
applicable on a large scale due to the limited searchability. Inspired by the
active learning in generative flow networks, we propose a novel approach to
learning a DAG from observational data called GFlowCausal. It converts the
graph search problem to a generation problem, in which direct edges are added
gradually. GFlowCausal aims to learn the best policy to generate high-reward
DAGs by sequential actions with probabilities proportional to predefined
rewards. We propose a plug-and-play module based on transitive closure to
ensure efficient sampling. Theoretical analysis shows that this module could
guarantee acyclicity properties effectively and the consistency between final
states and fully-connected graphs. We conduct extensive experiments on both
synthetic and real datasets, and results show the proposed approach to be
superior and also performs well in a large-scale setting."
205,,"6 https://spacy.io/api/entityrecognizer
7https://huggingface.co/socialmediaie/bertweet-base_wnut17_ner
8https://github.com/informagi/REL
9https://github.com/gammaliu/tagme
10https://cloud.google.com/natural-language

10

7 Conclusion

We described the largest dataset for NERD tasks on Tweets called TweetNERD and performed
benchmarking on popular NERD systems on its two subsets TweetNERD-OOD and TweetNERD-
Academic.","We hope that the release of this large-scale dataset enables research commu-
nity to revisit and conduct further research into the problem of entity linking on social me-
dia.","TweetNERD should foster research and development of robust NERD models for social
media which exhibit generalization across domains and time periods.",2022-10-14 21:55:07+00:00,TweetNERD -- End to End Entity Linking Benchmark for Tweets,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG', '68T50, 68T07', 'I.2.7']","[arxiv.Result.Author('Shubhanshu Mishra'), arxiv.Result.Author('Aman Saini'), arxiv.Result.Author('Raheleh Makki'), arxiv.Result.Author('Sneha Mehta'), arxiv.Result.Author('Aria Haghighi'), arxiv.Result.Author('Ali Mollahosseini')]","Named Entity Recognition and Disambiguation (NERD) systems are foundational
for information retrieval, question answering, event detection, and other
natural language processing (NLP) applications. We introduce TweetNERD, a
dataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on
Tweets. This is the largest and most temporally diverse open sourced dataset
benchmark for NERD on Tweets and can be used to facilitate research in this
area. We describe evaluation setup with TweetNERD for three NERD tasks: Named
Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End
Entity Linking (End2End); and provide performance of existing publicly
available methods on specific TweetNERD splits. TweetNERD is available at:
https://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0
International (CC BY 4.0) license. Check out more details at
https://github.com/twitter-research/TweetNERD."
206,,"It should be noted that for many 
of  the  definitions,  the  literature  is  not  consistent  and  can  be 
ambiguous.","The  authors  have  provided  succinct  definitions 
based on their experience with the subject and present them here 
as the basis for further research and discussion.",II.,2022-10-14 16:54:20+00:00,Artificial Intelligence Nomenclature Identified From Delphi Study on Key Issues Related to Trust and Barriers to Adoption for Autonomous Systems,cs.CY,"['cs.CY', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Thomas E. Doyle'), arxiv.Result.Author('Victoria Tucci'), arxiv.Result.Author('Calvin Zhu'), arxiv.Result.Author('Yifei Zhang'), arxiv.Result.Author('Basem Yassa'), arxiv.Result.Author('Sajjad Rashidiani'), arxiv.Result.Author('Md Asif Khan'), arxiv.Result.Author('Reza Samavi'), arxiv.Result.Author('Michael Noseworthy'), arxiv.Result.Author('Steven Yule')]","The rapid integration of artificial intelligence across traditional research
domains has generated an amalgamation of nomenclature. As cross-discipline
teams work together on complex machine learning challenges, finding a consensus
of basic definitions in the literature is a more fundamental problem. As a step
in the Delphi process to define issues with trust and barriers to the adoption
of autonomous systems, our study first collected and ranked the top concerns
from a panel of international experts from the fields of engineering, computer
science, medicine, aerospace, and defence, with experience working with
artificial intelligence. This document presents a summary of the literature
definitions for nomenclature derived from expert feedback."
207,,"Table Ⅶ Ablation study of data combinations 

Passenger 
Flow Data 
✔ 
✔ 
✔ 
✔ 

Data Type 
Confirmed 
Data 

Social 
Media Data 

✔ 

✔ 

✔ 
✔ 

Nanning Metro 

RMSE  MAE 

WMAPE 

45.909 
45.744 
45.631 
44.639 

25.234 
25.107 
24.954 
24.337 

14.54% 
14.34% 
14.21% 
13.82% 

Figure 17 The ablation study of Data Fusion 

Feature Extract block: To explain the effectiveness of the 
feature  extract  block  in  our  model,  we  conduct  the  control 
variable experiment.","Periodicity Modeling: To further research the influence of 
periodicity,  we  develop  one  variant  of  ST-former  without 
weekly periodicity and daily periodicity.","The results of the control variable experiment are shown in 
Table Ⅷ and Figure 18.",2022-10-14 01:51:33+00:00,ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Shuxin Zhang'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Chengcheng Wang'), arxiv.Result.Author('Ziyou Gao')]","Accurate passenger flow prediction of urban rail transit is essential for
improving the performance of intelligent transportation systems, especially
during the epidemic. How to dynamically model the complex spatiotemporal
dependencies of passenger flow is the main issue in achieving accurate
passenger flow prediction during the epidemic. To solve this issue, this paper
proposes a brand-new transformer-based architecture called STformer under the
encoder-decoder framework specifically for COVID-19. Concretely, we develop a
modified self-attention mechanism named Causal-Convolution ProbSparse
Self-Attention (CPSA) to model the multiple temporal dependencies of passenger
flow with low computational costs. To capture the complex and dynamic spatial
dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network
(AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally,
the Multi-source Data Fusion block fuses the passenger flow data, COVID-19
confirmed case data, and the relevant social media data to study the impact of
COVID-19 to passenger flow. Experiments on real-world passenger flow datasets
demonstrate the superiority of ST-former over the other eleven state-of-the-art
methods. Several ablation studies are carried out to verify the effectiveness
and reliability of our model structure. Results can provide critical insights
for the operation of URT systems."
208,,"While MAPL can be applied in many useful appli-
cations (e.g., aiding visually-impaired people), it
also makes it simpler to create malicious or offen-
sive multimodal systems from existing unimodal
models.","Further research efforts are needed on how
to safely deploy such systems so that their behavior
always aligns with ethical values.","Acknowledgements

We thank Pauline Luc for detailed and constructive
feedback on an early version of the paper.",2022-10-13 17:02:23+00:00,MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","[arxiv.Result.Author('Oscar Mañas'), arxiv.Result.Author('Pau Rodriguez'), arxiv.Result.Author('Saba Ahmadi'), arxiv.Result.Author('Aida Nematzadeh'), arxiv.Result.Author('Yash Goyal'), arxiv.Result.Author('Aishwarya Agrawal')]","Large pre-trained models have proved to be remarkable zero- and
(prompt-based) few-shot learners in unimodal vision and language tasks. We
propose MAPL, a simple and parameter-efficient method that reuses frozen
pre-trained unimodal models and leverages their strong generalization
capabilities in multimodal vision-language (VL) settings. MAPL learns a
lightweight mapping between the representation spaces of unimodal models using
aligned image-text data, and can generalize to unseen VL tasks from just a few
in-context examples. The small number of trainable parameters makes MAPL
effective at low-data and in-domain learning. Moreover, MAPL's modularity
enables easy extension to other pre-trained models. Extensive experiments on
several visual question answering and image captioning benchmarks show that
MAPL achieves superior or competitive performance compared to similar methods
while training orders of magnitude fewer parameters. MAPL can be trained in
just a few hours using modest computational resources and public datasets. We
plan to release the code and pre-trained models."
209,,"In this work, we have compared
RE3 to baselines solely through human evaluation,
which can be both noisy as well as costly even with
non-expert annotators.","While prior works have
proposed some possible measures (Barzilay and
Lapata, 2008; Castricato et al., 2021), we hope that
analyzing our generated stories (both RE3 and base-
lines) can inspire further research on metrics for
which we currently rely solely on human annota-
tion.","For example, while there exist reasonable
metrics for text similarity on a sentence or para-
graph level, long-form generation could beneﬁt
from metrics detecting when a longer passage be-
gins on-topic but slowly veers off-topic, or when
a passage uses on-topic vocabulary but is other-
wise nonsensical in context.",2022-10-13 06:29:57+00:00,Re3: Generating Longer Stories With Recursive Reprompting and Revision,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kevin Yang'), arxiv.Result.Author('Yuandong Tian'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Dan Klein')]","We consider the problem of automatically generating longer stories of over
two thousand words. Compared to prior work on shorter stories, long-range plot
coherence and relevance are more central challenges here. We propose the
Recursive Reprompting and Revision framework (Re3) to address these challenges
by (a) prompting a general-purpose language model to construct a structured
overarching plan, and (b) generating story passages by repeatedly injecting
contextual information from both the plan and current story state into a
language model prompt. We then revise by (c) reranking different continuations
for plot coherence and premise relevance, and finally (d) editing the best
continuation for factual consistency. Compared to similar-length stories
generated directly from the same base model, human evaluators judged
substantially more of Re3's stories as having a coherent overarching plot (by
14% absolute increase), and relevant to the given initial premise (by 20%)."
210,,"3) We contribute three trimodal matching
datasets, containing high-quality brain activity, visual features
and textual features.","Our code and datasets have been released
to facilitate further research1.","4) Our experimental results show
several interesting conclusions and cognitive insights about the
human visual system.",2022-10-13 05:49:33+00:00,Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features,cs.CV,"['cs.CV', 'cs.AI', 'cs.MM', 'cs.NE']","[arxiv.Result.Author('Changde Du'), arxiv.Result.Author('Kaicheng Fu'), arxiv.Result.Author('Jinpeng Li'), arxiv.Result.Author('Huiguang He')]","Decoding human visual neural representations is a challenging task with great
scientific significance in revealing vision-processing mechanisms and
developing brain-like intelligent machines. Most existing methods are difficult
to generalize to novel categories that have no corresponding neural data for
training. The two main reasons are 1) the under-exploitation of the multimodal
semantic knowledge underlying the neural data and 2) the small number of paired
(stimuli-responses) training data. To overcome these limitations, this paper
presents a generic neural decoding method called BraVL that uses multimodal
learning of brain-visual-linguistic features. We focus on modeling the
relationships between brain, visual and linguistic features via multimodal deep
generative models. Specifically, we leverage the mixture-of-product-of-experts
formulation to infer a latent code that enables a coherent joint generation of
all three modalities. To learn a more consistent joint representation and
improve the data efficiency in the case of limited brain activity data, we
exploit both intra- and inter-modality mutual information maximization
regularization terms. In particular, our BraVL model can be trained under
various semi-supervised scenarios to incorporate the visual and textual
features obtained from the extra categories. Finally, we construct three
trimodal matching datasets, and the extensive experiments lead to some
interesting conclusions and cognitive insights: 1) decoding novel visual
categories from human brain activity is practically possible with good
accuracy; 2) decoding models using the combination of visual and linguistic
features perform much better than those using either of them alone; 3) visual
perception may be accompanied by linguistic influences to represent the
semantics of visual stimuli. Code and data: https://github.com/ChangdeDu/BraVL."
211,,"1, a) that by certain metrics operate at an order of magnitude
larger scale than prior works.","To accelerate further research in
this setting, we accordingly provide our associated recipe, dataset,
models, hardware environment description, simulated analogue
environment, and a research benchmark for language conditioned
manipulation (Fig.","1, c).",2022-10-12 17:03:41+00:00,Interactive Language: Talking to Robots in Real Time,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Corey Lynch'), arxiv.Result.Author('Ayzaan Wahid'), arxiv.Result.Author('Jonathan Tompson'), arxiv.Result.Author('Tianli Ding'), arxiv.Result.Author('James Betker'), arxiv.Result.Author('Robert Baruch'), arxiv.Result.Author('Travis Armstrong'), arxiv.Result.Author('Pete Florence')]","We present a framework for building interactive, real-time, natural
language-instructable robots in the real world, and we open source related
assets (dataset, environment, benchmark, and policies). Trained with behavioral
cloning on a dataset of hundreds of thousands of language-annotated
trajectories, a produced policy can proficiently execute an order of magnitude
more commands than previous works: specifically we estimate a 93.5% success
rate on a set of 87,000 unique natural language strings specifying raw
end-to-end visuo-linguo-motor skills in the real world. We find that the same
policy is capable of being guided by a human via real-time language to address
a wide range of precise long-horizon rearrangement goals, e.g. ""make a smiley
face out of blocks"". The dataset we release comprises nearly 600,000
language-labeled trajectories, an order of magnitude larger than prior
available datasets. We hope the demonstrated results and associated assets
enable further advancement of helpful, capable, natural-language-interactable
robots. See videos at https://interactive-language.github.io."
212,,"For directly-trained deep SNNs, existing research on the
gradient vanishing or explosion problem is limited.","It is
worth noting that the threshold-dependent batch normaliza-
tion (tdBN) method proposed by [Zheng et al., 2021] can ad-
just the ﬁring rate and avoid gradient vanishing or explosion
to some extent, which is helpful for our further research on
gradient vanishing.","On this basis, we will combat the gradi-
ent vanishing problem in SD caused by the limited width of
the approximate derivative.",2022-10-12 16:39:46+00:00,Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks,cs.NE,"['cs.NE', 'cs.AI']","[arxiv.Result.Author('Lang Feng'), arxiv.Result.Author('Qianhui Liu'), arxiv.Result.Author('Huajin Tang'), arxiv.Result.Author('De Ma'), arxiv.Result.Author('Gang Pan')]","Spiking neural networks (SNNs) are bio-inspired neural networks with
asynchronous discrete and sparse characteristics, which have increasingly
manifested their superiority in low energy consumption. Recent research is
devoted to utilizing spatio-temporal information to directly train SNNs by
backpropagation. However, the binary and non-differentiable properties of spike
activities force directly trained SNNs to suffer from serious gradient
vanishing and network degradation, which greatly limits the performance of
directly trained SNNs and prevents them from going deeper. In this paper, we
propose a multi-level firing (MLF) method based on the existing spatio-temporal
back propagation (STBP) method, and spiking dormant-suppressed residual network
(spiking DS-ResNet). MLF enables more efficient gradient propagation and the
incremental expression ability of the neurons. Spiking DS-ResNet can
efficiently perform identity mapping of discrete spikes, as well as provide a
more suitable connection for gradient propagation in deep SNNs. With the
proposed method, our model achieves superior performances on a non-neuromorphic
dataset and two neuromorphic datasets with much fewer trainable parameters and
demonstrates the great ability to combat the gradient vanishing and degradation
problem in deep SNNs."
213,,"Afterward,
neighbor classiﬁcation to assign the most
likely
label to each document.","In this regard, their early
work had a major impact on further research, which
subsequently heavily focused on adding a lot of
world knowledge for dataless classiﬁcation.",Yin et al.,2022-10-12 08:57:01+00:00,Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Tim Schopf'), arxiv.Result.Author('Daniel Braun'), arxiv.Result.Author('Florian Matthes')]","In this paper, we consider the task of retrieving documents with predefined
topics from an unlabeled document dataset using an unsupervised approach. The
proposed unsupervised approach requires only a small number of keywords
describing the respective topics and no labeled document. Existing approaches
either heavily relied on a large amount of additionally encoded world knowledge
or on term-document frequencies. Contrariwise, we introduce a method that
learns jointly embedded document and word vectors solely from the unlabeled
document dataset in order to find documents that are semantically similar to
the topics described by the keywords. The proposed method requires almost no
text preprocessing but is simultaneously effective at retrieving relevant
documents with high probability. When successively retrieving documents on
different predefined topics from publicly available and commonly used datasets,
we achieved an average area under the receiver operating characteristic curve
value of 0.95 on one dataset and 0.92 on another. Further, our method can be
used for multiclass document classification, without the need to assign labels
to the dataset in advance. Compared with an unsupervised classification
baseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the
respective datasets. For easy replication of our approach, we make the
developed Lbl2Vec code publicly available as a ready-to-use tool under the
3-Clause BSD license."
214,,"Although such costs are naturally
involved in our case, they are quite low compared
with current state-of-the-art language models.","Thus,
our approach is comparably environmentally friendly
and enables ﬁnancially disadvantaged users to conduct
further research.","3https://www.aclweb.org/portal/content/acl-code-ethics

ACKNOWLEDGEMENTS

The authors would like to thank Thomas Kinkeldei of
ROKIN for his contributions to this paper.",2022-10-12 08:57:01+00:00,Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Tim Schopf'), arxiv.Result.Author('Daniel Braun'), arxiv.Result.Author('Florian Matthes')]","In this paper, we consider the task of retrieving documents with predefined
topics from an unlabeled document dataset using an unsupervised approach. The
proposed unsupervised approach requires only a small number of keywords
describing the respective topics and no labeled document. Existing approaches
either heavily relied on a large amount of additionally encoded world knowledge
or on term-document frequencies. Contrariwise, we introduce a method that
learns jointly embedded document and word vectors solely from the unlabeled
document dataset in order to find documents that are semantically similar to
the topics described by the keywords. The proposed method requires almost no
text preprocessing but is simultaneously effective at retrieving relevant
documents with high probability. When successively retrieving documents on
different predefined topics from publicly available and commonly used datasets,
we achieved an average area under the receiver operating characteristic curve
value of 0.95 on one dataset and 0.92 on another. Further, our method can be
used for multiclass document classification, without the need to assign labels
to the dataset in advance. Compared with an unsupervised classification
baseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the
respective datasets. For easy replication of our approach, we make the
developed Lbl2Vec code publicly available as a ready-to-use tool under the
3-Clause BSD license."
215,,"This improvement in entropy mini-
mization is translated into lower model errors and higher detec-
tion rates of contamination peaks.","However, regression meth-
ods have been shown to be very sensitive to acquisition policies

14

(a)(b)(c)(d)(e)σ(X|Xmeas)t/T00.20.40.60.8101and require further research.","The convergence of the regression
is not guaranteed, and the estimation error could be used as part
of the reward function.",2022-10-12 07:33:46+00:00,Censored Deep Reinforcement Patrolling with Information Criterion for Monitoring Large Water Resources using Autonomous Surface Vehicles,cs.RO,"['cs.RO', 'cs.AI']","[arxiv.Result.Author('Samuel Yanes Luis'), arxiv.Result.Author('Daniel Gutiérrez Reina'), arxiv.Result.Author('Sergio Toral Marín')]","Monitoring and patrolling large water resources is a major challenge for
conservation. The problem of acquiring data of an underlying environment that
usually changes within time involves a proper formulation of the information.
The use of Autonomous Surface Vehicles equipped with water quality sensor
modules can serve as an early-warning system agents for contamination
peak-detection, algae blooms monitoring, or oil-spill scenarios. In addition to
information gathering, the vehicle must plan routes that are free of obstacles
on non-convex maps. This work proposes a framework to obtain a collision-free
policy that addresses the patrolling task for static and dynamic scenarios.
Using information gain as a measure of the uncertainty reduction over data, it
is proposed a Deep Q-Learning algorithm improved by a Q-Censoring mechanism for
model-based obstacle avoidance. The obtained results demonstrate the usefulness
of the proposed algorithm for water resource monitoring for static and dynamic
scenarios. Simulations showed the use of noise-networks are a good choice for
enhanced exploration, with 3 times less redundancy in the paths. Previous
coverage strategies are also outperformed both in the accuracy of the obtained
contamination model by a 13% on average and by a 37% in the detection of
dangerous contamination peaks. Finally, these results indicate the
appropriateness of the proposed framework for monitoring scenarios with
autonomous vehicles."
216,,"Although they have
facilitated new approaches to stance detection con-
sidering also interaction data, most of the them em-
ploy manually engineered features tailored to each
speciﬁc data type (Espinosa et al., 2020; Lai et al.,
2021; Alkhalifa and Zubiaga, 2020) making it difﬁ-
cult to generalize over other languages and targets.","Thus, further research is required to fully under-
stand the potentiality of interaction data to perform
stance detection and its relation with concepts such
as political homophily, political polarization, echo
chambers or demographic analysis (Conover et al.,
2011; Colleoni et al., 2014; Zubiaga et al., 2019).","This paper aims to perform stance detection of
tweets by placing the emphasis on the interaction
data commonly available in social media.",2022-10-11 18:13:43+00:00,Relational Embeddings for Language Independent Stance Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Joseba Fernandez de Landa'), arxiv.Result.Author('Rodrigo Agerri')]","The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating relational embeddings,
namely, dense vector representations of interaction pairs. Our method can be
applied to any language and target without any manual tuning. Our experiments
on seven publicly available datasets and four different languages show that
combining our relational embeddings with textual methods helps to substantially
improve performance, obtaining best results for six out of seven evaluation
settings, outperforming strong baselines based on large pre-trained language
models."
217,,"While this technique is lan-
guage independent, cheap and fast to train and to
apply, the relational embeddings behave robustly
across different datasets, stance targets and lan-
guages, helping to substantially and consistently
improve results by combining them with text-based
classiﬁers.","The results and analysis performed shows that
we need to pay more attention to social network
data, aiming to address the shortcomings discussed
by further researching different strategies to lever-
age such interaction data.","We are aware that our
system is conditioned to the availability of the rela-
tional data of the user that wrote the tweet, which
means that when collecting data for training and
inference such data should also be gathered.",2022-10-11 18:13:43+00:00,Relational Embeddings for Language Independent Stance Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Joseba Fernandez de Landa'), arxiv.Result.Author('Rodrigo Agerri')]","The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating relational embeddings,
namely, dense vector representations of interaction pairs. Our method can be
applied to any language and target without any manual tuning. Our experiments
on seven publicly available datasets and four different languages show that
combining our relational embeddings with textual methods helps to substantially
improve performance, obtaining best results for six out of seven evaluation
settings, outperforming strong baselines based on large pre-trained language
models."
218,,A.6.,"Reproducibility

The BodyM dataset is publicly available at https:
//adversarialbodysim.github.io to enable re-
producibility of our method and further research in this area.",,2022-10-11 17:58:10+00:00,Human Body Measurement Estimation with Adversarial Augmentation,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Nataniel Ruiz'), arxiv.Result.Author('Miriam Bellver'), arxiv.Result.Author('Timo Bolkart'), arxiv.Result.Author('Ambuj Arora'), arxiv.Result.Author('Ming C. Lin'), arxiv.Result.Author('Javier Romero'), arxiv.Result.Author('Raja Bala')]","We present a Body Measurement network (BMnet) for estimating 3D
anthropomorphic measurements of the human body shape from silhouette images.
Training of BMnet is performed on data from real human subjects, and augmented
with a novel adversarial body simulator (ABS) that finds and synthesizes
challenging body shapes. ABS is based on the skinned multiperson linear (SMPL)
body model, and aims to maximize BMnet measurement prediction error with
respect to latent SMPL shape parameters. ABS is fully differentiable with
respect to these parameters, and trained end-to-end via backpropagation with
BMnet in the loop. Experiments show that ABS effectively discovers adversarial
examples, such as bodies with extreme body mass indices (BMI), consistent with
the rarity of extreme-BMI bodies in BMnet's training set. Thus ABS is able to
reveal gaps in training data and potential failures in predicting
under-represented body shapes. Results show that training BMnet with ABS
improves measurement prediction accuracy on real bodies by up to 10%, when
compared to no augmentation or random body shape sampling. Furthermore, our
method significantly outperforms SOTA measurement estimation methods by as much
as 3x. Finally, we release BodyM, the first challenging, large-scale dataset of
photo silhouettes and body measurements of real human subjects, to further
promote research in this area. Project website:
https://adversarialbodysim.github.io"
219,,"identiﬁcation step (Emam et al., 2015), they were
argued not immune to the hack for re-identiﬁcation
(El Emam et al., 2011; Choi et al., 2017).","Alter-
natively, generating synthetic but realistic EHRs
can circumvent data leakage while preserving the
patterns of real EHRs for further research and de-
velopment (Biswal et al., 2020).","Deep generative models like GANs (Goodfellow
et al., 2014) and VAEs (Kingma and Welling, 2013)
have become popular for unconditional EHRs gen-
eration (Choi et al., 2017) and longitudinal EHRs
generation (Biswal et al., 2020; Zhang et al., 2020)
for diagnosis codes.",2022-10-11 14:48:15+00:00,PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Zifeng Wang'), arxiv.Result.Author('Jimeng Sun')]","Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is
challenging due to privacy concerns, which hinders the use of ML for healthcare
applications. Synthetic EHRs generation bypasses the need to share sensitive
real patient records. However, existing methods generate single-modal EHRs by
unconditional generation or by longitudinal inference, which falls short of low
flexibility and makes unrealistic EHRs. In this work, we propose to formulate
EHRs generation as a text-to-text translation task by language models (LMs),
which suffices to highly flexible event imputation during generation. We also
design prompt learning to control the generation conditioned by numerical and
categorical demographic features. We evaluate synthetic EHRs quality by two
perplexity measures accounting for their longitudinal pattern (longitudinal
imputation perplexity, lpl) and the connections cross modalities
(cross-modality imputation perplexity, mpl). Moreover, we utilize two
adversaries: membership and attribute inference attacks for privacy-preserving
evaluation. Experiments on MIMIC-III data demonstrate the superiority of our
methods on realistic EHRs generation (53.1\% decrease of lpl and 45.3\%
decrease of mpl on average compared to the best baselines) with low privacy
risks. Software is available at https://github.com/RyanWangZf/PromptEHR."
220,,"The results show that the similarity of the
predicted and ground truth sequences can reach 0.44, a medium
correlation measured by Kendall’s τ .","Meanwhile, we compared
the different effects of node features and edge features and
generated a feasible and reasonable assembly sequence as a
benchmark for further research.","Our data set and code is
available on https://github.com/AIR-DISCOVER/ICRA ASP.",2022-10-11 08:06:16+00:00,Planning Assembly Sequence with Graph Transformer,cs.AI,"['cs.AI', 'cs.LG']","[arxiv.Result.Author('Lin Ma'), arxiv.Result.Author('Jiangtao Gong'), arxiv.Result.Author('Hao Xu'), arxiv.Result.Author('Hao Chen'), arxiv.Result.Author('Hao Zhao'), arxiv.Result.Author('Wenbing Huang'), arxiv.Result.Author('Guyue Zhou')]","Assembly sequence planning (ASP) is the essential process for modern
manufacturing, proven to be NP-complete thus its effective and efficient
solution has been a challenge for researchers in the field. In this paper, we
present a graph-transformer based framework for the ASP problem which is
trained and demonstrated on a self-collected ASP database. The ASP database
contains a self-collected set of LEGO models. The LEGO model is abstracted to a
heterogeneous graph structure after a thorough analysis of the original
structure and feature extraction. The ground truth assembly sequence is first
generated by brute-force search and then adjusted manually to in line with
human rational habits. Based on this self-collected ASP dataset, we propose a
heterogeneous graph-transformer framework to learn the latent rules for
assembly planning. We evaluated the proposed framework in a series of
experiment. The results show that the similarity of the predicted and ground
truth sequences can reach 0.44, a medium correlation measured by Kendall's
$\tau$. Meanwhile, we compared the different effects of node features and edge
features and generated a feasible and reasonable assembly sequence as a
benchmark for further research. Our data set and code is available on
https://github.com/AIR-DISCOVER/ICRA\_ASP."
221,,"An idea of the “leaf” attention and the optimization
over parameters of kernels can be directly transferred to the gradient boosting machine.","This is a direction
for further research.","One of the important results presented in the paper is the usage of a speciﬁc mixture of contamination
models which can be regarded as a variant of the well-known multi-head attention [16], where each “head”
is deﬁned by the kernel parameter.",2022-10-11 06:14:12+00:00,LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Lev V. Utkin')]","New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest."
222,,"The introduced two-level attention mechanisms may also improve interpretability of
RFs by taking into account additional factors.","The corresponding procedures of the interpretation is also
a direction for further research.","Finally, we have developed the proposed modiﬁcations by using the Huber’s (cid:15)-contamination model and
the mixture of the models.",2022-10-11 06:14:12+00:00,LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Lev V. Utkin')]","New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest."
223,,"A proper choice of the mixture components may signiﬁcantly improve the whole
attention-based RF.",This is also a direction for further research.,"Acknowledgement

The research is partially funded by the Ministry of Science and Higher Education of the Russian Federa-
tion under the strategic academic leadership program ’Priority 2030’ (Agreement N 075-15-2021-1333 dd
30.09.2021).",2022-10-11 06:14:12+00:00,LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Lev V. Utkin')]","New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest."
224,,"The main issue of poor condition and  structure of  vehicle blobs has been related to inefficient 
colour band combination and a technique has been developed called colour band selection technique which 
has improved the vehicle blobs, as shown in Figure 8 and 10.","Some further research is to be carried out  to  
improve both template matching  criteria  and  the shapes of the vehicle blobs.","References 

Ballard, D.H., and  Brown, C.M.",2022-10-11 02:03:32+00:00,Automatic Real-time Vehicle Classification by Image Colour Component Based Template Matching,cs.CV,"['cs.CV', 'cs.AI']",[arxiv.Result.Author('Ahmet Orun')],"Selection of appropriate template matching algorithms to run effectively on
real-time low-cost systems is always major issue. This is due to unpredictable
changes in image scene which often necessitate more sophisticated real-time
algorithms to retain image consistency. Inefficiency of low cost auxiliary
hardware and time limitations are the major constraints in using these sorts of
algorithms. The real-time system introduced here copes with these problems
utilising a fast running template matching algorithm, which makes use of best
colour band selection. The system uses fast running real-time algorithms to
achieve template matching and vehicle classification at about 4 frames /sec. on
low-cost hardware. The colour image sequences have been taken by a fixed CCTV
camera overlooking a busy multi-lane road"
225,,"Regardless, the agent may
have encountered—and learned through exploration—aspects
of the source domain that, despite not being useful to the
source task, are in fact useful to exploring and learning in
the target domain.","In this paper, we re-examine RL’s exploration-exploitation
trade-off in the context of online task transfer in reinforcement
learning, and question what exploration approaches should be
further researched when considering transfer efﬁciency and
performance in reinforcement learning .","We deﬁne a taxonomy
with which to organize and assess the potential effectiveness of
broad types of exploration methods, argue how the different
aspects of the taxonomy make methods more or less suited
for online task transfer in reinforcement learning, and suggest
how this taxonomy might be used to drive future research into
exploration for open world task transfer.",2022-10-11 01:23:21+00:00,The Role of Exploration for Task Transfer in Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jonathan C Balloch'), arxiv.Result.Author('Julia Kim'), arxiv.Result.Author('and Jessica L Inman'), arxiv.Result.Author('Mark O Riedl')]","The exploration--exploitation trade-off in reinforcement learning (RL) is a
well-known and much-studied problem that balances greedy action selection with
novel experience, and the study of exploration methods is usually only
considered in the context of learning the optimal policy for a single learning
task. However, in the context of online task transfer, where there is a change
to the task during online operation, we hypothesize that exploration strategies
that anticipate the need to adapt to future tasks can have a pronounced impact
on the efficiency of transfer. As such, we re-examine the
exploration--exploitation trade-off in the context of transfer learning. In
this work, we review reinforcement learning exploration methods, define a
taxonomy with which to organize them, analyze these methods' differences in the
context of task transfer, and suggest avenues for future investigation."
226,,"Moreover, proce-
dure flow graphs are only needed at the task level, rather than on a per video
bases.","As such, we believe the proposed formulation to hold a great promise in
minimizing labeling efforts and defines new avenues for further research.","Acknowledgement

We thank Ran Zhang for the help with flow graph creation and processing.",2022-10-10 20:02:58+00:00,Graph2Vid: Flow graph to Video Grounding for Weakly-supervised Multi-Step Localization,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Nikita Dvornik'), arxiv.Result.Author('Isma Hadji'), arxiv.Result.Author('Hai Pham'), arxiv.Result.Author('Dhaivat Bhatt'), arxiv.Result.Author('Brais Martinez'), arxiv.Result.Author('Afsaneh Fazly'), arxiv.Result.Author('Allan D. Jepson')]","In this work, we consider the problem of weakly-supervised multi-step
localization in instructional videos. An established approach to this problem
is to rely on a given list of steps. However, in reality, there is often more
than one way to execute a procedure successfully, by following the set of steps
in slightly varying orders. Thus, for successful localization in a given video,
recent works require the actual order of procedure steps in the video, to be
provided by human annotators at both training and test times. Instead, here, we
only rely on generic procedural text that is not tied to a specific video. We
represent the various ways to complete the procedure by transforming the list
of instructions into a procedure flow graph which captures the partial order of
steps. Using the flow graphs reduces both training and test time annotation
requirements. To this end, we introduce the new problem of flow graph to video
grounding. In this setup, we seek the optimal step ordering consistent with the
procedure flow graph and a given video. To solve this problem, we propose a new
algorithm - Graph2Vid - that infers the actual ordering of steps in the video
and simultaneously localizes them. To show the advantage of our proposed
formulation, we extend the CrossTask dataset with procedure flow graph
information. Our experiments show that Graph2Vid is both more efficient than
the baselines and yields strong step localization results, without the need for
step order annotation."
227,,"But we refer the

interested reader to previous heuristics studies which try to estimate a low-rank estimation

[176, 193] for an overview.","In our case, it also requires further research on the relationship

between rank R and rank L of each block and it is beyond the scope of this chapter.","For

our work, we set rank R of tensor X as 5 and vary the rank L of block between 10 − 30,

experimental analysis is provided in sub-section 4.5.2.",2022-10-10 02:26:00+00:00,Modeling and Mining Multi-Aspect Graphs With Scalable Streaming Tensor Decomposition,cs.SI,"['cs.SI', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Ekta Gujral')],"Graphs emerge in almost every real-world application domain, ranging from
online social networks all the way to health data and movie viewership
patterns. Typically, such real-world graphs are big and dynamic, in the sense
that they evolve over time. Furthermore, graphs usually contain multi-aspect
information i.e. in a social network, we can have the ""means of communication""
between nodes, such as who messages whom, who calls whom, and who comments on
whose timeline and so on.
  How can we model and mine useful patterns, such as communities of nodes in
that graph, from such multi-aspect graphs? How can we identify dynamic patterns
in those graphs, and how can we deal with streaming data, when the volume of
data to be processed is very large? In order to answer those questions, in this
thesis, we propose novel tensor-based methods for mining static and dynamic
multi-aspect graphs. In general, a tensor is a higher-order generalization of a
matrix that can represent high-dimensional multi-aspect data such as
time-evolving networks, collaboration networks, and spatio-temporal data like
Electroencephalography (EEG) brain measurements.
  The thesis is organized in two synergistic thrusts: First, we focus on static
multi-aspect graphs, where the goal is to identify coherent communities and
patterns between nodes by leveraging the tensor structure in the data. Second,
as our graphs evolve dynamically, we focus on handling such streaming updates
in the data without having to re-compute the decomposition, but incrementally
update the existing results."
228,,"Hence,

we compare our proposed method against algorithms that decompose full tensor.","Also,

extending BTD-NLS to online settings is challenging and requires further research both to

ﬁnd the best ﬁt and to interpret the role of the independent variables used in various inherit

methods.","It faces diﬃculties in ﬁtting due to the narrow boundaries on the model and less

ﬂexibility.",2022-10-10 02:26:00+00:00,Modeling and Mining Multi-Aspect Graphs With Scalable Streaming Tensor Decomposition,cs.SI,"['cs.SI', 'cs.AI', 'cs.LG']",[arxiv.Result.Author('Ekta Gujral')],"Graphs emerge in almost every real-world application domain, ranging from
online social networks all the way to health data and movie viewership
patterns. Typically, such real-world graphs are big and dynamic, in the sense
that they evolve over time. Furthermore, graphs usually contain multi-aspect
information i.e. in a social network, we can have the ""means of communication""
between nodes, such as who messages whom, who calls whom, and who comments on
whose timeline and so on.
  How can we model and mine useful patterns, such as communities of nodes in
that graph, from such multi-aspect graphs? How can we identify dynamic patterns
in those graphs, and how can we deal with streaming data, when the volume of
data to be processed is very large? In order to answer those questions, in this
thesis, we propose novel tensor-based methods for mining static and dynamic
multi-aspect graphs. In general, a tensor is a higher-order generalization of a
matrix that can represent high-dimensional multi-aspect data such as
time-evolving networks, collaboration networks, and spatio-temporal data like
Electroencephalography (EEG) brain measurements.
  The thesis is organized in two synergistic thrusts: First, we focus on static
multi-aspect graphs, where the goal is to identify coherent communities and
patterns between nodes by leveraging the tensor structure in the data. Second,
as our graphs evolve dynamically, we focus on handling such streaming updates
in the data without having to re-compute the decomposition, but incrementally
update the existing results."
229,,"Table 5: The overall network-level resilience in 2012 and
2017 at different geographical scales.","Scale

State
Division
Region

𝑅2012
𝑛𝑒𝑡

0.867
0.766
0.647

𝑅2017
𝑛𝑒𝑡

0.880
0.763
0.604

Changes (%)

+1.5%
-0.4%
-6.7%

4 DISCUSSION
We acknowledge several limits associated with the current data
source, each of which prompts further research directions.","First,
utilizing CFS data requires that we represent both food imported
internationally as well as that sourced within the U.S.",2022-10-09 23:12:16+00:00,Measuring Network Resilience via Geospatial Knowledge Graph: a Case Study of the US Multi-Commodity Flow Network,cs.SI,"['cs.SI', 'cs.AI', 'I.2.4']","[arxiv.Result.Author('Jinmeng Rao'), arxiv.Result.Author('Song Gao'), arxiv.Result.Author('Michelle Miller'), arxiv.Result.Author('Alfonso Morales')]","Quantifying the resilience in the food system is important for food security
issues. In this work, we present a geospatial knowledge graph (GeoKG)-based
method for measuring the resilience of a multi-commodity flow network.
Specifically, we develop a CFS-GeoKG ontology to describe geospatial semantics
of a multi-commodity flow network comprehensively, and design resilience
metrics that measure the node-level and network-level dependence of
single-sourcing, distant, or non-adjacent suppliers/customers in food supply
chains. We conduct a case study of the US state-level agricultural
multi-commodity flow network with hierarchical commodity types. The results
indicate that, by leveraging GeoKG, our method supports measuring both
node-level and network-level resilience across space and over time and also
helps discover concentration patterns of agricultural resources in the spatial
network at different geographic scales."
230,,"We note that this doesn’t neces-
sarily mean that those models are creative or have
commonsense reasoning skills as they could have
simply memorized those analogies, which a known
problem of such models (Bender et al., 2021).","It
requires further research to test whether the models
generate novel analogies unseen during training.","Moreover, upon inspection, we found that the
human-generated analogies sometimes had minor
issues, such as grammatical errors, which could
impact their rating by annotators.",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
231,,"On the other hand,
the larger models that already performed very well,
likely do not have much to gain from such help
and, in fact, perform worse due to the analogical
reasoning argument made above.","Overall, this highlights some limitations of the
InstructGPT model for analogical reasoning, which
requires further research for improvement.","5.4.3 Error Analysis

The annotators were also asked to explain their an-
swer choice (i.e, meaningful analogy or not).",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
232,,"It has a speciﬁc
shape and size, and it can carry the genetic instruc-
tions for making a particular organism.”

Some error types found in other natural language
generations from GPT-3 (Dou et al., 2021), e.g.,
incoherence and grammar, were also found in our
task.","Further research is required to quantify them
for analogical generation and attempt to ﬁx them.","6 Limitations

A major limitation of our study is that we only stud-
ied analogies on a small reference dataset in one
domain (high-school science).",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
233,,"Therefore, we believe that it would empower the developers of
safety-, reliability- and fairness-critical systems to develop safer models.","Moreover, we hope that this
work inspires further research into unsupervised robustness, which can contribute to more robust and
secure machine learning systems.","REPRODUCIBILITY STATEMENT

The experiments in this paper were implemented using open-source software packages (Harris et al.,
2020; Virtanen et al., 2020; McKinney, 2010; Paszke et al., 2019), as well as the publicly available
MOCOv2 and MOCOv3 models (He et al., 2020; Chen et al., 2020d; 2021).",2022-10-08 18:03:28+00:00,Robustness of Unsupervised Representation Learning without Labels,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Aleksandar Petrov'), arxiv.Result.Author('Marta Kwiatkowska')]","Unsupervised representation learning leverages large unlabeled datasets and
is competitive with supervised learning. But non-robust encoders may affect
downstream task robustness. Recently, robust representation encoders have
become of interest. Still, all prior work evaluates robustness using a
downstream classification task. Instead, we propose a family of unsupervised
robustness measures, which are model- and task-agnostic and label-free. We
benchmark state-of-the-art representation encoders and show that none dominates
the rest. We offer unsupervised extensions to the FGSM and PGD attacks. When
used in adversarial training, they improve most unsupervised robustness
measures, including certified robustness. We validate our results against a
linear probe and show that, for MOCOv2, adversarial training results in 3 times
higher certified accuracy, a 2-fold decrease in impersonation attack success
rate and considerable improvements in certified robustness."
234,,"In her research, she focuses
on Machine Learning on structural-dynamic graphs,
considering graph preprocessing, representation
and embedding techniques.","Her further research
interests cover topics from Timeseries Analysis,
Graph Theory, Machine Learning and Deep Neural
Networks.","Giuseppe Alessio D’Inverno is currently a PhD
student in Information and Engineering Science at
University of Siena.",2022-10-08 10:14:41+00:00,Weisfeiler--Lehman goes Dynamic: An Analysis of the Expressive Power of Graph Neural Networks for Attributed and Dynamic Graphs,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Silvia Beddar-Wiesing'), arxiv.Result.Author(""Giuseppe Alessio D'Inverno""), arxiv.Result.Author('Caterina Graziani'), arxiv.Result.Author('Veronica Lachi'), arxiv.Result.Author('Alice Moallemy-Oureh'), arxiv.Result.Author('Franco Scarselli'), arxiv.Result.Author('Josephine Maria Thomas')]","Graph Neural Networks (GNNs) are a large class of relational models for graph
processing. Recent theoretical studies on the expressive power of GNNs have
focused on two issues. On the one hand, it has been proven that GNNs are as
powerful as the Weisfeiler-Lehman test (1-WL) in their ability to distinguish
graphs. Moreover, it has been shown that the equivalence enforced by 1-WL
equals unfolding equivalence. On the other hand, GNNs turned out to be
universal approximators on graphs modulo the constraints enforced by
1-WL/unfolding equivalence. However, these results only apply to Static
Undirected Homogeneous Graphs with node attributes. In contrast, real-life
applications often involve a variety of graph properties, such as, e.g.,
dynamics or node and edge attributes. In this paper, we conduct a theoretical
analysis of the expressive power of GNNs for these two graph types that are
particularly of interest. Dynamic graphs are widely used in modern
applications, and its theoretical analysis requires new approaches. The
attributed type acts as a standard form for all graph types since it has been
shown that all graph types can be transformed without loss to Static Undirected
Homogeneous Graphs with attributes on nodes and edges (SAUHG). The study
considers generic GNN models and proposes appropriate 1-WL tests for those
domains. Then, the results on the expressive power of GNNs are extended by
proving that GNNs have the same capability as the 1-WL test in distinguishing
dynamic and attributed graphs, the 1-WL equivalence equals unfolding
equivalence and that GNNs are universal approximators modulo 1-WL/unfolding
equivalence. Moreover, the proof of the approximation capability holds for
SAUHGs, which include most of those used in practical applications, and it is
constructive in nature allowing to deduce hints on the architecture of GNNs
that can achieve the desired accuracy."
235,,"Out of the LLMs we evalu-
ate, we show evidence that T5-based models are ideal due to their bidirectional
encoder-decoder architecture.","To promote further research on LLMs for HTML
understanding, we create and open-source a large-scale HTML dataset distilled
and auto-labeled from CommonCrawl.1

1

INTRODUCTION

Web crawling (Olston et al., 2010), form-ﬁlling (Diaz et al., 2013; Gur et al., 2021), or information
retrieving web agents (Nogueira & Cho, 2016) are important for both automating and assisting
users in web-based tasks.","These and similar applications rely on models that can search for speciﬁc
content or controls on a web page as well as navigate a website autonomously.",2022-10-08 07:27:17+00:00,Understanding HTML with Large Language Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Izzeddin Gur'), arxiv.Result.Author('Ofir Nachum'), arxiv.Result.Author('Yingjie Miao'), arxiv.Result.Author('Mustafa Safdari'), arxiv.Result.Author('Austin Huang'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Sharan Narang'), arxiv.Result.Author('Noah Fiedel'), arxiv.Result.Author('Aleksandra Faust')]","Large language models (LLMs) have shown exceptional performance on a variety
of natural language tasks. Yet, their capabilities for HTML understanding --
i.e., parsing the raw HTML of a webpage, with applications to automation of
web-based tasks, crawling, and browser-assisted retrieval -- have not been
fully explored. We contribute HTML understanding models (fine-tuned LLMs) and
an in-depth analysis of their capabilities under three tasks: (i) Semantic
Classification of HTML elements, (ii) Description Generation for HTML inputs,
and (iii) Autonomous Web Navigation of HTML pages. While previous work has
developed dedicated architectures and training procedures for HTML
understanding, we show that LLMs pretrained on standard natural language
corpora transfer remarkably well to HTML understanding tasks. For instance,
fine-tuned LLMs are 12% more accurate at semantic classification compared to
models trained exclusively on the task dataset. Moreover, when fine-tuned on
data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks
using 192x less data compared to the previous best supervised model. Out of the
LLMs we evaluate, we show evidence that T5-based models are ideal due to their
bidirectional encoder-decoder architecture. To promote further research on LLMs
for HTML understanding, we create and open-source a large-scale HTML dataset
distilled and auto-labeled from CommonCrawl."
236,,"This enables us

to compare the reconstruction against the original image.","The results and potential avenues for further research are

discussed in section 4.",The paper ends with the conclusion in section 5.,2022-10-07 19:42:09+00:00,Can Artificial Intelligence Reconstruct Ancient Mosaics?,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Author('Fernando Moral-Andrés'), arxiv.Result.Author('Elena Merino-Gómez'), arxiv.Result.Author('Pedro Reviriego'), arxiv.Result.Author('Fabrizio Lombardi')]","A large number of ancient mosaics have not reached us because they have been
destroyed by erosion, earthquakes, looting or even used as materials in newer
construction. To make things worse, among the small fraction of mosaics that we
have been able to recover, many are damaged or incomplete. Therefore,
restoration and reconstruction of mosaics play a fundamental role to preserve
cultural heritage and to understand the role of mosaics in ancient cultures.
This reconstruction has traditionally been done manually and more recently
using computer graphics programs but always by humans. In the last years,
Artificial Intelligence (AI) has made impressive progress in the generation of
images from text descriptions and reference images. State of the art AI tools
such as DALL-E2 can generate high quality images from text prompts and can take
a reference image to guide the process. In august 2022, DALL-E2 launched a new
feature called outpainting that takes as input an incomplete image and a text
prompt and then generates a complete image filling the missing parts. In this
paper, we explore whether this innovative technology can be used to reconstruct
mosaics with missing parts. Hence a set of ancient mosaics have been used and
reconstructed using DALL-E2; results are promising showing that AI is able to
interpret the key features of the mosaics and is able to produce
reconstructions that capture the essence of the scene. However, in some cases
AI fails to reproduce some details, geometric forms or introduces elements that
are not consistent with the rest of the mosaic. This suggests that as AI image
generation technology matures in the next few years, it could be a valuable
tool for mosaic reconstruction going forward."
237,,"Furthermore, user-driven algorithm auditing could
harm users if their labors are co-opted in ways that are not aligned
with what they might have wanted.","We highlight these burdens
on users as vital areas for further research.","Despite the burdens,
user-driven auditing when implemented well can serve to reduce
algorithmic harms present and acting in the world now.",2022-10-07 17:25:38+00:00,"Understanding Practices, Challenges, and Opportunities for User-Driven Algorithm Auditing in Industry Practice",cs.HC,"['cs.HC', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Wesley Hanwen Deng'), arxiv.Result.Author('Bill Boyuan Guo'), arxiv.Result.Author('Alicia DeVrio'), arxiv.Result.Author('Hong Shen'), arxiv.Result.Author('Motahhare Eslami'), arxiv.Result.Author('Kenneth Holstein')]","Recent years have seen growing interest among both researchers and
practitioners in user-driven approaches to algorithm auditing, which directly
engage users in detecting problematic behaviors in algorithmic systems.
However, we know little about industry practitioners' current practices and
challenges around user-driven auditing, nor what opportunities exist for them
to better leverage such approaches in practice. To investigate, we conducted a
series of interviews and iterative co-design activities with practitioners who
employ user-driven auditing approaches in their work. Our findings reveal
several challenges practitioners face in appropriately recruiting and
incentivizing user auditors, scaffolding user audits, and deriving actionable
insights from user-driven audit reports. Furthermore, practitioners shared
organizational obstacles to user-driven auditing, surfacing a complex
relationship between practitioners and user auditors. Based on these findings,
we discuss opportunities for future HCI research to help realize the potential
(and mitigate risks) of user-driven auditing in industry practice."
238,,"to inject structured
knowledge about entities and the relations between them from a pre-existing
knowledge base into a pre-trained language model (Peters et al., 2019; Wang
et al., 2021b), enabling domain adaptation at a limited cost.","However, the
latter still needs further research for its systematic application in a produc-
tion environment.","Many NLP tasks can be solved equally well by using a machine learning-
based approach or a symbolic approach.",2022-10-07 15:50:17+00:00,Artificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('José Manuel Gómez-Pérez'), arxiv.Result.Author('Andrés García-Silva'), arxiv.Result.Author('Rosemarie Leone'), arxiv.Result.Author('Mirko Albani'), arxiv.Result.Author('Moritz Fontaine'), arxiv.Result.Author('Charles Poncet'), arxiv.Result.Author('Leopold Summerer'), arxiv.Result.Author('Alessandro Donati'), arxiv.Result.Author('Ilaria Roma'), arxiv.Result.Author('Stefano Scaglioni')]","The European Space Agency is well known as a powerful force for scientific
discovery in numerous areas related to Space. The amount and depth of the
knowledge produced throughout the different missions carried out by ESA and
their contribution to scientific progress is enormous, involving large
collections of documents like scientific publications, feasibility studies,
technical reports, and quality management procedures, among many others.
Through initiatives like the Open Space Innovation Platform, ESA also acts as a
hub for new ideas coming from the wider community across different challenges,
contributing to a virtuous circle of scientific discovery and innovation.
Handling such wealth of information, of which large part is unstructured text,
is a colossal task that goes beyond human capabilities, hence requiring
automation. In this paper, we present a methodological framework based on
artificial intelligence and natural language processing and understanding to
automatically extract information from Space documents, generating value from
it, and illustrate such framework through several case studies implemented
across different functional areas of ESA, including Mission Design, Quality
Assurance, Long-Term Data Preservation, and the Open Space Innovation Platform.
In doing so, we demonstrate the value of these technologies in several tasks
ranging from effortlessly searching and recommending Space information to
automatically determining how innovative an idea can be, answering questions
about Space, and generating quizzes regarding quality procedures. Each of these
accomplishments represents a step forward in the application of increasingly
intelligent AI systems in Space, from structuring and facilitating information
access to intelligent systems capable to understand and reason with such
information."
239,,"[20] have utilized Topic modeling in cloud computing and to discover eﬃ-
cient cloud services, LDA is leveraged.","To ﬁnd the research trends, methodology
and ﬁelds of further research in blockchain technology, Shahid et al.","[21] have

4

Joy et al.",2022-10-07 13:04:58+00:00,An Empirical Studies on How the Developers Discussed about Pandas Topics,cs.SE,"['cs.SE', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Sajib Kumar Saha Joy'), arxiv.Result.Author('Farzad Ahmed'), arxiv.Result.Author('Al Hasib Mahamud'), arxiv.Result.Author('Nibir Chandra Mandal')]","Pandas is defined as a software library which is used for data analysis in
Python programming language. As pandas is a fast, easy and open source data
analysis tool, it is rapidly used in different software engineering projects
like software development, machine learning, computer vision, natural language
processing, robotics, and others. So a huge interests are shown in software
developers regarding pandas and a huge number of discussions are now becoming
dominant in online developer forums, like Stack Overflow (SO). Such discussions
can help to understand the popularity of pandas library and also can help to
understand the importance, prevalence, difficulties of pandas topics. The main
aim of this research paper is to find the popularity and difficulty of pandas
topics. For this regard, SO posts are collected which are related to pandas
topic discussions. Topic modeling are done on the textual contents of the
posts. We found 26 topics which we further categorized into 5 board categories.
We observed that developers discuss variety of pandas topics in SO related to
error and excepting handling, visualization, External support, dataframe, and
optimization. In addition, a trend chart is generated according to the
discussion of topics in a predefined time series. The finding of this paper can
provide a path to help the developers, educators and learners. For example,
beginner developers can learn most important topics in pandas which are
essential for develop any model. Educators can understand the topics which seem
hard to learners and can build different tutorials which can make that pandas
topic understandable. From this empirical study it is possible to understand
the preferences of developers in pandas topic by processing their SO posts"
240,,[26] indicate that privacy awareness should be created with the help of XAI.,"Further research should investigate
whether privacy concerns about a healthcare application persist even if they can be explained.",Users Want Detailed Explanations Similar to the results of pilot studies conducted in Hald et al.,2022-10-07 12:51:27+00:00,What Do End-Users Really Want? Investigation of Human-Centered XAI for Mobile Health Apps,cs.HC,"['cs.HC', 'cs.AI', 'J.3']","[arxiv.Result.Author('Katharina Weitz'), arxiv.Result.Author('Alexander Zellner'), arxiv.Result.Author('Elisabeth André')]","In healthcare, AI systems support clinicians and patients in diagnosis,
treatment, and monitoring, but many systems' poor explainability remains
challenging for practical application. Overcoming this barrier is the goal of
explainable AI (XAI). However, an explanation can be perceived differently and,
thus, not solve the black-box problem for everyone. The domain of
Human-Centered AI deals with this problem by adapting AI to users. We present a
user-centered persona concept to evaluate XAI and use it to investigate
end-users preferences for various explanation styles and contents in a mobile
health stress monitoring application. The results of our online survey show
that users' demographics and personality, as well as the type of explanation,
impact explanation preferences, indicating that these are essential features
for XAI design. We subsumed the results in three prototypical user personas:
power-, casual-, and privacy-oriented users. Our insights bring an interactive,
human-centered XAI closer to practical application."
241,,"Additionally, as noted earlier, our method may
not be able to reduce all kinds of toxicity, especially
when it comes to subtler toxicity (sexism, microag-
gressions, etc.)","Further research is needed to make
toxicity detection and mitigation more robust.","References

Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al.",2022-10-06 18:52:24+00:00,Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('David Wingate'), arxiv.Result.Author('Mohammad Shoeybi'), arxiv.Result.Author('Taylor Sorensen')]","We explore the idea of compressing the prompts used to condition language
models, and show that compressed prompts can retain a substantive amount of
information about the original prompt. For severely compressed prompts, while
fine-grained information is lost, abstract information and general sentiments
can be retained with surprisingly few parameters, which can be useful in the
context of decode-time algorithms for controllability and toxicity reduction.
We explore contrastive conditioning to steer language model generation towards
desirable text and away from undesirable text, and find that some complex
prompts can be effectively compressed into a single token to guide generation.
We also show that compressed prompts are largely compositional, and can be
constructed such that they can be used to control independent aspects of
generated text."
242,,"Experimental results suggest that existing vision models
are not sufﬁciently equipped to provide meaningful outputs for ambiguous images
and that datasets of this nature can be used to assess and improve such models
through model training and direct evaluation of model calibration.","These ﬁndings
motivate large-scale ambiguous dataset creation and further research focusing on
noisy visual data.1

1

Introduction

When making decisions, the human brain uses perceptual uncertainty judgments to account for
missing visual information and other noise [22, 2, 26].","For instance, when humans enter a new
environment, they must quickly gauge what events are taking place in it using limited sensory
input [53, 80, 79].",2022-10-06 17:52:20+00:00,Ambiguous Images With Human Judgments for Robust Visual Event Classification,cs.CV,"['cs.CV', 'cs.AI', 'I.2.10; I.4.8; I.2.0']","[arxiv.Result.Author('Kate Sanders'), arxiv.Result.Author('Reno Kriz'), arxiv.Result.Author('Anqi Liu'), arxiv.Result.Author('Benjamin Van Durme')]","Contemporary vision benchmarks predominantly consider tasks on which humans
can achieve near-perfect performance. However, humans are frequently presented
with visual data that they cannot classify with 100% certainty, and models
trained on standard vision benchmarks achieve low performance when evaluated on
this data. To address this issue, we introduce a procedure for creating
datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a
collection of noisy images extracted from videos. All images are annotated with
ground truth values and a test set is annotated with human uncertainty
judgments. We use this dataset to characterize human uncertainty in vision
tasks and evaluate existing visual event classification models. Experimental
results suggest that existing vision models are not sufficiently equipped to
provide meaningful outputs for ambiguous images and that datasets of this
nature can be used to assess and improve such models through model training and
direct evaluation of model calibration. These findings motivate large-scale
ambiguous dataset creation and further research focusing on noisy visual data."
243,,"We compare the target embeddings produced
by MLE and those by BERT, ﬁnding the latter to
be considerably stronger.","Those results in low re-
sources settings, encourage further research aiming
to address the problem of large action space for TG
in richer data settings by adapting and extending
our methods.","Future work will increase the exploration abil-
ity of RL training in NMT.",2022-10-06 16:58:27+00:00,Reinforcement Learning with Large Action Spaces for Neural Machine Translation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Asaf Yehudai'), arxiv.Result.Author('Leshem Choshen'), arxiv.Result.Author('Lior Fox'), arxiv.Result.Author('Omri Abend')]","Applying Reinforcement learning (RL) following maximum likelihood estimation
(MLE) pre-training is a versatile method for enhancing neural machine
translation (NMT) performance. However, recent work has argued that the gains
produced by RL for NMT are mostly due to promoting tokens that have already
received a fairly high probability in pre-training. We hypothesize that the
large action space is a main obstacle to RL's effectiveness in MT, and conduct
two sets of experiments that lend support to our hypothesis. First, we find
that reducing the size of the vocabulary improves RL's effectiveness. Second,
we find that effectively reducing the dimension of the action space without
changing the vocabulary also yields notable improvement as evaluated by BLEU,
semantic similarity, and human evaluation. Indeed, by initializing the
network's final fully connected layer (that maps the network's internal
dimension to the vocabulary dimension), with a layer that generalizes over
similar actions, we obtain a substantial improvement in RL performance: 1.5
BLEU points on average."
244,,"Through an extensive empirical evaluation comparing
eight state-of-the-art methods on the basis of (i) metrics directly evaluating the
skills’ diversity, (ii) the skills’ performance on adaptation tasks, and (iii) the skills’
performance when used as primitives for hierarchical planning; QD methods are
found to provide equal, and sometimes improved, performance whilst being less
sensitive to hyperparameters and more scalable.","As no single method is found to
provide near-optimal performance across all environments, there is a rich scope for
further research which we support by proposing future directions and providing
optimized open-source implementations.","1

INTRODUCTION

In the past decade, Reinforcement Learning (RL) has shown great promise at tackling sequential
decision making problems in a generic fashion, leading to breakthroughs in many ﬁelds such as games
Silver et al.",2022-10-06 11:06:39+00:00,Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery,cs.NE,"['cs.NE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Felix Chalumeau'), arxiv.Result.Author('Raphael Boige'), arxiv.Result.Author('Bryan Lim'), arxiv.Result.Author('Valentin Macé'), arxiv.Result.Author('Maxime Allard'), arxiv.Result.Author('Arthur Flajolet'), arxiv.Result.Author('Antoine Cully'), arxiv.Result.Author('Thomas Pierrot')]","Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for
training neural policies to solve complex control tasks. However, these
policies tend to be overfit to the exact specifications of the task and
environment they were trained on, and thus do not perform well when conditions
deviate slightly or when composed hierarchically to solve even more complex
tasks. Recent work has shown that training a mixture of policies, as opposed to
a single one, that are driven to explore different regions of the state-action
space can address this shortcoming by generating a diverse set of behaviors,
referred to as skills, that can be collectively used to great effect in
adaptation tasks or for hierarchical planning. This is typically realized by
including a diversity term - often derived from information theory - in the
objective function optimized by RL. However these approaches often require
careful hyperparameter tuning to be effective. In this work, we demonstrate
that less widely-used neuroevolution methods, specifically Quality Diversity
(QD), are a competitive alternative to information-theory-augmented RL for
skill discovery. Through an extensive empirical evaluation comparing eight
state-of-the-art methods on the basis of (i) metrics directly evaluating the
skills' diversity, (ii) the skills' performance on adaptation tasks, and (iii)
the skills' performance when used as primitives for hierarchical planning; QD
methods are found to provide equal, and sometimes improved, performance whilst
being less sensitive to hyperparameters and more scalable. As no single method
is found to provide near-optimal performance across all environments, there is
a rich scope for further research which we support by proposing future
directions and providing optimized open-source implementations."
245,,"Furthermore, the ratio of different sources is
unclear.","Thus, further research in realistic cost functions is
required, but this wasn’t part of the scope of this project.","Future work should seek to build prototypes of optimized
designs and validate them on the physical system.",2022-10-06 08:37:52+00:00,Meta Reinforcement Learning for Optimal Design of Legged Robots,cs.RO,"['cs.RO', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Álvaro Belmonte-Baeza'), arxiv.Result.Author('Joonho Lee'), arxiv.Result.Author('Giorgio Valsecchi'), arxiv.Result.Author('Marco Hutter')]","The process of robot design is a complex task and the majority of design
decisions are still based on human intuition or tedious manual tuning. A more
informed way of facing this task is computational design methods where design
parameters are concurrently optimized with corresponding controllers. Existing
approaches, however, are strongly influenced by predefined control rules or
motion templates and cannot provide end-to-end solutions. In this paper, we
present a design optimization framework using model-free meta reinforcement
learning, and its application to the optimizing kinematics and actuator
parameters of quadrupedal robots. We use meta reinforcement learning to train a
locomotion policy that can quickly adapt to different designs. This policy is
used to evaluate each design instance during the design optimization. We
demonstrate that the policy can control robots of different designs to track
random velocity commands over various rough terrains. With controlled
experiments, we show that the meta policy achieves close-to-optimal performance
for each design instance after adaptation. Lastly, we compare our results
against a model-based baseline and show that our approach allows higher
performance while not being constrained by predefined motions or gait patterns."
246,,"There 

remain a great deal of practical challenges with attaining the intended virtues of reflexivity 

in organisational spaces fraught with multiple, conflicting logics such as universities 

[author date].","Despite this, we support a reflexive turn in data science and recommend 

much needed further research in this direction.","At the very least, a reflexive and 

transparent approach seeks to avoid shifting the blame to the data and external sources, 

acknowledge partiality (as opposed to deceptive efforts to debias) and the distribution of 

collective responsibility within the actors and institutions involved in constructing and 

deploying a model.",2022-10-05 17:34:51+00:00,Addressing contingency in algorithmic misinformation detection: Toward a responsible innovation agenda,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'cs.SI']","[arxiv.Result.Author('Andrés Domínguez Hernández'), arxiv.Result.Author('Richard Owen'), arxiv.Result.Author('Dan Saattrup Nielsen'), arxiv.Result.Author('Ryan McConville')]","Machine learning (ML) enabled classification models are becoming increasingly
popular for tackling the sheer volume and speed of online misinformation. In
building these models, data scientists need to take a stance on the legitimacy,
authoritativeness and objectivity of the sources of `truth' used for model
training and testing. This has political, ethical and epistemic implications
which are rarely addressed in technical papers. Despite (and due to) their
reported high performance, ML-driven moderation systems have the potential to
shape online public debate and create downstream negative impacts such as undue
censorship and reinforcing false beliefs. This article reports on a responsible
innovation (RI) inflected collaboration at the intersection of social studies
of science and data science. We identify a series of algorithmic
contingencies--key moments during model development which could lead to
different future outcomes, uncertainty and harmful effects. We conclude by
offering an agenda of reflexivity and responsible development of ML tools for
combating misinformation."
247,,Table 5 summarizes this ablation.,"Comparing
the two variants demonstrates that an appropriately initialized prompt consistently outperforms a
randomly initialized prompt, highlighting the necessity for further research of the prompt space.",Table 5: Prompt initialization.,2022-10-05 17:05:56+00:00,Variational prompt tuning improves generalization of vision-language models,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Mohammad Mahdi Derakhshani'), arxiv.Result.Author('Enrique Sanchez'), arxiv.Result.Author('Adrian Bulat'), arxiv.Result.Author('Victor Guilherme Turrisi da Costa'), arxiv.Result.Author('Cees G. M. Snoek'), arxiv.Result.Author('Georgios Tzimiropoulos'), arxiv.Result.Author('Brais Martinez')]","Prompt tuning provides an efficient mechanism to adapt large vision-language
models to downstream tasks by treating part of the input language prompts as
learnable parameters while freezing the rest of the model. Existing works for
prompt tuning are however prone to damaging the generalization capabilities of
the foundation models, because the learned prompts lack the capacity of
covering certain concepts within the language model. To avoid such limitation,
we propose a probabilistic modeling of the underlying distribution of prompts,
allowing prompts within the support of an associated concept to be derived
through stochastic sampling. This results in a more complete and richer
transfer of the information captured by the language model, providing better
generalization capabilities for downstream tasks. The resulting algorithm
relies on a simple yet powerful variational framework that can be directly
integrated with other developments. We show our approach is seamlessly
integrated into both standard and conditional prompt learning frameworks,
improving the performance on both cases considerably, especially with regards
to preserving the generalization capability of the original model. Our method
provides the current state-of-the-art for prompt learning, surpassing CoCoOp by
1.6% average Top-1 accuracy on the standard benchmark. Remarkably, it even
surpasses the original CLIP model in terms of generalization to new classes.
Implementation code will be released."
248,,"One could also generalize our approach to non-Gaussian
likelihoods, making UNLIMITD effective for classiﬁcation tasks.","Finally, further research can push

9

−4−2024x−3−2−10123yIn-distOoD1OoD2246810K0.50.60.70.80.91.0AUC-ROCscoreUNLIMTD-F(inﬁnite)UNLIMTD-F(ﬁnite)UNLIMTD-R(inﬁnite)UNLIMTD-R(ﬁnite)UNLIMTD-I(inﬁnite)UNLIMTD-I(ﬁnite)246810K0.00.10.20.30.40.5MSEUNLIMTD-F(inﬁnite)UNLIMTD-F(ﬁnite)UNLIMTD-R(inﬁnite)UNLIMTD-R(ﬁnite)UNLIMTD-I(inﬁnite)UNLIMTD-I(ﬁnite)MAML(inﬁnite)MAML(ﬁnite)−4−2024x−3−2−10123yIn-dist1In-dist2OoD246810K0.50.60.70.80.91.0AUC-ROCscoreUNLIMTD-F(mixture)UNLIMTD-F(singleGP)246810K0.00.10.20.30.40.5MSEUNLIMTD-F(mixture)UNLIMTD-F(singleGP)MMAMLMAMLthe limits of multimodal meta-learning, e.g., by implementing non-parametric Bayesian methods
to automatically infer an optimal number of clusters, thereby eliminating a hyperparameter of the
current approach.","ACKNOWLEDGEMENTS

The authors acknowledge the MIT SuperCloud (Reuther et al., 2018) and Lincoln Laboratory Super-
computing Center for providing HPC resources that have contributed to the research results reported
within this paper.",2022-10-04 20:02:25+00:00,Uncertainty-Aware Meta-Learning for Multimodal Task Distributions,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cesar Almecija'), arxiv.Result.Author('Apoorva Sharma'), arxiv.Result.Author('Navid Azizan')]","Meta-learning or learning to learn is a popular approach for learning new
tasks with limited data (i.e., few-shot learning) by leveraging the
commonalities among different tasks. However, meta-learned models can perform
poorly when context data is limited, or when data is drawn from an
out-of-distribution (OoD) task. Especially in safety-critical settings, this
necessitates an uncertainty-aware approach to meta-learning. In addition, the
often multimodal nature of task distributions can pose unique challenges to
meta-learning methods. In this work, we present UnLiMiTD (uncertainty-aware
meta-learning for multimodal task distributions), a novel method for
meta-learning that (1) makes probabilistic predictions on in-distribution tasks
efficiently, (2) is capable of detecting OoD context data at test time, and (3)
performs on heterogeneous, multimodal task distributions. To achieve this goal,
we take a probabilistic perspective and train a parametric, tuneable
distribution over tasks on the meta-dataset. We construct this distribution by
performing Bayesian inference on a linearized neural network, leveraging
Gaussian process theory. We demonstrate that UnLiMiTD's predictions compare
favorably to, and outperform in most cases, the standard baselines, especially
in the low-data regime. Furthermore, we show that UnLiMiTD is effective in
detecting data from OoD tasks. Finally, we confirm that both of these findings
continue to hold in the multimodal task-distribution setting."
249,,"Indoor environment quality plays an essen-
tial role in the health and well-being of human be-
ings, Clements et al.","(2019) presented a living lab to
simulate real oﬃce spaces to support further research
on environmental monitoring in the built environment.","Occupancy monitoring is essential to determine air-
conditioning and illumination requirements in build-
ings, Erickson et al.",2022-10-04 18:16:36+00:00,Detecting Anomalies within Smart Buildings using Do-It-Yourself Internet of Things,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yasar Majib'), arxiv.Result.Author('Mahmoud Barhamgi'), arxiv.Result.Author('Behzad Momahed Heravi'), arxiv.Result.Author('Sharadha Kariyawasam'), arxiv.Result.Author('Charith Perera')]","Detecting anomalies at the time of happening is vital in environments like
buildings and homes to identify potential cyber-attacks. This paper discussed
the various mechanisms to detect anomalies as soon as they occur. We shed light
on crucial considerations when building machine learning models. We constructed
and gathered data from multiple self-build (DIY) IoT devices with different
in-situ sensors and found effective ways to find the point, contextual and
combine anomalies. We also discussed several challenges and potential solutions
when dealing with sensing devices that produce data at different sampling rates
and how we need to pre-process them in machine learning models. This paper also
looks at the pros and cons of extracting sub-datasets based on environmental
conditions."
